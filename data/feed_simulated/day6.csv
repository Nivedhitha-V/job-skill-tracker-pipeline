job_title,company,job_location,job_link,first_seen,search_city,search_country,job level,job_type,job_summary,job_skills,job_skills_raw,job_skills_cleaned
Data Analyst,Millennium Corporation,"Huntsville, AL",https://www.linkedin.com/jobs/view/data-analyst-at-millennium-corporation-3773576986,2023-12-17,Alabama,United States,Mid senior,Onsite,"Millennium Corporation is hiring a
Data Analyst to work in Huntsville, AL. The candidate must have an active secret clearance.
The Data Analyst will:
Interact with government personnel to provide comprehensive information within tight time constraints.
Demonstrate ability to research and create detailed computing system documentation.
Create visual representations of cyber activities relating to persistent operations.
Comprehensively document significant findings.
Collaborate with the Data Analyst team, cyber operators, and customer to generate weekly reports to decision makers.
Qualifications:
Active Secret Clearance.
Bachelor’s Degree from an accredited college or university.
5 or more years of practical experience as a Data Analyst.
Proven analytics skills, including mining, evaluation, and visualization.
Technical writing experience in relevant areas, including queries, reports, and presentations.
Ability to interpret on network activities and represent those activities graphically.
Strong SQL or Excel skills, with aptitude for learning other analytics tools.
Experience with improving deliverables, refining content, and creating visuals and diagrams for technical support content.
Preferred Experience:
Experience working in a secure DoD environment.
Experience with database and model design and segmentation techniques.
Practical experience in statistical analysis using statistical packages, including Excel, SPSS, and SAS.
Proven success in a collaborative, team-oriented environment.
Show more
Show less","Data Analysis, Secret Clearance, Cybersecurity, SQL, Excel, Data Visualization, Documentation, Technical Writing, Network Analysis, Statistical Analysis, DoD Environment, Database Design, Segmentation Techniques, SPSS, SAS","data analysis, secret clearance, cybersecurity, sql, excel, data visualization, documentation, technical writing, network analysis, statistical analysis, dod environment, database design, segmentation techniques, spss, sas","cybersecurity, dataanalytics, database design, documentation, dod environment, excel, network analysis, sas, secret clearance, segmentation techniques, spss, sql, statistical analysis, technical writing, visualization"
Senior Data Analyst Engineer Levels I - III,Gray Analytics,"Huntsville, AL",https://www.linkedin.com/jobs/view/senior-data-analyst-engineer-levels-i-iii-at-gray-analytics-3772936548,2023-12-17,Alabama,United States,Mid senior,Onsite,"Gray Analytics was founded in 2018 with a vision to bring innovative and creative solutions in the cybersecurity, IT, engineering, and scientific spheres. Our customers span across the commercial and federal domains with our goal being to bring excellent customer service to our clients and employees.
Without the bureaucracy that often exists in larger corporations, Gray Analytics offers increased work flexibility, visibility in company progress, and greater opportunities for advancement. It's with our employees' support that we can help our clients achieve mission and operational success.
At Gray Analytics, our goal is simple: to help our country, its businesses, and its organizations improve security in the Cyber realm. Period.
Location:
Redstone Arsenal, AL
Status:
Full-time exempt
**This role is contingent upon contract award**
What You'll Be Doing
Perform database design activities for software development and lab infrastructure support capabilities.
Work with the Software Development Team to define the structure and metadata for collection of MDA Test Resource and Asset objects.
Must Haves
Experienced in the development of a SQL Server based database architecture for application defined assets, conditions, constraints, test objectives, and other objects required to support an asset-based scheduling application.
Expertise in designing an operational database that supports optimization assessments of scheduling assets.
Level I Requirements:
0-10 years of experience.
Level II Requirements:
10-14 years of experience.
Level III Requirements:
15+ years of experience.
Security Requirements
An Active Secret Clearance is required in order to be considered.
About Gray Analytics
Gray Analytics values our employees as our most important resource. To showcase these values, we offer not only traditional medical, disability, life, etc. coverages that begin on day one of employment, but also unique benefits to improve our employees' quality of life. Some of these unique benefits include:
A PTO policy based on total years of experience, not years of service to the company. PTO is available for use immediately at hire, subject to company needs.
Eligibility for 401K contributions and company matching, Pet Insurance through Spot, Flexible Spending Account, and Tuition and Professional Development Funds begin on day one of employment.
Charitable donations program on a yearly and quarterly basis where employees can nominate a non-profit of choice to receive donations.
Gray Analytics is an Equal Opportunity Employer and VEVRAA Federal Contractor. This contractor and subcontractor shall abide by the requirements of 41 CFR 60-1.4(a), 60-300.5(a) and 60-741.5(a). These regulations prohibit discrimination against qualified individuals based on their status as protected veterans or individuals with disabilities and prohibit discrimination against all individuals based on their race, color, religion, sex, gender identity, sexual orientation, or national origin. Moreover, these regulations require that covered prime contractors and subcontractors take affirmative action to employ and advance in employment individuals without regard to race, color, religion, sex, gender identity, sexual orientation, national origin, protected veteran status or disability. Gray Analytics, Inc. welcomes minority and veteran applicants.
Show more
Show less","SQL Server, Database design, Software development, Optimization assessments, Scheduling assets, Secret Clearance","sql server, database design, software development, optimization assessments, scheduling assets, secret clearance","database design, optimization assessments, scheduling assets, secret clearance, software development, sql server"
Principal Data Engineer,Motion,"Birmingham, AL",https://www.linkedin.com/jobs/view/principal-data-engineer-at-motion-3696998097,2023-12-17,Alabama,United States,Mid senior,Onsite,"Job Description…
This Principal Data Engineer role will lead the design and development of solutions that power Motion’s data analytics. The role will work with data partners all over the world to build enterprise-class solutions that promote innovation and insights with data. Motion offers an excellent benefits package that includes options for healthcare coverage, 401(k), tuition reimbursement, vacation, sick, and holiday pay. You must be eligible to work in the US without Visa Sponsorship.
Responsibilities:
Develop and maintain a framework for consuming data from various sources in various formats
Design and develop database objects/models to support data reporting and analytics
Develop and support business intelligence reporting across the organization
Responsible for stewarding and/or leading team standards, procedures and methodologies associated with data ingestion, warehousing, and analytics
Responsible for mentoring peers in standards and practices. May lead and direct the work of others
Collaborate with internal and external stakeholders to understand the business needs related to data
Maintain knowledge of the tools and technology available to forward data reporting and analysis
Qualifications:
7+ years of data engineering/ETL with focus on data warehousing
3+ years of data reporting with business intelligence tools
Proficiency in SQL
Proficiency with data warehouse design, concepts, and practices
Proficiency with data integration/ETL tools, Informatica specifically
Exposure to Google BigQuery
A Bachelor's, or associates, Degree in a related field or equivalent work experience
Attention to detail
Preferred:
Proficiency with Power BI
Experience with DB2
Experience with AI/ML development
Experience with Java or Python
Understanding of service-oriented architecture
Not the right fit? Let us know you're interested in a future opportunity by joining our Talent Community on jobs.genpt.com or create an account to set up email alerts as new job postings become available that meet your interest!
GPC conducts its business without regard to sex, race, creed, color, religion, marital status, national origin, citizenship status, age, pregnancy, sexual orientation, gender identity or expression, genetic information, disability, military status, status as a veteran, or any other protected characteristic. GPC's policy is to recruit, hire, train, promote, assign, transfer and terminate employees based on their own ability, achievement, experience and conduct and other legitimate business reasons.
Show more
Show less","Data Engineering, ETL, Data Warehousing, Database Design, Business Intelligence, SQL, Data Integration, Informatica, Google BigQuery, Power BI, DB2, AI/ML Development, Java, Python, ServiceOriented Architecture","data engineering, etl, data warehousing, database design, business intelligence, sql, data integration, informatica, google bigquery, power bi, db2, aiml development, java, python, serviceoriented architecture","aiml development, business intelligence, data engineering, data integration, database design, datawarehouse, db2, etl, google bigquery, informatica, java, powerbi, python, serviceoriented architecture, sql"
Clinical Data Analyst,Children's of Alabama,"Birmingham, AL",https://www.linkedin.com/jobs/view/clinical-data-analyst-at-children-s-of-alabama-3747486689,2023-12-17,Alabama,United States,Mid senior,Onsite,"Summary
Job Summary
Responsible for providing support to the Performance Improvement Department with statistical analysis of performance improvement data using quality control tools. Identifies trends and other relationships among data. Develops report specifications and collaborate with clinical report writers on data extraction and new requests. An ability to translate other types of data into easily understood formats. Ability to present a summary of quality data for use by the medical staff, administration, and departments in performance improvement.
Education
Bachelor's degree in Health Information Management, Healthcare Administration or other healthcare related field required.
Master's degree in Healthcare Administration, Public Health, or Business Analytics preferred
Experience
2 years experience in healthcare data management and/or quality improvement
Licensures, Certifications, and/or Registries
Additional Information
Main Campus Children's of Alabama
Show more
Show less","Statistical analysis, Data extraction, Report writing, Data visualization, Quality improvement, Healthcare data management, Health Information Management, Healthcare Administration, Public Health, Business Analytics","statistical analysis, data extraction, report writing, data visualization, quality improvement, healthcare data management, health information management, healthcare administration, public health, business analytics","business analytics, data extraction, health information management, healthcare administration, healthcare data management, public health, quality improvement, report writing, statistical analysis, visualization"
DATA BASE ANALYST III,University of Alabama at Birmingham,"Birmingham, AL",https://www.linkedin.com/jobs/view/data-base-analyst-iii-at-university-of-alabama-at-birmingham-3760654811,2023-12-17,Alabama,United States,Mid senior,Onsite,"Position Summary:
Participates in analysis, design, estimates, development, documentation, testing and implementation of new database back-end web applications. Performs standard database maintenance and administration tasks Enhance database security to follow industry best practices Perform duties of a scrum master Performs routine configuration, installation, and reconfiguration of database and related products Identifies and investigates complex database problems and issues and recommends corrective actions Uses database management system software and tools to collect performance statistics Create innovative reports and dashboards Provides maintenance and support for programs within existing systems. Liaison with user, technical support, training, and operations personnel. Provides technical and administrative support to the department. .Net web application development using ASP, C#, VB, PL/SQL, CSS, AJAX and other JavaScript Frameworks like JQuery and HTML5 Experience in MS SQL, MySQL, IIS, PHP, and Apache .NET Framework, MVC, Razor Experience in Web Development technologies: XML, JSON, Web Services Expertise in PL/SQL(triggers, procedures, functions and packages), SQL Developer, Oracle SQL Knowledge of Team Foundation Server, IIS, Windows Services, Task Schedulers Knowledge of Agile Development Methodologies and frameworks such as SCRUM An understanding of system analysis and design and a proven ability to interface effectively with users and administrative personnel Must have excellent interpersonal skills and have had client facing roles which encompass all phases of the SDLC. Train junior staff
Duties and Responsibilities :
Directs design of new data bases/files and fine-tune performance of existing data bases/files.
Consults with members of department as required to perform problem determination.
Maintains data base recoverability.
Provides support to the department as technical consultant for planning and implementing systems with ready accessibility to appropriate data.
Controls the security of all data in the data base.
Handles special file-related projects
Extends professional skills and knowledge.
Provides technical and administrative support to the department
Perform other duties as assigned
Salary Range: $67,070 - $108,990
Bachelor's degree in Computer Science, Engineering, Math or a related field and five (5) years of related experience required. Work experience may substitute for education requirement. A list of experience and education substitutions can be found on the UAB Compensation website, if job allows for such substitutions.
Preferred Qualifications:
We are always on the lookout for dynamic and energetic person to assist us with these responsibilities. There are tremendous opportunities for growth and professional development. In addition to the technical responsibilities, the person will have these additional duties.
Be a team player
Self-motivated
Effortlessly work in a constantly demanding environment
Work under minimal guidance
Excel in Customer Service
Evaluate, test, and suggest new technologies
Documentation
Work closely with HSIS and UAB Central IT on integration.
Primary Location
University
Job Category
Information Technology
Organization
311400000 Department of Medicine Chair Office
Employee Status
Regular
Shift
Day/1st Shift
Show more
Show less","ASP, C#, VB, PL/SQL, CSS, JavaScript, JQuery, HTML5, MS SQL, MySQL, IIS, PHP, Apache, Oracle SQL, Team Foundation Server, Windows Services, Task Schedulers, Agile Development, SCRUM, system analysis and design, SDLC, data structures, algorithms, data management, databases, networking, XML, JSON, Web Services","asp, c, vb, plsql, css, javascript, jquery, html5, ms sql, mysql, iis, php, apache, oracle sql, team foundation server, windows services, task schedulers, agile development, scrum, system analysis and design, sdlc, data structures, algorithms, data management, databases, networking, xml, json, web services","agile development, algorithms, apache, asp, c, css, data management, data structures, databases, html5, iis, javascript, jquery, json, ms sql, mysql, networking, oracle sql, php, plsql, scrum, sdlc, system analysis and design, task schedulers, team foundation server, vb, web services, windows services, xml"
Big Data Integration Engineer,"KBR, Inc.","Houston, AL",https://www.linkedin.com/jobs/view/big-data-integration-engineer-at-kbr-inc-3765779744,2023-12-17,Alabama,United States,Mid senior,Onsite,"Title:
Big Data Integration Engineer
About This Position
The successful candidate will be part of the KBR team supporting the Test Resource Management Center’s (TRMC) Big Data (BD) and Knowledge Management (KM) Team deploying BD and KM systems for DoD testing Ranges and various acquisition programs.
This is being hired nationwide as it is a remote work capable position. The candidate can either work in one of KBR’s facilities or work from home, assuming the candidate has a stable internet connection.
Responsibilities:
Deployment and integration of a highly visible data analytic project called Cloud Hybrid Edge-to-Enterprise Evaluation Test & Analysis Suite (CHEETAS) at multiple DoD ranges and labs
Work with the data science and software engineering team members to support our customers by demonstrating the ‘art of the possible’ with insights gained from analyzing DoD Test & Evaluation data
Deploy and configure Big Data and Knowledge Management tools in an enterprise environment
Configure and troubleshoot a variety of Big Data ecosystem tools
Work with a wide range of stakeholders and functional teams at various levels of experience
Become a CHEETAS deployment subject matter expert
Act as a critical part of our technical team responsible for deploying CHEETAS within customer environments
Work closely with system administrators and software developers to communicate, document and ultimately resolve deployment issues as they arise
Provide deployment services to various DoD testing Ranges and acquisition programs
Deploy CHEETAS within disparate environments (on different non-standard hardware stacks and integrated into different existing ecosystems) sometimes located within DoD vaults with no outside internet connectivity
Act as the frontline interface that customers will have when first experiencing CHEETAS within their DoD Range and lab environments
This requisition will be used to hire multiple individuals
Entry level Integration Engineers will
NOT
be considered due to the breadth of knowledge necessary to be successful in the position. This position is anticipated to require travel of 25% with surges possible up to 50% to support end users located at various DoD Ranges and Labs across the United States.
Come join the KBR BDKM team and be a part of the award-winning team responsible for revolutionizing how data analysis is performed across the entire Department of Defense!
Basic Qualifications
This position requires a bachelor's degree in a STEM Computer Science, Data Science, Statistics or related, technical field, and 7-10 years of experience. Entry level Integration Engineers will NOT be considered.
Previous experience must include five (5) years of hands-on experience in big data environments.
Previous experience must include three (3) years of hands-on experience with Kubernetes. Experience in the integration with and configuration of: Hadoop, SQL Server Big Data Cluster, Kubernetes, CentOS, Ubuntu, RedHat, Windows Server, VMWare, etc.)
Active or Current Secret Clearance required - Top Secret Clearance preferred.
Knowledge / Skills / Abilities:
Experience with installation, configuration, integration with and usage of the following tools and technologies: Helms Charts, YAML, Kubernetes, Kubectl, Kubernetes IDE, NFS, SMB, S3, SQL Server, Windows Server, Windows 10/11, Linux (CentOS, Ubuntu, RedHat), Hadoop.
Must be prepared to learn new business processes or CHEETAS application nuances every Agile sprint release (roughly every 6 weeks) prior to deploying to customer sites.
Experience with working in distributed team environment is preferred.
Ability to problem solve, debug, and troubleshoot while under pressure and time constraints is required.
Ability to communicate effectively about technical topics to both experts and non-experts at both the management and technical level is required.
Excellent interpersonal skills, oral and written communication skills, and strong personal motivation are necessary to succeed within this position.
Ability to work independently and provide appropriate recommendations for optimal design, analysis, and development.
Excellent written and verbal communications skills are required, as the Integration Engineer will be in frequent contact with the project technical lead, be taking direction from various government leads, and will frequently be interacting with end users to gather requirements and implement solutions while away from other team members.
Ability to teach and mentor engineers with a variety of skill levels and backgrounds is a plus.
Excellent testing, debugging and problem-solving skills are required to be successful in this position.
Experience designing, building, integrating with and maintaining both new and existing big data systems and solutions.
Additional Qualifications
The preferred candidate will have experience working in government/defense labs and their computing restrictions.
Knowledge of the Test and Training Enabling Architecture (TENA), the Joint Mission Environment Testing Capability (JMETC), and Distributed Testing and Training is a plus.
Experience working with major DoD Acquisition programs such as Joint Strike Fighter (JSF) or Missile Defense Agency (MDA) is a plus.
Knowledge of DoD Cybersecurity policies is a plus.
KBR is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, disability, sex, sexual orientation, gender identity or expression, age, national origin, veteran status, genetic information, union status and/or beliefs, or any other characteristic protected by federal, state, or local law.
Show more
Show less","Kubernetes, Big Data, SQL, Linux, Hadoop, CentOS, Ubuntu, RedHat, Windows Server, SQL Server, VMWare, YAML, Helms Charts, Kubernetes IDE, NFS, SMB, S3, Windows 10/11","kubernetes, big data, sql, linux, hadoop, centos, ubuntu, redhat, windows server, sql server, vmware, yaml, helms charts, kubernetes ide, nfs, smb, s3, windows 1011","big data, centos, hadoop, helms charts, kubernetes, kubernetes ide, linux, nfs, redhat, s3, smb, sql, sql server, ubuntu, vmware, windows 1011, windows server, yaml"
Data Engineer - Scala(U.S. remote),Railroad19,"Tuscaloosa, AL",https://www.linkedin.com/jobs/view/data-engineer-scala-u-s-remote-at-railroad19-3782858017,2023-12-17,Alabama,United States,Mid senior,Remote,"Railroad19, Inc.
is hiring
remote
Senior Data Engineers
to be a solid technical resource on a dynamic and growing team of accomplished engineers.
Our ideal candidate is passionate about creating well-architected solutions containing thoroughly tested code. The ability to communicate effectively and develop relationships by empathizing with client goals is a highly valued skill within our company culture.
Core Responsibilities:
Develop new and enhance existing application services
Writing tests to maintain code quality
Understand and adapt to our client's evolving business requirements within the television advertising domain.
Participate in detailed technical design sessions to understand client needs and provide productive feedback
Identify new opportunities, tools, and services to enhance the software platform
Support and troubleshoot issues, identify the root cause, and proactively recommend corrective actions
Skills & Experience:
Scala 2.12 + development experience
Passionate about developing clean and maintainable code with little or no side-effects
Experience building Restful APIs in Scala using Spark 2.4
Strong hands-on experience with AWS running spark jobs on ephemeral EMR clusters on AWS and S3.
Experience with relational and non-relational databases
Willingness to learn new technologies and takes pride in keeping up with the latest technologies and practices within the Scala and Spark development community
Excellent oral and written communication skills
Strong analytical and problem-solving skills
Self-directed and can effectively deliver solutions with little oversight
A bachelor's or master's degree in computer science, computer engineering, or other technical disciplines or equivalent work experience is preferred but not required.
$120,000 - $160,000 a year
Salary is commensurate with experience.
We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender, gender identity or expression, or veteran status. We are proud to be an equal-opportunity workplace.
#Hiringnow
We are actively hiring (Data Engineers)
#remote #Scala #DataEngineering #ApacheSpark
Show more
Show less","Scala 2.12, Spark 2.4, AWS, EMR, S3, Relational databases, Nonrelational databases, Restful APIs, Analytical skills, Problemsolving skills, Communication skills","scala 212, spark 24, aws, emr, s3, relational databases, nonrelational databases, restful apis, analytical skills, problemsolving skills, communication skills","analytical skills, aws, communication skills, emr, nonrelational databases, problemsolving skills, relational databases, restful apis, s3, scala 212, spark 24"
Data Engineer - Scala(U.S. remote),Railroad19,"Montgomery, AL",https://www.linkedin.com/jobs/view/data-engineer-scala-u-s-remote-at-railroad19-3782858015,2023-12-17,Alabama,United States,Mid senior,Remote,"Railroad19, Inc.
is hiring
remote
Senior Data Engineers
to be a solid technical resource on a dynamic and growing team of accomplished engineers.
Our ideal candidate is passionate about creating well-architected solutions containing thoroughly tested code. The ability to communicate effectively and develop relationships by empathizing with client goals is a highly valued skill within our company culture.
Core Responsibilities:
Develop new and enhance existing application services
Writing tests to maintain code quality
Understand and adapt to our client's evolving business requirements within the television advertising domain.
Participate in detailed technical design sessions to understand client needs and provide productive feedback
Identify new opportunities, tools, and services to enhance the software platform
Support and troubleshoot issues, identify the root cause, and proactively recommend corrective actions
Skills & Experience:
Scala 2.12 + development experience
Passionate about developing clean and maintainable code with little or no side-effects
Experience building Restful APIs in Scala using Spark 2.4
Strong hands-on experience with AWS running spark jobs on ephemeral EMR clusters on AWS and S3.
Experience with relational and non-relational databases
Willingness to learn new technologies and takes pride in keeping up with the latest technologies and practices within the Scala and Spark development community
Excellent oral and written communication skills
Strong analytical and problem-solving skills
Self-directed and can effectively deliver solutions with little oversight
A bachelor's or master's degree in computer science, computer engineering, or other technical disciplines or equivalent work experience is preferred but not required.
$120,000 - $160,000 a year
Salary is commensurate with experience.
We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender, gender identity or expression, or veteran status. We are proud to be an equal-opportunity workplace.
#Hiringnow
We are actively hiring (Data Engineers)
#remote #Scala #DataEngineering #ApacheSpark
Show more
Show less","Apache Spark, AWS S3, EMR, Scala, Spark 2.4, Restful APIs, Databases, Data Engineering, Computer Science, Computer Engineering","apache spark, aws s3, emr, scala, spark 24, restful apis, databases, data engineering, computer science, computer engineering","apache spark, aws s3, computer engineering, computer science, data engineering, databases, emr, restful apis, scala, spark 24"
Data Engineer - Scala(U.S. remote),Railroad19,"Birmingham, AL",https://www.linkedin.com/jobs/view/data-engineer-scala-u-s-remote-at-railroad19-3782852780,2023-12-17,Alabama,United States,Mid senior,Remote,"Railroad19, Inc.
is hiring
remote
Senior Data Engineers
to be a solid technical resource on a dynamic and growing team of accomplished engineers.
Our ideal candidate is passionate about creating well-architected solutions containing thoroughly tested code. The ability to communicate effectively and develop relationships by empathizing with client goals is a highly valued skill within our company culture.
Core Responsibilities:
Develop new and enhance existing application services
Writing tests to maintain code quality
Understand and adapt to our client's evolving business requirements within the television advertising domain.
Participate in detailed technical design sessions to understand client needs and provide productive feedback
Identify new opportunities, tools, and services to enhance the software platform
Support and troubleshoot issues, identify the root cause, and proactively recommend corrective actions
Skills & Experience:
Scala 2.12 + development experience
Passionate about developing clean and maintainable code with little or no side-effects
Experience building Restful APIs in Scala using Spark 2.4
Strong hands-on experience with AWS running spark jobs on ephemeral EMR clusters on AWS and S3.
Experience with relational and non-relational databases
Willingness to learn new technologies and takes pride in keeping up with the latest technologies and practices within the Scala and Spark development community
Excellent oral and written communication skills
Strong analytical and problem-solving skills
Self-directed and can effectively deliver solutions with little oversight
A bachelor's or master's degree in computer science, computer engineering, or other technical disciplines or equivalent work experience is preferred but not required.
$120,000 - $160,000 a year
Salary is commensurate with experience.
We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender, gender identity or expression, or veteran status. We are proud to be an equal-opportunity workplace.
#Hiringnow
We are actively hiring (Data Engineers)
#remote #Scala #DataEngineering #ApacheSpark
Show more
Show less","Scala, Spark, AWS, EMR, S3, Restful APIs, Relational databases, Nonrelational databases, Data Engineering, Software development, Troubleshooting, Problemsolving, Communication, Analytical skills","scala, spark, aws, emr, s3, restful apis, relational databases, nonrelational databases, data engineering, software development, troubleshooting, problemsolving, communication, analytical skills","analytical skills, aws, communication, data engineering, emr, nonrelational databases, problemsolving, relational databases, restful apis, s3, scala, software development, spark, troubleshooting"
Senior Data Engineer (50% REMOTE) with Security Clearance,ClearanceJobs,"Huntsville, AL",https://www.linkedin.com/jobs/view/senior-data-engineer-50%25-remote-with-security-clearance-at-clearancejobs-3753481742,2023-12-17,Alabama,United States,Mid senior,Remote,"Senior Data Engineer- ETL
Locations: Huntsville, Alabama (50% REMOTE)
Clearance Requirement: Active Top-Secret Clearance (SCI Eligibility) Technology is constantly changing, and our adversaries are digitally exceeding law enforcement’s
ability to keep pace. Those charged with protecting the United States are not always able to access the
evidence needed to prosecute crime and prevent terrorism. In response to this challenge, we are
seeking a Senior Data Engineer with ETL expertise to provide proven, industry leading capabilities to
our customer.
Experience with data exploration techniques and development of quantitative and qualitative data analysis process, design robust ETL pipelines.
Experience with ETL development, extraction, and parsing.
Work in a team environment to design, develop, and support a software system which is undergoing a modernization.
Participate in developing new functionality and migrating the application into the cloud and introducing new technologies into the tech stack.
Participate in Agile Scrum SDLC activities.
Support developing Agile SDLC phase documentation.
Perform unit and integration testing of software/systems prior to release to the users for user acceptance testing. Required Qualifications
BS degree and twelve (12) years of experience with programming and software development including analysis, design, development, implementation, testing, maintenance, quality assurance, troubleshooting and/or upgrading of software systems.
Five (5) years of experience architecting software solutions based on customer requirement.
Led a technical team for at least five (5) years.
Three (3) years of experience designing and implementing automated build and deployment pipelines and supporting platform.
Experience troubleshooting and supporting enterprise applications while maintaining 24/7 availability.
A current Top Secret security clearance with SCI eligibility and the ability to obtain a polygraph. Desired Qualifications
Knowledge of Tika
Experience with AWS cloud services
Knowledge of data acquisition and ingestion of structured and unstructured data sources ensuring quality and data integrity
Experience with open-source technologies like Docker, Elasticsearch, and NoSQL Databases,
Experience with an Agile environment and have developed User Stories.
Experience with AWS Lamda, SQS, or NiFi
Show more
Show less","ETL, Data exploration techniques, Quantitative analysis, Qualitative analysis, Data pipelines, Extraction, Parsing, Software modernization, Agile Scrum SDLC, Unit testing, Integration testing, User acceptance testing, Programming, Software development, Analysis, Design, Implementation, Maintenance, Quality assurance, Troubleshooting, Upgrading, Software solutions, Technical team leadership, Automated build and deployment pipelines, Platform support, Enterprise applications, Top Secret security clearance, Polygraph ability, Tika, AWS cloud services, Data acquisition, Data ingestion, Structured data sources, Unstructured data sources, Data quality, Data integrity, Opensource technologies, Docker, Elasticsearch, NoSQL databases, Agile environment, User Stories, AWS Lamda, SQS, NiFi","etl, data exploration techniques, quantitative analysis, qualitative analysis, data pipelines, extraction, parsing, software modernization, agile scrum sdlc, unit testing, integration testing, user acceptance testing, programming, software development, analysis, design, implementation, maintenance, quality assurance, troubleshooting, upgrading, software solutions, technical team leadership, automated build and deployment pipelines, platform support, enterprise applications, top secret security clearance, polygraph ability, tika, aws cloud services, data acquisition, data ingestion, structured data sources, unstructured data sources, data quality, data integrity, opensource technologies, docker, elasticsearch, nosql databases, agile environment, user stories, aws lamda, sqs, nifi","agile environment, agile scrum sdlc, analysis, automated build and deployment pipelines, aws cloud services, aws lamda, data acquisition, data exploration techniques, data ingestion, data integrity, data quality, datapipeline, design, docker, elasticsearch, enterprise applications, etl, extraction, implementation, integration testing, maintenance, nifi, nosql databases, opensource technologies, parsing, platform support, polygraph ability, programming, qualitative analysis, quality assurance, quantitative analysis, software development, software modernization, software solutions, sqs, structured data sources, technical team leadership, tika, top secret security clearance, troubleshooting, unit testing, unstructured data sources, upgrading, user acceptance testing, user stories"
Enterprise Data Architect Location-Remote,Executive Staff Recruiters / ESR Healthcare,"Mobile, AL",https://www.linkedin.com/jobs/view/enterprise-data-architect-location-remote-at-executive-staff-recruiters-esr-healthcare-3645960440,2023-12-17,Alabama,United States,Mid senior,Remote,"Company Profile
esrhealthcare.com.mysmartjobboard.com
Enterprise Data Architect
Location-Remote
Fulltime
Job Description:
10+ years of experience implementing analytics data solutions by working with and leveraging Microsoft Azure resources such as Azure Data Lake Storage, Azure Data Factory, Synapse, Data bricks, Logic Apps, Power BI and ML Studio
7 years of experience in data warehousing with expert knowledge of dimensional modeling concepts and performance tuning
The ability to evaluate existing data models and physical databases for variances, discrepancies, and redundancies
The ability to independently perform data profiling and analysis to understand source data
An understanding of the role of data security and privacy compliance and related best practices in their implementation
4 years of experience using Erwin Data Modeler R9.7 or later
Experience with data lineage and data cataloging tools such as Informatica EDC and Alation Data Catalog
Experience with data virtualization platforms such as Dremio or Denodo
Working knowledge and experience with Python, Scala, or R
Bachelor’s Degree or equivalent experience in Computer Science, Information Systems, or related disciplines.
Strong leadership skills.
Excellent communication and presentation skills
Domain knowledge in one or more corporate functions such as HR, Finance is preferred
Excellent problem solving skills with innovative thinking and proactive approach
Ability to recommend and implement best practices and processes
Experience with Microsoft AI/ML solutions
Experience in Agile development methodologies
Experience implementing Delta Lake and Data Lakehouse is preferred
Microsoft Azure Certifications related to Data and Analytics strongly preferred (e.g. Microsoft Certified: Azure Data Engineer DP200/201, and/or 70-467, or related MCSE )
Work across a broad set of stakeholders across both IT and business teams
Lead and manage a team of 3-4 Data Architects – need to have team management skills
Powered by Webbtree
Show more
Show less","Microsoft Azure, Azure Data Lake Storage, Azure Data Factory, Apache Synapse, Data Bricks, Logic Apps, Power BI, ML Studio, Data warehousing, Dimensional modeling, Data profiling, Data analysis, Data security, Data privacy compliance, Erwin Data Modeler, Informatica EDC, Alation Data Catalog, Dremio, Denodo, Python, Scala, R, Agile development methodologies, Delta Lake, Data Lakehouse, Microsoft AI/ML solutions","microsoft azure, azure data lake storage, azure data factory, apache synapse, data bricks, logic apps, power bi, ml studio, data warehousing, dimensional modeling, data profiling, data analysis, data security, data privacy compliance, erwin data modeler, informatica edc, alation data catalog, dremio, denodo, python, scala, r, agile development methodologies, delta lake, data lakehouse, microsoft aiml solutions","agile development methodologies, alation data catalog, apache synapse, azure data factory, azure data lake storage, data bricks, data lakehouse, data privacy compliance, data profiling, data security, dataanalytics, datawarehouse, delta lake, denodo, dimensional modeling, dremio, erwin data modeler, informatica edc, logic apps, microsoft aiml solutions, microsoft azure, ml studio, powerbi, python, r, scala"
Data Analyst,Marathon TS,"Huntsville, AL",https://www.linkedin.com/jobs/view/data-analyst-at-marathon-ts-3772606046,2023-12-17,Alabama,United States,Mid senior,Hybrid,"Marathon TS is seeking a
Senior Data Analyst
to join a contract with a federal government client in support of an important mission.
This
position requires employees to be located in the Huntsville, AL area
in order to report to work on-site at Redstone Arsenal.
Responsibilities
Provide highly complex data mining, statistical analysis, trend analysis, and causal analysis.
Perform as a technical lead responsible for monitoring and providing monthly contract reports and deliverables.
Responsible for integrating multiple disciplines in an operations research team and translating applicable methods into language and application understandable by operational managers throughout the organization.
Review and provide quality control methods on products.
Prepare and update training materials to ensure newly assigned personnel gain an understanding of key analytic tools, procedures, and methodologies used.
Take structured and unstructured data and distill the information into a cohesive analytical product for a contracting functional business area audience.
Support pattern analysis methods to formulate recommendations to operational managers based upon exploiting patterns in the past, current, and anticipated operational environment.
Education and Experience
8 plus years in a technical field
BA/BS required
Required Skills
Have experience advising senior DoD decision makers on methodologies, results, and conclusions from applied operations research.
Have experience with the primary tools used for research services include, but are not limited to, Logistics Management Program, Vantage, Virtual Contracting Enterprise, General Fund Enterprise Business System (
GFEBS
), SAP Business Objects/Web Intelligence Reports, Microsoft SharePoint, Army-specific contract writing systems (Procurement Desktop Defense (
PD2
) and Procurement Automated Data and Document System (
PADDS
)), and various Government and Commercial business process automation systems.
Personnel should have strong data manipulation and problem-solving skills.
Must have strong technical skills in areas such as statistics, programming languages like R or
Python
,
SQL
(Structured Query Language), data visualization, and data cleaning and preparation.
Have good communication skills and problem-solving ability.
Security Clearance
Active Secret clearance is required
Show more
Show less","Data mining, Statistical analysis, Trend analysis, Causal analysis, Technical lead, Contract reports, Deliverables, Operations research, Quality control, Training materials, Data visualization, Data cleaning, Data preparation, Logistics Management Program, Vantage, Virtual Contracting Enterprise, General Fund Enterprise Business System (GFEBS), SAP Business Objects/Web Intelligence Reports, Microsoft SharePoint, Armyspecific contract writing systems (Procurement Desktop Defense (PD2) and Procurement Automated Data and Document System (PADDS)), Government and Commercial business process automation systems, R, Python, SQL (Structured Query Language)","data mining, statistical analysis, trend analysis, causal analysis, technical lead, contract reports, deliverables, operations research, quality control, training materials, data visualization, data cleaning, data preparation, logistics management program, vantage, virtual contracting enterprise, general fund enterprise business system gfebs, sap business objectsweb intelligence reports, microsoft sharepoint, armyspecific contract writing systems procurement desktop defense pd2 and procurement automated data and document system padds, government and commercial business process automation systems, r, python, sql structured query language","armyspecific contract writing systems procurement desktop defense pd2 and procurement automated data and document system padds, causal analysis, contract reports, data cleaning, data mining, data preparation, deliverables, general fund enterprise business system gfebs, government and commercial business process automation systems, logistics management program, microsoft sharepoint, operations research, python, quality control, r, sap business objectsweb intelligence reports, sql structured query language, statistical analysis, technical lead, training materials, trend analysis, vantage, virtual contracting enterprise, visualization"
Senior Data Engineer,StoneX Group Inc.,"Greater Birmingham, Alabama Area",https://www.linkedin.com/jobs/view/senior-data-engineer-at-stonex-group-inc-3752005531,2023-12-17,Alabama,United States,Mid senior,Hybrid,"Overview
The Data Platform Team looks to raise the level and productivity of data engineering and data science by building, scaling, and supporting our big data infrastructure with an emphasis on simple and efficient solutions on top of complex distributed data stores. As a contributing senior data engineer, you will assist in architecting, designing, and implementing components within our cloud data platform expanding our data assets while continuously improving the architecture and processes around our daily operations.
Responsibilities
Create cloud and big data technical design recommendations for developing and integrating new software and system technologies – from the physical layer through to the virtual layer – per written specifications; test, evaluate, engineer, implement and support said technologies.
Review, influence and contribute to new and evolving design, architecture, standards, and methods for operating and contributing to services within our big data ecosystem.
Add to our existing business and data models. Reviews existing designs and processes to highlight more efficient ways to complete existing workload more effectively through industry perspectives.
Drive technical innovation and efficiency in infrastructure operations through automation by assisting in improvements to continuous integration, continuous deployment and
Create cloud and big data technical design recommendations for developing and integrating new software and system technologies – from the physical layer through to the virtual layer – per written specifications; test, evaluate, engineer, implement and support those technologies
Collaborates with technical teams and utilizes system expertise to deliver technical solutions, continuously learning and evolving big data skillsets.
Monitors and evaluates overall strategic data infrastructure; tracks system efficiency and reliability; identifies and recommends efficiency improvements and mitigates operational vulnerabilities. Respond to and resolve emergent service problems. Design solutions using automation and self-repair rather than relying on alarming and human intervention
Qualifications
Pursuing a Bachelor’s degree or relevant work experience in Computer Science, Mathematics, Electrical Engineering or related technical discipline.
5-7 years experience developing software in a professional environment (preferably financial services but not required)
Exposure to Docker/Containers, microservices, distributed systems architecture, Kubernetes, and cloud computing preferably Azure.
Comfortable with core programming concepts and techniques (e.g. concurrency, memory management)
ETL Python, Java, Scala, Data Sets, Data Catalogs, Data Warehousing, Realtime Data Streaming Technologies, AWS Glue, EMR, S3, Lambda, Redshift, Kinesis, Kafka, AWS Certifications","data engineering, big data, aws, gcp, azure, data pipelines, etl, data modeling, data storage, data processing, data retrieval, data quality, python, java, scala, data sets, data catalogs, data warehousing, realtime data streaming technologies, aws glue, emr, s3, lambda, redshift, kinesis, kafka, aws certifications","aws, aws certifications, aws glue, azure, big data, data catalogs, data engineering, data processing, data quality, data retrieval, data sets, data storage, datamodeling, datapipeline, datawarehouse, emr, etl, gcp, java, kafka, kinesis, lambda, python, realtime data streaming technologies, redshift, s3, scala",
Sr SAP DATA Engineer,Kellton,"Tampa, FL",https://www.linkedin.com/jobs/view/sr-sap-data-engineer-at-kellton-3726476208,2023-12-17,Clearwater,United States,Mid senior,Onsite,"Remote Long Term
12 Months with possible extension
Need someone with strong HANA DBA, performance management, tuning, skills with automation experience using Ansible, Terraform etc.
I have seen many strong SAP NW basis HANA DBA but they do not understand HANA for multi-tenant database and with automation skills.
You will be successful in this role if you have:
5+ years' experience administering high throughput, highly available, transactional database systems covering designing, installation, trouble shooting, and performance tuning in Hana.
Demonstrated programming skills in one or more of: Shell Scripting, Python specifically for infrastructure automation
Demonstrated DevOps automation using Terraform/Ansible.
Demonstrated deploying, managing, migrating production databases in GCP and AWS.
Tenacious troubleshooting skills that span systems, network, and application: you can fix anything
Willingness to take the initiative to contribute beyond your own responsibilities
Ability and interest to learn and deploy new technology quickly
Obsessively detail oriented
Bonus Points
B.S. or higher in Computer Science or other technical discipline
Cassandra and Redis Experience
Database geek
Deep UNIX/Linux systems knowledge and/or systems administration background
Passionate and self-motivated
Desire to not only maintain but improve existing systems
Strong focus on efficiency, simplicity and maintainability
Adept at using source code revision systems such as git or Perforce
Exposure to containerization.
CI/CD experience using Jenkins.
For detailed job description please send your resume at
nikhil.naik@kelltontech.net.
Desk- 630-725-1800 X 1060
Show more
Show less","HANA DBA, Performance management, Tuning, Ansible, Terraform, SAP NW basis, Shell Scripting, Python, Infrastructure automation, Terraform/Ansible, GCP, AWS, Troubleshooting, Cassandra, Redis, UNIX/Linux, Git, Perforce, Containerization, CI/CD, Jenkins","hana dba, performance management, tuning, ansible, terraform, sap nw basis, shell scripting, python, infrastructure automation, terraformansible, gcp, aws, troubleshooting, cassandra, redis, unixlinux, git, perforce, containerization, cicd, jenkins","ansible, aws, cassandra, cicd, containerization, gcp, git, hana dba, infrastructure automation, jenkins, perforce, performance management, python, redis, sap nw basis, shell scripting, terraform, terraformansible, troubleshooting, tuning, unixlinux"
(W2 Contract)  Data Scientist / Sr ML Engineer @ Tampa FL Or Irving Texas,Infinite Computer Solutions,"Tampa, FL",https://www.linkedin.com/jobs/view/w2-contract-data-scientist-sr-ml-engineer-%40-tampa-fl-or-irving-texas-at-infinite-computer-solutions-3776960304,2023-12-17,Clearwater,United States,Mid senior,Onsite,"Requirements:
Bachelor's or advanced degree in Computer Science, Data Science, or a related field.
Proven experience in developing and deploying machine learning models in a production environment.
Strong programming skills in languages such as Python, Java, or Scala.
Experience with machine learning frameworks such as TensorFlow, PyTorch, or scikit-learn.
Solid understanding of statistical modeling, data mining, and predictive analytics.
Familiarity with cloud platforms (e.g., AWS, Azure) and containerization technologies (e.g., Docker).
Knowledge of SRE principles and practices is a strong advantage.
Excellent problem-solving skills and ability to work in a collaborative team environment.
Effective communication skills to convey complex technical concepts to non-technical stakeholders.
Show more
Show less","Computer Science, Data Science, Machine Learning, Python, Java, Scala, TensorFlow, PyTorch, ScikitLearn, Statistical Modeling, Data Mining, Predictive Analytics, AWS, Azure, Docker, SRE, ProblemSolving, Teamwork, Communication","computer science, data science, machine learning, python, java, scala, tensorflow, pytorch, scikitlearn, statistical modeling, data mining, predictive analytics, aws, azure, docker, sre, problemsolving, teamwork, communication","aws, azure, communication, computer science, data mining, data science, docker, java, machine learning, predictive analytics, problemsolving, python, pytorch, scala, scikitlearn, sre, statistical modeling, teamwork, tensorflow"
Database Engineer,Allen Integrated Solutions LLC,"Tampa, FL",https://www.linkedin.com/jobs/view/database-engineer-at-allen-integrated-solutions-llc-3767319942,2023-12-17,Clearwater,United States,Mid senior,Onsite,"Job Title:
Database Engineer
Job ID: 1DE.TF
Location:
Tampa, FL - 100% onsite
Required Security Clearance:
current DoD Top Secret Clearance and eligible for SCI access and ACCM read-on
About Allen Integrated Solutions
We are proud to be a Veteran-Owned Small Business, committed to developing our employees. Being an Allen Integrated Solutions Team Member is more than a job, it is being part of a team committed to the security and defense of our Nation by serving our Government clients to achieve optimal performance with their mission, system, or program. If you share this same commitment, we are interested in you!
About The Role
Database engineers are responsible for developing and translating computer algorithms into prototype code and maintaining, organizing, and identifying trends in large data sets.
Design SQL databases 3
Create process documentation
Provide written and verbal communication
Work independently and on teams
Code in python, java, Kafka, hive, R, or storm
Oversee real-time business metric aggregation, data warehousing and querying, schema and data management, and related duties.
Design, develop, and test state-of-the-art Cloud-based database platforms
Implementation and maintain complex databases
Analyze procedures to control the access and allocation of data
Maintain security controls in supported databases
Apply advanced principles, theories, and concepts to job assignments and contribute to the development of new ideas and principles
Solve complex problems, under consultative direction, and represent the SOF Data Science teams across the SOF Intelligence Enterprise working on long-range programs and objectives
Provide advice to the SOF Intelligence Data Team customer and assist in overall functional strategic data planning
Work directly with the SOF DST to ensure the data systems supporting the DST are functioning optimally and can integrate any needed capabilities to support the team
Requirements:
Possess a minimum of a bachelor's degree in a computer science discipline or equivalent.
Six years' experience providing services similar in required tasks, scope, and complexity.
Current DoD Top Secret clearance and eligible for SCI access and ACCM read-on.
Show more
Show less","SQL, Python, Java, Kafka, Hive, R, Storm, Data warehousing, Schema management, Data management, Cloudbased database platforms, Complex databases, Data access control, Data allocation, Security controls, Data science, Data planning, Data systems, Data integration","sql, python, java, kafka, hive, r, storm, data warehousing, schema management, data management, cloudbased database platforms, complex databases, data access control, data allocation, security controls, data science, data planning, data systems, data integration","cloudbased database platforms, complex databases, data access control, data allocation, data integration, data management, data planning, data science, data systems, datawarehouse, hive, java, kafka, python, r, schema management, security controls, sql, storm"
Lead Database Engineer (DoD Top Secret),"Chickasaw Nation Industries, Inc.","Tampa, FL",https://www.linkedin.com/jobs/view/lead-database-engineer-dod-top-secret-at-chickasaw-nation-industries-inc-3780404122,2023-12-17,Clearwater,United States,Mid senior,Onsite,"We are #hiring a Lead Database Engineer (Top Secret clearance required and eligible for SCI access) - Tampa, FL ❗
The Lead Database Engineer will provide technical leadership and data expertise to the Data Technical Support team. This engineer will use technology for developing and translating computer algorithms into prototype code and maintaining, organizing, quality management and identifying trends in large datasets. Python, Javascript, r, C/C++
Interested? Apply below!
https://cni.wd1.myworkdayjobs.com/CNI/job/Tampa-FL/Lead-Database-Engineer_R5526
#python #C++ #topsecret #databaseengineer #tampa
Show more
Show less","Python, JavaScript, R, C/C++, Database Engineering, Data Technical Support, Computer Algorithms, Prototype Code, Data Quality Management, Trend Identification, Top Secret Clearance, SCI Access","python, javascript, r, cc, database engineering, data technical support, computer algorithms, prototype code, data quality management, trend identification, top secret clearance, sci access","cc, computer algorithms, data quality management, data technical support, database engineering, javascript, prototype code, python, r, sci access, top secret clearance, trend identification"
Staff Data Engineer,Recruiting from Scratch,"Clearwater, FL",https://www.linkedin.com/jobs/view/staff-data-engineer-at-recruiting-from-scratch-3744393615,2023-12-17,Clearwater,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our client is a dating app.
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, deliver analysis to further improve engagement in existing features, and empower our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Solve technical problems of the highest scope and complexity
Exert significant influence on the company’s analytical long-range goals and data architecture
Define and extend our internal standards for style, maintenance, and best practices for a high-scale data platform
Provide mentorship for all on your team to help them grow in their technical responsibilities
Propose ideas to improve the scale, performance, and capabilities of the Data Platform
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We'll Love About You
7+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field
7+ years experience using Python
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention
Location
: This role will be located in office twice a week minimum in Palo Alto, San Francisco or Chicago.
Salary Range: $180,000 - 250,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, SQL, Kafka, Storm, SparkStreaming, Data Warehouses, ETL, TDD, Pair Programming, Continuous Integration, Automated testing, Deployment","python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, kafka, storm, sparkstreaming, data warehouses, etl, tdd, pair programming, continuous integration, automated testing, deployment","airflow, automated testing, continuous integration, data warehouses, deployment, docker, etl, helm, kafka, kubernetes, pair programming, python, snowflake, spark, sparkstreaming, sql, storm, tdd"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Clearwater, FL",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3744397088,2023-12-17,Clearwater,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
They are a profitable dating application.
Location: This is a hybrid role based in their Chicago office and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location
: This role is twice a week in a hybrid role minimum.
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Data Science, Business Intelligence, Python, SQL, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, Kafka, Storm, SparkStreaming, Data Warehouses, ETL, TDD, Pair Programming, Continuous Integration, Automated Testing, Deployment, Dimensional Data Modeling, Schema Design","data engineering, data science, business intelligence, python, sql, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, kafka, storm, sparkstreaming, data warehouses, etl, tdd, pair programming, continuous integration, automated testing, deployment, dimensional data modeling, schema design","airflow, automated testing, business intelligence, continuous integration, data engineering, data science, data warehouses, deployment, dimensional data modeling, docker, etl, helm, kafka, kubernetes, pair programming, python, schema design, snowflake, spark, sparkstreaming, sql, storm, tdd"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Clearwater, FL",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3748832219,2023-12-17,Clearwater,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our next evolution is underway as a newly public company looking to expand and continue to build meaningful experiences for our users. From social issues to original content, we’re blazing innovative paths with impact for our community, all while leveraging the latest tech stacks and striving for engineering excellence. At the heart of our work in this new chapter is a shared set of core values: openness and exploration, a bias for action, and strong support of the LGBTQ community.
With a track record of strong financial performance and plans for continued headcount growth, we’re looking to build a team of talented, passionate, and open-minded people who believe in our mission, align with our values, and are excited to work at the intersection of innovative technology and social impact. Come be a part of this exciting journey with us.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location: Palo Alto, San Francisco or Chicago
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, TDD, Pair Programming, Continuous Integration, Automated Testing, Kafka, Storm, SparkStreaming, ETL, Data Warehouses, Data classification, Data retention","python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, tdd, pair programming, continuous integration, automated testing, kafka, storm, sparkstreaming, etl, data warehouses, data classification, data retention","airflow, automated testing, continuous integration, data classification, data retention, data warehouses, docker, etl, helm, kafka, kubernetes, pair programming, python, snowflake, spark, sparkstreaming, storm, tdd"
Senior Data Integration Engineer - Remote,Healthesystems,"Tampa, FL",https://www.linkedin.com/jobs/view/senior-data-integration-engineer-remote-at-healthesystems-3781120290,2023-12-17,Clearwater,United States,Mid senior,Remote,"Healthesystems offers workplace flexibility with our Work-From-Home model, and a competitive compensation and benefits package including healthcare coverage, PTO, paid holidays, 401(k), company-provided life insurance/disability coverage, wellness options, and more.
Note: we are unable to hire in every state
Summary:
Responsible for the analysis, design, documentation, development, unit testing, and support of Data Integration and database objects development for software applications. Provides support and guidance regarding Data Integration and T-SQL best practices and development standards. Promotes approved agile methodologies, leading the design and development efforts for the agile team. Actively coaches, guides, and mentors team members in providing valuable solutions to our customer.
Key Responsibilities: “To simplify complexities for each customer.”
Collaborates with stakeholders and development team members to achieve business results.
Work closely with other engineers to integrate databases with other applications.
Leads the design, development, and implementation of database applications and solutions for managing and integrating data between operational systems, data repositories, and reporting and analytical applications. This includes but is not limited to ETL, stored procedures, views, and functions.
Recommends and provide guidance regarding Data Integration and database development, T-SQL best practices, and standards to the development team members as needed.
Create and propose technical design documentation which includes current and future functionality, database objects affected, specifications, and flows/diagrams to detail the proposed database and/or Data Integration implementation.
Has a deep understanding of the business processes and the technology platform that enables it.
Translates stakeholder’s requirements into common language that can be adopted for the use with Behavior Driven Development (BDD) or Test Driven Development (TDD).
Participates in industry and other professional networks to ensure awareness of industry standards, trends and best practices in order to strengthen organizational and technical knowledge.
Provides support for investigating and troubleshooting production issues.
Promotes the establishment of group standards and processes. Participates in the Communities of Practice.
Works continually on improving performance of source code using industry standard methodologies.
Helps drive technology direction and choices of technologies by making recommendations based on experience and research.
Qualifications/Education/Certifications:
Bachelor's degree from four-year college or university (in Information Technology or Computer Science preferred), plus five to eight years related experience and/or training; or equivalent combination of education and experience.
Knowledge, Skills and Abilities:
Prefer experience in Healthcare, PBM and/or ABM, workers’ compensation and/or insurance industry.
Required experience:
5+ years SQL Server 2008/2014
5+ years Data Integration technologies and principles
Advanced knowledge of T-SQL including complex SQL queries (ex: using various joins and sub-queries) and best practices
Advanced knowledge of index design and T-SQL performance tuning techniques
Advanced experience integrating data from structured and unstructured formats: flat files, XML, EDI, JSON, EXCEL
Advanced knowledge and experience in online transactional (OLTP) processing and analytical processing (OLAP) databases and schemas
Advanced knowledge of Data Warehousing methodologies and concepts
Experience with TDD / BDD
The following knowledge is not required, but is preferred:
Experience with BI Tools is a plus
Basic understanding of object oriented programming
Experience in distributed architectures such as Microservices, SOA, and RESTful APIs
Continuous Integration
Cucumber, Gherkin
Jira
Agile Competency Requirements:
Requires an understanding of the application of Agile development methodology.
Must be comfortable with change, close collaboration, and have conflict resolution skills.
Knowledge of or willingness to learn Agile / DevOps values.
Takes initiative and are passionate about what they do.
Adaption, Ability & Desire to Learn, Team Oriented - tolerance & helpful, and Quality Focus
Physical Demands/Working Conditions:
Duties are performed primarily in a home office setting utilizing computer equipment. Travel to attend meetings and visit locations throughout the country may be required. While performing the duties of this job, the employee is regularly required to sit and talk or hear. The employee is frequently required to use hands. The employee is occasionally required to stand and walk.
*** Job descriptions will be reviewed and are subject to changes of business necessity. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.
Pay is based on several factors including but not limited to education, work experience, certifications, geographical cost of labor, etc. In addition to base pay, Healthesystems offers a comprehensive benefits package including, health, dental, vision, disability and life insurance, wellness resources, recognition programs, 401k contribution, and PTO & Holiday pay (all subject to eligibility requirements). Applicable statutory benefits also provided. https://healthesystems.com/careers/
Anticipated Starting Pay Range
$101,000—$139,040 USD
To facilitate working from home, and as a requirement for this role, candidates must provide their own reliable, high speed internet access with sufficient bandwidth to execute all job functions. Company laptop will be provided.
Show more
Show less","Agile, Microservices, Jenkins, Continuous Integration, SQL server, Cucumber, Gherkin, Jira, TSQL, SQL, EDI, JSON, XML, SOAP, RESTful API, OLTP, OLAP, ETL, Stored procedures, Views, Functions, Behavior Driven Development (BDD), Test Driven Development (TDD), Indexing, Flat files","agile, microservices, jenkins, continuous integration, sql server, cucumber, gherkin, jira, tsql, sql, edi, json, xml, soap, restful api, oltp, olap, etl, stored procedures, views, functions, behavior driven development bdd, test driven development tdd, indexing, flat files","agile, behavior driven development bdd, continuous integration, cucumber, edi, etl, flat files, functions, gherkin, indexing, jenkins, jira, json, microservices, olap, oltp, restful api, soap, sql, sql server, stored procedures, test driven development tdd, tsql, views, xml"
Data Scientist,Burtch Works,"Tampa, FL",https://www.linkedin.com/jobs/view/data-scientist-at-burtch-works-3764336716,2023-12-17,Clearwater,United States,Mid senior,Remote,"Our client is looking for a Data Scientist for a 6-month Contract-to-Hire role. This candidate is an exceptional critical thinker that is adept at solving complex problems and creating elegant business solutions. You will be involved in the design and development efforts for our big data solutions including data lake, Business Intelligence Solutions, Machine Learning, Data Pipeline, and cloud-based data warehouse products.
You must have direct experience in the design and implementation in areas such as machine learning, artificial intelligence, operational research, or statistical methods.
Essential Functions
Demonstrated experience in machine learning techniques, probabilistic reasoning, data science, and/or optimization
Proven ability in creating explainable models and implementing advanced algorithms into production
Experience in implementing supervised and unsupervised machine learning techniques, analysis of variance (ANOVA) and statistical significance test
Demonstrated programming experience in Python, R, Keras, TensorFlow with .NET integration
Acts as a key contributor to all phases of the design and development lifecycle of analytic applications utilizing various technology platforms
Experience developing/implementing analytic solutions in the Amazon or Azure cloud that leverage relational, in-memory, NoSQL, document and/or graph databases
Strong analytical skills, able to translate complex business requirements into sound architectural solutions
Experience designing and implementing data pipeline for analytical model consumption of structured, semi-structured, and/or unstructured data in batch and real-time environments
Experience with APIs, Web Services
Good understanding of dimensional data modeling, data transformation & designing analytical data structures
Good SQL skills, broad exposure to all language constructs
Ability to work in a fast-paced, collaborative team environment
Data Integration / ETL / ELT tools
Excellent written and verbal communication skills and ability to express ideas clearly and concisely
Skills & Competencies
Power BI with ML integration
Working in Azure environments including Azure ML
Familiarity with DevOps and CI/CD as well as Agile tools and processes including Git, and Azure DevOps
Education or Prior Work Experience
Minimum of a bachelor's degree in computer science, Applied Mathematics or related technical field
Show more
Show less","Data Science, Machine Learning, Artificial Intelligence, Operational Research, Statistical Methods, Python, R, Keras, TensorFlow, .NET, Amazon Web Services, Azure, NoSQL, Dimensional Data Modeling, Data Transformation, SQL, Data Integration, ETL, ELT, Power BI, DevOps, CI/CD, Git, Azure DevOps, Computer Science, Applied Mathematics","data science, machine learning, artificial intelligence, operational research, statistical methods, python, r, keras, tensorflow, net, amazon web services, azure, nosql, dimensional data modeling, data transformation, sql, data integration, etl, elt, power bi, devops, cicd, git, azure devops, computer science, applied mathematics","amazon web services, applied mathematics, artificial intelligence, azure, azure devops, cicd, computer science, data integration, data science, data transformation, devops, dimensional data modeling, elt, etl, git, keras, machine learning, net, nosql, operational research, powerbi, python, r, sql, statistical methods, tensorflow"
Senior Cloud Data Engineer,BDO USA,"Tampa, FL",https://www.linkedin.com/jobs/view/senior-cloud-data-engineer-at-bdo-usa-3765473103,2023-12-17,Clearwater,United States,Mid senior,Hybrid,"Job Description
Job Summary:
This position will work with cutting edge technology, deliver high quality solutions across various industries, and oversee team(s) on engagements that range in size and scope. This position will receive continuous career development opportunities, given the size and potential of client engagements. This role will perform hands-on delivery of data analytics projects, contributing to the development and unit testing of solutions.
Job Duties
Designs and implements best in class data ingestion strategies, data warehouse and data mart structures, semantic layers and models, visualizations, streaming processes, API integrations, and automation (RPA) solutions for end-to-end data analytics solutions on primarily, but not limited to, cloud analytics platforms such as Azure and AWS
Listens to client needs to align solution with business requirements and delivery schedule
Creates written functional and technical designs
Participates in project status and stand meetings, and assists with providing aggregated project status for project and program managers
Assists with SLA compliance of solutions, and performs performance tuning and optimization efforts of end-to-end solutions
Writes code using multiple languages and correctly applies frameworks, architectural patterns, and software development principles
Delivers high-performance, scalable, repeatable, and secure deliverables with broad impact (high throughput and low latency)
Assists with implementation of data governance programs and best practices
Performs the cleaning and transforming of data from source systems into analytics models
Implements models to support data visualizations and integrations
Assists with implementing DevOps, DataOps and MLOps methodologies on projects
Writes custom integration logic in applicable programming languages
Assists project managers with work breakdown structure creation, project estimation, resource staffing, workload planning and adjustments throughout the project lifecycle
Assists clients with licensing, security, and cost estimation of solutions
Performs code reviews to ensure adherence to standards
Works directly with clients and team members to establish secure data analytics platforms and infrastructure
Contributes to successful deployments of developed solutions and integration of DevOps tools
Maintains a broad and current understanding of data analytics and business intelligence strategies, cloud platforms, methodologies, and tools
Builds client relationships during project execution, effectively becoming a trusted advisor of the client
Participates in support activities for existing software solutions
Other duties as assigned
Supervisory Responsibilities
Supervises the day-to-day workload of Associates on assigned engagements to ensure that timelines and deliverables are met, and reviews work product
Education
Qualifications, Knowledge, Skills and Abilities:
High School Diploma or GED equivalent, required
Bachelor’s degree, preferred; focus in Information Systems, Data Science or Computer Science, preferred
Experience
Five (5) or more years of experience within Data Analytics, Business Intelligence, Artificial Intelligence, or Application Development, required
One (1) or more years of experience technically leading development projects, preferred
One (1) or more years of consulting experience or implementation of cloud-based data analytics solutions, preferred
Software
Strong SQL skills including Data Definition Language (DDL), Data Manipulation Language (DML), views, functions, stored procedures, or performance tuning, required
Experience with Data Warehousing, Data Modeling, Semantic Model Definition or Star Schema Construction, required
Hands on delivery experience of end-to-end cloud data analytics solutions within Azure or AWS, preferred
Experience with one (1) or more of the following computer languages, preferred:
C#
Python
Java
Scala
Experience with tabular modeling within Microsoft Fabric, Power BI, or Azure Analysis Services, preferred
Experience with Git and DevOps deployment technologies, preferred
Experience with Linux, preferred
Experience with one (1) or more of the following, preferred:
Data Lake Medallion Architecture
Batch and/or streaming data ingestion into a data lake
AI Algorithms/Machine Learning
Automation tools such as UiPath, Alteryx, etc.
Computer Vision based AI technologies
Other Knowledge, Skills & Abilities
Ability to work with a high degree of professionalism and autonomy
Excellent verbal and written communication skills
Solid organizational skills, especially the ability to meet project deadlines with a focus on details
Ability to successfully multi-task while working independently or within a group environment
Ability to work in a deadline-driven environment, and handle multiple projects simultaneously
Ability to interact effectively with people at all organizational levels of the Firm
Ability to effectively interact with a team of professionals and delegating work assignments, as needed
Ability to build and maintain strong relationships with internal and client personnel
Ability to encourage a team environment on engagements, and contribute to the professional development of assigned personnel
Keywords:
Data Analytics, Business Intelligence, BI, Synapse, IoT, Machine Learning, Data Lake, Stream, Cube, Microsoft, SQL Server, Tableau, .Net, C#, Qlik, Power BI, Machine Learning, Azure Data Factory, RedShift, UiPath, Cloud, RPA, AWS, Redshift, Kinesis, QuickSight, SageMaker, S3, Databricks, AWS Lake Formation, Snowflake, Python, Qlik, Athena, Data Pipeline, Glue, Star Schema, Data Modeling, SQL, SSIS, SSAS, SSRS, PySpark, Microsoft Fabric, dbt, Linux, Terraform, Bicep, Data Ops, Purview, Git, Delta, Pandas, Spark SQL
Individual salaries that are offered to a candidate are determined after consideration of numerous factors including but not limited to the candidate’s qualifications, experience, skills, and geography.
California Range: $111,000 - $152,000
Colorado Range: $111,000 - $152,000
New York City/ Valhalla Range: $111,000 - $152,000
Washington Range: $111,000 - $152,000
About Us
BDO delivers assurance, tax, digital technology solutions and financial advisory services to clients throughout the country and around the globe. We offer numerous industry-specific practices, world-class resources, and an unparalleled commitment to meeting our clients’ needs. We currently serve more than 400 publicly traded domestic and international clients.
Unparalleled partner-involvement
Deep industry knowledge and participation
Geographic coverage across the U.S.
Cohesive global network
Focused capabilities across disciplines
BDO brings world-class resources and exceptional service to each and every one of our clients. BDO USA is a member of BDO International, the world’s fifth largest accounting network.
BDO offers a competitive Total Rewards package that encompass so much more than – “traditional benefits”. Our wide range of rewards and our employees’ ability to customize rewards to their individual needs are two of the reasons why BDO has been honored with so many workplace awards, including 100 Best Companies for Working Parents, Working Mother 100 Best Companies, Top Entry Level Employer, 2022 National Best & Brightest Companies to Work For and more.
Some Examples Of Our Total Rewards Offerings Include
Competitive pay and eligibility for an annual performance bonus.
A 401k plan plus an employer match
Comprehensive, medical, dental, vision, FSA, and prescription insurance from day one
Competitive Paid Time Off with daily accrual from day one of employment, plus paid holidays
Paid Parental Leave
Adoption Assistance
Firm paid life insurance
Wellness programs
Additional offerings include BDO Flex, Group Legal insurance, Pet insurance and Long-Term Care Insurance
Above offerings may be subject to eligibility requirements.
Click here to find out more!
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability or protected veteran status.
""BDO USA, P.A. is an EO employer M/F/Veteran/Disability""
Show more
Show less","Data Analytics, Business Intelligence, Machine Learning, Data Warehousing, Data Modeling, SQL, Python, Java, Scala, C#, Microsoft Fabric, Power BI, Azure Analysis Services, Git, Linux, Data Lake Medallion Architecture, Batch and/or streaming data ingestion, AI Algorithms, Automation tools, Computer Vision, Tableau, .Net, Qlik, Azure Data Factory, RedShift, UiPath, Cloud, RPA, AWS, Kinesis, QuickSight, SageMaker, S3, Databricks, AWS Lake Formation, Snowflake, PySpark, dbt, Terraform, Bicep, Data Ops, Purview, Delta, Pandas, Spark SQL, SSIS, SSAS, SSRS","data analytics, business intelligence, machine learning, data warehousing, data modeling, sql, python, java, scala, c, microsoft fabric, power bi, azure analysis services, git, linux, data lake medallion architecture, batch andor streaming data ingestion, ai algorithms, automation tools, computer vision, tableau, net, qlik, azure data factory, redshift, uipath, cloud, rpa, aws, kinesis, quicksight, sagemaker, s3, databricks, aws lake formation, snowflake, pyspark, dbt, terraform, bicep, data ops, purview, delta, pandas, spark sql, ssis, ssas, ssrs","ai algorithms, automation tools, aws, aws lake formation, azure analysis services, azure data factory, batch andor streaming data ingestion, bicep, business intelligence, c, cloud, computer vision, data lake medallion architecture, data ops, dataanalytics, databricks, datamodeling, datawarehouse, dbt, delta, git, java, kinesis, linux, machine learning, microsoft fabric, net, pandas, powerbi, purview, python, qlik, quicksight, redshift, rpa, s3, sagemaker, scala, snowflake, spark, spark sql, sql, ssas, ssis, ssrs, tableau, terraform, uipath"
Lead Cloud Database Engineer,INSPYR Solutions,"Tampa, FL",https://www.linkedin.com/jobs/view/lead-cloud-database-engineer-at-inspyr-solutions-3779675925,2023-12-17,Clearwater,United States,Mid senior,Hybrid,"Title:
Lead Cloud Database Engineer
Location:
Remote
Duration:
Direct Hire
Compensation:
125-130k
Work Requirements:
US Citizen, GC Holders or Authorized to Work in the U.S.
Job Description
Purpose of the Position:
The Lead Cloud Database Engineer leads in the systems implementation and management of cloud and on-prem database systems in support of day-to-day 24x7 IT operations. The Lead Cloud Database Engineer is responsible for hands-on delivery and leadership of a small database team that supports, maintains, administers our enterprise data platform solutions. This position operates as the leader of Data Platform engineering initiative as we migrate to a could architecture.
The Lead Cloud Database Engineer reports to the Director, Enterprise Data Engineering.
Pay & Benefits:
Anticipated starting salary of $125,000; pay rate negotiable based on experience and qualifications.
Medical (including prescription), Dental, Vision
FSA/HSA (Depending on Medical Plan chosen)
$50,000 Life Insurance
Additional Voluntary Life Insurance
Employee Assistance Program EAP
Long Term Disability
Short Term Disability
Supplemental Insurance such as Critical Illness, Accident, and Hospital
Paid Time Off 15 days accrued in year 1, 9 holidays, and 1 day of Volunteering Time Off
401k (eligible upon completion of 90 days of employment and must be at least 21 years of age)
Pet Insurance
Identity Theft Protection
Key Responsibilities:
Functional
Own the outcome. Execute as one, together with IT peers.
Lead projects and team to create and maintain optimal database architecture.
Guide development to assemble large, complex databases that meet enterprise requirements for both OLTP and analytical workloads.
Steer expansion and optimization of database architecture for cross functional teams.
Provide operational and functional leadership for creating, storing, managing, and maintaining enterprise data, including the ability to incorporate policies and procedures for centrally managing and sharing data through the data life cycle.
Partner with application and product teams to define and deliver on database needs.
Participate in short-and intermediate-term planning for data initiatives and projects including identifying dependencies, capacity planning and resource allocation.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, and re-designing for flexibility and scalability.
Design and implement proactive database monitoring and guide rapid issue resolution of issues. Ensure databases are performing to SLAs,
Define infrastructure required for a variety of cloud and on-premises databases.
Work with stakeholders including to assist with database technical issues and support their data infrastructure needs.
Work closely with Infrastructure, DevOps, system engineering, data architecture, data governance and data analytics teams to ensure databases adhere to enterprise standards, usability, and performance.
Guide and mentor team members, peers, and business partners on functional requirements, best practices, effective design patterns and data maturity.
Be responsible for implementation of database safeguards and security adherent to enterprise standards.
Assist with implementation of DataOps and DevSecOps for data initiatives using preferred processes and tools. Participate as team member / leader in Agile/Scrum team.
Work on continuous improvement initiatives and effectively address root cause analysis outcomes to deliver sustainable excellence.
Mentor, coach and support direct reports to excel at team deliverables.
Ensure data engineering design and deliverables adhere to regulatory and compliance requirements.
Participate in On-Call rotation for after-hours support and troubleshooting
Perform other duties as assigned.
Technical:
Ensure overall performance, reliability, stability, and availability of Cloud and on-prem database systems and infrastructure.
Lead the configuration, management, and implantation of cloud and on-prem data systems.
Perform proactive maintenance, monitoring, and troubleshooting of SQL Server, Azure SQL, Databrick, Cosmos and all other database systems.
Troubleshoot database service outages as they occur, including after-hours and weekends.
Create documentation on database recovery and architecture effective provisioning, installation/configuration, operation, and maintenance of systems hardware and software and related infrastructure.
Work to ensure that the associated hardware resources allocated to the databases are configured for high resilience and are tuned for optimum performance.
When performance issues arise, determine the most effective way to increase performance server configuration changes, or index/query changes.
Identify and partner with developers and leaders to remediate inefficient or suboptimal performance of queries, processes, or data structures.
Guide complex and critical data/database management initiatives that typically involve multiple disciplines and multiple business groups, including responsibility for database architecture, design, integration, and/or data modeling.
Responsible for improvement and maintenance of the databases to include rollout and upgrades.
Work with IT Engineering and DevOps to ensure all database systems are backed up in a way that meets the business's Recovery Point Objectives (RPO).
Test backups to ensure we can meet the business' Recovery Time Objectives (RTO).
Lead architectural decisions for cloud and on-prem data system implementations and upgrades including clustering, log shipping, mirroring, SQL Database servers, Azure SQL Databases, Datalakes, Data Factory, Databricks, or other technologies.
Lead the configuration, tuning, and management SQL Always-On Availability Groups.
Deploy database change scripts provided by third party vendors.
Perform load and execution testing.
Develop and deploy methodologies for testing database performance and providing performance statistics and reports.
Responsible for assisting developers in creating stored procedures and SQL commands to properly query databases.
Continued development of database administration skills; keep abreast of latest techniques, best practices, and trends and able to apply to project work.
Lead developing SQL queries/scripts and similar artifacts to validate the completeness, integrity, and accuracy of data within data load/refresh, backup and recovery processes.
Lead in data analysis efforts to resolve complex database performance issues.
Lead with the design and development of automated solutions (scripts, functions, programs, processes) to increase the efficiency of the database administration and monitoring processes.
Document designs and specifications that adhere to company practices.
Communicate needs as well as deployment and operations standards to Infrastructure Services.
Support Institutional Initiatives
Keep current on institutional goals, objectives, and progress.
Engage in institutional sponsored activities and initiatives as appropriate.
Lead with courage in support of change initiatives that impact organization.
Manages and assists in coordinating efforts between departments.
Compliance:
Demonstrates knowledge of, and carefully follows all applicable federal and state compliance requirements and regulations including those prescribed by the Department of Education, accrediting agencies, CIE, and internal policies and procedures.
Effectively communicates compliance requirements to students and other staff as appropriate and quickly escalates any compliance concerns to the Compliance department.
Work Experience, Skills & Abilities:
Minimum Requirements
Bachelor's degree or equivalent combination of education and experience.
2+ years of leadership experience.
9+ years of experience with database design, and administration experience with Microsoft SQL Server.
9+ years of experience writing T-SQL and stored procedures and query tuning on high transaction systems.
5+ years of experience with Microsoft SQL Server in a high availability environment including Failover Clusters, Availability Groups and Transactional Replication.
5+ years of experience with Microsoft Azure cloud database services such as Azure SQL Databases, SQL Server Stretch Databases.
5+ years in CDC and Data Replication techniques, processes, and best practices and able to provide guidance to other development teams (BI, ETL, Software Engineering, etc.).
Deep knowledge of both OLTP and Analytical databases.
Possess in-depth SQL knowledge to understand and review SQL and host language programs and to recommend changes for optimization.
Ability to prioritize in the face of demanding time frames.
Proficient at collecting, storing, and managing the ability to query the organization's metadata.
Able to integrate database administration requirements and tasks with general systems management requirements and tasks (such as job scheduling, network management, transaction processing, etc.).
Able to professionally communicate fluently in verbal and written English and effectively communicate with all level of the organization.
Able to work in a team focused environment.
Excellent customer service mindset and approach.
Self-motivated; strong work ethic.
Proficient in MS Office (Word, Excel, PowerPoint) and other business tools such as Skype and Microsoft Teams.
Able to support a diverse and inclusive work environment.
Preferred Requirements:
5+ years of experience with Microsoft Azure cloud data services such as ADLS, DataBricks, Power BI, and Data Factory.
Experience with COSMOS DB
Proficient experience with Database monitoring tool sets such as SolarWinds DPA.
Proficient experience with Database Modeling and Design Tools (e.g., Erwin, Oracle Designer, ER Studio, etc.).
Ability to translate a data model or logical database design into an actual physical database implementation and to manage that database once it has been implemented.
Proficient experience with SQL Source control tools such as RedGate or VSTS.
Possess procedural skills to help design, debug, implement, and maintain stored procedures, triggers, and user-defined functions that are stored in the DBMS.
Possess understanding of the advanced tenets of relational database technology and be able to accurately communicate them to others.
Working Environment:
Ability to work full-remote, on-site, or a hybrid attendance model.
Minimal travel required-Valid drivers license required.
Flexibility to work evenings and weekends as needed.
Physical Demands:
Requires long periods of sitting at a desk working on a computer.
Requires ability to travel.
Requires occasional bending, stooping and squatting.
Requires occasional lifting of up to 10 lbs.
About INSPYR Solutions
Technology is our focus and quality is our commitment. As a national expert in delivering flexible technology and talent solutions, we strategically align industry and technical expertise with our clients’ business objectives and cultural needs. Our solutions are tailored to each client and include a wide variety of professional services, project, and talent solutions. By always striving for excellence and focusing on the human aspect of our business, we work seamlessly with our talent and clients to match the right solutions to the right opportunities. Learn more about us at inspyrsolutions.com.
INSPYR Solutions provides Equal Employment Opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability, or genetics. In addition to federal law requirements, INSPYR Solutions complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities.
Show more
Show less","Cloud Computing, Database Administration, SQL Server, Azure SQL, Databricks, Cosmos DB, TSQL, Stored Procedures, Query Tuning, High Availability, Failover Clusters, Availability Groups, Transactional Replication, CDC, Data Replication, OLTP, Analytical Databases, SQL, Optimization, Metadata Management, MS Office, Skype, Microsoft Teams, ADLS, Power BI, Data Factory, SolarWinds DPA, Database Modeling, Design Tools, RedGate, VSTS, DBMS, Relational Database Technology, Agile/Scrum, DataOps, DevSecOps, Data Governance, Data Architecture, Data Analytics, Regulatory Compliance","cloud computing, database administration, sql server, azure sql, databricks, cosmos db, tsql, stored procedures, query tuning, high availability, failover clusters, availability groups, transactional replication, cdc, data replication, oltp, analytical databases, sql, optimization, metadata management, ms office, skype, microsoft teams, adls, power bi, data factory, solarwinds dpa, database modeling, design tools, redgate, vsts, dbms, relational database technology, agilescrum, dataops, devsecops, data governance, data architecture, data analytics, regulatory compliance","adls, agilescrum, analytical databases, availability groups, azure sql, cdc, cloud computing, cosmos db, data architecture, data factory, data governance, data replication, dataanalytics, database administration, database modeling, databricks, dataops, dbms, design tools, devsecops, failover clusters, high availability, metadata management, microsoft teams, ms office, oltp, optimization, powerbi, query tuning, redgate, regulatory compliance, relational database technology, skype, solarwinds dpa, sql, sql server, stored procedures, transactional replication, tsql, vsts"
Technical Data Analyst - SQL,Motion Recruitment,"Tampa, FL",https://www.linkedin.com/jobs/view/technical-data-analyst-sql-at-motion-recruitment-3771187638,2023-12-17,Clearwater,United States,Mid senior,Hybrid,"Grow your career as a Technical Data Analyst with an innovative global bank working in Tampa, FL. Contract role with strong possibility of extension. Will require working a hybrid schedule 2-3 days onsite per week.
Join one of the world's most renowned global banks and trusted brand with over 200 years of continuously evolving financial services worldwide. Will be responsible for establishing and implementing new or revised application systems and programs in coordination with the Technology team. The overall objective of this role is to lead applications with system analysis, data analysis and data mapping, produce clear system flowcharts, BRDs & FRDs to implement the project. You will work alongside some of the smartest minds in the industry who are excited to share their knowledge and to learn from you.
Contract Duration: 12+ Months
Required Skills & Experience
Bachelor’s degree/University degree.
3 to 5 years of experience in Applications Development or systems analysis role.
4+ years of experience with SQL Server.
1 to 3 years of understanding of .NET perform on Windows.
Extensive experience of system analysis and in programming of software applications.
Experience in managing and implementing successful projects.
Excellent understanding of RDBMS & SQL Queries.
Must be very good in SQL writing, understanding – looking at mapping programs, read those old programs and how data is mapped.
Financial background with Market Data experience, preferably a Subject Matter Expert (SME) in this domain. Understand equity, QC, equity, etc. (entire system based on market data).
Ability to prepare BRD & FRD.
Ability to adjust priorities quickly as circumstances dictate to work in an Agile framework environment.
Demonstrated leadership and project management skills.
Desired Skills & Experience
Master’s degree.
Financial background in pricing & product.
What You Will Be Doing
Partner with multiple management teams to ensure appropriate integration of functions to meet goals as well as identify and define necessary system enhancements to deploy new products and process improvements.
Resolve variety of high impact problems/projects through in-depth evaluation of complex business processes, system processes, and industry standards.
Provide expertise in area and advanced knowledge of applications programming and ensure application design adheres to the overall architecture blueprint.
Utilize advanced knowledge of system flow and develop standards for coding, testing, debugging, and implementation.
Develop comprehensive knowledge of how areas of business, such as architecture and infrastructure, integrate to accomplish business goals.
Provide in-depth analysis with interpretive thinking to define issues and develop innovative solutions.
Serve as advisor or coach to mid-level developers and analysts, allocating work as necessary.
Posted By:
Melissa Klein
Show more
Show less","Data Analysis, Applications Development, Systems Analysis, SQL Server, .NET, RDBMS, SQL Queries, Market Data, Financial Background, Agile Framework, Leadership, Project Management, Business Process Analysis, Application Programming, System Architecture, Coding, Testing, Debugging, Business Analysis, Analytical Thinking","data analysis, applications development, systems analysis, sql server, net, rdbms, sql queries, market data, financial background, agile framework, leadership, project management, business process analysis, application programming, system architecture, coding, testing, debugging, business analysis, analytical thinking","agile framework, analytical thinking, application programming, applications development, business analysis, business process analysis, coding, dataanalytics, debugging, financial background, leadership, market data, net, project management, rdbms, sql queries, sql server, system architecture, systems analysis, testing"
Software Development Engineer in Test (Data),BST Global,"Tampa, FL",https://www.linkedin.com/jobs/view/software-development-engineer-in-test-data-at-bst-global-3787374186,2023-12-17,Clearwater,United States,Mid senior,Hybrid,"Summary of Duties & Responsibilities:
BST Global is seeking a highly driven and effective professional to join the quality assurance team. As an SDET with a focus on data, you will leverage your expertise, experience and passion to ensure we uphold the highest levels of quality and excellence for our clients.
Essential Functions:
Technical
– E
xperience in integrating a strong software development background with a passion for the discipline of quality assurance
Build testing framework design, development, automation for our Big data infrastructure leveraging the latest technologies from Microsoft Azure in a multi-tenant architecture with multiple data sources spanning both on-premise and in the cloud
Develop Azure API and Web-based API testing and automation of data pipelines, data services, cloud data warehouses, Azure Databricks data lakehouse, business intelligence, and machine learning platforms, especially around unified transactional data
Creating quality metrics to evaluate data pipelines, products, and customer deliverables
Work closely with the Scrum team throughout a project to continually monitor and provide feedback on the quality of the product
Produce test estimates for testing activities working in an Agile development process
Produce status reports and quality metrics working in an Agile development process
Experience working with Azure DevOps CI/CD environment
Methodology
– E
xpertise and knowledge in quality assurance standards, processes, policies and procedures
Knowledge of statistical methods, models, and processes to develop automated testing solutions in order to validate the predicted outcome
Ability to work within an iterative software development lifecycle, under Agile development processes
Proficiency with common software engineering best practices, such as pairing, test-driven development (TDD), writing unit and integration tests, and participating in code reviews
Demonstrated experience defining, communicating and presenting data-driven test plans and test scenarios to stakeholders and product owners.
Assist the QA team with feature and regression testing
Team Support
–
Deep commitment to working in and fostering a highly collaborative, innovative and high performing product team
Be an active participant in all aspects of test planning and execution through all phases of the product development lifecycle, including testing strategies, and communication
Evangelize, support and embody BST’s Company Mission, Strategy and Values
Foster a culture of ownership and pride for delivering the highest levels of quality and excellence
Performs other related duties as directed
Skills & Competencies
Abilities
Strong team player with ability to collaborate with all levels of the organization
Ability to influence others and motivate with a positive and confident personal style
Possess a drive towards forward progress and delivering results while taking responsibility
Multi-tasker with ability to set and manage priorities
Strong analytical and problem-solving skills with a high aptitude to learn
Proactively and transparently communicate challenges/successes to leadership
Exhibit and relentlessly demonstrate a high level of attention to detail
Flexibility, adaptability and willingness to help in other areas as priorities shift
Ability to effectively operate with minimal supervision, but keeping leadership in the loop
Skills/Competencies
4-6 years hands-on programming experience with C#, ASP.Net, React and Python
4-6 years advanced hands-on experience with PowerShell, Azure CLI, ARM templates, YAML, JSON, Parquet
4-6 years advanced hands-on experience with automated testing of Power BI embedded, Power BI services, Power BI dashboards and reports
4-6 years advanced hands-on experience writing SQL Queries and Stored Procedures, Spark SQL
4-6 years of hands-on working experience of testing REST, Web APIs or SOAP APIs (ASP.Net, Karate, MSTest, NUnit, Postman, and SoapUI)
4-6 years UI Test automation experience with (Selenium, Appium, and Cypress)
4-6 years experience testing cloud-based multi-tenant Data Pipelines
Knowledge of Data Warehousing and data modeling
Knowledge of Azure APIs for Azure Services such as Azure AD API, Azure Data Lake , Log Analytics, Power BI Services API,
Education or Prior Work Experience
Bachelor’s or Master’s degree in Computer Science or a related field such as Mathematics and Statistics, preferably with a focus on data analytics
Azure Certified Cloud Architect – (Desired)
Reports to
QA Manager
Number Supervised
0
Travel
Minimal to no travel required, but travel as needed
Classification
Exempt
Work Environment
This job operates in a professional office environment or remotely. This role routinely uses standard office equipment such as computers, phones, photocopiers, filing cabinets and fax machines.
Physical Demands
While performing the duties of this job, the employee is regularly required to talk or hear. The employee frequently is required to stand; walk; use hands to finger, handle or feel; and reach with hands and arms.
Show more
Show less","Software Development, Data Assurance, Quality Assurance, Data Pipelines, Azure, Web API, Machine Learning, Power BI, SQL, REST, Selenium, Appium, Cypress, Python, C#, ASP.Net, React, PowerShell, Azure CLI, ARM templates, YAML, JSON, Parquet, Karate, MSTest, NUnit, Postman, SoapUI, Data Warehousing, Data Modeling, Azure APIs, Azure AD API, Azure Data Lake, Log Analytics, Power BI Services API","software development, data assurance, quality assurance, data pipelines, azure, web api, machine learning, power bi, sql, rest, selenium, appium, cypress, python, c, aspnet, react, powershell, azure cli, arm templates, yaml, json, parquet, karate, mstest, nunit, postman, soapui, data warehousing, data modeling, azure apis, azure ad api, azure data lake, log analytics, power bi services api","appium, arm templates, aspnet, azure, azure ad api, azure apis, azure cli, azure data lake, c, cypress, data assurance, datamodeling, datapipeline, datawarehouse, json, karate, log analytics, machine learning, mstest, nunit, parquet, postman, power bi services api, powerbi, powershell, python, quality assurance, react, rest, selenium, soapui, software development, sql, web api, yaml"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Portland, OR",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3748833026,2023-12-17,Vancouver,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our next evolution is underway as a newly public company looking to expand and continue to build meaningful experiences for our users. From social issues to original content, we’re blazing innovative paths with impact for our community, all while leveraging the latest tech stacks and striving for engineering excellence. At the heart of our work in this new chapter is a shared set of core values: openness and exploration, a bias for action, and strong support of the LGBTQ community.
With a track record of strong financial performance and plans for continued headcount growth, we’re looking to build a team of talented, passionate, and open-minded people who believe in our mission, align with our values, and are excited to work at the intersection of innovative technology and social impact. Come be a part of this exciting journey with us.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location: Palo Alto, San Francisco or Chicago
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Python, SQL, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, Kafka, Storm, SparkStreaming, Agile engineering practices, TDD, Pair Programming, Continuous Integration, Automated testing, Deployment, Data Warehouses, ETL, Legal compliance, Data management tools, Data classification, Data retention","python, sql, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, kafka, storm, sparkstreaming, agile engineering practices, tdd, pair programming, continuous integration, automated testing, deployment, data warehouses, etl, legal compliance, data management tools, data classification, data retention","agile engineering practices, airflow, automated testing, continuous integration, data classification, data management tools, data retention, data warehouses, deployment, docker, etl, helm, kafka, kubernetes, legal compliance, pair programming, python, snowflake, spark, sparkstreaming, sql, storm, tdd"
Data Center Facilities Operator,JLL,"Aurora, IL",https://www.linkedin.com/jobs/view/data-center-facilities-operator-at-jll-3743108115,2023-12-17,Sheridan,United States,Mid senior,Onsite,"The Data Center Facilities Engineer is a mechanical hands on role and is non-IT that is responsible for delivery of best practice systems and problem resolution on all data center electrical and mechanical infrastructure (UPS, MV electrical systems, generators, cooling systems etc.)
Location:
Principal Duties and Responsibilities
Task will include but not be limited to:
Responsible for maintaining, monitoring, and performing preventive maintenance and continuous operation of all building systems to maintain 100% Up-time including: fire/life safety, mechanical systems such as (HVAC, chillers, crac, crah, plumbing, controls), electrical including emergency backup systems such as (lighting, UPS, ATS, STS, PDU, generators, primary switchgear, power distribution, transformers), and hot water systems. Monitors operation, adjusts, and maintains refrigeration, chilled water, and air conditioning equipment; boilers, and ventilating and water heaters; pumps, valves, piping, and filters; other mechanical and electrical equipment. Must record readings and make and adjust where necessary to ensure proper operation of equipment.
Requires the ability to analyze the operation of various systems, determine the cause of any problems/malfunctions and take corrective action as required.
Comply with departmental policy for the safe storage, usage, and disposal of hazardous materials. Maintains a clean and safe workplace.
Learn and understand the data center site in-order to manage incidents and events that put the critical systems at risk.
Work order management, including CMMS, Vendor Management, and Customer Facing Tickets.
Understanding and complying with emergency escalation procedures.
Perform additional job duties as required.
Minimum Requirements:
Preferred to have hands-on experience working in a data center/critical facility, including UPS.
Systems, emergency generators, and switchgears.
High School diploma or GED equivalent
2+ years related work experience.
Working knowledge of computer applications including Word and Excel.
Demonstrated verbal/written communication skills.
Preferred Requirements:
Corrigo Experience.
MCIM / Salesforce Experience.
Zendesk Experience.
Service Now Experience.
Received EPA 608.
Trained in NFPA70E.
PHYSICAL WORK ABILITIES AND REQUIREMENTS
:
This position requires frequent walking, climbing, bending, kneeling, lifting, stooping, and working/extending overhead, including:
Walking large, campus-like settings.
Lifting a minimum of 50 lbs.
Climbing stairs and navigating rooftops to access equipment.
Using ladders up to 30 ft and working from heights.
Ability to Climb a ladder with a 300-lb weight limit.
Must be able to work different schedules.
Must be able to work Holidays.
Must be able to respond to site emergencies.
Personalized benefits that support personal well-being and growth:
JLL recognizes the impact that the workplace can have on your wellness, so we offer a supportive culture and comprehensive benefits package that prioritizes mental, physical and emotional health. Some of these benefits, include:
401(k) plan with matching company contributions
Comprehensive Medical, Dental & Vision Care
FMLA at 100% of salary after 1 year of employment.
Paid Time Off and Company Holidays.
Compensated for Holidays Worked.
15% Pay differential for Night Shift Employment.
If this job description resonates with you, we encourage you to apply, even if you don’t meet all the requirements. We’re interested in getting to know you and what you bring to the table!
Pay is the union rate
Show more
Show less","Data Center Operations, Facilities Management, Electrical Systems, HVAC Systems, Plumbing, Controls, Generators, UPS Systems, PDUs, Transformers, Refrigeration, Chillers, Air Conditioning, Boilers, Ventilating, Water Heaters, Pumps, Valves, Piping, Filters, CMMS, Vendor Management, Customer Service, Corrigo, MCIM, Salesforce, Zendesk, Service Now, EPA 608, NFPA70E","data center operations, facilities management, electrical systems, hvac systems, plumbing, controls, generators, ups systems, pdus, transformers, refrigeration, chillers, air conditioning, boilers, ventilating, water heaters, pumps, valves, piping, filters, cmms, vendor management, customer service, corrigo, mcim, salesforce, zendesk, service now, epa 608, nfpa70e","air conditioning, boilers, chillers, cmms, controls, corrigo, customer service, data center operations, electrical systems, epa 608, facilities management, filters, generators, hvac systems, mcim, nfpa70e, pdus, piping, plumbing, pumps, refrigeration, salesforce, service now, transformers, ups systems, valves, vendor management, ventilating, water heaters, zendesk"
Data Center Facilities Operator,JLL,"Aurora, IL",https://www.linkedin.com/jobs/view/data-center-facilities-operator-at-jll-3743102754,2023-12-17,Sheridan,United States,Mid senior,Onsite,"The Data Center Facilities Engineer is a mechanical hands on role and is non-IT that is responsible for delivery of best practice systems and problem resolution on all data center electrical and mechanical infrastructure (UPS, MV electrical systems, generators, cooling systems etc.)
Location:
Principal Duties and Responsibilities
Task will include but not be limited to:
Responsible for maintaining, monitoring, and performing preventive maintenance and continuous operation of all building systems to maintain 100% Up-time including: fire/life safety, mechanical systems such as (HVAC, chillers, crac, crah, plumbing, controls), electrical including emergency backup systems such as (lighting, UPS, ATS, STS, PDU, generators, primary switchgear, power distribution, transformers), and hot water systems. Monitors operation, adjusts, and maintains refrigeration, chilled water, and air conditioning equipment; boilers, and ventilating and water heaters; pumps, valves, piping, and filters; other mechanical and electrical equipment. Must record readings and make and adjust where necessary to ensure proper operation of equipment.
Requires the ability to analyze the operation of various systems, determine the cause of any problems/malfunctions and take corrective action as required.
Comply with departmental policy for the safe storage, usage, and disposal of hazardous materials. Maintains a clean and safe workplace.
Learn and understand the data center site in-order to manage incidents and events that put the critical systems at risk.
Work order management, including CMMS, Vendor Management, and Customer Facing Tickets.
Understanding and complying with emergency escalation procedures.
Perform additional job duties as required.
Minimum Requirements:
Preferred to have hands-on experience working in a data center/critical facility, including UPS.
Systems, emergency generators, and switchgears.
High School diploma or GED equivalent
2+ years related work experience.
Working knowledge of computer applications including Word and Excel.
Demonstrated verbal/written communication skills.
Preferred Requirements:
Corrigo Experience.
MCIM / Salesforce Experience.
Zendesk Experience.
Service Now Experience.
Received EPA 608.
Trained in NFPA70E.
PHYSICAL WORK ABILITIES AND REQUIREMENTS
:
This position requires frequent walking, climbing, bending, kneeling, lifting, stooping, and working/extending overhead, including:
Walking large, campus-like settings.
Lifting a minimum of 50 lbs.
Climbing stairs and navigating rooftops to access equipment.
Using ladders up to 30 ft and working from heights.
Ability to Climb a ladder with a 300-lb weight limit.
Must be able to work different schedules.
Must be able to work Holidays.
Must be able to respond to site emergencies.
Personalized benefits that support personal well-being and growth:
JLL recognizes the impact that the workplace can have on your wellness, so we offer a supportive culture and comprehensive benefits package that prioritizes mental, physical and emotional health. Some of these benefits, include:
401(k) plan with matching company contributions
Comprehensive Medical, Dental & Vision Care
FMLA at 100% of salary after 1 year of employment.
Paid Time Off and Company Holidays.
Compensated for Holidays Worked.
15% Pay differential for Night Shift Employment.
If this job description resonates with you, we encourage you to apply, even if you don’t meet all the requirements. We’re interested in getting to know you and what you bring to the table!
Pay is the union rate
Show more
Show less","Data Center Facilities Engineer, UPS, MV electrical systems, Generators, Cooling systems, HVAC, Chillers, Crac, Crah, Plumbing, Controls, Electrical, Lighting, ATS, STS, PDU, Primary switchgear, Power distribution, Transformers, Hot water systems, Refrigeration, Chilled water, Air conditioning equipment, Boilers, Ventilating, Water heaters, Pumps, Valves, Piping, Filters, CMMS, Vendor Management, Customer Facing Tickets, Corrigo, MCIM, Salesforce, Zendesk, Service Now, EPA 608, NFPA70E, Microsoft Word, Microsoft Excel","data center facilities engineer, ups, mv electrical systems, generators, cooling systems, hvac, chillers, crac, crah, plumbing, controls, electrical, lighting, ats, sts, pdu, primary switchgear, power distribution, transformers, hot water systems, refrigeration, chilled water, air conditioning equipment, boilers, ventilating, water heaters, pumps, valves, piping, filters, cmms, vendor management, customer facing tickets, corrigo, mcim, salesforce, zendesk, service now, epa 608, nfpa70e, microsoft word, microsoft excel","air conditioning equipment, ats, boilers, chilled water, chillers, cmms, controls, cooling systems, corrigo, crac, crah, customer facing tickets, data center facilities engineer, electrical, epa 608, filters, generators, hot water systems, hvac, lighting, mcim, microsoft excel, microsoft word, mv electrical systems, nfpa70e, pdu, piping, plumbing, power distribution, primary switchgear, pumps, refrigeration, salesforce, service now, sts, transformers, ups, valves, vendor management, ventilating, water heaters, zendesk"
Data Center Operations Manager,CyrusOne,"Aurora, IL",https://www.linkedin.com/jobs/view/data-center-operations-manager-at-cyrusone-3784887751,2023-12-17,Sheridan,United States,Mid senior,Onsite,"The Data Center Operations Manager is responsible for ensuring optimal performance of data center and facility environments, systems and personnel, as well as, the daily over-site and guidance for administrative and maintenance functions.
Essential Functions:
Environmental Management
Manage day to day operations of cooling (production & distribution) and electrical systems
Troubleshoot issues related to the critical infrastructure using test equipment, observation and automation, monitoring and control applications
Proactively develop and implement maintenance activities to include both critical infrastructure and general maintenance of the physical plant (appearance)
Work with Operations and Sales leadership to assure all environmental issues are handled and communicated correctly to the customer
Report on all SLA required activities
Capacity Management
Maintain current status and future status reporting on all capacity thresholds within the facility
Access Management
Work with Security Manager to assure all access points are monitored and controlled
In conjunction with Security Manager, manage all security issues to resolution and conduct post mortems
Building Management
Management of the building and its related services such as janitorial, grounds, plumbing, fuel, phone systems, etc.
Manage all sub-contractor agreements to maintain the facility
Operation and maintenance of cable plant and fiber entrance, raised floor systems, automation and control systems, fire detection and suppression systems and security systems
Develop and document operational standards, maintenance schedules, training and testing procedures, emergency contacts, inventory requirements and vendor relationships
Environmental Services Management
Set up and maintain environmental monitoring solutions for all managed facilities
Work with Operations to assure monitoring is in place for delivered services
Asset Management
Management of asset tracking processes and systems
Support of Customer Implementations
Work with Implementation Manager to assure a smooth implementation process for the customer
Review and close tickets
Design and create service delivery procedural manual
Oversee training and development process for service delivery group
Participate in 24 X 7 X 365 rotation
Minimum Requirements:
Extensive experience and knowledge of electrical and mechanical equipment and systems operations and maintenance including, but not limited to, Caterpillar generator sets, Square D paralleling gear, MGE UPS systems, Trane Chiller plants and pumps, smoke detection systems and fire alarm systems
Cable plant design and management
Experience with building automation, monitoring and control systems (i.e. Trane Tracer Summit, SquareD PowerLogic, Caterpillar/ISO Power Lynx and Eaton Foreseer)
Energy audits
Project management
AutoCad & Visio a plus
Knowledge of building automation, monitoring and control systems including Trane Tracer Summit, SquareD PowerLogic, and Caterpillar/ISO Power Lynx
Energy audits
Ability to work well with all facility personnel and delegate tasks appropriately
Strong customer service skills
Excellent oral and written communication skills
Ability to develop and document procedures and train other personnel
Must be able to work with people at all levels internally and within the customer environment
Must be able to work in a team environment
Strong analytical and problem-solving skills
Ability to work under pressure and manage multiple concurrent priorities
Willingness to adjust hours as required by the business to include being onsite for extended hours during emergency situations such as hurricane
Strong Leadership skills and capabilities
Experience/Skills:
5+ years technical experience working in a High Availability (Data Centers, Manufacturing, Hospitals) environment
Education:
Bachelor’s Degree in Computer Science, Engineering or Facilities Management (or related field). May substitute work-related experience for degree
Certifications:
Certified Plant Engineer (CPE), Certified Plant Maintenance manager (CPMM) or similar facilities-based certification preferred
Experience/Skills:
5+ years technical experience working in a High Availability (Data Centers, Manufacturing, Hospitals) environment
Education:
Bachelor’s Degree in Computer Science, Engineering or Facilities Management (or related field). May substitute work-related experience for degree
Certifications:
Certified Plant Engineer (CPE), Certified Plant Maintenance manager (CPMM) or similar facilities-based certification preferred
CyrusOne is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, sex, sexual orientation, gender identity, religion, national origin, disability, veteran status, or other legally protected status.
CyrusOne provides reasonable accommodation for qualified individuals with disabilities in accordance with the Americans with Disabilities Act (ADA) and any other state or local laws. We will respond to requests for reasonable accommodations to assist you in applying for positions at CyrusOne, or to submit a resume. If you need to request an accommodation, please contact our Human Resources at 214.488.1365 (Option 7) or by email at HR@cyrusone.com.
Show more
Show less","Cooling, Electrical systems, Troubleshooting, Test equipment, Observation, Automation, Monitoring, Control applications, Maintenance activities, Critical infrastructure, General maintenance, Environmental issues, Capacity Management, Capacity thresholds, Access Management, Security Issues, Building Management, Janitorial, Grounds, Plumbing, Fuel, Phone systems, Subcontractor agreements, Cable plant, Fiber entrance, Raised floor systems, Automation, Control systems, Fire detection systems, Suppression systems, Security systems, Operational standards, Maintenance schedules, Training, Testing procedures, Emergency contacts, Inventory requirements, Vendor relationships, Environmental Services Management, Environmental monitoring solutions, Monitoring, Asset tracking processes, Systems, Customer Implementations, Implementation Manager, Service delivery procedural manual, Training and development process, Customer service skills, Communication skills, Procedures, Training, Leadership skills, Computer Science, Engineering, Facilities Management, Plant Engineer (CPE), Plant Maintenance manager (CPMM), High Availability, Data Centers, Manufacturing, Hospitals, AutoCAD, Visio, Trane Tracer Summit, SquareD PowerLogic, Caterpillar/ISO Power Lynx, Eaton Foreseer, Energy audits, Project management","cooling, electrical systems, troubleshooting, test equipment, observation, automation, monitoring, control applications, maintenance activities, critical infrastructure, general maintenance, environmental issues, capacity management, capacity thresholds, access management, security issues, building management, janitorial, grounds, plumbing, fuel, phone systems, subcontractor agreements, cable plant, fiber entrance, raised floor systems, automation, control systems, fire detection systems, suppression systems, security systems, operational standards, maintenance schedules, training, testing procedures, emergency contacts, inventory requirements, vendor relationships, environmental services management, environmental monitoring solutions, monitoring, asset tracking processes, systems, customer implementations, implementation manager, service delivery procedural manual, training and development process, customer service skills, communication skills, procedures, training, leadership skills, computer science, engineering, facilities management, plant engineer cpe, plant maintenance manager cpmm, high availability, data centers, manufacturing, hospitals, autocad, visio, trane tracer summit, squared powerlogic, caterpillariso power lynx, eaton foreseer, energy audits, project management","access management, asset tracking processes, autocad, automation, building management, cable plant, capacity management, capacity thresholds, caterpillariso power lynx, communication skills, computer science, control applications, control systems, cooling, critical infrastructure, customer implementations, customer service skills, data centers, eaton foreseer, electrical systems, emergency contacts, energy audits, engineering, environmental issues, environmental monitoring solutions, environmental services management, facilities management, fiber entrance, fire detection systems, fuel, general maintenance, grounds, high availability, hospitals, implementation manager, inventory requirements, janitorial, leadership skills, maintenance activities, maintenance schedules, manufacturing, monitoring, observation, operational standards, phone systems, plant engineer cpe, plant maintenance manager cpmm, plumbing, procedures, project management, raised floor systems, security issues, security systems, service delivery procedural manual, squared powerlogic, subcontractor agreements, suppression systems, systems, test equipment, testing procedures, training, training and development process, trane tracer summit, troubleshooting, vendor relationships, visio"
Data Scientist,Navigator - Powered By LifeRaft,"Halifax, Nova Scotia, Canada",https://ca.linkedin.com/jobs/view/data-scientist-at-navigator-powered-by-liferaft-3774692425,2023-12-17,Nova Scotia, Canada,Associate,Hybrid,"We are currently looking to expand our R&D team with the addition of a Data Scientist to help accelerate our development of innovative initiatives out of our recently announced AI Think Tank, LifeRaft Labs.
As a Data Scientist, you will report to the Chief Innovation Officer and work closely with fellow Data Scientists, Data Engineers, Security Experts and our Product and Engineering team members to solve complex challenges involving the use of open source data for intelligence purposes.
We are looking for a Data Scientist with a passion for data and a thirst for solving complex problems. If you’re looking for an opportunity to help shape the future of our innovative application as we embark on a complete transformation of the product, we want to hear from you. If you want to work with a dynamic, fast-growing team with an innovative spirit and a determination to help solve new challenges developed by the complexities of open source data – this could be the opportunity for you.
What You Do As a Data Scientist
Identify valuable data sources and automate collection processes
Undertake preprocessing of structured and unstructured data
Conduct exploratory data analysis to identify patterns and trends
Present information using data visualization techniques
Apply advanced statistical techniques and machine learning algorithms to analyze OSINT data to solve complex security challenges
Develop predictive models to identify potential threats and anomalies within large datasets
Maintain and optimize data pipelines, ensuring data integrity and accuracy
Drive innovation by staying up-to-date with industry trends, emerging technologies, and best practices in AI
Collaborate closely with product managers, data scientists, developers, and other stakeholders to understand the problem space and provide innovative and effective solutions
Incorporate fairness, transparency, and accountability into machine learning models and data-driven solutions to prevent bias and discrimination
Be an integral part of the LifeRaft Labs team and contribute ideas to solve complex challenges in the OSINT space
WHY LIFERAFT?
We pride ourselves on our innovative spirit and determination to help solve new challenges developed by the complexities of open source data. LifeRaft provides a threat intelligence and investigations platform, Navigator, to corporate security teams around the world, including some of the biggest brands you've probably referenced today! Navigator is designed to identify, track, and validate issues from open source channels (surface, deep web, and darknet) related to executive safety, fraud prevention, and asset & infrastructure protection. Our technology is helping keep these companies, their people, and their operations safe – making a real impact in the world we all live in.
LifeRaft provides a threat intelligence and investigations platform, Navigator, to corporate security teams around the world, including some of the biggest brands you've probably referenced today! Our technology is helping keep these companies, their people, and their operations safe – making a real impact in the world we all live in.
Requirements
Proven experience as a Data Scientist or similar relevant role
Experience with cleansing and handling large volumes of data, preparing datasets, manually evaluating data and spotting patterns in visualization
Strong experience with Python data stack: - Pandas - Scikit-learn - PyTorch or TF/Keras - HF Transformers
Solid understanding of statistical analysis and machine learning techniques
Experience in natural language processing, language modeling, unsupervised methods (clustering)
Experience optimizing code for performance and resource efficiency
Experience with data visualization in Python
Experience working with commercial or open source Large Language Models
Analytical mind and business acumen
Ability to work both independently and collaboratively in a fast-paced environment
Passion for staying updated on industry trends and emerging technologies
We will value experience in tuning Large Language Models
We will value experience in MLOps, setting up ML infrastructure and deployment of models
Benefits
The diversity of our team is integral to our success. We are a team of passionate and supportive individuals and pride ourselves in fostering a collaborative, innovative, and fun culture. We offer our team:
Attractive & competitive compensation plan & benefits
Investment in personal and professional growth
Remote work/office space (with dogs!) with flexible hours
Flexible time off – Take a minimum of 15 days/year with no cap beyond!
Health Benefits & $750 Yearly Lifestyle Subsidy
Diversity & Inclusion Committee
Hilarious authentic co-workers & fun social activities
Show more
Show less","data science, data engineering, natural language processing, AI, machine learning, predictive modeling, Python, Pandas, Scikitlearn, PyTorch, TF/Keras, HF Transformers, statistical analysis, MLOps, data visualization, Jupyter Notebook, Linux, Git, Tableau, Power BI, data pipelines, large language models","data science, data engineering, natural language processing, ai, machine learning, predictive modeling, python, pandas, scikitlearn, pytorch, tfkeras, hf transformers, statistical analysis, mlops, data visualization, jupyter notebook, linux, git, tableau, power bi, data pipelines, large language models","ai, data engineering, data science, datapipeline, git, hf transformers, jupyter notebook, large language models, linux, machine learning, mlops, natural language processing, pandas, powerbi, predictive modeling, python, pytorch, scikitlearn, statistical analysis, tableau, tfkeras, visualization"
"Data Validation Engineer (C#, QA experience)",Verisk,"Halifax, Nova Scotia, Canada",https://ca.linkedin.com/jobs/view/data-validation-engineer-c%23-qa-experience-at-verisk-3783713037,2023-12-17,Nova Scotia, Canada,Mid senior,Onsite,"Company Description
We help the world see new possibilities and inspire change for better tomorrows. Our analytic solutions bridge content, data, and analytics to help business, people, and society become stronger, more resilient, and sustainable.
Job Description
At Verisk Extreme Event Solutions, we do some cool advanced analytic stuff! We build stochastic models to simulate Catastrophic Events that will inform the insurance industry. Events include Hurricanes, Earthquakes, and Flooding, just to name a few. We then run Monte Carlo simulations to provide hundreds of thousands of years of simulated events. These help the insurance and reinsurance industry make objective and data driven decisions based on their risk tolerances. As a company, we have a strong sense of purpose and know we are helping resilient communities world wide.
Job Description
We hire smart people - if you understand the entire company description then we are interested to speak with you.
This role is focused on our Reinsurance Solutions, doing Quality Assurance on the implementation of our Reinsurance products. As a part of the EES Team at Verisk, you will be responsible for the design and maintenance of robust and thorough quality assurance testing suites for our products.
A strong analytical focus and attention to detail along with a methodical approach to problem solving is strong foundation for success in this position. Candidates will encounter unstructured problems and apply their analytical creative thinking skills to advance testing capabilities.
You are also seen as an advocate for process improvements which will positively affect the stability the Extreme Event Solutions suite of products. Position requires a strong commitment quality assurance that leverages the best practices already place and help to enhance them.
You will work closely with product managers, software engineers and customer success analysts to understand user workflows, use cases and feature requirements in an effort to develop business driven test cases, frameworks and best practices.
You will analyze business requirements, use cases and architectural layers to estimate efforts, develop test strategies and ensure testing requirements are accounted for in the development life cycle of new and existing features
You will partner with the product developers to design, develop test scripts using programming or scripting languages and maintain test automation frameworks while you also ensure that optimal automated test coverage is included for the most common and business critical business flows.
You will perform manual testing when automated testing is not appropriate or cost effective.
You will be responsible for execution of automated tests for functional, regression, performance and load testing.
You will investigate, troubleshoot and help to resolve test environment instabilities and production incidents.
You will contribute to excellent user experience from an agent or customer perspective and perform business analysis on an as needed basis.
You will use your quantitative data analytics mindset to deliver on your day-to-day tasks, as you need to understand complex reinsurance scenarios, ensure your tests are robust, while you participate in cross-functional Agile Scrum teams.
Qualifications
Candidates must have Undergraduate/Graduate in STEM-related areas, including data science, engineering, science, mathematics & finance/economics. A graduate degree is preferable.
Candidates must have demonstrated experience in objected programming languages such as C++, C# or Java, and Python as well as data analysis and statistical modeling tools Like R or MATLAB
Candidates must have demonstrated and significant experience with development or testing cloud-native UI and API products (GRPC and REST), web development, web testing, and/or cloud testing environments (Serverless SaaS stack). Strong prior experience with the AWS Ecosystem and technology stack is preferable.
Candidates must have an aptitude for quantitative problem-solving or advanced analytics. Candidates must have demonstrated QA testing experience in building test strategies using BDD test frameworks
Candidates must have demonstrated experience working on using DevOps frameworks like GitHub, Jenkins, JIRA
Successful candidates must demonstrate a strong aptitude for relational databases and good working knowledge of on-prem or cloud-based advanced database systems.
Candidates must have a deep desire to drive efficiency through process and methodology improvements.
Additional Information
In 2022, Verisk received Great Place to Work® Certification for our outstanding workplace culture for the sixth year in a row and second-time certification in the UK, Spain, and India. We’re also one of the 38 companies on the UK’s Best Workplaces™ list and one of 18 companies on Spain’s Best Workplaces™ list.
For over fifty years and through innovation, interpretation, and professional insight, Verisk has replaced uncertainty with precision to unlock opportunities that deliver significant and demonstrable impact. From our historic roots in risk assessment, we’ve grown to provide analytic insights that help transform industries focused on some of the world’s most critical areas. Today, the insurance industry relies on Verisk to be, and to make the world, more productive, resilient, and sustainable.
Verisk works in collaboration with our customers and at the intersection of people, data, and advanced technologies. Through proprietary platformed analytics, advanced modeling, and interpretation, we deliver immediate and sustained value to our customers and through them, to the individuals and societies they serve, with greater speed, precision, and scale. We’re 9,000 people strong, committed to translating big data into big ideas. We help others see new possibilities and empower certainty into big decisions that impact individuals and societies. And we relentlessly and ethically pursue innovation to help move our customers, and the world, toward better tomorrows.
Everyone at Verisk—from our chief executive officer to our newest employee—is guided by The Verisk Way, to Be Remarkable, Add Value, and Innovate.
Be Remarkable by doing something better each day in service to our customers and each other
Add Value by delivering immediate and sustained results that drive positive outcomes
Innovate by redefining what’s possible, embracing challenges, and pushing boundaries
Verisk Businesses
Underwriting Solutions — provides underwriting and rating solutions for auto and property, general liability, and excess and surplus to assess and price risk with speed and precision
Claims Solutions — supports end-to-end claims handling with analytic and automation tools that streamline workflow, improve claims management, and support better customer experiences
Property Estimating Solutions — offers property estimation software and tools for professionals in estimating all phases of building and repair to make day-to-day workflows the most efficient
Extreme Event Solutions — provides risk modeling solutions to help individuals, businesses, and society become more resilient to extreme events.
Specialty Business Solutions — provides an integrated suite of software for full end-to-end management of insurance and reinsurance business, helping companies manage their businesses through efficiency, flexibility, and data governance
Marketing Solutions — delivers data and insights to improve the reach, timing, relevance, and compliance of every consumer engagement
Life Insurance Solutions – offers end-to-end, data insight-driven core capabilities for carriers, distribution, and direct customers across the entire policy lifecycle of life and annuities for both individual and group.
Verisk Maplecroft — provides intelligence on sustainability, resilience, and ESG, helping people, business, and societies become stronger
Verisk Analytics is an equal opportunity employer.
All members of the Verisk Analytics family of companies are equal opportunity employers. We consider all qualified applicants for employment without regard to race, religion, color, national origin, citizenship, sex, gender identity and/or expression, sexual orientation, veteran's status, age or disability.
http://www.verisk.com/careers.html
Unsolicited resumes sent to Verisk, including unsolicited resumes sent to a Verisk business mailing address, fax machine or email address, or directly to Verisk employees, will be considered Verisk property. Verisk will NOT pay a fee for any placement resulting from the receipt of an unsolicited resume.
Show more
Show less","Quality assurance, Software testing, BDD test frameworks, GitHub, Jenkins, JIRA, Cloudnative UI, API products, GRPC, REST, DevOps, Relational databases, AWS Ecosystem, C++, C#, Java, Python, R, MATLAB, Agile Scrum","quality assurance, software testing, bdd test frameworks, github, jenkins, jira, cloudnative ui, api products, grpc, rest, devops, relational databases, aws ecosystem, c, c, java, python, r, matlab, agile scrum","agile scrum, api products, aws ecosystem, bdd test frameworks, c, cloudnative ui, devops, github, grpc, java, jenkins, jira, matlab, python, quality assurance, r, relational databases, rest, software testing"
Senior Data Engineer,Smart TechLink Solutions Inc.,"Halifax, Nova Scotia, Canada",https://ca.linkedin.com/jobs/view/senior-data-engineer-at-smart-techlink-solutions-inc-3747922862,2023-12-17,Nova Scotia, Canada,Mid senior,Onsite,"Responsibilities
8+ Years in engineering solutions using PySpark and Python
Strong SQL development Experience
Experienced in working in a Cloud environment
Able to create cohesive technology solutions using microservices
our system and implementing new features
Collaborating with other team members in a Feature Team perspective
Developing and delivering complex software requirements to accomplish business goals
Show more
Show less","PySpark, Python, SQL, Microservices, Cloud Computing, Software Development, Feature Engineering, Business Intelligence","pyspark, python, sql, microservices, cloud computing, software development, feature engineering, business intelligence","business intelligence, cloud computing, feature engineering, microservices, python, software development, spark, sql"
Sr Data Engineer,Diverse Lynx,"Halifax, Nova Scotia, Canada",https://ca.linkedin.com/jobs/view/sr-data-engineer-at-diverse-lynx-3686438547,2023-12-17,Nova Scotia, Canada,Mid senior,Onsite,"Primary Skillset - Databricks
Secondary skillset - AWS Services.
Job summary Skills: Python, Snowflake, AWS, Data Bricks Description: This role will be part of a team focused on cloud transformation, modernizing analytics platforms and improving agility. The role requires hands-on experience in building and managing analytics solutions in Snowflake, Provide direction on adoption of Cloud technologies (Snowflake) and industry best practices in the field of Data Engineering architecture and Development. Primary duties and responsibilities: - Good understanding in Snowflake
Diverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company.
Show more
Show less","Databricks, Python, Snowflake, AWS, Cloud technologies, Data Engineering, Architecture, Development","databricks, python, snowflake, aws, cloud technologies, data engineering, architecture, development","architecture, aws, cloud technologies, data engineering, databricks, development, python, snowflake"
Data Engineering Consultant,CGI,"Halifax, Nova Scotia, Canada",https://ca.linkedin.com/jobs/view/data-engineering-consultant-at-cgi-3731702363,2023-12-17,Nova Scotia, Canada,Mid senior,Onsite,"Position Description
Want to leverage your experience and development skills in the Healthcare industry as a Data Engineer.
CGI is seeking a Data Engineer who can bring in expertise and industry best practices define better development and engineering approaches. This is an exciting opportunity to augment your current skills, as well as learn new technologies.
Your future duties and responsibilities
As a Data Engineer you will contribute to our Services practice and will have the below responsibilities:
Work with technical development team and team lead to understand desired application capabilities.
Continuously improve software engineering practices.
Work within and across Agile teams to test and support technical solutions across a full-stack of development tools and technologies
Candidate would need to do development using application development by lifecycles, & continuous integration/deployment practices.
Working to integrate open source components into data-analytic solutions
Working with vendors to enhance tool capabilities to meet enterprise needs
Willingness to continuously learn & share learnings with others
Required Qualifications To Be Successful In This Role
Possess 5 plus years of overall experience within distributed systems
Data application development and version control systems (i.e. Git or similar)
Knowledge of application development lifecycles, & continuous integration/deployment practices
Familiar with Database systems (SQL and NoSQL) ability to build efficient queries
Understand Data warehousing solutions i.e. Snowflake / Star Schema
Experience working with ETL tools for ingesting, processing and storing of data
Familiar with and Leveraging Data APIs
Demonstrated proficiency with either Python, Java, or Scala programming languages
Knowledge of algorithms and data structures
Familiar with AWS services
Knowledge of IAC using terraform is preferred
Snowflake MPP and graph database experience is preferred but not mandatory
Skill Set/ Years of Experience/ Proficiency Level
AWS services such as Glue, EMR, Lambda, Step Functions, CloudTrail, CloudWatch, SNS, SQS, S3, VPC, EC2, RDS, IAM
2+
Excellent
Python/Scala/Java etc Programming
3+
Excellent
Terraform
1+
Good
SQL– Simple to Complex SQL analysis
2+
Excellent
Minimum Education Required:
Bachelor’s degree in Computer Science or a related discipline, at least eight, typically ten or more years of solid, diverse work experience in IT with a minimum of eight years’ experience application program development, or the equivalent in education and work experience.
Insights you can act on
While technology is at the heart of our clients’ digital transformation, we understand that people are at the heart of business success.
When you join CGI, you become a trusted advisor, collaborating with colleagues and clients to bring forward actionable insights that deliver meaningful and sustainable outcomes. We call our employees ""members"" because they are CGI shareholders and owners and owners who enjoy working and growing together to build a company we are proud of. This has been our Dream since 1976, and it has brought us to where we are today — one of the world’s largest independent providers of IT and business consulting services.
At CGI, we recognize the richness that diversity brings. We strive to create a work culture where all belong and collaborate with clients in building more inclusive communities. As an equal-opportunity employer, we want to empower all our members to succeed and grow. If you require an accommodation at any point during the recruitment process, please let us know. We will be happy to assist.
Ready to become part of our success story? Join CGI — where your ideas and actions make a difference.
Show more
Show less","Data Engineering, Healthcare industry, Software Engineering, Agile, Application Development, Data Warehousing, ETL Tools, Data APIs, Python, Java, Scala, Algorithms, Data Structures, AWS, Terraform, Snowflake, SQL, CloudTrail, CloudWatch, SNS, SQS, S3, VPC, EC2, RDS, IAM, Git","data engineering, healthcare industry, software engineering, agile, application development, data warehousing, etl tools, data apis, python, java, scala, algorithms, data structures, aws, terraform, snowflake, sql, cloudtrail, cloudwatch, sns, sqs, s3, vpc, ec2, rds, iam, git","agile, algorithms, application development, aws, cloudtrail, cloudwatch, data apis, data engineering, data structures, datawarehouse, ec2, etl tools, git, healthcare industry, iam, java, python, rds, s3, scala, snowflake, sns, software engineering, sql, sqs, terraform, vpc"
Data Engineering Consultant,CGI,"Halifax, Nova Scotia, Canada",https://ca.linkedin.com/jobs/view/data-engineering-consultant-at-cgi-3775626573,2023-12-17,Nova Scotia, Canada,Mid senior,Onsite,"Position Description
Are you ready to join an outstanding culture that cares about diversity and inclusion, corporate social responsibility and gives the freedom to innovate, influence decisions, and chart an exciting career?
CGI is more than just an IT consulting company; be part of a team that supports the local community with the ability draw on global best-in-class talent. CGI is looking for a Data Engineer for its expanding team to assist our clients with their enterprise data projects. Benefits include a share purchase program, profit sharing, wellness credits, training and development programs, and flexible work schedules and locations.
The responsibilities and requirements that follow provide insight into what a Member at CGI will typically experience in this role and the characteristics they have that lead to success. We are looking for candidates that have a similar mix of experience and qualifications. These are individuals who have a passion to create value for clients by helping them unlock the value in their data assets.
Your future duties and responsibilities
Create and support analytical data infrastructure by gathering, processing, analyzing, and structuring large volumes of data from many structured and unstructured data sources, at scale.
Design, develop and implement highly scalable, repeatable, and secure data pipelines and transformation processes.
Design and build transformation models and data flows for batch, real-time, and complex event-driven processes.
Develop data ingest processes across a variety of third-party APIs, applications, and file stores.
Ensure that appropriate controls are in place and all in-motion and at-rest data is secured at all times.
Develop data catalogs and data validation scripts to ensure data accuracy, clarity, and correctness of key business metrics.
Employ proper data governance to ensure data security and integrity.
Research and make recommendations for new data management technologies and software engineering practices. Collaborate on decisions around the use of new tools and practices.
Provide guidance to a customer and project team with respect to data requirements, data gaps, and the level of effort required to deliver a solution.
Produce and maintain support documentation and data dictionaries.
Required Qualifications To Be Successful In This Role
Experience with Azure or AWS Cloud and on-prem environments.
Experience with modern software development techniques and methodologies: DevOps, Agile, etc.
Experience with data ecosystem tools such as Azure Data Factory, Azure Databricks, Azure Synapse Analytics, Snowflake, Hadoop, Spark, etc.
Experience working with SQL, PowerBI, and or other query and reporting tools.
Experience using scripting languages such as Python, Scala, Spark, Spark-Streaming, Kafka, etc.
Knowledge and practice of secure software development processes.
Attributes
Up-to-date with the latest technology trends and have a strong desire to constantly learn.
Highly detailed-oriented with exceptional organizational and follow-through skills.
Self-starter and able to execute without a lot of direction or oversight - demonstrable problem-solving skills.
Value teamwork and urgency, with a passion for driving impact.
Ability to handle multiple priorities and deadlines.
Exceptional communication skills, with an ability to make technical concepts accessible and understandable to non-technical business users.
Passionate for turning disparate streams of data into organized and actionable analytics programming acumen, competency in manipulating large volumes of data, and a solid knowledge of a broad range of technologies for data processing and modeling.
Education
Bachelor’s degree or diploma in mathematics, informatics, statistics, computer science, or information systems (or equivalent combination of skill and experience).
Insights you can act on
While technology is at the heart of our clients’ digital transformation, we understand that people are at the heart of business success.
When you join CGI, you become a trusted advisor, collaborating with colleagues and clients to bring forward actionable insights that deliver meaningful and sustainable outcomes. We call our employees ""members"" because they are CGI shareholders and owners and owners who enjoy working and growing together to build a company we are proud of. This has been our Dream since 1976, and it has brought us to where we are today — one of the world’s largest independent providers of IT and business consulting services.
At CGI, we recognize the richness that diversity brings. We strive to create a work culture where all belong and collaborate with clients in building more inclusive communities. As an equal-opportunity employer, we want to empower all our members to succeed and grow. If you require an accommodation at any point during the recruitment process, please let us know. We will be happy to assist.
Ready to become part of our success story? Join CGI — where your ideas and actions make a difference.
Show more
Show less","Azure, AWS, DevOps, Agile, Azure Data Factory, Azure Databricks, Azure Synapse Analytics, Snowflake, Hadoop, Spark, SQL, PowerBI, Python, Scala, SparkStreaming, Kafka, Secure software development, Mathematics, Informatics, Statistics, Computer science, Information systems","azure, aws, devops, agile, azure data factory, azure databricks, azure synapse analytics, snowflake, hadoop, spark, sql, powerbi, python, scala, sparkstreaming, kafka, secure software development, mathematics, informatics, statistics, computer science, information systems","agile, aws, azure, azure data factory, azure databricks, azure synapse analytics, computer science, devops, hadoop, informatics, information systems, kafka, mathematics, powerbi, python, scala, secure software development, snowflake, spark, sparkstreaming, sql, statistics"
Data Infrastructure Analyst,Dalhousie University,"Halifax, Nova Scotia, Canada",https://ca.linkedin.com/jobs/view/data-infrastructure-analyst-at-dalhousie-university-3785740697,2023-12-17,Nova Scotia, Canada,Mid senior,Onsite,"Position Details
Position Information
Position Title
Data Infrastructure Analyst
Department/Unit
Enterprise Application Services
Location
Halifax, Nova Scotia, Canada
Posting Number
S460-23
Employee Group
DPMG
Position Type
Regular, On-Going
Duration of Contract (if applicable)
Employment Type
Full Time
Full-time Equivalency (FTE)
1
Salary
$66,445 – $89,012, per annum.
Classification
ADM-06
Provisional Statement
This is a provisional classification. Formal regular review classification procedures may be initiated at any time by the incumbent or supervisor after the incumbent has worked a minimum of six months in the role.
About Dalhousie University
Dalhousie University is Atlantic Canada’s leading research-intensive university and a driver of the region’s intellectual, social and economic development. Located in the heart of Halifax, Nova Scotia, with an Agricultural Campus in Truro/Bible Hill, Dalhousie is a truly national and international university, with more than half of our almost 21,000 students coming from outside of the province. Our 6,000 faculty and staff foster a vibrant, purpose-driven community, that celebrated 200 years of academic excellence in 2018.
Job Summary
Enterprise Application Services ( EAS ) department in Information Technology Services ( ITS ) is dedicated to providing comprehensive services that enhance and support university administration. Our responsibilities encompass maintaining existing systems, implementing new administrative solutions, supporting a diverse array of enterprise application platforms, database solutions, and business intelligence. Reporting to the Associate Director, Databases & Infrastructure, you will be responsible for coordinating the implementation, upgrades, and maintenance of enterprise application systems managed by EAS , including ERP , Database, and Business Intelligence solutions. Additionally, the Data Infrastructure Analyst manages and coordinates work requests in the area of database application development and works with the Associate Director, Databases & Infrastructure, to design and implement new procedures where required to ensure smooth, timely, and high quality work.
Key Responsibilities
Performs analytical services pertaining to the proposal, feasibility, specifications, implementation, and support of enterprise application systems; Collaborates with Data Stewards, ITS Workgroup Managers, and Systems Architects to define optimal solutions and application access strategies.
Researches the best methods for securely integrating enterprise application systems within the EAS infrastructure environment.
Defines and enforces Databases & Infrastructure project standards and rules, while negotiating mutually acceptable priorities, specifications, deadlines, and resources with client management and development groups.
Researches and makes recommendations to EAS’s management on the direction and impact of new technology, and on opportunities for use of information technology to solve business related problems.
Defines and ensures compliance with standards for building and maintaining proper database structures required by applications.
Maintains full technical knowledge of all phases of programming activities; maintains technology expertise, keeping current with evolving systems analysis, programming and database technology.
Note
The successful applicant will be eligible for hybrid work (combination of in-person work on campus and remote work) as agreed by all parties based on operational requirements and university guidelines.
Required Qualifications
University degree in Computer Science or other relevant field, with minimum five years’ experience in the IT Field, preferably as an enterprise application and/or database analyst (or an equivalent combination of training and experience).
Demonstrated experience with analysis, implementation, and support of enterprise systems, such as ERP , database/warehouse (Oracle and MS SQL ), and business intelligence solutions (e.g. Cognos Analytics, Tableau, Power BI)
Demonstrated experience with programming languages, such as Oracle PLSQL , SQL , Python, andJava.
Demonstrated experience working within enterprise system environments (both Linux and Windows) with Linux Shell and PowerShell scripting.
Excellent knowledge of systems development methodology and project management processes.
Strong analytical, problem-solving, time management and organizational skills, with proven communication (verbal and written), teamwork and interpersonal skills.
Assets
Experience working within Ellucian Banner environments
Experience with modern data warehouse methodology
Job Competencies
The successful candidate is expected to demonstrate proficiency in Dalhousie’s core and leadership competencies (https://www.dal.ca/dept/leaders.html), in particular:
Knowledge & Thinking Skills
Communication
Thinking and Acting Strategically
Accountability for Performance & Results
Change & Innovation
Additional Information
Dalhousie University supports a healthy and balanced lifestyle. Our total compensation package includes a defined benefit pension plan, health and dental plans, a health spending account, an employee and family assistance program and a tuition assistance program.
Application Consideration
Applications from current university employees and external candidates are assessed concurrently. Current university employees will be given special consideration.
We sincerely appreciate all applications and note that only candidates selected for an interview will be contacted directly by the hiring department or Human Resources.
Diversity Statement
Dalhousie University commits to achieving inclusive excellence through continually championing equity, diversity, inclusion, and accessibility. The university encourages applications from Indigenous persons (especially Mi’kmaq), persons of Black/African descent (especially African Nova Scotians), and members of other racialized groups, persons with disabilities, women, persons identifying as members of 2SLGBTQIA+ communities, and all candidates who would contribute to the diversity of our community. For more information, please visit
www.dal.ca/hiringfordiversity
.
Posting Detail Information
Number of Vacancies
1
Open Date
12/15/2023
Close Date
01/05/2024
Open Until Filled
No
Special Instructions to Applicant
Quick Link for Direct Access to Posting
https://dal.peopleadmin.ca/postings/15356
Show more
Show less","Computer Science, Enterprise application and/or database analyst, ERP, Oracle, MSSQL, Cognos Analytics, Tableau, Power BI, Oracle PLSQL, SQL, Python, Java, Linux Shell, PowerShell, Ellucian Banner, Data warehouse","computer science, enterprise application andor database analyst, erp, oracle, mssql, cognos analytics, tableau, power bi, oracle plsql, sql, python, java, linux shell, powershell, ellucian banner, data warehouse","cognos analytics, computer science, datawarehouse, ellucian banner, enterprise application andor database analyst, erp, java, linux shell, mssql, oracle, oracle plsql, powerbi, powershell, python, sql, tableau"
Customer Service Representative/Data Analyst/Data Entry Clerk,Newyorkuniversity,"Cole Harbour, Nova Scotia, Canada",https://ca.linkedin.com/jobs/view/customer-service-representative-data-analyst-data-entry-clerk-at-newyorkuniversity-3755590273,2023-12-17,Nova Scotia, Canada,Mid senior,Onsite,"Summary:
The Data Analyst will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : hiring@jobsai.live
Powered by Webbtree
Show more
Show less","Data Analysis, Statistical Techniques, SQL, R, Python, DataDriven Decision Making, A/B Testing, Data Quality, Data Cleansing, Data Manipulation, Tableau, Power BI, Statistical Modeling, Hypothesis Testing, ETL Processes","data analysis, statistical techniques, sql, r, python, datadriven decision making, ab testing, data quality, data cleansing, data manipulation, tableau, power bi, statistical modeling, hypothesis testing, etl processes","ab testing, data manipulation, data quality, dataanalytics, datacleaning, datadriven decision making, etl, hypothesis testing, powerbi, python, r, sql, statistical modeling, statistical techniques, tableau"
Customer Service Representative/Data Analyst/Data Entry Clerk,Univisioncommunicationsinc,"Pictou, Nova Scotia, Canada",https://ca.linkedin.com/jobs/view/customer-service-representative-data-analyst-data-entry-clerk-at-univisioncommunicationsinc-3757208247,2023-12-17,Nova Scotia, Canada,Mid senior,Onsite,"Summary:
The Data Analyst will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : hiring@jobsai.live
Powered by Webbtree
Show more
Show less","Data Analysis, R, Python, SQL, Data Visualization, Tableau, Power BI, Statistical Modeling, Hypothesis Testing, A/B Testing, Data Management, ETL Processes","data analysis, r, python, sql, data visualization, tableau, power bi, statistical modeling, hypothesis testing, ab testing, data management, etl processes","ab testing, data management, dataanalytics, etl, hypothesis testing, powerbi, python, r, sql, statistical modeling, tableau, visualization"
Senior Data Engineer,Invitae,United States,https://www.linkedin.com/jobs/view/senior-data-engineer-at-invitae-3783850169,2023-12-17,Nova Scotia, Canada,Mid senior,Remote,"Invitae (NYSE: NVTA) is a leading medical genetics company trusted by millions of patients and their providers to deliver timely genetic information using digital technology. We aim to provide accurate and actionable answers to strengthen medical decision-making for individuals and their families. Invitae's genetics experts apply a rigorous approach to data and research, serving as the foundation of their mission to bring comprehensive genetic information into mainstream medicine to improve healthcare for billions of people.
By joining Invitae, you’ll work alongside some of the world’s experts in genetics and healthcare at the forefront of genetic medicine. We’ve crafted a culture that empowers our teams and our teammates to have the biggest impact and to explore their interests and capabilities. We prize freedom with accountability and offer significant flexibility, along with excellent benefits and competitive compensation in a fast-growing organization!
We are looking for a reliable and motivated
Senior Data Engineer
to join our Data Solutions Team in developing the data ingestion pipelines and data platform architecture that supports the analytical and reporting needs of data scientists, our bioinformatics team, customers, and internal stakeholders.
What you’ll do:
Support and collaborate with multiple teams to gather requirements, design software, and implement features that support multiple teams and use cases across Data Science, Bioinformatics, and Finance.
Design and implement reliable, scalable and efficient data framework, data driven products and software solutions for external and internal customers
Create a secure, flexible and powerful world-class Health Data Platform for medical research and treatment
Enhance existing systems to automate and use latest technologies and tools
Ability and passion for data to become the Subject Matter Expert working with users on databases, tables, schemas and meta-data
Follow and contribute to agile best practices within the organization, looking for ways to streamline, automate and reduce redundancy and costs
Support and respond to teammate and user questions in a fast-paced, collaborative environment in a timely manner
What you bring:
Minimum of 4 years of related experience with a Bachelor’s degree, 2 years and a Master’s degree or beyond.
Skilled in one or more high-level languages (Python and/or Scala preferred). Willingness to learn new languages and technologies.
Hands-on experience with troubleshooting, debugging, log collection, and alerting systems.
Proficient in AWS, Azure, or Google Cloud Platform (AWS preferred) including databases, monitoring, security, provisioning, and scalability.
Experience with relational and columnar databases including Snowflake and RDS. Proven experience in writing, debugging and modifying SQL queries.
Experience with one or more containerization tools, especially Docker and Kubernetes
Experience with messaging/queuing or stream processing systems (Kafka preferred)
Laser focus on high quality code and process, including automated testing, documentation and coding best practices
Demonstrated track record of working with cross functional teams to deliver value and features across the organization
Experience (or aptitude and interest) in contributing to and maintaining DevOps/Cloud Infrastructure.
Additional Preferred but not Required Skills:
Hands-on experience working with large datasets, ETL pipelines, and modern warehouse technologies.
Hands-on functional programming in Scala or other language
Hands-on parallel programming in Spark or other platforms
Experience with dbt or similar tools for data transformation, with Debezium or similar tools for change data capture, and Kafka for streaming data applications
Experience with maintaining and administering Kubernetes clusters
Experience with build automation and CI/CD pipelines (e.g. GitHub Actions)
Experience with one or more data visualization tools (Looker preferred)
Nice to have:
Demonstrated experience with data modeling/dimensional modeling
Demonstrated experience with database performance tuning
Familiarity with data lineage/data governance
Demonstrated understanding of security principals including OAuth, Role-Based Access Control and encryption. Experience with Snowflake Security and Data Governance
Join Us!
This salary range is an estimate, and the actual salary may vary based on a wide range of factors, including your skills, qualifications, experience and location. This position is eligible for benefits including but not limited to medical, dental, vision, life insurance, disability coverage, flexible paid time off, Spring Health, Carrot Fertility, participation in a 401k with company match, ESPP, and many other additional voluntary benefits. Invitae also offers generous paid leave programs so you can spend time with your new child, recover from your own illness or care for a sick family member.
USA National Pay Range
$138,400—$173,000 USD
Please apply even if you don’t meet all of the “What you bring” requirements noted. It’s rare that someone checks every single item, it’s ok, we encourage you to apply anyways.
Join us!
At Invitae, we value diversity and provide equal employment opportunities (EEO) to all employees and applicants without regard to race, color, religion, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. We will consider for employment qualified applicants with criminal histories in a manner consistent with the requirements of the San Francisco Fair Chance Ordinance.
We truly believe a diverse workplace is crucial to our company's success and to better serve our diverse patients. Your input is especially valuable. We’d greatly appreciate it if you can take a quick moment to make your selection(s) below. Submissions will be anonymous.
You can find a detailed explanation of our privacy practices here.
Show more
Show less","Data Engineering, Data Science, Bioinformatics, Python, Scala, AWS, Azure, Google Cloud Platform, Snowflake, RDS, SQL, Docker, Kubernetes, Kafka, ETL, Spark, dbt, Debezium, Looker, OAuth, RoleBased Access Control, Encryption, Data modeling, Dimensional modeling, Data governance, Security principals, Data lineage","data engineering, data science, bioinformatics, python, scala, aws, azure, google cloud platform, snowflake, rds, sql, docker, kubernetes, kafka, etl, spark, dbt, debezium, looker, oauth, rolebased access control, encryption, data modeling, dimensional modeling, data governance, security principals, data lineage","aws, azure, bioinformatics, data engineering, data governance, data lineage, data science, datamodeling, dbt, debezium, dimensional modeling, docker, encryption, etl, google cloud platform, kafka, kubernetes, looker, oauth, python, rds, rolebased access control, scala, security principals, snowflake, spark, sql"
Data Engineer,Atlassian,United States,https://www.linkedin.com/jobs/view/data-engineer-at-atlassian-3726930391,2023-12-17,Nova Scotia, Canada,Mid senior,Remote,"Overview
Working at Atlassian
Atlassians can choose where they work – whether in an office, from home, or a combination of the two. That way, Atlassians have more control over supporting their family, personal goals, and other priorities. We can hire people in any country where we have a legal entity. Interviews and onboarding are conducted virtually, a part of being a distributed-first company.
Responsibilities
This is a remote position. To help our teams work together effectively, this role requires you to be located in the CST or PST timezone.
Your future team
Atlassian is looking for a Data Engineer to join our Go-To Market Data Engineering (GTM-DE) team which is responsible for building our data lake, maintaining our big data pipelines / services and facilitating the movement of billions of messages each day. We work directly with business teams and plenty of platform and engineering teams to ensure growth and retention strategies at Atlassian. We are looking for an open-minded, structured thinker who is passionate about building services that scale. You will be reporting into the Senior Data Engineering Manager.
On a typical day you will help our partner teams ingest data faster into our data lake, you’ll find ways to make our data products more efficient, or come up with ideas to help build self-serve data engineering within the company. Then you will move on to building micro-services, architecting, designing, and promoting self serve capabilities at scale to help Atlassian grow.
What You'll Do
As a data engineer in the GTM-DE team, you will have the opportunity to apply your strong technical experience building highly reliable services on managing and orchestrating a multi-petabyte scale data lake. You enjoy working in an agile environment you are able to take vague requirements and transform them into solid solutions. You are motivated by solving challenging problems, where creativity is as crucial as your ability to write code and test cases.
Qualifications
On your first day, we'll expect you to have:
At least 3 years of professional experience as a software engineer or data engineer
A BS in Computer Science or equivalent experience
Strong programming skills (some combination of Python, Java, and Scala)
Experience writing SQL, structuring data, and data storage practices
Experience with data modeling
Knowledge of data warehousing concepts
Experienced building data pipelines and micro services
Experience with Spark, Hive, Airflow and other streaming technologies to process incredible volumes of streaming data
A willingness to accept failure, learn and try again
An open mind to try solutions that may seem impossible at first
Experience working on Amazon Web Services (in particular using EMR, Kinesis, RDS, S3, SQS and the like)
It's Preferred, But Not Technically Required, That You Have
Experience building self-service tooling and platforms
Built and designed Kappa architecture platforms
A passion for building and running continuous integration pipelines.
Built pipelines using Databricks and well versed with their API’s
Contributed to open source projects (Ex: Operators in Airflow)
Compensation
At Atlassian, we strive to design equitable, explainable, and competitive compensation programs. To support this goal, the baseline of our range is higher than that of the typical market range, but in turn we expect to hire most candidates near this baseline. Base pay within the range is ultimately determined by a candidate's skills, expertise, or experience. In the United States, we have three geographic pay zones. For this role, our current base pay ranges for new hires in each zone are:
Zone A: $140,100 - $186,800
Zone B: $126,100 - $168,100
Zone C: $116,300 - $155,000
This role may also be eligible for benefits, bonuses, commissions, and equity.
Please visit go.atlassian.com/payzones for more information on which locations are included in each of our geographic pay zones. However, please confirm the zone for your specific location with your recruiter.
Our Perks & Benefits
Atlassian offers a variety of perks and benefits to support you, your family and to help you engage with your local community. Our offerings include health coverage, paid volunteer days, wellness resources, and so much more. Visit
go.atlassian.com/perksandbenefits
to learn more.
About Atlassian
At Atlassian, we're motivated by a common goal: to unleash the potential of every team. Our software products help teams all over the planet and our solutions are designed for all types of work. Team collaboration through our tools makes what may be impossible alone, possible together.
We believe that the unique contributions of all Atlassians create our success. To ensure that our products and culture continue to incorporate everyone's perspectives and experience, we never discriminate based on race, religion, national origin, gender identity or expression, sexual orientation, age, or marital, veteran, or disability status. All your information will be kept confidential according to EEO guidelines.
To provide you the best experience, we can support with accommodations or adjustments at any stage of the recruitment process. Simply inform our Recruitment team during your conversation with them.
Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.
To learn more about our culture and hiring process, visit
go.atlassian.com/crh
.
Show more
Show less","Python, Java, Scala, SQL, Data modeling, Data warehousing, Data pipelines, Microservices, Spark, Hive, Airflow, Streaming technologies, Amazon Web Services, EMR, Kinesis, RDS, S3, SQS, Selfservice tooling, Platforms, Kappa architecture, Continuous integration pipelines, Databricks, Open source projects, Airflow Operators","python, java, scala, sql, data modeling, data warehousing, data pipelines, microservices, spark, hive, airflow, streaming technologies, amazon web services, emr, kinesis, rds, s3, sqs, selfservice tooling, platforms, kappa architecture, continuous integration pipelines, databricks, open source projects, airflow operators","airflow, airflow operators, amazon web services, continuous integration pipelines, databricks, datamodeling, datapipeline, datawarehouse, emr, hive, java, kappa architecture, kinesis, microservices, open source projects, platforms, python, rds, s3, scala, selfservice tooling, spark, sql, sqs, streaming technologies"
Data Engineer,Atlassian,"San Francisco, CA",https://www.linkedin.com/jobs/view/data-engineer-at-atlassian-3726930392,2023-12-17,Nova Scotia, Canada,Mid senior,Remote,"Overview
Working at Atlassian
Atlassians can choose where they work – whether in an office, from home, or a combination of the two. That way, Atlassians have more control over supporting their family, personal goals, and other priorities. We can hire people in any country where we have a legal entity. Interviews and onboarding are conducted virtually, a part of being a distributed-first company.
Responsibilities
This is a remote position. To help our teams work together effectively, this role requires you to be located in the CST or PST timezone.
Your future team
Atlassian is looking for a Data Engineer to join our Go-To Market Data Engineering (GTM-DE) team which is responsible for building our data lake, maintaining our big data pipelines / services and facilitating the movement of billions of messages each day. We work directly with business teams and plenty of platform and engineering teams to ensure growth and retention strategies at Atlassian. We are looking for an open-minded, structured thinker who is passionate about building services that scale. You will be reporting into the Senior Data Engineering Manager.
On a typical day you will help our partner teams ingest data faster into our data lake, you’ll find ways to make our data products more efficient, or come up with ideas to help build self-serve data engineering within the company. Then you will move on to building micro-services, architecting, designing, and promoting self serve capabilities at scale to help Atlassian grow.
What You'll Do
As a data engineer in the GTM-DE team, you will have the opportunity to apply your strong technical experience building highly reliable services on managing and orchestrating a multi-petabyte scale data lake. You enjoy working in an agile environment you are able to take vague requirements and transform them into solid solutions. You are motivated by solving challenging problems, where creativity is as crucial as your ability to write code and test cases.
Qualifications
On your first day, we'll expect you to have:
At least 3 years of professional experience as a software engineer or data engineer
A BS in Computer Science or equivalent experience
Strong programming skills (some combination of Python, Java, and Scala)
Experience writing SQL, structuring data, and data storage practices
Experience with data modeling
Knowledge of data warehousing concepts
Experienced building data pipelines and micro services
Experience with Spark, Hive, Airflow and other streaming technologies to process incredible volumes of streaming data
A willingness to accept failure, learn and try again
An open mind to try solutions that may seem impossible at first
Experience working on Amazon Web Services (in particular using EMR, Kinesis, RDS, S3, SQS and the like)
It's Preferred, But Not Technically Required, That You Have
Experience building self-service tooling and platforms
Built and designed Kappa architecture platforms
A passion for building and running continuous integration pipelines.
Built pipelines using Databricks and well versed with their API’s
Contributed to open source projects (Ex: Operators in Airflow)
Compensation
At Atlassian, we strive to design equitable, explainable, and competitive compensation programs. To support this goal, the baseline of our range is higher than that of the typical market range, but in turn we expect to hire most candidates near this baseline. Base pay within the range is ultimately determined by a candidate's skills, expertise, or experience. In the United States, we have three geographic pay zones. For this role, our current base pay ranges for new hires in each zone are:
Zone A: $140,100 - $186,800
Zone B: $126,100 - $168,100
Zone C: $116,300 - $155,000
This role may also be eligible for benefits, bonuses, commissions, and equity.
Please visit go.atlassian.com/payzones for more information on which locations are included in each of our geographic pay zones. However, please confirm the zone for your specific location with your recruiter.
Our Perks & Benefits
Atlassian offers a variety of perks and benefits to support you, your family and to help you engage with your local community. Our offerings include health coverage, paid volunteer days, wellness resources, and so much more. Visit
go.atlassian.com/perksandbenefits
to learn more.
About Atlassian
At Atlassian, we're motivated by a common goal: to unleash the potential of every team. Our software products help teams all over the planet and our solutions are designed for all types of work. Team collaboration through our tools makes what may be impossible alone, possible together.
We believe that the unique contributions of all Atlassians create our success. To ensure that our products and culture continue to incorporate everyone's perspectives and experience, we never discriminate based on race, religion, national origin, gender identity or expression, sexual orientation, age, or marital, veteran, or disability status. All your information will be kept confidential according to EEO guidelines.
To provide you the best experience, we can support with accommodations or adjustments at any stage of the recruitment process. Simply inform our Recruitment team during your conversation with them.
Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.
To learn more about our culture and hiring process, visit
go.atlassian.com/crh
.
Show more
Show less","Python, Java, Scala, SQL, Data modeling, Data warehousing, Data pipelines, Micro services, Spark, Hive, Airflow, Streaming technologies, Amazon Web Services, EMR, Kinesis, RDS, S3, SQS, Selfservice tooling, Platforms, Kappa architecture, Continuous integration pipelines, Databricks","python, java, scala, sql, data modeling, data warehousing, data pipelines, micro services, spark, hive, airflow, streaming technologies, amazon web services, emr, kinesis, rds, s3, sqs, selfservice tooling, platforms, kappa architecture, continuous integration pipelines, databricks","airflow, amazon web services, continuous integration pipelines, databricks, datamodeling, datapipeline, datawarehouse, emr, hive, java, kappa architecture, kinesis, micro services, platforms, python, rds, s3, scala, selfservice tooling, spark, sql, sqs, streaming technologies"
Data Engineer,Tech Mahindra,United States,https://www.linkedin.com/jobs/view/data-engineer-at-tech-mahindra-3727254438,2023-12-17,Nova Scotia, Canada,Mid senior,Remote,"Our client is looking for Strong Data Engineer with 6+ year’s experience.
Required Skills:
ADF pipelines, SQL, Kusto, Power BI, Cosmos (Scope Scripts). Power Bi, ADX (Kusto), ADF, ADO, Python/C#.
Good to have
– Azure anomaly Alerting, App Insights, Azure Functions, Azure Fabric
Show more
Show less","ADF pipelines, SQL, Kusto, Power BI, Cosmos (Scope Scripts), Power Bi, ADX (Kusto), ADO, Python, C#, Azure anomaly Alerting, App Insights, Azure Functions, Azure Fabric","adf pipelines, sql, kusto, power bi, cosmos scope scripts, power bi, adx kusto, ado, python, c, azure anomaly alerting, app insights, azure functions, azure fabric","adf pipelines, ado, adx kusto, app insights, azure anomaly alerting, azure fabric, azure functions, c, cosmos scope scripts, kusto, powerbi, python, sql"
Senior Data Engineer,FinThrive,United States,https://www.linkedin.com/jobs/view/senior-data-engineer-at-finthrive-3770146987,2023-12-17,Nova Scotia, Canada,Mid senior,Remote,"About FinThrive
FinThrive is advancing the healthcare economy. We rethink revenue management to pave the way for a healthcare system that ensures every transaction and patient experience is addressed holistically. We’re making breakthroughs in technology—developing award-winning revenue management solutions that adapt with healthcare professionals, freeing providers and payers from complexity and inefficiency, so they can focus on doing their best work. Our end-to-end revenue management platform delivers a smarter, smoother revenue experience that increases revenue, reduces costs, expands cash collections, and ensures regulatory compliance across the entire revenue cycle continuum. We’ve delivered over $10 billion in net revenue and cash to more than 3,245 customers worldwide. When healthcare finance becomes effortless, the boundaries of what’s possible in healthcare expand. For more information on our new vision for healthcare revenue management, visit finthrive.com
About Award-winning Culture of Customer-centricity and Reliability
At FinThrive we’re proud of our agile and committed culture, which has led to certification as a ""Great Place to Work"" since 2017. We’re honored to have also been ranked #21 among the Best Workplaces in Healthcare for 2023, and we know that it's our collective dedication that makes FinThrive an exceptional place to work.
Find balance with our remote-friendly organization
Take time to recharge and pursue your passions
Be part of a positive and supportive work environment
Grow and excel your career with training and education
Our Perks and Benefits
FinThrive is committed to continually enhancing the employee experience by actively seeking new perks and benefits. For the most up-to-date offerings visit finthrive.com/careers-benefits.
Impact you will make
The Staff Data Engineer is responsible for designing, developing, and supporting highly scalable data warehousing solutions and expected to work with the latest Microsoft BI tools to create solutions that meet internal and external stakeholders’ business needs.
What you will do
Participate in analysis, design, development, deployment, and support of Data Warehouse solutions
Work closely with Lead and Architect in implementing best practices and managing production and development/test environments
Work with stakeholders to understand business needs and develop highly scalable solutions and make recommendations to help solve problems or improve processes
Understanding and working with multiple data sources to meet business rules and supports analytical needs
Create Databrick notebooks, stored procedures, SSIS packages, and using other methods to import/translate/manipulate data
Analyze potential data quality issues to determine the root cause and creating effective solutions
Optimize processes involving large data sets to improve performance
Participate in on-going evolution, improvement, and automation of data warehouse solutions
Mentor other developers and analysts
What you will bring
Bachelor’s Degree in Computer Science
6+ years of experience working directly with data warehouses and Business Intelligence
6+ years’ experience designing, developing, testing, and supporting of ETL, and OLAP cubes
4+ years’ experience on Microsoft BI stack (SSIS, SSAS, and SSRS)
1+ years’ experience on Microsoft Azure data warehouse stack and/or Databricks or equivalent cloud
Advanced SQL Server programming and knowledge of data warehousing best practices
Working experience of OLAP technologies, and dimensional modeling
Working experience in Microsoft Azure Data Warehouse technologies such as
Azure Data Factory
, Polybase, U-SQL, SQL Data warehouse, Azure Analysis Services or equivalent
The ability to extrapolate database schema to meet the business needs of the application
Excellent problem solving and analytical skills
Excellent verbal and written communication skills
What we would like to see
2+ years’ experience on Microsoft Azure data warehouse stack and/or Databricks or equivalent cloud
.NET or Java programming skills
Healthcare Revenue Cycle Experience in a data science environment
Working experience in at least one of the BI tools (i.e., Power BI, Tableau, Spotfire, MicroStrategy)
Working experience in Databricks
Statement of EEO
FinThrive values diversity and belonging and is proud to be an Equal Employment Opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or protected veteran status. We're committed to providing reasonable accommodation for qualified applicants with disabilities in our job application and recruitment process.
Show more
Show less","Data Warehousing, Microsoft BI Tools, SSIS, SSAS, SSRS, Microsoft Azure Data Warehouse, Databricks, SQL Server, OLAP Technologies, Dimensional Modeling, Azure Data Factory, Polybase, USQL, SQL Data Warehouse, Azure Analysis Services, .NET, Java, Healthcare Revenue Cycle Experience, Power BI, Tableau, Spotfire, MicroStrategy","data warehousing, microsoft bi tools, ssis, ssas, ssrs, microsoft azure data warehouse, databricks, sql server, olap technologies, dimensional modeling, azure data factory, polybase, usql, sql data warehouse, azure analysis services, net, java, healthcare revenue cycle experience, power bi, tableau, spotfire, microstrategy","azure analysis services, azure data factory, databricks, datawarehouse, dimensional modeling, healthcare revenue cycle experience, java, microsoft azure data warehouse, microsoft bi tools, microstrategy, net, olap technologies, polybase, powerbi, spotfire, sql data warehouse, sql server, ssas, ssis, ssrs, tableau, usql"
Data Engineer,Atlassian,"San Francisco, CA",https://www.linkedin.com/jobs/view/data-engineer-at-atlassian-3774851924,2023-12-17,Nova Scotia, Canada,Mid senior,Remote,"Overview
Working at Atlassian
Atlassians can choose where they work – whether in an office, from home, or a combination of the two. That way, Atlassians have more control over supporting their family, personal goals, and other priorities. We can hire people in any country where we have a legal entity. Interviews and onboarding are conducted virtually, a part of being a distributed-first company.
Atlassian is looking for a Data Engineer to join our Data Experience Enablement team. This team owns the customer-facing data model available to Atlassian customers in the Atlassian Analytics product offering. Building a high-quality, easy-to-use, and insightful data model for our Enterprise customers involves ingesting, modeling, and validating the data in the Atlassian Data Lake.
That's where you come in. We're looking for a data engineer familiar with data pipelines, analysis, and modeling to help us expand and improve our customer-facing data offerings. You'll be working in a team of 6 engineers within the larger Analytics and Visualization Platform group. We are excited to have you join our team!
This is a remote position. To help our teams work together effectively, this role requires you to be located in the PST timezone.
Responsibilities
What you'll do
Launch new data capabilities to Atlassian customers through our Atlassian Analytics product suite.
Build tooling to empower the business to launch new data capabilities faster and easier.
Collaborate with teams across Atlassian to build high quality data models for our customers.
Work alongside a team of engineers, contributing to the team culture through your passion, creativity, and experience.
Learn and grow as an engineer through various career development opportunities.
Qualifications
Your background
BS in Computer Science or equivalent experience with 3+ years of data engineering experience
Fluency in Python
SQL skills that enable advanced analysis and data modeling
Experience with data pipeline tooling and warehouses, specifically DBT, AWS data services (Redshift, Athena, EMR), and Apache projects (Spark, Flink, Hive, and Kafka)
Familiar with modern software development practices (Agile, TDD, CICD) applied to data engineering
General familiarity with cloud environments such as AWS or GCP
Compensation
At Atlassian, we strive to design equitable, explainable, and competitive compensation programs. To support this goal, the baseline of our range is higher than that of the typical market range, but in turn we expect to hire most candidates near this baseline. Base pay within the range is ultimately determined by a candidate's skills, expertise, or experience. In the United States, we have three geographic pay zones. For this role, our current base pay ranges for new hires in each zone are:
Zone A: $140,100 - $186,800
Zone B: $126,100 - $168,000
Zone C: $116,300 - $155,000
This role may also be eligible for benefits, bonuses, commissions, and equity.
Please visit go.atlassian.com/payzones for more information on which locations are included in each of our geographic pay zones. However, please confirm the zone for your specific location with your recruiter.
Our Perks & Benefits
Atlassian offers a variety of perks and benefits to support you, your family and to help you engage with your local community. Our offerings include health coverage, paid volunteer days, wellness resources, and so much more. Visit
go.atlassian.com/perksandbenefits
to learn more.
About Atlassian
At Atlassian, we're motivated by a common goal: to unleash the potential of every team. Our software products help teams all over the planet and our solutions are designed for all types of work. Team collaboration through our tools makes what may be impossible alone, possible together.
We believe that the unique contributions of all Atlassians create our success. To ensure that our products and culture continue to incorporate everyone's perspectives and experience, we never discriminate based on race, religion, national origin, gender identity or expression, sexual orientation, age, or marital, veteran, or disability status. All your information will be kept confidential according to EEO guidelines.
To provide you the best experience, we can support with accommodations or adjustments at any stage of the recruitment process. Simply inform our Recruitment team during your conversation with them.
Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.
To learn more about our culture and hiring process, visit
go.atlassian.com/crh
.
Show more
Show less","Python, SQL, Data Pipeline Tooling, AWS Data Services, Apache Projects, DBT, Redshift, Athena, EMR, Spark, Flink, Hive, Kafka, Agile, TDD, CICD, Cloud Environments, AWS, GCP","python, sql, data pipeline tooling, aws data services, apache projects, dbt, redshift, athena, emr, spark, flink, hive, kafka, agile, tdd, cicd, cloud environments, aws, gcp","agile, apache projects, athena, aws, aws data services, cicd, cloud environments, data pipeline tooling, dbt, emr, flink, gcp, hive, kafka, python, redshift, spark, sql, tdd"
Data Engineer (Remote),ezCater,United States,https://www.linkedin.com/jobs/view/data-engineer-remote-at-ezcater-3772657953,2023-12-17,Nova Scotia, Canada,Mid senior,Remote,"ezCater is the most trusted provider of corporate food solutions and is purpose-built for business. ezCater's corporate food platform and flexible, scalable food solutions allow organizations to centralize and track their food spend, and fulfill everything from daily employee meals to client meetings and company all-hands. ezCater backs this up with business-grade, best-in-class, customer service and an unmatched nationwide footprint. We're backed by top investors including Insight, Iconiq, Lightspeed, GIC, SoftBank, and Quadrille.
Are you passionate about data? How about leveraging data to drive meaningful impact across a fast growing two-sided marketplace? Have opinions on how to best enable data scientists across a billion-dollar company to do elastic workforce planning or real-time customer lifetime value prediction? Then we should definitely talk!
The Data Technology team at ezCater is growing! As we look towards 2023 and beyond, data is a key strategic component across the company - from advanced, real-time machine learning to business intelligence and data governance. Data is our differentiator and how we will drive real, meaningful impact to the $60+billion Catering industry.
We are hiring a Data Engineer to join our expanding team in solving complex data and platform challenges to accelerate our growing business. The ideal candidate lives and breathes data while driving systems and architecture best practices. They care about driving business impact through producing solid and efficient infrastructure alongside accurate and performant data. You will have the opportunity to work directly with executive stakeholders as we embark on a massive-scale data modeling effort across the organization, so flexibility and the ability to translate business requests to implementation are key.
What You’ll Do
Write and ship a lot of code. Mostly within dbt (SQL, Jinja)
Work directly with analysts and stakeholders to refine requirements, nail down logic, and debug and qualify produced data sets to ensure they meet the underlying business needs
Design and develop high-performance data pipelines. Adhering to SDLC, including CI/CD, best practices
Identify opportunities to optimize or scale existing parts of our stack
Utilize tooling and automation to improve developer efficiency
Monitor data systems to ensure quality and availability while seeking to drive down costs
Contribute to the team processes and community
Be part of our innovation and transformation story
What You Have
Experience with data warehousing, data lakes, ELT process, and enterprise data platforms such as Snowflake (preferred), Redshift & BigQuery
Experience with building performant data pipelines across disparate systems
Experience with cloud platforms such as AWS (preferred), GCP & Azure
Strong in SQL and experience in Python
Ability to work independently and collaboratively
An open mind and willingness to be flexible. We have a large and complex business, and believe in driving real value out of every project we do
Our stack is Snowflake, dbt, Fivetran, Airflow, AWS, Sagemaker, MLFlow, Kubernetes + Docker, Monte Carlo, Hightouch and Python for custom ETL and data science integrations. Experience with the above is a nice-to-have, but a desire to learn is a must.
A sharp mind, a soft heart and a large funny bone
The national cash compensation range for this role is $116,000 - $143,000* per year
ezCater does not sponsor applicants for work visas or legal permanent residence.
What You’ll Get From Us
You’ll get a terrifically compelling opportunity, in an environment of radical transparency, open access to all the data, and collaborative colleagues at every level of our organization. You’ll also get sane working hours and great flexibility around work/life balance.
Have people in your life – of any age – who always, often, or sometimes need your help? We make room for that. Have a bad thing or a good thing happen to you? We make room for that, too.
Oh, and you’ll get all this: Market salary, stock options that you’ll help make worth a lot, the usual holidays, all-you-can-eat vacation, 401K with ezCater match, health/dental/FSA, long-term disability insurance, remote-hybrid work from our awesome Boston or Denver offices OR your home OR a mixture of both home and office (you choose!), a tremendous amount of responsibility and autonomy, wicked awesome co-workers, cupcakes (and many more goodies)
when you’re in one of
our offices, and knowing that you helped get this rocket ship to the moon.
ezCater is an equal opportunity employer. We embrace humans of every background, appearance, race, religion, color, national origin, gender, gender identity, sexual orientation, age, marital status, veteran status, and disability status. At the same time, we do not employ jerks, even brilliant ones.
For information on how ezCater collects and uses job applicants' personal information, visit our Job Applicant Privacy Policy.
Show more
Show less","SQL, Jinja, dbt, Data warehousing, Data lakes, ELT process, Enterprise data platforms, Snowflake, Redshift, BigQuery, Data pipelines, Cloud platforms, AWS, GCP, Azure, Python, Airflow, Sagemaker, MLFlow, Kubernetes, Docker, Monte Carlo, Hightouch, Machine learning, Business intelligence, Data governance","sql, jinja, dbt, data warehousing, data lakes, elt process, enterprise data platforms, snowflake, redshift, bigquery, data pipelines, cloud platforms, aws, gcp, azure, python, airflow, sagemaker, mlflow, kubernetes, docker, monte carlo, hightouch, machine learning, business intelligence, data governance","airflow, aws, azure, bigquery, business intelligence, cloud platforms, data governance, data lakes, datapipeline, datawarehouse, dbt, docker, elt process, enterprise data platforms, gcp, hightouch, jinja, kubernetes, machine learning, mlflow, monte carlo, python, redshift, sagemaker, snowflake, sql"
Senior Data Engineer,Juul Labs,United States,https://www.linkedin.com/jobs/view/senior-data-engineer-at-juul-labs-3748329225,2023-12-17,Nova Scotia, Canada,Mid senior,Remote,"THE COMPANY:
Juul Labs’ mission is to impact the lives of the world’s one billion adult smokers by eliminating combustible cigarettes. We have the opportunity to address one of the world’s most intractable challenges through a commitment to exceptional quality, research, design, and innovation. Backed by leading technology investors, we are committed to the same excellence when it comes to hiring great talent.
We are a diverse team that is united by this common purpose and we are hiring the world’s best engineers, scientists, designers, product managers, operations experts, and customer service and business professionals. If the opportunity to build your career at one of the fastest growing companies is compelling, read on for more details.
Must live in US
Data at Juul means working with varied, large, data sets – where we apply analytical methods to help inform and drive business and product decisions. We are looking for an output-focused problem solver with a strong conceptual mindset and superb communication skills.
The team sees itself as analytical generalists – we choose the right technique for each problem, pride ourselves on building beautiful systems and dashboards while moving fast, and are ultimately driven by the value and insight data science can generate for the business and our customers. Our work directly impacts and shapes key executive decisions. This role offers tremendous upwards exposure towards senior business leaders and the chance to truly impact the decision making in a startup.
Role and Responsibilities:
Design robust, reusable and scalable data solutions and data pipeline frameworks to automate the ingestion, processing and delivery of both structured and unstructured data using python.
Lead active development of large-scale data engineering projects, including tools and solutions for orchestration, lineage, real time data ingestion, pub/sub, kubernetes
Create data pipelines in airflow, DBT and the general suite of Google Cloud Platform.
Independently lead the team to build, manage, and support data models. Ensure data quality with data tests in Monte Carlo and Datafold.
Work in an Agile environment using Trello.
Partner with Data Scientists, Data Engineers and Business Analysts across the organization to build and maintain configurable, scalable, and robust data processing infrastructure
Work closely with our Sales, Operations, Research, and Finance teams on data storage, retrieval, and analysis
Develop new systems and tools to enable stakeholders to consume and understand data more intuitively
Create and establish design standards and assurance processes to ensure compatibility and operability of data connections, flows and storage requirements
Keep Juul on the cutting edge of data technology
Our Data Stack:
Airflow, Fivetran
Google Cloud Platform - GCP (BigQuery, Storage, Dataflow, Pub/Sub, Cloud Functions/Run, Vertex AI, Cloud Build)
DBT
Monte Carlo, Datafold
Tableau
Personal and Professional Qualifications:
8-10 years of data engineering or software engineering experience with a focus on data
Significant experience independently driving engineering solutions
Advanced knowledge in developing using Python for data processing large-scale datasets and workflows
Skilled using python libraries and packages (pandas, pyarrow) in conjunction with the Google Cloud Platform (BigQuery, Storage, Pub/Sub)
Knowledge of bash/shell and orchestration tools (e.g. Airflow), is preferred.
Experience with version control (Git) and containers (Docker)
Skilled in analytical SQL in support of data modeling/ transformations and manipulating multiple data formats
Foundational expertise of deploying and maintaining machine learning pipelines is a plus
Education:
Preferred masters degree in Computer Science, Engineering, Math, or equivalent experience
Bachelor's degree required
JUUL LABS PERKS & BENEFITS:
A place to grow your career. We’ll help you set big goals - and exceed them
People. Work with talented, committed and supportive teammates
Equity and performance bonuses. Every employee is a stakeholder in our success
Cell phone subsidy, commuter benefits and discounts on JUUL products
Excellent medical, dental and vision benefits
Juul Labs is proud to be an equal opportunity employer and is committed to creating a diverse and inclusive work environment for all employees and job applicants, without regard to race, color, religion, sex, sexual orientation, age, gender identity or gender expression, national origin, disability or veteran status. We will consider for employment qualified applicants with arrest and conviction records, pursuant to the San Francisco Fair Chance Ordinance. Juul Labs also complies with the employment eligibility verification requirements of the Immigration and Nationality Act. All applicants must have authorization to work for Juul Labs in the US.
SALARY RANGES:
Salary varies by role, level and location, and is dependent on the cost of labor in a given
geographic region among other factors. These ranges may be modified at any time.
LOCATIONS:
Tier 1 Locations:
Greater New York City, and San Francisco Bay Area
Tier 2 Locations:
Greater Boston, Washington DC Metropolitan Area, Seattle/Tacoma,
Greater Sacramento, Los Angeles/OC/San Diego
Tier 3 Locations:
Rest of New England, NY Capital District, Rest of New Jersey, Greater
Philadelphia, Pittsburgh, Delaware, Rest of Maryland, Rest of Virginia, North Carolina,
Atlanta, Miami-Fort Lauderdale-WPB, Chicagoland, Dallas, Houston, Austin,
Minneapolis/St. Paul, Colorado, Phoenix, Reno, Las Vegas, Portland Ore./Vancouver
Wash., Rest of California, Hawaii
Tier 4 Locations:
Rest of US including Alaska and Puerto Rico
Tier 1 Range:
$156,000—$215,000 USD
Tier 2 Range:
$143,000—$197,000 USD
Tier 3 Range:
$134,000—$185,000 USD
Tier 4 Range:
$123,000—$170,000 USD
Show more
Show less","Python, Pandas, Largescale data, Pyarrow, Airflow, DBT, Monte Carlo, Datafold, SQL, Bash, Docker, Git, Google Cloud Platform, Vertex AI, Tableau, BigQuery, Cloud Build, Pub/Sub, Dataflow, Data storage, Data retrieval, Data analysis, Data engineering, Machine learning, Data processing","python, pandas, largescale data, pyarrow, airflow, dbt, monte carlo, datafold, sql, bash, docker, git, google cloud platform, vertex ai, tableau, bigquery, cloud build, pubsub, dataflow, data storage, data retrieval, data analysis, data engineering, machine learning, data processing","airflow, bash, bigquery, cloud build, data engineering, data processing, data retrieval, data storage, dataanalytics, dataflow, datafold, dbt, docker, git, google cloud platform, largescale data, machine learning, monte carlo, pandas, pubsub, pyarrow, python, sql, tableau, vertex ai"
Data Engineer,O'Reilly Auto Parts,United States,https://www.linkedin.com/jobs/view/data-engineer-at-o-reilly-auto-parts-3770165539,2023-12-17,Nova Scotia, Canada,Mid senior,Remote,"The
Data Engineer
oversees the full life cycle development, implementation, production support, and performance tuning of the Business Intelligence Reporting environments, and support the integration with other applications. They design and implement reporting and analytical solutions, including both the design of tables and the processes used to populate those tables with data from internal and external sources. The data they provide helps business teams drive improvement in key business metrics, customer experience, and business results. Data Engineers produce Technical Design Specifications that direct Data Analysts on requirements for the ETL processes. Members of this role understand how data is turned into information and knowledge and how the knowledge supports and enables key business processes. They must have an in-depth understanding of the business environment, and strong analytical and communication skills. Individuals must work well within a team environment.
Essential Job Functions
Design BI application technical solution based on business requirement and design
Executing deliverables as assigned including but not limited to:
Performance Tuning
Data Modeling
Data Transformation Specifications
Application Object Creation
Schema Object Creation
BI Tool Administrative Tasks
Designing, Developing, Testing, and Deploying Dashboards and Report Solutions
Organizes information by developing data dictionary and metadata structures, recommends changes in databases and data warehouse schemas, translates user requirements into data structure and elements, prepares prototypes, mockups, workflow diagrams, and flowcharts.
Enhances department reputation by accepting ownership of BI systems, accomplishes new and diverse project requests, and explores opportunities to add value to BI applications.
Validates Data to ensure data Quality
Skills and Qualifications
Bachelor's degree or equivalent in Computer Science, Engineering, Information Systems, or related field
JAVA and JAVA Script
3 years of experience in software development
Docker exposure
RDBMS experience (PostgreSQL, Oracle, SQL Server)
Java 8, Spring Boot Framework
Experience using Git
Software debugging and testing
Continuous integration, Jenkins
Excellent written and oral communication skills
Helpful Additional skills:
Event driven architecture (RabbitMQ, Kafka)
Redis
Test driven development
Scrum Methodology
JavaScript frameworks (Vue JS, React, Angular)
Docker Compose
Python 3
Go (Golang)
Show more
Show less","Data Engineering, ETL, Data Modeling, Data Transformation, Dashboards, Data Dictionary, Metadata, Data Quality, JAVA, JavaScript, Software Development, Docker, RDBMS, SQL, Git, Jenkins, Continuous Integration, Written and Oral Communication, Event Driven Architecture, Redis, Test Driven Development, Scrum Methodology, JavaScript Frameworks, Docker Compose, Python, Go","data engineering, etl, data modeling, data transformation, dashboards, data dictionary, metadata, data quality, java, javascript, software development, docker, rdbms, sql, git, jenkins, continuous integration, written and oral communication, event driven architecture, redis, test driven development, scrum methodology, javascript frameworks, docker compose, python, go","continuous integration, dashboard, data dictionary, data engineering, data quality, data transformation, datamodeling, docker, docker compose, etl, event driven architecture, git, go, java, javascript, javascript frameworks, jenkins, metadata, python, rdbms, redis, scrum methodology, software development, sql, test driven development, written and oral communication"
Data Engineer,Atlassian,"San Francisco, CA",https://www.linkedin.com/jobs/view/data-engineer-at-atlassian-3736870804,2023-12-17,Nova Scotia, Canada,Mid senior,Remote,"Overview
Working at Atlassian
Atlassians can choose where they work – whether in an office, from home, or a combination of the two. That way, Atlassians have more control over supporting their family, personal goals, and other priorities. We can hire people in any country where we have a legal entity. Interviews and onboarding are conducted virtually, a part of being a distributed-first company.
Atlassian is looking for a Data Engineer to join our Data Engineering Team. You will build top-notch data solutions and applications that inspire important decisions across the organization. You will be reporting to the Senior Data Engineering Manager.
You'll have flexibility in where you work – whether in an office, from home (remote), or a combination of the two.
Responsibilities
A typical day may involve collaborating with partners, you will design data models, acquisition processes, and applications to address needs. With experience in large-scale data processing systems (batch and streaming), you will lead business growth and enhance product experiences. And will collaborate with Technology Teams, Global Analytical Teams, and Data Scientists across programs.
You'll take ownership of problems from end-to-end: extracting/cleaning data, and understanding generating systems. Improving the quality of data by adding sources, coding rules, and producing metrics is crucial as requirements evolve. Agility and smart risk-taking are important qualities in this industry where digital innovation meets partner/customer needs over time.
Qualifications
On your first day, we'll expect you to have:
BS in Computer Science or equivalent experience with 3+ years as a Data Engineer or a similar role
Programming skills in Python & Java (good to have)
Design data models for storage and retrieval to meet product and requirements
Build scalable data pipelines using Spark, Airflow, AWS data services (Redshift, Athena, EMR), Apache projects (Spark, Flink, Hive, and Kafka)
Familiar with modern software development practices (Agile, TDD, CICD) applied to data engineering
Enhance data quality through internal tools/frameworks detecting DQ issues. Working knowledge of relational databases and SQL query authoring
We’d Be Super Excited If You Have
Followed a Kappa architecture with any of your previous deployments and domain knowledge of Financial and People System
Compensation
At Atlassian, we strive to design equitable, explainable, and competitive compensation programs. To support this goal, the baseline of our range is higher than that of the typical market range, but in turn we expect to hire most candidates near this baseline. Base pay within the range is ultimately determined by a candidate's skills, expertise, or experience. In the United States, we have three geographic pay zones. For this role, our current base pay ranges for new hires in each zone are:
Zone A: $140,100 - $186,800
Zone B: $126,100 - $168,100
Zone C: $116,300 - $155,000
This role may also be eligible for benefits, bonuses, commissions, and equity.
Please visit go.atlassian.com/payzones for more information on which locations are included in each of our geographic pay zones. However, please confirm the zone for your specific location with your recruiter.
Our Perks & Benefits
Atlassian offers a variety of perks and benefits to support you, your family and to help you engage with your local community. Our offerings include health coverage, paid volunteer days, wellness resources, and so much more. Visit
go.atlassian.com/perksandbenefits
to learn more.
About Atlassian
At Atlassian, we're motivated by a common goal: to unleash the potential of every team. Our software products help teams all over the planet and our solutions are designed for all types of work. Team collaboration through our tools makes what may be impossible alone, possible together.
We believe that the unique contributions of all Atlassians create our success. To ensure that our products and culture continue to incorporate everyone's perspectives and experience, we never discriminate based on race, religion, national origin, gender identity or expression, sexual orientation, age, or marital, veteran, or disability status. All your information will be kept confidential according to EEO guidelines.
To provide you the best experience, we can support with accommodations or adjustments at any stage of the recruitment process. Simply inform our Recruitment team during your conversation with them.
Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.
To learn more about our culture and hiring process, visit
go.atlassian.com/crh
.
Show more
Show less","Data Engineering, Programming, Python, Java, Data modeling, Apache projects, Agile, TDD, CICD, SQL, Relational databases, Kappa architecture, Financial and People System","data engineering, programming, python, java, data modeling, apache projects, agile, tdd, cicd, sql, relational databases, kappa architecture, financial and people system","agile, apache projects, cicd, data engineering, datamodeling, financial and people system, java, kappa architecture, programming, python, relational databases, sql, tdd"
Senior Data Engineer (US Remote),Octave,United States,https://www.linkedin.com/jobs/view/senior-data-engineer-us-remote-at-octave-3781905809,2023-12-17,Nova Scotia, Canada,Mid senior,Remote,"About the Company:
Octave is a modern behavioral health practice creating a new standard for care delivery that’s both high-quality and accessible to more people. With in-person and virtual clinics in California, Connecticut, Florida, New Jersey, New York, Texas, and Washington D.C., the company offers personalized care plans that can include individual, couples, and family therapy, while pioneering relationships with payers to make care more affordable through insurance. By raising the bar on how care is delivered and how providers are supported, we are building a sustainable system that values equity, affordability, and effectiveness. Learn more at www.findoctave.com.
About the Role:
We are seeking a senior level Data Engineer to write high-quality production code and perform as a key contributor to our data platform and ETL pipelines to support the company’s data and insights needs. Octave is a full-stack behavioral health services company: we employ therapists, contract with insurance providers for reimbursing care, and run our own tech stack to support across the business. We are a technology-enabled service provider working to increase access to mental health care for insured people in the US. The team is distributed across the US and has varied backgrounds, from education to fitness, advertising, travel, and healthcare. The role is virtual and reports to the VP of Engineering.
Responsibilities Include:
Build and maintain the data platform using best-in-class technologies (e.g., AWS, Python, Spark, Glue, Redshift, Athena, etc.).
Create analytic & data processing tools and reporting dashboards utilizing data pipelines to provide insights for operational teams.
Build data extraction jobs from a variety of data sources (relational databases, APIs, FTP sites, etc.).
Jointly architect a high-dimensional data-warehouse using industry standard data warehousing principles.
Evolve the core data model and schema through business growth.
Maintain high quality data for reporting and analysis through the use of automated testing and data quality validation.
Preferred Qualifications:
Preferred 5+ years of data engineering experience, with emphasis on building/maintaining data platforms.
Expertise with the Python, PySpark, and the AWS Big Data ecosystem.
Deep knowledge of SQL (preferably PostgreSQL).
Expertise in building out data pipelines, ETL design (both implementation and maintenance), and data warehousing.
Previous experience working with agile collaborative software development methodology and related tools.
Excellent written, communication, analytical skills, and attention to detail.
Willingness to learn and improve data engineering skills through collaboration with senior engineers.
Architected star or snowflake schema data warehouses.
Experience with basic infra as code technologies for managing infrastructure.
Bonus:
Experience with the Google Cloud and/or BigQuery.
Experience with practical applications of data science and AI/ML concepts and technologies.
Experience with business analytics tools such as Tableau and Looker.
Octave's Company Values:
The below values drive our day-to-day operations.
We’re human beings first. We operate with empathy and kindness – with our clients, with our collaborators, and with ourselves.
People deserve better than status quo. We’re willing to tackle the intractable problems, no matter how big, because someone should. We ask big questions, we craft big solutions, and we challenge ourselves and others to make it happen.
No bystanders. No stars. No tourists. Each person has been selected to be here, and with that comes a responsibility to bring your expertise, share your ideas, and help make this company better.
Partnership paves the path ahead. We don’t operate in a silo, internally or externally. To transform the system, we believe in working with others to create something bigger, better, and stronger.
Quality is crucial at scale. Quality is core to our business, and we refuse to sacrifice it as we grow.
Progress is a process. In the pursuit of progress, we iterate, reflect, learn, adjust – and always leave things better than we found them.
There are people behind every data point. We recognize that numbers tell only one part of the story, and we also do the work to understand impacts at the individual level.
Physical Requirements:
Prolonged periods sitting at a desk and working on a computer.
Must be able to frequently communicate with others through virtual meeting applications such as Zoom and Google Meet.
Must be able to observe and communicate information on company provided laptop.
Move up to 10 pounds on occasion.
Must be eligible to work in the United States without sponsorship now or in the future.
Compensation:
Octave is committed to pay equity. To maintain our commitment to pay equity, Octave will follow Pay Transparency regulations on all open job postings. Current Pay Transparency laws require companies to include a position's salary or hourly wage range (not including bonuses or equity-based compensation) in any internal or external job posting. This requirement extends to job postings published by a third party at an employer's request.
Octave will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with Octave’s legal duty to furnish information.
Starting pay for qualified applicants will depend on a combination of job-related factors, which may include education, training, experience, location, business needs, or market demands. The expected salary range for this role is set forth below and this range may be modified in the future.
The salary range for this role is $165,000 - $190,000.
Additionally, this position is eligible for the following benefits: company sponsored life insurance, disability and AD&D plans. Voluntary benefits such as 401k retirement, medical, dental, vision, FSA, HSA, dependent care and commuter/parking options are also available. Octave offers generous Paid Time Off as well as paid parental leave benefits.
This job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities and activities may change at any time with or without notice.
Application Instructions:
Please complete the following application. Please note that the U.S. Equal Opportunity Employment Information questions below are used for the purposes of EEOC reporting and are optional to complete. Octave is unable to change these questions and we acknowledge that many of the U.S. Equal Opportunity Employment Information questions are not inclusive or affirming of all aspects of cultural identity. Octave is committed to an inclusive workplace environment, and this information will not inform how we approach hiring or employment.
Show more
Show less","AWS, Python, PySpark, Spark, Glue, Redshift, Athena, PostgreSQL, SQL, Tableau, Looker, BigQuery, Data engineering, ETL, Data warehousing, Data pipelines, Agile, Data science, AI/ML, Business analytics, Star schema, Snowflake schema, Infra as code","aws, python, pyspark, spark, glue, redshift, athena, postgresql, sql, tableau, looker, bigquery, data engineering, etl, data warehousing, data pipelines, agile, data science, aiml, business analytics, star schema, snowflake schema, infra as code","agile, aiml, athena, aws, bigquery, business analytics, data engineering, data science, datapipeline, datawarehouse, etl, glue, infra as code, looker, postgresql, python, redshift, snowflake schema, spark, sql, star schema, tableau"
Data Engineer,Atlassian,United States,https://www.linkedin.com/jobs/view/data-engineer-at-atlassian-3736875013,2023-12-17,Nova Scotia, Canada,Mid senior,Remote,"Overview
Working at Atlassian
Atlassians can choose where they work – whether in an office, from home, or a combination of the two. That way, Atlassians have more control over supporting their family, personal goals, and other priorities. We can hire people in any country where we have a legal entity. Interviews and onboarding are conducted virtually, a part of being a distributed-first company.
Atlassian is looking for a Data Engineer to join our Data Engineering Team. You will build top-notch data solutions and applications that inspire important decisions across the organization. You will be reporting to the Senior Data Engineering Manager.
You'll have flexibility in where you work – whether in an office, from home (remote), or a combination of the two.
Responsibilities
A typical day may involve collaborating with partners, you will design data models, acquisition processes, and applications to address needs. With experience in large-scale data processing systems (batch and streaming), you will lead business growth and enhance product experiences. And will collaborate with Technology Teams, Global Analytical Teams, and Data Scientists across programs.
You'll take ownership of problems from end-to-end: extracting/cleaning data, and understanding generating systems. Improving the quality of data by adding sources, coding rules, and producing metrics is crucial as requirements evolve. Agility and smart risk-taking are important qualities in this industry where digital innovation meets partner/customer needs over time.
Qualifications
On your first day, we'll expect you to have:
BS in Computer Science or equivalent experience with 3+ years as a Data Engineer or a similar role
Programming skills in Python & Java (good to have)
Design data models for storage and retrieval to meet product and requirements
Build scalable data pipelines using Spark, Airflow, AWS data services (Redshift, Athena, EMR), Apache projects (Spark, Flink, Hive, and Kafka)
Familiar with modern software development practices (Agile, TDD, CICD) applied to data engineering
Enhance data quality through internal tools/frameworks detecting DQ issues. Working knowledge of relational databases and SQL query authoring
We’d Be Super Excited If You Have
Followed a Kappa architecture with any of your previous deployments and domain knowledge of Financial and People System
Compensation
At Atlassian, we strive to design equitable, explainable, and competitive compensation programs. To support this goal, the baseline of our range is higher than that of the typical market range, but in turn we expect to hire most candidates near this baseline. Base pay within the range is ultimately determined by a candidate's skills, expertise, or experience. In the United States, we have three geographic pay zones. For this role, our current base pay ranges for new hires in each zone are:
Zone A: $140,100 - $186,800
Zone B: $126,100 - $168,100
Zone C: $116,300 - $155,000
This role may also be eligible for benefits, bonuses, commissions, and equity.
Please visit go.atlassian.com/payzones for more information on which locations are included in each of our geographic pay zones. However, please confirm the zone for your specific location with your recruiter.
Our Perks & Benefits
Atlassian offers a variety of perks and benefits to support you, your family and to help you engage with your local community. Our offerings include health coverage, paid volunteer days, wellness resources, and so much more. Visit
go.atlassian.com/perksandbenefits
to learn more.
About Atlassian
At Atlassian, we're motivated by a common goal: to unleash the potential of every team. Our software products help teams all over the planet and our solutions are designed for all types of work. Team collaboration through our tools makes what may be impossible alone, possible together.
We believe that the unique contributions of all Atlassians create our success. To ensure that our products and culture continue to incorporate everyone's perspectives and experience, we never discriminate based on race, religion, national origin, gender identity or expression, sexual orientation, age, or marital, veteran, or disability status. All your information will be kept confidential according to EEO guidelines.
To provide you the best experience, we can support with accommodations or adjustments at any stage of the recruitment process. Simply inform our Recruitment team during your conversation with them.
Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.
To learn more about our culture and hiring process, visit
go.atlassian.com/crh
.
Show more
Show less","Python, Java, Spark, Airflow, AWS, Redshift, Athena, EMR, Apache, Hive, Kafka, Agile, TDD, CICD, Relational databases, SQL, Kappa architecture, Data engineering, Data models, Data pipelines, Data quality","python, java, spark, airflow, aws, redshift, athena, emr, apache, hive, kafka, agile, tdd, cicd, relational databases, sql, kappa architecture, data engineering, data models, data pipelines, data quality","agile, airflow, apache, athena, aws, cicd, data engineering, data models, data quality, datapipeline, emr, hive, java, kafka, kappa architecture, python, redshift, relational databases, spark, sql, tdd"
Data Engineer,Atlassian,"Mountain View, CA",https://www.linkedin.com/jobs/view/data-engineer-at-atlassian-3737406757,2023-12-17,Nova Scotia, Canada,Mid senior,Remote,"Overview
Working at Atlassian
Atlassians can choose where they work – whether in an office, from home, or a combination of the two. That way, Atlassians have more control over supporting their family, personal goals, and other priorities. We can hire people in any country where we have a legal entity. Interviews and onboarding are conducted virtually, a part of being a distributed-first company.
Atlassian is looking for a Data Engineer to join our Data Engineering Team. You will build top-notch data solutions and applications that inspire important decisions across the organization. You will be reporting to the Senior Data Engineering Manager.
You'll have flexibility in where you work – whether in an office, from home (remote), or a combination of the two.
Responsibilities
A typical day may involve collaborating with partners, you will design data models, acquisition processes, and applications to address needs. With experience in large-scale data processing systems (batch and streaming), you will lead business growth and enhance product experiences. And will collaborate with Technology Teams, Global Analytical Teams, and Data Scientists across programs.
You'll take ownership of problems from end-to-end: extracting/cleaning data, and understanding generating systems. Improving the quality of data by adding sources, coding rules, and producing metrics is crucial as requirements evolve. Agility and smart risk-taking are important qualities in this industry where digital innovation meets partner/customer needs over time.
Qualifications
On your first day, we'll expect you to have:
BS in Computer Science or equivalent experience with 3+ years as a Data Engineer or a similar role
Programming skills in Python & Java (good to have)
Design data models for storage and retrieval to meet product and requirements
Build scalable data pipelines using Spark, Airflow, AWS data services (Redshift, Athena, EMR), Apache projects (Spark, Flink, Hive, and Kafka)
Familiar with modern software development practices (Agile, TDD, CICD) applied to data engineering
Enhance data quality through internal tools/frameworks detecting DQ issues. Working knowledge of relational databases and SQL query authoring
We’d Be Super Excited If You Have
Followed a Kappa architecture with any of your previous deployments and domain knowledge of Financial and People System
Compensation
At Atlassian, we strive to design equitable, explainable, and competitive compensation programs. To support this goal, the baseline of our range is higher than that of the typical market range, but in turn we expect to hire most candidates near this baseline. Base pay within the range is ultimately determined by a candidate's skills, expertise, or experience. In the United States, we have three geographic pay zones. For this role, our current base pay ranges for new hires in each zone are:
Zone A: $140,100 - $186,800
Zone B: $126,100 - $168,100
Zone C: $116,300 - $155,000
This role may also be eligible for benefits, bonuses, commissions, and equity.
Please visit go.atlassian.com/payzones for more information on which locations are included in each of our geographic pay zones. However, please confirm the zone for your specific location with your recruiter.
Our Perks & Benefits
Atlassian offers a variety of perks and benefits to support you, your family and to help you engage with your local community. Our offerings include health coverage, paid volunteer days, wellness resources, and so much more. Visit
go.atlassian.com/perksandbenefits
to learn more.
About Atlassian
At Atlassian, we're motivated by a common goal: to unleash the potential of every team. Our software products help teams all over the planet and our solutions are designed for all types of work. Team collaboration through our tools makes what may be impossible alone, possible together.
We believe that the unique contributions of all Atlassians create our success. To ensure that our products and culture continue to incorporate everyone's perspectives and experience, we never discriminate based on race, religion, national origin, gender identity or expression, sexual orientation, age, or marital, veteran, or disability status. All your information will be kept confidential according to EEO guidelines.
To provide you the best experience, we can support with accommodations or adjustments at any stage of the recruitment process. Simply inform our Recruitment team during your conversation with them.
Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.
To learn more about our culture and hiring process, visit
go.atlassian.com/crh
.
Show more
Show less","Data Engineering, Python, Java, Spark, Airflow, AWS data services, Redshift, Athena, EMR, Apache projects, Hive, Kafka, Agile, TDD, CICD, Kappa architecture, Financial, People System, SQL, Relational databases","data engineering, python, java, spark, airflow, aws data services, redshift, athena, emr, apache projects, hive, kafka, agile, tdd, cicd, kappa architecture, financial, people system, sql, relational databases","agile, airflow, apache projects, athena, aws data services, cicd, data engineering, emr, financial, hive, java, kafka, kappa architecture, people system, python, redshift, relational databases, spark, sql, tdd"
Data Engineer,Energize Group,United States,https://www.linkedin.com/jobs/view/data-engineer-at-energize-group-3774811333,2023-12-17,Nova Scotia, Canada,Mid senior,Remote,"Our client is a start-up with a mission to mitigate the impact of clinician burnout and turnover in our nation’s health system. They are expanding our team and looking to hire the critical role of Data Engineer. The Data Engineer will work closely with the Chief Technology Officer, Chief Product Officer, and Chief Executive Officer to deliver actionable insights from data in a variety of situations.
Their mission is to improve the wellness of clinicians and their patients. They blend labor economics, machine learning, and clinical psychology. They are looking to contribute innovative and sustainable solutions to one of the most pressing problems in the healthcare industry. Building upon their proof-of-concept success, they have recently secured multiple engagements with nationwide healthcare organizations and have successfully raised institutional funding to expand their reach.
They are a small, close-knit team. This is an excellent opportunity for someone to join a mission-oriented, early-stage company at the ground floor and grow with them. There will be plenty of room for growth and creativity, and opportunities to tackle challenges that present one of the toughest problems in our society.
Requirements
Strong hands-on experience of designing, developing and managing data pipelines
Strong coding skills in Python (required) and SQL (required) and R (strongly preferred)
Experience building and maintaining data pipelines on AWS using EC2, EBS, Redshift, Postgres, S3, Data Pipeline, Kinesis, Lambda, etc.
Thorough understanding and experience with task management tools (Airflow, NiFi, etc.)
Knowledge of data management best practices for ETL, data governance, data warehousing, data modeling and data science
Basic knowledge of Linux (Redhat preferred) administration
Minimum B.S in Computer Science, software engineering or related field (or equivalent in related professional experience)
Strong communication skills
Strong problem-solving/critical thinking skills
Ability to work in a fast-paced, dynamic start-up environment
Hands-on knowledge of healthcare, health tech, or healthcare data analytics (preferred)
Typical Tasks and Responsibilities include:
Working closely with CTO and data science team to build, manage and scale data pipelines for machine learning
Assisting the CTO & CPO in managing and supporting analysis of enterprise data to drive better products and solutions
Managing company’s data infrastructure on AWS (Postgres/RDS, Athena, Redshift, S3, etc.)
Synthesizing multiple data sources, such as internal medical records and HR data
Integrating with client systems including EHR (Epic, Cerner), HRIS, flat files, etc.
Show more
Show less","Python, Linux, AWS, SQL, Apache Airflow, R, Apache NiFi, Data Pipelines, ETL, Data Governance, Data Warehousing, Data Modeling, Data Science, Machine Learning, Postgres/RDS, Amazon Athena, Amazon Redshift, Amazon S3, Apache Hadoop, Apache Hive, Apache Flume, Apache Spark","python, linux, aws, sql, apache airflow, r, apache nifi, data pipelines, etl, data governance, data warehousing, data modeling, data science, machine learning, postgresrds, amazon athena, amazon redshift, amazon s3, apache hadoop, apache hive, apache flume, apache spark","amazon athena, amazon redshift, amazon s3, apache airflow, apache flume, apache hadoop, apache hive, apache nifi, apache spark, aws, data governance, data science, datamodeling, datapipeline, datawarehouse, etl, linux, machine learning, postgresrds, python, r, sql"
Senior Data Engineer,Next Ventures,"New York, NY",https://www.linkedin.com/jobs/view/senior-data-engineer-at-next-ventures-3783596569,2023-12-17,Nova Scotia, Canada,Mid senior,Remote,"Job Title: Senior Data Engineer
Role:
Our fintech client is a market leader in providing data and business intelligence to help financial services professionals make faster and smarter investment decisions. They are seeking a highly skilled and experienced Senior Data Engineer, with a strong background in building and managing data pipelines, data warehouses and data lakes. You
will play an integral role in orchestrating the organization's data infrastructure, enabling efficient and reliable data processing, storage and analysis.
Responsibilities:
Design and develop robust, scalable and efficient data pipelines to support the extraction, transformation, and loading (ETL) processes from various data sources into data warehouses and data lakes.
Build and manage data warehouses and data lakes to store and organize large volumes of structured and unstructured data efficiently.
Identify and address performance bottlenecks, data inconsistencies, and data quality issues in data pipelines, warehouses and lakes.
Develop monitoring and alert systems to proactively identify and resolve data related issues.
Implement data governance processes and best practices to ensure data quality, integrity and security.
Collaborate with cross-functional teams to understand data requirements and design optimal solutions.
Constantly evaluate and explore emerging technologies and tools.
Mentor and guide junior data engineers by sharing knowledge, setting high standards and promoting best practices.
Requirements:
5+ years building and maintaining data pipelines, data warehouses and data lakes in a production environment
Proficiency with Python and SQL
Experience with data processing frameworks like Apache Spark or Apache Beam
Experience with ETL/ELT frameworks and tools like AWS Glue, Airflow, Airbyte, dbt, etc.
Strong knowledge of relational databases (MySQL, PostgreSQL) and experience with columnar storage technologies (Redshift, Snowflake)
Strong understanding of distributed systems, data modeling and database design principles
Familiarity with cloud platforms (AWS, Azure, GCP) and experience in deploying data infrastructure on the cloud
Bachelor's degree in Computer Science or related field
Master's degree is a plus
Perks
Annual Bonus
Comprehensive Health Benefits
401k and pension plans with employer match
Generous PTO and parental leave
Gym subsidies
Educational Reimbursements for career development
Recognition programs
Pet-friendly offices (if local)
Show more
Show less","Data Pipelines, Data Warehouses, Data Lakes, ETL, ELT, Apache Spark, Apache Beam, AWS Glue, Airflow, Airbyte, dbt, MySQL, PostgreSQL, Redshift, Snowflake, Distributed Systems, Data Modeling, Database Design, AWS, Azure, GCP, Cloud Platforms, Python, SQL","data pipelines, data warehouses, data lakes, etl, elt, apache spark, apache beam, aws glue, airflow, airbyte, dbt, mysql, postgresql, redshift, snowflake, distributed systems, data modeling, database design, aws, azure, gcp, cloud platforms, python, sql","airbyte, airflow, apache beam, apache spark, aws, aws glue, azure, cloud platforms, data lakes, data warehouses, database design, datamodeling, datapipeline, dbt, distributed systems, elt, etl, gcp, mysql, postgresql, python, redshift, snowflake, sql"
Data Engineer,Illumination Works,United States,https://www.linkedin.com/jobs/view/data-engineer-at-illumination-works-3523521348,2023-12-17,Nova Scotia, Canada,Mid senior,Remote,"Location and Travel Details:
Varies by Client
As a
Data Engineer,
you will work hands-on with challenging data engineering, data management, and analytics projects. You will collaborate with data scientists, analysts, business users, and IT teams to design, implement, and deploy data services and analytics.
Do you have what it takes?
Are you driven to implement creative solutions that unravel complex and ever-changing challenges? We value passion, curiosity, and perseverance with an ability to communicate ideas and results to diverse audiences. We look for people who thrive in collaborative and independent assignments, have the aptitude to learn new data quickly, and who are willing to mentor junior team members.
Key skills for this position include:
Must have or be willing to obtain Secret Clearance (this requires US Citizenship)
Strong SQL skills
Experience with Snowflake, Databricks, Spark SQL, PySpark, and Python
Cloud experience: Azure, AWS, or GCP
Develop and maintain ETL pipelines
Database design and principles
Data modeling, schema development, and data-centric documentation
Experience integrating data from a variety of data source types
Recommend and advise on optimal data models for data ingestion, integration, and visualization
Experience improving code performance and query optimization
Use Continuous Integration/Continuous Delivery (CI/CD) concepts to engineer a standardized data environment
Outstanding problem-solving skills
Excellent verbal and written communication skills and the ability to interact professionally with a diverse group, including executives, managers, and subject matter experts
Additional Desired Qualifications:
Experience with both structured and unstructured data
Experience with ML software, like SciKit Learn, SparkML, TensorFlow, Horovod, and Petastorm
Experience with Hadoop, Hive, and R
Understand best practices for effective predictive models, including cluster management and algorithms, and evangelize with data scientists
Minimum Education: Bachelor’s or Master’s degree in Computer Science, Mathematics, or comparable academic discipline
Experience Requirements: Opportunities for all levels of experience from Mid-Level to Principal
Acceptable candidates must successfully pass a drug test and background screen
A little more about us.
At Illumination Works, we know data, and we should, we’ve been doing it since we started in 2006! We specialize in everything data from big data to data science, software engineering, data management, AR/IoT, and cloud development. Illumination Works is a trusted technology partner in user-centered digital transformation—delivering impactful business results to clients. We partner with customers to solve their unique technology and data challenges, and stay on top of modern technologies and advancements leveraging our Innovation Lab.
Why choose us?
Taking care of our people is our number one priority. We offer market competitive salary, a generous PTO package, and comprehensive medical, dental, vision and life insurance plans. We also offer 401K, short/long-term disability insurance, a fun and engaging culture, and training opportunities to keep you up to speed on the latest technologies.
Illumination Works is committed to hiring and retaining a diverse workforce. We are an Equal Opportunity Employer, making decisions without regard to race, color, religion, sexual orientation, gender identity or national origin, age, veteran status, disability, or any other protected class. Acceptable candidates must successfully pass a drug test and background screen.
Show more
Show less","Data Engineering, Data Management, Data Analytics, SQL, Snowflake, Databricks, Spark SQL, PySpark, Python, Azure, AWS, GCP, ETL, Database Design, Data Modeling, Schema Development, DataCentric Documentation, Data Integration, Data Visualization, Code Performance, Query Optimization, Continuous Integration, Continuous Delivery, ProblemSolving, Communication, Structured Data, Unstructured Data, ML Software, SciKit Learn, SparkML, TensorFlow, Horovod, Petastorm, Hadoop, Hive, R, Predictive Models, Cluster Management, Algorithms","data engineering, data management, data analytics, sql, snowflake, databricks, spark sql, pyspark, python, azure, aws, gcp, etl, database design, data modeling, schema development, datacentric documentation, data integration, data visualization, code performance, query optimization, continuous integration, continuous delivery, problemsolving, communication, structured data, unstructured data, ml software, scikit learn, sparkml, tensorflow, horovod, petastorm, hadoop, hive, r, predictive models, cluster management, algorithms","algorithms, aws, azure, cluster management, code performance, communication, continuous delivery, continuous integration, data engineering, data integration, data management, dataanalytics, database design, databricks, datacentric documentation, datamodeling, etl, gcp, hadoop, hive, horovod, ml software, petastorm, predictive models, problemsolving, python, query optimization, r, schema development, scikit learn, snowflake, spark, spark sql, sparkml, sql, structured data, tensorflow, unstructured data, visualization"
Data Engineer,Sky Consulting Inc.,United States,https://www.linkedin.com/jobs/view/data-engineer-at-sky-consulting-inc-3783990350,2023-12-17,Nova Scotia, Canada,Mid senior,Remote,"Data Engineer
Support the development of high-performance data platforms, including integrating data from a variety of internal and external sources to support data and analytics activities
Implement scalable and efficient processes to populate or modify data.
Hands-on data processing experience in cloud and on-premises environments.
Design and implement efficient data pipelines (ETLs) to integrate data from a variety of sources
data platforms, create ER diagram, data flow chart, Data model
Develop and execute testing strategies to ensure high quality data
Provide documentation, training, and consulting for data platform
Show more
Show less","Data Engineering, Data Integration, Cloud Computing, Data Analytics, Data Pipelines, ETL, ER Diagram, Data Flow Chart, Data Modeling, Data Testing, Data Quality, Documentation, Training, Consulting","data engineering, data integration, cloud computing, data analytics, data pipelines, etl, er diagram, data flow chart, data modeling, data testing, data quality, documentation, training, consulting","cloud computing, consulting, data engineering, data flow chart, data integration, data quality, data testing, dataanalytics, datamodeling, datapipeline, documentation, er diagram, etl, training"
100% REMOTE Mid-Level Data Engineer [23-00287],Summit Human Capital,United States,https://www.linkedin.com/jobs/view/100%25-remote-mid-level-data-engineer-23-00287-at-summit-human-capital-3775047082,2023-12-17,Nova Scotia, Canada,Mid senior,Remote,"Summit Human Capital is seeking a 100% REMOTE Mid-Level Data Engineer supporting a top Fortune 125 client. This person will be working with large data sets within a cloud environment and collaborating with their team to build applications for the client, touching millions of their customers! This person should have a passion for using cutting edge technology, such as AWS, to help our client achieve both their short-term and long-term goals. The ideal candidate will meet the following criteria:
Requirements:
Bachelor or master’s degree in computer science, Computer Engineering, Statistics or Mathematics related discipline.
2-5 years of Data engineering experience including SQL and ETL
2+ years of cloud data platform experiences in AWS services including: RDS, DynamoDB, Redshift, EMR, Glue, S3, EC2
Strong team collaborations skills working with architects, data scientists, developers, and other IT team members.
Desired:
Good Java or Python programming skills, with hands-on experience building scalable, high performing and robust applications.
Experience with PySpark/Spark in handling big data
Responsibilities:
Perform large scale system architecture, implementation and project management duties when needed.
Design and develop new microservices in AWS and migrate from on-prem to AWS.
Participate in ensuring that builds are running, and web application is running smoothly.
Collaborate in all agile SCRUM meetings.
Write documentation for the APIs.
Help other developers through feedback and code reviews.
Show more
Show less","AWS, RDS, DynamoDB, Redshift, EMR, Glue, S3, EC2, SQL, ETL, Java, Python, PySpark, Spark, Scrum, Microservices, Documentation, Code review","aws, rds, dynamodb, redshift, emr, glue, s3, ec2, sql, etl, java, python, pyspark, spark, scrum, microservices, documentation, code review","aws, code review, documentation, dynamodb, ec2, emr, etl, glue, java, microservices, python, rds, redshift, s3, scrum, spark, sql"
Data Engineer,Vertisystem,"San Francisco, CA",https://www.linkedin.com/jobs/view/data-engineer-at-vertisystem-3632745582,2023-12-17,Nova Scotia, Canada,Mid senior,Remote,"Title: Data Engineer
Contract: W2
Summary:
We are hiring for our direct clients in multiple locations/ remote.
The main function of the Data Engineer is to develop, evaluate, test and maintain architectures and data solutions within our organization. The typical Data Engineer executes plans, policies, and practices that control, protect, deliver, and enhance the value of the organization’s data assets.
Job Responsibilities:
• Manage data engineering projects through the full cycle.
• Identify and underline business initiatives from a data engineering perspective
• Design, construct, install, test and maintain highly scalable data management systems.
• Ensure systems meet business requirements and industry practices.
• Design, implement, automate and maintain large scale enterprise data ETL processes.
• Build high-performance algorithms, prototypes, predictive models and proof of concepts.
Job Responsibilities:
• Design, construct, install, test and maintain highly scalable data management systems.
• Ensure systems meet business requirements and industry practices.
• Design, implement, automate and maintain large scale enterprise data ETL processes.
• Build high-performance algorithms, prototypes, predictive models and proof of concepts.
Skills:
• Ability to work as part of a team, as well as work independently or with minimal direction.
• Excellent written, presentation, and verbal communication skills.
• Collaborate with data architects, modelers and IT team members on project goals.
• Strong PC skills including knowledge of Microsoft SharePoint.
Top Must have skills:
SQL= Python Pipelines
Show more
Show less","Data Engineering, Software Development, Algorithms, Prototyping, Predictive Modeling, SQL, Python, Pipelines, Project Management, Communication, Microsoft SharePoint, ETL, Data Migration","data engineering, software development, algorithms, prototyping, predictive modeling, sql, python, pipelines, project management, communication, microsoft sharepoint, etl, data migration","algorithms, communication, data engineering, data migration, etl, microsoft sharepoint, pipelines, predictive modeling, project management, prototyping, python, software development, sql"
Data Engineer,Webologix Ltd/ INC,United States,https://www.linkedin.com/jobs/view/data-engineer-at-webologix-ltd-inc-3777158016,2023-12-17,Nova Scotia, Canada,Mid senior,Remote,"Position :- Data Engineer
Location :- Remote
Type of Hire :- Fulltime
Years of experience : 8+
Role and responsibilities
Design, Build, Develop and maintain Data ingestion pipeline and data lake on the AWS cloud.
Suggest and build serverless architecture, APIs, Enterprise services, containerized services, and data integration using AWS.
Perform detailed analysis, data profiling, and validation rules at every stage of data processing.
Experience working with different source file types like PARQUET, AVRO, CSV, JSON, XML etc.
Hands-on experience with AWS services like Athena, Glue, S3, lambda, Airflow, Redshift etc.
Expert in migration and modernizing on-prem data application to the AWS cloud platform.
Strong Programming skills in languages such as SQL, Stored Procs, Python, PySpark, PostgreSQL.
Experience with event driven and serverless architecture (SQS, event bridge, lambda, etc.)
Experience of using Code Versioning and Build Process tools like GIT.
Self-motivated and self-driven individual who can work against tight timelines.
Prioritize across multiple tasks and manage time effectively.
Learn new technologies as per the demand in the project.
Collaborate and participate in the solution review call and technical meetings with other project leads, data engineers and product owners.
Effectively communicate with team members, project managers and clients, as required.
Technical skills requirements
The candidate must demonstrate proficiency in AWS Data services skills as mentioned above.
Expert in providing technical solutions for client business problems and highlight the roadblocks in the project lifecycle.
Define technical roadmaps in collaboration with the product owners, refine the requirement and come up with the estimation.
Strong development experience using major AWS services, proficient in setting up AWS environment.
Experience in migrating on-prem legacy data to the AWS cloud.
Able to develop and optimize the ETL glue jobs to improve the performance and orchestration through Airflow.
Intelligent Monitoring of the jobs to check the job status and resource utilization.
Orchestration of AWS jobs through Airflow and maintain it on a regular basis.
Scaling of the jobs to handle large data and to meet the execution time requirement.
Able to perform detailed analysis, data quality and validation rules at every stage of data processing.
Nice-to-have skills
Experience with containerized platforms like Kubernetes, Docker etc.
Multi-cloud implementation experience will be added advantage.
Deployment of CI/CD pipelines in AWS environments
Exposure to Streaming technologies, Typescript, Confluence, Team City and Jira
Show more
Show less","AWS Data services, Athena, Glue, S3, Lambda, Airflow, Redshift, SQL, Stored Procs, Python, PySpark, PostgreSQL, SQS, Event Bridge, GIT, Kubernetes, Docker, CI/CD pipelines, Typescript, Confluence, Team City, Jira","aws data services, athena, glue, s3, lambda, airflow, redshift, sql, stored procs, python, pyspark, postgresql, sqs, event bridge, git, kubernetes, docker, cicd pipelines, typescript, confluence, team city, jira","airflow, athena, aws data services, cicd pipelines, confluence, docker, event bridge, git, glue, jira, kubernetes, lambda, postgresql, python, redshift, s3, spark, sql, sqs, stored procs, team city, typescript"
Associate Data Engineer,Arvest Bank,United States,https://www.linkedin.com/jobs/view/associate-data-engineer-at-arvest-bank-3783127222,2023-12-17,Nova Scotia, Canada,Mid senior,Remote,"Pay is based on a number of factors including the successful candidate’s job-related knowledge and skills, qualifications, and prior experience. Arvest offers a comprehensive suite of benefits, including a full range of health and life, financial, and wellness benefits. For more information about benefits, please visit www.arvest.com/careers/benefits .
Position is Monday through Friday from 8 am to 5 pm with the ability to work additional hours as project needs demand.
Incumbent should be located within the Arvest 4 State Footprint (AR, KS, MO, OK).
Remote work options may be available outside of the 4-state footprint upon further review during the interview process.
We are seeking an innovative Data Engineer to join our Data Engineering team at Arvest. You will play a key role in building out our Data Platform. The ideal candidate will have experience within OLTP, PostgresSQL(SQL, stored procs/functions), Liquibase, Gitlab & CI/CD, Python, Google Cloud Platform (GCP), Airflow/Cloud Composer, and Dataflow. It is preferred that the candidate has some exposure to Reltio development and Master Data Management.
The story of Arvest is one of commitment started by our founders in 1961, with an intense dedication to focusing on our customers. We will always be active and involved members of the communities we serve, and we will always work to put the needs of our customers first as we continue to fulfill our mission –
People helping people find financial solutions for life.
Job Title: Associate Data Engineer
An Associate Data Engineer at Arvest is an entry level technical team member who will assist with creating, maintaining, and evolving the strategy for data storing, transformation and distribution. They use common data architecture practices to translate business requirements into conceptual, logical, and physical data models that will support data analysis/visualization and decision-making across the organization.
We are seeking candidates who embrace diversity, equity, and inclusion in a workplace where everyone feels valued and inspired.
What You’ll Do at Arvest:
(Other duties may be assigned.)
Assist with developing resilient data pipeline solutions that are sustainable, fault-tolerant, and highly scalable using modern and new technologies of varying complexity and scope.
Troubleshoot low to moderate problems and assists with root cause analysis. Support production workloads as necessary. Participate in on-call rotation, as needed.
Utilize technical expertise to develop and execute queries to extract internal and external data from various sources that will be required for a robust and reliable data infrastructure.
Assist with software that performs well, is secure, and is accessible to customers. Ensure that work product delivered by the team meets standards for reusability, security, and performance and that data is available, usable, and fit-for-purpose.
Collaborate with Engineers, contractors, and 3rd parties to deliver solutions that are efficient, reusable, and impactful. May work with contractors and 3rd parties to accomplish goals.
Collaborate with the Product Owner and End Users to ensure that acceptance criteria are met and satisfies the business need.
Manage and monitor data quality and data loads using automated testing frameworks and methodologies such as Data-Driven Testing (DDT).
Assist with proofs-of-concept and proofs-of-technology to evaluate the feasibility of solutions, including recommendations based on the results.
Make sound design/coding decisions keeping customer experience in the forefront.
Research data for acquisition and evaluates suitability. Support the identification of anomalies and data quality issues.
Participate in cross-product Communities of Practice and/or Guilds by attending sessions, volunteering for research topics, and presenting findings to the group. Promote the re-use of data across the Company.
Perform code reviews. Test own work and reviews tests performed by more junior team members, as appropriate.
Exhibit strong problem solving and analytical skills, as well as strong communication and interpersonal skills. Contribute to healthy working relationships among teams and individuals.
Understand and comply with bank policy, laws, regulations, and the bank's BSA/AML Program, as applicable to your job duties. This includes but is not limited to; complete compliance training and adhere to internal procedures and controls; report any known violations of compliance policy, laws, or regulations and report any suspicious customer and/or account activity.
Responsibilities:
Toolbox for Success:
Bachelor’s Degree in Information Systems, Computer Science, Business Intelligence, or related field, or equivalent related work or military experience, is required.
Experience with programming languages such as SQL, Python, Java, and C# required.
Knowledge of data movement of structured data sources is required.
Knowledge in the following is preferred:
ETL tools (DataFlow, Dataproc, Data Fusion, or similar tools)
Transformation tools, such as DBT
Pipeline Orchestration tools (Apache Airflow or Cloud Composer)
Cloud data solutions within Google Cloud Platform, Azure, or AWS
Standardization, security, governance, and compliance
Data Visualization tools, such as Tableau, Looker, or PowerBI)
Prior experience in banking or financial services is preferred.
Relevant course work and/or group projects that have data engineering related course curriculum preferred
Relevant internship experience in data engineering preferred
Relevant military experience is considered for veterans and transitioning service members
Physical Demands:
Reasonable accommodations may be made to enable qualified individuals with disabilities to perform the essential functions.
We offer competitive compensation, benefits packages, and significant professional growth.
Along with an excellent benefits package, our associates are engaged, rewarded for performance, and encouraged to grow professionally and personally. Our future is driven by our associates. If you want to be recognized for your results and empowered to reach your potential, we urge you to apply.
Grade 15I
Pay Range:
$84850 - $100757 per year
Show more
Show less","Data Platform, OLTP, PostgresSQL, Python, Google Cloud Platform (GCP), Airflow/Cloud Composer, Dataflow, Reltio development, Master Data Management, Data architecture, Data modeling, Data analysis/visualization, Decisionmaking, Data pipeline solutions, Troubleshooting, Root cause analysis, Production workloads, Oncall rotation, Data infrastructure, Software development, Data quality, Data loads, Automated testing frameworks, DataDriven Testing (DDT), Proofsofconcept, Proofsoftechnology, Data acquisition, Data anomalies, Data quality issues, Communities of Practice, Guilds, Code reviews, Problem solving, Analytical skills, Communication skills, Interpersonal skills, Compliance training, BSA/AML Program, ETL tools, Transformation tools, Pipeline Orchestration tools, Cloud data solutions, Data Visualization tools","data platform, oltp, postgressql, python, google cloud platform gcp, airflowcloud composer, dataflow, reltio development, master data management, data architecture, data modeling, data analysisvisualization, decisionmaking, data pipeline solutions, troubleshooting, root cause analysis, production workloads, oncall rotation, data infrastructure, software development, data quality, data loads, automated testing frameworks, datadriven testing ddt, proofsofconcept, proofsoftechnology, data acquisition, data anomalies, data quality issues, communities of practice, guilds, code reviews, problem solving, analytical skills, communication skills, interpersonal skills, compliance training, bsaaml program, etl tools, transformation tools, pipeline orchestration tools, cloud data solutions, data visualization tools","airflowcloud composer, analytical skills, automated testing frameworks, bsaaml program, cloud data solutions, code reviews, communication skills, communities of practice, compliance training, data acquisition, data analysisvisualization, data anomalies, data architecture, data infrastructure, data loads, data pipeline solutions, data platform, data quality, data quality issues, data visualization tools, datadriven testing ddt, dataflow, datamodeling, decisionmaking, etl tools, google cloud platform gcp, guilds, interpersonal skills, master data management, oltp, oncall rotation, pipeline orchestration tools, postgressql, problem solving, production workloads, proofsofconcept, proofsoftechnology, python, reltio development, root cause analysis, software development, transformation tools, troubleshooting"
Senior Data Engineer,Apexon,United States,https://www.linkedin.com/jobs/view/senior-data-engineer-at-apexon-3766947158,2023-12-17,Nova Scotia, Canada,Mid senior,Remote,"Description
Position at Apexon
About Apexon
Apexon is a digital-first technology services firm specializing in accelerating business transformation and delivering human-centric digital experiences. We have been meeting customers wherever they are in the digital lifecycle and helping them outperform their competition through speed and innovation.
Apexon brings together distinct core competencies – in AI, analytics, app development, cloud, commerce, CX, data, DevOps, IoT, mobile, quality engineering and UX, and our deep expertise in BFSI, healthcare, and life sciences – to help businesses capitalize on the unlimited opportunities digital offers. Our reputation is built on a comprehensive suite of engineering services, a dedication to solving clients’ toughest technology problems, and a commitment to continuous improvement.
Backed by Goldman Sachs Asset Management and Everstone Capital, Apexon now has a global presence of 15 offices (and 10 delivery centers) across four continents.
We enable #HumanFirstDIGITAL
Role Description
You’ll be responsible for (Responsibilities):
10+ Overall industry experience
7+ years' experience with building large-scale bigdata applications development
Bachelors in computer science or related field
Provide technical leadership in developing data solutions and building frameworks.
Expertise in solutions for processing large volumes of data, using data processing tools and Big Data platforms.
Experience building Data Lake, EDW, and data applications using Azure, AWS and
Hands-on experience in cloud Data stack (preference is Azure)
Understanding of cluster and parallel architecture as well as high-scale or distributed RDBMS, SQL experience
Hands-on experience with major programming/scripting languages like Java
Java experience with OOPS concepts, multithreading
It's nice to have experience deploying code on containers.
Conduct code reviews and strive for improvement in software engineering quality.
Hands-on experience in production rollout and infrastructure configuration
Demonstrable experience in successfully delivering big data projects using Kafka, Spark
Exposure working on NoSQL Databases such as Cassandra, HBase, DynamoDB, and Elastic Search
Experience working with PCI Data and working with data scientists is a plus.
In-depth knowledge of design principles and patterns
You’ll Have (Qualification & Experience)
Bachelor's Degree in related field is required.
Don’t worry if you don’t check all the boxes; we’d still love to hear from you.
Our Commitment To Diversity & Inclusion
Did you know that Apexon has been Certified™ by Great Place To Work®, the global authority on workplace culture, in each of the three regions in which it operates: USA (for the fourth time in 2023), India (seven consecutive certifications as of 2023), and the UK.
Apexon is committed to being an equal opportunity employer and promoting diversity in the workplace. We take affirmative action to ensure equal employment opportunity for all qualified individuals. Apexon strictly prohibits discrimination and harassment of any kind and provides equal employment opportunities to employees and applicants without regard to gender, race, color, ethnicity or national origin, age, disability, religion, sexual orientation, gender identity or expression, veteran status, or any other applicable characteristics protected by law.
You can read about our Job Applicant Privacy policy here Job Applicant Privacy Policy (apexon.com)
Our Perks And Benefits
Our benefits and rewards program has been thoughtfully designed to recognize your skills and contributions, elevate your learning/upskilling experience and provide care and support for you and your loved ones.
As an Apexer, you get continuous skill-based development, opportunities for career advancement, and access to comprehensive health and well-being benefits and assistance.
We Also Offer
Health Insurance with Dental & Vision
401K Plan
Life Insurance, STD & LTD
Paid Vacations & Holidays
Paid Parental Leave
FSA Dependent & Limited Purpose care
Learning & Development
Show more
Show less","Big data, Data processing, Data lake, EDW, Data applications, Azure, AWS, Java, OOPS, Multithreading, Containers, Code reviews, Software engineering, Production rollout, Infrastructure configuration, Kafka, Spark, NoSQL, Cassandra, HBase, DynamoDB, Elastic Search, PCI, Data science, Design principles","big data, data processing, data lake, edw, data applications, azure, aws, java, oops, multithreading, containers, code reviews, software engineering, production rollout, infrastructure configuration, kafka, spark, nosql, cassandra, hbase, dynamodb, elastic search, pci, data science, design principles","aws, azure, big data, cassandra, code reviews, containers, data applications, data lake, data processing, data science, design principles, dynamodb, edw, elastic search, hbase, infrastructure configuration, java, kafka, multithreading, nosql, oops, pci, production rollout, software engineering, spark"
Senior Data Engineer,Apexon,United States,https://www.linkedin.com/jobs/view/senior-data-engineer-at-apexon-3766941810,2023-12-17,Nova Scotia, Canada,Mid senior,Remote,"Description
Position at Apexon
About Apexon
Apexon is a digital-first technology services firm specializing in accelerating business transformation and delivering human-centric digital experiences. We have been meeting customers wherever they are in the digital lifecycle and helping them outperform their competition through speed and innovation.
Apexon brings together distinct core competencies – in AI, analytics, app development, cloud, commerce, CX, data, DevOps, IoT, mobile, quality engineering and UX, and our deep expertise in BFSI, healthcare, and life sciences – to help businesses capitalize on the unlimited opportunities digital offers. Our reputation is built on a comprehensive suite of engineering services, a dedication to solving clients’ toughest technology problems, and a commitment to continuous improvement.
Backed by Goldman Sachs Asset Management and Everstone Capital, Apexon now has a global presence of 15 offices (and 10 delivery centers) across four continents.
We enable #HumanFirstDIGITAL
Role Description
You’ll be responsible for (Responsibilities):
10+ Overall industry experience
7+ years' experience with building large-scale bigdata applications development
Bachelors in computer science or related field
Provide technical leadership in developing data solutions and building frameworks.
Expertise in solutions for processing large volumes of data, using data processing tools and Big Data platforms.
Experience building Data Lake, EDW, and data applications using Azure, AWS and
Hands-on experience in cloud Data stack (preference is Azure)
Understanding of cluster and parallel architecture as well as high-scale or distributed RDBMS, SQL experience
Hands-on experience with major programming/scripting languages like Java
Java experience with OOPS concepts, multithreading
It's nice to have experience deploying code on containers.
Conduct code reviews and strive for improvement in software engineering quality.
Hands-on experience in production rollout and infrastructure configuration
Demonstrable experience in successfully delivering big data projects using Kafka, Spark
Exposure working on NoSQL Databases such as Cassandra, HBase, DynamoDB, and Elastic Search
Experience working with PCI Data and working with data scientists is a plus.
In-depth knowledge of design principles and patterns
You’ll Have (Qualification & Experience)
Bachelor's Degree in related field is required.
Don’t worry if you don’t check all the boxes; we’d still love to hear from you.
Our Commitment To Diversity & Inclusion
Did you know that Apexon has been Certified™ by Great Place To Work®, the global authority on workplace culture, in each of the three regions in which it operates: USA (for the fourth time in 2023), India (seven consecutive certifications as of 2023), and the UK.
Apexon is committed to being an equal opportunity employer and promoting diversity in the workplace. We take affirmative action to ensure equal employment opportunity for all qualified individuals. Apexon strictly prohibits discrimination and harassment of any kind and provides equal employment opportunities to employees and applicants without regard to gender, race, color, ethnicity or national origin, age, disability, religion, sexual orientation, gender identity or expression, veteran status, or any other applicable characteristics protected by law.
You can read about our Job Applicant Privacy policy here Job Applicant Privacy Policy (apexon.com)
Our Perks And Benefits
Our benefits and rewards program has been thoughtfully designed to recognize your skills and contributions, elevate your learning/upskilling experience and provide care and support for you and your loved ones.
As an Apexer, you get continuous skill-based development, opportunities for career advancement, and access to comprehensive health and well-being benefits and assistance.
We Also Offer
Health Insurance with Dental & Vision
401K Plan
Life Insurance, STD & LTD
Paid Vacations & Holidays
Paid Parental Leave
FSA Dependent & Limited Purpose care
Learning & Development
Show more
Show less","Big data, Data processing tools, Data Lake, EDW, Azure, AWS, Cloud Data stack, SQL, Java, OOPS concepts, Multithreading, Containers, Kafka, Spark, NoSQL Databases, Cassandra, HBase, DynamoDB, Elastic Search, PCI Data, Data scientists, Design principles, Patterns","big data, data processing tools, data lake, edw, azure, aws, cloud data stack, sql, java, oops concepts, multithreading, containers, kafka, spark, nosql databases, cassandra, hbase, dynamodb, elastic search, pci data, data scientists, design principles, patterns","aws, azure, big data, cassandra, cloud data stack, containers, data lake, data processing tools, data scientists, design principles, dynamodb, edw, elastic search, hbase, java, kafka, multithreading, nosql databases, oops concepts, patterns, pci data, spark, sql"
Data Engineer,Beacon Specialized Living,United States,https://www.linkedin.com/jobs/view/data-engineer-at-beacon-specialized-living-3766939606,2023-12-17,Nova Scotia, Canada,Mid senior,Remote,"Essential Functions
Assist in selecting and building Data Warehouse
Define and Build Tabular Data Model
Improving observability, discoverability, governance, and implementing a common data integrity and data quality testing framework.
Constructing reliable and performant high-volume ETL or ELT pipelines for sensitive healthcare data.
Contributing to and maintaining legacy ETL and ELT data pipelines.
Proactively monitoring data pipelines for potential problems and debugging issues if they arise.
Helping to model data at various stages of refinement, curation, and enrichment to best suit different downstream targets and marts.
Partnering with leadership to identify data objectives, targets, and bringing data insights to life.
Other duties as assigned.
Qualifications
A Bachelor's degree in computer science, data science, or information systems.
3 years of proven data and performance engineering.
Expert in SQL.
Experience using data warehouses and databases like Azure, SQLAAS.
Experience developing custom-built data/analytics solutions.
Experience with Azure, Data Factory, API’s.
A strong understanding of healthcare.
Established project management skills.
Advanced training certifications may be advantageous.
Excellent verbal and written communication skills, interpersonal, and teaching skills.
Good anticipation, analytical, and problem-solving skills.
The ability to remain current on the latest technology and best practices in information security.
Valid Driver’s License with acceptable driving record as determined by Motor Vehicle Report and insurance guidelines.
Show more
Show less","Data Warehouse, Data Modeling, Data Governance, Data Quality, ETL/ELT Pipelines, Healthcare Data, Azure, SQL, Data Factory, APIs, Project Management, Communication Skills, ProblemSolving Skills, Information Security","data warehouse, data modeling, data governance, data quality, etlelt pipelines, healthcare data, azure, sql, data factory, apis, project management, communication skills, problemsolving skills, information security","apis, azure, communication skills, data factory, data governance, data quality, datamodeling, datawarehouse, etlelt pipelines, healthcare data, information security, problemsolving skills, project management, sql"
Data Engineer,Crystal Equation Corporation,"California, United States",https://www.linkedin.com/jobs/view/data-engineer-at-crystal-equation-corporation-3778862983,2023-12-17,Nova Scotia, Canada,Mid senior,Remote,"Pay range is $67 - $84 per hour with full benefits available, including paid time off, medical/dental/vision/life insurance, 401K, parental leave, and more. Our compensation reflects the cost of labor across several US geographic markets. Pay is based on several factors including market location and may vary depending on job-related knowledge, skills, and experience.
THE PROMISES WE MAKE:
At Crystal Equation, we empower people and advance technology initiatives by building trust. Your recruiter will prep you for the interview, obtain feedback, guide you through any necessary paperwork and provide everything you need for a successful start. We will serve to empower you along the way and provide the path for your professional journey.
Data Engineer III
Responsibilities:
Design, construct, install, test and maintain highly scalable data management systems.
Ensure systems meet business requirements and industry practices.
Design, implement, automate and maintain large scale enterprise data ETL processes.
Build high-performance algorithms, prototypes, predictive models and proof of concepts.
Skills:
Ability to work as part of a team, as well as work independently or with minimal direction.
Excellent written, presentation, and verbal communication skills.
Collaborate with data architects, modelers and IT team members on project goals.
Education/Experience:
Bachelor's degree in a technical field such as computer science, computer engineering or related field required.
Show more
Show less","Data Engineering, ETL, Algorithms, Prototyping, Predictive Modeling, Proof of Concepts, Teamwork, Written Communication, Presentation Skills, Verbal Communication, Collaboration, Data Architecture, Computer Science, Computer Engineering","data engineering, etl, algorithms, prototyping, predictive modeling, proof of concepts, teamwork, written communication, presentation skills, verbal communication, collaboration, data architecture, computer science, computer engineering","algorithms, collaboration, computer engineering, computer science, data architecture, data engineering, etl, predictive modeling, presentation skills, proof of concepts, prototyping, teamwork, verbal communication, written communication"
Data Engineer,Red Oak Technologies,"Denver, CO",https://www.linkedin.com/jobs/view/data-engineer-at-red-oak-technologies-3775480584,2023-12-17,Nova Scotia, Canada,Mid senior,Remote,"Red Oak Technologies
is a leading provider of comprehensive resourcing solutions across a variety of industries and sectors including IT, Marketing, Finance, Business Operations, Manufacturing and Engineering.
Senior Software Data Engineer
to develop scalable data platform on which we run our company and ensure we scale to meet our rapid employee and business growth.
The purpose of this data platform is to ETL and aggregate data from various systems such as Workday,Adaptive, NetSuite, GreenHouse, etc. into our home-grown data lake,
and using Looker as the BI tool for data visualization. The Client is using Amazon technology such as Glue, open source tech such as Spark, Airflow.
QUALIFICATIONS:
- 5+ years professional software development experience
- Strong understanding of and experience with distributed computing framework like Hive/Hadoop and Apache Spark, Airflow, Kafka, etc.
- Experience in ETL tools and processes.
- Experience in Relational database design.
- Experience in NoSQL (DynamoDB, Cassandra, etc.) database design is a plus.
- Experience in Python / Java /SQL.
- Experience building REST services using Java/ Python frameworks
- Experience with AWS services.
- Excellent verbal and written communication skills.
- BS or MS degree in Computer Science or equivalent.
RESPONSIBILITIES:
- As a senior engineer, develop innovative solutions using modern software frameworks and paradigms.
- Design, develop, and maintain efficient large-scale data pipelines to collect and transform rich data using big data technologies
- Design, develop and maintain data services with millisecond latencies
- Design, develop and maintain user-profiles infrastructure using technologies like Aerospike, Spark and Hive
- Conduct low level systems debugging, performance measurement & optimization on large production clusters.
- Identify, design, and implement improvements to current architecture. This may include internal process improvements,
automating manual processes, optimizing data delivery, reducing cost, re-designing infrastructure for greater reliability, etc.
- Participate in architecture discussions, influence product roadmap, and take ownership and responsibility over new projects
JOIN RED OAK TECHNOLOGIES!
Please check out our available job opportunities: https://www.linkedin.com/company/red-oak-technologies/jobs/. Let us help you find your next career opportunity!!
Show more
Show less","Software development, Distributed computing frameworks, ETL tools and processes, Relational database design, NoSQL database design, Python, Java, SQL, REST services, AWS services, Hive/Hadoop, Apache Spark, Airflow, Kafka, DynamoDB, Cassandra, Aerospike, Looker, Glue","software development, distributed computing frameworks, etl tools and processes, relational database design, nosql database design, python, java, sql, rest services, aws services, hivehadoop, apache spark, airflow, kafka, dynamodb, cassandra, aerospike, looker, glue","aerospike, airflow, apache spark, aws services, cassandra, distributed computing frameworks, dynamodb, etl tools and processes, glue, hivehadoop, java, kafka, looker, nosql database design, python, relational database design, rest services, software development, sql"
Data Engineer,Lawrence Harvey,"Florida, United States",https://www.linkedin.com/jobs/view/data-engineer-at-lawrence-harvey-3780008241,2023-12-17,Nova Scotia, Canada,Mid senior,Remote,"We've partnered with Series A Startup, a cutting-edge innovator in asset management and data analytics technology. Leveraging AI and Advanced Imagery technologies, they provide invaluable insights for partners in commercial real estate, construction, and infrastructure management. Their mission is to extend the lifespan of properties, saving both time and money for their clients.
Having recently secured a $10 million funding round, Series A Startup is now focusing on expanding their R&D sector. This expansion aims to accelerate the enhancement of their AI Intelligence Platform, with a revolutionary approach to transforming the real estate industry using drones, satellites, manned aircraft, and other advanced methods to gather real-time property data. The ultimate goal is to significantly improve the efficiency and accuracy of facility condition
assessments.
Currently, they are actively seeking a skilled Data Engineer to join their team. In this role, you'll act as the lead, working collaboratively with other team members. Your primary focus will be on designing, building, and maintaining the data infrastructure and systems supporting their intelligence platform.
Qualifications:
Bachelor’s degree in a Quantitative field
6+ Years experience as a Data Engineer role
Strong programing skills in Python
Extensive experience in data modeling, database design, and SQL development (especially PostgreSQL)
Hands-on experience in data integration (Airflow/Kafka)
Cloud experience, with a preference for AWS
Data Visualization experience a plus!
This is a full time opportunity not C2C
Show more
Show less","Python, Airflow, Kafka, PostgreSQL, SQL, Cloud, AWS, Data Engineer, Data Modeling, Database Design, Data Integration, Data Visualization","python, airflow, kafka, postgresql, sql, cloud, aws, data engineer, data modeling, database design, data integration, data visualization","airflow, aws, cloud, data integration, database design, dataengineering, datamodeling, kafka, postgresql, python, sql, visualization"
Data Developer,Cayzen Technologies,"Lacey, WA",https://www.linkedin.com/jobs/view/data-developer-at-cayzen-technologies-3598178950,2023-12-17,Centralia,United States,Mid senior,Onsite,"Cayzen Technologies is looking to hire
Data Developers
for various projects in the Olympia/Tumwater area. The candidate must have the knowledge and recent experience in database design, data modeling, and data conversions to build databases for new solutions, integrations, or data migrations to Commercial Off-the-Shelf (COTS), Software as a Service (SaaS) or custom solutions utilizing Microsoft .NET/SQL technologies. Candidate must be able to lead, manage and collaborate with other solution architects, developers, and DBAs. Candidate must have a foundation in system development, depth in data modeling and database design, establishing and merging large data projects programmatically.
Cayzen is open to various employment types such contracting, full-time and C2C. Applicants who meet the following requirement will be contracted.
The Data Developer will design and develop integrated solutions. These include, but are not limited to:
Analyze existing data sources to design stage migration database.
Design and develop optimized ETLs from existing sources to stage migration database.
Centralized APIs and interfaces.
Core development delivery processes, such as deployment and package delivery.
Experience in data migration and integration with COTS, SaaS and custom solutions and developing their interfaces to other solutions.
Design and build databases for data warehouse or data marts from multiple sources with built-in retention rules.
Design and build transaction databases for new solutions or replacement solutions.
Re-engineer and optimizing existing databases or ETL’s (SSIS packets) using C# for large and for various confident categories of data.
Cayzen Technologies will assess the resumes relative to the following capabilities:
At least 7-10 years of recent experience hands-on design, development, delivering software and support.
Expert knowledge Database design: entity-relationship model, database schema design
Experience building complex, normalize database for big data, which included building conceptual, logical and physical data models.
Advanced proficiency writing complex queries from large data sources or multiple data sources and using tools to optimize SQL Server query performance
Designing and implementing data retention rules
Advance proficiency writing complex ETL’s (SSIS packets) using C# which include logging, erroring, and reporting for large data.
Advanced proficiency with SQL Server administration and writing SQL queries and ETL’s.
Strong development skills with hands-on experience and troubleshooting skills.
Experience developing and working knowledge of .NET and SQL development languages, environments and tools – including Visual Studio 2015 or later, Data Tools, C#, SQL Server 2014 or later, T-SQL.
Expert knowledge Database design: entity-relationship model, database schema design.
Strong understanding of the software development cycle and experience with agile delivery.
Strong written and verbal communication skills.
Proven ability to deliver products with the highest quality and on time.
Why work for Cayzen Technologies
We have a great crew of incredible professionals from different walks of life performing at the highest level. Our staff is enjoyable to work with, and there is quite often a bunch of laughter in the office, not yelling or mean-spirited folks that don't want to be bothered. Those types of people won't be tolerated here. We only work with good-natured, kind-hearted people that strive for excellence.
We have fun! If you are hired, you will soon learn about outings to painted plates, baseball games, running/biking clubs, Sci-Fri, potlucks and other fun things to enjoy as well! We want our employees to enjoy working here and get to know the people they work with.
We believe that every single employee is incredibly important to our success as a company, and we understand that. Come work where you are genuinely appreciated.
As a small agile company, we pride ourselves on providing not only exceptional technical solutions but a team-oriented, responsive & innovative approach toward sustainable solutions in solving today's development challenges. We recognize communication as a key factor to the success of any project.
We recognize that our employees (The Team) are what makes us so successful in providing exceptional services and satisfied clients. As such we offer competitive compensation and full medical benefits package, educational opportunities, and fun in the office as well as out of the office.
EEO Statement
Cayzen Technologies is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, veteran status, disability, sexual orientation, gender identity, or any other protected status.
Show more
Show less","Data modeling, Database design, Data conversions, Database architecture, Data integration, Data migration, ETL, COTS, SaaS, Microsoft .NET, SQL, Visual Studio, Data Tools, C#, SQL Server, EntityRelationship model, Database schema design, SQL queries, Agile development, Communication skills, Software development cycle","data modeling, database design, data conversions, database architecture, data integration, data migration, etl, cots, saas, microsoft net, sql, visual studio, data tools, c, sql server, entityrelationship model, database schema design, sql queries, agile development, communication skills, software development cycle","agile development, c, communication skills, cots, data conversions, data integration, data migration, data tools, database architecture, database design, database schema design, datamodeling, entityrelationship model, etl, microsoft net, saas, software development cycle, sql, sql queries, sql server, visual studio"
Data Engineer - Arcadis Gen,Arcadis,"Merseyside, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-arcadis-gen-at-arcadis-3770621454,2023-12-17,Liverpool, United Kingdom,Mid senior,Onsite,"Role: Data Engineer
Location: UK, Remote
As an Arcadian, you already help us deliver world leading sustainable design, engineering, and consultancy solutions for natural and built assets. You are part of our global business comprising 36,000 people, in over 70 countries, dedicated to improving quality of life.
Everyone has an important role to play. With the power of many curious minds, together we can continue to solve the world’s most complex challenges and deliver more impact together.
Role description:
The Data Engineer is responsible for migrating the data systems, delivery of the technical leadership in data migration, support of data migration strategies, tools, design and build. As well as formulating various methods to transform raw data into useful data systems. Overall, you’ll strive for efficiency by aligning data systems with business goals. This includes data ETL and modelling pipelines, hosting environments, automated testing and deployment, information storage, and retrieval and support enabling infrastructure across multiple client projects.
The Data Engineer will execute migration projects and may be a coach to other Junior Engineers within the Service Engineering Group (SEG). They will work together with Data architects, Data Consultants, and QA Consultants to enable technical solutions throughout the project lifecycle, including assessments, tool evaluations, demonstrations, requirements gathering, results presentation.
Role accountabilities:
Support and own the end-to-end migration journey on cloud-based products.
Technical delivery of ETL data migration with framework agreed.
Execute data migration runs.
Perform data migration using Snowflake, Dbt and Airflow or similar toolset.
Analyse and organize raw data; build data systems and pipelines.
Assess the compatibility with your cloud destination.
Ensure that the design fully meets the client’s needs.
Manage outputs and quality throughout delivery of a project ensuring all work packages align to the project scope.
Qualifications & Experience:
The ideal candidate will have a background in computer science and/or data engineering, ideally within a data migration consultancy or physical asset owning company context. They will have good leadership, communication, and organisational skills. They will have a strong understanding of how to deploy and scale machine learning and advance analytics techniques in a modern cloud environment. A keen interest in the latest technologies is important, to help ensure that the product (as well as Gen in general) stays on the cutting edge.
More importantly than these technical skills, however, we are looking for someone to be a data solution leader, pushing forward the boundaries of what we do, and the standards expected of the wider Gen data architecture. The successful candidate will need to be resilient and maintain a positive attitude when faced with challenges, be an excellent problem-solver, and be happy to be out of their comfort zone. They will need to have excellent communication skills, not just at a technical level, but also with more business-focused colleagues. They will not shy away from a challenge and will stand up and be counted. They will never be satisfied, and always look to find the next improvement in what Gen does, and how we do it.
We are looking for someone who possesses:
Must haves:
Experience in cloud-based data migration tools and validations
Experience using AWS and/or Azure
Experience using the following tools: Snowflake & DBT
Excellent understanding of SQL
Familiarity with DevOps processes/best practices
Part of assessment, tool options and data migration enablement
Experience of delivery of client facing projects, ideally in a data migration consultancy
Knowledge of programming languages (e.g., Java and Python)
You have experience in the use of automated testing, CI/CD, the use and creation of APIs and of virtual package environments to ensure repeatability of code.
Demonstrable skills in database management
Desirable:
Experience using Airflow
Experience using Azure Data Factory
Continue your career journey as an Arcadian.
We can only achieve our goals when everyone is empowered to be their best. We believe everyone's contribution matters. It’s why we are pioneering a skills based approach, where you can harness your unique experience and expertise to carve your career path and maximize the impact we can make together.
You do meaningful work, and no matter where your next role in Arcadis takes you, you’ll be helping to deliver sustainable solutions for a more prosperous planet. Make your mark; on your career, your colleagues, your clients, your life and the world around you.
Together, we can create a lasting legacy.
Our Commitment to Equality, Diversity, Inclusion & Belonging
We want you to be able to bring your best self to work every day which is why we take equality and inclusion seriously and hold ourselves to account for our actions. Our ambition is to be an employer of choice and provide a great place to work for all our people. We believe that by working together diverse people with different experiences develop the most innovative ideas. Equality, diversity and inclusion is at the heart of how we improve quality of life and we work closely with our people across six ED&I Workstreams: Age, Disability, Faith, Gender, LGBT+ and Race. A diverse and skilled workforce is essential to our success.
Show more
Show less","Data engineering, Data migration, ETL, SQL, Apache Airflow, Data modeling, Data pipelines, Machine learning, Advanced analytics, Cloud computing, AWS, Azure, Snowflake, DBT, DevOps, API, Virtual package environments, Programming languages (Java Python), Data storage, Data retrieval, Data architecture, Communication, Leadership, Organizational skills, Problemsolving, Resilience, Teamwork","data engineering, data migration, etl, sql, apache airflow, data modeling, data pipelines, machine learning, advanced analytics, cloud computing, aws, azure, snowflake, dbt, devops, api, virtual package environments, programming languages java python, data storage, data retrieval, data architecture, communication, leadership, organizational skills, problemsolving, resilience, teamwork","advanced analytics, apache airflow, api, aws, azure, cloud computing, communication, data architecture, data engineering, data migration, data retrieval, data storage, datamodeling, datapipeline, dbt, devops, etl, leadership, machine learning, organizational skills, problemsolving, programming languages java python, resilience, snowflake, sql, teamwork, virtual package environments"
Data Analyst - SQL & Power BI,Nigel Frank International,"Liverpool, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-analyst-sql-power-bi-at-nigel-frank-international-3780307889,2023-12-17,Liverpool, United Kingdom,Mid senior,Onsite,"A healthcare provider are looking for an Information Analyst with skills in SQL and either SSRS or Power BI to join their established Management Information team, where you'll be responsible for delivering timely and accurate data and reporting solutions for users across the organisation.
This organisation are headquartered in Hertfordshire, although this is a home-based role, so you can be based anywhere in the UK. It's likely you will be asked to travel to their Head Office around twice per year for full IT team meetings, and this will be fully expensed.
We are looking for an individual with strong SQL and T-SQL skills in order to effectively work with the data in their SQL and Snowflake database, as well as experience with either SSRS or Power BI to produce insightful reports and visualisations - currently they're on a journey to migrate most of their SSRS reports over to Power BI, with any new reports being created in Power BI.
This is a really exciting opportunity for an ambitious data professional to progress their career with an organisation who will be committed to your ongoing professional development, with the chance to work towards your Microsoft certifications - you'll have the opportunity to complete your Power BI and Snowflake certifications, and get your ITIL qualifications too.
Requirements:
Strong SQL and T-SQL skills - including complex queries, stored procedures, tables, views etc.
SSRS or Power BI Report builder experience (including DAX)
Excellent communication, problem solving and stakeholder management skills
Benefits:
Salary of £25-32,000, depending upon experience
25 days' annual leave plus bank holidays plus day off for your birthday (increasing by 1 day for every 2 years' service up to 28 days)
Pension with 5% employer contribution and 3% employee contribution
Enhanced maternity package
Death in service (2X salary)
Learning and development opportunities
Please Note: This is a permanent role for UK residents only. This role does not offer Sponsorship. You must have the right to work in the UK with no restrictions. Some of our roles may be subject to successful background checks including a DBS and Credit Check.
Nigel Frank are the go-to recruiter for Power BI and Azure Data Platform roles in the UK, offering more opportunities across the country than any other. We're the proud sponsor and supporter of SQLBits, Power Platform World Tour, the London Power BI User Group, Newcastle Power BI User Group and Newcastle Data Platform and Cloud User Group. To find out more and speak confidentially about your job search or hiring needs, please contact me directly at v.simpson@nigelfrank.com
Show more
Show less","SQL, TSQL, SSRS, Power BI, DAX, Snowflake, Databases, Tables, Views, Stored procedures, Reporting, Data visualization, ITIL","sql, tsql, ssrs, power bi, dax, snowflake, databases, tables, views, stored procedures, reporting, data visualization, itil","databases, dax, itil, powerbi, reporting, snowflake, sql, ssrs, stored procedures, tables, tsql, views, visualization"
Business Intelligence & Data Science Lead,Accessplc,"Warrington, England, United Kingdom",https://uk.linkedin.com/jobs/view/business-intelligence-data-science-lead-at-accessplc-3768236117,2023-12-17,Liverpool, United Kingdom,Mid senior,Onsite,"I am recruiting for a Business Intelligence and Data Science Lead to work on a hybrid basis - 3 days in Warrington, 2 days remote.
This is a great opportunity to work for an organisation that is investing in Data and BI, building and developing their team as they strive for data excellence.
We are seeking a dynamic and experienced Senior Business Intelligence Lead who will play a pivotal role in shaping the data operations and driving excellence in data management. You will lead the Data Engineering team, contributing to their personal development and fostering a culture of continuous learning and innovation.
Skills And Knowledge Required
You must have worked in a Lead role previously and have outstanding leadership skills to mentor and develop a data-centric team.
Familiarisation with MS BI Stack (SSIS. SSRS, PowerBI)
Awareness of Data Modelling tools and techniques
Well versed with Data Story Telling
Interest and exposure to Data Science and how it can be used to enhance published information
Comfortable operating where data literacy & data cataloguing may be challenging
In return for you experience I can offer a competitive base salary, comprehensive benefits package, free parking, subsidised canteen and free gym when on site as well as a generous training and development budget.
Interested? Apply now for further information
Show more
Show less","Business Intelligence, Data Science, Data Engineering, MS BI Stack, SSIS, SSRS, PowerBI, Data Modelling, Data Storytelling, Data Literacy, Data Cataloguing","business intelligence, data science, data engineering, ms bi stack, ssis, ssrs, powerbi, data modelling, data storytelling, data literacy, data cataloguing","business intelligence, data cataloguing, data engineering, data literacy, data modelling, data science, data storytelling, ms bi stack, powerbi, ssis, ssrs"
Junior Data Engineer,Pepper Mill,"Liverpool, England, United Kingdom",https://uk.linkedin.com/jobs/view/junior-data-engineer-at-pepper-mill-3783941330,2023-12-17,Liverpool, United Kingdom,Mid senior,Hybrid,"About Sparta Global
Embark on a transformative career journey with Sparta Global, where diversity, innovation, and passion for technology converge. We welcome individuals from all backgrounds, offering not just jobs, but dynamic careers in the tech industry. You'll work alongside enthusiastic professionals, receiving top-tier training and mentorship to hone your skills for success in both public and private sectors. Our commitment to designing impactful careers and coaching future leaders is evident in our over 10 prestigious awards in Learning & Development and Equality, Diversity & Inclusion. As a Top 20 Employer for Social Mobility and a proud B-Corp certified organisation, we're not just advancing careers; we're fostering a more diverse, equitable, and inclusive tech landscape. Join us in shaping the future of technology - where your growth is our mission, and your success, our pride. Apply now to be part of our award-winning team at Sparta Global.
About This Role
You'll become versatile in a wide array of tools across topics covering data focused coding, data visualisation, Cloud Services and Big Data.
You'll be designing, building, maintaining, and troubleshooting the data pipelines that enable organizations to store, process, and analyse their data, and ensuring the data is reliable through testing and debugging. Looking for efficiencies and optimising the data pipelines for scalability and performance will be a focus.
You'll be handling and working with large sets of structured and unstructured data and will be responsible for ensuring that the data is organized and available for data scientists and analysts to use.
Working with others is key, you could be working with other engineers, developers, data scientists, analysts and even stakeholders to understand their data needs.
We're not expecting you to have the proficiencies right away - that's where our award-winning Academy comes in. We are the experts in building skills and confidence in a fun and supportive environment that will not only challenge you but also develop your specialist capabilities ready to work on our clients' projects.
What we're looking for.
To be successful for this role you will demonstrate a level of ability in Python or similar. You will be passionate about technology and eager to learn programme development to an advanced level.
We're Seeking Candidates Who Can Exemplify Our Values
Empathy and Diversity: Integrity, respect, and a commitment to inclusivity.
Drive: A goal-oriented mindset with pride in exceeding targets.
Collaboration: A team-focused approach, fostering positive relationships.
Innovation: Curiosity, creativity, and openness to diverse ideas.
Flexibility: Adaptability and composure in the face of change.
As a national organisation with clients across the UK, we require flexibility and a willingness to relocate post-remote training. Deployment locations vary and cannot be guaranteed. We encourage applications from diverse backgrounds and experience levels. Eligibility to work in the UK by the start of employment is mandatory.
Why You Should Apply
Our environment is designed to nurture your talents and skills, your hard work and progress are not just appreciated - they're tangibly rewarded. We conduct performance-based reviews every six months, offering you the chance to increase your earning potential twice a year. This regular appraisal system is our way of ensuring that your efforts and achievements are consistently recognised and rewarded.
We Also Provide
20 days annual leave + bank holidays.
An extra day off for your birthday.
Pension.
Discounted gym membership.
Eye care.
Death in service cover.
Cycle to work scheme.
Season ticket loan.
Employee assistance program.
Yearly budget for personal development.
Access to alumni and community networks.
Opportunities to be brand ambassadors.
Being employed by Sparta Global is an investment in your future that pays dividends along the way. We give you breadth of experience and skills, along with increasing opportunities to develop further and earn more. No two career paths look the same at Sparta.
Our Recruitment Process
Begin your journey via our supportive recruitment process. Apply online and our team will promptly review your application, contacting successful candidates within 48 hours to initiate the next steps. If you pass our initial screening, candidates will proceed to online assessments which vary depending on the opportunity you are applying for. The final stage is a competency interview, here you'll have the opportunity to impress us with your ability to communicate effectively and exhibit behavioural competencies through relevant examples. We're looking for candidates who can demonstrate a collaborative spirit and a growth mindset.
Your dedicated Talent Team member will be with you every step of the way to support and answer any questions you have. You can also visit our YouTube channel to gain valuable insights and expert advice on virtual interviews, strategies to manage nerves, and tips on nonverbal communication.
We look forward to receiving your application - good luck!
Show more
Show less","Python, Data Visualization, Cloud Services, Big Data, Data Pipelines, Data Analysis, Data Storage, Data Processing, Data Reliability, Data Testing, Data Debugging, Scalability, Performance, Structured Data, Unstructured Data, Data Organization, Data Availability, Collaboration, Stakeholder Engagement, Empathy, Diversity, Integrity, Inclusion, GoalOrientation, Target Exceeding, TeamFocused Approach, Positive Relationships, Curiosity, Creativity, Openness to Diverse Ideas, Adaptability, Composure, Annual Leave, Bank Holidays, Birthday Off, Pension, Gym Membership Discount, Eye Care, Death in Service Cover, Cycle to Work Scheme, Season Ticket Loan, Employee Assistance Program, Personal Development Budget, Alumni and Community Networks, Brand Ambassador Opportunities, Supportive Recruitment Process, Online Application, Prompt Application Review, Online Assessments, Competency Interview, Effective Communication, Behavioural Competencies, Collaborative Spirit, Growth Mindset, Talent Team Support, Virtual Interviews, Nerves Management Strategies, Nonverbal Communication","python, data visualization, cloud services, big data, data pipelines, data analysis, data storage, data processing, data reliability, data testing, data debugging, scalability, performance, structured data, unstructured data, data organization, data availability, collaboration, stakeholder engagement, empathy, diversity, integrity, inclusion, goalorientation, target exceeding, teamfocused approach, positive relationships, curiosity, creativity, openness to diverse ideas, adaptability, composure, annual leave, bank holidays, birthday off, pension, gym membership discount, eye care, death in service cover, cycle to work scheme, season ticket loan, employee assistance program, personal development budget, alumni and community networks, brand ambassador opportunities, supportive recruitment process, online application, prompt application review, online assessments, competency interview, effective communication, behavioural competencies, collaborative spirit, growth mindset, talent team support, virtual interviews, nerves management strategies, nonverbal communication","adaptability, alumni and community networks, annual leave, bank holidays, behavioural competencies, big data, birthday off, brand ambassador opportunities, cloud services, collaboration, collaborative spirit, competency interview, composure, creativity, curiosity, cycle to work scheme, data availability, data debugging, data organization, data processing, data reliability, data storage, data testing, dataanalytics, datapipeline, death in service cover, diversity, effective communication, empathy, employee assistance program, eye care, goalorientation, growth mindset, gym membership discount, inclusion, integrity, nerves management strategies, nonverbal communication, online application, online assessments, openness to diverse ideas, pension, performance, personal development budget, positive relationships, prompt application review, python, scalability, season ticket loan, stakeholder engagement, structured data, supportive recruitment process, talent team support, target exceeding, teamfocused approach, unstructured data, virtual interviews, visualization"
Junior Data Engineer,Sparta Global,"Liverpool, England, United Kingdom",https://uk.linkedin.com/jobs/view/junior-data-engineer-at-sparta-global-3783939331,2023-12-17,Liverpool, United Kingdom,Mid senior,Hybrid,"About Sparta Global
Embark on a transformative career journey with Sparta Global, where diversity, innovation, and passion for technology converge. We welcome individuals from all backgrounds, offering not just jobs, but dynamic careers in the tech industry. You'll work alongside enthusiastic professionals, receiving top-tier training and mentorship to hone your skills for success in both public and private sectors. Our commitment to designing impactful careers and coaching future leaders is evident in our over 10 prestigious awards in Learning & Development and Equality, Diversity & Inclusion. As a Top 20 Employer for Social Mobility and a proud B-Corp certified organisation, we're not just advancing careers; we're fostering a more diverse, equitable, and inclusive tech landscape. Join us in shaping the future of technology - where your growth is our mission, and your success, our pride. Apply now to be part of our award-winning team at Sparta Global.?
About This Role
You'll become versatile in a wide array of tools across topics covering data focused coding, data visualisation, Cloud Services and Big Data.
You'll be designing, building, maintaining, and troubleshooting the data pipelines that enable organizations to store, process, and analyse their data, and ensuring the data is reliable through testing and debugging. Looking for efficiencies and optimising the data pipelines for scalability and performance will be a focus.
You'll be handling and working with large sets of structured and unstructured data and will be responsible for ensuring that the data is organized and available for data scientists and analysts to use.
Working with others is key, you could be working with other engineers, developers, data scientists, analysts and even stakeholders to understand their data needs.
We're not expecting you to have the proficiencies right away - that's where our award-winning Academy comes in. We are the experts in building skills and confidence in a fun and supportive environment that will not only challenge you but also develop your specialist capabilities ready to work on our clients' projects.
What we're looking for.
To be successful for this role you will demonstrate a level of ability in Python or similar. You will be passionate about technology and eager to learn programme development to an advanced level.
We're seeking candidates who can exemplify our values:??
Empathy and Diversity: Integrity, respect, and a commitment to inclusivity.??
Drive: A goal-oriented mindset with pride in exceeding targets.??
Collaboration: A team-focused approach, fostering positive relationships.??
Innovation: Curiosity, creativity, and openness to diverse ideas.??
Flexibility: Adaptability and composure in the face of change.
As a national organisation with clients across the UK, we require flexibility and a willingness to relocate post-remote training. Deployment locations vary and cannot be guaranteed. We encourage applications from diverse backgrounds and experience levels. Eligibility to work in the UK by the start of employment is mandatory.
Why you should apply:?
Our environment is designed to nurture your talents and skills, your hard work and progress are not just appreciated - they're tangibly rewarded. We conduct performance-based reviews every six months, offering you the chance to increase your earning potential twice a year. This regular appraisal system is our way of ensuring that your efforts and achievements are consistently recognised and rewarded.?
We also provide:??
20 days annual leave + bank holidays.??
An extra day off for your birthday.??
Pension.??
Discounted gym membership.??
Eye care.??
Death in service cover.??
Cycle to work scheme.??
Season ticket loan.??
Employee assistance program.??
Yearly budget for personal development.??
Access to alumni and community networks.??
Opportunities to be brand ambassadors.??
Being employed by Sparta Global is an investment in your future that pays dividends along the way. We give you breadth of experience and skills, along with increasing opportunities to develop further and earn more. No two career paths look the same at Sparta.??
Our Recruitment Process:?
Begin your journey via our supportive recruitment process. Apply online and our team will promptly review your application, contacting successful candidates within 48 hours to initiate the next steps. If you pass our initial screening, candidates will proceed to online assessments which vary depending on the opportunity you are applying for. The final stage is a competency interview, here you'll have the opportunity to impress us with your ability to communicate effectively and exhibit behavioural competencies through relevant examples. We're looking for candidates who can demonstrate a collaborative spirit and a growth mindset.??
Your dedicated Talent Team member will be with you every step of the way to support and answer any questions you have. You can also visit our YouTube channel to gain valuable insights and expert advice on virtual interviews, strategies to manage nerves, and tips on nonverbal communication.???
We look forward to receiving your application - good luck!???
Show more
Show less","Python, Data Visualization, Cloud Services, Data Pipelines, Data Analysis, Data Management, Machine Learning, Artificial Intelligence, Big Data, Software Development, Software Engineering, DevOps, Scrum, Agile Development, Software Testing, Software Debugging, Version Control, Git, Project Management, Teamwork, Communication, Problem Solving, Critical Thinking","python, data visualization, cloud services, data pipelines, data analysis, data management, machine learning, artificial intelligence, big data, software development, software engineering, devops, scrum, agile development, software testing, software debugging, version control, git, project management, teamwork, communication, problem solving, critical thinking","agile development, artificial intelligence, big data, cloud services, communication, critical thinking, data management, dataanalytics, datapipeline, devops, git, machine learning, problem solving, project management, python, scrum, software debugging, software development, software engineering, software testing, teamwork, version control, visualization"
Data Engineer,FORT,"Liverpool, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-fort-3778445057,2023-12-17,Liverpool, United Kingdom,Mid senior,Hybrid,"The Opportunity
Our client are looking to expand their platform from two territories to fifty within a few years. As a result their CEO is putting a priority focus on data as they look to scale their personalised service to a global audience.
HYBRID WORKING:
They are ideally looking for someone who could attend their offices in Liverpool weekly but the reality could only be once or twice a month.
The Why? (TOP 3 as we could put far more in there!)
Opportunity to work with AI/ML as the organisation look to scale and increase efficiency
Genuine growth opportunity as the organisations major focus has turned to data as they scale
Given it is early in the organisations data journey you will have the opportunity to make a significant impact on their major long term strategy
Experience Required
Background in AWS
Capable of working with Data Lakes
Good understanding of ETL tools
Any knowledge of BI tools such as PowerBI
And finally.. Who are we?
FORT - Future of Recruiting Technologists is a technical & IT search consultancy that specialises in placing experienced professionals & teams with leading technology companies in the North of England.
We strive to provide our candidates with the most transformative opportunities available. For our clients we understand that technical expertise alone does not make for the best employee. We take time to ensure a strong cultural and technical fit through deep understanding of our client's business and project needs. Ultimately, we pair technologists with businesses based on more than just skills.
Our values and standards are the bedrock of every interaction; Continual Development, Compassion and Consistency.
Show more
Show less","AWS, Data Lakes, ETL tools, PowerBI, AI/ML","aws, data lakes, etl tools, powerbi, aiml","aiml, aws, data lakes, etl tools, powerbi"
Data Engineer,BJSS,"Liverpool, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-bjss-3438256412,2023-12-17,Liverpool, United Kingdom,Mid senior,Hybrid,"About Us
We’re an award-winning innovative tech consultancy - a team of creative problem solvers. Since 1993 we’ve been finding better, more sustainable ways to solve complex technology problems for some of the world’s leading organisations and delivered solutions that millions of people use every day.
In the last 30 years we won several awards, including a prestigious Queen’s Award for Enterprise in the Innovation category for our Enterprise Agile delivery approach.
Operating from 26 locations across the world, we bring together teams of creative experts with diverse backgrounds and experiences, who enjoy working and learning in our collaborative and open culture and are committed to world-class delivery.
We want to continue to grow our team with people just like you!
About the Role
We are DataOps advocates and use software engineering best practices to build scalable and re-usable data solutions to help clients use their data to gain insights, drive decisions and deliver business value. Clients don’t engage BJSS to do the straightforward things, they ask us to help on their biggest challenges which means we get to work with a wide range of tools and technologies and there are always new things to learn.
BJSS data engineers are specialist software engineers that build, optimise and maintain data applications, systems and services. This role combines the discipline of software engineering with the knowledge and experience of building data solutions in order to deliver business value.
As a BJSS data engineer you’ll help our clients deploy data pipelines and processes in a production-safe manner, using the latest technologies and with a DataOps culture.
You’ll work in a fast moving, agile environment, within multi-disciplinary teams of highly skilled consultants, delivering modern data platforms into large organisations.
You can expect to get involved in variety of projects in the cloud (AWS, Azure, GCP), learning about and using data services such as Databricks, Data Factory, Synapse, Kafka, Redshift, Glue, Athena, BigQuery, S3, Cloud Data Fusion etc.
About You
You're an engineer at heart and enjoy the challenge of building reliable, efficient data applications systems, services and platforms.
You have a good understanding of coding best practices and design patterns and experience with code and data versioning, dependency management, code quality and optimisation, error handling, logging, monitoring, validation and alerting.
You have experience in writing well tested object-oriented python.
You have experience with using CI/CD tooling to analyse, build, test and deploy your code.
You have a good understanding of design choices for data storage and data processing, with a particular focus on cloud data services.
You have experience in using parallel computing to process large datasets and to optimise computationally intensive tasks.
You have experience in programmatically deploying, scheduling and monitoring components in a workflow.
You have experience in writing complex queries against relational and non-relational data stores.
Some of the Perks
Flexible benefits allowance – you choose how to spend your allowance (additional pension contributions, healthcare, dental and more)
Industry leading health and wellbeing plan - we partner with several wellbeing support functions to cater to each individual's need, including 24/7 GP services, mental health support, and other
Life Assurance (4 x annual salary)
25 days annual leave plus bank holidays
Hybrid working - Our roles are not fully remote as we take pride in the tight knit communities we have created at our local offices. But we offer plenty of flexibility and you can split your time between the office, client site and WFH
Discounts – we have preferred rates from dozens of retail, lifestyle, and utility brands
An industry-leading referral scheme with no limits on the number of referrals
Flexible holiday buy/sell option
Electric vehicle scheme
Training opportunities and incentives – we support professional certifications across engineering and non-engineering roles, including unlimited access to O’Reilly
Giving back – the ability to get involved nationally and regionally with partnerships to get people from diverse backgrounds into tech
You will become part of a squad with people from different areas within the business who will help you grow at BJSS
We have a busy social calendar that you can chose to join– quarterly town halls/squad nights out/weekends away with families included/office get togethers
GymFlex gym membership programme
Show more
Show less","DataOps, Software engineering practices, Data solutions, Cloud computing (AWS Azure GCP), Data services (Databricks Data Factory Synapse Kafka Redshift Glue Athena BigQuery S3 Cloud Data Fusion), Python, Coding best practices, Design patterns, Code versioning, Dependency management, Code quality and optimisation, Error handling, Logging, Monitoring, Validation, Alerting, CI/CD tooling, Data storage, Data processing, Parallel computing, Workflow management, Relational data stores, Nonrelational data stores","dataops, software engineering practices, data solutions, cloud computing aws azure gcp, data services databricks data factory synapse kafka redshift glue athena bigquery s3 cloud data fusion, python, coding best practices, design patterns, code versioning, dependency management, code quality and optimisation, error handling, logging, monitoring, validation, alerting, cicd tooling, data storage, data processing, parallel computing, workflow management, relational data stores, nonrelational data stores","alerting, cicd tooling, cloud computing aws azure gcp, code quality and optimisation, code versioning, coding best practices, data processing, data services databricks data factory synapse kafka redshift glue athena bigquery s3 cloud data fusion, data solutions, data storage, dataops, dependency management, design patterns, error handling, logging, monitoring, nonrelational data stores, parallel computing, python, relational data stores, software engineering practices, validation, workflow management"
Data Warehouse Engineer,Premier Group Recruitment,"Chester, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-warehouse-engineer-at-premier-group-recruitment-3778470767,2023-12-17,Liverpool, United Kingdom,Mid senior,Hybrid,"Data Warehouse Developer
£40,000
Cheshire
We’re looking for a Data Warehouse Developer to come onboard a financial services company and help to contribute to the administration, design and development of the Data Warehouse.
The Data Warehouse Developer will be experienced with working in a SAS environment and be comfortable with helping the newer platform users by promoting best practice and methodologies.
As well as the design and development the Developer will also be working within GDPR compliance and the data surrounding that.
Tech Stack:
SAS background, SAS Base and SAS Macros
ETL
Python
Process Automation
Package:
Annual Bonus
Pension scheme 5%
Income Protection
Service Awards
Life Assurance
Training and Exam Sponsorship
Please apply with your most up to date CV or email bbradley@ pg-rec.com for more information.
Data Warehouse Developer, Data Warehouse, ETL, Software as Service, SAS, SAS Base, SaS Macros, Python,
Show more
Show less","Data Warehouse, SAS, SAS Base, SAS Macros, ETL, Python, Process Automation","data warehouse, sas, sas base, sas macros, etl, python, process automation","datawarehouse, etl, process automation, python, sas, sas base, sas macros"
Data Warehouse Engineer,Searchability,"Chester, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-warehouse-engineer-at-searchability-3783622756,2023-12-17,Liverpool, United Kingdom,Mid senior,Hybrid,"Job Title: SAS Data Warehouse Engineer
Location: Chester
Salary: £40,000
Hybrid working capacity
About Us:
We are a leading financial services provider specializing in finance solutions for both the public and business clients. With a commitment to innovation and excellence, we are dedicated to shaping the future of financing our sector. As we continue to grow, we are seeking a talented and experienced Data Warehouse Developer with expertise in SAS to join our dynamic team.
Position Overview:
We are looking for a highly skilled and motivated individual to fill the role of Data Warehouse Developer. The successful candidate will play a crucial role in designing, developing, and maintaining our data warehouse, with a specific focus on utilizing SAS technologies. This is an exciting opportunity to contribute to the enhancement of our motor finance operations through advanced data analytics and reporting.
Key Responsibilities:
Collaborate with cross-functional teams to understand business requirements and translate them into effective data warehouse solutions.
Design, develop, and maintain data models, ETL processes, and data integration pipelines using SAS technologies.
Optimize and tune existing data warehouse processes for improved performance and efficiency.
Implement data quality and validation procedures to ensure the accuracy and reliability of the data.
Work closely with business analysts and stakeholders to gather and refine requirements for reporting and analytics.
Provide technical support and troubleshooting for data warehouse-related issues.
Qualifications:
Bachelor's degree in Computer Science, Information Technology, or a related field.
Proven experience as a Data Warehouse Developer with a focus on SAS technologies.
Strong proficiency in SAS programming and data manipulation.
Experience with data modeling, ETL processes, and data integration.
Knowledge of motor finance industry processes and data requirements is a plus.
Excellent problem-solving and analytical skills.
Strong communication and collaboration skills.
Show more
Show less","SAS, Data Warehousing, Data Engineering, ETL, Data Integration, Data Modeling, Data Quality, Data Validation, Business Intelligence, Data Analytics, Reporting, Motor Finance","sas, data warehousing, data engineering, etl, data integration, data modeling, data quality, data validation, business intelligence, data analytics, reporting, motor finance","business intelligence, data engineering, data integration, data quality, data validation, dataanalytics, datamodeling, datawarehouse, etl, motor finance, reporting, sas"
Lead Data Engineer,Sherborne Talent Solutions,"Chester, England, United Kingdom",https://uk.linkedin.com/jobs/view/lead-data-engineer-at-sherborne-talent-solutions-3776947065,2023-12-17,Liverpool, United Kingdom,Mid senior,Hybrid,"I am currently recruiting for multiple roles within the data space for my awesome client at senior and Lead data engineer level! The successful candidates will be individuals with robust commercial experience in a Senior/Lead Data Engineering role and well-versed in a similar tech stack. Your background should encompass mid-large sized organizations and be adept at handling extensive datasets.
Data is key to my clients continued success as they look to harness data engineering, analysis, science, machine learning, and interactive visualizations to extract insights, crafting superior products and fostering innovation.
These technologies extend to shaping our internal product strategy. Managing diverse technical platforms and back-office systems from acquisitions necessitates creating a unified view of the internal processes and customer lifecycle.
Key tech used within this area are:
Cassandra
Apache Spark
ActiveMQ
Python
Pandas
AWS (S3, Redshift, Lambda)
Tableau
Key aspects of the day-to-day role:
Crafting data pipelines for innovative products
Maintaining existing pipelines and jobs
Scaling the data platform to accommodate our expanding data footprint
Assessing new data sources and integrating them into our existing data lake
Applying machine learning for new data products and dashboards
Conducting purpose-driven data analysis to guide business decisions
This opportunity will see you working on an awesome team within a highly successful global organisation doing incredibly well in multiple specialist domains. Interested? Apply ASAP to avoid disappointment
Show more
Show less","Data Engineering, Machine Learning, Data Science, Data Analysis, Data Visualization, Cassandra, Apache Spark, ActiveMQ, Python, Pandas, AWS, S3, Redshift, Lambda, Tableau, Data Pipelines, Data Platform Scaling, Data Lake Integration, Machine Learning Applications, DataDriven Analysis","data engineering, machine learning, data science, data analysis, data visualization, cassandra, apache spark, activemq, python, pandas, aws, s3, redshift, lambda, tableau, data pipelines, data platform scaling, data lake integration, machine learning applications, datadriven analysis","activemq, apache spark, aws, cassandra, data engineering, data lake integration, data platform scaling, data science, dataanalytics, datadriven analysis, datapipeline, lambda, machine learning, machine learning applications, pandas, python, redshift, s3, tableau, visualization"
Senior Data Engineer,Convatec,"Deeside, Wales, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-at-convatec-3748465005,2023-12-17,Liverpool, United Kingdom,Mid senior,Hybrid,"Pioneering trusted medical solutions to improve the lives we touch: Convatec is a global medical products and technologies company, focused on solutions for the management of chronic conditions, with leading positions in advanced wound care, ostomy care, continence care, and infusion care. With around 10,000 colleagues, we provide our products and services in almost 100 countries, united by a promise to be forever caring. Our solutions provide a range of benefits, from infection prevention and protection of at-risk skin, to improved patient outcomes and reduced care costs. Group revenues in 2022 were over $2 billion. The company is a constituent of the FTSE 100 Index (LSE:CTEC). To learn more about Convatec, please visit http://www.convatecgroup.com
Job Summary
If you are looking to join a small, driven, and growing team where you can leverage your data engineering experience to build highly impactful digital solutions that impacts the lives of our patients and healthcare providers, then this is the position for you. You will be the senior data engineer within the team. You will be responsible for working within project teams and ensuring the solution architecture creates delightful experience for internal and external stakeholders.
Please apply to this opportunity with both CV & Cover letter.
Key Duties And Responsibilities
Design, build, and maintain cloud based ETL (extract, transform, load) processes, data pipelines, data warehouses, and data lakes to extract medical data (databases - patient data, images, or videos)
Lead architectural design of enterprise-grade data engineering platforms that meet compliance, regulatory, performance/scalability, and operational requirements.
Develop standards, procedures, techniques, and tools to accelerate data engineering projects.
Build dashboards to convey clinical insight to multi-disciplinary teams (clinical, marketing, engineering).
Effectively communicate with and influence key stakeholders across the enterprise, at all levels of the organization
Be part of a high-performing team developing and implementing AI algorithms to help address solve complex problems and create unique solutions in healthcare.
Partner closely with data scientists, and machine learning engineers, and other technical teams to optimize our delivery process for production grade ML/AI systems and dashboards.
Build awareness, increase knowledge, and drive adoption of data engineering technologies and architecture patterns, sharing customer and engineering benefits to gain buy-in (working closely with leaders, other SMEs, and engineers)
Maintain knowledge of advances of data engineering systems, techniques, open-source, and cloud offerings
Strong engineering fundamentals in software design, version control, continuous integration and delivery, testing, containers and orchestration, monitoring and logging for enterprise-grade machine learning platforms in real world environments.
Carryout architecture reviews and enabling teams fix issues that arise from the reviews.
Ability to effectively communicate, influence and drive consensus between the business, technology teams, and executive leadership in an organization with multiple lines of business.
Responsible for working in an Agile environment and evaluating pre-project ideas in terms of technical complexity and solution architecture. Participates in high-level estimation.
About You
Bachelor’s degree in computer science, Data Analytics, Software or Computer Engineering, Computational Statistics, Mathematics, or a closely related discipline. Master’s or above is highly preferred.
Experience working in a regulated industry, or the medical device space is highly preferred.
5+ years of experience in a data engineer role covering designing, building and maintaining ETL (extract, transform, load) processes, data pipelines, data warehouses, and data lakes.
Demonstrable experience in using cloud-based platform including AWS, Google cloud and Azure. Experience in building pipelines and data warehouses and lakes in AWS and Databricks is preferred. Use of Terraform to build cloud-based infrastructure is also desirable.
Demonstrable expertise using scripting and database languages such as: Python, SQL, and Scala
Experience building pipelines (ingestion and data lakes) for databases, images and videos
Experience in building dashboards to convey insight to multi-disciplinary teams (clinical, marketing, engineering). Tools may include Power BI, streamlit, Tableau or other BI tools.
Experience working with RESTful API and general event-driven architectures.
Understanding of DevOps and MLOps tool chains and processes, including ML model deployment. Proven expertise in data curation and data architecting to support machine learning projects. Has solutioned complex data processing and storage challenges through scalable, fault-tolerant architecture
Familiarity with tools and technologies in the data engineering ecosystem for Data Storage (S3, Hive, Cassandra, Lakehouse, etc.), Data Pipelines (ffmpeg, Kafka, Airflow, Prefect, etc.) and datalakes (Snowpipe, AWS, Databricks etc.)
Working Conditions
This position will be mainly remote; however, we will expect our successful candidate to travel for team meetings to either our Paddington or Deeside site (approx. once per month).
Travel
Position may involve travel up to 10% of the time, mostly within UK/US but overseas travel is expected. Most trips will include overnight travel.
Our transformation will change your career. For good. You'll be pushed to think bigger and aim for excellence. Your ideas will be heard, and you'll be supported to bring them to life. There'll be challenges. But stretch yourself and embrace the opportunities, and you could make your biggest impact yet.
This is stepping outside of your comfort zone.
This is work that'll
move
you.
Beware of scams online or from individuals claiming to represent Convatec
A formal recruitment process is required for all our opportunities prior to any offer of employment. This will include an interview confirmed by an official Convatec email address.
If you receive a suspicious approach over social media, text message, email or phone call about recruitment at Convatec, do not disclose any personal information or pay any fees whatsoever. If you’re unsure, please contact us at careers@Convatec.com.
Equal opportunities
Convatec provides equal employment opportunities for all current employees and applicants for employment. This policy means that no one will be discriminated against because of race, religion, creed, color, national origin, nationality, citizenship, ancestry, sex, age, marital status, physical or mental disability, affectional or sexual orientation, gender identity, military or veteran status, genetic predisposing characteristics or any other basis prohibited by law.
Notice to Agency and Search Firm Representatives
Convatec is not accepting unsolicited resumes from agencies and/or search firms for this job posting. Resumes submitted to any Convatec employee by a third party agency and/or search firm without a valid written and signed search agreement, will become the sole property of Convatec. No fee will be paid if a candidate is hired for this position as a result of an unsolicited agency or search firm referral. Thank you.
Already a Convatec employee?
If you are an active employee at Convatec, please do not apply here. Go to the Career Worklet on your Workday home page and View ""Convatec Internal Career Site - Find Jobs"". Thank you!
Show more
Show less","ETL, Data Warehousing, Data Lakes, Cloud Computing, AWS, Google Cloud, Azure, Databricks, Terraform, Python, SQL, Scala, Machine Learning, DevOps, MLOps, Data Storage, Data Pipelines, Datalakes, RESTful API, EventDriven Architectures","etl, data warehousing, data lakes, cloud computing, aws, google cloud, azure, databricks, terraform, python, sql, scala, machine learning, devops, mlops, data storage, data pipelines, datalakes, restful api, eventdriven architectures","aws, azure, cloud computing, data lakes, data storage, databricks, datalakes, datapipeline, datawarehouse, devops, etl, eventdriven architectures, google cloud, machine learning, mlops, python, restful api, scala, sql, terraform"
Data & Compliance Analyst,Strategic Resourcing,"Liverpool, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-compliance-analyst-at-strategic-resourcing-3784528458,2023-12-17,Liverpool, United Kingdom,Mid senior,Hybrid,"Data & Compliance Analyst
Industry: Construction
Job description
Project Specification
Data & Compliance Analyst role based in our Liverpool office, great environment and a business undergoing exciting period of growth. Salary negotiable (dependant on experience) and bonus scheme available.
Compliance Chain is a exciting all in one reporting solution available to the construction industry. Compliance Chain is also one of four companies existing within The Black Capital Group.
Role
We are presently seeking a Data & Compliance Analyst to join the dynamic, rapidly growing Compliance Chain team.
Activities include (but not limited to): Onboarding new clients, verifying documentation against our industry leading standard, internal/external support, relationship building, software demonstrations and support additional requests from head of operations.
The candidate must be a highly organised, thorough in their approach, problem solver, self-motivated and will ideally have both construction and administrative experience.
This really is a huge opportunity to be part of driving a company which is absolutely taking off as well as opportunities for growth and development.
Responsibilities
Monitor and analyse client profiles
Maintain clear and regular communication with clients
Review/assess the accuracy of compliance records submitted
Develop an understanding of the information's importance
Build a rapport with clients
Professional across all communication methods
The ability to take own initiative and prioritise workload
The ability to work efficiently to meet deadlines
The ability to work well within a team and individually
Perform other duties as required by the Operations Manager
Skills & Attributes Required
Construction Experience
Health and safety qualifications
Computer literate
Professionally presented and a confident/clear communicator
A positive and proactive approach to work
Demonstrate excellent communication skills at all levels
Flexible and adaptable to a rapidly changing workplace / environment
Benefits:
Company pension
Hybrid Working
Schedule:
Monday to Friday
Supplemental pay types:
Bonus scheme
Licence/Certification:
Health and safety qualification or experience (required)
Work Location: Liverpool, L3 9QJ
Show more
Show less","Data Analysis, Compliance, Communication, Problem Solving, SelfMotivation, Organization, Initiative, Prioritization, Teamwork, Health and Safety, Construction Experience, Computer Literacy, Proactive Approach, Adaptability, Microsoft Office Suite","data analysis, compliance, communication, problem solving, selfmotivation, organization, initiative, prioritization, teamwork, health and safety, construction experience, computer literacy, proactive approach, adaptability, microsoft office suite","adaptability, communication, compliance, computer literacy, construction experience, dataanalytics, health and safety, initiative, microsoft office suite, organization, prioritization, proactive approach, problem solving, selfmotivation, teamwork"
Data Scientist,Roche,"Mississauga, Ontario, Canada",https://ca.linkedin.com/jobs/view/data-scientist-at-roche-3779950449,2023-12-17,Ontario, Canada,Associate,Onsite,"The Position
As a Data Scientist you will join the data science cluster in the Roche Informatics Data and Analytics Chapter (DnA). You will be part of one or several multi-disciplinary agile teams where you’ll actively shape the future of healthcare by using data science methods and principles to generate deeper insights from a great variety of data sources. To achieve this, you will proactively identify needs, design and implement analytical solutions, provide advice and consulting support to our key stakeholders and show impact by executing proof-of-value initiatives, or contributing to existing products.
As a Data Scientist you will:
Be accountable for the development and implementation of Data Science products
Support proactive identification of the most relevant analytical use cases in collaboration with the key stakeholders
Support prioritisation efforts, understand feasibility and business impact, take smart risks to make informed decisions in a fast-paced, evolving environment to deliver patient benefits faster
Collaborate within global agile teams in the Roche Informatics business and foundational domains to develop products that provide the highest value to both Roche Pharma and Diagnostics business stakeholders.
Provide methodical and implementation guidance as well as hands-on support around analytical use cases
Communicate findings and market the value of use cases to key stakeholders
Contribute to positioning data science as a key competency within the enterprise
Continuously look for opportunities to broaden knowledge, capabilities and skill set to enable talent to flow into different specialties
Be a role model for knowledge sharing within the DnA chapter.
Act as a coach, mentor, or buddy to help colleagues grow and develop
Qualifications
M.Sc. or PhD in Computer Science, Physics, Statistics, Mathematics, Health/Medical Informatics or equivalent degree and experience with machine learning/data mining/artificial intelligence.
At least 3 years of hands-on experience as a Data Scientist
Proven experience to add value and insight by providing advanced analytical solutions
Hands-on experience with one or several fields of data mining, machine learning, statistics and prediction, image processing, natural language processing, deep learning, and other ML algorithms
Hands-on experience with RWD (EHR, claims, registries), particularly commercial datasets (e.g., IQVIA, Optum, Merative, etc.), RWD vocabularies/terminologies (e.g., ICD-9/10, RxNorm, LOINC, SNOMED, etc.) and crosswalks
Good knowledge of regulatory aspects regarding the validation of AI-based solutions in healthcare production processes and digital products
Experience with data and statistical languages/tools such as SQL, R, Python, etc
Data storytelling skills and using visualisation tools to communicate data and results with a non-technical audience
International, goal oriented mindset with can do attitude
Experience in the pharmaceutical/biotech industry is preferred.
Experience in working with cloud technologies and data science workbenches is preferred.
Fluency in written and spoken English
Roche is an equal opportunity employer.
Relocation benefits are not available for this position.
Who we are
At Roche, more than 100,000 people across 100 countries are pushing back the frontiers of healthcare. Working together, we’ve become one of the world’s leading research-focused healthcare groups. Our success is built on innovation, curiosity and diversity.
Roche Pharma Canada has its office in Mississauga, Ontario and employs over 850 employees. The Mississauga facility is bright, vibrant, fosters collaboration and teamwork, and is reflective of Roche's truly innovative culture.
As of January 4, 2022, Roche requires all new employees who work in Canada to be fully vaccinated against COVID-19 on the date they take office. This requirement is a condition of employment at Roche that applies regardless of whether the position is on a Roche campus or remotely. If you have a valid reason for not being fully immunized, which is limited to certain specific medical reasons or other valid reasons protected by applicable human rights laws, you may request an exemption and / or adaptation measures regarding this vaccination requirement.
Roche is an Equal Opportunity Employer.
Show more
Show less","Data Science, Machine Learning, Data Mining, Artificial Intelligence, Statistics, Prediction, Image Processing, Natural Language Processing, Deep Learning, RWD, EHR, Claims, Registries, Commercial Datasets, IQVIA, Optum, Merative, ICD9, ICD10, RxNorm, LOINC, SNOMED, SQL, R, Python, Data Storytelling, Visualisation Tools, Cloud Technologies, Data Science Workbenches","data science, machine learning, data mining, artificial intelligence, statistics, prediction, image processing, natural language processing, deep learning, rwd, ehr, claims, registries, commercial datasets, iqvia, optum, merative, icd9, icd10, rxnorm, loinc, snomed, sql, r, python, data storytelling, visualisation tools, cloud technologies, data science workbenches","artificial intelligence, claims, cloud technologies, commercial datasets, data mining, data science, data science workbenches, data storytelling, deep learning, ehr, icd10, icd9, image processing, iqvia, loinc, machine learning, merative, natural language processing, optum, prediction, python, r, registries, rwd, rxnorm, snomed, sql, statistics, visualisation tools"
Junior Product Information Management Data Analyst,UCS Forest Group,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/junior-product-information-management-data-analyst-at-ucs-forest-group-3774924688,2023-12-17,Ontario, Canada,Associate,Onsite,"What We Offer:
Comprehensive benefit package that includes health, dental, life and disability insurance, retirement savings plan and paid vacation. We also offer competitive pay and a sales incentive program. You will be provided with a company paid car and up to date technology. We have our own UCS university and we offer extensive training to ensure that you are equipped for success. We are a dynamic company and provide fast growing careers for great people. We have multiple locations, hire from within, and have the option for mobility.
About the Role:
The PIM (Product Information Management) Junior Analyst - Data is responsible for cleaning product data and contribute to the data modelling of PIM data and WoodPro data. The PIM Junior Analyst is a critical support of the product information management process, including the structuring, cleaning preparation, review import of product data prior to import. The Junior Analyst will constantly track and evaluate Product data to ensure that it is complete, accurate and updated in a timely manner. They will also interpret and present data in collaboration with their teammates, especially the PIM Analyst – Data, and PIM Administrator. They will also support other product data activities, as required.
The PIM Junior Analyst reports into the Director of Product Manager and is supervised by the PIM Analyst – Data.
Roles And Responsibilities
Work closely with the PIM Administrator, PIM Analyst — Data, ERP, DCX, and Product Teams to organize, structure and manage product data and asset readiness for PIM import templates.
Identify opportunities and develop assumptions related to data structures and variable relationships. With the expectation & ability to present discoveries with subject experts to validate recommendations.
Source, clean and format product data in alignment with the established conventions, product taxonomy and PIM import templates, including:
Work with Product Experts to develop, identify and finalize data dictionaries to migrate from legacy prompt system and values to new prompt system and values
QAdata cleanliness and completeness at the SKU level, prior to formatting data for import
Update import templates with clean product data for import, ensuring work is double-checked before submission to ensure accuracy.
Track and record all findings & duplicates during projects, and report any inconsistencies to the PIM Analyst- Data or the team.
Support the development and documentation of new product data conventions and data modelling, working with PIM Analyst — Data, Purchasers, Product Experts and the PIM team, as required
Support the PIM Analyst & PIM Administrator with data import file preparation as required
Create accurate and comprehensive test cases to support product testing
Demonstrate excellence in completion of all other duties as assigned
Requirements
An excellent project manager who delivers to quality outcomes to deadline;
Excellent written and verbal communication skills
Advanced MS Excel skills are a must, including extensive Power Query knowledge.
Highly-skilled at self-directed work and priority-setting
Passionate about data, process, workflows and governance
The ability to use raw data to create predictions, critique and draw conclusions is an asset.
Strong collaborator and structural thinker, who can effectively manage multiple perspectives and processes to achieve a successful collective result
Exceptional attention-to-detail is a must
Skilled in organizing and structuring large sets of unstructured data.
Passion for our products and learning; existing product knowledge is an asset
A quick learner who has an ability to grasp complex subjects, and leads with curiosity and likes to wrestle a problem to the ground
Training or experience in data management or processes
We are an equity opportunity employer that encourages Indigenous people, members of the 2SLGBTQ+, racialized, newcomer communities, people of all genders and abilities, and members of other under-represented communities to apply. If there is anything related to this job application process that needs to change to accommodate your accessibility, please reach out and we can discuss what might be possible.
Show more
Show less","PIM (Product Information Management), Data Cleaning, Data Modelling, Data Validation, Data Interpretation, Data Presentation, Data Structures, Data Relationships, Data Dictionaries, Data Migration, Data QA, Data Import, Data Duplication, Data Conventions, Data Governance, Data Analysis, Data Visualization, Data Prediction, Data Critique, Data Conclusions, Data Collaboration, Data Management, Data Processes, Data Organization, Data Structuring, Problem Solving, Project Management, MS Excel, Power Query, ERP, DCX","pim product information management, data cleaning, data modelling, data validation, data interpretation, data presentation, data structures, data relationships, data dictionaries, data migration, data qa, data import, data duplication, data conventions, data governance, data analysis, data visualization, data prediction, data critique, data conclusions, data collaboration, data management, data processes, data organization, data structuring, problem solving, project management, ms excel, power query, erp, dcx","data cleaning, data collaboration, data conclusions, data conventions, data critique, data dictionaries, data duplication, data governance, data import, data interpretation, data management, data migration, data modelling, data organization, data prediction, data presentation, data processes, data qa, data relationships, data structures, data structuring, data validation, dataanalytics, dcx, erp, ms excel, pim product information management, power query, problem solving, project management, visualization"
Senior/Staff Data Engineer,EvenUp,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/senior-staff-data-engineer-at-evenup-3728642810,2023-12-17,Ontario, Canada,Mid senior,Onsite,"EvenUp is a venture-backed generative AI startup that ensures injury victims are awarded the full value of their claims, expanding the $100B+ in awards granted to injury victims every year.
Every year, the legal system has made it difficult for millions of ordinary people to seek justice, especially for folks without means or who come from underrepresented backgrounds
. Our vision is to help these injury victims get the justice they deserve, irrespective of their income, demographics, or the quality of their legal representation.
EvenUp operates across all types of injury cases, from police brutality and child abuse to California wildfires and motor vehicle accidents. Our ML-driven software empowers attorneys to accurately assess the value of these cases by doing a core part of their workflow (legal drafting), enabling them to secure larger settlements in record time. As EvenUp evaluates more cases, our proprietary data grows, enhancing the precision of our predictions and delivering more value to both attorneys and victims alike.
As one of the fastest growing startups ($0 to $10M in ARR in <2 years), we raised $65M in investment from some of the best investors in the world (Bessemer, Bain Capital, Signalfire, DCM, NFX, Tribe Capital), seasoned tech executives (i.e. founder of Quora, SVP at Google, former CPO at Uber), and public figures that care about our social mission (Nas, Jared Leto, Byron Jones). Our team comes from top tech, legal, and investing backgrounds including Waymo, Google, Amazon, Uber, Quora, Blizzard, Norton Rose, Warburg Pincus, Bain, and McKinsey.
Why we are hiring a Senior/Staff Data Engineer now?
We have experienced unprecedented growth and need to scale out our data warehousing, data tooling and internal analytics. We need to architect the future of our data infrastructure at EvenUp and we’re seeking engineering leaders to help drive that vision.
We will need to 10x our pipeline processing throughput over the next 12 months. We’ll need to rethink and rebuild how we extract, process and model our ingestion to enable our organization with precise and actionable data.
We need to design & build data warehousing that democratizes data for our entire organization. We will invest in identifying and integrating tools and services that empower our teams to build on top of our data and analytics.
What you’ll do:
Democratize data at EvenUp. Ensure our organization can scale with consistent, standardized access to our data stores and accelerate our ability to build and experiment with data products
Architect and build out the future of data warehousing at EvenUp
Enable and empower our Data Science team to rapidly iterate on model experimentation
Design, organize and refine data storage strategies that reduce development friction for our tech organization
Collaborate with cross functional teams to solve critical data problems
Help grow our nascent Data Insights team and define a “data first” mentality across our organization
What we are seeking:
8+ years of data engineering experience
Previous experience building out data warehousing, data pipelines, and internal analytics
Strong understanding and practical experience with data tooling, BI tools, and systems such as DBT, BigQuery, Elasticsearch
The ability to communicate cross-functionally with various stakeholders to derive requirements and architect scalable solutions
Have several years of industry experience building high-quality software, shipping production-ready code and infrastructure
You enjoy owning a project from start to finish and love to drive a project across the finish line
Interest in making the world a fairer place (we don’t get paid unless we’re helping injured victims and/or their attorneys)
Nice to haves:
Have previously built out a Data Insights team at a data-oriented startup
Have previously planned and architected data migrations at scale
Have stood up analytics tooling to enable cross-functional teams
Domain expertise in legal technology, medical records, and working with unstructured data
A successful first year may look like:
75% doing system design and contributing code, starting with shipping code within 2 weeks!
25% collaborating with stakeholders and mentoring, lunch and learns, and more
Leverage a self-starter mindset by taking a product concept and building the feature end to end (whether it’s a component of the system or a significant piece of functionality).
Collaborate with the team to scale the tech stack based on our rapidly growing user base!
Benefits & Perks:
We seek to empower all of our team members to fulfill our mission of making the world a more just place, regardless of our team’s function, geography, or experience level. To that end, we offer:
Fully remote setup - work from wherever you feel is best (Plus a stipend to upgrade your home office!)
Flexible working hours to match your style
Offsites - get to meet your coworkers on a fully-expensed trip every 6-12 months!
Choice of great medical, dental, and vision insurance plan options
Flexible paid time off
A variety of virtual team events such as game nights & happy hours
EvenUp is an equal-opportunity employer. We are committed to diversity and inclusion in our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.
Show more
Show less","Data Warehousing, Data Pipelines, Internal Analytics, Data Tooling, BI Tools, DBT, BigQuery, Elasticsearch, CrossFunctional Communication, Software Development, ProductionReady Code, Infrastructure, Project Management, Data Insights Team, Data Migration, Analytics Tooling, Legal Technology, Medical Records, Unstructured Data, System Design, Collaboration, SelfStarter Mindset, Scaling Tech Stack","data warehousing, data pipelines, internal analytics, data tooling, bi tools, dbt, bigquery, elasticsearch, crossfunctional communication, software development, productionready code, infrastructure, project management, data insights team, data migration, analytics tooling, legal technology, medical records, unstructured data, system design, collaboration, selfstarter mindset, scaling tech stack","analytics tooling, bi tools, bigquery, collaboration, crossfunctional communication, data insights team, data migration, data tooling, datapipeline, datawarehouse, dbt, elasticsearch, infrastructure, internal analytics, legal technology, medical records, productionready code, project management, scaling tech stack, selfstarter mindset, software development, system design, unstructured data"
Data Security engineer (Endpoint + Data Loss Prevention),Quantum World Technologies Inc.,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/data-security-engineer-endpoint-%2B-data-loss-prevention-at-quantum-world-technologies-inc-3784835428,2023-12-17,Ontario, Canada,Mid senior,Onsite,"• Proven experience in implementing DLP (Data Loss Prevention) controls for Microsoft 365 SharePoint, OneDrive, Email, and Teams.
• Expertise in utilizing Intune and JAMF Protect for endpoint security.
• Hands-on experience with Microsoft Entra ID Password Protection Services for implementing password controls and identity management.
• Strong understanding of cybersecurity best practices and a proactive approach to identifying and mitigating security risks.
• Excellent communication and collaboration skills, with the ability to work effectively with cross-functional teams.
Show more
Show less","Data Loss Prevention (DLP), Microsoft 365, SharePoint, OneDrive, Email, Teams, Intune, JAMF Protect, Endpoint security, Microsoft Entra ID Password Protection Services, Password controls, Identity management, Cybersecurity best practices, Security risks, Communication skills, Collaboration skills, Crossfunctional teams","data loss prevention dlp, microsoft 365, sharepoint, onedrive, email, teams, intune, jamf protect, endpoint security, microsoft entra id password protection services, password controls, identity management, cybersecurity best practices, security risks, communication skills, collaboration skills, crossfunctional teams","collaboration skills, communication skills, crossfunctional teams, cybersecurity best practices, data loss prevention dlp, email, endpoint security, identity management, intune, jamf protect, microsoft 365, microsoft entra id password protection services, onedrive, password controls, security risks, sharepoint, teams"
Sr. Data Engineer [Ernst & Young],CareerBeacon,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/sr-data-engineer-ernst-young-at-careerbeacon-3751455153,2023-12-17,Ontario, Canada,Mid senior,Onsite,"At EY, you'll have the chance to build a career as unique as you are, with the global scale, support, inclusive culture and technology to become the best version of you. And we're counting on your unique voice and perspective to help EY become even better, too. Join us and build an exceptional experience for yourself, and a better working world for all.
The Opportunity
We are seeking a highly skilled and experienced Senior Data Engineer with demonstrated success delivering complex data integration requirements, to join our dynamic team. In this role, you will play a pivotal part in architecting, developing, and optimizing data integration solutions that drive our client's data-driven decisions. You will collaborate with cross-functional teams to design, implement, and maintain data pipelines that support our client's growing business needs. Strong client-facing and communication skills are essential, as you will engage with clients to understand their requirements and provide technical guidance.
Key Responsibilities
Data Integration and ETL Development
Design, develop, and maintain complex data integration solutions using third-party technologies including Informatica Intelligent Cloud Services (IICS) and Azure Data Factory (ADF)
Create ETL processes for data extraction, transformation, and loading (ETL) into target storage systems based on batch, micro-batch, or real-time ingestion needs
Architecture and Design
Collaborate with architects and data engineers to design scalable, efficient, and robust data pipelines based on technical best practices
Translate business requirements into technical solutions, ensuring data quality and integrity
Design and deployment of analytical data models
Client Engagement
Interact with clients to understand their data needs and provide technical guidance and solutions
Communicate effectively with non-technical stakeholders to ensure clear project requirements and objectives
Performance Optimization
Identify and resolve performance bottlenecks in data integration processes and recommend best practices for optimization
Data Quality and Governance
Implement data quality checks and data governance principles to maintain high data integrity
Monitor and resolve data issues promptly
Documentation
Create and maintain technical documentation for data integration processes, configurations, and best practices
Continuous Learning and Innovation
Stay updated on industry best practices and emerging technologies related to IICS, ADF, Snowflake and other relevant technologies
Recommend and implement innovative solutions to enhance data processes
Qualifications
Bachelor's degree in Computer Science, Information Systems, or a related field
Minimum of 10 years of experience in data integration and ETL development
Minimum 5 years of practical, hands-on project experience with Informatica Intelligent Cloud Services (IICS) and Azure Data Factory (ADF); Certifications with Informatica IICS, ADF considered an asset
Demonstrated experience with other ETL platforms considered an asset (e.g., Talend, DBT, DataStage)
Demonstrated experience/knowledge with ingesting data from a broad range of methods including files, APIs, event streaming (e.g., Kafka, Kinesis)
Demonstrated experience with design and implementation of various workloads on the Snowflake Data Cloud platform; SnowPro Core and SnowPro Advanced Architect certifications considered an asset
Proficiency in SQL, Python, or other scripting languages
Knowledge of data modeling, data warehousing, and ETL principles
Demonstrated experience designing and deploying various analytical data modeling methodologies (e.g., Star, Snowflake, Data Vault 2.0)
Strong problem-solving skills and attention to detail
Excellent communication, interpersonal and collaboration skills
Familiarity with cloud services such as Azure, AWS, or GCP
Experience with Agile and traditional SDLC delivery methodologies
Experience working independently, efficiently, and effectively under tight timelines and delivering results by critical deadlines
What We Offer
At EY, our Total Rewards package supports our commitment to creating a leading people culture - built on high-performance teaming - where everyone can achieve their potential and contribute to building a better working world for our people, our clients and our communities. It's one of the many reasons we repeatedly win awards for being a great place to work.
Benefits
We offer a competitive compensation package where you'll be rewarded based on your performance and recognized for the value you bring to our business. In addition, our Total Rewards package allows you decide which benefits are right for you and which ones help you create a solid foundation for your future. Our Total Rewards package includes a comprehensive medical, prescription drug and dental coverage, a defined contribution pension plan, a great vacation policy plus firm paid days that allow you to enjoy longer long weekends throughout the year, statutory holidays and paid personal days (based on province of residence), and a range of exciting programs and benefits designed to support your physical, financial and social well-being. Plus, we offer:
Support and coaching from some of the most engaging colleagues in the industry
Learning opportunities to develop new skills and progress your career
The freedom and flexibility to handle your role in a way that's right for you
Diversity and Inclusion at EY
Diversity and inclusiveness are at the heart of who we are and how we work. We're committed to fostering an environment where differences are valued, policies and practices are equitable, and our people feel a sense of belonging. We embrace diversity and are committed to combating systemic racism, advocating for the 2SLGBT+ community, promoting our Neurodiversity Centre of Excellence and Accessibility initiatives, and are dedicated to amplifying the voices of Indigenous people (First Nations, Inuit, and Métis) nationally as we strive towards reconciliation. Our diverse experiences, abilities, backgrounds, and perspectives make our people unique and help guide us. Because when people feel free to be their authentic selves at work, they bring their best and are empowered to build a better working world.
EY | Building a better working world
EY exists to build a better working world, helping to create long-term value for clients, people and society and build trust in the capital markets. Enabled by data and technology, diverse EY teams in over 150 countries provide trust through assurance and help clients grow, transform and operate. Working across assurance, consulting, law, strategy, tax and transactions, EY teams ask better questions to find new answers for the complex issues facing our world today.
Show more
Show less","Data Integration, ETL Development, Informatica Intelligent Cloud Services (IICS), Azure Data Factory (ADF), Data Extraction, Data Transformation, Data Loading, Data Pipelines, Data Quality, Data Governance, Data Modeling, Data Warehousing, SQL, Python, Scripting Languages, Star Schema, Snowflake Data Cloud, SnowPro Core, SnowPro Advanced Architect, Agile, SDLC, Cloud Services, AWS, GCP","data integration, etl development, informatica intelligent cloud services iics, azure data factory adf, data extraction, data transformation, data loading, data pipelines, data quality, data governance, data modeling, data warehousing, sql, python, scripting languages, star schema, snowflake data cloud, snowpro core, snowpro advanced architect, agile, sdlc, cloud services, aws, gcp","agile, aws, azure data factory adf, cloud services, data extraction, data governance, data integration, data loading, data quality, data transformation, datamodeling, datapipeline, datawarehouse, etl development, gcp, informatica intelligent cloud services iics, python, scripting languages, sdlc, snowflake data cloud, snowpro advanced architect, snowpro core, sql, star schema"
Azure Databricks Engineer,J&M Group,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/azure-databricks-engineer-at-j-m-group-3768025636,2023-12-17,Ontario, Canada,Mid senior,Onsite,""" Bachelor's degree in Computer Science, Engineering, IT, or other scientific or quantitative fields
"" 10+ years of experience in a Data Engineer or equivalent role in enterprise initiatives
"" 4+ years of experience with Azure: ADF, ADLS Gen 2, Azure Synapse, Databricks or equivalent technologies,
"" 3+ years of experience in working with SQL databases, Transaction replication and Change data capture technologies
"" Experience building processes to support data transformation, data structures, metadata, dependencies, and workload management
"" Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement
"" Experience with Data Ops , DevSecOps, CI/CD workflows
"" Strong communication, interpersonal, and collaboration skills with both business and technical resources
"" Experience supporting and working with cross-functional teams in a dynamic environment
"" Demonstrated team player with proactive, can-do attitude with a demonstrated ability to handle multiple priorities
"" Able to support a diverse and inclusive work environment
Show more
Show less","Data Engineering, Computer Science, Engineering, IT, Azure, ADF, ADLS Gen 2, Azure Synapse, Databricks, SQL, Transaction replication, Change data capture, Data Ops, DevSecOps, CI/CD","data engineering, computer science, engineering, it, azure, adf, adls gen 2, azure synapse, databricks, sql, transaction replication, change data capture, data ops, devsecops, cicd","adf, adls gen 2, azure, azure synapse, change data capture, cicd, computer science, data engineering, data ops, databricks, devsecops, engineering, it, sql, transaction replication"
Data Engineer III [TD Bank],CareerBeacon,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/data-engineer-iii-td-bank-at-careerbeacon-3774748529,2023-12-17,Ontario, Canada,Mid senior,Onsite,"Hours37.5Workplace ModelHybridPay Details
We're committed to providing fair and equitable compensation to all our colleagues. As a candidate, we encourage you to have an open dialogue with your recruiter and ask compensation related questions, including pay details for this role.
Job DetailsYou will be part of a team responsible for developing, enhancing, and supporting all technical solutions.
In This Role You Will
Write and support complex and original software code
Participate in all aspects of product development and rollout, including design, development, debugging implementation, documentation and support
Assist developing specification, program, and documentation standards; assist maintaining functional operation of assigned production systems
Embrace challenges and learn new skills; be a catalyst for change
Gain knowledge of the business side of finance and risk management
Learn to write and maintain system design documents
Communicate status with project managers
Learn to estimate task effort and timeframes
Perform other tasks as assigned
Job Requirements
Undergraduate Degree or Technical Certificate.
Must have work experience (7+ years):
Azure platform / Big Data technologies
Data Flow Processes
SQL Development
ETL
Preferred experience in:
Python, Pyspark, Spark, Sqoop, Hive
Azure platform and tools like Azure Data Factory, Azure Databricks, Synapse
Working experience with data modeling, relational modeling and dimensional modeling
Working knowledge of source code control tool such as GIT
Experience in implementing data management/data catalog, DQ tools, REST APIs is a plus
Implementation experience in managing and working in multiple environments, release and change management and knowledge of firewall, network work protocols, file transfer – TIBCO
Familiar with Agile development methodologies
Sound to advanced knowledge of business, standards, infrastructure, architecture and technology from a design/support/ solutions perspective
Readiness and motivation (as an experienced developer and subject matter expert) to address and resolve complex issues, guide/advise/support clients, partners and project teams, often working on multiple medium-to-large sized projects.
Company Overview
Our Values
At TD we're guided by our purpose is to enrich the lives of our customers, communities and colleagues, and share a set of values that shape our culture and guide our behavior. In exchange for how our colleagues show up to help TD succeed, we are committed to delivering a colleague experience grounded in Impact, Growth and a Culture of Care. No matter where you work across TD, we empower you to make an impact at work and in your community, explore and grow your career and be part of our caring and inclusive culture.
Making Your Well-being a Priority
A supportive culture that promotes colleague well-being is core to who we are. At TD, we focus on total well-being with extensive programs to help colleagues assess, manage, and improve their well-being across four core pillars — physical, financial, social and mental/emotional. In addition, we champion a safe and inclusive work environment so colleagues feel a sense of belonging and feel supported in their personal and professional growth. Through our focus on well-being, we know that we can help our colleagues thrive, contribute to our culture of care, and support better business outcomes, because when colleagues feel their best, they're more likely to do their best.
Our Total Rewards Package
Our Total Rewards package reflects the investment we make in our colleagues to help them, and their families achieve their well-being goals. Total Rewards at TD includes a base salary, variable compensation, and several other key plans such as health and well-being benefits including medical, dental, vision & mental health coverage, savings and retirement programs, paid time off, banking benefits and discounts, career development, and reward and recognition programs.
How We Work
At TD, we believe in-person connections fuel collaboration and collective creativity. Our workplace experience empowers colleagues to do great work side-by-side at TD locations, while offering flexibility to work remotely where it makes sense for the work and team. Our teams work in one of three workplace models: Hybrid, Onsite and Primarily Remote. Wherever our colleagues are working, they'll always have access to the TD community and experience our culture of care.
Who We Are
TD is one of the world's leading global financial institutions and is the fifth largest bank in North America by branches. Every day, we deliver legendary customer experiences to over 27 million households and businesses in Canada, the United States and around the world. More than 95,000 TD colleagues bring their skills, talent, and creativity to the Bank, those we serve, and the economies we support. We are guided by our vision to Be the Better Bank and our purpose to enrich the lives of our customers, communities and colleagues.
TD is deeply committed to being a leader in customer experience, that is why we believe that all colleagues, no matter where they work, are customer facing. As we build our business and deliver on our strategy, we are innovating to enhance the customer experience and build capabilities to shape the future of banking. Whether you've got years of banking experience or are just starting your career in financial services, we can help you realize your potential. Through regular leadership and development conversations to mentorship and training programs, we're here to support you towards your goals. As an organization, we keep growing – and so will you.
Inclusiveness
Our Commitment to Diversity, Equity, and Inclusion
At TD, we're committed to fostering an environment where all colleagues are encouraged to bring their authentic selves to work, experience equitable opportunities, and feel respected and supported. We're dedicated to building an inclusive workforce that reflects the diversity of the customers and the communities in which we live and serve.
Accommodation
Your accessibility is important to us. Please let us know if you'd like accommodations (including accessible meeting rooms, captioning for virtual interviews, etc.) to help us remove barriers so that you can participate throughout the interview process.
How We're Helping Make An Impact In Communities
TD has a long-standing commitment to help drive progress towards a more inclusive and sustainable future. That's why we launched the TD Ready Commitment in 2018, now a multi-year North American initiative. Under the TD Ready Commitment, we are targeting a total of $1 billion by 2030 in community giving four key, interconnected drivers of change: Financial Security, Vibrant Planet, Connected Communities, and Better Health. It's our goal to help support change, nurture progress, and contribute to making the world a better, more inclusive place for our customers, colleagues, and communities.
We look forward to hearing from you!
Show more
Show less","Azure, Big Data technologies, Data Flow Processes, SQL, ETL, Python, Pyspark, Spark, Sqoop, Hive, Synapse, Data Modeling, Relational Modeling, Dimensional Modeling, GIT, Data Management, Data Catalog, DQ tools, REST APIs, Agile","azure, big data technologies, data flow processes, sql, etl, python, pyspark, spark, sqoop, hive, synapse, data modeling, relational modeling, dimensional modeling, git, data management, data catalog, dq tools, rest apis, agile","agile, azure, big data technologies, data catalog, data flow processes, data management, datamodeling, dimensional modeling, dq tools, etl, git, hive, python, relational modeling, rest apis, spark, sql, sqoop, synapse"
P2- Sr Data Engineer- WECJP00027488,Randstad Canada,"Mississauga, Ontario, Canada",https://ca.linkedin.com/jobs/view/p2-sr-data-engineer-wecjp00027488-at-randstad-canada-3765636826,2023-12-17,Ontario, Canada,Mid senior,Onsite,"Are you a Senior Data Engineer looking for a new opportunity?Are you looking for a new contract opportunity?
We are pleased to offer you a new contract opportunity for you to consider: Senior Data Engineer
Start: ASAP
Estimated length: 12 months
Location: Mississauga
Hybrid 50% (Wednesday and Thursday in office)
Advantages
You will have an opportunity to work with a leading employer in the local market.
Responsibilities
Design and implement GCP based data solutions to meet business requirements
Develop and maintain data pipelines, data models, and data warehouses
Optimize data processing and data storage to improve system performance and reliability
Develop ETL processes to ingest, transform, and load data from various sources into GCP
Implement data security and governance best practices
Troubleshoot and resolve data related issues
Qualifications
Mandatory:
Bachelor's degree in Computer Science, Engineering, or related field
At least 3 years of experience in data engineering with a focus on GCP
Strong experience in Spark/Scala
Expertise in GCP services such as BigQuery, Cloud Storage, Dataflow, Pub/Sub, DataProc
Strongly preferred:
Strong programming skills in Python or Java
Experience with SQL and NoSQL databases
Experience with version control tools such as Git
Strong problem solving and analytical skills
Strong communication and collaboration skills
Preferred Qualifications:
Experience with containerization and orchestration tools such as Kubernetes
Summary
Do you have this experience? If you answer YES, then please apply IMMEDIATELY to so we can then discuss your experience and interest in this opportunity!
Randstad Technologies Group
Canada's largest provider of IT Staffing Solutions, offering hundreds of permanent and contract opportunities across all roles, levels and platforms. Our Web based tools help you see and apply for jobs matched automatically to your skills and preferences. When you're ready to interview we meet with you in person to help you build the technology career path you've always wanted. Visit www.randstad.ca to get started!
Randstad Canada is committed to fostering a workforce reflective of all peoples of Canada. As a result, we are committed to developing and implementing strategies to increase the equity, diversity and inclusion within the workplace by examining our internal policies, practices, and systems throughout the entire lifecycle of our workforce, including its recruitment, retention and advancement for all employees. In addition to our deep commitment to respecting human rights, we are dedicated to positive actions to affect change to ensure everyone has full participation in the workforce free from any barriers, systemic or otherwise, especially equity-seeking groups who are usually underrepresented in Canada's workforce, including those who identify as women or non-binary/gender non-conforming; Indigenous or Aboriginal Peoples; persons with disabilities (visible or invisible) and; members of visible minorities, racialized groups and the LGBTQ2+ community.
Randstad Canada is committed to creating and maintaining an inclusive and accessible workplace for all its candidates and employees by supporting their accessibility and accommodation needs throughout the employment lifecycle. We ask that all job applications please identify any accommodation requirements by sending an email to accessibility@randstad.ca to ensure their ability to fully participate in the interview process.
Show more
Show less","Data Engineering, GCP, Spark, Scala, BigQuery, Cloud Storage, Dataflow, Pub/Sub, DataProc, Python, Java, SQL, NoSQL, Git, Kubernetes","data engineering, gcp, spark, scala, bigquery, cloud storage, dataflow, pubsub, dataproc, python, java, sql, nosql, git, kubernetes","bigquery, cloud storage, data engineering, dataflow, dataproc, gcp, git, java, kubernetes, nosql, pubsub, python, scala, spark, sql"
"Senior Data Scientist (Optimization + AWS), Canada",Tiger Analytics,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/senior-data-scientist-optimization-%2B-aws-canada-at-tiger-analytics-3771274256,2023-12-17,Ontario, Canada,Mid senior,Remote,"Tiger Analytics is looking for experienced Data Scientists to join our fast-growing advanced analytics consulting firm. Our employees bring deep expertise in Data Science, Machine Learning, and AI. We are the trusted analytics partner for multiple Fortune 500 companies, enabling them to generate business value from data. Our business value and leadership has been recognized by various market research firms, including Forrester and Gartner. We are looking for top-notch talent as we continue to build the best global analytics consulting team in the world.
We are looking for a Senior Data Scientist with robust optimization experience (in particular, integer programming, mixed integer programming), AWS experience, and strong coding capabilities, preferably from Ad-Tech/Media-Tech domains. This person will be responsible for writing complex production-level codes.
Key Responsibilities
Effectively communicate the analytics approach and how it will meet and address objectives to business partners
Advocate and educate on the value of data-driven decision-making; focus on the “how and why” of solutions
Lead analytic approaches; integrate solutions collaboratively into applications and tools with data engineers, business leads, analysts, and developers
Create repeatable, interpretable, dynamic, and scalable models seamlessly incorporated into analytic data products
Collaborate, coach, and learn with a growing team of experienced Data Scientists
Stay connected with external sources of ideas through conferences and community engagements
Support demands from regulators, investor relations, etc., to develop innovative solutions to meet objectives utilizing cutting-edge techniques and tools.
Requirements
Ph.D. (highly preferred) or Master’s in Computer Science, Statistics, Economics, Data Science, or a related field
At least 10+ years of extensive Data Science experience with strong Optimization experience, AWS experience, and capabilities
Robust Optimization experience, including Combinatorial Optimization, Integer programming, Mixed Integer Programming, Greedy Heuristics, Commercial and Open Source Solvers, Time Series and Forecasting Techniques, Cardinality Algorithms, and Retrieval Augmented Generation, is a must.
Strong statistics foundation and knowledge of statistical packages. Highly proficient with Python, Python + Scala, and SQL coding skills. This person will be responsible for writing complex production-level codes
Hands-on experience building time-series / forecasting models is a must.
Ad-Tech / Media-Tech industry experience is highly preferred
Exceptional communication and collaboration skills to understand business partner needs and deliver solutions
Benefits
This position offers an excellent opportunity for significant career development in a fast-growing and challenging entrepreneurial environment with a high degree of individual responsibility.
Show more
Show less","Data Science, Machine Learning, AI, Optimization, Integer Programming, Mixed Integer Programming, AWS, Coding, Python, Scala, SQL, TimeSeries Analysis, Forecasting, Statistics, Statistical Packages, AdTech, MediaTech, Combinatorial Optimization, Greedy Heuristics, Commercial Solvers, Open Source Solvers, Cardinality Algorithms, Retrieval Augmented Generation","data science, machine learning, ai, optimization, integer programming, mixed integer programming, aws, coding, python, scala, sql, timeseries analysis, forecasting, statistics, statistical packages, adtech, mediatech, combinatorial optimization, greedy heuristics, commercial solvers, open source solvers, cardinality algorithms, retrieval augmented generation","adtech, ai, aws, cardinality algorithms, coding, combinatorial optimization, commercial solvers, data science, forecasting, greedy heuristics, integer programming, machine learning, mediatech, mixed integer programming, open source solvers, optimization, python, retrieval augmented generation, scala, sql, statistical packages, statistics, timeseries analysis"
Part-Time Data Analyst (Entry Level),Mbstaffingservicesllc,"Windsor, Ontario, Canada",https://ca.linkedin.com/jobs/view/part-time-data-analyst-entry-level-at-mbstaffingservicesllc-3729292446,2023-12-17,Ontario, Canada,Mid senior,Remote,"Minimum 1 year of work experience - fully remote position. Freshers are also encouraged to apply.
About us: The Future of AI is Patterned We are a stealth-mode technology startup that is revolutionizing the way AI is used. Our platform uses pattern recognition to train AI models that are more accurate, efficient, and robust than ever before.
We are backed by top investors and we are hiring for almost everything! If you are passionate about AI and want to be a part of something big, then we want to hear from you.
Make a positive impact on the world. Be a part of a fast-growing startup. If you are interested in learning more, please visit our website.
We Are Looking For People Who Are
Passionate about AI.
Excellent problem solvers.
Team players.
Driven to succeed.
Requirements
Role Responsibilities:
Work in close collaboration with the Business Intelligence Lead, Federal Data Lead, and other Program teams
Develop, maintain, and improve BI tools, build and enhance standard operating procedures (SOPs)
Manage various data sets and active Google workbooks with adjacent contract teams, monitor and analyze financial health information at the project and program levels
Communicate with client leadership to assess data needs and emerging requirements
Work with large data sets, workbooks, and spreadsheets to manipulate and manage program-level information using macros, queries, scripts, etc.
Gather requirements and lead the development of long-term data management tools, processes, and solutions based on organizational needs.
Be comfortable working with collaboration tools such as; Google Suite, Microsoft Office
Providing general support to the client including, but not limited to, analysis, data calls, financial management, risk management, audits, and project management-related tasks.
Qualifications
Bachelor's Degree in business, business intelligence, data or information management, or similar.
Proficient in Google Scripts
Minimum 1 year of data or information management and/or data analysis experience.
Experience using Microsoft Excel and Google Sheets (macros, imports, query functions).
Experience with developing Google App Script is a plus.
Experience using SQL Developer is a plus.
Excellent written and verbal communication skills.
Willing to work in an administratively manual environment while working towards automation of processes in the future.
Clearable (able to pass both a criminal background check and credit check).
Highly motivated, self-learner, and technically inquisitive
Benefits
Special Benefits you will love:
Flexible vacation paid unlimited holidays and paid sick days
401(k) with up to 2% employer match
Health, vision, and dental insurance
Why Patterned Learning AI?
Patterned Learning AI is made up of incredibly bright, mission-driven coworkers who are passionate about using technology to solve real-world problems---and we're growing quickly. In order to continue building an engaging and dynamic organization, we're committed to giving everyone the support they need to do great work.
We believe diverse perspectives and backgrounds are critical to building great technology, and our goal is to cultivate an environment where people feel equally valued and respected. Patterned Learning AI is proud to be an equal opportunity workplace, and we welcome applicants from all backgrounds regardless of race, color, ancestry, religion, gender identity or expression, sexual orientation, marital status, age, citizenship, socioeconomic status, disability, or veteran status.
Powered by Webbtree
Show more
Show less","Data Management, Data Analysis, Business Intelligence, Google Scripts, Microsoft Excel, Google Sheets, SQL Developer, Google App Script, Macros, Queries, Scripts, Spreadsheets, Collaboration Tools, Google Suite, Microsoft Office","data management, data analysis, business intelligence, google scripts, microsoft excel, google sheets, sql developer, google app script, macros, queries, scripts, spreadsheets, collaboration tools, google suite, microsoft office","business intelligence, collaboration tools, data management, dataanalytics, google app script, google scripts, google sheets, google suite, macros, microsoft excel, microsoft office, queries, scripts, spreadsheets, sql developer"
Senior Data Scientist (Customer Analytics),Tiger Analytics,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/senior-data-scientist-customer-analytics-at-tiger-analytics-3775699079,2023-12-17,Ontario, Canada,Mid senior,Remote,"Tiger Analytics is pioneering what AI and analytics can do to solve some of the toughest problems faced by organizations globally. We develop bespoke solutions powered by data and technology for several Fortune 100 companies. We have offices in multiple cities across the US, UK, India, and Singapore, and a substantial remote global workforce.
We are also market leaders in AI and analytics consulting in the CPG & retail industry with over 40% of our revenues coming from the sector. This is our fastest-growing sector, and we are beefing up our talent in the space.
We are looking for a Senior Data Scientist with a good blend of data analytics background, who holds solid knowledge of Personalization, and Web/Mobile Analytics, and Customer Analytics, quick learner, and has strong coding capabilities to add to our team.
Key Responsibilities
Work on the latest applications of data science to solve business problems in the Customer Analytics space of Retail, in particular omnichannel retailing
Effectively communicate the analytics approach and how it will meet and address objectives to business partners
Lead data analytic and modeling approaches; integrate solutions collaboratively into applications and tools with data engineers, business leads, analysts, and developers
Create repeatable, interpretable, dynamic, and scalable models seamlessly incorporated into analytic data products
Collaborate, coach, and learn with a growing team of experienced Data Scientists
Stay connected with external sources of ideas through conferences and community engagements
Support demands from regulators, investor relations, etc., to develop innovative solutions to meet objectives utilizing cutting-edge techniques and tools
Requirements
>5 years of Data Science experience required
Graduate Degree in Data Science, Computer Science, or a related field is required
Deep Knowledge and Understanding of Personalization, Web/Mobile Analytics, and Customer Analytics
Strong python coding with production experience is preferred, MLops knowledge and experience is plus
At least 3 years of experience in CPG and Retail space
Ability to apply various analytical models to business use cases
Exceptional communication and collaboration skills to understand business partner needs and deliver solutions
Bias for action, with the ability to deliver outstanding results through task prioritization and time management
Benefits
This position offers an excellent opportunity for significant career development in a fast-growing and challenging entrepreneurial environment with a high degree of individual responsibility.
Show more
Show less","Data Science, Data Analytics, Personalization, Web Analytics, Mobile Analytics, Customer Analytics, Python, Machine Learning (ML), MLops, CPG, Retail, Analytical Modeling, Communication, Collaboration, Task Prioritization, Time Management","data science, data analytics, personalization, web analytics, mobile analytics, customer analytics, python, machine learning ml, mlops, cpg, retail, analytical modeling, communication, collaboration, task prioritization, time management","analytical modeling, collaboration, communication, cpg, customer analytics, data science, dataanalytics, machine learning ml, mlops, mobile analytics, personalization, python, retail, task prioritization, time management, web analytics"
Sr. Database Engineer / Analyst (IAM) - (Nearshore: Canada),"Gardner Resources Consulting, LLC","Ontario, Canada",https://ca.linkedin.com/jobs/view/sr-database-engineer-analyst-iam-nearshore-canada-at-gardner-resources-consulting-llc-3757805753,2023-12-17,Ontario, Canada,Mid senior,Remote,"Sr. Data Analyst / Developer
(IAM)
MUST Have
Experience with Radiant Logic HDAP, ICS, and FID (formerly aka VDS).
Well versed with MySQL database queries and creation of database views.
Development experience with REST, SOAP, LDAP, MySQL
Development experience with Java
Intangibles: Self-starter who is excellent with managing external relationships and communications (aka following up) with technical contacts
Show more
Show less","Radiant Logic HDAP, ICS, FID, MySQL, REST, SOAP, LDAP, Java","radiant logic hdap, ics, fid, mysql, rest, soap, ldap, java","fid, ics, java, ldap, mysql, radiant logic hdap, rest, soap"
Manager/Lead Data Engineer,Badal.io,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/manager-lead-data-engineer-at-badal-io-3746814297,2023-12-17,Ontario, Canada,Mid senior,Remote,"About Badal.io
We are a boutique, rapidly growing, GCP (Google Cloud Platform) consulting company based out of Toronto. We work with GCP’s top customers (banking, telco, energy, retail, etc.) to help them with cloud transformation, security, analytics, ML, and data governance. Clients usually engage us to solve their most challenging business problems and help raise the bar at their organization.
Why Badal?
You get the best of both worlds. We operate like an early-stage startup with all the associated benefits (e.g., talent, growth, learning opportunities, flexibility), but get to solve enterprise-level technical challenges.
People: We hire top-tier talent. Our team consists of ex-Googlers, YCombinator Alumni, and individuals that built software for 400M users.
GCP is the Best Cloud: Maybe we are biased, but GCP is the most cutting-edge cloud provider, building on technologies that have been pioneered by Google: BigQuery, K8s/Anthos, Vertex AI, etc.
Growth: We have doubled in size in the last 6 months, and are looking to double again this year.
You will
Manage a team who works closely with backend engineers, product managers, and analysts
Provide leadership and contribute to the definition, development, integration, test, documentation, and support across multiple platforms
Serve as a process expert and provide architectural leadership to Jr engineers as needed
Work closely with technical leads and client teams to fully demonstrate the benefits of GCP technology
Introduce clients to data architecture and analytics best practices
Solve some of the most challenging, high-scale data, and IoT problems (banking, telco, energy, retail, etc.).
Write high-quality and testable batch and real-time data pipelines
Work with Data Scientists to design pipelines that improve their productivity and enable them to implement their ideas
Support clients in troubleshooting issues in their test and/or production environments and identifying root causes and solutions
You have
Mentoring and helping the team build technical capability
Ability to effectively communicate data insights and negotiate options at senior management levels
Experience in leading significant project steps and communicating progress/approach with technical/non-technical peers/clients and leaders
GCP experience ( for junior candidates, or exceptional talent with other cloud experience, we will provide training programs)
Experience in large-scale, secure, and high-availability solutions in a Cloud environment, such as GCP.
Experience with one Dataflow or Spark
Experience with orchestration frameworks such as Airflow, Kubeflow, Azkaban, etc.
Extensive programming experience in Python, Java and/or Scala.
Good understanding of modern data architecture
Good understanding of GCP services such as IAM, and Google Cloud Storage buckets
Experience with business intelligence tools like QuickSight, Looker and Data Studio
Technical writing experience; preparing and presenting technical material to a variety of audiences.
Experience in working in or with, Agile delivery teams.
Nice to have
Streaming experience with Dataflow, Spark, Flink, Kafka Streams, etc
Strong understanding of Data Governance principles and experience working with tools such as Collibra or Immuta
Type of qualities we look for (across all roles)
Passionate about delivering high-quality commercial software products and platforms to market
Team player. We are a small team and enjoy working with each other and our clients - we would like to keep it the same way as we grow.
Strong understanding of modern software engineering processes
Client-focused and passionate about delivering strong business values.
Able to communicate clearly and effectively with various audiences, including developers, clients, customers, partners, and executives.
Flexible and willing to use the right technology for each problem in the context of timelines and business goals
Ability to complete the job regardless of the circumstance.
Our Benefits
Flexible vacation policy.
Three weeks of vacation, plus we are closed over the winter holidays (+ ~1 week)
One team-building event each quarter.
Great health benefits with a $1500-$3000 HSA/WSA.
Certification opportunities. We will pay for your GCP certification as well as other relevant training and certifications.
Learning opportunities. We work across different industries, technologies and roles and will work with you to help you explore your interests.
Blogging, open source, meetups and conference opportunities. We will provide a platform and time for you to pursue your ideas.
Our organization values action over politics, and our management team is made up of engineers who thrive on achieving tangible results.
Badal is an equal-opportunity employer committed to creating a safe, diverse and inclusive environment. We encourage qualified applicants of all backgrounds including ethnicity, religion, disability status, gender identity, sexual orientation, family status, age, nationality, and education levels to apply. If you are contacted for an interview and require accommodation during the interviewing process, please let us know.
Show more
Show less","Google Cloud Platform (GCP), Cloud transformation, Security, Analytics, ML, Data governance, Data pipelines, Data Scientists, Data architecture, Dataflow, Spark, Airflow, Kubeflow, Azkaban, Python, Java, Scala, Data Science, GCP services, IAM, Google Cloud Storage buckets, Business intelligence tools, QuickSight, Looker, Data Studio, Technical writing, Agile delivery teams, Data Governance principles, Collibra, Immuta, Kafka Streams","google cloud platform gcp, cloud transformation, security, analytics, ml, data governance, data pipelines, data scientists, data architecture, dataflow, spark, airflow, kubeflow, azkaban, python, java, scala, data science, gcp services, iam, google cloud storage buckets, business intelligence tools, quicksight, looker, data studio, technical writing, agile delivery teams, data governance principles, collibra, immuta, kafka streams","agile delivery teams, airflow, analytics, azkaban, business intelligence tools, cloud transformation, collibra, data architecture, data governance, data governance principles, data science, data scientists, data studio, dataflow, datapipeline, gcp services, google cloud platform gcp, google cloud storage buckets, iam, immuta, java, kafka streams, kubeflow, looker, ml, python, quicksight, scala, security, spark, technical writing"
Sr Data Engineer (contract),Tundra Technical Solutions,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/sr-data-engineer-contract-at-tundra-technical-solutions-3775478893,2023-12-17,Ontario, Canada,Mid senior,Remote,"Position: Sr Data Engineer
Location: Remote (EST/CST)
Duration: 4 Months
Project Overview: To develop a robust and scalable marketing solution that can meet the growing needs of the market. The solution will leverage the Enterprise Consumer Database Marketing framework, that collects, stores, and organizes data in a connected technology stack, to deliver a comprehensive marketing tool.
Project scope: Migrate and install all Marketing campaigns (~25) on the Enterprise Consumer Database Marketing platform using Adobe and SFMC stack.
Top Skill Sets/ Experiences Required
Experience working with Azure Data factory, Azure Databricks, PySpark and Snowflake
Analyze the data sources and data attributes
Industry Experience
Health industry experience is a plus but not mandatory
Marketing campaigns experience is good to have
Job 67258
Show more
Show less","Azure Data Factory, Azure Databricks, PySpark, Snowflake, Data analysis, Data sources, Data attributes, Health industry, Marketing campaigns","azure data factory, azure databricks, pyspark, snowflake, data analysis, data sources, data attributes, health industry, marketing campaigns","azure data factory, azure databricks, data attributes, data sources, dataanalytics, health industry, marketing campaigns, snowflake, spark"
Part-Time Data Analyst (Entry Level),Staffingandrecruiting,"Belleville, Ontario, Canada",https://ca.linkedin.com/jobs/view/part-time-data-analyst-entry-level-at-staffingandrecruiting-3729288886,2023-12-17,Ontario, Canada,Mid senior,Remote,"Minimum 1 year of work experience - fully remote position. Freshers are also encouraged to apply.
About us: The Future of AI is Patterned We are a stealth-mode technology startup that is revolutionizing the way AI is used. Our platform uses pattern recognition to train AI models that are more accurate, efficient, and robust than ever before.
We are backed by top investors and we are hiring for almost everything! If you are passionate about AI and want to be a part of something big, then we want to hear from you.
Make a positive impact on the world. Be a part of a fast-growing startup. If you are interested in learning more, please visit our website.
We Are Looking For People Who Are
Passionate about AI.
Excellent problem solvers.
Team players.
Driven to succeed.
Requirements
Role Responsibilities:
Work in close collaboration with the Business Intelligence Lead, Federal Data Lead, and other Program teams
Develop, maintain, and improve BI tools, build and enhance standard operating procedures (SOPs)
Manage various data sets and active Google workbooks with adjacent contract teams, monitor and analyze financial health information at the project and program levels
Communicate with client leadership to assess data needs and emerging requirements
Work with large data sets, workbooks, and spreadsheets to manipulate and manage program-level information using macros, queries, scripts, etc.
Gather requirements and lead the development of long-term data management tools, processes, and solutions based on organizational needs.
Be comfortable working with collaboration tools such as; Google Suite, Microsoft Office
Providing general support to the client including, but not limited to, analysis, data calls, financial management, risk management, audits, and project management-related tasks.
Qualifications
Bachelor's Degree in business, business intelligence, data or information management, or similar.
Proficient in Google Scripts
Minimum 1 year of data or information management and/or data analysis experience.
Experience using Microsoft Excel and Google Sheets (macros, imports, query functions).
Experience with developing Google App Script is a plus.
Experience using SQL Developer is a plus.
Excellent written and verbal communication skills.
Willing to work in an administratively manual environment while working towards automation of processes in the future.
Clearable (able to pass both a criminal background check and credit check).
Highly motivated, self-learner, and technically inquisitive
Benefits
Special Benefits you will love:
Flexible vacation paid unlimited holidays and paid sick days
401(k) with up to 2% employer match
Health, vision, and dental insurance
Why Patterned Learning AI?
Patterned Learning AI is made up of incredibly bright, mission-driven coworkers who are passionate about using technology to solve real-world problems---and we're growing quickly. In order to continue building an engaging and dynamic organization, we're committed to giving everyone the support they need to do great work.
We believe diverse perspectives and backgrounds are critical to building great technology, and our goal is to cultivate an environment where people feel equally valued and respected. Patterned Learning AI is proud to be an equal opportunity workplace, and we welcome applicants from all backgrounds regardless of race, color, ancestry, religion, gender identity or expression, sexual orientation, marital status, age, citizenship, socioeconomic status, disability, or veteran status.
Powered by Webbtree
Show more
Show less","Data Analysis, Business Intelligence, Data Management, Information Management, Google Sheets, Microsoft Excel, Macros, Queries, SQL Developer, Google App Script, Google Suite, Microsoft Office, Google Scripts","data analysis, business intelligence, data management, information management, google sheets, microsoft excel, macros, queries, sql developer, google app script, google suite, microsoft office, google scripts","business intelligence, data management, dataanalytics, google app script, google scripts, google sheets, google suite, information management, macros, microsoft excel, microsoft office, queries, sql developer"
Senior Data Engineer (Remote),Lightci (Light Consulting),"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/senior-data-engineer-remote-at-lightci-light-consulting-3770199039,2023-12-17,Ontario, Canada,Mid senior,Remote,"About us
Lightci (Light Consulting) is a full-service engineering & talent solution created for engineers, by engineers.
We build and ship products for clients like Google, Meta, Brex, and Walmart - and help Fortune 500 companies grow by way of full team construction, staff augmentation, executive search, and tailored recruitment to address unique hiring needs.
Visit lightci.com to learn more.
Role Mission
As a Senior Data Engineer consulting with our client in the EdTech space, you’ll play a pivotal role in building the data infrastructure for an educational product that impacts over 60,000 students. You’ll pioneer the centralization and normalization of data from third-party applications, ensuring that our client’s platform remains at the forefront of delivering high-quality educational experiences.
Deliverables
In this hands-on role, you’ll build the architecture, and models [for what the data will look like] within the first 1-2 weeks
You’ll take ownership of the ETL process for 3rd party data, automating as much as possible to enhance efficiency and reduce manual intervention
You will lead the effort to standardize data formats and structures, making it easier to analyze and derive valuable insights
Utilize your expertise in data transformation techniques to enrich raw data, making it more accessible and valuable for analytics and reporting purposes
Continuously monitor data pipelines and proactively address performance bottlenecks
Work closely with data scientists, analysts, and other stakeholders to understand their data requirements and assist in building data solutions that cater to their needs
About you
Familiarity with Big Data technologies like Apache Hadoop, Apache Spark, and their respective ecosystems
Experience in building and optimizing data pipelines, architectures, and data sets
Ability to create data models and have knowledge of normalization and denormalization techniques
Expertise in working with relational databases (e.g., PostgreSQL, MySQL) and NoSQL databases (e.g., MongoDB, Cassandra)
Experience with ETL (extract, transform, load) processes, and tools like Apache NiFi, Talend, or Microsoft SSIS
Proficiency in programming languages, particularly Python and Typescript to develop data processing scripts
Nice to have
Backend engineering background, familiarity with Domain Driven Design
Proficiency with Typescript
Familiarity with TS Web frameworks like NestJS or Express
Perks & Benefits
100% fully remote work (anywhere in North America)
Flexible hours
Competitive compensation package
Bonus or commission [depending on your role]
Excellent benefits package
Generous RRSP [or] 401k matching program
Unlimited vacation
Quarterly/Yearly team ‘offsite’ events
At Lightci, we are committed to fostering a diverse, equitable, and inclusive workplace. We believe that diverse perspectives drive innovation and enrich our work environment. We welcome applicants of all backgrounds, experiences, and identities to join our team. By embracing a culture of inclusivity, we aim to create a space where every individual feels valued, respected, and empowered to contribute their best. Join us in our journey to create a more equitable future for all.
#LI-Remote
Show more
Show less","Apache Hadoop, Apache Spark, Big Data, PostgreSQL, MySQL, MongoDB, Cassandra, ETL, Apache NiFi, Talend, Microsoft SSIS, Python, Typescript, NestJS, Express, Backend engineering, Domain Driven Design","apache hadoop, apache spark, big data, postgresql, mysql, mongodb, cassandra, etl, apache nifi, talend, microsoft ssis, python, typescript, nestjs, express, backend engineering, domain driven design","apache hadoop, apache nifi, apache spark, backend engineering, big data, cassandra, domain driven design, etl, express, microsoft ssis, mongodb, mysql, nestjs, postgresql, python, talend, typescript"
Data Analyst,Iris Software Inc.,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/data-analyst-at-iris-software-inc-3779370540,2023-12-17,Ontario, Canada,Mid senior,Remote,"IRIS’ direct client, one of the world's largest financial institutions is currently looking for a strong
Data Analyst
for a
Full Time / Long Term Contract
opportunity.
Location: Remote to begin with
JOB DESCRIPTION
· Strong excel skills. Must know SQL, Hadoop (Big Data).
· Nice to have some BI experience.
· Analyzing Data using Excel primarily.
· Looking for Senior Data Analyst to join the team coming from Banking / financial services /capital markets background.
· 10+ years of experience
· Must have extensive Banking / Financial services/capital markets background and needs to understand financial products such as equities, fixed income, derivatives such as rating and pricing
· Effectively communicating requirements and plans to cross-functional team members and senior management.
· Define solutions meet business needs and requirements, at the same time ensuring technical compatibility.
· STRONG SQL experience – some queries, debugging to validate the data and detect the problems
Anurag Dang
Iris Software
https://www.linkedin.com/in/anuragdang/
Show more
Show less","SQL, Hadoop, Business Intelligence, Excel, Data Analysis, Banking, Financial Services, Capital Markets, Equities, Fixed Income, Derivatives, Ratings, Pricing","sql, hadoop, business intelligence, excel, data analysis, banking, financial services, capital markets, equities, fixed income, derivatives, ratings, pricing","banking, business intelligence, capital markets, dataanalytics, derivatives, equities, excel, financial services, fixed income, hadoop, pricing, ratings, sql"
"Data Engineer(AWS, Snowflake, Informatica IICS & Axon)",Hays,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/data-engineer-aws-snowflake-informatica-iics-axon-at-hays-3787105433,2023-12-17,Ontario, Canada,Mid senior,Hybrid,"Client
: Real estate company
Role:
Data Engineer
Job Type:
Contract
Duration:
12 months
Location:
Hybrid /Toronto, 3 days a week in office
Your New Company
Our client is global real estate company, invests in, owns, and manages commercial real estate, they are looking for Data Engineer to join their team on a contact basis role.
Your New Role
We seek a highly skilled and motivated Data Engineer to join our client’s Data Management team. As a Data Engineer, you will be responsible for designing, implementing, and maintaining our data infrastructure. You will work closely with data architects and business analysts to ensure our data infrastructure is optimized for performance and scalability. More specifically, you will:
Design and develop data pipelines to ingest, store, and transform data from a variety of sources
Develop data models and algorithms to optimize data architecture
Monitor data quality and performance, and provide troubleshooting when needed
Job Accountabilities:
Accelerate data-informed decision-making to transform our product and engineering strategy
Architect scalable date models and build efficient and reliable ETL pipelines to bring data to our core data warehouse
Design and develop highly scalable and reliable data pipelines and ETL processes to ensure smooth data flow and accessibility (Informatica IICS preferred)
Demonstrably deep understanding of SQL and analytical data warehouses (Snowflake preferred)
Professional experience using Python, Java, or Scala for data processing (Python preferred)
Knowledge of and experience with data-related Python packages
Develop and maintain data warehousing and storage solutions to ensure data is stored securely and efficiently.
Collaborate with cross-functional teams to design and implement data models and data warehouse architecture that supports business needs.
Optimize the performance of our data pipelines and data warehouse by implementing best practices and continuous improvement.
Develop and implement data governance policies and procedures to ensure data accuracy, security, and compliance with industry regulations.
Constantly improve product quality, security, and performance
Understand and implement data engineering best practices
Improve, manage, and teach standards for code maintainability and performance in code submitted and reviewed
Generate architecture recommendations and the ability to implement them
Great communication: Regularly achieve consensus amongst teams
Documentation of the work in Confluence
What You’ll Need to Succeed
A university degree in computer science or an approved equivalent combination of education and experience.
Minimum of five years of experience as a data engineer focusing on building scalable data pipelines and data warehouses.
Strong proficiency in
Snowflake
,
Informatica IICS
, Snowpipe, SQL, Unix, Python, and API integrations.
Strong knowledge of data warehousing and modelling concepts, with experience designing and building large-scale data solutions.
Experience developing and maintaining ETL/ELT pipelines and data integration solutions.
Strong SQL skills, with experience in performance tuning and optimization of complex queries.
Strong programming skills in
Python
, with experience developing and maintaining data processing scripts.
Experience in API development and integration with third-party systems.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Strong problem-solving skills with the ability to analyze complex data sets and identify trends and insights; think creatively and outside the box.
Excellent communication skills with the ability to communicate technical concepts to non-technical stakeholders.
Ability to work independently and in a team environment, prioritize tasks, and manage multiple projects simultaneously.
Experience with agile development methodologies such as Scrum or Kanban.
Design and optimize the SQL syntax and queries for faster data processing
Dimensional modelling experience is mandatory
Adoption of best development practices for batch processing
Managing all work in Jira, including but not limited to story creation, defect resolution etc.
Identifies opportunities for new architectural initiatives and recommends the increased scalability and robustness of platforms and solutions.
Core Competencies and Skills:
Extensive experience in establishing data warehouses/lakes, has solid experience with Cloud-based data storage solutions (
AWS S3 specifically
).
Strong experience with data stack like Snowflake, Informatica IICS, and
Informatica Axon
and their data models.
Good understanding of data-driven system integration (web services and ETL/batch jobs), establishing the organization’s canonical data models and ability to work with XML and JSON-based data formats.
Scripting expert
Experience with description techniques for data architecture and the relationship of data with other architecture domains (e.g. data representations in business and application process models
What you’ll get in Return
They are looking for great people who thrive in a respectful, collaborative, inclusive, and productive culture to join their team. You will be working with smart, passionate and dedicated new team members.
Interested?
If you’re available and interested in this role, please reply to larina.liu@hays.com with your updated word resume and salary requirement.
Show more
Show less","Data Engineer, SQL, Python, Java, Scala, Snowflake, Informatica IICS, Snowpipe, Unix, AWS S3, Informatica Axon, XML, JSON, ETL, ELT, API, Scrum, Kanban, Jira, Data warehouse, Data lake, Cloudbased data storage, Datadriven system integration, Web services, Canonical data models, Data architecture, Data representations, Business process models, Application process models","data engineer, sql, python, java, scala, snowflake, informatica iics, snowpipe, unix, aws s3, informatica axon, xml, json, etl, elt, api, scrum, kanban, jira, data warehouse, data lake, cloudbased data storage, datadriven system integration, web services, canonical data models, data architecture, data representations, business process models, application process models","api, application process models, aws s3, business process models, canonical data models, cloudbased data storage, data architecture, data lake, data representations, datadriven system integration, dataengineering, datawarehouse, elt, etl, informatica axon, informatica iics, java, jira, json, kanban, python, scala, scrum, snowflake, snowpipe, sql, unix, web services, xml"
Senior Engineer – Data Center Facility Operations,GEICO,"Fredericksburg, VA",https://www.linkedin.com/jobs/view/senior-engineer-%E2%80%93-data-center-facility-operations-at-geico-3774632111,2023-12-17,Bowling Green,United States,Mid senior,Onsite,"Position Summary
GEICO is seeking an experienced Engineer with technical experience and outstanding customer service skills to oversee associated vendors for critical infrastructure, including engineering and maintenance. You will support site engineering projects, budgets, risks, and changes, ensuring the smooth functioning of our data center infrastructure as we transition from a traditional IT model to a tech organization with engineering excellence as its mission, while co-creating the culture of psychological safety and continuous improvement.
Position Description
Our Senior Engineer - Data Center Facility Operations is responsible for executing operational objectives, policies, and procedures. Our team thrives and succeeds in delivering high-quality technology products and services in a hyper-growth environment where priorities shift quickly. The ideal candidate has working knowledge of mechanical HVAC systems, electrical systems, plumbing operations, and data center operations related to cooling and electrical equipment.
Position Responsibilities
As a Senior Engineer, you will:
Reinforce a team of 24/7/365 skilled facility engineers and professionals
Support facility operations, including electrical, UPS, mechanical, and control systems such as HVAC systems, generators, and safety systems
Troubleshoot and resolve complex incident management issues, producing incident reports for further analysis
Coordinate contractual activities
Oversee facilities projects, including budget, schedule, and scope
Execute strategies to prevent potential problems
Participate in commissioning processes and testing of existing and new equipment
Maintain overall site capacity, including electrical, mechanical, and control systems
Successfully deliver projects on time, on budget, and within scope
Communicate and collaborate effectively with external project teams and resources
Handle and resolve escalations while partnering with relevant stakeholders to deliver projects
Assist in budget management
Execute policies and procedures for essential operating protocols
Qualifications
Strong experience in a 24/7/365 critical facility environment, including troubleshooting
Knowledge of EHS (Environment, Health, and Safety) compliance requirements and experience with audits
Understanding of power distribution and control systems, redundancy, resiliency, and
Physical ability to lift and carry up to 50 pounds and perform activities such as walking, kneeling, climbing, pulling, bending, lifting, carrying, and standing for extended periods.
Capable of climbing ladders up to 30 feet and structures up to 45 feet, and maneuvering around trucks, scaffolds, and equipment
Proficient in operating tools, equipment, vehicles, and machinery
Able to work in limited light and differentiate colors
Willingness to work in emergency situations and inclement weather, with the availability for overtime as needed
Must use and maintain personal protective equipment as required
Comfortable working in indoor and outdoor environments, including adverse weather
conditions and noise
Able to work in areas covered by brush and trees or in trenches with poor footing and uneven or wet ground
Willingness to work with energized conductors and equipment rated from low to medium voltages
Must have and maintain an acceptable driving record according to company policy and a valid state driver's license
Familiarity with driving and operating vehicles such as automobiles, pickup trucks, vans, ATVs, aerial man-lifts, and forklifts (forklift and man-lift operation require prior certification)
Ability to operate test equipment for meters and electrical fault location
Proficient using computers and calculators
Adaptability to future changes in equipment, vehicles, and tools as required
Proven ability to concentrate and demonstrate a capacity for learning technical concepts and adapting to new technologies quickly
Proficient in computer applications such as MS Word, Excel, and Power Point
Strong verbal and written communication skills, including effectively communicating progress of programs/projects
Experience
5+ years operations-related experience or equivalent work experience
4+ years of hands-on work experience in a technical environment
Education
Associate’s degree in related Engineering, Engineering Technology, or equivalent experience
At this time, GEICO will not sponsor a new applicant for employment authorization for this position.
Benefits:
As an Associate, you’ll enjoy our Total Rewards Program* to help secure your financial future and preserve your health and well-being, including:
Premier Medical, Dental and Vision Insurance with no waiting period**
Paid Vacation, Sick and Parental Leave
401(k) Plan
Tuition Reimbursement
Paid Training and Licensures
Benefits may be different by location. Benefit eligibility requirements vary and may include length of service.
Coverage begins on the date of hire. Must enroll in New Hire Benefits within 30 days of the date of hire for coverage to take effect.
The equal employment opportunity policy of the GEICO Companies provides for a fair and equal employment opportunity for all associates and job applicants regardless of race, color, religious creed, national origin, ancestry, age, gender, pregnancy, sexual orientation, gender identity, marital status, familial status, disability or genetic information, in compliance with applicable federal, state and local law. GEICO hires and promotes individuals solely on the basis of their qualifications for the job to be filled.
GEICO reasonably accommodates qualified individuals with disabilities to enable them to receive equal employment opportunity and/or perform the essential functions of the job, unless the accommodation would impose an undue hardship to the Company. This applies to all applicants and associates. GEICO also provides a work environment in which each associate is able to be productive and work to the best of their ability. We do not condone or tolerate an atmosphere of intimidation or harassment. We expect and require the cooperation of all associates in maintaining an atmosphere free from discrimination and harassment with mutual respect by and for all associates and applicants.
Show more
Show less","Mechanical HVAC systems, Electrical systems, Plumbing operations, Data center operations, Incident management, Budget management, Project management, Electrical distribution and control systems, Redundancy, Resiliency, EHS compliance, MS Word, Excel, Power Point, Communication skills","mechanical hvac systems, electrical systems, plumbing operations, data center operations, incident management, budget management, project management, electrical distribution and control systems, redundancy, resiliency, ehs compliance, ms word, excel, power point, communication skills","budget management, communication skills, data center operations, ehs compliance, electrical distribution and control systems, electrical systems, excel, incident management, mechanical hvac systems, ms word, plumbing operations, power point, project management, redundancy, resiliency"
Senior Data Analyst,Smith Drug Company,"Spartanburg, SC",https://www.linkedin.com/jobs/view/senior-data-analyst-at-smith-drug-company-3766957570,2023-12-17,Spartanburg,United States,Mid senior,Onsite,"Company Description
Smith Drug Company is a trusted name in the pharmaceutical industry, catering to community independent pharmacies and hospitals. The company has a long-standing tradition of providing exceptional service that meets its customers' needs. Smith Drug Company is committed to understanding pharmacists' challenges and providing unique solutions to help them compete against large national chains. The company is located in Spartanburg, SC.
Role Description
We seek a highly skilled and experienced Senior Data Analyst to join our team and help drive data-driven decision-making within our organization. As a Senior Data Analyst, you will be critical in collecting, analyzing, and interpreting data to provide valuable insights and support strategic business initiatives. You will collaborate with cross-functional teams to identify opportunities for improvement and implement data-driven solutions.
Responsibilities:
Data Collection and Analysis: Gather, clean, and organize data from various sources, ensuring its accuracy and reliability. Perform advanced data analysis and develop data models to extract meaningful insights.
Data Visualization: Create informative and visually appealing dashboards and reports to present data findings effectively to stakeholders. Use tools like Tableau, Power BI, or others to communicate insights.
Predictive Modeling: Develop predictive models and statistical analyses to forecast trends, identify patterns, and make recommendations for decision-makers.
Data Interpretation: Translate complex data into actionable insights and recommendations for business strategy, process improvements, and other initiatives.
Hypothesis Testing: Design and conduct A/B tests and experiments to evaluate the impact of changes and initiatives.
Data Quality Assurance: Ensure data accuracy, integrity, and consistency and recommend data quality improvements when necessary.
Collaboration: Work closely with cross-functional teams, including marketing, product development, and finance, to support their data-related needs and collaborate on data-driven projects.
Stay Current: Keep abreast of industry trends, emerging technologies, and best practices in data analytics, and proactively suggest their implementation to enhance our data analysis capabilities.
Qualifications
Bachelor's or Master's degree in a relevant field such as Statistics, Mathematics, Computer Science, or a related discipline.
3+ years of professional experience in data analysis, focusing on progressively more complex and impactful roles.
Proficiency in data analysis tools and languages (e.g., SQL, Python, R).
Strong expertise in data visualization tools (e.g., Tableau, Power BI, or others).
Experience with statistical analysis and predictive modeling.
Knowledge of data warehousing and ETL processes.
Excellent problem-solving and critical-thinking skills.
Strong communication and presentation skills, with the ability to convey complex ideas to non-technical stakeholders.
Team player with the ability to collaborate effectively in a cross-functional environment.
Detail-oriented and committed to delivering high-quality work.
Demonstrated leadership and mentorship skills.
Relevant certifications (e.g., Certified Data Analyst, Certified Business Intelligence Professional) are a plus.
Show more
Show less","Data analysis, Data visualization, Predictive modeling, Data interpretation, Hypothesis testing, Data quality assurance, Data collection, Data cleaning, Data organization, Statistical analysis, Business intelligence, Datadriven decisionmaking, Crossfunctional collaboration, SQL, Python, R, Tableau, Power BI, ETL processes, Data warehousing, Machine learning, Big data, Artificial intelligence","data analysis, data visualization, predictive modeling, data interpretation, hypothesis testing, data quality assurance, data collection, data cleaning, data organization, statistical analysis, business intelligence, datadriven decisionmaking, crossfunctional collaboration, sql, python, r, tableau, power bi, etl processes, data warehousing, machine learning, big data, artificial intelligence","artificial intelligence, big data, business intelligence, crossfunctional collaboration, data cleaning, data collection, data interpretation, data organization, data quality assurance, dataanalytics, datadriven decisionmaking, datawarehouse, etl, hypothesis testing, machine learning, powerbi, predictive modeling, python, r, sql, statistical analysis, tableau, visualization"
Data Analyst-DA - Canada,Zortech Solutions,"Calgary, Alberta, Canada",https://ca.linkedin.com/jobs/view/data-analyst-da-canada-at-zortech-solutions-3702640639,2023-12-17,Alberta, Canada,Associate,Onsite,"Role: Data Analyst-DA
Location: Alberta-Onsite
Duration: 6-12+ Months
Job Description
Roles, Responsibilities and Qualifications
Role Description
A Data Analyst provides expertise in instrumentation and data collection, data-driven storytelling, statistical analysis, modelling and data visualizations, developing data policies and governance standards, and developing service analytics standards and practices (inclusive of both digital and non-digital service implementation).
Responsibilities
Creates plans and strategies that will identify the various linkages between new and existing data forms, resulting in the integration of data models, development of data policies, and presentation of statistical analysis and data visualizations to help understand and improve service delivery online and offline.
Supports governance based on the service data model, service analytics standards, and the development of analytics tools, inclusive of both digital and non-digital service implementation.
Works to implement develop, and share service metrics and service performance dashboards for internal and public use.
Facilitates and informs program area workshops about current data and performance practices.
Uses the service journey to frame future measurement models.
Mentors team members and others to develop and grow their analytics fluency.
Researches best practices and makes recommendations for the direction of data-driven governance policies to support the adoption of digital services, and service delivery information management.
Creates data statistical analysis and data visualizations leveraging data querying languages within available data management technologies.
Supports corporate priorities based upon data-driven evidence by leveraging existing and new analytics, data visualizations, data modeling and storytelling.
Other responsibilities as required or requested.
Qualifications a) Experience leading data science and analytics work, including digital and non-digital services.
Experience with leadership, communications, relationship building, and planning.
Experience with current methodologies in analytics, data visualizations, data modeling and storytelling.
Experience working with cross-functional teams to understand detailed requirements and align these requirements with product vision and user needs.
Experience working in a complex enterprise environment (10,000 employees or greater).
Experience with agile projects in a public sector organization.
Experience with web development and digital product design.
Experience providing analytics support to user experience, customer experience, or service design teams.
Experience with quantitative research methods such as surveys.
Experience developing and maintaining relationships with multiple clients and stakeholders, including negotiating agreements and resolving conflicts.
Show more
Show less","Data Analysis, Instrumentation, Data Collection, Statistical Analysis, Data Visualization, Data Governance, Service Analytics, Data Modeling, Data Policies, Analytics Tools, Service Metrics, Service Performance Dashboards, DataDriven Governance, Data Querying Languages, Data Management Technologies, Web Development, Digital Product Design, User Experience, Customer Experience, Service Design, Quantitative Research Methods","data analysis, instrumentation, data collection, statistical analysis, data visualization, data governance, service analytics, data modeling, data policies, analytics tools, service metrics, service performance dashboards, datadriven governance, data querying languages, data management technologies, web development, digital product design, user experience, customer experience, service design, quantitative research methods","analytics tools, customer experience, data collection, data governance, data management technologies, data policies, data querying languages, dataanalytics, datadriven governance, datamodeling, digital product design, instrumentation, quantitative research methods, service analytics, service design, service metrics, service performance dashboards, statistical analysis, user experience, visualization, web development"
Data Engineer – Integration Specialist,BURNCO Rock Products Ltd,"Calgary, Alberta, Canada",https://ca.linkedin.com/jobs/view/data-engineer-%E2%80%93-integration-specialist-at-burnco-rock-products-ltd-3778860573,2023-12-17,Alberta, Canada,Associate,Onsite,"Rock it with BURNCO!!!
BURNCO is a growing, rock-solid business supplying aggregate construction materials in Canada and the US.
We've been supporting family values and building for the next generation for over a hundred years, and we need you to help make the next hundred years even better. Be a key player in leading our business out of the Flintstones™ era and into the age of the Jetsons™.
Talented. Strategic. Responsive. Nimble. Flexible. Dynamic. Data & Technology Driven.
BURNCO Rock Products Ltd has a fantastic opportunity for an Data Engineer – Integration Specialist based out of our corporate headquarters in Calgary, Alberta. You will report to our Manager, Data & Analytics and work closely with our internal key stakeholders in IT, HR, Finance, as well as our lines of business (Concrete, Asphalt, Aggregate).
The Data Engineer - Integrations Specialist at BURNCO, will play a key role in ensuring the seamless operation of our data integrations within the Production environment. This dynamic position requires a proactive approach to monitoring, production support, and collaborative engagement with both business and IT stakeholders.
What You Will Be Doing
How we see your role breaking down:
Monitoring and support – 50%
Oversee and provide proactive support for integrations within the Production environment.
Ensure timely execution of SQL server jobs, ADF, and OIC integrations as per schedule. Address any failures by triaging and resolving issues promptly.
Collaborate with business and IT stakeholders to facilitate weekly and month-end business activities and processes effectively.
Work hand in hand with contractors and suppliers to complete the required design, installation, commissioning of systems until complete handover to IT operations and the business.
Manage interdependencies with other projects and operational activities.
Assist in SQL Server, OIC and system integration related scenarios where the infrastructure team lacks specialized knowledge.
Provide backup support for Cinchy queries and ensure adherence to security protocols.
Maintain lookup data within the SQL Server database for smooth information transfer during integrations.
Issue Triaging and Resolution – 40%
Identify, debug, and rectify production integration issues as they arise, ensuring minimal disruptions.
Collaborate with the ERP team, IT and business stakeholders to resolve major issues/deploy enhancements efficiently.
Continuously improve data integration frameworks and methodologies used within
BURNCO
.
Ensure delivery transparency with timely and effective communications and reporting.
Documentation and process adherence – 10%
Maintain comprehensive integration documentation for reference and future improvements.
Lead the creation of step-by-step deployment procedures for fixes and enhancements, with backout and roll-back sequences and timings, test plans and procedures.
Maintain excellent relationships with stakeholders and vendors and ensure
BURNCO
is receiving top quality service.
Represent Data & Analytics team within the IT change management process.
Gather lessons learned from production failures and fixes, analyze feedback and incorporate the same into on-going and future projects.
Other duties as assigned
Working Conditions
The role has standard working conditions in an office environment with a regular work week from Monday to Friday and on-call availability during implementation and burn-in/warranty periods, dependent upon activity scope.
What We Would Like From You
Competencies
Strong analytical skills with the ability to diagnose problems efficiently and provide effective solutions.
Excellent communication skills, with the ability to work collaboratively with cross-functional teams, business stakeholders, and external vendors.
Experience in collaborating with external teams or vendors to resolve integration-related issues or enhancements.
Attention to detail with a focus on maintaining comprehensive documentation for integrations, fixes, and enhancements.
Strong organizational skills to manage multiple tasks, prioritize effectively, and meet deadlines in a dynamic environment.
Ability to adapt to evolving technologies and proactively seek opportunities for process improvement and optimization.
Self-motivated individual with a proactive approach to problem-solving and continuous learning.
Experience/Technical Skills/Knowledge
Minimum of 8 years of experience in a similar role, preferably in a production environment dealing with Oracle integrations and SQL server management.
Proficiency in SQL Server, Oracle Integration Cloud (OIC) and Azure Data Factory (ADF).
Demonstrated experience in debugging, identifying, and resolving integration issues across multiple platforms.
Familiarity with Command, Apex, Onbase, Cinchy, and Oracle ERP will be advantageous.
Proven experience in troubleshooting and resolution of complex integration issues promptly.
Education/Certification/Designation
Bachelor's degree in Computer Science, Information Technology, or a related field. Equivalent practical experience will be considered.
COMPETITIVE SALARIES
……
Worth it!!
PERFORMANCE INCENTIVES
……
They rock!!
GREAT BENEFITS
......
Count on it!!
CHANCE TO MAKE A DIFFERENCE
……
Absolutely!!
LEARNING OPPORTUNITIES
……
Always!!
Show more
Show less","Data Integration, SQL Server, Azure Data Factory (ADF), Oracle Integration Cloud (OIC), Business Intelligence, Analytics, Debugging, Oracle ERP, Command, Apex, Onbase, Cinchy, Change Management, Project Management, Troubleshooting","data integration, sql server, azure data factory adf, oracle integration cloud oic, business intelligence, analytics, debugging, oracle erp, command, apex, onbase, cinchy, change management, project management, troubleshooting","analytics, apex, azure data factory adf, business intelligence, change management, cinchy, command, data integration, debugging, onbase, oracle erp, oracle integration cloud oic, project management, sql server, troubleshooting"
Big Data Engineer (Azure),Tiger Analytics,"Chicago, IL",https://www.linkedin.com/jobs/view/big-data-engineer-azure-at-tiger-analytics-3590302059,2023-12-17,Alberta, Canada,Associate,Remote,"Tiger Analytics is a fast-growing advanced analytics consulting firm. Our consultants bring deep expertise in Data Science, Machine Learning and AI. We are the trusted analytics partner for multiple Fortune 500 companies, enabling them to generate business value from data. Our business value and leadership has been recognized by various market research firms, including Forrester and Gartner. We are looking for top-notch talent as we continue to build the best global analytics consulting team in the world.
The Big Data Azure Engineer will be responsible for architecting, designing, and implementing advanced analytics capabilities. These capabilities include batch and streaming analytics, machine learning models, natural language generation, and other emerging technologies in the field of advanced analytics.
Requirements
Bachelor’s degree in Computer Science or similar field
4+ years of experience in traditional and modern Big Data technologies (HDFS, Hadoop, Hive, Pig, Sqoop, Kafka, Apache Spark, hBase, Oozie, No SQL databases)
Experience in Java/Python/Scala
Experience extracting/querying/joining large data sets at scale
Experience building data platforms using Azure stack
Experience building data ingestion pipelines using Azure Data Factory to ingest structured and unstructured data
Strong knowledge on Azure Storage schematics such as Gen1 and Gen2
Experience in harmonizing raw data into a consumer-friendly format using Azure Databricks
Knowledge of Azure networking, security, key vaults, etc.
Experience in data wrangling, advanced analytic modeling, and AI/ML capabilities is preferred
Experience utilizing Snowflake to build data marts with the data residing in Azure storage is a plus
Strong communication and organizational skills
Benefits
This position offers an excellent opportunity for significant career development in a fast-growing and challenging entrepreneurial environment with a high degree of individual responsibility.
Show more
Show less","Data Science, Machine Learning, AI, Hadoop, Hive, Pig, Sqoop, Kafka, Apache Spark, hBase, Oozie, No SQL databases, Java, Python, Scala, Azure stack, Azure Data Factory, Azure Storage, Gen1, Gen2, Azure Databricks, Azure networking, Azure security, Azure key vaults, Data wrangling, Advanced analytic modeling, AI/ML, Snowflake, Data marts","data science, machine learning, ai, hadoop, hive, pig, sqoop, kafka, apache spark, hbase, oozie, no sql databases, java, python, scala, azure stack, azure data factory, azure storage, gen1, gen2, azure databricks, azure networking, azure security, azure key vaults, data wrangling, advanced analytic modeling, aiml, snowflake, data marts","advanced analytic modeling, ai, aiml, apache spark, azure data factory, azure databricks, azure key vaults, azure networking, azure security, azure stack, azure storage, data marts, data science, data wrangling, gen1, gen2, hadoop, hbase, hive, java, kafka, machine learning, no sql databases, oozie, pig, python, scala, snowflake, sqoop"
Data Engineer,iTech Solutions,United States,https://www.linkedin.com/jobs/view/data-engineer-at-itech-solutions-3714396845,2023-12-17,Alberta, Canada,Associate,Remote,"Role: Data Engineer
Location: Remote (preferred west coast as the candidate shall be working in PST)
Bill Rate: 65/hr.
Description:
Design, develop, test, deploy, support, enhance data integration solutions seamlessly to connect and integrate the enterprise systems in our Enterprise Data Platform.
Innovate for data integration in Apache Spark-based Platform to ensure the technology solutions leverage cutting edge integration capabilities.
Experience with ETL, data pipeline creation to load data from multiple data sources.
Primary Skills:
4+ years working experience in data integration and pipeline development.
BS degree in CS, CE or EE.
2+ years of Experience with AWS Cloud on data integration with Apache Spark, EMR, Glue, Kafka, Kinesis, and Lambda in S3, Redshift, RDS, MongoDB/DynamoDB ecosystems
Strong real-life experience in python development especially in pySpark in AWS Cloud environment.
Design, develop test, deploy, maintain and improve data integration pipeline.
Experience in Python and common python libraries.
Strong analytical experience with database in writing complex queries, query optimization, debugging, user defined functions, views, indexes etc.
Strong experience with source control systems such as Git, Bitbucket, and Jenkins build and continuous integration tools.
Databricks, Redshift Experience is a plus.
Note: Please look for 4-5 years of work experience only as the budget here is limited.
Show more
Show less","Data Integration, Apache Spark, ETL, Data Pipeline, AWS Cloud, EMR, Glue, Kafka, Kinesis, Lambda, Amazon S3, Redshift, RDS, MongoDB, DynamoDB, Python, PySpark, Git, Bitbucket, Jenkins, Continuous Integration, Databricks","data integration, apache spark, etl, data pipeline, aws cloud, emr, glue, kafka, kinesis, lambda, amazon s3, redshift, rds, mongodb, dynamodb, python, pyspark, git, bitbucket, jenkins, continuous integration, databricks","amazon s3, apache spark, aws cloud, bitbucket, continuous integration, data integration, data pipeline, databricks, dynamodb, emr, etl, git, glue, jenkins, kafka, kinesis, lambda, mongodb, python, rds, redshift, spark"
PI Data Analyst/Developer,Rapport Talents,United States,https://www.linkedin.com/jobs/view/pi-data-analyst-developer-at-rapport-talents-3783166045,2023-12-17,Alberta, Canada,Associate,Remote,"Job Title: PI Data Analyst/Developer
Location: Remote
Duration: 12-24 months contract
Partner Rate: Open
Project Scope/ Job Responsibilities
On Call Required (1 time every 2 months and potentially late-night triage calls 1 time per week)**
A “Client” is looking for a PI Data Analyst to support their PI Systems. This person will be helping to lead the security team, delegate work and handle a lot of PI triaging. PI System knowledge is required to help debug code in production. .Net/C# experience is required, although this person may not be doing much development from scratch as he/she will be doing more bug fixing.
This is 90% Data Analysis and 10% Development. Must have heavy SQL, SSIS/SSRS or other reporting tools.
Required Skills
Top Required Skills: (5+ years’ experience)
OSISoft PI Systems / PI WebAPI experience
Programming (.Net/C#) experience
Organizational leadership experience Other Skills:
OSIsoft PI System knowledge/PI WebAPI
Maintaining applications in portfolio, triaging, talking to other teams in the company to problem solve, helping clients use product (mainly PI WebAPI, analytics and security area)
Windows File System/NAS
Systems Analysis
SQL DML/MS SQL Server Administration
.NET/C#
This person will be helping some with development for project work but not their primary role as this skillset is more for bug fixing.
Network/Infrastructure (“Client” specific)
Former “Client” experience is nice to have as their network/infrastructure isn’t setup in the most common way so would be helpful to understand how theirs is set up.
Security/Vulnerabilities management
As it relates to PI systems and working with the patching team. This person may not do the patching themselves but should be able to talk to the team and communicate what needs to happen.
Windows/OS level troubleshooting/Patching
Powershell/Batch processing/Automation
NERC CIP experience
Nice to have
Project/team leadership experience
General Skills
Equal Opportunity Employer Minorities/Women/Veterans/Individuals with Disabilities
Nice to have
OSISoft
Show more
Show less","PI Data Analyst, PI Systems, PI WebAPI, .Net, C#, Programming, SQL, SSIS, SSRS, Reporting tools, Windows File System, NAS, Systems Analysis, SQL DML, MS SQL Server Administration, Network Infrastructure, Security Management, Vulnerabilities Management, Patching, Powershell, Batch Processing, Automation, NERC CIP, Project Leadership, Team Leadership, General Skills","pi data analyst, pi systems, pi webapi, net, c, programming, sql, ssis, ssrs, reporting tools, windows file system, nas, systems analysis, sql dml, ms sql server administration, network infrastructure, security management, vulnerabilities management, patching, powershell, batch processing, automation, nerc cip, project leadership, team leadership, general skills","automation, batch processing, c, general skills, ms sql server administration, nas, nerc cip, net, network infrastructure, patching, pi data analyst, pi systems, pi webapi, powershell, programming, project leadership, reporting tools, security management, sql, sql dml, ssis, ssrs, systems analysis, team leadership, vulnerabilities management, windows file system"
Data Analyst,Accroid Inc,United States,https://www.linkedin.com/jobs/view/data-analyst-at-accroid-inc-3763530856,2023-12-17,Alberta, Canada,Associate,Remote,"Remote
Data Analyst
PH and Skype
IOWA
Preferred attributes include self-starting, inquisitive, willingness to learn through courses or from others, and adaptability.
Experience in data analytics.
Familiarity with SQL Server Reporting Services (SSRS), Microsoft Report Builder, or similar reporting software.
Experience with data analytics software such as Power BI, Tableau, etc.
Proficiency in writing relational database queries, including subqueries, versioning, and temp tables; experience with SQL Server Management Studio (SSMS) is a plus.
Show more
Show less","Data Analytics, SQL Server Reporting Services (SSRS), Microsoft Report Builder, Power BI, Tableau, SQL, Subqueries, Versioning, Temp tables, SQL Server Management Studio (SSMS)","data analytics, sql server reporting services ssrs, microsoft report builder, power bi, tableau, sql, subqueries, versioning, temp tables, sql server management studio ssms","dataanalytics, microsoft report builder, powerbi, sql, sql server management studio ssms, sql server reporting services ssrs, subqueries, tableau, temp tables, versioning"
Data Analyst,"UrBench, LLC",United States,https://www.linkedin.com/jobs/view/data-analyst-at-urbench-llc-3782726688,2023-12-17,Alberta, Canada,Associate,Remote,"We are looking for an experienced Data Analyst on a remote, contract basis to assist with creating solutions specifically around escalations, surveys, and internal free form order notes; this will require working with unstructured data, a strong proficiency in Power BI, and a deep understanding of SQL. The ideal candidate will play a pivotal role in extracting insights from these data sources, transforming them into actionable information, and visualizing the findings using Power BI. If you are passionate about data analysis, possess a strong analytical mindset, and are excited to work with diverse data sets, we encourage you to apply for this position.
Responsibilities
Data Collection and Preparation:
Acquire, collect, and manage unstructured data from various sources.
Clean, preprocess, and organize the data to make it suitable for analysis.
Data Analysis
Perform exploratory data analysis to uncover trends, patterns, and anomalies in the unstructured data.
Develop data models and algorithms to extract meaningful insights.
SQL Expertise
Write complex SQL queries to retrieve and manipulate structured data from relational databases.
Combine structured and unstructured data to create a comprehensive dataset for analysis.
Visualization And Reporting
Create interactive and informative dashboards and reports using Power BI to communicate findings effectively.
Collaborate with business stakeholders to understand their requirements and design dashboards that address their needs.
Data Interpretation
Translate data findings into actionable recommendations for business improvements.
Provide data-driven insights to support decision-making processes.
Requirements
Proven experience in working with unstructured data and extracting valuable insights.
Proficiency in SQL for data retrieval, manipulation, and transformation.
Strong expertise in Power BI for data visualization and reporting.
Excellent problem-solving and critical-thinking skills.
Strong communication skills to present complex data findings in a clear and understandable manner.
Attention to detail and the ability to handle large datasets.
Knowledge of data governance and privacy regulations is a plus.
Familiarity with programming languages such as Python or R is an advantage.
Show more
Show less","Data Analysis, Data Visualization, Power BI, SQL, Data Collection, Data Cleaning, Data Modeling, Data Interpretation, Problem Solving, Critical Thinking, Communication, Data Governance, Privacy Regulations, Python, R","data analysis, data visualization, power bi, sql, data collection, data cleaning, data modeling, data interpretation, problem solving, critical thinking, communication, data governance, privacy regulations, python, r","communication, critical thinking, data cleaning, data collection, data governance, data interpretation, dataanalytics, datamodeling, powerbi, privacy regulations, problem solving, python, r, sql, visualization"
Data Analyst,SmartIPlace,United States,https://www.linkedin.com/jobs/view/data-analyst-at-smartiplace-3744862049,2023-12-17,Alberta, Canada,Associate,Remote,"Note:- I only need profiles who has worked with Microsoft Client in past.
Title - Data Analyst
Location - Remote
Visa- GC USC TN GC EAD OPT EAD H4 EAD
Experience: 10+Years
Job Description
Exp 10+ Mandatory
The main function of a Data Analyst is to coordinate changes to computer databases, test, and implement the database applying knowledge of database management systems.
Job Responsibilities
Work with senior management, technical and client teams in order to determine data requirements, business data implementation approaches, best practices for advanced data manipulation, storage and analysis strategies
Write and code logical and physical database descriptions and specify identifiers of database to management system or direct others in coding descriptions
Design, implement, automate and maintain large scale enterprise data ETL processes
Modify existing databases and database management systems and/or direct programmers and analysts to make changes
Test programs or databases, correct errors and make necessary modifications
Skills
Experience with database technologies
Knowledge of the ETL process
Knowledge of at least one scripting language
Strong written and oral communication skills
Strong troubleshooting and problem-solving skills
Demonstrated history of success
Desire to be working with data and helping businesses make better data driven decisions
Education/Experience
Bachelor's degree in a technical field such as computer science, computer engineering or related field required
10+ years applicable experience required""
""
Years of Experience Required – 10+ years of experience
Degrees or certifications required – none required, portfolios are recommended but not required
Disqualifiers – none
Best – Asks great questions and pulls their own data
Provides recommendations on the data
Advanced excell, python, and SQL skills
Able to provide stories based on the data
Experience in running case studies
Average –
Performance Indicators –
Delivery
Speed and quality
Collaboration and partnership
Msft systems| Required # Years of Experience
Excel | Required # Years of Experience
SQL
Python | Required # Years of Experience""
Show more
Show less","Data Analysis, Database Management Systems, ETL, Scripting Languages, Troubleshooting, Problem Solving, Data Manipulation, Data Storage, Data Analysis Strategies, Database Descriptions, Database Implementation, Enterprise Data ETL, DataDriven Decision Making, Excel, SQL, Python, Case Studies","data analysis, database management systems, etl, scripting languages, troubleshooting, problem solving, data manipulation, data storage, data analysis strategies, database descriptions, database implementation, enterprise data etl, datadriven decision making, excel, sql, python, case studies","case studies, data analysis strategies, data manipulation, data storage, dataanalytics, database descriptions, database implementation, database management systems, datadriven decision making, enterprise data etl, etl, excel, problem solving, python, scripting languages, sql, troubleshooting"
Data Analyst,Steneral Consulting,United States,https://www.linkedin.com/jobs/view/data-analyst-at-steneral-consulting-3746274421,2023-12-17,Alberta, Canada,Associate,Remote,"Share only 2 profiles
This is CTH role. Only GC or USC
Must have a BA degree and a Linkedin Profile with a profile photo.
Role: Data Analyst
Duration: 6 month contract to hire
Location: Chicago, IL ( can be remote )
Interview: Video
Looking for candidate who have strong experience with SQL, PySpark, PowerBI and working with large data sets.
Must Have-
Bachelors Degree
App/web site related- have experience working with digital data
Experience working in sql and pyspark and powerbi - Working with large data sets
Must be strong with data charts, dashboards, and filtering complex data sets
Nice to Have-
Airline/ecommerce - BIG PLUS
Plantir Foundry
We are seeking an experienced analyst to conduct performance measurement, analysis and reporting of our digital channels and products. You will work with multiple technology and product teams to measure the performance and effectiveness of our retail products, develop insights about existing customer experience, and suggest improvements that potentially enhance how customers interact with our brand. The ideal candidate will have a strong understanding of commercial and digital analytics and be able to use data to drive strategic decision-making.
Performance reporting & financial oversight: provide management with timely, accurate and concise reporting on the performance of digital division initiatives, goal attainment, and overall IT division progress to achievement of enterprise goals
Provide analyses facilitating the creation of business cases for digitally sponsored projects
Provide business insights based upon data to further drive digital revenue growth and optimization
Perform ad hoc analyses
Show more
Show less","SQL, PySpark, PowerBI, Data Visualization, Data Analytics, Data Analysis, Data Reporting, Data Dashboards, Data Mining, Data Interpretation, Digital Analytics, Commercial Analytics, Performance Measurement, Performance Reporting, Business Intelligence, Business Insights, Ad Hoc Analysis, DataDriven Decision Making","sql, pyspark, powerbi, data visualization, data analytics, data analysis, data reporting, data dashboards, data mining, data interpretation, digital analytics, commercial analytics, performance measurement, performance reporting, business intelligence, business insights, ad hoc analysis, datadriven decision making","ad hoc analysis, business insights, business intelligence, commercial analytics, data dashboards, data interpretation, data mining, data reporting, dataanalytics, datadriven decision making, digital analytics, performance measurement, performance reporting, powerbi, spark, sql, visualization"
Data Analyst - Sales Engineer,Peel Insights,United States,https://www.linkedin.com/jobs/view/data-analyst-sales-engineer-at-peel-insights-3787506411,2023-12-17,Alberta, Canada,Associate,Remote,"For Shopify merchants who know the value of data, Peel is the automated analytics platform that helps you grow faster by making it easy to see what’s happening and why.
We are a product-driven team reinventing the way ecommerce teams access and comprehend data analysis and are working to fulfill our vision to make data simple, reliable, and visually intuitive for everyone.
Peel is a Seed stage company started in New York with a fully remote team in California, Texas, Brazil, Costa Rica, and Uruguay. We are backed by some of the best investors in ecommerce tech.
About The Role
We are looking to hire our first Data Analyst in a Sales Engineer role. Their role will be to join our team and lead in-depth data analysis, strategic query handling, and effective communication with customers. This role involves collaborating closely with both the customer success and product teams to enhance our BI capabilities and provide valuable insights to our most active customers.
Our users are passionate about truly understanding their best customers and this role is for someone curious with technical skills to work with them and the customer success team.
What you'll do
You will work on our web-based analytics product (See it here on Shopify) and with all the data we have available
Work in data, whether it is on Peel or with files, code, or database to analyze unique situations and offer guidance
You will conduct advanced analyses to provide comprehensive solutions each day
Generate detailed, customer-friendly summaries of findings to enhance understanding, decision-making and confidence in the data and reports
Meet with some of our customers to understand their special projects when they’re too technical for our CS team
Document issues in our data processing or dashboards for the engineering team to improve our existing solutions
Investigate possible differences between the custom reports our users migrate from and the ones Peel produces
Requirements
What you'll need
Exceptional written communication skills in English, with the ability to convey technical concepts clearly and concisely
Great listener capable of translating non-technical requests into meaningful data queries and vice versa. Strong teaching ability to simplify technical explanations for broader comprehension
Proven experience working on data analysis
Understanding of financial business metrics such as revenue, costs, profit, churn, etc
Comfort handling large datasets across CSV and spreadsheet files, Google Sheet or Excel, and databases (MySQL, Snowflake, Redshift)
Happy in a SQL console environment. Strong SQL querying skills to access data and create reports
Some experience as a user of data visualizations (Looker, Tableau, or other applications)
You’re excited to communicate with customers and teammates to learn about each situation
Advanced understanding for relational data models to avoid pitfalls from querying many-to-many entries and creating summaries on filtered datasets
Bonus: Ability to read code and proficiency in writing data processing scripts in Python, R, JSON, or other relevant languages
Bonus: Interest and experience in ecommerce, including working with Shopify and Amazon data
Benefits
Competitive salary
Generous stock options (Seed stage)
Health benefits
Flexible vacation, paid company holidays, and parental leave
Team bonding activities even in our remote worlds
Laptop and other equipment fundamentals; monitor, keyboard, and mouse
Endless learning opportunities
Show more
Show less","Data Analysis, Data Visualization, SQL, MySQL, Snowflake, Redshift, CSV, Spreadsheet, Google Sheet, Excel, Looker, Tableau, Python, R, JSON, AI, Machine Learning, Business Intelligence, Ecommerce, Shopify, Amazon","data analysis, data visualization, sql, mysql, snowflake, redshift, csv, spreadsheet, google sheet, excel, looker, tableau, python, r, json, ai, machine learning, business intelligence, ecommerce, shopify, amazon","ai, amazon, business intelligence, csv, dataanalytics, ecommerce, excel, google sheet, json, looker, machine learning, mysql, python, r, redshift, shopify, snowflake, spreadsheet, sql, tableau, visualization"
Data Analyst ( 99% Remote),SmartIPlace,United States,https://www.linkedin.com/jobs/view/data-analyst-99%25-remote-at-smartiplace-3755291662,2023-12-17,Alberta, Canada,Associate,Remote,"Position: Data Profiler / Data Analyst
(99% remote)
Location: Houston, TX
Duration: 12+ months
Visa: GC,USC
Experience: 12+years
Skills
Data Analysis.
SQL.
Show more
Show less","Data Analysis, SQL","data analysis, sql","dataanalytics, sql"
Data Analyst Junior - Entry Level - Junior,Data Nomad,United States,https://www.linkedin.com/jobs/view/data-analyst-junior-entry-level-junior-at-data-nomad-3782714419,2023-12-17,Alberta, Canada,Associate,Remote,"Join us as we expand our team with the addition of a Remote Data Entry Clerk. In this position, you will oversee data entry tasks, ensuring their punctual completion and the preservation of high-quality standards. Proficiency in data entry roles, razor-sharp analytical abilities, and the ability to collaborate seamlessly with diverse teams constitute the bedrock of this role. If you are a proactive individual armed with exceptional data entry skills, we wholeheartedly encourage your participation in our organization's journey to achievement, all within the convenience of your remote workspace.
Responsibilities
Precisely input information into computer systems and databases.
Thoroughly validate and examine data for errors or inconsistencies.
Safeguard the integrity and confidentiality of data.
Organize and categorize documents in preparation for data input.
Cooperate with colleagues to guarantee the accuracy and uniformity of data.
Adhere to predefined protocols and standards for data entry.
Execute data entry assignments within designated deadlines.
Requirements
Skilled in Microsoft Office Suite, especially Excel and Word
Outstanding typing speed with precision
Possession of a high school diploma or its equivalent, along with exceptional typing skills
Adept in the utilization of data entry software and tools
Demonstrated precision, strong organizational capabilities, and the competence to detect and rectify errors effectively
Capable of working both autonomously and collaboratively with minimal oversight
Proficiency in Microsoft Office Suite, specifically Excel and Word applications
Aptitude for managing sensitive information with integrity
We look forward to welcoming you to our team and witnessing the valuable contributions you'll make in this role. Apply today to embark on an exciting journey of data excellence from the comfort of your own workspace
Show more
Show less","Data Entry, Microsoft Office Suite, Excel, Word, Data Entry Software, Data Validation, Error Detection, Error Correction, Data Organization, Data Categorization, Data Integrity, Data Confidentiality, Data Entry Standards, Deadlines, Typing Speed, Accuracy, High School Diploma or Equivalent, Precision, Organizational Skills, Autonomy, Collaboration, Sensitive Information Management","data entry, microsoft office suite, excel, word, data entry software, data validation, error detection, error correction, data organization, data categorization, data integrity, data confidentiality, data entry standards, deadlines, typing speed, accuracy, high school diploma or equivalent, precision, organizational skills, autonomy, collaboration, sensitive information management","accuracy, autonomy, collaboration, data categorization, data confidentiality, data entry, data entry software, data entry standards, data integrity, data organization, data validation, deadlines, error correction, error detection, excel, high school diploma or equivalent, microsoft office suite, organizational skills, precision, sensitive information management, typing speed, word"
Sr. Data Analyst,Accroid Inc,United States,https://www.linkedin.com/jobs/view/sr-data-analyst-at-accroid-inc-3745554657,2023-12-17,Alberta, Canada,Associate,Remote,"Requirement Details
The data analyst role is all about learning and understanding the data in order to become data stewards by ensuring data quality and accuracy then mastering how to manipulate the data to
Skills and Experience
Bachelor's degree in a relevant field of study or comparative experience
Strong analytical and problem solving skills
Great verbal and written communication skills
Understanding of data management concepts
Advanced with SQL (T-SQL and Postgres tech stack)
Advanced skills in Business Intelligence report creation and maintenance (PowerBI preferred)
DAX
Paginated Reports
Familiarity with Python, Spark, and/or Databricks is a bonus
Role and Responsibilities
Help develop reports and analysis
Aid business and product partners in executing queries
Perform data quality and integrity analysis on data feeds
Perform analysis on feasibility and value when exploring additional data elements to be integrated with current datasets
Create and maintain a living data dictionary
Manage data lake structure and utilization
Show more
Show less","SQL, TSQL, Postgres, PowerBI, DAX, Paginated Reports, Python, Spark, Databricks, Data warehouse, Data Stewardship, Business Intelligence, Data mining, Data management, Data analysis, Data quality, Data integration, Data dictionary, Data lake","sql, tsql, postgres, powerbi, dax, paginated reports, python, spark, databricks, data warehouse, data stewardship, business intelligence, data mining, data management, data analysis, data quality, data integration, data dictionary, data lake","business intelligence, data dictionary, data integration, data lake, data management, data mining, data quality, data stewardship, dataanalytics, databricks, datawarehouse, dax, paginated reports, postgres, powerbi, python, spark, sql, tsql"
Healthcare Data Analyst - US,Zortech Solutions,United States,https://www.linkedin.com/jobs/view/healthcare-data-analyst-us-at-zortech-solutions-3773337546,2023-12-17,Alberta, Canada,Associate,Remote,"Role: Healthcare Data Analyst
Location: Remote/Eden Prairie, MN/US
Duration: 6-12+ Months
Job Description
Minimum 10 Years of IT exp
Healthcare industry experience, particularly familiarity with Medicaid & Medicare claims, member data.
Experience with overseeing collection, storage and use of data, preparing data to share
Experience with Cloud technology, such as Snowflake, Databricks, ADF
Work history demonstrating excellent interpersonal, verbal, and written communication skills
Show more
Show less","Cloud technology, Snowflake, Databricks, ADF, Medicaid, Medicare","cloud technology, snowflake, databricks, adf, medicaid, medicare","adf, cloud technology, databricks, medicaid, medicare, snowflake"
Urgent Role - Cloud Data Analyst || Remote,Steneral Consulting,United States,https://www.linkedin.com/jobs/view/urgent-role-cloud-data-analyst-remote-at-steneral-consulting-3714631366,2023-12-17,Alberta, Canada,Associate,Remote,"Title:
Cloud Data Analyst
Location:
Remote
Duration: 3-6 Months
Description
100% Remote
Candidates need to take a video screen with prime vendor prior to end client submittal
Required Information for Submittals (need candidate response) -4 bullet points on candidate experience:
BI reporting exp.
ETL experience
Experience With Business Requirements Gathering.
Experience using AWS Redshift
Knockout Questions (need candidate response)
Provide an example of a past project where they used redshift to pull data from a system using an ETL process.
What reports have you had to create in a BI reporting tool for the business?
Have you worked with business users or stakeholders to identify business requirements and what reports they need?
Understanding Of Stakeholder Needs
The candidate should mention their approach to understanding the needs and objectives of stakeholders, including business users or project managers.
Data Exploration
They should discuss how they explore and analyze existing data sources, if available, to get a sense of the data landscape.
Required Skills
Experience pulling data from a system using Redshift and ETL process
Experience creating complex BI Reports and using BI reporting tools (Tableau/PowerBI and QuickSight)
Business requirements gathering and translating that into technical requirements (working with both the business and technical teams)
Strong knowledge in SQL
Reason for Opening
They have an old legacy system(Ultimate) they need to pull data from and create reports for stakeholders.
Business Objectives & Key Project Details
They are starting to utilize BI reporting tools. They have an old legacy system they need to pull data from into a Datawarehouse so they can create reports. They will be using AWS Redshift to do this along with ETL processes and using either Tableau or PowerBI to create the reports.
% breakdown of what they will be doing
They will be working with stakeholders (Purchasing managers) to gather business requirements and what reports they need.
Work with the technical team to translate those business requirements into what Data they need to pull
They will be using the raw data they get from the data engineers to do ETL work into their BI reporting tool to create the reports for stakeholders.
They will be creating a new ETL process on how to do that within NPW.
They will be creating reports for Purchasing, invoice, inventory data, etc.
Position Overview
As a Cloud Data Analyst you will play a pivotal role in designing, implementing, and maintaining our data analysis processes, utilizing AWS cloud services, Redshift, SQL, and QuickSight. You will collaborate closely with cross-functional teams to ensure our data-driven decisions are accurate, insightful, and support our business objectives. This role demands a proactive problem solver with a deep understanding of data analysis techniques, cloud architecture, and data visualization tools.
Key Responsibilities
Utilize your expertise in AWS services (or similar cloud platforms), Redshift, SQL, and QuickSight to design and develop robust data pipelines, ETL processes, and data models.
Collaborate with data engineers, data scientists, and business stakeholders to understand data requirements and translate them into actionable insights.
Write and optimize complex SQL queries to extract, transform, and load data from various sources into our Redshift data warehouse.
Develop interactive and visually appealing dashboards and reports using QuickSight to convey meaningful insights to business users.
Monitor and maintain the performance, availability, and security of our data infrastructure, identifying and resolving issues promptly.
Stay up to date with the latest industry trends, best practices, and emerging technologies related to cloud data analysis and visualization.
Requirements
Bachelor's degree in computer science, Information Technology, or a related field.
Proven experience in cloud data analysis, preferably using AWS services (or similar cloud platforms).
Strong proficiency in SQL and experience with database design and optimization.
Familiarity with data warehousing concepts and working knowledge of Amazon Redshift or similar data warehousing solutions.
Experience with Pick Basic systems and Rocket Universe database preferred but not required.
Proficiency in data visualization tools such as Quicksight, Tableau, or Power BI.
Excellent problem-solving skills and the ability to analyze complex data sets.
Effective communication skills, both written and verbal, to collaborate with technical and non-technical stakeholders.
Strong attention to detail and a commitment to producing high-quality work.
AWS certification(s) is a plus, but not mandatory.
Thanks & Regards,
Deepak Pandey
Talent Acquisition - North America
deepak@steneral.com
Show more
Show less","AWS, Redshift, SQL, QuickSight, Tableau, Power BI, ETL, BI reporting, Business requirements gathering, Data exploration, Rocket Universe, Pick Basic systems, Data warehousing, Data analysis, Data modeling","aws, redshift, sql, quicksight, tableau, power bi, etl, bi reporting, business requirements gathering, data exploration, rocket universe, pick basic systems, data warehousing, data analysis, data modeling","aws, bi reporting, business requirements gathering, data exploration, dataanalytics, datamodeling, datawarehouse, etl, pick basic systems, powerbi, quicksight, redshift, rocket universe, sql, tableau"
Remote Job--Data Analyst,Accroid Inc,United States,https://www.linkedin.com/jobs/view/remote-job-data-analyst-at-accroid-inc-3782784553,2023-12-17,Alberta, Canada,Associate,Remote,"Data Analyst
12+ Months
Bachelor's degree in Data Science, Statistics, Mathematics, Business, or a related field.
Proven experience as a Data Analyst, preferably in the context of incentive compensation management or a related field.
Proficiency in data manipulation and analysis using tools such as
Excel, SQL, and Informatica ETL
Strong analytical and problem-solving skills with attention to detail.
Show more
Show less","Data Analysis, Data Manipulation, Excel, SQL, Informatica ETL, Data Science, Statistics, Mathematics, Business, Problemsolving, Attention to Detail","data analysis, data manipulation, excel, sql, informatica etl, data science, statistics, mathematics, business, problemsolving, attention to detail","attention to detail, business, data manipulation, data science, dataanalytics, excel, informatica etl, mathematics, problemsolving, sql, statistics"
Programmer/Data Analyst,Kellton,United States,https://www.linkedin.com/jobs/view/programmer-data-analyst-at-kellton-3750867026,2023-12-17,Alberta, Canada,Associate,Remote,"Position: Programmer/Data Analyst
Remote Role
Major Job Function
Data Analysis and Interpretation
○ Perform exploratory data analysis to identify patterns, trends, and anomalies within datasets.
○ Develop web services for accessing the data.
○ Develop and execute queries using Elastic (or OpenSearch) for efficient data retrieval
Collaboration And Communication
○ Work closely with cross-functional teams to understand business requirements and provide data-driven insights.
○ Collaborate with data engineers to ensure data availability, quality, and reliability.
○ Proven experience as a Data Analyst with a strong command of Python programming.
○ Hands-on experience with Elastic (or OpenSearch) for data indexing and search is preferred.
○ Familiarity with AWS services such as S3, Redshift, Glue, Athena, etc.
○ Strong analytical and problem-solving skills with the ability to work with large datasets.
○ Excellent communication and collaboration skills to interact effectively with various teams.
Other Requirements
Need someone strong on OpenSearch / Elastic Search with Python
Ability to work with large volumes of Data collections capturing the telematics data that gets stored into the database to be organized, queried and supplied down the stream to various apps that consume the data.
This person needs to guide the rest of the team and take a leadership role, on the Open Search space.
Location does not matter, but need to be available for a minimum 4 hours (initially more overlap) in the US Eastern Time zone for knowledge sharing ideation etc.
Education
Bachelors degree in a relevant field such as Computer Science, Data Science, Statistics, Engineering or related disciplines.
Show more
Show less","Data Analysis, Data Interpretation, Exploratory Data Analysis, Elastic, OpenSearch, Queries, Python, AWS, Redshift, Glue, Athena, S3, Analytical skills, Problemsolving skills, Large Datasets, Communication skills, Collaboration skills, Team Work","data analysis, data interpretation, exploratory data analysis, elastic, opensearch, queries, python, aws, redshift, glue, athena, s3, analytical skills, problemsolving skills, large datasets, communication skills, collaboration skills, team work","analytical skills, athena, aws, collaboration skills, communication skills, data interpretation, dataanalytics, elastic, exploratory data analysis, glue, large datasets, opensearch, problemsolving skills, python, queries, redshift, s3, team work"
Programmer Analyst/Data Analyst,Sonitalent Corp,United States,https://www.linkedin.com/jobs/view/programmer-analyst-data-analyst-at-sonitalent-corp-3728264747,2023-12-17,Alberta, Canada,Associate,Remote,"Role: Programmer Analyst/Data Analyst
Location: REMOTE
MUST Have
Experience with Identity and Access Management in a large organization.
IAM familarity
Well versed with MySQL database queries and creation of database views.
Development experience with REST, SOAP, LDAP, MySQL
Development experience with Java
Show more
Show less","Programmer Analyst, Data Analyst, Identity and Access Management, IAM, MySQL, REST, SOAP, LDAP, Java","programmer analyst, data analyst, identity and access management, iam, mysql, rest, soap, ldap, java","dataanalytics, iam, identity and access management, java, ldap, mysql, programmer analyst, rest, soap"
Software Developer & Data Analyst,IBSS,United States,https://www.linkedin.com/jobs/view/software-developer-data-analyst-at-ibss-3781197955,2023-12-17,Alberta, Canada,Associate,Remote,"Job Title: Software Developer & Data Analyst
Location: Remote (PST working hours)
Clearance Required: Public Trust
Description (scope of work)
IBSS is seeking a Full-Time Software Developer to assist the Alaska Fisheries Science Center (AFSC) in providing a full range of services and products to support the development, implementation, and maintenance of technology that aligns with the AFSC mission and business functions across various programs. While data management is an essential part of this role, the primary focus is on acting as a full-stack developer with the ability to identify and communicate potential improvements for the future. The role also involves creating comprehensive technical documentation to facilitate efficient project execution.
Key Responsibilities:
Design and build user interfaces, data structures, and processes for program users to populate databases.
Assist users with data acquisition from primary or secondary data sources and maintain data systems and integrity.
Develop innovative approaches, code, and procedures to enhance data input, output, reporting, query, data manipulation, and data sharing capabilities to enable AFSC users to improve scientific collaboration.
Work with development and program staff to ensure the integrity and proper integration of all sources of AFSC data.
Provide software coding, including guidance and quality adherence in languages such as Python, Java, R, JavaScript, SQL, PL/SQL, MS Access, etc.
Analyze and document existing business processes and software applications.
Analyze and document user needs and requirements and translate them into technical system requirements.
Offer advice and education to staff on business process or application improvement options.
Understand and support existing business processes, software applications, and databases.
Assist users with identifying and resolving data-related production issues, data modeling, storage, migration, translation, and reporting.
Coordinate and assist AFSC staff with evaluation, testing, and migration of data to newer versions of database management systems and software, as needed.
Work with AFSC customers to assess their needs, provide information or assistance, resolve their problems, or satisfy their expectations under the guidance of AFSC staff.
Assist the application development and data management staff to plan and execute activities, promoting and using software development best practices, has the structure, processes, appropriate requirements, technical specifications, direction, and has the resources to deliver effective solutions.
Translate user needs and requirements into technical system requirements.
Recommend improvements, remediation, or requirements for projects.
Required Skills /Education/ Certifications & Qualifications:
Essential Qualifications and Skills
Bachelor's Degree in Computer Science/Engineering or related field and six years of working experience.
Familiarity with different diagrams such as class diagrams, object diagrams, sequence diagrams, use case diagrams, activity diagrams, state machines, package diagrams, component diagrams, and deployment diagrams.
Proficiency with gathering requirements and communicating with key stakeholders
Comfortable with unit testing and UAT
Comprehensive knowledge of at least one OOP programming language to implement a given diagram., which involves working with various diagrams and implementing them using programming languages.
Extensive knowledge with SQL and NoSQL and being able to implement Entity-Relationship Diagrams (ERD), Database Schema Diagrams, Table Relationship Diagrams, Data Flow Diagram, Normalization Diagram, Dependency Diagram, Hierarchical Diagram, UML Class Diagrams (for Object-Relational Mapping), Index Diagram.
Familiarity with any cloud environment ( AWS/GCP/Azure ), or being certified.
Technical Skills and Experience
Experience performing full-stack (web/app-development) activities, including coding, testing, debugging, documenting, troubleshooting, maintaining, and modifying application systems, with an emphasis on security, customer experience, mobility, and service reliability.
Knowledge of full-stack data-driven application development and software development cycle, including software architectural design, data integration from various sources, building backend services, development of highly interactive web applications, software deployment using continuous delivery, and monitoring to ensure high availability.
Experience with Excel/Google spreadsheets and programmatic ETL methods, & experience graphing in Excel.
Experience with designing, writing, testing, and reviewing code using Figma/Draw.io, GitHub & Kanban boards, Project Management, and ticketing systems.
Ability to produce specifications and determine operational feasibility.
Ability to integrate software components into a fully functional system.
Develop software test plans, verification plans, and quality assurance procedures.
Ability to work on existing software, including learning the system, maintaining, troubleshooting, debugging, and contributing to the system.
Problem solver with a keen eye for detail.
Ability to actively participate in various problem-solving efforts and processes. (offline-first, design thinking).
Experience with project management (agile/scrum) and contributing to progressive frameworks for the client-side, server, and database software development and delivery (progressive web apps).
Soft Skills and Professionalism
Excellent communication and interpersonal skills.
Professionalism in all manners of business intercourse, including meetings with clients, conference calls, emails, etc.
Flexible, eager, ambitious, and adaptable to change.
Understands the importance of good documentation.
About IBSS Corp.
Since 1992, IBSS, a woman-owned small business, has provided transformational consulting services to the Federal defense, civilian, and commercial sectors. Our services include cybersecurity and enterprise information technology, environmental science and engineering (including oceans, coasts, climate, and weather), and professional management services.
Our approach is to serve our employees by investing in their growth and development. As a result, our employees bring greater capabilities and provide an exceptional level of service to our clients. In addition to creating career development opportunities for our employees, IBSS is passionate about giving back to the community and serving the environment. We strive to leave something better behind for the next generation.
We measure our success by the positive impact we have on our employees, clients, partners, and the communities we serve. Our tagline, Powered by Excellence, is a recognition of the employees that make up IBSS and ensures we deliver results with quality, applying industry best practices and certifications.
IBSS offers a competitive benefits package including medical, dental, vision and prescription drug coverage with company-paid deductible, paid time off, federal holidays, matching 401K plan, tuition/professional development reimbursement, and Flex-Spending (FSA)/Dependent Care Account (DCA) options.
IBSS is an affirmative action and equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, disability, age, sexual orientation, gender identity, national origin, veteran status, or genetic information. Click  https://www.eeoc.gov/poster to see, The EEO is the law.
If you require reasonable accommodation in completing this application, interviewing, completing any pre-employment testing, or otherwise participating in the employee selection process, please direct your inquiries to the HR Department, Francesca Urrutia at (703) 826-4302, or email at HR@ibsscorp.com
Show more
Show less","Python, Java, R, JavaScript, SQL, PL/SQL, MS Access, OOP programming languages, UML, AWS, GCP, Azure, Figma, Draw.io, GitHub, Kanban, Scrum, Agile","python, java, r, javascript, sql, plsql, ms access, oop programming languages, uml, aws, gcp, azure, figma, drawio, github, kanban, scrum, agile","agile, aws, azure, drawio, figma, gcp, github, java, javascript, kanban, ms access, oop programming languages, plsql, python, r, scrum, sql, uml"
Data Analyst III,WinMax,United States,https://www.linkedin.com/jobs/view/data-analyst-iii-at-winmax-3728859338,2023-12-17,Alberta, Canada,Associate,Remote,"Title:Data Analyst III, Req# 26410337
Location:Remote (CA)
Contract:12+ Month
Job Description
In this engagement, you will act as a liaison between various stakeholders, including in-
house legal teams, outside counsel firms, and cross-functional client business groups,
managing and delivering on often large-scale data projects. You will be responsible for
understanding the scope of client systems, collection, and processing of datasets, along
with interpretation and analysis of the data by applying available technologies.
Key Qualifications
Managing and executing the identification and processing of large datasets of
client customer data relevant to litigation and investigations.
Experience working in a legal environment with attorneys, paralegals/legal
specialists, and eDiscovery operations.
Understanding of the data and processes associated with customer transactions
and support interactions.
Experience working with enterprise data management tools (Tableau, Business
Objects, BBEdit).
Demonstrates sound judgment, discretion, and ability to properly handle
confidential and sensitive information.
Ability to work independently and proactively to meet aggressive timelines in a
dynamic environment, including managing and prioritizing deliverables with
Show more
Show less","Data Analysis, Tableau, Business Objects, BBEdit, Data Management, Litigation, Investigations, eDiscovery, Confidentiality, Time Management, Prioritization","data analysis, tableau, business objects, bbedit, data management, litigation, investigations, ediscovery, confidentiality, time management, prioritization","bbedit, business objects, confidentiality, data management, dataanalytics, ediscovery, investigations, litigation, prioritization, tableau, time management"
Senior Data Analyst (At least 10+ Year of exp),"Donato Technologies, Inc.",United States,https://www.linkedin.com/jobs/view/senior-data-analyst-at-least-10%2B-year-of-exp-at-donato-technologies-inc-3716390930,2023-12-17,Alberta, Canada,Associate,Remote,"Job Title: Senior Data Analyst / Senior Advanced Analytics Developer
Location: 100% Remote (Consultant should be located in any of these states Nevada, Illinois, Indiana, Iowa, Kansas, Louisiana, Mississippi, Missouri, Ohio, and Pennsylvania)
Duration: 6 Months Contract (Possibility to long term extension)
Key Skills: Data Analysis & PowerBI, Sql, Data exploration, Predictive modeling, and Statistical analysis, Python or R.
Note: Las Vegas Preferred. Minimal Travel will be required to Las Vegas, if not already in Las Vegas. Consultant should be located in a these state (Nevada, Illinois, Indiana, Iowa, Kansas, Louisiana, Mississippi, Missouri, Ohio, and Pennsylvania)
Description
We are seeking a highly skilled and results-driven Senior Data Analyst to spearhead our advanced analytics initiatives. As the Senior Data Analyst, you will be responsible for driving our organization's data-driven decision-making processes by leveraging advanced analytical techniques and cutting-edge technologies. Your role will involve leading a team of talented analysts, collaborating with cross-functional teams, and providing strategic guidance to uncover valuable insights from complex data sets.
Responsibilities
Lead the development and execution of advanced analytics projects, including data exploration, predictive modeling, and statistical analysis.
Define the analytical roadmap, ensuring alignment with business objectives and priorities.
Work closely with stakeholders to understand business requirements and translate them into analytical solutions.
Conduct in-depth data analysis using various tools and techniques to uncover trends, patterns, and insights.
Develop and deploy predictive models, machine learning algorithms, and statistical models to support business decision-making.
Provide strategic guidance on data collection, data quality, and data integration best practices.
Collaborate with data engineering teams to ensure efficient data preparation and integration for advanced analytics initiatives.
Communicate complex analytical findings and insights to non-technical stakeholders in a clear and concise manner.
Stay updated with emerging trends, tools, and techniques in the field of advanced analytics and data science.
Mentor and coach junior analysts, providing guidance and support in their professional growth.
Requirements
Bachelor's or Master's degree in a quantitative field (e.g., Statistics, Mathematics, Computer Science, Data Science).
Proven experience in advanced analytics, data mining, predictive modeling, and statistical analysis.
Proficiency in programming languages such as Python or R for data manipulation, statistical modeling, and machine learning.
Strong understanding of statistical concepts and machine learning algorithms.
Experience working with large and complex datasets, using SQL or similar query languages.
Familiarity with data visualization tools such as Tableau, Power BI, or similar.
Excellent analytical thinking and problem-solving skills, with the ability to derive actionable insights from data.
Effective communication skills, with the ability to present complex ideas and findings to both technical and non-technical audiences.
Leadership qualities, with the ability to drive and manage advanced analytics projects from inception to implementation.
Strong organizational skills, with the ability to prioritize and manage multiple projects simultaneously.
Show more
Show less","Data Analysis, PowerBI, SQL, Data Exploration, Predictive Modeling, Statistical Analysis, Python, R, Machine Learning, Tableau, Data Visualization, Data Mining, Data Manipulation, Statistical Concepts, Statistical Algorithms, Programming, Data Warehousing, Data Integration, Data Collection, Data Preparation, Business Intelligence, Data Science","data analysis, powerbi, sql, data exploration, predictive modeling, statistical analysis, python, r, machine learning, tableau, data visualization, data mining, data manipulation, statistical concepts, statistical algorithms, programming, data warehousing, data integration, data collection, data preparation, business intelligence, data science","business intelligence, data collection, data exploration, data integration, data manipulation, data mining, data preparation, data science, dataanalytics, datawarehouse, machine learning, powerbi, predictive modeling, programming, python, r, sql, statistical algorithms, statistical analysis, statistical concepts, tableau, visualization"
Contract: Database Automation Engineer / DBA-DevOps,Upwork,"California, United States",https://www.linkedin.com/jobs/view/contract-database-automation-engineer-dba-devops-at-upwork-3739501687,2023-12-17,Alberta, Canada,Associate,Remote,"Upwork ($UPWK) is the world’s work marketplace. We serve everyone from one-person startups to large, Fortune 100 enterprises with a powerful, trust-driven platform that enables companies and talent to work together in new ways that unlock their potential.
Last year, more than $3.8 billion of work was done through Upwork by skilled professionals who are gaining more control by finding work they are passionate about and innovating their careers.
This is an engagement through Upwork’s Hybrid Workforce Solutions (HWS) Team. Our Hybrid Workforce Solutions Team is a global group of professionals that support Upwork’s business. Our HWS team members are located all over the world.
Work/Project Scope:
Provisioning
Maintenance
Right-scaling
Cost-effective use
Create and maintain vulnerability management policies, procedures, and training
Must Haves (Required Skills):
Relational database management experience (Postgres/MySQL/Oracle)
Proficiency with database languages: SQL, PL/SQL or pgPL/SQL.
Scripting: Strong experience with Python (preferred), shell (secondary)
On-call assistance with DB-related incidents.
Automation mindset: Desire and ability to automate repetitive tasks.
Nice to Haves:
Cloud management: Experience with Terraform (CloudFormation, Hashicorp Packer, Chef/Ansible).
Technologies: Kafka / Kinesis (on-prem or managed), ElasticSearch/OpenSearch/Mongo, Redis/Memcache
Databases: Analytical databases like Snowflake/Clickhouse/Greenplum, data federation engines like Presto/Trino/Dremio/Athena.
Upwork is proudly committed to fostering a diverse and inclusive workforce. We never discriminate based on race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical condition), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.
To learn more about how Upwork processes and protects your personal information as part of the application process, please review our Global Job Applicant Privacy Notice
Show more
Show less","Relational database management, PostgreSQL, MySQL, Oracle, SQL, PL/SQL, pgPL/SQL, Python, Shell scripting, Automation, Terraform, CloudFormation, Hashicorp Packer, Chef, Ansible, Kafka, Kinesis, ElasticSearch, OpenSearch, Mongo, Redis, Memcache, Snowflake, Clickhouse, Greenplum, Presto, Trino, Dremio, Athena","relational database management, postgresql, mysql, oracle, sql, plsql, pgplsql, python, shell scripting, automation, terraform, cloudformation, hashicorp packer, chef, ansible, kafka, kinesis, elasticsearch, opensearch, mongo, redis, memcache, snowflake, clickhouse, greenplum, presto, trino, dremio, athena","ansible, athena, automation, chef, clickhouse, cloudformation, dremio, elasticsearch, greenplum, hashicorp packer, kafka, kinesis, memcache, mongo, mysql, opensearch, oracle, pgplsql, plsql, postgresql, presto, python, redis, relational database management, shell scripting, snowflake, sql, terraform, trino"
Contract: Database Automation Engineer / DBA-DevOps,Upwork,"Washington, United States",https://www.linkedin.com/jobs/view/contract-database-automation-engineer-dba-devops-at-upwork-3739299756,2023-12-17,Alberta, Canada,Associate,Remote,"Upwork ($UPWK) is the world’s work marketplace. We serve everyone from one-person startups to large, Fortune 100 enterprises with a powerful, trust-driven platform that enables companies and talent to work together in new ways that unlock their potential.
Last year, more than $3.8 billion of work was done through Upwork by skilled professionals who are gaining more control by finding work they are passionate about and innovating their careers.
This is an engagement through Upwork’s Hybrid Workforce Solutions (HWS) Team. Our Hybrid Workforce Solutions Team is a global group of professionals that support Upwork’s business. Our HWS team members are located all over the world.
Work/Project Scope:
Provisioning
Maintenance
Right-scaling
Cost-effective use
Create and maintain vulnerability management policies, procedures, and training
Must Haves (Required Skills):
Relational database management experience (Postgres/MySQL/Oracle)
Proficiency with database languages: SQL, PL/SQL or pgPL/SQL.
Scripting: Strong experience with Python (preferred), shell (secondary)
On-call assistance with DB-related incidents.
Automation mindset: Desire and ability to automate repetitive tasks.
Nice to Haves:
Cloud management: Experience with Terraform (CloudFormation, Hashicorp Packer, Chef/Ansible).
Technologies: Kafka / Kinesis (on-prem or managed), ElasticSearch/OpenSearch/Mongo, Redis/Memcache
Databases: Analytical databases like Snowflake/Clickhouse/Greenplum, data federation engines like Presto/Trino/Dremio/Athena.
Upwork is proudly committed to fostering a diverse and inclusive workforce. We never discriminate based on race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical condition), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.
To learn more about how Upwork processes and protects your personal information as part of the application process, please review our Global Job Applicant Privacy Notice
Show more
Show less","Postgres, MySQL, Oracle, SQL, PL/SQL, pgPL/SQL, Python, Linux Shell, Terraform, CloudFormation, Hashicorp Packer, Chef, Ansible, Kafka, Kinesis, ElasticSearch, OpenSearch, Mongo, Redis, Memcache, Snowflake, Clickhouse, Greenplum, Presto, Trino, Dremio, Athena","postgres, mysql, oracle, sql, plsql, pgplsql, python, linux shell, terraform, cloudformation, hashicorp packer, chef, ansible, kafka, kinesis, elasticsearch, opensearch, mongo, redis, memcache, snowflake, clickhouse, greenplum, presto, trino, dremio, athena","ansible, athena, chef, clickhouse, cloudformation, dremio, elasticsearch, greenplum, hashicorp packer, kafka, kinesis, linux shell, memcache, mongo, mysql, opensearch, oracle, pgplsql, plsql, postgres, presto, python, redis, snowflake, sql, terraform, trino"
Data Quality Analyst_________________remote,Steneral Consulting,United States,https://www.linkedin.com/jobs/view/data-quality-analyst-remote-at-steneral-consulting-3699004803,2023-12-17,Alberta, Canada,Associate,Remote,"Hi,
Please find attached Job Description. If you are interested please do share with me your updated resume or call me on ""+1 3026017375"".
Job Title:- Data Quality Analyst (Marketing Data, Salesforce, MarTech, AdTech)
Work Location:- remote
Duration: 18+month
Work Authorization:- Citizen, GC
Interview : Zoom Video Call for out of town people, and in face for options who are local to Memphis.
Title- Data Quality Analyst (Marketing Data, Salesforce, MarTech, AdTech)
Location- REMOTE or Memphis, TN (local of Memphis, TN highly Preferred)
Linekdin must.
JD-
I am looking for candidates who have HANDS ON experience w/ MarTech and/or AdTech and IT SHOWS ON THE RESUME.
We are looking for a Data Quality Analyst excited about data and marketing technology. The Quality Analyst will be responsible for reviewing and ensuring the integrity and robustness of data, customer profile, Marketing-Tech, and Ad-Tech, and implementations through rigorous testing and validation methodologies.
Notes
Experience with marketing data is preferred but not a dealbreaker.
Experience with data QA is required.
Salesforce and/or Adobe MarTech stack experience is preferred but not required. (Salesforce would be more beneficial)
MarTech or AdTech background is preferred but not required
Automation experience is a nice to have. Selenium, uipath, etc
Requirements
Ability to design, execute, and maintain test cases, detect defects, collaborate with the development team to rectify issues, and provide feedback and insights to optimize ongoing development
Ability to interact well with team members and business users
Excellent written and verbal communications skills
Fast-paced adoption of new and emerging technologies
Understanding of software engineering principles and techniques
Salesforce and Adobe Marketing stacks, architecture and infrastructure
Experience with third party databases, libraries, interfaces, and internet protocols
Knowledge of LINUX, JavaScript, J2EE, Relational and Document Databases, JSON, Shell Scripting, automation
Kirti Rani
Associate Talent Acquisition -North America
Desk: +1 3026017375
kirti@steneral.com
In my absence please reach out to Mr. Harish Sharma at harish@steneral.com &
3027216151
Show more
Show less","Data Quality Analyst, MarTech, AdTech, Salesforce, Adobe MarTech, Linux, JavaScript, J2EE, Relational Databases, Document Databases, JSON, Shell Scripting, Automation, Selenium, UiPath","data quality analyst, martech, adtech, salesforce, adobe martech, linux, javascript, j2ee, relational databases, document databases, json, shell scripting, automation, selenium, uipath","adobe martech, adtech, automation, data quality analyst, document databases, j2ee, javascript, json, linux, martech, relational databases, salesforce, selenium, shell scripting, uipath"
Contract: Database Automation Engineer / DBA-DevOps,Upwork,"Texas, United States",https://www.linkedin.com/jobs/view/contract-database-automation-engineer-dba-devops-at-upwork-3739298808,2023-12-17,Alberta, Canada,Associate,Remote,"Upwork ($UPWK) is the world’s work marketplace. We serve everyone from one-person startups to large, Fortune 100 enterprises with a powerful, trust-driven platform that enables companies and talent to work together in new ways that unlock their potential.
Last year, more than $3.8 billion of work was done through Upwork by skilled professionals who are gaining more control by finding work they are passionate about and innovating their careers.
This is an engagement through Upwork’s Hybrid Workforce Solutions (HWS) Team. Our Hybrid Workforce Solutions Team is a global group of professionals that support Upwork’s business. Our HWS team members are located all over the world.
Work/Project Scope:
Provisioning
Maintenance
Right-scaling
Cost-effective use
Create and maintain vulnerability management policies, procedures, and training
Must Haves (Required Skills):
Relational database management experience (Postgres/MySQL/Oracle)
Proficiency with database languages: SQL, PL/SQL or pgPL/SQL.
Scripting: Strong experience with Python (preferred), shell (secondary)
On-call assistance with DB-related incidents.
Automation mindset: Desire and ability to automate repetitive tasks.
Nice to Haves:
Cloud management: Experience with Terraform (CloudFormation, Hashicorp Packer, Chef/Ansible).
Technologies: Kafka / Kinesis (on-prem or managed), ElasticSearch/OpenSearch/Mongo, Redis/Memcache
Databases: Analytical databases like Snowflake/Clickhouse/Greenplum, data federation engines like Presto/Trino/Dremio/Athena.
Upwork is proudly committed to fostering a diverse and inclusive workforce. We never discriminate based on race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical condition), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.
To learn more about how Upwork processes and protects your personal information as part of the application process, please review our Global Job Applicant Privacy Notice
Show more
Show less","Relational database management, SQL, PL/SQL, pgPL/SQL, Python, Shell scripting, Cloud management, Terraform, CloudFormation, Hashicorp Packer, Chef, Ansible, Kafka, Kinesis, ElasticSearch, OpenSearch, Mongo, Redis, Memcache, Snowflake, Clickhouse, Greenplum, Presto, Trino, Dremio, Athena","relational database management, sql, plsql, pgplsql, python, shell scripting, cloud management, terraform, cloudformation, hashicorp packer, chef, ansible, kafka, kinesis, elasticsearch, opensearch, mongo, redis, memcache, snowflake, clickhouse, greenplum, presto, trino, dremio, athena","ansible, athena, chef, clickhouse, cloud management, cloudformation, dremio, elasticsearch, greenplum, hashicorp packer, kafka, kinesis, memcache, mongo, opensearch, pgplsql, plsql, presto, python, redis, relational database management, shell scripting, snowflake, sql, terraform, trino"
Contract: Database Automation Engineer / DBA-DevOps,Upwork,"Maryland, United States",https://www.linkedin.com/jobs/view/contract-database-automation-engineer-dba-devops-at-upwork-3739296979,2023-12-17,Alberta, Canada,Associate,Remote,"Upwork ($UPWK) is the world’s work marketplace. We serve everyone from one-person startups to large, Fortune 100 enterprises with a powerful, trust-driven platform that enables companies and talent to work together in new ways that unlock their potential.
Last year, more than $3.8 billion of work was done through Upwork by skilled professionals who are gaining more control by finding work they are passionate about and innovating their careers.
This is an engagement through Upwork’s Hybrid Workforce Solutions (HWS) Team. Our Hybrid Workforce Solutions Team is a global group of professionals that support Upwork’s business. Our HWS team members are located all over the world.
Work/Project Scope:
Provisioning
Maintenance
Right-scaling
Cost-effective use
Create and maintain vulnerability management policies, procedures, and training
Must Haves (Required Skills):
Relational database management experience (Postgres/MySQL/Oracle)
Proficiency with database languages: SQL, PL/SQL or pgPL/SQL.
Scripting: Strong experience with Python (preferred), shell (secondary)
On-call assistance with DB-related incidents.
Automation mindset: Desire and ability to automate repetitive tasks.
Nice to Haves:
Cloud management: Experience with Terraform (CloudFormation, Hashicorp Packer, Chef/Ansible).
Technologies: Kafka / Kinesis (on-prem or managed), ElasticSearch/OpenSearch/Mongo, Redis/Memcache
Databases: Analytical databases like Snowflake/Clickhouse/Greenplum, data federation engines like Presto/Trino/Dremio/Athena.
Upwork is proudly committed to fostering a diverse and inclusive workforce. We never discriminate based on race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical condition), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.
To learn more about how Upwork processes and protects your personal information as part of the application process, please review our Global Job Applicant Privacy Notice
Show more
Show less","Python, SQL, PL/SQL, pgPL/SQL, Shell scripting, Postgres, MySQL, Oracle, Terraform, CloudFormation, Hashicorp Packer, Chef, Ansible, Kafka, Kinesis, ElasticSearch, OpenSearch, Mongo, Redis, Memcache, Snowflake, Clickhouse, Greenplum, Presto, Trino, Dremio, Athena","python, sql, plsql, pgplsql, shell scripting, postgres, mysql, oracle, terraform, cloudformation, hashicorp packer, chef, ansible, kafka, kinesis, elasticsearch, opensearch, mongo, redis, memcache, snowflake, clickhouse, greenplum, presto, trino, dremio, athena","ansible, athena, chef, clickhouse, cloudformation, dremio, elasticsearch, greenplum, hashicorp packer, kafka, kinesis, memcache, mongo, mysql, opensearch, oracle, pgplsql, plsql, postgres, presto, python, redis, shell scripting, snowflake, sql, terraform, trino"
Contract: Database Automation Engineer / DBA-DevOps,Upwork,"Oregon, United States",https://www.linkedin.com/jobs/view/contract-database-automation-engineer-dba-devops-at-upwork-3739501699,2023-12-17,Alberta, Canada,Associate,Remote,"Upwork ($UPWK) is the world’s work marketplace. We serve everyone from one-person startups to large, Fortune 100 enterprises with a powerful, trust-driven platform that enables companies and talent to work together in new ways that unlock their potential.
Last year, more than $3.8 billion of work was done through Upwork by skilled professionals who are gaining more control by finding work they are passionate about and innovating their careers.
This is an engagement through Upwork’s Hybrid Workforce Solutions (HWS) Team. Our Hybrid Workforce Solutions Team is a global group of professionals that support Upwork’s business. Our HWS team members are located all over the world.
Work/Project Scope:
Provisioning
Maintenance
Right-scaling
Cost-effective use
Create and maintain vulnerability management policies, procedures, and training
Must Haves (Required Skills):
Relational database management experience (Postgres/MySQL/Oracle)
Proficiency with database languages: SQL, PL/SQL or pgPL/SQL.
Scripting: Strong experience with Python (preferred), shell (secondary)
On-call assistance with DB-related incidents.
Automation mindset: Desire and ability to automate repetitive tasks.
Nice to Haves:
Cloud management: Experience with Terraform (CloudFormation, Hashicorp Packer, Chef/Ansible).
Technologies: Kafka / Kinesis (on-prem or managed), ElasticSearch/OpenSearch/Mongo, Redis/Memcache
Databases: Analytical databases like Snowflake/Clickhouse/Greenplum, data federation engines like Presto/Trino/Dremio/Athena.
Upwork is proudly committed to fostering a diverse and inclusive workforce. We never discriminate based on race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical condition), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.
To learn more about how Upwork processes and protects your personal information as part of the application process, please review our Global Job Applicant Privacy Notice
Show more
Show less","Postgres, MySQL, Oracle, SQL, PL/SQL, pgPL/SQL, Python, Shell, CloudFormation, Hashicorp Packer, Chef, Ansible, Terraform, Kafka, Kinesis, ElasticSearch, OpenSearch, Mongo, Redis, Memcache, Snowflake, Clickhouse, Greenplum, Presto, Trino, Dremio, Athena","postgres, mysql, oracle, sql, plsql, pgplsql, python, shell, cloudformation, hashicorp packer, chef, ansible, terraform, kafka, kinesis, elasticsearch, opensearch, mongo, redis, memcache, snowflake, clickhouse, greenplum, presto, trino, dremio, athena","ansible, athena, chef, clickhouse, cloudformation, dremio, elasticsearch, greenplum, hashicorp packer, kafka, kinesis, memcache, mongo, mysql, opensearch, oracle, pgplsql, plsql, postgres, presto, python, redis, shell, snowflake, sql, terraform, trino"
Contract: Database Automation Engineer / DBA-DevOps,Upwork,"Michigan, United States",https://www.linkedin.com/jobs/view/contract-database-automation-engineer-dba-devops-at-upwork-3739297861,2023-12-17,Alberta, Canada,Associate,Remote,"Upwork ($UPWK) is the world’s work marketplace. We serve everyone from one-person startups to large, Fortune 100 enterprises with a powerful, trust-driven platform that enables companies and talent to work together in new ways that unlock their potential.
Last year, more than $3.8 billion of work was done through Upwork by skilled professionals who are gaining more control by finding work they are passionate about and innovating their careers.
This is an engagement through Upwork’s Hybrid Workforce Solutions (HWS) Team. Our Hybrid Workforce Solutions Team is a global group of professionals that support Upwork’s business. Our HWS team members are located all over the world.
Work/Project Scope:
Provisioning
Maintenance
Right-scaling
Cost-effective use
Create and maintain vulnerability management policies, procedures, and training
Must Haves (Required Skills):
Relational database management experience (Postgres/MySQL/Oracle)
Proficiency with database languages: SQL, PL/SQL or pgPL/SQL.
Scripting: Strong experience with Python (preferred), shell (secondary)
On-call assistance with DB-related incidents.
Automation mindset: Desire and ability to automate repetitive tasks.
Nice to Haves:
Cloud management: Experience with Terraform (CloudFormation, Hashicorp Packer, Chef/Ansible).
Technologies: Kafka / Kinesis (on-prem or managed), ElasticSearch/OpenSearch/Mongo, Redis/Memcache
Databases: Analytical databases like Snowflake/Clickhouse/Greenplum, data federation engines like Presto/Trino/Dremio/Athena.
Upwork is proudly committed to fostering a diverse and inclusive workforce. We never discriminate based on race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical condition), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.
To learn more about how Upwork processes and protects your personal information as part of the application process, please review our Global Job Applicant Privacy Notice
Show more
Show less","Upwork, Python, SQL, PL/SQL, pgPL/SQL, Shell, Terraform, CloudFormation, Hashicorp Packer, Chef, Ansible, Kafka, Kinesis, ElasticSearch, OpenSearch, Mongo, Redis, Memcache, Snowflake, Clickhouse, Greenplum, Presto, Trino, Dremio, Athena, Postgres, MySQL, Oracle","upwork, python, sql, plsql, pgplsql, shell, terraform, cloudformation, hashicorp packer, chef, ansible, kafka, kinesis, elasticsearch, opensearch, mongo, redis, memcache, snowflake, clickhouse, greenplum, presto, trino, dremio, athena, postgres, mysql, oracle","ansible, athena, chef, clickhouse, cloudformation, dremio, elasticsearch, greenplum, hashicorp packer, kafka, kinesis, memcache, mongo, mysql, opensearch, oracle, pgplsql, plsql, postgres, presto, python, redis, shell, snowflake, sql, terraform, trino, upwork"
Contract: Database Automation Engineer / DBA-DevOps,Upwork,"Massachusetts, United States",https://www.linkedin.com/jobs/view/contract-database-automation-engineer-dba-devops-at-upwork-3739299758,2023-12-17,Alberta, Canada,Associate,Remote,"Upwork ($UPWK) is the world’s work marketplace. We serve everyone from one-person startups to large, Fortune 100 enterprises with a powerful, trust-driven platform that enables companies and talent to work together in new ways that unlock their potential.
Last year, more than $3.8 billion of work was done through Upwork by skilled professionals who are gaining more control by finding work they are passionate about and innovating their careers.
This is an engagement through Upwork’s Hybrid Workforce Solutions (HWS) Team. Our Hybrid Workforce Solutions Team is a global group of professionals that support Upwork’s business. Our HWS team members are located all over the world.
Work/Project Scope:
Provisioning
Maintenance
Right-scaling
Cost-effective use
Create and maintain vulnerability management policies, procedures, and training
Must Haves (Required Skills):
Relational database management experience (Postgres/MySQL/Oracle)
Proficiency with database languages: SQL, PL/SQL or pgPL/SQL.
Scripting: Strong experience with Python (preferred), shell (secondary)
On-call assistance with DB-related incidents.
Automation mindset: Desire and ability to automate repetitive tasks.
Nice to Haves:
Cloud management: Experience with Terraform (CloudFormation, Hashicorp Packer, Chef/Ansible).
Technologies: Kafka / Kinesis (on-prem or managed), ElasticSearch/OpenSearch/Mongo, Redis/Memcache
Databases: Analytical databases like Snowflake/Clickhouse/Greenplum, data federation engines like Presto/Trino/Dremio/Athena.
Upwork is proudly committed to fostering a diverse and inclusive workforce. We never discriminate based on race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical condition), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.
To learn more about how Upwork processes and protects your personal information as part of the application process, please review our Global Job Applicant Privacy Notice
Show more
Show less","Postgres, MySQL, Oracle, SQL, PL/SQL, pgPL/SQL, Python, Shell, Terraform, CloudFormation, Hashicorp Packer, Chef, Ansible, Kafka, Kinesis, ElasticSearch, OpenSearch, Mongo, Redis, Memcache, Snowflake, Clickhouse, Greenplum, Presto, Trino, Dremio, Athena","postgres, mysql, oracle, sql, plsql, pgplsql, python, shell, terraform, cloudformation, hashicorp packer, chef, ansible, kafka, kinesis, elasticsearch, opensearch, mongo, redis, memcache, snowflake, clickhouse, greenplum, presto, trino, dremio, athena","ansible, athena, chef, clickhouse, cloudformation, dremio, elasticsearch, greenplum, hashicorp packer, kafka, kinesis, memcache, mongo, mysql, opensearch, oracle, pgplsql, plsql, postgres, presto, python, redis, shell, snowflake, sql, terraform, trino"
"PowerBI Developer/ Data Analyst/Report Writer - Austin, TX (Locals only)",SmartIPlace,United States,https://www.linkedin.com/jobs/view/powerbi-developer-data-analyst-report-writer-austin-tx-locals-only-at-smartiplace-3783156143,2023-12-17,Alberta, Canada,Associate,Remote,"Job Title:
PowerBI Developer/ Data Analyst/Report Writer
Location:
Austin, Texas
– 100% Remote (Local Preferred)
Interview: V
ideo
Duration: 6 to 12
Months
Client:
Texas Education Agency
Please find below job details and send me suitable candidates with updated resume and Skill matrix doc.
Prior State experience candidates are a plus.
Job Description:
PowerBI developer builds complex dimensional data models and reports from the bottom up, visualizes compelling data stories on the report canvas, collaborates with other teams to engineer revolutionary agency wide data solutions, develops and implements data visualization solutions.
4 or more years of experience, relies on experience and judgment to plan and accomplish goals, independently performs a variety of complicated tasks, may lead and direct the work of others, a wide degree of creativity and latitude is expected.
Minimum Requirements:
II. CANDIDATE SKILLS AND QUALIFICATIONS
Candidates that do not meet or exceed the
minimum
stated requirements (skills/experience) will be displayed to customers but may not be chosen for this opportunity.
Years
Required/Preferred
Experience
4
Required
Using Power BI creating dashboards and interactive visual reports
4
Required
Mastery in data analytics
4
Required
Create, test, and deploy Power BI Scripts (Python,M and DAX) as well as execute efficient deep analysis
4
Preferred
Prior experience in data-related tasks is preferred
2
Previous Experience With Source Control, Git/BitBucket Is Preferred
1
Preferred
Previous experience with SAS and its dataset (.sas7bdat) is preferred
1
Previous Experience With Jira Is Preferred
1
Previous State Experience Is Preferred
1
Show more
Show less","Power BI, DAX, Python, M, Data Analytics, Data Visualization, Data Modeling, SAS, .sas7bdat, Git, Bitbucket, Jira","power bi, dax, python, m, data analytics, data visualization, data modeling, sas, sas7bdat, git, bitbucket, jira","bitbucket, dataanalytics, datamodeling, dax, git, jira, m, powerbi, python, sas, sas7bdat, visualization"
Contract: Database Automation Engineer / DBA-DevOps,Upwork,"Montana, United States",https://www.linkedin.com/jobs/view/contract-database-automation-engineer-dba-devops-at-upwork-3739296968,2023-12-17,Alberta, Canada,Associate,Remote,"Upwork ($UPWK) is the world’s work marketplace. We serve everyone from one-person startups to large, Fortune 100 enterprises with a powerful, trust-driven platform that enables companies and talent to work together in new ways that unlock their potential.
Last year, more than $3.8 billion of work was done through Upwork by skilled professionals who are gaining more control by finding work they are passionate about and innovating their careers.
This is an engagement through Upwork’s Hybrid Workforce Solutions (HWS) Team. Our Hybrid Workforce Solutions Team is a global group of professionals that support Upwork’s business. Our HWS team members are located all over the world.
Work/Project Scope:
Provisioning
Maintenance
Right-scaling
Cost-effective use
Create and maintain vulnerability management policies, procedures, and training
Must Haves (Required Skills):
Relational database management experience (Postgres/MySQL/Oracle)
Proficiency with database languages: SQL, PL/SQL or pgPL/SQL.
Scripting: Strong experience with Python (preferred), shell (secondary)
On-call assistance with DB-related incidents.
Automation mindset: Desire and ability to automate repetitive tasks.
Nice to Haves:
Cloud management: Experience with Terraform (CloudFormation, Hashicorp Packer, Chef/Ansible).
Technologies: Kafka / Kinesis (on-prem or managed), ElasticSearch/OpenSearch/Mongo, Redis/Memcache
Databases: Analytical databases like Snowflake/Clickhouse/Greenplum, data federation engines like Presto/Trino/Dremio/Athena.
Upwork is proudly committed to fostering a diverse and inclusive workforce. We never discriminate based on race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical condition), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.
To learn more about how Upwork processes and protects your personal information as part of the application process, please review our Global Job Applicant Privacy Notice
Show more
Show less","Postgres, MySQL, Oracle, SQL, PL/SQL, pgPL/SQL, Python, Shell, Terraform, CloudFormation, Hashicorp Packer, Chef, Ansible, Kafka, Kinesis, ElasticSearch, OpenSearch, Mongo, Redis, Memcache, Snowflake, Clickhouse, Greenplum, Presto, Trino, Dremio, Athena","postgres, mysql, oracle, sql, plsql, pgplsql, python, shell, terraform, cloudformation, hashicorp packer, chef, ansible, kafka, kinesis, elasticsearch, opensearch, mongo, redis, memcache, snowflake, clickhouse, greenplum, presto, trino, dremio, athena","ansible, athena, chef, clickhouse, cloudformation, dremio, elasticsearch, greenplum, hashicorp packer, kafka, kinesis, memcache, mongo, mysql, opensearch, oracle, pgplsql, plsql, postgres, presto, python, redis, shell, snowflake, sql, terraform, trino"
Contract: Database Automation Engineer / DBA-DevOps,Upwork,"Georgia, United States",https://www.linkedin.com/jobs/view/contract-database-automation-engineer-dba-devops-at-upwork-3739501691,2023-12-17,Alberta, Canada,Associate,Remote,"Upwork ($UPWK) is the world’s work marketplace. We serve everyone from one-person startups to large, Fortune 100 enterprises with a powerful, trust-driven platform that enables companies and talent to work together in new ways that unlock their potential.
Last year, more than $3.8 billion of work was done through Upwork by skilled professionals who are gaining more control by finding work they are passionate about and innovating their careers.
This is an engagement through Upwork’s Hybrid Workforce Solutions (HWS) Team. Our Hybrid Workforce Solutions Team is a global group of professionals that support Upwork’s business. Our HWS team members are located all over the world.
Work/Project Scope:
Provisioning
Maintenance
Right-scaling
Cost-effective use
Create and maintain vulnerability management policies, procedures, and training
Must Haves (Required Skills):
Relational database management experience (Postgres/MySQL/Oracle)
Proficiency with database languages: SQL, PL/SQL or pgPL/SQL.
Scripting: Strong experience with Python (preferred), shell (secondary)
On-call assistance with DB-related incidents.
Automation mindset: Desire and ability to automate repetitive tasks.
Nice to Haves:
Cloud management: Experience with Terraform (CloudFormation, Hashicorp Packer, Chef/Ansible).
Technologies: Kafka / Kinesis (on-prem or managed), ElasticSearch/OpenSearch/Mongo, Redis/Memcache
Databases: Analytical databases like Snowflake/Clickhouse/Greenplum, data federation engines like Presto/Trino/Dremio/Athena.
Upwork is proudly committed to fostering a diverse and inclusive workforce. We never discriminate based on race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical condition), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.
To learn more about how Upwork processes and protects your personal information as part of the application process, please review our Global Job Applicant Privacy Notice
Show more
Show less","Postgres, MySQL, Oracle, SQL, PL/SQL, PgPL/SQL, Python, Shell, Terraform, CloudFormation, Hashicorp Packer, Chef, Ansible, Kafka, Kinesis, Elasticsearch, OpenSearch, Mongo, Redis, Memcache, Snowflake, Clickhouse, Greenplum, Presto, Trino, Dremio, Athena","postgres, mysql, oracle, sql, plsql, pgplsql, python, shell, terraform, cloudformation, hashicorp packer, chef, ansible, kafka, kinesis, elasticsearch, opensearch, mongo, redis, memcache, snowflake, clickhouse, greenplum, presto, trino, dremio, athena","ansible, athena, chef, clickhouse, cloudformation, dremio, elasticsearch, greenplum, hashicorp packer, kafka, kinesis, memcache, mongo, mysql, opensearch, oracle, pgplsql, plsql, postgres, presto, python, redis, shell, snowflake, sql, terraform, trino"
Contract: Database Automation Engineer / DBA-DevOps,Upwork,"Arizona, United States",https://www.linkedin.com/jobs/view/contract-database-automation-engineer-dba-devops-at-upwork-3739503188,2023-12-17,Alberta, Canada,Associate,Remote,"Upwork ($UPWK) is the world’s work marketplace. We serve everyone from one-person startups to large, Fortune 100 enterprises with a powerful, trust-driven platform that enables companies and talent to work together in new ways that unlock their potential.
Last year, more than $3.8 billion of work was done through Upwork by skilled professionals who are gaining more control by finding work they are passionate about and innovating their careers.
This is an engagement through Upwork’s Hybrid Workforce Solutions (HWS) Team. Our Hybrid Workforce Solutions Team is a global group of professionals that support Upwork’s business. Our HWS team members are located all over the world.
Work/Project Scope:
Provisioning
Maintenance
Right-scaling
Cost-effective use
Create and maintain vulnerability management policies, procedures, and training
Must Haves (Required Skills):
Relational database management experience (Postgres/MySQL/Oracle)
Proficiency with database languages: SQL, PL/SQL or pgPL/SQL.
Scripting: Strong experience with Python (preferred), shell (secondary)
On-call assistance with DB-related incidents.
Automation mindset: Desire and ability to automate repetitive tasks.
Nice to Haves:
Cloud management: Experience with Terraform (CloudFormation, Hashicorp Packer, Chef/Ansible).
Technologies: Kafka / Kinesis (on-prem or managed), ElasticSearch/OpenSearch/Mongo, Redis/Memcache
Databases: Analytical databases like Snowflake/Clickhouse/Greenplum, data federation engines like Presto/Trino/Dremio/Athena.
Upwork is proudly committed to fostering a diverse and inclusive workforce. We never discriminate based on race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical condition), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.
To learn more about how Upwork processes and protects your personal information as part of the application process, please review our Global Job Applicant Privacy Notice
Show more
Show less","Relational database management, SQL, PL/SQL, pgPL/SQL, Python, Shell scripting, Terraform, CloudFormation, Hashicorp Packer, Chef, Ansible, Kafka, Kinesis, ElasticSearch, OpenSearch, Mongo, Redis, Memcache, Snowflake, Clickhouse, Greenplum, Presto, Trino, Dremio, Athena","relational database management, sql, plsql, pgplsql, python, shell scripting, terraform, cloudformation, hashicorp packer, chef, ansible, kafka, kinesis, elasticsearch, opensearch, mongo, redis, memcache, snowflake, clickhouse, greenplum, presto, trino, dremio, athena","ansible, athena, chef, clickhouse, cloudformation, dremio, elasticsearch, greenplum, hashicorp packer, kafka, kinesis, memcache, mongo, opensearch, pgplsql, plsql, presto, python, redis, relational database management, shell scripting, snowflake, sql, terraform, trino"
Business Data Analyst,Enexus Global Inc.,United States,https://www.linkedin.com/jobs/view/business-data-analyst-at-enexus-global-inc-3748682811,2023-12-17,Alberta, Canada,Associate,Remote,"Exp: 8 to 20
Location - Remote but need local to Dallas, TX
Tax Term- W2,1099(No C2C, No H1B)
Business Data Analyst with Guidewire and snowflake .
Good communication skill
Responsibilities
A business analyst but understanding of data is needed
Understanding of pricing and underwriting insurance principles and data
Understanding of Property & Casualty data related to risks & losses
Utilization of IOT and how data is used to help with P&C rating.
Strong communication skills and ability to talk with business partners to pull needed information for requirements building
Experience with Guidewire is a plus
Qualifications
Bachelor's Degree in related field is required
Show more
Show less","Guidewire, Snowflake, Data Analysis, Pricing, Underwriting, Insurance, Property & Casualty, Risk, Loss, Internet of Things (IoT), Communication, Requirement Building","guidewire, snowflake, data analysis, pricing, underwriting, insurance, property casualty, risk, loss, internet of things iot, communication, requirement building","communication, dataanalytics, guidewire, insurance, internet of things iot, loss, pricing, property casualty, requirement building, risk, snowflake, underwriting"
Contract: Database Automation Engineer / DBA-DevOps,Upwork,"Illinois, United States",https://www.linkedin.com/jobs/view/contract-database-automation-engineer-dba-devops-at-upwork-3739501696,2023-12-17,Alberta, Canada,Associate,Remote,"Upwork ($UPWK) is the world’s work marketplace. We serve everyone from one-person startups to large, Fortune 100 enterprises with a powerful, trust-driven platform that enables companies and talent to work together in new ways that unlock their potential.
Last year, more than $3.8 billion of work was done through Upwork by skilled professionals who are gaining more control by finding work they are passionate about and innovating their careers.
This is an engagement through Upwork’s Hybrid Workforce Solutions (HWS) Team. Our Hybrid Workforce Solutions Team is a global group of professionals that support Upwork’s business. Our HWS team members are located all over the world.
Work/Project Scope:
Provisioning
Maintenance
Right-scaling
Cost-effective use
Create and maintain vulnerability management policies, procedures, and training
Must Haves (Required Skills):
Relational database management experience (Postgres/MySQL/Oracle)
Proficiency with database languages: SQL, PL/SQL or pgPL/SQL.
Scripting: Strong experience with Python (preferred), shell (secondary)
On-call assistance with DB-related incidents.
Automation mindset: Desire and ability to automate repetitive tasks.
Nice to Haves:
Cloud management: Experience with Terraform (CloudFormation, Hashicorp Packer, Chef/Ansible).
Technologies: Kafka / Kinesis (on-prem or managed), ElasticSearch/OpenSearch/Mongo, Redis/Memcache
Databases: Analytical databases like Snowflake/Clickhouse/Greenplum, data federation engines like Presto/Trino/Dremio/Athena.
Upwork is proudly committed to fostering a diverse and inclusive workforce. We never discriminate based on race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical condition), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.
To learn more about how Upwork processes and protects your personal information as part of the application process, please review our Global Job Applicant Privacy Notice
Show more
Show less","SQL, PL/SQL, pgPL/SQL, Python, Bash, Terraform, CloudFormation, Hashicorp Packer, Chef, Ansible, Kafka, Kinesis, ElasticSearch, OpenSearch, Mongo, Redis, Memcache, Snowflake, Clickhouse, Greenplum, Presto, Trino, Dremio, Athena","sql, plsql, pgplsql, python, bash, terraform, cloudformation, hashicorp packer, chef, ansible, kafka, kinesis, elasticsearch, opensearch, mongo, redis, memcache, snowflake, clickhouse, greenplum, presto, trino, dremio, athena","ansible, athena, bash, chef, clickhouse, cloudformation, dremio, elasticsearch, greenplum, hashicorp packer, kafka, kinesis, memcache, mongo, opensearch, pgplsql, plsql, presto, python, redis, snowflake, sql, terraform, trino"
Contract: Database Automation Engineer / DBA-DevOps,Upwork,"Tennessee, United States",https://www.linkedin.com/jobs/view/contract-database-automation-engineer-dba-devops-at-upwork-3739504028,2023-12-17,Alberta, Canada,Associate,Remote,"Upwork ($UPWK) is the world’s work marketplace. We serve everyone from one-person startups to large, Fortune 100 enterprises with a powerful, trust-driven platform that enables companies and talent to work together in new ways that unlock their potential.
Last year, more than $3.8 billion of work was done through Upwork by skilled professionals who are gaining more control by finding work they are passionate about and innovating their careers.
This is an engagement through Upwork’s Hybrid Workforce Solutions (HWS) Team. Our Hybrid Workforce Solutions Team is a global group of professionals that support Upwork’s business. Our HWS team members are located all over the world.
Work/Project Scope:
Provisioning
Maintenance
Right-scaling
Cost-effective use
Create and maintain vulnerability management policies, procedures, and training
Must Haves (Required Skills):
Relational database management experience (Postgres/MySQL/Oracle)
Proficiency with database languages: SQL, PL/SQL or pgPL/SQL.
Scripting: Strong experience with Python (preferred), shell (secondary)
On-call assistance with DB-related incidents.
Automation mindset: Desire and ability to automate repetitive tasks.
Nice to Haves:
Cloud management: Experience with Terraform (CloudFormation, Hashicorp Packer, Chef/Ansible).
Technologies: Kafka / Kinesis (on-prem or managed), ElasticSearch/OpenSearch/Mongo, Redis/Memcache
Databases: Analytical databases like Snowflake/Clickhouse/Greenplum, data federation engines like Presto/Trino/Dremio/Athena.
Upwork is proudly committed to fostering a diverse and inclusive workforce. We never discriminate based on race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical condition), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.
To learn more about how Upwork processes and protects your personal information as part of the application process, please review our Global Job Applicant Privacy Notice
Show more
Show less","Database management, PostgreSQL, MySQL, Oracle, SQL, PL/SQL, pgPL/SQL, Python, Shell scripting, Terraform, CloudFormation, Hashicorp Packer, Chef, Ansible, Kafka, Kinesis, ElasticSearch, OpenSearch, Mongo, Redis, Memcache, Snowflake, Clickhouse, Greenplum, Presto, Trino, Dremio, Athena","database management, postgresql, mysql, oracle, sql, plsql, pgplsql, python, shell scripting, terraform, cloudformation, hashicorp packer, chef, ansible, kafka, kinesis, elasticsearch, opensearch, mongo, redis, memcache, snowflake, clickhouse, greenplum, presto, trino, dremio, athena","ansible, athena, chef, clickhouse, cloudformation, database management, dremio, elasticsearch, greenplum, hashicorp packer, kafka, kinesis, memcache, mongo, mysql, opensearch, oracle, pgplsql, plsql, postgresql, presto, python, redis, shell scripting, snowflake, sql, terraform, trino"
Senior Data Platform Developer,Neo Financial,"Calgary, Alberta, Canada",https://ca.linkedin.com/jobs/view/senior-data-platform-developer-at-neo-financial-3750158695,2023-12-17,Alberta, Canada,Mid senior,Onsite,"Come Build a Better Financial Future for all Canadians
At Neo, we're building a more rewarding financial experience for all Canadians.
Life at a rapidly expanding tech startup is demanding, exhilarating, and not for everyone.
From world-class creative minds to brilliant engineers, it's high-performing people that make Neo a workplace with passion and purpose.
Since being founded in 2019, Neo has built incredible traction and is one of the fastest growing fintechs in Canada.
LinkedIn's Top Startup in Canada for 2022 and 2023
Top-ranked mobile apps and credit cards
Canada's top-rated credit card
Team of 700+ people
1M+ customers in 3 years
11K+ retail partners
High Performance at Neo
We recruit, hire, and build our company culture around these attributes:
Teamwork
: We trust, respect, encourage, and show up for each other — through good times and hard. We're on this mission not just for ourselves, but also for the people we work with — and ultimately, for our customers.
Ownership
: We all have a stake in Neo's success — so we go out of our way to do what needs to get done. We hold ourselves accountable to deliver on our commitments — to our customers, to our partners, and to our team. When we fall short, we find a way to do better in the future.
Professional Integrity
: We're asking millions of Canadians to trust us with their hard-earned money — so we hold ourselves (and each other) to the highest standards of integrity.
The role
Neo Financial has an opportunity for a Data Platform Developer to join our team in Calgary, AB.
At Neo, we produce, ingest and process a huge amount of data on a daily basis. We believe that data is a competitive advantage for Neo and would like to build systems that allow us to realize the immense potential brought in by extracting the power of data. We want data to be a core part of our engineering stack, guide our business decisions and help our clients and partners achieve a better future. We want to build systems that will facilitate efficient, reliable and productive data processing while being compliant to all the required standards.
Some of the exciting opportunities for this role include building advanced analytics platforms facilitating automatic management of data, building on a variety of data architectures including batch and real time stream processing, building data pipelines that can support machine learning, diverse data stores (transactional, operational and analytical), working on encryption schemes to protect the privacy of data and more. Equally important is to build systems that are highly efficient to work with and support a growing team of engineers.
As a Data Platform Developer, you'll play a pivotal role in setting the technical direction for Neo's data platform and building out key systems and solutions with data at the core. You will contribute to building industry-leading, cutting edge, highly robust and high-throughput data solutions. We don't work with legacy technologies at Neo; join a team that empowers you to build greenfield solutions and truly create something you're really proud of.
What You'll Be Doing
Design systems and solve problems related to systems that handle large volumes of data with challenging requirements on reliability and performance.
Build features on the platform that will unlock roadmap items (i.e. AI/ML for fraud detection, advanced credit analytics models, customer-facing insights and forecasts, etc)
Build solutions using our high-tech data platform that includes DBT, Snowflake, Apache Spark, Airflow, Kafka, AWS, Databricks and more.
Learn to master encryption and security practices where they apply to data storage, transformation, or transmission.
Be hands-on in developing services, libraries, automated pipelines, tests by writing high quality code.
Use infrastructure as code tools like TerraForm to effectively manage complex cloud deployments.
Perform code reviews to ensure that the work done by the team meets the highest standards and best practices, coaching your team with insightful comments and feedback.
Own engineering objectives such as scalability, performance, security, and maintainability.
Actively make an effort to teach others, mentor the team, collaborate, and be involved.
Who We Are Looking For
3+ years of experience with Python (or similar languages)
Experience with data modeling and building data processing pipelines, beyond just using SQL.
Experience building scalable backend platform services, and working with strict requirements on reliability, scalability and performance.
Experience using Airflow and/or Databricks is an asset.
Ability or interest to leverage development best practices throughout your work - like source-control, automated testing, CI/CD deployment pipelines, code reviews, etc.
Comfortable building solutions in cloud-based environments like AWS, Azure, or similar.
Strong leadership behaviors including giving confident feedback, presenting solution plans to executives, or participating in mentorship activities every day.
Applicants must be eligible to work in Canada and willing to relocate
Working at Neo
Joining Neo means betting on yourself and discovering your full potential. As individuals and as a team, we continually challenge ourselves and each other to do our best work. We're making change happen at a rapid pace — providing endless opportunities to sharpen your skills, expand your knowledge, and find new solutions to complex problems. That means rapid career progression and constant learning opportunities.
The people who thrive at Neo are resourceful, relentless, and want to win. We hold ourselves to high standards, because we're on a mission that matters — to transform financial services for the better. If that's what you're looking for, read on.
We trust, respect, and show up for each other. That means truthful conversations, frequent feedback, and working with people who push you to be your best. We're evolving quickly as an organization, we work together in person, and the pace of progress isn't for everyone. That's why we're looking for change-makers who love a challenge — who would rather blaze a trail through uncertainty than travel a well-paved road.
Our team members earn meaningful equity in the company through stock options — so Neo's growth benefits everyone who helps make it happen. That also means taking on more responsibility than you may have had at your last job. We don't get hung up on job titles or hierarchy — we're focused on doing what it takes to accomplish our mission.
Check out these videos from our employees to learn more about Working at Neo.
Apply with Us
We believe in equal opportunity, and are committed to creating an inclusive climate where everyone can thrive. Customers trust us with their finances, so successful candidates for this position will be required to undergo a security screening, including a criminal records check and a credit check.
Show more
Show less","Python, Data modeling, Data processing pipelines, Backend platform services, Apache Spark, Airflow, Databricks, Machine learning, Data encryption, AWS, TerraForm, Cloud deployments, Code reviews, Scalability, Performance, Security, Maintainability, CI/CD deployment pipelines, Azure","python, data modeling, data processing pipelines, backend platform services, apache spark, airflow, databricks, machine learning, data encryption, aws, terraform, cloud deployments, code reviews, scalability, performance, security, maintainability, cicd deployment pipelines, azure","airflow, apache spark, aws, azure, backend platform services, cicd deployment pipelines, cloud deployments, code reviews, data encryption, data processing pipelines, databricks, datamodeling, machine learning, maintainability, performance, python, scalability, security, terraform"
Senior Data Analytics Developer,Neo Financial,"Calgary, Alberta, Canada",https://ca.linkedin.com/jobs/view/senior-data-analytics-developer-at-neo-financial-3750162254,2023-12-17,Alberta, Canada,Mid senior,Onsite,"Come Build a Better Financial Future for all Canadians
At Neo, we're building a more rewarding financial experience for all Canadians.
Life at a rapidly expanding tech startup is demanding, exhilarating, and not for everyone.
From world-class creative minds to brilliant engineers, it's high-performing people that make Neo a workplace with passion and purpose.
Since being founded in 2019, Neo has built incredible traction and is one of the fastest growing fintechs in Canada.
LinkedIn's Top Startup in Canada for 2022 and 2023
Top-ranked mobile apps and credit cards
Canada's top-rated credit card
Team of 700+ people
1M+ customers in 3 years
11K+ retail partners
High Performance at Neo
We recruit, hire, and build our company culture around these attributes:
Teamwork
: We trust, respect, encourage, and show up for each other — through good times and hard. We're on this mission not just for ourselves, but also for the people we work with — and ultimately, for our customers.
Ownership
: We all have a stake in Neo's success — so we go out of our way to do what needs to get done. We hold ourselves accountable to deliver on our commitments — to our customers, to our partners, and to our team. When we fall short, we find a way to do better in the future.
Professional Integrity
: We're asking millions of Canadians to trust us with their hard-earned money — so we hold ourselves (and each other) to the highest standards of integrity.
What you'll be doing:
Build solutions using our high-tech data platform that includes Dbt, Snowflake, Apache Spark, Airflow, Kafka, AWS and Databricks (plus more).
Write Python or SQL code for most projects, including automated tests.
Providing input and subject-matter expertise on future data products
Working with product teams to define and capture data modeling requirements
Developing test cases and validations to ensure high-quality data products
Perform code reviews to ensure that the work done by the team meets the highest standards and best practices.
Be an expert with databases including SQL and NoSQL varieties; you should be a master of building and optimizing complex SQL queries.
Develop automated testing for all your projects, ensuring that your solutions are high quality when released, but also helping avoid regressions over time.
Develop patterns, abstractions, libraries, templates and other standards to help the team move fast and avoid reinventing the wheel.
Own projects and systems in production, ensuring they are operating to the highest standards and meeting challenging SLAs.
Actively make an effort to teach others, mentor the team, collaborate, and be involved.
Lead standup meetings, retrospectives, or technical reviews of product stories
Who We Are Looking For
5+ years of strong experience with SQL and Python (or similar languages)
3+ years of experience creating and maintaining mission-critical reports and dashboards.
2+ years of experience in data modeling in an enterprise data warehouse setting.
Experience using Dbt, Airflow, s3, and Databricks are highly important.
Expertise configuring and using modern Business Intelligence / Analytics services
Ability to leverage development best practices throughout your work - like source-control, automated testing, CI/CD deployment pipelines, code reviews, etc.
Comfortable building solutions in cloud-based environments like AWS, Azure, or similar.
Strong leadership behaviours including giving confident feedback, presenting solution plans to executives, or participating in mentorship activities every day.
Applicants must be eligible to work in Canada and willing to relocate
Working at Neo
Joining Neo means betting on yourself and discovering your full potential. As individuals and as a team, we continually challenge ourselves and each other to do our best work. We're making change happen at a rapid pace — providing endless opportunities to sharpen your skills, expand your knowledge, and find new solutions to complex problems. That means rapid career progression and constant learning opportunities.
The people who thrive at Neo are resourceful, relentless, and want to win. We hold ourselves to high standards, because we're on a mission that matters — to transform financial services for the better. If that's what you're looking for, read on.
We trust, respect, and show up for each other. That means truthful conversations, frequent feedback, and working with people who push you to be your best. We're evolving quickly as an organization, we work together in person, and the pace of progress isn't for everyone. That's why we're looking for change-makers who love a challenge — who would rather blaze a trail through uncertainty than travel a well-paved road.
Our team members earn meaningful equity in the company through stock options — so Neo's growth benefits everyone who helps make it happen. That also means taking on more responsibility than you may have had at your last job. We don't get hung up on job titles or hierarchy — we're focused on doing what it takes to accomplish our mission.
Check out these videos from our employees to learn more about Working at Neo.
Apply with Us
We believe in equal opportunity, and are committed to creating an inclusive climate where everyone can thrive. Customers trust us with their finances, so successful candidates for this position will be required to undergo a security screening, including a criminal records check and a credit check.
Show more
Show less","Python, SQL, Airflow, Databricks, DBT, Apache Spark, AWS, Machine Learning, NoSQL, Kafka, Snowflake, Data Modeling, Business Intelligence, Analytics, Cloud Computing, CI/CD, Agile","python, sql, airflow, databricks, dbt, apache spark, aws, machine learning, nosql, kafka, snowflake, data modeling, business intelligence, analytics, cloud computing, cicd, agile","agile, airflow, analytics, apache spark, aws, business intelligence, cicd, cloud computing, databricks, datamodeling, dbt, kafka, machine learning, nosql, python, snowflake, sql"
Senior Data Engineer - AWS,Pare,"Calgary, Alberta, Canada",https://ca.linkedin.com/jobs/view/senior-data-engineer-aws-at-pare-3624934490,2023-12-17,Alberta, Canada,Mid senior,Onsite,"We’re focused on growing and monetizing the world's most passionate digital audiences by recruiting high performers who know how to go for gold, work with their team, solve problems in real-time, and speak up with creative ideas.
As a Senior Data Warehouse Engineer, you will support the Director of Data Warehousing in using cutting-edge technologies to build scalable DW software-as-a-service products that are used by industry leaders across the world.
You will leverage AWS to implement the newest serverless, data ingestion and processing patterns in the cloud. You will have the opportunity to experiment with evolving tools and learn new patterns from even more experienced data and software developers.
Execute the buildout plan of the warehousing product line.
Model and build data pipelining patterns to deliver the most modern DW products to our client cloud DBs.
Assemble large, complex data solutions that meet functional / non-function business requirements
Participate in data analysis and data architecture direction with valuable client-facing development insights
Author and maintain relevant documentation for developers, clients, and users
Work closely with account management in formulating delivery plans, schedules, and deadlines
Ensure proper monitoring and alerts are in place to detect issues as soon as possible.
Lead and plan sprints
Mentor data engineers in good techniques, patterns, processes, and practices
Peer review other team members’ code and learn and adapt from peer review of your own code You have been through many projects, have had successes and failures, and have learned from both, developing a robust toolbox for building data solutions, implementing patterns, and leading the next generation of data engineers. You are a superb communicator, able to gain understanding from both technical and non-technical audiences, and pride yourself on your interpersonal and relationship management skills. You enjoy being part of a results-oriented, cross-functional team, and the words ‘ fast-paced’ don’t intimidate you- you immediately think, GAME ON! Finally, at the end of the day, you’re a data nerd at heart and proud of it!
Requirements
Must be a Canadian resident or citizen/have work authorization
Must have a BS degree in Computer Science, Computer Engineering, or equivalent certificate program
Must have 5+ years of working on data solutions
You are comfortable working in cross-functional teams and can share insights and expectations with stakeholders, clients, team members, and various levels of management
You are comfortable digging into data problems, no matter the domain, and helping to organize understanding, generating solutions, and developing a plan to execute
You have a strong working knowledge of SQL, or MySQL/MSSQL/Oracle/PgSQL.
Tuning and optimization of SQL
Experience with cloud platforms and data pipelines in Azure or AWS
Experience with shell scripting (bash, kshell, PowerShell)
Experience with ETL frameworks or tools (SSIS, Informatica, Kettle or similar)
Experience with python packages to ingest data and perform cleansing
Experience working with REST APIs, flat files (csv and Excel)
Working knowledge with NoSQL platforms (MongoDB, Cosmos)
Strong experience breaking down user problems into stories and solutions
Experience working on iterative projects with continuous delivery pipelines
Experience building and optimizing data pipelines, architectures and data sets
Experience with python packages to ingest data and perform cleansing
DBA experience
https://pare.pro?refer=C4UYgEjOVpdbBAeVjUfi
Show more
Show less","Data Warehousing, ETL, SQL, Python, AWS, Azure, NoSQL, MongoDB, CosmosDB, REST APIs, Shell Scripting, SSIS, Informatica, Kettle, Data Pipelines, Data Architecture, Data Analysis, Data Engineering, Data Solutions, DBA","data warehousing, etl, sql, python, aws, azure, nosql, mongodb, cosmosdb, rest apis, shell scripting, ssis, informatica, kettle, data pipelines, data architecture, data analysis, data engineering, data solutions, dba","aws, azure, cosmosdb, data architecture, data engineering, data solutions, dataanalytics, datapipeline, datawarehouse, dba, etl, informatica, kettle, mongodb, nosql, python, rest apis, shell scripting, sql, ssis"
Senior Data Engineering Advisor,Petroplan,"Calgary, Alberta, Canada",https://ca.linkedin.com/jobs/view/senior-data-engineering-advisor-at-petroplan-3786558681,2023-12-17,Alberta, Canada,Mid senior,Onsite,"Senior Data Engineering Advisor
Location: Calgary
Contract: 12 months with a view to extend
Schedule: Monday to Friday. 4 days from office, 1 day from home.
About the Job
Petroplan are working on behalf of a leading Oil & Gas owner/operator:
You'll be joining the Data Engineering team in the Corporate Data Office (CDO). Our vision is to enable all staff to be more creative through insights derived from our trusted data and analytical capabilities leading to improved safety, increased production, lower GHG emissions, and actionable efficiencies. Collaborating with stakeholders across the organization, the CDO provides the capabilities, services and governance to protect, deploy and exploit value from data assets.
The Data Engineering team endeavors to deliver trusted, secure, high quality and easily consumable data sets to data consumers and practitioners across the organization. You'll be joining a fun, exciting team comprised of highly motivated members working towards the common goal of democratizing our data.
What you'll do
* You will apply your skills to help engineer data that provides business insights for our company
* Work with various data stakeholders throughout the organization to identify opportunities for leveraging company data that drives business solutions
* Building, fostering and participating in a data community to connect data producers with data consumers and develop data literacy
* Work with various data stakeholders across the organization in the utilization of a modern data architecture that enables the collection, transformation, storage, modeling and delivery of data to consumers and practitioners
* Apply data governance practices and procedures in the design and development of data pipelines
* Collaborate with cross-functional teams to design and develop data pipelines for visualization or analytic solutions
* Perform maintenance and support activities for data pipelines
* Support build of big data environments that enable analytics solutions on a variety of big data platforms, including assessing the usefulness of new technologies and advocating for their adoption
What you'll bring to this role
* Undergraduate degree in Computer Science, Engineering, Applied Mathematics, Commerce with a Business Analytics concentration or related Information Science.
* Good knowledge of oil and gas operations
* Familiarity with common oil & gas applications (Well & Land, Subsurface, Production operations, Drilling, Engineering, etc.)
* Competently communicates both verbal and written with strong listening skills
* 5 - 10+ years of experience in a data related role
* Willingness to learn and offer new creative ways to work that makes our team more agile and adaptable
* Commitment to being a team player who values and respects the differences in working within a diverse and inclusive team.
* Technical expertise in developing data integrations, logical and physical data models, data mining, and data warehousing.
* Proficiency in SQL
* Deep understanding of architectures of traditional data architecture practices (MDM, ODS, data warehousing and other), as well transition to Next-Gen platforms such as Lakehouse Architecture, Cloud, distributed ledgers, etc.
* Hands-on experience with various relational and NoSQL database technologies, including SQL Server, DB2, Oracle, HANA, Azure DB, AWS RDS, etc.
* Understanding of Data Governance strategy, practices and procedures to effectively deliver secure, trusted, and high-quality data.
* Experience creating and maintaining data pipelines that operate on a variety of sources, such as APIs, FTP sites, cloud-based blob stores, databases (relational and non-relational), unstructured data, etc.
* Experience with big data platforms, cloud technologies and server-less environments (such as AWS, Azure, Redshift, EMR, etc.)
* Proficiency in one or more of the following data modelling/ data munging/ data management toolsets such as Databricks, Data Virtualization, SQL Server, Streamsets, Azure Synapse, Redshift, Azure Data factory, AWS Glue, Alation Data Catalog, etc.
* Proficiency in big data technologies including querying, creating and populating data lakes.
* Proficiency in any of the following programming languages, frameworks and technologies: Python, R, open source databases such as PostgreSQL, MySQL, MongoDB
Nice to Haves
* Proficiency in data analytics and visualization platforms such as Microsoft Power BI or Spotfire
* Proficiency in common machine Learning libraries in Python, Spark, and Deep Learning/AI frameworks
About Petroplan
Petroplan is the trusted, specialist global Talent Solutions partner of choice for employers and professionals in the Energy sector. Since 1976, Petroplan has been here to help people like you make the most of the opportunities available and find the best fit for you as an individual.
Over 10,000 placements in more than 55 countries for over 550 clients across 65 disciplines.
In an industry where skilled and experienced professionals are increasingly sought after resource, we appreciate the true value of what you have to offer. What's more, we understand that different things are important to different people in today's world of work - it's about making exactly the right connections for you as an individual.
Our reputation for contractor and candidate care, understanding what makes you tick, and finding you the best match is second-to-none. We understand the industry inside-out and seek to understand YOU and your motivations in the same way. We take the time to evaluate your individual strengths, understand the most important things to you, and establish exactly what you're looking for from your next role.
Show more
Show less","SQL, Data Integration, Data Modelling, Data Munging, Data Management, SQL Server, Streamsets, Azure Synapse, Redshift, Azure Data Factory, AWS Glue, Alation Data Catalog, Databricks, Data Virtualization, Python, R, PostgreSQL, MySQL, MongoDB, Microsoft Power BI, Spotfire, Machine Learning, Spark, Deep Learning, AI, AWS, Azure, Redshift, EMR","sql, data integration, data modelling, data munging, data management, sql server, streamsets, azure synapse, redshift, azure data factory, aws glue, alation data catalog, databricks, data virtualization, python, r, postgresql, mysql, mongodb, microsoft power bi, spotfire, machine learning, spark, deep learning, ai, aws, azure, redshift, emr","ai, alation data catalog, aws, aws glue, azure, azure data factory, azure synapse, data integration, data management, data modelling, data munging, data virtualization, databricks, deep learning, emr, machine learning, microsoft power bi, mongodb, mysql, postgresql, python, r, redshift, spark, spotfire, sql, sql server, streamsets"
Senior Data Engineer,BURNCO Rock Products Ltd,"Calgary, Alberta, Canada",https://ca.linkedin.com/jobs/view/senior-data-engineer-at-burnco-rock-products-ltd-3778855873,2023-12-17,Alberta, Canada,Mid senior,Onsite,"Rock it with BURNCO!!!
BURNCO is a growing, rock-solid business supplying aggregate construction materials in Canada and the US.
We've been supporting family values and building for the next generation for over a hundred years, and we need you to help make the next hundred years even better. Be a key player in leading our business out of the Flintstones™ era and into the age of the Jetsons™.
Talented. Strategic. Responsive. Nimble. Flexible. Dynamic. Data & Technology Driven.
BURNCO Rock Products Ltd has a fantastic opportunity for a Senior Data Engineer out of our corporate headquarters in Calgary, Alberta. You will report to our Manager, Data & Analytics and work closely with our internal key stakeholders in IT, HR, Finance, as well as our lines of business (Concrete, Asphalt, Aggregate).
As a Senior Data Engineer, drive the design, implementation, testing, and maintenance of our robust data architecture, models, and pipelines. As a key contributor to our organizational success, you will empower business and technology leaders to leverage data for informed decision-making. We are seeking a candidate who not only possesses deep technical expertise but also thrives on challenging the conventional and embracing cutting-edge tools and technologies. Reporting to the Manager of Data and Analytics, this role stands at the forefront of shaping our data landscape.
What You Will Be Doing
How we see your role breaking down:
Solution Design and Architecture – 20%
Ensure optimization and alignment of data engineering pipelines and architecture with business needs and technological advancements.
Manage and enhance data architecture in alignment with our strategic business objectives.
Develop and implement architectural standards, patterns, and principles for efficient ETL development and testing.
ETL and Data Model development, testing and maintenance – 60%
Develop, test and maintain data pipelines and models utilizing modern solutions like Fivetran, Python, Snowflake, Powershell and similar.
Collaborate closely with business and technology experts to translate requirements into scalable, effective data models.
Champion data pipeline and model ownership, management, and enhancements to adapt to evolving business needs.
Identify, debug, and rectify data pipeline and model issues as they arise, ensuring minimal disruptions.
Directly engage in data ingestion, transformation, and the development of robust pipelines supporting reporting and analytics activities.
Continuously improve data architecture frameworks and methodologies used within
BURNCO
.
Ensure delivery transparency with timely and effective communications and reporting.
Documentation and process adherence – 20%
Maintain comprehensive documentation for reference and future improvements.
Lead the creation of step-by-step deployment procedures for fixes and enhancements, with backout and roll-back sequences and timings, test plans and procedures.
Represent Data & Analytics team within the IT change management process.
Gather lessons learned from failures and fixes, analyze feedback and incorporate the same into on-going and future projects.
Other duties as assigned
Working Conditions
The role has standard working conditions in an office environment with a regular work week from Monday to Friday and on-call availability during implementation and burn-in/warranty periods, dependent upon activity scope.
What We Would Like From You
Competencies
Strong analytical skills with the ability to diagnose problems efficiently and provide effective solutions.
Excellent communication skills, with the ability to work collaboratively with cross-functional teams, business stakeholders, and external vendors.
Experience in collaborating with external teams or vendors to resolve integration-related issues or enhancements.
Attention to detail with a focus on maintaining comprehensive documentation for integrations, fixes, and enhancements.
Strong organizational skills to manage multiple tasks, prioritize effectively, and meet deadlines in a dynamic environment.
Ability to adapt to evolving technologies and proactively seek opportunities for process improvement and optimization.
Self-motivated individual with a proactive approach to problem-solving and continuous learning.
Experience/Technical Skills/Knowledge
10+ years of immersive technical experience in data engineering and architecture.
Proficiency in data modeling, ETL/ELT pipeline design, data transformation, business intelligence, data security, and master data management.
5+ years' hands-on involvement in designing, developing, and orchestrating Python and UI-based (e.g., Informatica, Fivetran) data pipelines.
3+ years of dedicated work with cloud data lakes, delta lakes, and managed data warehouse solutions such as Snowflake among others.
Proficient in diverse data ingestion patterns, including CDC, batch, streaming, and APIs.
Expertise in data transformation and utilization of tools & technologies such as Informatica, DBT, Coalesce, among others.
Sound knowledge of CI/CD and DevOps practices, driving efficiency and reliability in development pipelines.
Understanding and familiarity with applications such as ERP systems, Apex, Onbase, and Command are also seen as valuable assets.
Education/Certification/Designation
Bachelor's degree in Computer Science, Information Technology, or a related field. Equivalent practical experience will be considered.
COMPETITIVE SALARIES
……
Worth it!!
PERFORMANCE INCENTIVES
……
They rock!!
GREAT BENEFITS
......
Count on it!!
CHANCE TO MAKE A DIFFERENCE
……
Absolutely!!
LEARNING OPPORTUNITIES
……
Always!!
Show more
Show less","Apache Airflow, Apache NiFi, Python, Snowflake, PowerBI, AWS, Azure, Fivetran, PySpark, Scala, ETL, ELT, Data Modeling, Data Warehousing, Data Analytics, Data Visualization, Machine Learning, Artificial Intelligence, Cloud Computing, DevOps, Agile, CI/CD, Microservices, APIs, Data Security, Data Governance, Informatica, DBT, Coalesce, Informatica, Apex, Onbase, Command, ERP Systems","apache airflow, apache nifi, python, snowflake, powerbi, aws, azure, fivetran, pyspark, scala, etl, elt, data modeling, data warehousing, data analytics, data visualization, machine learning, artificial intelligence, cloud computing, devops, agile, cicd, microservices, apis, data security, data governance, informatica, dbt, coalesce, informatica, apex, onbase, command, erp systems","agile, apache airflow, apache nifi, apex, apis, artificial intelligence, aws, azure, cicd, cloud computing, coalesce, command, data governance, data security, dataanalytics, datamodeling, datawarehouse, dbt, devops, elt, erp systems, etl, fivetran, informatica, machine learning, microservices, onbase, powerbi, python, scala, snowflake, spark, visualization"
Senior Data Engineer,BURNCO LLC,"Calgary, Alberta, Canada",https://ca.linkedin.com/jobs/view/senior-data-engineer-at-burnco-llc-3782943527,2023-12-17,Alberta, Canada,Mid senior,Onsite,"Rock it with BURNCO!!!
BURNCO is a growing, rock-solid business supplying aggregate construction materials in Canada and the US.
We've been supporting family values and building for the next generation for over a hundred years, and we need you to help make the next hundred years even better. Be a key player in leading our business out of the Flintstones™ era and into the age of the Jetsons™.
Talented. Strategic. Responsive. Nimble. Flexible. Dynamic. Data & Technology Driven.
BURNCO Rock Products Ltd has a fantastic opportunity for a Senior Data Engineer out of our corporate headquarters in Calgary, Alberta. You will report to our Manager, Data & Analytics and work closely with our internal key stakeholders in IT, HR, Finance, as well as our lines of business (Concrete, Asphalt, Aggregate).
As a Senior Data Engineer, drive the design, implementation, testing, and maintenance of our robust data architecture, models, and pipelines. As a key contributor to our organizational success, you will empower business and technology leaders to leverage data for informed decision-making. We are seeking a candidate who not only possesses deep technical expertise but also thrives on challenging the conventional and embracing cutting-edge tools and technologies. Reporting to the Manager of Data and Analytics, this role stands at the forefront of shaping our data landscape.
What You Will Be Doing
How we see your role breaking down:
Solution Design and Architecture – 20%
Ensure optimization and alignment of data engineering pipelines and architecture with business needs and technological advancements.
Manage and enhance data architecture in alignment with our strategic business objectives.
Develop and implement architectural standards, patterns, and principles for efficient ETL development and testing.
ETL and Data Model development, testing and maintenance – 60%
Develop, test and maintain data pipelines and models utilizing modern solutions like Fivetran, Python, Snowflake, Powershell and similar.
Collaborate closely with business and technology experts to translate requirements into scalable, effective data models.
Champion data pipeline and model ownership, management, and enhancements to adapt to evolving business needs.
Identify, debug, and rectify data pipeline and model issues as they arise, ensuring minimal disruptions.
Directly engage in data ingestion, transformation, and the development of robust pipelines supporting reporting and analytics activities.
Continuously improve data architecture frameworks and methodologies used within
BURNCO
.
Ensure delivery transparency with timely and effective communications and reporting.
Documentation and process adherence – 20%
Maintain comprehensive documentation for reference and future improvements.
Lead the creation of step-by-step deployment procedures for fixes and enhancements, with backout and roll-back sequences and timings, test plans and procedures.
Represent Data & Analytics team within the IT change management process.
Gather lessons learned from failures and fixes, analyze feedback and incorporate the same into on-going and future projects.
Other Duties As Assigned
Working Conditions
The role has standard working conditions in an office environment with a regular work week from Monday to Friday and on-call availability during implementation and burn-in/warranty periods, dependent upon activity scope.
What We Would Like From You
Competencies
Strong analytical skills with the ability to diagnose problems efficiently and provide effective solutions.
Excellent communication skills, with the ability to work collaboratively with cross-functional teams, business stakeholders, and external vendors.
Experience in collaborating with external teams or vendors to resolve integration-related issues or enhancements.
Attention to detail with a focus on maintaining comprehensive documentation for integrations, fixes, and enhancements.
Strong organizational skills to manage multiple tasks, prioritize effectively, and meet deadlines in a dynamic environment.
Ability to adapt to evolving technologies and proactively seek opportunities for process improvement and optimization.
Self-motivated individual with a proactive approach to problem-solving and continuous learning.
Experience/Technical Skills/Knowledge
10+ years of immersive technical experience in data engineering and architecture.
Proficiency in data modeling, ETL/ELT pipeline design, data transformation, business intelligence, data security, and master data management.
5+ years' hands-on involvement in designing, developing, and orchestrating Python and UI-based (e.g., Informatica, Fivetran) data pipelines.
3+ years of dedicated work with cloud data lakes, delta lakes, and managed data warehouse solutions such as Snowflake among others.
Proficient in diverse data ingestion patterns, including CDC, batch, streaming, and APIs.
Expertise in data transformation and utilization of tools & technologies such as Informatica, DBT, Coalesce, among others.
Sound knowledge of CI/CD and DevOps practices, driving efficiency and reliability in development pipelines.
Understanding and familiarity with applications such as ERP systems, Apex, Onbase, and Command are also seen as valuable assets.
Education/Certification/Designation
Bachelor's degree in Computer Science, Information Technology, or a related field. Equivalent practical experience will be considered.
COMPETITIVE SALARIES
……
Worth it!!
PERFORMANCE INCENTIVES
……
They rock!!
GREAT BENEFITS
......
Count on it!!
CHANCE TO MAKE A DIFFERENCE
……
Absolutely!!
LEARNING OPPORTUNITIES
……
Always!!
Show more
Show less","Data Engineering, Data Architecture, ETL, Data Modeling, Python, Informatica, Fivetran, Snowflake, Powershell, Data Pipeline Development, Data Pipeline Testing, Data Pipeline Maintenance, Data Security, Master Data Management, Cloud Data Lakes, Delta Lakes, Managed Data Warehouse Solutions, CDC, Batch, Streaming, APIs, DBT, Coalesce, CI/CD, DevOps, ERP Systems, Apex, Onbase, Command","data engineering, data architecture, etl, data modeling, python, informatica, fivetran, snowflake, powershell, data pipeline development, data pipeline testing, data pipeline maintenance, data security, master data management, cloud data lakes, delta lakes, managed data warehouse solutions, cdc, batch, streaming, apis, dbt, coalesce, cicd, devops, erp systems, apex, onbase, command","apex, apis, batch, cdc, cicd, cloud data lakes, coalesce, command, data architecture, data engineering, data pipeline development, data pipeline maintenance, data pipeline testing, data security, datamodeling, dbt, delta lakes, devops, erp systems, etl, fivetran, informatica, managed data warehouse solutions, master data management, onbase, powershell, python, snowflake, streaming"
SCM Data Analyst,Airswift,"Calgary, Alberta, Canada",https://ca.linkedin.com/jobs/view/scm-data-analyst-at-airswift-3779194263,2023-12-17,Alberta, Canada,Mid senior,Onsite,"Airswift is working with one of our major Oil and Gas clients to find a SCM Data Analyst to join their team on a 1-year contract in Calgary.
Responsibilities:
Review, manage, and transcribe SCM contract and purchase order details currently in various formats (Word, PDF, Excel) into standardized Excel templates
Proactively verify data quality of contract and purchase order data through manual and digital validation
Support the Supply Chain Management (SCM) Governance & Performance Management Lead in the development, rollout, and maintenance of SCM reporting using data analysis tools and techniques utilizing Maximo, Power BI, Excel, PowerPoint and other systems
Support the SCM Category Management Leads in performing various administrative tasks, including data analysis, simple contract amendments, and filing of digital copies
Support SCM Materials Management Lead in data cleansing of item master and inventory data
Qualifications:
Proficiency in Microsoft Office applications (Excel, Word, PowerPoint, Outlook)
Experience with Maximo, Power BI, and basic SQL knowledge would be an asset
Proficiency with data analytics and visualization tools and methods
Minimum of 2 years experience in Supply Chain Management, IT, Data Analytics, or a business analysis role
Demonstrated ability to work in a fast-paced environment while managing multiple tasks
Strong communication skills (oral & written)
Detail-orientated with a high level of accuracy
Ability to work independently
Ability to work collaboratively with subject matter experts within SCM, IT, and other business unit clients to support SCM data initiatives and projects
Show more
Show less","Data Analysis, Data Visualization, Maximo, Power BI, SQL, Excel, PowerPoint, Microsoft Office, Supply Chain Management, IT, Data Analytics, Business Analysis","data analysis, data visualization, maximo, power bi, sql, excel, powerpoint, microsoft office, supply chain management, it, data analytics, business analysis","business analysis, dataanalytics, excel, it, maximo, microsoft office, powerbi, powerpoint, sql, supply chain management, visualization"
Senior Data Analyst,Sport Chek,"Calgary, Alberta, Canada",https://ca.linkedin.com/jobs/view/senior-data-analyst-at-sport-chek-3779610210,2023-12-17,Alberta, Canada,Mid senior,Onsite,"What You’ll Do
At Canadian Tire, we're proud to be on the leading edge of retail analytics using statistical modelling, predictive analysis, machine learning and artificial intelligence to help drive our business forward. Using the latest analytics tools, we provide our stakeholders the ability to quickly understand data and make well informed decisions while providing a suite of self-serve tools, reporting and capabilities to answer any business question. Supporting the Sport Chek banner, the ideal candidate will be highly skilled in all aspects of data analytics, including data mining, transformation, and visualization so that they may help drive innovation and growth and ultimately help us achieve our strategic vision.
The Primary Responsibilities Of This Role Include
Use predictive/statistical modelling techniques and traditional analytics to identify and quantify sales drivers, conduct what-if scenario analysis and measure the causal impacts
Drive processes related to optimizing our markdown strategy and location level pricing
Provide the business with a full analytical service offering. This includes developing operational reporting and executive dashboards, providing analysis, conclusions and recommendations for the business
Support the financial planning process by optimizing sales, inventory and receipt forecasting to help drive strategy and align the business to achieve our plan
Take an active role in understanding each department’s activities to gain a deeper understanding of the business and help integrate their strategies into one holistic vision
Support the business with ad-hoc data requests to help them answer questions or provide data driven recommendations
Champion the Data Science capabilities and service offerings within the organization
What You Bring
4+ years work experience in various roles within forecasting, analytics, or merchandising.
University degree in engineering, math, computer science, statistics, business or related field
Ability to formulate or quickly understand business problems, find patterns and insights within structured and unstructured data
Proficiency with programming in Python, Spark, SQL. Knowledge of R and VBA would also be an asset.
Ability to clearly communicate results to technical and non-technical audiences
Self-starter attitude with willingness to learn and master new technologies
Ability to work independently and in an ambiguous environment
Experience with Alteryx, Tableau and/or other data manipulation and visualization software considered an asset.
Hybrid
We value flexibility. We have adopted a hybrid work model whereby employees use a combination of working in office and virtually in service of outcomes. Each leader is empowered to decide what work is best achieved in person based on the unique needs of their team.
About Us
At Sport Chek, we want to find what moves you, and help build your career from there. As Canada’s destination for footwear, apparel, and sporting goods; we believe that sport and activity has the power to help shape your style, your relationships, and your mind. Along with colleagues across the Canadian Tire family of companies, you’ll be a part of a collaborative network of like-minded individuals who draw on their collective experience to best serve customers across all banners from coast-to-coast. Join us, where there's a place for you here.
Our Commitment to Diversity, Inclusion and Belonging
We are committed to fostering an environment where belonging thrives, and diversity, inclusion and equity are infused into everything we do. We believe in building an organizational culture where people are consistently treated with dignity while respecting individual religion, nationality, gender, race, age, perceived ability, spoken language, sexual orientation, and identification. We are united in our purpose of being here to help make life in Canada better.
Accommodations
We stand firm in our Core Value that inclusion is a must. We welcome and encourage candidates from equity-seeking groups such as people who identify as racialized, Indigenous, 2SLGBTQIA+, women, people with disabilities, and beyond. Should you require any accommodation in applying for this role, or throughout the interview process, please make them known when contacted and we will work with you to help meet your needs.
Show more
Show less","Statistical modelling, Predictive analysis, Machine learning, Artificial intelligence, Data mining, Data transformation, Data visualization, Python, Spark, SQL, R, VBA, Alteryx, Tableau, Data manipulation software, Data visualization software, Business intelligence","statistical modelling, predictive analysis, machine learning, artificial intelligence, data mining, data transformation, data visualization, python, spark, sql, r, vba, alteryx, tableau, data manipulation software, data visualization software, business intelligence","alteryx, artificial intelligence, business intelligence, data manipulation software, data mining, data transformation, data visualization software, machine learning, predictive analysis, python, r, spark, sql, statistical modelling, tableau, vba, visualization"
Staff Data Engineer,Recruiting from Scratch,"Edmonton, Alberta, Canada",https://ca.linkedin.com/jobs/view/staff-data-engineer-at-recruiting-from-scratch-3744397218,2023-12-17,Alberta, Canada,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our client is a dating app.
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, deliver analysis to further improve engagement in existing features, and empower our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Solve technical problems of the highest scope and complexity
Exert significant influence on the company’s analytical long-range goals and data architecture
Define and extend our internal standards for style, maintenance, and best practices for a high-scale data platform
Provide mentorship for all on your team to help them grow in their technical responsibilities
Propose ideas to improve the scale, performance, and capabilities of the Data Platform
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We'll Love About You
7+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field
7+ years experience using Python
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention
Location
: This role will be located in office twice a week minimum in Palo Alto, San Francisco or Chicago.
Salary Range: $180,000 - 250,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Big Data, Data Engineering, Data Science, Data Governance, Data Architecture, Scalability, ETL, SQL, NoSQL, Python, Kubernetes, Docker, Helm, Spark, pySpark, Kafka, Storm, SparkStreaming, Dimensional Data Modeling, Schema Design, Data Warehouses, Agile Engineering Practices, TDD, Pair Programming, Continuous Integration, Automated Testing, Deployment, StreamProcessing Systems","big data, data engineering, data science, data governance, data architecture, scalability, etl, sql, nosql, python, kubernetes, docker, helm, spark, pyspark, kafka, storm, sparkstreaming, dimensional data modeling, schema design, data warehouses, agile engineering practices, tdd, pair programming, continuous integration, automated testing, deployment, streamprocessing systems","agile engineering practices, automated testing, big data, continuous integration, data architecture, data engineering, data governance, data science, data warehouses, deployment, dimensional data modeling, docker, etl, helm, kafka, kubernetes, nosql, pair programming, python, scalability, schema design, spark, sparkstreaming, sql, storm, streamprocessing systems, tdd"
Data Analyst Part Time,Voxmediallc,"Fort McMurray, Alberta, Canada",https://ca.linkedin.com/jobs/view/data-analyst-part-time-at-voxmediallc-3757203918,2023-12-17,Alberta, Canada,Mid senior,Onsite,"Summary:
Data Analyst Part Time will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : hiring@jobsai.live
Powered by Webbtree
Show more
Show less","Data analysis, Statistical techniques, Data visualization, SQL, R, Python, Tableau, Power BI, Statistical modeling, Hypothesis testing, A/B testing, Data management, ETL, Cloud Computing, Machine Learning","data analysis, statistical techniques, data visualization, sql, r, python, tableau, power bi, statistical modeling, hypothesis testing, ab testing, data management, etl, cloud computing, machine learning","ab testing, cloud computing, data management, dataanalytics, etl, hypothesis testing, machine learning, powerbi, python, r, sql, statistical modeling, statistical techniques, tableau, visualization"
Data Engineering Consultant,CGI,"Edmonton, Alberta, Canada",https://ca.linkedin.com/jobs/view/data-engineering-consultant-at-cgi-3779908375,2023-12-17,Alberta, Canada,Mid senior,Onsite,"Position Description
*** Role is required in Edmonton, AB, Canada ***
Are you ready to join an outstanding culture that cares about diversity and inclusion, corporate social responsibility and gives the freedom to innovate, influence decisions, and chart an exciting career?
CGI is more than just an IT consulting company; be part of a team that supports the local community with the ability draw on global best-in-class talent. CGI is looking for a Data Engineer for its expanding team to assist our clients with their enterprise data projects. Benefits include a share purchase program, profit sharing, wellness credits, training and development programs, and flexible work schedules and locations.
The responsibilities and requirements that follow provide insight into what a Member at CGI will typically experience in this role and the characteristics they have that lead to success. We are looking for candidates that have a similar mix of experience and qualifications. These are individuals who have a passion to create value for clients by helping them unlock the value in their data assets.
Your future duties and responsibilities
Create and support analytical data infrastructure by gathering, processing, analyzing, and structuring large volumes of data from many structured and unstructured data sources, at scale.
Design, develop and implement highly scalable, repeatable, and secure data pipelines and transformation processes.
Design and build transformation models and data flows for batch, real-time, and complex event-driven processes.
Develop data ingest processes across a variety of third-party APIs, applications, and file stores.
Ensure that appropriate controls are in place and all in-motion and at-rest data is secured at all times.
Develop data catalogs and data validation scripts to ensure data accuracy, clarity, and correctness of key business metrics.
Employ proper data governance to ensure data security and integrity.
Research and make recommendations for new data management technologies and software engineering practices. Collaborate on decisions around the use of new tools and practices.
Provide guidance to a customer and project team with respect to data requirements, data gaps, and the level of effort required to deliver a solution.
Produce and maintain support documentation and data dictionaries.
Qualifications To Be Successful In This Role
Experience with Azure or AWS Cloud and on-prem environments.
Experience with modern software development techniques and methodologies: DevOps, Agile, etc.
Experience with data ecosystem tools such as Azure Data Factory, Azure Databricks, Azure Synapse Analytics, Snowflake, Hadoop, Spark, etc.
Experience working with SQL, PowerBI, and or other query and reporting tools.
Experience using scripting languages such as Python, Scala, Spark, Spark-Streaming, Kafka, etc.
Knowledge and practice of secure software development processes.
Attributes
Up-to-date with the latest technology trends and have a strong desire to constantly learn.
Highly detailed-oriented with exceptional organizational and follow-through skills.
Self-starter and able to execute without a lot of direction or oversight - demonstrable problem-solving skills.
Value teamwork and urgency, with a passion for driving impact.
Ability to handle multiple priorities and deadlines.
Exceptional communication skills, with an ability to make technical concepts accessible and understandable to non-technical business users.
Passionate for turning disparate streams of data into organized and actionable analytics programming acumen, competency in manipulating large volumes of data, and a solid knowledge of a broad range of technologies for data processing and modeling.
Education
Bachelor’s degree or diploma in mathematics, informatics, statistics, computer science, or information systems (or equivalent combination of skill and experience).
Your future duties and responsibilities
Required Qualifications To Be Successful In This Role
Insights you can act on
While technology is at the heart of our clients’ digital transformation, we understand that people are at the heart of business success.
When you join CGI, you become a trusted advisor, collaborating with colleagues and clients to bring forward actionable insights that deliver meaningful and sustainable outcomes. We call our employees ""members"" because they are CGI shareholders and owners and owners who enjoy working and growing together to build a company we are proud of. This has been our Dream since 1976, and it has brought us to where we are today — one of the world’s largest independent providers of IT and business consulting services.
At CGI, we recognize the richness that diversity brings. We strive to create a work culture where all belong and collaborate with clients in building more inclusive communities. As an equal-opportunity employer, we want to empower all our members to succeed and grow. If you require an accommodation at any point during the recruitment process, please let us know. We will be happy to assist.
Ready to become part of our success story? Join CGI — where your ideas and actions make a difference.
Show more
Show less","Data Engineering, Data Analysis, Data Pipelines, Data Transformation, Data Governance, Data Security, Azure, AWS, DevOps, Agile, Azure Data Factory, Azure Databricks, Azure Synapse Analytics, Snowflake, Hadoop, Spark, SQL, PowerBI, Python, Scala, SparkStreaming, Kafka, Mathematics, Informatics, Statistics, Computer Science, Information Systems","data engineering, data analysis, data pipelines, data transformation, data governance, data security, azure, aws, devops, agile, azure data factory, azure databricks, azure synapse analytics, snowflake, hadoop, spark, sql, powerbi, python, scala, sparkstreaming, kafka, mathematics, informatics, statistics, computer science, information systems","agile, aws, azure, azure data factory, azure databricks, azure synapse analytics, computer science, data engineering, data governance, data security, data transformation, dataanalytics, datapipeline, devops, hadoop, informatics, information systems, kafka, mathematics, powerbi, python, scala, snowflake, spark, sparkstreaming, sql, statistics"
Staff Data Engineer,Recruiting from Scratch,"Calgary, Alberta, Canada",https://ca.linkedin.com/jobs/view/staff-data-engineer-at-recruiting-from-scratch-3744399190,2023-12-17,Alberta, Canada,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our client is a dating app.
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, deliver analysis to further improve engagement in existing features, and empower our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Solve technical problems of the highest scope and complexity
Exert significant influence on the company’s analytical long-range goals and data architecture
Define and extend our internal standards for style, maintenance, and best practices for a high-scale data platform
Provide mentorship for all on your team to help them grow in their technical responsibilities
Propose ideas to improve the scale, performance, and capabilities of the Data Platform
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We'll Love About You
7+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field
7+ years experience using Python
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention
Location
: This role will be located in office twice a week minimum in Palo Alto, San Francisco or Chicago.
Salary Range: $180,000 - 250,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Python, SQL, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, Kafka, Storm, SparkStreaming, Data Warehouses, ETL, Data management tools, TDD, Pair Programming, Continuous Integration, Automated testing, Deployment","python, sql, snowflake, airflow, kubernetes, docker, helm, spark, kafka, storm, sparkstreaming, data warehouses, etl, data management tools, tdd, pair programming, continuous integration, automated testing, deployment","airflow, automated testing, continuous integration, data management tools, data warehouses, deployment, docker, etl, helm, kafka, kubernetes, pair programming, python, snowflake, spark, sparkstreaming, sql, storm, tdd"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Edmonton, Alberta, Canada",https://ca.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3748833043,2023-12-17,Alberta, Canada,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our next evolution is underway as a newly public company looking to expand and continue to build meaningful experiences for our users. From social issues to original content, we’re blazing innovative paths with impact for our community, all while leveraging the latest tech stacks and striving for engineering excellence. At the heart of our work in this new chapter is a shared set of core values: openness and exploration, a bias for action, and strong support of the LGBTQ community.
With a track record of strong financial performance and plans for continued headcount growth, we’re looking to build a team of talented, passionate, and open-minded people who believe in our mission, align with our values, and are excited to work at the intersection of innovative technology and social impact. Come be a part of this exciting journey with us.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location: Palo Alto, San Francisco or Chicago
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, SQL, Kafka, Storm, SparkStreaming, Data Warehouses, ETL, Data Governance, Data Security, Data Scalability, TDD, Pair Programming, Continuous Integration, Automated testing, Deployment","python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, kafka, storm, sparkstreaming, data warehouses, etl, data governance, data security, data scalability, tdd, pair programming, continuous integration, automated testing, deployment","airflow, automated testing, continuous integration, data governance, data scalability, data security, data warehouses, deployment, docker, etl, helm, kafka, kubernetes, pair programming, python, snowflake, spark, sparkstreaming, sql, storm, tdd"
Senior Data Analyst - Azure,CGI,"Edmonton, Alberta, Canada",https://ca.linkedin.com/jobs/view/senior-data-analyst-azure-at-cgi-3779910013,2023-12-17,Alberta, Canada,Mid senior,Onsite,"Position Description
CGI Edmonton is looking for a Data Analyst to support the data migration of a few legacy systems to a new cloud-based COTS solution. Additionally, there are data quality improvement requirements and migration requirements and there may be an opportunity to support the development of data-based enhancements (i.e., automate a reporting process).
Become a part of an outstanding culture that gives you the freedom to innovate, influence decisions, achieve your full potential, and chart your career. Our benefits include a share purchase program, profit sharing, wellness credits, training and development programs, and flexible work schedules).
Your future duties and responsibilities
Support data migration from legacy systems to a new cloud-based COTS solution
Identify opportunities for data quality improvements and enhancements
Solve some of the most challenging and impactful problems using your data expertise, and ability to communicate compelling insight through data visualizations, reports, and dashboards
Help create the platform, tools, and APIs necessary to enable other teams to work with data
Efficiently handle vast amounts of data from multiple sources and destinations, including relational and NoSQL databases as well as external systems, both in batch processing and real-time delivery
Work on high-priority initiatives using advanced analytics, predictive modeling, and data from a variety of data sources to produce actionable business insights
Provide guidance to a customer and project team with respect to technical feasibility, complexity, and level of effort required to deliver a solution
Work closely with other team members to further develop metrics, KPIs, and insights that measure business performance improvements
Assist in the development and delivery of pre and post-sales POCs, presentations, and proposals for client engagements
Required Qualifications To Be Successful In This Role
7-10 years of experience doing data analysis, data engineering, reporting, and advanced analytical modeling
Hands-on experience in deploying reporting and analytical solutions to on-premise and cloud-based infrastructure
5+ years of hands-on experience with Azure, SQL Server, DataFactory
Hands-on experience with data reporting and visualization tools like Power BI, Tableau etc.
Experience with modern software development techniques and methodologies: DevOps, Agile, etc.
Knowledge and practice of secure software development processes
Desired Experience/Attributes:
Experience with motor vehicle registry data
Previous experience performing data migration is an asset
Staying up to date with the latest technology trends and strongly desire to learn constantly.
Highly detail-oriented with exceptional organizational and follow-through skills.
Self-starter and able to execute without direction or oversight - demonstrable problem-solving skills.
Value teamwork and urgency, with a passion for driving impact.
Ability to handle multiple priorities and deadlines.
Exceptional communication skills, with an ability to make technical concepts accessible and understandable to non-technical business users.
Education:
Bachelor’s degree or diploma in mathematics, informatics, statistics, computer science, or information systems (or equivalent combination of skill and experience)
Insights you can act on
While technology is at the heart of our clients’ digital transformation, we understand that people are at the heart of business success.
When you join CGI, you become a trusted advisor, collaborating with colleagues and clients to bring forward actionable insights that deliver meaningful and sustainable outcomes. We call our employees ""members"" because they are CGI shareholders and owners and owners who enjoy working and growing together to build a company we are proud of. This has been our Dream since 1976, and it has brought us to where we are today — one of the world’s largest independent providers of IT and business consulting services.
At CGI, we recognize the richness that diversity brings. We strive to create a work culture where all belong and collaborate with clients in building more inclusive communities. As an equal-opportunity employer, we want to empower all our members to succeed and grow. If you require an accommodation at any point during the recruitment process, please let us know. We will be happy to assist.
Ready to become part of our success story? Join CGI — where your ideas and actions make a difference.
Show more
Show less","Data Analysis, Data Engineering, Data Migration, Reporting, Advanced Analytical Modeling, Data Quality Improvement, Data Visualization, Predictive Modeling, DevOps, Agile, Azure, SQL Server, DataFactory, Power BI, Tableau, NoSQL Databases, Relational Databases, Machine Learning, Artificial Intelligence, Data Science, Python, Java, R, C++, MATLAB, Statistics, Mathematics, Business Intelligence, Data Integration, Cloud Computing, Big Data","data analysis, data engineering, data migration, reporting, advanced analytical modeling, data quality improvement, data visualization, predictive modeling, devops, agile, azure, sql server, datafactory, power bi, tableau, nosql databases, relational databases, machine learning, artificial intelligence, data science, python, java, r, c, matlab, statistics, mathematics, business intelligence, data integration, cloud computing, big data","advanced analytical modeling, agile, artificial intelligence, azure, big data, business intelligence, c, cloud computing, data engineering, data integration, data migration, data quality improvement, data science, dataanalytics, datafactory, devops, java, machine learning, mathematics, matlab, nosql databases, powerbi, predictive modeling, python, r, relational databases, reporting, sql server, statistics, tableau, visualization"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Edmonton, Alberta, Canada",https://ca.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3744398263,2023-12-17,Alberta, Canada,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
They are a profitable dating application.
Location: This is a hybrid role based in their Chicago office and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location
: This role is twice a week in a hybrid role minimum.
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, SQL, ETL, Data Science, Big Data, TDD, Pair Programming, Continuous Integration, Kafka, Storm, SparkStreaming, Dimensional Data Modeling, Data Warehouses","python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, etl, data science, big data, tdd, pair programming, continuous integration, kafka, storm, sparkstreaming, dimensional data modeling, data warehouses","airflow, big data, continuous integration, data science, data warehouses, dimensional data modeling, docker, etl, helm, kafka, kubernetes, pair programming, python, snowflake, spark, sparkstreaming, sql, storm, tdd"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Calgary, Alberta, Canada",https://ca.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3744394681,2023-12-17,Alberta, Canada,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
They are a profitable dating application.
Location: This is a hybrid role based in their Chicago office and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location
: This role is twice a week in a hybrid role minimum.
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, TDD, Automation, Continuous Delivery, Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, SQL, Agile, Pair Programming, Continuous Integration, Automated Testing, Kafka, Storm, SparkStreaming, Dimensional Data Modeling, Schema Design, Data Warehouses, ETL, Data Classification, Data Compliance","data engineering, tdd, automation, continuous delivery, python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, agile, pair programming, continuous integration, automated testing, kafka, storm, sparkstreaming, dimensional data modeling, schema design, data warehouses, etl, data classification, data compliance","agile, airflow, automated testing, automation, continuous delivery, continuous integration, data classification, data compliance, data engineering, data warehouses, dimensional data modeling, docker, etl, helm, kafka, kubernetes, pair programming, python, schema design, snowflake, spark, sparkstreaming, sql, storm, tdd"
Senior Data Engineering Advisor,Spirit Omega (Formerly Spirit Staffing & Consulting),"Calgary, Alberta, Canada",https://ca.linkedin.com/jobs/view/senior-data-engineering-advisor-at-spirit-omega-formerly-spirit-staffing-consulting-3782809275,2023-12-17,Alberta, Canada,Mid senior,Onsite,"Engineering and Technical – Calgary, AB, Canada – Contracted
Reference Number:
HSKYJP00008090
Company Overview
Office Location:
Calgary, AB, Canada.
Contract Duration:
1 year, 40 hours per week, 8 hours per day
Language:
English.
Description
You’ll be joining the Data Engineering team in the Corporate Data Office (CDO). Our vision is to enable all staff to be more creative through insights derived from our trusted data and analytical capabilities leading to improved safety, increased production, lower GHG emissions, and actionable efficiencies. Collaborating with stakeholders across the organization, the CDO provides the capabilities, services, and governance to protect, deploy, and exploit value.
The Data Engineering team endeavors to deliver trusted, secure, high-quality, and easily consumable data sets to data consumers and practitioners across the organization. You’ll be joining a fun, exciting team comprised of highly motivated members working towards the common goal of democratizing our data.
Key Responsibilities
You will apply your skills to help engineer data that provides business insights for our company.
Work with various data stakeholders throughout the organization to identify opportunities for leveraging company data that drives business solutions.
Building, fostering, and participating in a data community to connect data producers with data consumers and develop data literacy.
Work with various data stakeholders across the organization in the utilization of a modern data architecture that enables the collection, transformation, storage, modeling, and delivery of data to consumers and practitioners.
Apply data governance practices and procedures in the design and development of data pipelines.
Collaborate with cross-functional teams to design and develop data pipelines for visualization or analytic solutions.
Perform maintenance and support activities for data pipelines.
Support the build of big data environments that enable analytics solutions on a variety of big data platforms, including assessing the usefulness of new technologies and advocating for their adoption.
Education/Experience
Undergraduate degree in Computer Science, Engineering, Applied Mathematics, or Commerce with a Business Analytics concentration, or related Information Science.
5 - 10+ years of experience in a data-related role
Good knowledge of oil and gas operations
Familiarity with common oil & gas applications (Well and land, Subsurface, Production operations, Drilling, Engineering, etc.)
Competently communicates both verbally and in writing with strong listening skills.
Willingness to learn and offer new creative ways to work that makes our team more agile and adaptable.
Commitment to being a team player who values and respects the differences in working within a diverse and inclusive team.
Technical expertise in developing data integrations, logical and physical data models, data mining, and data warehousing.
Proficiency in SQL
Deep understanding of architectures of traditional data architecture practices (MDM, ODS, data warehousing, and others), as well as transition to Next-Gen platforms such as Lakehouse Architecture, Cloud, distributed ledgers, etc.
Hands-on experience with various relational and NoSQL database technologies, including SQL Server, DB2, Oracle, HANA, Azure DB, AWS RDS, etc.
Understanding of Data Governance strategy, practices, and procedures to effectively deliver secure, trusted, and high-quality data.
Experience creating and maintaining data pipelines that operate on a variety of sources, such as APIs, FTP sites, cloud-based blob stores, databases (relational and non-relational), unstructured data, etc.
Experience with big data platforms, cloud technologies, and server-less environments (such as AWS, Azure, Redshift, EMR, etc.)
Proficiency in one or more of the following data modeling/ data munging/ data management toolsets such as Databricks, Data Virtualization, SQL Server, Stream sets, Azure Synapse, Redshift, Azure Data factory, AWS Glue, Alation Data Catalog, etc.
Proficiency in big data technologies including querying, creating, and populating data lakes.
Proficiency in any of the following programming languages, frameworks, and technologies: Python, R, and open-source databases such as PostgreSQL, MySQL, and MongoDB.
If you would like to apply to this position or any others, please send your resume and cover letter to [email protected]
**We thank all applicants for applying, however, only those considered for an interview will be contacted directly**
Spirit Omega is committed to a diverse and inclusive workplace. We welcome applications from anyone, including members of visible minorities, women, Indigenous peoples, persons with disabilities, persons of minority sexual orientations and gender identities, and others with the skills and knowledge to productively engage with diverse communities.
Show more
Show less","Data Engineering, Oil and Gas Operations, SQL, Data Mining, Logical and Physical Data Models, Traditional Data Architecture Practices, NextGen Platforms, Lakehouse Architecture, Cloud, Distributed Ledgers, Relational and NoSQL Database Technologies, Data Governance, Data Pipelines, Big Data Platforms, Cloud Technologies, Serverless Environments, Data Modeling, Data Munging, Data Management, Databricks, Data Virtualization, Stream Sets, Azure Synapse, Redshift, Azure Data Factory, AWS Glue, Alation Data Catalog, Big Data Technologies, Python, R, OpenSource Databases, PostgreSQL, MySQL, MongoDB","data engineering, oil and gas operations, sql, data mining, logical and physical data models, traditional data architecture practices, nextgen platforms, lakehouse architecture, cloud, distributed ledgers, relational and nosql database technologies, data governance, data pipelines, big data platforms, cloud technologies, serverless environments, data modeling, data munging, data management, databricks, data virtualization, stream sets, azure synapse, redshift, azure data factory, aws glue, alation data catalog, big data technologies, python, r, opensource databases, postgresql, mysql, mongodb","alation data catalog, aws glue, azure data factory, azure synapse, big data platforms, big data technologies, cloud, cloud technologies, data engineering, data governance, data management, data mining, data munging, data virtualization, databricks, datamodeling, datapipeline, distributed ledgers, lakehouse architecture, logical and physical data models, mongodb, mysql, nextgen platforms, oil and gas operations, opensource databases, postgresql, python, r, redshift, relational and nosql database technologies, serverless environments, sql, stream sets, traditional data architecture practices"
Senior Data Engineering Advisor,Oil and Gas Job Search Ltd,"Calgary, Alberta, Canada",https://ca.linkedin.com/jobs/view/senior-data-engineering-advisor-at-oil-and-gas-job-search-ltd-3785105358,2023-12-17,Alberta, Canada,Mid senior,Onsite,"Location: Calgary
Contract: 12 months with a view to extend
Schedule: Monday to Friday. 4 days from office, 1 day from home.
About The Job
Petroplan are working on behalf of a leading Oil & Gas owner/operator:
You'll be joining the Data Engineering team in the Corporate Data Office (CDO). Our vision is to enable all staff to be more creative through insights derived from our trusted data and analytical capabilities leading to improved safety, increased production, lower GHG emissions, and actionable efficiencies. Collaborating with stakeholders across the organization, the CDO provides the capabilities, services and governance to protect, deploy and exploit value from data assets.
The Data Engineering team endeavors to deliver trusted, secure, high quality and easily consumable data sets to data consumers and practitioners across the organization. You'll be joining a fun, exciting team comprised of highly motivated members working towards the common goal of democratizing our data.
What you'll do
You will apply your skills to help engineer data that provides business insights for our company
Work with various data stakeholders throughout the organization to identify opportunities for leveraging company data that drives business solutions
Building, fostering and participating in a data community to connect data producers with data consumers and develop data literacy
Work with various data stakeholders across the organization in the utilization of a modern data architecture that enables the collection, transformation, storage, modeling and delivery of data to consumers and practitioners
Apply data governance practices and procedures in the design and development of data pipelines
Collaborate with cross-functional teams to design and develop data pipelines for visualization or analytic solutions
Perform maintenance and support activities for data pipelines
Support build of big data environments that enable analytics solutions on a variety of big data platforms, including assessing the usefulness of new technologies and advocating for their adoption
What you'll bring to this role
Undergraduate degree in Computer Science, Engineering, Applied Mathematics, Commerce with a Business Analytics concentration or related Information Science.
Good knowledge of oil and gas operations
Familiarity with common oil & gas applications (Well & Land, Subsurface, Production operations, Drilling, Engineering, etc.)
Competently communicates both verbal and written with strong listening skills
5 - 10 years of experience in a data related role
Willingness to learn and offer new creative ways to work that makes our team more agile and adaptable
Commitment to being a team player who values and respects the differences in working within a diverse and inclusive team.
Technical expertise in developing data integrations, logical and physical data models, data mining, and data warehousing.
Proficiency in SQL
Deep understanding of architectures of traditional data architecture practices (MDM, ODS, data warehousing and other), as well transition to Next-Gen platforms such as Lakehouse Architecture, Cloud, distributed ledgers, etc.
Hands-on experience with various relational and NoSQL database technologies, including SQL Server, DB2, Oracle, HANA, Azure DB, AWS RDS, etc.
Understanding of Data Governance strategy, practices and procedures to effectively deliver secure, trusted, and high-quality data.
Experience creating and maintaining data pipelines that operate on a variety of sources, such as APIs, FTP sites, cloud-based blob stores, databases (relational and non-relational), unstructured data, etc.
Experience with big data platforms, cloud technologies and server-less environments (such as AWS, Azure, Redshift, EMR, etc.)
Proficiency in one or more of the following data modelling/ data munging/ data management toolsets such as Databricks, Data Virtualization, SQL Server, Streamsets, Azure Synapse, Redshift, Azure Data factory, AWS Glue, Alation Data Catalog, etc.
Proficiency in big data technologies including querying, creating and populating data lakes.
Proficiency in any of the following programming languages, frameworks and technologies: Python, R, open source databases such as PostgreSQL, MySQL, MongoDB
Nice to Haves
Proficiency in data analytics and visualization platforms such as Microsoft Power BI or Spotfire
Proficiency in common machine Learning libraries in Python, Spark, and Deep Learning/AI frameworks
About Petroplan
Petroplan is the trusted, specialist global Talent Solutions partner of choice for employers and professionals in the Energy sector. Since 1976, Petroplan has been here to help people like you make the most of the opportunities available and find the best fit for you as an individual.
Over 10,000 placements in more than 55 countries for over 550 clients across 65 disciplines.
In an industry where skilled and experienced professionals are increasingly sought after resource, we appreciate the true value of what you have to offer. What's more, we understand that different things are important to different people in today's world of work - it's about making exactly the right connections for you as an individual.
Our reputation for contractor and candidate care, understanding what makes you tick, and finding you the best match is second-to-none. We understand the industry inside-out and seek to understand YOU and your motivations in the same way. We take the time to evaluate your individual strengths, understand the most important things to you, and establish exactly what you're looking for from your next role.
Show more
Show less","SQL, Data Governance, Data Modelling, Data Munging, Data Management, Data Warehousing, Data Pipelines, Data Lakes, Big Data Platforms, Cloud Technologies, Machine Learning, Python, R, PostgreSQL, MySQL, MongoDB, Microsoft Power BI, Spotfire, Deep Learning, AI","sql, data governance, data modelling, data munging, data management, data warehousing, data pipelines, data lakes, big data platforms, cloud technologies, machine learning, python, r, postgresql, mysql, mongodb, microsoft power bi, spotfire, deep learning, ai","ai, big data platforms, cloud technologies, data governance, data lakes, data management, data modelling, data munging, datapipeline, datawarehouse, deep learning, machine learning, microsoft power bi, mongodb, mysql, postgresql, python, r, spotfire, sql"
Senior Data Engineering Specialist (Insurance),AMA - Alberta Motor Association,"Edmonton, Alberta, Canada",https://ca.linkedin.com/jobs/view/senior-data-engineering-specialist-insurance-at-ama-alberta-motor-association-3751625513,2023-12-17,Alberta, Canada,Mid senior,Onsite,"AMA
It’s All About The
(insurance) data
Posted: 2023-11-06
Closing: When Filled
Location: Edmonton Kingsway (Hybrid)
You’re definitely a “Techie” who’s passionate about finding the right solutions and ensuring a smooth-running system environment. You are not afraid to pave the way for bigger and better things. As a Senior Data Engineer for the AMA Insurance department, you’ll be responsible for the collecting, sorting, processing, and analyzing of huge sets of data. We are looking for someone whose primary focus will be on finding creative optimal solutions to use for these purposes, then maintaining, implementing, and monitoring them.
What Moves You
You have strong analytical and problem-solving skills for technology and business.
You’re an excellent communicator and an even better listener — it’s probably why you’re also a great negotiator.
You’re not only motivated to go above and beyond in your own role, you know how to motivate others and get everyone on board.
You love being able to share your knowledge and experience to help others grow and excel in their careers.
Your superior organizational skills leave no room for clutter — in your workspace or in your brain.
You’re just as good at managing people as you are at managing projects, thanks to your upbeat attitude and ability to stay calm when others aren’t.
What You’ll Do
Report to the Manager, Data Engineering.
Design & implement solutions to enhance our insurance customer’s experience.
Design & develop data framework to support customer events (Data Modelling, ETL, Data Cleansing, Data Enrichment).
Investigate new products, tools, and data technologies (Big Data, Data Mining, Deep Learning) to add value to the business; ensure applications are on the right platform and that we’re in a sustainable position moving forward.
Provide operational support for existing data warehouse environment.
Use best practices to write well designed, testable, and efficient code.
Identify technical debt/operational improvements & implement solutions.
Meet with business partners to understand & gather requirements for strategic initiatives.
Work in a collaborative space with a team of people.
What You’ve Done
You have a university degree in computer science or computer engineering.
You have a minimum of 7 years’ progressive experience in data analysis and/or business analysis.
You are an expert with Relational Database engines such as Snowflake, SQL Server, Oracle, mySQL, etc.
Proficiency with Big Data technologies. Experience with Snowflake, AWS, Attunity Replicate and WhereScape RED is preferred.
You are familiar with Design & Engineering of Data Warehouses (Data Modelling, ETL, Data Cleansing, Data Enrichment).
Experience with Business Intelligence technologies such as Power BI, Tableau, Pentaho, etc.
Proven track record with the Kimball Methodology, Data Vault and Agile Methodologies.
Practical experience with software development and data engineering practices.
Skilled with test driven development (TDD), continuous integration and deployment.
What You'll Get
Competitive salary.
Flexible benefits.
Outstanding employer paid Retirement Savings Program.
Great AMA discounts.
Unlimited learning opportunities.
Paid Vacation and other paid time off including a Volunteer Day and Me-Day.
We thank all applicants for their interest; however, only those selected for an interview will be contacted.
Show more
Show less","Data Engineering, SQL, Snowflake, AWS, Pentaho, Power BI, Tableau, Data Modeling, ETL, Data Cleansing, Data Enrichment, Kimball Methodology, Data Vault, Agile Methodologies, Test Driven Development (TDD), Continuous Integration and Deployment, Data Mining, Deep Learning, Big Data, Data Warehouse, Data Analysis, Business Analysis, Communication, ProblemSolving, Negotiation, Motivation, Organization, Project Management","data engineering, sql, snowflake, aws, pentaho, power bi, tableau, data modeling, etl, data cleansing, data enrichment, kimball methodology, data vault, agile methodologies, test driven development tdd, continuous integration and deployment, data mining, deep learning, big data, data warehouse, data analysis, business analysis, communication, problemsolving, negotiation, motivation, organization, project management","agile methodologies, aws, big data, business analysis, communication, continuous integration and deployment, data engineering, data enrichment, data mining, data vault, dataanalytics, datacleaning, datamodeling, datawarehouse, deep learning, etl, kimball methodology, motivation, negotiation, organization, pentaho, powerbi, problemsolving, project management, snowflake, sql, tableau, test driven development tdd"
Senior Manager - Data Architect - Calgary,EY,"Calgary, Alberta, Canada",https://ca.linkedin.com/jobs/view/senior-manager-data-architect-calgary-at-ey-3779708325,2023-12-17,Alberta, Canada,Mid senior,Onsite,"At EY, you’ll have the chance to build a career as unique as you are, with the global scale, support, inclusive culture and technology to become the best version of you. And we’re counting on your unique voice and perspective to help EY become even better, too. Join us and build an exceptional experience for yourself, and a better working world for all.
Role Description: Data Architect
At EY, you’ll have the chance to build a career as unique as you are, with the global scale, support, inclusive culture and technology to become the best version of you. And we’re counting on your unique voice and perspective to help EY become even better, too. Join us and build an exceptional experience for yourself, and a better working world for all.
The opportunity
We are seeking an experienced Data Architect that can translate business strategies and requirements into world-class technical solutions, applications, and process designs. The role assumes the responsibility for all technical aspects of solution delivery, from inception through design to implementation. They will conduct regular detailed solution analysis and provide expert judgment pertaining to highly complex technical issues including all aspects related to development, infrastructure, data, and configuration management perspectives. This role requires a rare blend of skills normally found in the specialized roles of Cloud Architect, Data Analyst, Data Engineer and Reporting Specialist. The Data Architect will also assist in the establishment and enforcement of architectural standards and best practices to ensure consistency in data solutions delivery across development teams.
Your Key Responsibilities:
Lead and provide oversight into the design of end-to-end data solutions including data architecture, data integration, data modelling, security, privacy, regulatory, performance, data governance, business intelligence, etc. and to ensure alignment to the overall business needs
Provide technical oversight and recommendations on data infrastructure (on-prem and Cloud)
Lead discovery sessions with primary business stakeholders to obtain knowledge of business objectives, challenges, and requirements
Present and socialize proposed solutions to technical boards during review sessions as well as articulate and communicate designs to stakeholder groups
Support solution delivery by providing insights to the application development teams and project managers that plan and execute delivery activities
Support implementation processes in association with internal and external development teams. Participate in project-related activities.
Assume responsibility for the quality of the implemented solution and its adherence to technology strategies, standards, and best practices
Create solution architecture documents that describe and explain solution attributes and associated benefits
Stay current on emergent technologies relevant to supporting the data solutions delivered for customers
Support the identification of opportunities for continuous improvement of solution management methodologies, including obtaining feedback after the solution has been implemented
Assist in the identification and recommendation of appropriate solutions, upgrades, replacements, or decommissioning options incorporating business and technology productivity, usability, and total cost of ownership
To qualify for the role you must have:
Bachelor’s degree in Information Technology, Software Engineering, Computer Science, or related field. Related professional certifications and/or training is considered an asset.
10+ years demonstrated experience in the design and implementation of cloud infrastructure (Azure ADF, Synapse, Databricks and other cloud technologies)
Expert knowledge of data patterns to support architectures and data management concepts/processes including ETL/ELT, Data Mesh, date lake, data lakehouse, data warehouse, data streaming, data virtualization, etc.
Strong knowledge of ETL tools such as Informatica, SSIS, Talend, DataStage, etc. as well as standards and best practices for ETL development
Strong knowledge of data governance framework principles and vendor platforms (e.g., Collibra, Informatica EDC/Axon)
Experience with data manipulation and visualization tools such as Tableau, Power BI, etc.
Strong knowledge of SQL and demonstrated ability to apply SQL to a wide variety of business requirements.
Strong Knowledge of ETL frameworks using Azure Data Factory and Databricks Notebooks.
Strong knowledge in data warehousing concepts and data platforms (e.g., Snowflake, Databricks)
Strong knowledge of Azure-based architecture and services such as RBAC, AAD, VM, ADLS, Azure DevOps, and Events hub.
Proficient in building secure architecture using: Encryption, Azure key vault, Access control, Network security, and Advanced data security.
Demonstrated experience deploying data solutions in Big Data platforms (e.g., AWS, Azure, GCP, etc.).
Strong analytical and decision-making skills.
Experience working within multidisciplinary, distributed, and collaborative environments.
What We Look For
We’re interested in passionate leaders with a strong vision and a desire to stay on top of trends in the IT Advisory space. If you have a strong passion for helping businesses achieve their IT strategy, this role is for you.
What EY Offers
At EY, our Total Rewards package supports our commitment to creating a leading people culture - built on high-performance teaming - where everyone can achieve their potential and contribute to building a better working world for our people, our clients and our communities. It's one of the many reasons we repeatedly win awards for being a great place to work.
We offer a competitive compensation package where you’ll be rewarded based on your performance and recognized for the value you bring to our business. In addition, our Total Rewards package allows you decide which benefits are right for you and which ones help you create a solid foundation for your future. Our Total Rewards package includes a comprehensive medical, prescription drug and dental coverage, a defined contribution pension plan, a great vacation policy plus firm paid days that allow you to enjoy longer long weekends throughout the year, statutory holidays and paid personal days (based on province of residence), and a range of exciting programs and benefits designed to support your physical, financial and social well-being. Plus, we offer:
Support and coaching from some of the most engaging colleagues in the industry
Learning opportunities to develop new skills and progress your career
The freedom and flexibility to handle your role in a way that’s right for you
Diversity and Inclusion at EY
Diversity and inclusiveness are at the heart of who we are and how we work. We’re committed to fostering an environment where differences are valued, policies and practices are equitable, and our people feel a sense of belonging. We embrace diversity and are committed to combating systemic racism, advocating for the LGBT+ community, promoting our Neurodiversity Centre of Excellence and Accessibility initiatives, and are dedicated to amplifying the voices of Indigenous people (First Nations, Inuit, and Métis) nationally as we strive towards reconciliation. Our diverse experiences, abilities, backgrounds, and perspectives make our people unique and help guide us. Because when people feel free to be their authentic selves at work, they bring their best and are empowered to build a better working world.
If you can demonstrate that you meet the criteria above, please contact us as soon as possible.
The exceptional EY experience. It’s yours to build.
EY | Building a better working world
EY exists to build a better working world, helping to create long-term value for clients, people and society and build trust in the capital markets.
Enabled by data and technology, diverse EY teams in over 150 countries provide trust through assurance and help clients grow, transform and operate.
Working across assurance, consulting, law, strategy, tax and transactions, EY teams ask better questions to find new answers for the complex issues facing our world today.
Show more
Show less","Cloud Architecture, Data Analyst, Data Engineer, Reporting Specialist, Data Architecture, Data Integration, Data Modelling, Data Governance, Business Intelligence, Data Infrastructure, ETL/ELT, Data Lake, Data Lakehouse, Data Warehouse, Data Streaming, Data Virtualization, Data Manipulation, Data Visualization, Informatica, SSIS, Talend, DataStage, Collibra, Informatica EDC/Axon, Tableau, Power BI, SQL, Azure Data Factory, Azure Notebooks, Snowflake, Databricks, RBAC, AAD, VM, ADLS, Azure DevOps, Events Hub, Encryption, Azure Key Vault, Access Control, Network Security, Advanced Data Security, Big Data, AWS, Azure, GCP, Tableau, Power BI","cloud architecture, data analyst, data engineer, reporting specialist, data architecture, data integration, data modelling, data governance, business intelligence, data infrastructure, etlelt, data lake, data lakehouse, data warehouse, data streaming, data virtualization, data manipulation, data visualization, informatica, ssis, talend, datastage, collibra, informatica edcaxon, tableau, power bi, sql, azure data factory, azure notebooks, snowflake, databricks, rbac, aad, vm, adls, azure devops, events hub, encryption, azure key vault, access control, network security, advanced data security, big data, aws, azure, gcp, tableau, power bi","aad, access control, adls, advanced data security, aws, azure, azure data factory, azure devops, azure key vault, azure notebooks, big data, business intelligence, cloud architecture, collibra, data architecture, data governance, data infrastructure, data integration, data lake, data lakehouse, data manipulation, data modelling, data streaming, data virtualization, dataanalytics, databricks, dataengineering, datastage, datawarehouse, encryption, etlelt, events hub, gcp, informatica, informatica edcaxon, network security, powerbi, rbac, reporting specialist, snowflake, sql, ssis, tableau, talend, visualization, vm"
"Senior/Staff Software Engineer, Data",EvenUp,"Calgary, Alberta, Canada",https://ca.linkedin.com/jobs/view/senior-staff-software-engineer-data-at-evenup-3782667222,2023-12-17,Alberta, Canada,Mid senior,Remote,"EvenUp is a venture-backed generative AI startup that ensures injury victims are awarded the full value of their claims, expanding the $100B+ in awards granted to injury victims every year.
Every year, the legal system has made it difficult for millions of ordinary people to seek justice, especially for folks without means or who come from underrepresented backgrounds
. Our vision is to help these injury victims get the justice they deserve, irrespective of their income, demographics, or the quality of their legal representation.
EvenUp operates across all types of injury cases, from police brutality and child abuse to California wildfires and motor vehicle accidents. Our ML-driven software empowers attorneys to accurately assess the value of these cases by doing a core part of their workflow (legal drafting), enabling them to secure larger settlements in record time. As EvenUp evaluates more cases, our proprietary data grows, enhancing the precision of our predictions and delivering more value to both attorneys and victims alike.
As one of the fastest growing startups ($0 to $10M in ARR in <2 years), we raised $65M in investment from some of the best investors in the world (Bessemer, Bain Capital, Signalfire, DCM, NFX, Tribe Capital), seasoned tech executives (i.e. founder of Quora, SVP at Google, former CPO at Uber), and public figures that care about our social mission (Nas, Jared Leto, Byron Jones). Our team comes from top tech, legal, and investing backgrounds including Waymo, Google, Amazon, Uber, Quora, Blizzard, Norton Rose, Warburg Pincus, Bain, and McKinsey.
The role:
We’re looking to bring on board
Senior/Staff Software Engineers
focused on our
Data Pipelines
as we’ve experienced unprecedented growth and need to build & scale out our data pipelines and infrastructure. We’re looking for strong team members to help architect and drive forward the vision of our ideal data infrastructure at EvenUp. We will need to 10x our pipeline processing throughput over the next 12 months. We’ll need to rethink and rebuild how we extract, process and model our ingestion to enable our organization with precise and actionable data.
What you'll do:
Build fault tolerant data pipelines to process diverse datasets at EvenUp
Design and develop modularized services to increase the capabilities and scope of our data infrastructure
Collaborate with our DS team to Integrate ML models into our production workflows and simplify ML deployment and observability
Implement event driven, low latency systems to empower our stakeholders with accurate and reliable data
Analyze and solve key performance bottlenecks, scaling challenges, and high availability issues.
Mentor and coach junior team members
Help grow our engineering team and define a “data first” mentality across our organization.
What we are seeking:
8+ years of industry experience designing and building distributed data systems
Previous experience architecting and scaling event driven architectures
Strong understanding and practical experience with data pipeline tooling and storage systems such as Dagster, DBT, BigQuery, Elasticsearch
The ability to communicate cross-functionally with various stakeholders to derive requirements and architect scalable solutions
Have several years of industry experience building high-quality software, shipping production-ready code and infrastructure
You enjoy owning a project from start to finish and love to drive a project across the finish line.
Interest in making the world a fairer place (we don’t get paid unless we’re helping injured victims and/or their attorneys)
Nice to haves:
Fluency in Python, SQL and GraphQL
Previous experience integrating ML models and LLMs into data services
Domain expertise in legal technology, medical records, and working with unstructured data
Benefits & Perks:
We seek to empower all of our team members to fulfill our mission of making the world a more just place, regardless of our team’s function, geography, or experience level. To that end, we offer:
Fully remote setup - work from wherever you feel is best (Plus a stipend to upgrade your home office!)
Flexible working hours to match your style
Offsites - get to meet your coworkers on a fully-expensed trip every 6-12 months!
Choice of great medical, dental, and vision insurance plan options
Flexible paid time off
A variety of virtual team events such as game nights & happy hours
EvenUp is an equal-opportunity employer. We are committed to diversity and inclusion in our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.
Show more
Show less","Data Pipelines, Data Infrastructure, Distributed Data Systems, Event Driven Architectures, Data Pipeline Tooling, Storage Systems, Dagster, DBT, BigQuery, Elasticsearch, SQL, GraphQL, Machine Learning Models, LLMs, Python, Medical Records, Unstructured Data, Legal Technology","data pipelines, data infrastructure, distributed data systems, event driven architectures, data pipeline tooling, storage systems, dagster, dbt, bigquery, elasticsearch, sql, graphql, machine learning models, llms, python, medical records, unstructured data, legal technology","bigquery, dagster, data infrastructure, data pipeline tooling, datapipeline, dbt, distributed data systems, elasticsearch, event driven architectures, graphql, legal technology, llms, machine learning models, medical records, python, sql, storage systems, unstructured data"
"Senior/Staff Software Engineer, Data Pipelines",EvenUp,"Calgary, Alberta, Canada",https://ca.linkedin.com/jobs/view/senior-staff-software-engineer-data-pipelines-at-evenup-3766508891,2023-12-17,Alberta, Canada,Mid senior,Remote,"EvenUp is a venture-backed generative AI startup that ensures injury victims are awarded the full value of their claims, expanding the $100B+ in awards granted to injury victims every year.
Every year, the legal system has made it difficult for millions of ordinary people to seek justice, especially for folks without means or who come from underrepresented backgrounds
. Our vision is to help these injury victims get the justice they deserve, irrespective of their income, demographics, or the quality of their legal representation.
EvenUp operates across all types of injury cases, from police brutality and child abuse to California wildfires and motor vehicle accidents. Our ML-driven software empowers attorneys to accurately assess the value of these cases by doing a core part of their workflow (legal drafting), enabling them to secure larger settlements in record time. As EvenUp evaluates more cases, our proprietary data grows, enhancing the precision of our predictions and delivering more value to both attorneys and victims alike.
As one of the fastest growing startups ($0 to $10M in ARR in <2 years), we raised $65M in investment from some of the best investors in the world (Bessemer, Bain Capital, Signalfire, DCM, NFX, Tribe Capital), seasoned tech executives (i.e. founder of Quora, SVP at Google, former CPO at Uber), and public figures that care about our social mission (Nas, Jared Leto, Byron Jones). Our team comes from top tech, legal, and investing backgrounds including Waymo, Google, Amazon, Uber, Quora, Blizzard, Norton Rose, Warburg Pincus, Bain, and McKinsey.
The role:
We’re looking to bring on board
Senior/Staff Software Engineers
focused on our
Data Pipelines
as we’ve experienced unprecedented growth and need to build & scale out our data pipelines and infrastructure. We’re looking for strong team members to help architect and drive forward the vision of our ideal data infrastructure at EvenUp. We will need to 10x our pipeline processing throughput over the next 12 months. We’ll need to rethink and rebuild how we extract, process and model our ingestion to enable our organization with precise and actionable data.
What you'll do:
Build fault tolerant data pipelines to process diverse datasets at EvenUp
Design and develop modularized services to increase the capabilities and scope of our data infrastructure
Collaborate with our DS team to Integrate ML models into our production workflows and simplify ML deployment and observability
Implement event driven, low latency systems to empower our stakeholders with accurate and reliable data
Analyze and solve key performance bottlenecks, scaling challenges, and high availability issues.
Mentor and coach junior team members
Help grow our engineering team and define a “data first” mentality across our organization.
What we are seeking:
8+ years of industry experience designing and building distributed data systems
Previous experience architecting and scaling event driven architectures
Strong understanding and practical experience with data pipeline tooling and storage systems such as Dagster, DBT, BigQuery, Elasticsearch
The ability to communicate cross-functionally with various stakeholders to derive requirements and architect scalable solutions
Have several years of industry experience building high-quality software, shipping production-ready code and infrastructure
You enjoy owning a project from start to finish and love to drive a project across the finish line.
Interest in making the world a fairer place (we don’t get paid unless we’re helping injured victims and/or their attorneys)
Nice to haves:
Fluency in Python, SQL and GraphQL
Previous experience integrating ML models and LLMs into data services
Domain expertise in legal technology, medical records, and working with unstructured data
Benefits & Perks:
We seek to empower all of our team members to fulfill our mission of making the world a more just place, regardless of our team’s function, geography, or experience level. To that end, we offer:
Fully remote setup - work from wherever you feel is best (Plus a stipend to upgrade your home office!)
Flexible working hours to match your style
Offsites - get to meet your coworkers on a fully-expensed trip every 6-12 months!
Choice of great medical, dental, and vision insurance plan options
Flexible paid time off
A variety of virtual team events such as game nights & happy hours
EvenUp is an equal-opportunity employer. We are committed to diversity and inclusion in our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.
Show more
Show less","Software Engineering, Python, SQL, GraphQL, Dagster, DBT, BigQuery, Elasticsearch, Legal technology, Medical records, Unstructured data","software engineering, python, sql, graphql, dagster, dbt, bigquery, elasticsearch, legal technology, medical records, unstructured data","bigquery, dagster, dbt, elasticsearch, graphql, legal technology, medical records, python, software engineering, sql, unstructured data"
Part-Time Data Analyst (Remote),Progilisyssolutions,"Calgary, Alberta, Canada",https://ca.linkedin.com/jobs/view/part-time-data-analyst-remote-at-progilisyssolutions-3736396455,2023-12-17,Alberta, Canada,Mid senior,Remote,"As a Data Analyst , you will support the company's data-driven decision-making process by analyzing large amounts of data related to consumer behavior, trends and patterns. You will have the opportunity to work with a team of experts and gain valuable insights into the retail industry while developing your skills and knowledge in data analytics.
This is a position that requires excellent attention to detail, critical thinking and problem solving skills and the ability to work independently in a fast-paced environment.
Responsibilities
Assist in collecting, cleaning and transforming large data sets from various sources
Analyze and interpret data to identify trends and insights related to consumer behavior, market trends, and sales patterns
Assist in building and maintaining reporting and visualization tools to communicate data insights to key stakeholders
Develop and maintain dashboards, data visualizations, and other reporting solutions
Conduct ad hoc data analysis to support business needs
Collaborate with cross-functional teams to identify opportunities for improvement, provide data-driven insights and recommendations to support strategic decisions
Requirements
Currently enrolled in or recently graduated from a Bachelors or Masters degree program
Strong analytical and problem-solving skills
Ability to work independently and in a team-oriented environment
Excellent attention to detail and accuracy
Proficient in Microsoft Office Suite, especially Excel and PowerPoint
Familiarity with SQL, Python, or R programming languages
Powered by Webbtree
Show more
Show less","Data analysis, Data mining, Data visualization, Data reporting, SQL, Python, R, Microsoft Office Suite, Excel, PowerPoint, Data cleaning, Data transformation, Data interpretation, Trend analysis, Pattern recognition, Reporting tools, Visualization tools, Dashboards, Ad hoc analysis, Crossfunctional collaboration, Datadriven decisionmaking, Problem solving, Communication, Attention to detail, Accuracy","data analysis, data mining, data visualization, data reporting, sql, python, r, microsoft office suite, excel, powerpoint, data cleaning, data transformation, data interpretation, trend analysis, pattern recognition, reporting tools, visualization tools, dashboards, ad hoc analysis, crossfunctional collaboration, datadriven decisionmaking, problem solving, communication, attention to detail, accuracy","accuracy, ad hoc analysis, attention to detail, communication, crossfunctional collaboration, dashboard, data cleaning, data interpretation, data mining, data reporting, data transformation, dataanalytics, datadriven decisionmaking, excel, microsoft office suite, pattern recognition, powerpoint, problem solving, python, r, reporting tools, sql, trend analysis, visualization, visualization tools"
Senior Data Engineer,StellarAlgo,"Calgary, Alberta, Canada",https://ca.linkedin.com/jobs/view/senior-data-engineer-at-stellaralgo-3778865432,2023-12-17,Alberta, Canada,Mid senior,Hybrid,"Who We Are
At StellarAlgo we are inspired and united around innovating the future of fan experiences. We are brought together by the belief that accessible and actionable data inspires the personalized and authentic experiences that create lifelong fans. Through a combination of embracing challenges, a belief that alignment enables autonomy and a committed team, we believe in empowering our players to achieve big things where it comes to solving interesting problems with data. In fact, our biggest investment is in you; we give you what you need to focus on your professional growth and career development, all while developing software that directly impacts our customers and their fans. We will challenge you and you will be surrounded by people equally committed to the company’s success, allowing for constant collaboration.
The Role: Are you up for the challenge?
StellarAlgo is the leading customer cloud platform for live audience businesses. We’re focused on growing and monetizing the world's most passionate digital audiences by recruiting high performers who know how to go for gold, work with their team, solve problems in real time, and speak up with creative ideas.
The Senior Data Engineer will support the Director, Data Engineering, working closely with clients to understand not only what they want but what they need, helping them to solve some big and interesting problems. Using cutting edge technologies, this position has the freedom to fully leverage the AWS Platform to implement data pipelines and other processes that will enable the most scalable industry solution on the market.
What You'll Do
Implementation of teams data lake (platform) through working with: SQL / Python / Trino / AWS cloud infrastructure / PySpark / Github / Visual Studio code
Ability to communicate and perform data validation with internal and external stakeholders.
Driving proper SDLC process and policies.
Provide feedback for more junior team members through pair programming and code reviews.
Ability to develop end to end data pipelines from source data ingestion to Data Lake using both ELT and ETL patterns.
Drives use of data lake modeling best practices.
Drive design and optimization discussions with team members
Creation of IaC (terraform / openTofu) for standing up cloud-based assets.
What We're Looking For
Computer Science or Software Engineering Degree
5+ years of data related development and ELT/ETL processes
Curiosity for Data and how it impacts our day to day lives and that of our clients and sports / live entertainment fans.
Work with previous cloud systems for ELT/ETL data processing
Demonstrates python development skills for data development.
Lead a previous project with data development (ELT/ETL) processes and demonstrated leadership of fellow team members.
Experience in working with SDLC tools like Github / GitLabs
Demonstrates continuous self-improvement.
Collaborate and drive data development optimizations and best practices.
Experience retrieving data from vendor API endpoints.
Who You Are
You have been through many projects, have had successes and failures, and have learned from both, developing a robust toolbox for building data solutions, implementing patterns and leading the next generation of Data Engineers. You are a superb communicator, able to gain understanding from both technical and non-technical audiences, and pride yourself on your interpersonal and relationship management skills. You have experience working in and driving agile development best practices, and have a proven ability to document code and processes. You enjoy. being part of a results oriented, cross-functional team, and the words 'fast paced' don't intimidate you - you immediately think 'GAME ON'! Finally, at the end of the day, you're a data nerd at heart and proud of it!
The data is clear – diverse teams are not only the right way to go, they’re the profitable way to go. StellarAlgo is an equal opportunity employer that prioritizes creating an inclusive work environment for our team. We focus on hiring candidates with unique perspectives and opinions that improve our ability to apply creative solutions to complex problems.
Show more
Show less","Data Engineering, AWS, SQL, Python, Trino, PySpark, Github, Visual Studio Code, SDLC, ELT, ETL, Terraform, OpenTofu, Computer Science, Software Engineering, GitLab, API","data engineering, aws, sql, python, trino, pyspark, github, visual studio code, sdlc, elt, etl, terraform, opentofu, computer science, software engineering, gitlab, api","api, aws, computer science, data engineering, elt, etl, github, gitlab, opentofu, python, sdlc, software engineering, spark, sql, terraform, trino, visual studio code"
Senior Data Engineer,InSync Systems,"Calgary, Alberta, Canada",https://ca.linkedin.com/jobs/view/senior-data-engineer-at-insync-systems-3781741349,2023-12-17,Alberta, Canada,Mid senior,Hybrid,"We are looking for a
Senior Data Engineer
for a
12-month contract position, with possible extensions
in
Calgary, Alberta.
Must be legally entitled to work in Canada.
Hybrid schedule, at least 4 days in the office. 8 hours a day, 40 hours a week. It is NOT a remote role.
Responsibilities
You will apply your skills to help engineer data that provides business insights for our company.
Work with various data stakeholders throughout the organization to identify opportunities for leveraging company data that drives business solutions.
Building, fostering, and participating in a data community to connect data producers with data consumers and develop data literacy.
Work with various data stakeholders across the organization in the utilization of a modern data architecture that enables the collection, transformation, storage, modeling, and delivery of data to consumers and practitioners.
Apply data governance practices and procedures in the design and development of data pipelines.
Collaborate with cross-functional teams to design and develop data pipelines for visualization or analytic solutions.
Perform maintenance and support activities for data pipelines.
Support build of big data environments that enable analytics solutions on a variety of big data platforms, including assessing the usefulness of new technologies and advocating for their adoption.
Required Skills and Experience
Undergraduate degree in Computer Science, Engineering, Applied Mathematics, Commerce with a Business Analytics concentration, or related Information Science.
Good knowledge of oil and gas operations.
Familiarity with common oil & gas applications (Well & Land, Subsurface, Production operations, Drilling, Engineering, etc.).
5 - 10+ years of experience in a data related role.
Technical expertise in developing data integrations, logical and physical data models, data mining, and data warehousing.
Proficiency in SQL.
Deep understanding of architectures of traditional data architecture practices (MDM, ODS, data warehousing and other), as well transition to Next-Gen platforms such as Lakehouse Architecture, Cloud, distributed ledgers, etc.
Hands-on experience with various relational and NoSQL database technologies, including SQL Server, DB2, Oracle, HANA, Azure DB, AWS RDS, etc.
Understanding of Data Governance strategy, practices, and procedures to effectively deliver secure, trusted, and high-quality data.
Experience creating and maintaining data pipelines that operate on a variety of sources, such as APIs, FTP sites, cloud-based blob stores, databases (relational and non-relational), unstructured data, etc.
Experience with big data platforms, cloud technologies and server-less environments (such as AWS, Azure, Redshift, EMR, etc.).
Proficiency in one or more of the following data modelling/ data munging/ data management toolsets such as Databricks, Data Virtualization, SQL Server, Streamsets, Azure Synapse, Redshift, Azure Data factory, AWS Glue, Alation Data Catalog, etc.
Proficiency in big data technologies including querying, creating and populating data lakes.
Proficiency in any of the following programming languages, frameworks and technologies: Python, R, open-source databases such as PostgreSQL, MySQL, MongoDB.
Desired Skills and Experience
Proficiency in data analytics and visualization platforms such as Microsoft Power BI or Spotfire.
Proficiency in common machine Learning libraries in Python, Spark, and Deep Learning/AI frameworks.
Please note that while all applications are appreciated,
only candidates selected for interview will be contacted.
InSync Systems Inc.
is a privately-owned boutique Canadian Resourcing and Consulting Services Company that works closely with a range of corporate clients across multiple industries to bring them solutions that effectively address their business needs.
Show more
Show less","SQL, Data Integration, Logical and Physical Data Modeling, Data Mining, Data Warehousing, MDM, ODS, Lakehouse Architecture, Cloud, Distributed Ledgers, NoSQL, Data Governance, Data Pipeline Creation and Maintenance, Big Data Platforms, Cloud Technologies, Serverless Environments, Databricks, Data Virtualization, Streamsets, Azure Synapse, Redshift, Azure Data Factory, AWS Glue, Alation Data Catalog, Python, R, PostgreSQL, MySQL, MongoDB, Power BI, Spotfire, Machine Learning Libraries, Spark, Deep Learning, AI","sql, data integration, logical and physical data modeling, data mining, data warehousing, mdm, ods, lakehouse architecture, cloud, distributed ledgers, nosql, data governance, data pipeline creation and maintenance, big data platforms, cloud technologies, serverless environments, databricks, data virtualization, streamsets, azure synapse, redshift, azure data factory, aws glue, alation data catalog, python, r, postgresql, mysql, mongodb, power bi, spotfire, machine learning libraries, spark, deep learning, ai","ai, alation data catalog, aws glue, azure data factory, azure synapse, big data platforms, cloud, cloud technologies, data governance, data integration, data mining, data pipeline creation and maintenance, data virtualization, databricks, datawarehouse, deep learning, distributed ledgers, lakehouse architecture, logical and physical data modeling, machine learning libraries, mdm, mongodb, mysql, nosql, ods, postgresql, powerbi, python, r, redshift, serverless environments, spark, spotfire, sql, streamsets"
Senior Data Analytics Developer (one year contract),ARC Resources Ltd.,"Calgary, Alberta, Canada",https://ca.linkedin.com/jobs/view/senior-data-analytics-developer-one-year-contract-at-arc-resources-ltd-3762976469,2023-12-17,Alberta, Canada,Mid senior,Hybrid,"ARC has had an exciting growth story driven by the invaluable contributions of our people, principled business strategy and high-performance culture. Today, we are one of Canada’s largest energy companies with industry-leading environmental, social and governance (ESG) performance and tremendous opportunity on the horizon.
We are proud to play an important role in the development of Canada’s energy resources and recognize that this must be done responsibly. Through innovation, teamwork and a commitment to operational excellence, our people drive our company’s success. From the office to the field, our team of talented professionals work hard each day to safely execute our business and create positive and lasting impacts for our stakeholders.
Since our inception, we have created an environment that has become more than just a workplace.  Our people care about one another, give back in the communities where we live and work, and deliver results. We take pride in what we do and are excited to find our next team member to join our winning team.
We are currently seeking a Senior Data Analytics Developer to play an integral role in building real-time and batch processing data solutions. This position is a one year contract with the potential for extension or conversion to fulltime. The work atmosphere is an open, fast-paced, team environment where the successful candidate will have the opportunity to work independently, but also as part of a close-knit group. The candidate will love technology and enjoy finding ways to embed modern technologies in a sustainable and supportable fashion. Also, a passion to understand business opportunities that will allow the candidate to empower our stakeholders to access and analyze data providing a simple path to data driven business decisions.
RESPONSIBILITIES:
Liaise directly with the business to collect, document, and analyze user requirements and map those business requirements to technical artifacts
Coordinate with the business and internal team resources to plan and execute on projects
Work with our team of senior technical employees to drive continual improvement and optimization of existing solutions and architectural standards
Guide the maintenance and support of all projects and environments of the Operations Data Analytics team
Provide mentorship and assist junior and intermediate resources in their day-to-day activities
Develop real-time and batch processing solutions to ingest operational data and transform it for analytical and reporting purposes
Work with the Asset teams to complete data engineering and advanced analytics projects
Python scripting to manage large data flows using Pyspark within an Azure Databricks environment
SQL scripting including writing complex logic SQL statements, functions, triggers, and stored procedures
Adhere to development standards and protocols including data governance, change management, code review, unit testing, and documentation
Learn and understand the business processes
Design, develop, and improve new and existing artifacts for the Data Analytics portfolio
Collaborate with other developers on each step in the implementation of data engineering projects
Good oral and written communication skills, including ability to communicate complex technical descriptions in terms the business can understand
OUR DEVELOPMENT ENVIRONMENT:
Development environments that use leading edge cloud technologies such as:
Azure Databricks, Azure Data Lake, Azure Stream Analytics, Azure IOT and Event Hubs, Data Lake Storage, Delta Tables, Power BI Premium, Azure DevOps, Azure Data Factory, Azure Synapse Analytics, Azure SQL Server, Azure Analysis Services,
High volumes, high velocity, and high variations of data
A strong software development lifecycle:
One click deployments and upgrades
Delivery of small- and large-scale impactful projects
Strong planning and execution processes
Direct partnerships with dedicated subject matter experts from the business to enable cross domain knowledge transfer
Access to a wide range of business problems with projects for Asset teams such as: Geosciences, Drilling, Completions, Development, Production, and Field
Access to a wide range of technology solutions including real time data ingestion and reporting, traditional batch processing, geospatial data, tabular modelling, Power BI Premium, and events based processing
Collaboration and support with our Advanced Analytics team for AI/ML project activities
A flexible hybrid work schedule
QUALIFICATIONS:
A bachelor's degree in Information Technology, Computer Science, Software Engineering, or any IT and/or Business-related discipline or equivalent work experience
A minimum of 7 years of experience architecting large scale enterprise data solutions, ideally within the oil & gas industry
Proven work experience building and deploying data engineering pipelines leveraging cloud technologies (Databricks, Azure, AWS, GCP), preferably using Azure
Experience developing secure solutions by limiting access with Azure Active Directory, leveraging identity management through service principles and group memberships
Solid software engineering background and strong comprehension of data structures and algorithms
Proficiency with Python, SQL, and Spark SQL is required
Good understanding of Lambda and Delta architectures for batch and real-time data processing
Experience in developing dashboards and reports through tools like Spotfire and Power BI
Experience working with semi-structured and unstructured databases, and with unit testing and continuous integration
Proven ability to stay current with industry trends and implement innovative solutions to existing solutions and architectural standards
A passion to continuously seek out and investigate new and emerging technologies
We thank you for your interest in ARC Resources Ltd. however, only those candidates selected for an interview will be contacted.
Show more
Show less","SQL, Python, Spark SQL, Data engineering, Data analytics, Azure Databricks, Azure Data Lake, Azure Stream Analytics, Azure IOT, Event Hubs, Data Lake Storage, Delta Tables, Power BI Premium, Azure DevOps, Azure Data Factory, Azure Synapse Analytics, Azure SQL Server, Azure Analysis Services, Lambda architecture, Delta architecture, Spotfire, Power BI, Unit testing, Continuous integration, AI, ML","sql, python, spark sql, data engineering, data analytics, azure databricks, azure data lake, azure stream analytics, azure iot, event hubs, data lake storage, delta tables, power bi premium, azure devops, azure data factory, azure synapse analytics, azure sql server, azure analysis services, lambda architecture, delta architecture, spotfire, power bi, unit testing, continuous integration, ai, ml","ai, azure analysis services, azure data factory, azure data lake, azure databricks, azure devops, azure iot, azure sql server, azure stream analytics, azure synapse analytics, continuous integration, data engineering, data lake storage, dataanalytics, delta architecture, delta tables, event hubs, lambda architecture, ml, power bi premium, powerbi, python, spark sql, spotfire, sql, unit testing"
Senior Data Analytics Specialist,Cadeon Inc.,"Calgary, Alberta, Canada",https://ca.linkedin.com/jobs/view/senior-data-analytics-specialist-at-cadeon-inc-3641926196,2023-12-17,Alberta, Canada,Mid senior,Hybrid,"Are you in the top 10% in the Analytics field? If so, here is your chance to prove it.
Cadeon Inc
. is the Trusted Authority on turning Information into money. We equip our clients to leverage their information which enables them to become highly efficient companies. We are seeking senior, like-minded, dynamic, focused, and creative individuals to join our growing team as we expand our workforce in our Calgary headquarters.
Cadeon Inc
. is seeking a
Senior
Data Analytics Specialist
to support the design, development, and delivery of business analytics capabilities using tools like Tibco Spotfire, PowerBI, Tableau, etc.
This is a client-facing role; some of the main duties include working with business users to develop complex data mining 'analyses' , deploy, and train new users on the use of analytics and data mining tools. The role may also involve developing complex visualizations.
Required Skills
At least 3+ years of development experience with TIBCO Spotfire
Experience in data analysis and data mining
Working knowledge in Tibco Spotfire technology specifically Spotfire Analyst & Consumer
Working knowledge of all graphical visualizations in Spotfire Professional
Working knowledge of key Spotfire capabilities (filtering scheme, Data on demand, property controls)
Ability to build data visualizations with complex calculations/functions
Working knowledge on using Information Designer, building Information links, elements, filters, joins, prompts, and prompt groups
Working knowledge of performance management concepts in Spotfire
Experience with Spotfire administration activities (User/group, library, server management etc.)
Experience using the Spotfire developer API.
Must have a working knowledge of how to migrate code across environments (Dev QA PROD)
Experience with Oracle and SQL Server Databases. Strong conceptual understanding of relational database systems and strong SQL skills
Able to independently design, visualize and implement data visualization solutions using data visualization best practices (i.e. Stephen Few).
Strong ability to understand analytic specifications, translate specifications into visualizations and/or underlying code.
Strong ability to develop meaningful, repeatable and consistent quality checks throughout the development process.
Nice To Have's
Experience with BI / Analytics tools such as Power BI, Tableau, Qlikview, etc.
Skilled in data modeling and working knowledge of statistics
IronPython, Java development experience
Degree in Mathematics, Finance, Computer Science, Engineering or equivalent
Experience in Business Intelligence
Experience in automation
Experience in Oil & Gas Upstream, Midstream and/or Downstream business
Experience in Finance or working with financial data
Client Facing Skill-set
Experience in working both independently, and in a team-oriented, cross-functional collaborative environment is essential
Ability to effectively prioritize and execute tasks in a fast-paced environment
Must have strong communication skills as this candidate will interface with the business clients directly
Able to actively address business requests, clearly communicate commitments, issues, completion, dates and provides meaningful status to project stakeholders
Must have excellent presentation skills, strong written and effective verbal communication skills
Ability to build relationships and work effectively with both technical and non-technical colleagues at all levels
Strong analytical/problem-solving skills with the ability to plan, prioritize and execute in a fast/ high-pressured environment
Ability to work independently and under minimal supervision while demonstrating strong self-management and self-organization skills
Close attention to detail; commitment to accuracy
Upstream, Midstream, or Downstream Oil and Gas domain knowledge would be a strong advantage
Our employees are considered the most valued part of our organization. We demonstrate this commitment to our employees by offering a competitive salary and a comprehensive employee benefits package.
""Data Analytics Developer""
Show more
Show less","Tibco Spotfire, PowerBI, Tableau, Data mining, Data visualization, SQL, Oracle, Relational databases, Information Designer, Spotfire Analyst & Consumer, Spotfire Professional, Spotfire Developer API, Business Intelligence, Automation, Data modeling, Statistics, IronPython, Java, ClientFacing Skillset, Presentation skills, Communication skills, Analytical skills, Problemsolving skills, Selfmanagement skills, Selforganization skills, Attention to detail, Accuracy, Oil and Gas domain knowledge","tibco spotfire, powerbi, tableau, data mining, data visualization, sql, oracle, relational databases, information designer, spotfire analyst consumer, spotfire professional, spotfire developer api, business intelligence, automation, data modeling, statistics, ironpython, java, clientfacing skillset, presentation skills, communication skills, analytical skills, problemsolving skills, selfmanagement skills, selforganization skills, attention to detail, accuracy, oil and gas domain knowledge","accuracy, analytical skills, attention to detail, automation, business intelligence, clientfacing skillset, communication skills, data mining, datamodeling, information designer, ironpython, java, oil and gas domain knowledge, oracle, powerbi, presentation skills, problemsolving skills, relational databases, selfmanagement skills, selforganization skills, spotfire analyst consumer, spotfire developer api, spotfire professional, sql, statistics, tableau, tibco spotfire, visualization"
Sr. Performance Data Analyst,Oil and Gas Job Search Ltd,"Calgary, Alberta, Canada",https://ca.linkedin.com/jobs/view/sr-performance-data-analyst-at-oil-and-gas-job-search-ltd-3769133474,2023-12-17,Alberta, Canada,Mid senior,Hybrid,"Airswift is seeking a Sr. Performance Data Analyst
to work with one of our clients in the oil and gas industry in Calgary, AB, on a 12-month contract.
As a senior role, the job requires a degree of influence, mentorship, coaching and guidance to our lesser experienced team members. In addition, the role requires collaboration cross functionally while demonstrating an ability to effectively communicate and influence decision making.
This position requires the ideal candidate to be able to provide strong governance over the finance business processes at the individual manufacturing sites while supporting key business function stakeholders in collecting, consolidating, and reporting on key financial KPIs and performance metrics. An essential part of the role is to maintain networks within the broader downstream operational teams, finance teams and performance networks.
The position requires an ability to identify, plan and implement key improvement initiatives in Finance to improve analysis quality, reduce reporting redundancies, increase productivity and improve cycle time resulting in significant business. The ideal candidate should be able to help the organization identify opportunities for improvement by analyzing data and conducting research.
Job Description And Key Deliverables
Strong business analysis skills: a. Business process improvements (Striving towards standardization and simplification in the reporting cycles) b. Support the delivery of various transformational improvement initiatives for Cenovus, Finance and Downstream. c. Actively participate and often lead the manufacturing Finance Modernization & digital initiatives. d. Conduct wide-ranging business analysis in identifying and documenting requirements across our key manufacturing finance business processes.
Strong Data analytical Skills: a. Develop actionable data models and data wrangling processes by integrating data extraction considerations into business solutions b. Analyze data to identify trends, patterns, and insights that inform business process improvements and efficiencies across manufacturing financial reporting. c. Communicate effectively with all stakeholder in ensuring transparent and clear understanding of financial reporting as well as simple and easy to understand reporting visualizations. d. Support back-end data models in Power BI and Spotfire to ensure stability in digital analytical reporting tools e. Support designing financial data model using SQL and DAX coding to help drive decision making and improve business performance transparency.
Project Management skills: a. Actively engage and project manage large financial modernization initiatives. b. Actively seek opportunities of deploying AI and digital analytics in various facets of financial reporting (i.e. data wrangling, data modelling, data gathering etc.)
Technical Skills
Bachelors degree in Computer Science, Finance, Engineering
CPA/MBA considered an asset
Advanced knowledge in Power BI, SQL and other digital analytics software (Required)
SAP Knowledge considered an asset
Hands-on experience building complex Power BI reports using various data sources and data integrations
Proficiency in creation of reporting data visualization solution and experience in building technical and process documentation requirements.
Excellent oral and written communication skills in order to work collaboratively various stakeholders across the organization
Show more
Show less","Business Analysis, Data Analytics, Power BI, Spotfire, SQL, DAX, Project Management, Artificial Intelligence, Digital Analytics, Data Extraction, Data Wrangling, Data Visualization, Reporting, Financial Data Modeling, SAP","business analysis, data analytics, power bi, spotfire, sql, dax, project management, artificial intelligence, digital analytics, data extraction, data wrangling, data visualization, reporting, financial data modeling, sap","artificial intelligence, business analysis, data extraction, data wrangling, dataanalytics, dax, digital analytics, financial data modeling, powerbi, project management, reporting, sap, spotfire, sql, visualization"
"Senior QA Analyst, Data Delivery",Alberta Investment Management Corporation (AIMCo),"Edmonton, Alberta, Canada",https://ca.linkedin.com/jobs/view/senior-qa-analyst-data-delivery-at-alberta-investment-management-corporation-aimco-3765626812,2023-12-17,Alberta, Canada,Mid senior,Hybrid,"DEPARTMENT:
Data Management
CLOSING DATE:
December 7, 2023
Opportunity
Get ready to supercharge your career! We're on the hunt for an electrifying Senior QA Analyst for Data Delivery to join our team at AIMCo. This isn't just any role – it's a game-changer. You'll be the gatekeeper of accuracy, reliability, and timeliness in our data delivery processes, ensuring that our organization is always one step ahead.
As a Senior QA Analyst, you'll be at the heart of our operations, working closely with dynamic cross-functional teams to implement and maintain cutting-edge quality assurance practices for our data products. Your role will be vital in ensuring that our data consistently meets business requirements and standards, shaping the very backbone of our operations.
Imagine being the catalyst for excellence, ensuring every piece of data we deliver is a cog in a well-oiled machine. You'll be the one who ensures that our data isn't just accurate - it's AIMCo accurate.
If you're ready to step into a role where your contribution makes a significant impact, where every day brings new challenges and opportunities, then we'd love to hear from you.
Responsibilities:
Quality Assurance
Develop and implement testing strategies for data products delivery processes, including data extraction, transformation, and loading (ETL) workflows.
Conduct thorough testing of data pipelines to identify and address data quality issues, ensuring adherence to data governance and security standards.
​
Test Planning and Execution
Collaborate with data developers, data scientists, and other stakeholders to understand data requirements and develop comprehensive test plans.
Execute test cases to validate data products development including data transformations, integrations, and ensure data accuracy throughout the delivery process.
Serve as a business resource for specified project teams and participate in design and technical reviews
Automation
Design, implement, and maintain automated testing frameworks for data pipelines to enhance testing efficiency and coverage.
Continuously improve and optimize automated test scripts to keep pace with evolving data delivery requirements.
Data Monitoring
Establish and maintain monitoring processes to proactively identify and address data quality issues in real-time.
Work with relevant teams to implement alerts and notifications for data anomalies and discrepancies.
Collaboration
Collaborate with cross-functional teams to understand business use cases and data requirements, ensuring alignment with data delivery objectives.
Work closely with data analysts, data developers and data architects to implement best practices for data quality and delivery.
Documentation
Create and maintain comprehensive documentation of test plans, test cases, and test results.
Document and communicate data quality issues, resolutions, and improvements to relevant stakeholders.
Our Ideal Candidate Qualifications:
Bachelor’s degree in Computer Science, Information Technology, or a related field.
Formal training and/or accreditation in data management is preferred.
Formal training and/or accreditation in IT service management is preferred.
Proven experience as a Quality Engineer or similar role, with a focus on data quality and data delivery. Typically requires 4-5 years of experience in Quality Assurance, testing and software development (or a combination of each)
Strong understanding of ETL processes, data warehousing, and data integration concepts.
Proficiency in scripting and programming languages (e.g., SQL, Python) for test automation.
Strong experience in CICD tools and technologies such as source code control, build and deployment, and testing.
Well-versed in all testing methodologies (for example white vs. black box test work, system vs. functional)
Experienced with DevOps best practices and culture.
Familiarity with data quality tools and techniques.
Excellent analytical and problem-solving skills.
Strong communication and collaboration skills.
Experience with big data technologies (e.g., Hadoop, Spark) is preferred.
Familiarity with various methodologies, frameworks, and technologies relevant to data management including data quality and data governance.
Knowledge of data governance and compliance standards.
Demonstrate high degree of flexibility and adaption to a high-volume, fast-paced, ever-changing environment.
Experience with Azure DevOps, Databricks, and DBT
ISTQB certification considered an asset
Investment industry experience typically supplemented with business specific training (CFA, CSC, etc.) are considered an asset
Demonstrates a strong commitment to AIMCo’s core values of excellence, transparency, humility, integrity, and collaboration, and inspiring the same in others.
Success Measure:
Strong understanding of the asset management business and data
Strong understanding of AIMCo's Modern Data Platform Architecture and Data Product Delivery Lifecycle Data test practice is established and documented
Build Test Plans and Test Cases for the data modernization projects
Note
: The application deadline for this position is 11:59 PM MT December 7, 2023
Next Steps
We are excited to meet you. Please submit your resume or CV to be considered for this opportunity. Applications are being reviewed on a rolling basis and we will be in touch with any questions.
Final candidates will be asked to undergo a security screening, which includes a credit bureau and a criminal record investigation, the results of which must be acceptable to AIMCo.
ALERT - Be on the lookout for AIMCo career opportunities advertised through third parties that request an application fee or too much information. To verify, all opportunities are posted on aimco.ca/jobs
Just like our investments, our international team is stronger when we are diversified. At AIMCo, we draw upon the differences in who we are, where we come from, and the way we think to foster sophisticated solutions for Albertans. Doing business the right way means building an equitable organization inclusive of all races, genders, ages, abilities, religions, neurodiversity, identities, and lived experiences. Simply put, when you can show up as your authentic self every day, you and your team can truly thrive — all in service of our clients and their beneficiaries.
Show more
Show less","SQL, Python, ETL, Data Warehousing, Data Integration, CICD, DevOps, Azure DevOps, Databricks, DBT, ISTQB, CFA, CSC, Data Governance, Data Quality, Data Delivery, Test Automation, Test Planning, Test Execution, Data Monitoring, Data Analysis, Problem Solving, Communication, Collaboration, Flexibility, Adaptability, Data Modernization, Data Product Delivery Lifecycle","sql, python, etl, data warehousing, data integration, cicd, devops, azure devops, databricks, dbt, istqb, cfa, csc, data governance, data quality, data delivery, test automation, test planning, test execution, data monitoring, data analysis, problem solving, communication, collaboration, flexibility, adaptability, data modernization, data product delivery lifecycle","adaptability, azure devops, cfa, cicd, collaboration, communication, csc, data delivery, data governance, data integration, data modernization, data monitoring, data product delivery lifecycle, data quality, dataanalytics, databricks, datawarehouse, dbt, devops, etl, flexibility, istqb, problem solving, python, sql, test automation, test execution, test planning"
Senior Data Engineer,Kforce Inc,"Milwaukee, WI",https://www.linkedin.com/jobs/view/senior-data-engineer-at-kforce-inc-3780636013,2023-12-17,Waterford,United States,Associate,Onsite,"Responsibilities
Kforce has a client in Milwaukee, WI that is seeking a Senior Data Engineer who will focus on quality engineering best practices to meet and exceed internal and external client expectations. In this position, you will analyze, design, develop, test and document solutions supporting data integration, performance tuning, and data modeling to drive organization growth objectives. The Senior Data Engineer will define the standards for data architecture, platform architecture, and data quality and governance. This role is responsible for ensuring that the function is aligned with the overall organization and continuously works to meet critical service levels in access, delivery and security. Essential Responsibilities:
Co-architect organization's next gen cloud data analytics platform
Increase operating efficiency and adapt to new requirements
Monitor and maintain the health of solutions generated
Support and enhance our data-ops practices
Provide task breakdowns, identify dependencies, and provide effort estimates
Model data warehouse entities in Erwin
Build data transformation pipelines with Data Build Tools (DBT)
Evaluate the latest technology trends and develop proof-of-concept prototypes that align with company's opportunities
Develop positive relationships with clients, stakeholders, and internal teams
Understand business goals, drivers, context, and processes to suggest technology solutions that improve the organization
Work collaboratively on creative solutions with engineers, product managers, and analysts in an agile like environment
Perform, design, and code reviews
Perform other position-related duties as assigned
Requirements
Bachelor's degree in Computer Engineering, Computer Science, Data Science, or related field
8+ years of experience working with data modeling, architecture and engineering
2+ years of experience designing and implementing data warehouses in Snowflake
Experience with all core software development activities, including requirements gathering, design, construction, and testing
Experience performing data transformation using DBT
Experience working with DQ products such as Monte Carlo, BigEye, or Great Expectations
Experience with Azure DevOps (Repos, Pipelines, Boards, Wiki, Test Plans)
Experience with formal software development methodologies including Software Development Life Cycle (SDLC), Agile or SCRUM
Experience building high-performance and highly reliable data pipelines
Experience Knowledge of data warehouse design patterns (star schema, data vault)
Experience building dashboards with business integrations tools
Knowledge of DataOps
with cloud-based compute, storage, integration and security patterns
Knowledge and understanding of RESTful APIs
Knowledge of current data engineering trends, best practices, and standards
Knowledge of SQL and Python
Ability to work in a collaborative environment
Ability to facilitate evaluation of technologies and achieve consensus on technical standards and solutions among a diverse group of information technology professionals
Ability to work in an organization driven by continuous improvement or with equivalent focus on process improvement
Ability to manage multiple, competing priorities and attain the best possible outcomes for the organization
Excellent verbal and written communication and effective listening skills
Preferred
SnowPro Advanced certification
DBT Analytical Engineer certification
Experience in delivering an end-to-end data analytics platform using modern data stack components
Experience with AI/ML
The pay range is the lowest to highest compensation we reasonably in good faith believe we would pay at posting for this role. We may ultimately pay more or less than this range. Employee pay is based on factors like relevant education, qualifications, certifications, experience, skills, seniority, location, performance, union contract and business needs. This range may be modified in the future.
We offer comprehensive benefits including medical/dental/vision insurance, HSA, FSA, 401(k), and life, disability & ADD insurance to eligible employees. Salaried personnel receive paid time off. Hourly employees are not eligible for paid time off unless required by law. Hourly employees on a Service Contract Act project are eligible for paid sick leave.
Note: Pay is not considered compensation until it is earned, vested and determinable. The amount and availability of any compensation remains in Kforce's sole discretion unless and until paid and may be modified in its discretion consistent with the law.
This job is not eligible for bonuses, incentives or commissions.
Kforce is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.
Salary: $130,000 - $140,000 per year
Show more
Show less","Snowflake, DBT (Data Build Tools), Erwin, Azure DevOps (Repos Pipelines Boards Wiki Test Plans), Monte Carlo, Great Expectations, BigEye, SnowPro Advanced certification, DBT Analytical Engineer certification, DataOps, Agile, SCRUM, RESTful APIs, SQL, Python, Data modeling, Data architecture, Data engineering, Data transformation, Data quality, Data governance, Cloud computing, Data analytics, Data integration, Data warehousing, Data pipelines, Machine learning, Artificial intelligence","snowflake, dbt data build tools, erwin, azure devops repos pipelines boards wiki test plans, monte carlo, great expectations, bigeye, snowpro advanced certification, dbt analytical engineer certification, dataops, agile, scrum, restful apis, sql, python, data modeling, data architecture, data engineering, data transformation, data quality, data governance, cloud computing, data analytics, data integration, data warehousing, data pipelines, machine learning, artificial intelligence","agile, artificial intelligence, azure devops repos pipelines boards wiki test plans, bigeye, cloud computing, data architecture, data engineering, data governance, data integration, data quality, data transformation, dataanalytics, datamodeling, dataops, datapipeline, datawarehouse, dbt analytical engineer certification, dbt data build tools, erwin, great expectations, machine learning, monte carlo, python, restful apis, scrum, snowflake, snowpro advanced certification, sql"
Product Information Management (PIM ) Data Analyst,nVent,"New Berlin, WI",https://www.linkedin.com/jobs/view/product-information-management-pim-data-analyst-at-nvent-3734113949,2023-12-17,Waterford,United States,Associate,Hybrid,"We’re looking for people who put their innovation to work to advance our success – and their own. Join an organization that ensures a more secure world through connecting and protecting our customers with inventive electrical solutions.
What You Will Experience In This Position
Product Data Management
Gather, input, analyze, reconcile and organize commercial product data in product information management (PIM) systems from a variety of sources and stakeholders
Be the owner and point person in the business segment for product data by being the lead for data enrichment, syndication support and day-to-day management
Subject Matter Expert for all data that goes into the product catalog, including bulk data import, export, cleansing and reorganization
Assist with digital and web activities to promote nVent’s products and ongoing campaigns, including working with these content management system (CMS) platforms: Acquia/Drupal, SiteCore and Salesforce
Partner with the new product introduction team to ensure all new products have required product data and available for data syndication at least three weeks before launch date
Work closely with the translation process owner and external translation vendor to execute the translation of product data
Collaborate with cross-functional teams responsible for commercial product data, defining business requirements and applying them to established standard
Process Improvement and Standardization
Drive the implementation of nVent’s commercial data strategy and act as an agent of change
Apply detailed rules and requirements around product data, and contribute to the development and enforcement of governance standards
Be a change agent in the process improvement and template standardization efforts across the business segment and digital platform teams
Customer/External Use of Product Data
Ensure and encourage key partners and customer to utilize our product data
Conduct routine validations on partners websites and marketing collateral for syndication
You Have
Ideally 3+ years of relevant experience in digital marketing, information services, customer service, data analytics or a similar field
Experience with a product information management (PIM) system
Experience with content management systems (CMS) or similar digital content management activities is a must. Experience and knowledge with Acquia/Drupal, SiteCore is preferred
Experience working with cross-functional teams such as Product Management, Marketing, Engineering and/or IT
Experience working with external entities such as data pools, sales agents, distributors or customers
We Have
A dynamic global reach with diverse operations around the world that will stretch your abilities, provide plentiful career opportunities, and allow you to make an impact every day
At nVent, we connect and protect a more sustainable and electrified world with inventive electrical solutions. We’re a nearly $3 billion high-performance electrical company with a dedicated team of 10,000+ people at more than 130 sites around the world. Our solutions deliver value to industrial, commercial, residential, energy and infrastructure customers, providing mission critical solutions that improve performance, lower costs and reduce downtime.
We design, manufacture, market, install and service high-performance products and solutions that connect and protect mission critical equipment, buildings and essential processes. Our robust portfolio of leading electrical product brands dates back more than 100 years and includes nVent CADDY, ERICO, HOFFMAN, RAYCHEM, SCHROFF and TRACER.
Commitment to strengthen communities where our employees live and work
We encourage and support the philanthropic activities of our employees worldwide
Through our nVent in Action matching program, we provide funds to nonprofit and educational organizations where our employees volunteer or donate money
Core values that shape our culture and drive us to deliver the best for our employees and our customers. We’re known for being:
Innovative & adaptable
Dedicated to absolute integrity
Focused on the customer first
Respectful and team oriented
Optimistic and energizing
Accountable for performance
Benefits to support the lives of our employees
At nVent, we connect and protect our customers with inventive electrical solutions. People are our most valuable asset. Inclusion and diversity means that we celebrate and encourage each other’s authenticity because we understand that uniqueness sparks growth.
#INDISC
Show more
Show less","Product Data Management, PIM, CMS, Acquia/Drupal, SiteCore, Salesforce, Translation, Process Improvement, Standardization, Customer/External Use of Product Data, Data Analytics, Crossfunctional Teams, External Entities","product data management, pim, cms, acquiadrupal, sitecore, salesforce, translation, process improvement, standardization, customerexternal use of product data, data analytics, crossfunctional teams, external entities","acquiadrupal, cms, crossfunctional teams, customerexternal use of product data, dataanalytics, external entities, pim, process improvement, product data management, salesforce, sitecore, standardization, translation"
Data Developer - Onsite,Generac Power Systems,"Waukesha, WI",https://www.linkedin.com/jobs/view/data-developer-onsite-at-generac-power-systems-3762677298,2023-12-17,Waterford,United States,Mid senior,Onsite,"Company
Generac Power Systems
Name
Data Developer
Req #
63734
Employment Type
Full Time
Shift
1st
It’s a challenge and an invitation. Most importantly, it’s an opportunity to join an industry leader that’s shaping the market and investing in its people, along with new growth and technologies. Our rapid growth equals rapid career advancement opportunities for those who want to be challenged and enjoy a fast-paced, high-performance culture. We focus on personal and professional development; encourage learning on the job and being a supportive team member; and hope each day of work brings purpose and pride.
The Data Developer is responsible for expanding and optimizing our data and data pipeline architecture as well as optimizing data flow and collection for cross functional teams. This position will optimize and build data systems as well as support software developers, data analysts and data scientists on data initiatives to ensure optimal data delivery architecture that is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing our data architecture to support our next generation of products and data initiatives.
This position is onsite at our headquarters in Waukesha, WI.
Essential Duties and Responsbilities:
Create and maintain optimal data pipeline architectures
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and Azure ‘big data’ technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Assemble large, complex data sets that meet functional / non-functional business requirements.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Work with data and analytics experts to strive for greater functionality in our data systems.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Identify, design, and implement designs to keep our global data separated and secure across national boundaries.
Minimum Qualifications:
Bachelor degree in Computer Science or equivalent experience
5 years of experience in a Data Engineer or Data Developer role
3 years of experience in building and optimizing ‘big data’ data pipelines, architectures and data sets
Preferred Qualifications:
Production experience using cloud based big data tools
Production experience with multiple SQL databases, such as SQL Server, Oracle, Progress, etc
Production experience with one or more non-SQL databases, such as CosmosDB, DynamoDB, Cassandra, or MongoDB
3+ years with object oriented/object function development languages such as C#, Python, Java, Javascript, etc
Knowledge, Skills and Abilities:
Experience with big data tools such as Azure Data Bricks, Azure Synapse, etc.
Experience with data pipeline and workflow management tools such as Azure Data FactorExperience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement
Experience with devops practices, CI/CD concepts, cloud infrastructure and resources
Strong analytic skills related to working with both structured and unstructured datasets
Experience manipulating, processing and extracting value from large disconnected datasets
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Strong project management and organizational skills
Experience supporting and working with cross-functional teams in a dynamic environment
Physical Demands:
While performing the duties of this job, the employee is regularly required to talk and hear; and use hands to manipulate objects or controls. The employee is regularly required to stand and walk. On occasion the incumbent may be required to stoop, bend or reach above the shoulders. The employee must occasionally lift up to 25 pounds. Specific conditions of this job include are typical of frequent and continuous computer-based work requiring periods of sitting, close vision and ability to adjust focus. Occasional travel.
Benefits:
We are an inclusive company that celebrates differences and keeps equity and respect at the forefront.
Competitive Benefits: Health, Dental, Vision, & 401k
401 (k) retirement savings plans with company match
Medical support programs: Maternity, Diabetes, Treatment Decision Support and more
Work-life benefits: PTO and Holidays
To be considered an official applicant, please apply directly on our company website:
https://generacta.avature.net/careers/JobDetail/Waukesha-Wisconsin-United-States-Data-Developer/11422
“We are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law.”
Show more
Show less","Data Pipeline Architecture, SQL, Big Data Technologies, Data Analytics, Data Structures, Metadata, Dependency Management, Workload Management, Data Governance, Cloud Infrastructure, Machine Learning, Python, Java, C#, JavaScript, Azure Data Bricks, Azure Synapse, Data Factory, Stream Processing, Message Queuing, DevOps Practices, CI/CD Concepts, Software Development, Data Scientist, Business Analyst","data pipeline architecture, sql, big data technologies, data analytics, data structures, metadata, dependency management, workload management, data governance, cloud infrastructure, machine learning, python, java, c, javascript, azure data bricks, azure synapse, data factory, stream processing, message queuing, devops practices, cicd concepts, software development, data scientist, business analyst","azure data bricks, azure synapse, big data technologies, business analyst, c, cicd concepts, cloud infrastructure, data factory, data governance, data pipeline architecture, data scientist, data structures, dataanalytics, dependency management, devops practices, java, javascript, machine learning, message queuing, metadata, python, software development, sql, stream processing, workload management"
Data Analyst I,Kelly,"Wauwatosa, WI",https://www.linkedin.com/jobs/view/data-analyst-i-at-kelly-3623071871,2023-12-17,Waterford,United States,Mid senior,Onsite,"Data Analyst I ****100% REMOTE***** Kelly Services top client is seeking a
Data Analyst I*
Job Description/Responsibilities:*
Translate business problems into technical requirements and reporting solutions
Responsible for using multiple sources of data to create ad-hoc analyses and reports
Build data validation tests and exception workflows to ensure data integrity
Maintain data and reporting solutions; perform troubleshooting as needed
Learn and apply new tools, technologies and techniques to analytics and reporting solutions
Type: Contract W2 PAY *
Pay Range: $25.00-$35.00 p/h: $negotiable/hr.*
Qualifications:*
Proficiency in writing SQL queries
Strong analytical skills
Intellectual curiosity and passion for solving problems using technology
Collaborative, solution focused and act with a sense of urgency
Proficiency in one or more data visualization and reporting tools: Power BI, OBIEE, Spotfire, Tableau, DOMO, or similar tool
2 or more years of experience working in an analyst or similar role
Preferred Qualifications:*
Previous work experience with one or more of the following: C#, .NET, Python, PowerShell
Knowledge of data modeling techniques and tools
Knowledge of relational databases and ETL principles ****PLEASE SUBMIT YOUR RESUME AND SOMEONE WILL CONTACT YOU FURTHER**** Apply Today!
#P2
As part of our promise to talent, Kelly supports those who work with us through a variety of benefits, perks, and work-related resources. Kelly offers eligible employees voluntary benefit plans including medical, dental, vision, telemedicine, term life, whole life, accident insurance, critical illness, a legal plan, and short-term disability. As a Kelly employee, you will have access to a retirement savings plan, service bonus and holiday pay plans (earn up to eight paid holidays per benefit year), and a transit spending account. In addition, employees are entitled to earn paid sick leave under the applicable state or local plan. Click [here](https://www.mykelly.com/help-support-working-with-kelly-us/#Benefits) for more information on benefits and perks that may be available to you as a member of the Kelly Talent Community. You should know:
Your safety matters! Visit the [COVID-19 Resource Center](https://www.mykelly.com/covid-19-resource-center) for the latest information, policies, and frequently asked questions. Why Kelly® Technology?
Looking to put your tech expertise to work on today’s most intriguing, innovative, and high-visibility projects? By partnering with Kelly Technology, you’ll gain direct connections to top companies around the globe. Our team creates expert talent solutions to solve the world’s most critical challenges. In a world where change is the only constant, our extensive network of industry relationships and IT market expertise help you take your skills exactly where you want to go. We’re here to help you gain experience, make an impact, and grow your tech career.
About Kelly
Work changes everything. And at Kelly, we’re obsessed with where it can take you. To us, it’s about more than simply accepting your next job opportunity. It’s the fuel that powers every next step of your life. It’s the ripple effect that changes and improves everything for your family, your community, and the world. Which is why, here at Kelly, we are dedicated to providing you with limitless opportunities to enrich your life—just ask the 300,000 people we employ each year. Kelly Services is proud to be an Equal Employment Opportunity and Affirmative Action employer. We welcome, value, and embrace diversity at all levels and are committed to building a team that is inclusive of a variety of backgrounds, communities, perspectives, and abilities. At Kelly, we believe that the more inclusive we are, the better services we can provide. Requests for accommodation related to our application process can be directed to Kelly’s Human Resource Knowledge Center. Kelly complies with the requirements of California’s state and local Fair Chance laws. A conviction does not automatically bar individuals from employment.
Show more
Show less","SQL, Power BI, OBIEE, Spotfire, Tableau, DOMO, C#, .NET, Python, PowerShell, Data modeling, Relational databases, ETL","sql, power bi, obiee, spotfire, tableau, domo, c, net, python, powershell, data modeling, relational databases, etl","c, datamodeling, domo, etl, net, obiee, powerbi, powershell, python, relational databases, spotfire, sql, tableau"
Progress OpenEdge Data Conversion Engineer,Fiserv,"Milwaukee, WI",https://www.linkedin.com/jobs/view/progress-openedge-data-conversion-engineer-at-fiserv-3752310024,2023-12-17,Waterford,United States,Mid senior,Onsite,"Calling all innovators – find your future at Fiserv.
We’re Fiserv, a global leader in Fintech and payments, and we move money and information in a way that moves the world. We connect financial institutions, corporations, merchants, and consumers to one another millions of times a day – quickly, reliably, and securely. Any time you swipe your credit card, pay through a mobile app, or withdraw money from the bank, we’re involved. If you want to make an impact on a global scale, come make a difference at Fiserv.
*This position will be performed onsite in our downtown Milwaukee, WI or Lincoln, NE office. After 90 days, Fridays can be worked remotely*
What does a successful
Progress OpenEdge Data Conversion Engineer
do at Fiserv?
The Precision Conversions group is a highly skilled, dedicated team that performs data conversions and technical support to clients moving from one core platform to the Precision core platform. Our group creates a variety of programs existing at different levels of complexity as well as functionality. This position will analyze and convert financial data from 3rd party applications to our Fiserv Precision platform. You will work closely with an internal installation team and client personnel to gather needed artifacts to convert the data to our Fiserv Precision platform. During the project timeline, the programmers, analysts, and implementation teams are paired tightly with our clients to meet a successful go live date
.
You will work closely with other team members to resolve questions of program intent, output requirements and input data use.
What you will do:
Review prior processor data and work with business analysts to create data conversion programs from prior core servicer to the Precision banking core platform
Evaluate client and business analyst custom requests for data manipulation
Review current internal processes and procedures seeking efficiencies and quality improvement
Work with a project team comprised of programmers, business analysts, implementation specialists, project managers, and clients to meet a targeted go live date
Other job duties and responsibilities as assigned
What you will need to have:
Minimum of 2 years of SQL programming experience (or Progress 4GL programming)
Understanding of data conversion processes within a financial environment
Financial industry experience
2 year minimum degree in Computer Science, Business, Math, Finance or Management Information Systems or equivalent experience.
Dedication to quality and high-level customer satisfaction
Weekend work is required for the live event, approximately 6 weekends per year.
What would be great to have:
Progress 4GL (OpenEdge) programming is highly desired
Detail oriented, good communication skills and good team player
Financial industry experience desired
Experience in AIX/UNIX
Show more
Show less","SQL programming, Data conversion, Financial data conversion, Data manipulation, AIX/UNIX, Progress 4GL (OpenEdge) programming, Precision core platform, Project management, Communication skills, Teamwork","sql programming, data conversion, financial data conversion, data manipulation, aixunix, progress 4gl openedge programming, precision core platform, project management, communication skills, teamwork","aixunix, communication skills, data conversion, data manipulation, financial data conversion, precision core platform, progress 4gl openedge programming, project management, sql, teamwork"
Healthcare Data Analyst (MedInsight - Milwaukee Office),Milliman,"Brookfield, WI",https://www.linkedin.com/jobs/view/healthcare-data-analyst-medinsight-milwaukee-office-at-milliman-3732806805,2023-12-17,Waterford,United States,Mid senior,Onsite,"Company Overview
Leading with our core values of Quality, Integrity, and Opportunity, MedInsight is one of the healthcare industry’s most trusted solutions for healthcare intelligence. Our company purpose is to empower easy, data-driven decision-making on important healthcare questions. Through our products, education, and services, MedInsight is making an impact on healthcare by helping to drive better outcomes for patients while reducing waste. Over 300 leading healthcare organizations have come to rely on MedInsight analytic solutions for healthcare cost and care management.
MedInsight is a subsidiary of Milliman; a global, employee-owned consultancy providing actuarial consulting, retirement funding and healthcare financing, enterprise risk management and regulatory compliance, data analytics and business transformation as well as a range of other consulting and technology solutions.
Position Summary
The MedInsight team develops an industry-leading data warehouse and analytics suite for major healthcare companies including insurers, providers, and public entities. We are a tech healthcare data company transforming how the industry understands and consumes healthcare data. We are accelerating and looking for a Healthcare Data Analyst to join our team. This position focuses on healthcare data profiling and analysis tasks that require quantitative reasoning skills, knowledge of tools and technologies used in data analysis, and an interest in the US healthcare industry. This position will be located in our Milwaukee office. This person’s primary duty will be to work with our consultants and healthcare analytics team by researching healthcare analytics inquiries, onboarding new data sources and supporting daily operations of the MedInsight business intelligence solution.
Primary Responsibilities
Import and interpret source data
Scrub, clean and validate data
Prepare input files for 3rd party data processing
Analyze data results and make changes if needed, before presenting data to data analysts, project managers, and internal consultants.
Assist with system documentation and research
Preferred Skills And Experience
Candidates must be team players with excellent interpersonal skills. They must also have some experience/ familiarity with data analysis using large data sets. Experience with healthcare datasets is a significant plus.
Education/experience
Bachelor's degree in data analytics or data science
Education/ experience with quantitative analysis, statistics, and/or data science.
Skills
Experience coding in SQL or similar language
Strong analytical ability
Microsoft Excel
Effective oral and written communication
Punctual and reliable
Team player with positive and energetic attitude
Compensation and Location:
The salary range is $50,000 to $75,000, depending on a combination of factors, including but not limited to education, relevant work experience, qualifications, skills, certifications, location, etc. This role will be based in our Milwaukee office.
What makes this a great opportunity?
Join an innovative, high growth company with a solid industry track record
Bring your expertise and ideas to directly impact and help build the next generation of MedInsight products and solutions
Enjoy significant visibility in your work and be recognized for your wins
Work for a company that values your wellbeing and professional growth, offering a flexible work environment, generous benefits package, and investment in the development of your career
Benefits
We offer competitive benefits which include the following based on plan eligibility:
Medical, dental and vision coverage for employees and their dependents, including domestic partners
A 401(k) plan with matching program, and profit sharing contribution
Employee Assistance Program (EAP)
A discretionary bonus program
Paid Time Off (PTO) starts accruing on the first day of work and can be used for any reason; full-time employees will accrue
15 days of PTO per year, and employees working less than a full-time schedule will accrue PTO at a prorated amount based on hours worked
Family building benefits, including adoption and fertility assistance and paid parental leave up to 12 weeks for employees who have worked for Milliman for at least 12 months and have worked at least 1,250 hours in the preceding 12-month period
A minimum of 8 paid holidays
Milliman covers 100% of the premiums for life insurance, AD&D, and both short-term and long-term disability coverage
Flexible spending accounts allow employees to set aside pre-tax dollars to pay for dependent care, transportation, and applicable medical needs
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.
Show more
Show less","SQL, Data Profiling, Data Analytics, Data Science, Statistics, Data Analysis, Data Management, Data Processing, Research, Quantitative Analysis, Communication, Teamwork, MS Excel, Business Intelligence, Data Interpretation","sql, data profiling, data analytics, data science, statistics, data analysis, data management, data processing, research, quantitative analysis, communication, teamwork, ms excel, business intelligence, data interpretation","business intelligence, communication, data interpretation, data management, data processing, data profiling, data science, dataanalytics, ms excel, quantitative analysis, research, sql, statistics, teamwork"
Staff Data Engineer,Recruiting from Scratch,"Milwaukee, WI",https://www.linkedin.com/jobs/view/staff-data-engineer-at-recruiting-from-scratch-3744391931,2023-12-17,Waterford,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our client is a dating app.
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, deliver analysis to further improve engagement in existing features, and empower our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Solve technical problems of the highest scope and complexity
Exert significant influence on the company’s analytical long-range goals and data architecture
Define and extend our internal standards for style, maintenance, and best practices for a high-scale data platform
Provide mentorship for all on your team to help them grow in their technical responsibilities
Propose ideas to improve the scale, performance, and capabilities of the Data Platform
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We'll Love About You
7+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field
7+ years experience using Python
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention
Location
: This role will be located in office twice a week minimum in Palo Alto, San Francisco or Chicago.
Salary Range: $180,000 - 250,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, SQL, Kafka, Storm, Spark Streaming, Data Modeling, Schema Design, Data Warehousing, ETL, Data Management, Data Classification, Data Retention","python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, kafka, storm, spark streaming, data modeling, schema design, data warehousing, etl, data management, data classification, data retention","airflow, data classification, data management, data retention, datamodeling, datawarehouse, docker, etl, helm, kafka, kubernetes, python, schema design, snowflake, spark, spark streaming, sql, storm"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Milwaukee, WI",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3748830441,2023-12-17,Waterford,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our next evolution is underway as a newly public company looking to expand and continue to build meaningful experiences for our users. From social issues to original content, we’re blazing innovative paths with impact for our community, all while leveraging the latest tech stacks and striving for engineering excellence. At the heart of our work in this new chapter is a shared set of core values: openness and exploration, a bias for action, and strong support of the LGBTQ community.
With a track record of strong financial performance and plans for continued headcount growth, we’re looking to build a team of talented, passionate, and open-minded people who believe in our mission, align with our values, and are excited to work at the intersection of innovative technology and social impact. Come be a part of this exciting journey with us.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location: Palo Alto, San Francisco or Chicago
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, SQL, TDD, Pair Programming, Continuous Integration, Automated testing, Deployment, Kafka, Storm, SparkStreaming, Dimensional data modeling, Schema design, ETL, Data Warehouses, Legal compliance, Data management tools, Data classification, Retention","python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, tdd, pair programming, continuous integration, automated testing, deployment, kafka, storm, sparkstreaming, dimensional data modeling, schema design, etl, data warehouses, legal compliance, data management tools, data classification, retention","airflow, automated testing, continuous integration, data classification, data management tools, data warehouses, deployment, dimensional data modeling, docker, etl, helm, kafka, kubernetes, legal compliance, pair programming, python, retention, schema design, snowflake, spark, sparkstreaming, sql, storm, tdd"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Milwaukee, WI",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3744390997,2023-12-17,Waterford,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
They are a profitable dating application.
Location: This is a hybrid role based in their Chicago office and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location
: This role is twice a week in a hybrid role minimum.
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Python, SQL, Data engineering, Business intelligence, Data science, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, Relational databases, TDD, Pair programming, Continuous integration, Automated testing, Deployment, Streamprocessing systems, Kafka, Storm, SparkStreaming, Dimensional data modeling, Schema design, Data warehouses, ETL, Data management tools, Data classification, Data retention, Legal compliance","python, sql, data engineering, business intelligence, data science, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, relational databases, tdd, pair programming, continuous integration, automated testing, deployment, streamprocessing systems, kafka, storm, sparkstreaming, dimensional data modeling, schema design, data warehouses, etl, data management tools, data classification, data retention, legal compliance","airflow, automated testing, business intelligence, continuous integration, data classification, data engineering, data management tools, data retention, data science, data warehouses, deployment, dimensional data modeling, docker, etl, helm, kafka, kubernetes, legal compliance, pair programming, python, relational databases, schema design, snowflake, spark, sparkstreaming, sql, storm, streamprocessing systems, tdd"
Data Center Technician,Tier4 Group,"Milwaukee, WI",https://www.linkedin.com/jobs/view/data-center-technician-at-tier4-group-3761238916,2023-12-17,Waterford,United States,Mid senior,Onsite,"Our top Fortune 100 client in Milwaukee, WI has an immediate opening for a Data Center Technician. This is an onsite position in downtown Milwaukee.
Description
The IT Facilities Engineer is responsible for deploying and maintaining servers and the supporting infrastructure in a dynamic environment with rapidly shifting priorities, while maintaining high availability. A confirmed attention to detail and effective documentation skills are important components to success.
The Data Center Services team strives to provide an efficient, risk driven approach to managing the raised floor area of the Data Center. These services include managing assets throughout their life cycle, data destruction, and ensuring appropriate physical and logical security for vendors within the environment. The work environment is a hands-on, fast paced, and deadline-oriented where adaptability is key to our success as a team, thus having depth knowledge of working in large-scale Data Center environments is a critical. The individual will be responsible for managing the inventory and assets in our Data Center in accordance with the standard policies and procedures designated by the group. This individual will play a key role in ensuring our Data Center inventory is up-to-date and accurate.
Job Responsibilities:
Providing ‘remote-hands’ support to data center customers.
Respond and resolve support tickets in system
Provide Level one Network support
Testing Fiber and copper cables
Cleaning Fiber and copper cables
Cleaning and replacing SFPs if needed
Installation and maintenance of the structured cabling system.
Maintain inventory of fiber and copper cabling
Quote and order additional materials as needed
Quote and order materials to build out additional Data Center capacity
Rack & stack of IT hardware.
Understanding of AC & DC power circuits.
Construction of ladder and fiber trays.
Maintaining records and documenting issues utilizing our DCIM system (Nlyte).
Operate, monitor, and respond to abnormal conditions with our facilities systems.
Crossover work with the CEE team as needed.
Show more
Show less","Data Center Operations, Server Deployment, Server Maintenance, Facilities Engineering, Asset Management, Data Destruction, Physical Security, Logical Security, Inventory Management, Remote Hands Support, Network Support, Fiber Optic Cabling, Copper Cabling, Structured Cabling, Ladder and Fiber Trays, DCIM Systems, Facility Systems Monitoring, AC and DC Power Circuits","data center operations, server deployment, server maintenance, facilities engineering, asset management, data destruction, physical security, logical security, inventory management, remote hands support, network support, fiber optic cabling, copper cabling, structured cabling, ladder and fiber trays, dcim systems, facility systems monitoring, ac and dc power circuits","ac and dc power circuits, asset management, copper cabling, data center operations, data destruction, dcim systems, facilities engineering, facility systems monitoring, fiber optic cabling, inventory management, ladder and fiber trays, logical security, network support, physical security, remote hands support, server deployment, server maintenance, structured cabling"
Sr. Data Engineer (Hybrid),Brady Corporation,"Milwaukee, WI",https://www.linkedin.com/jobs/view/sr-data-engineer-hybrid-at-brady-corporation-3658299090,2023-12-17,Waterford,United States,Mid senior,Hybrid,"Who we are:
Brady makes products that make the world a safer and more productive place. We are a global leader in safety, identification and compliance solutions for a diverse range of workplaces. From the depths of the ocean to outer space, from the factory floor to the delivery room - we’re just about everywhere you look. Companies around the world trust Brady because of our deep expertise and knowledge across a wide range of industries and applications - powered by our world-class manufacturing capabilities.
We have a diverse customer base in industries including electronics, telecommunications, manufacturing, electrical, construction, healthcare, aerospace and more. As of July 31, 2023, Brady employed approximately 5,600 people worldwide. Our fiscal 2023 sales were approximately $1.33 billion. Brady stock trades on the New York Stock Exchange under the symbol BRC. You can learn more about us at www.bradycorp.com.
Why work at Brady:
A career at Brady means working for a global company that has thrived for over 100 years, and whose innovative spirit drives our future growth.
Brady offers competitive pay and great benefits, supported by a culture that encourages collaboration and innovation. We strive to foster an inclusive workplace where diverse talent can learn, grow, and succeed. And with deeply rooted values, no matter where you work at Brady, you’ll feel connected to the community through our charitable contributions and opportunities to give back.
Our headquarters are in Milwaukee, Wisconsin, but we have more than 70 locations globally, giving our employees the opportunity to work with colleagues around the world.
What we need:
Brady is seeking a Senior BI Developer/Data Engineer who will design, development, and support of data pipelines from multiple applications and sources to facilitate data analytics needs. Our team partners with Sales & Marketing, Finance, and Operations business users and data scientists to help develop data driven insights and make data-driven decisions. The Senior BI Developer works with various data and BI tools and technologies, such as Python, SQL Server, Google Big Query, Google Cloud Composer, and etc.
What you'll be doing:
Develop data pipelines from multiple sources using Python with API calls.
Develop SQL code or data flow to integrate and transform data from multiple sources into useful, consistent and easily consumable data elements.
Work with business analysts and business users to understand data requirements and map into easy consumable data elements.
Provide ongoing enhancement and support for data pipelines in the production environment. Understand critical needs of the business, identify and fix issues with a sense of urgency.
Participate in discussions on design options and approaches, propose solutions and develop the new capabilities.
Perform and lead the code reviews to identify coding, process and functionality improvements.
Work on troubleshooting, identifying issues and performance bottlenecks, problem solving and impact analysis.
Follow the Scrum process and take ownership of Jira stories.
Lead the offshore developers, provide directions and guide them with development and support processes.
What you'll need to be successful:
Bachelor of Science in Computer Science, Engineering or related field
Self-motivated with a strong work ethic, ability to work independently as well as ability to work within a team
Strong communication skills, ability to communicate well with technical and non-technical staff
3+ years experience working with Python or .NET programming
3+ years of experience working with ETL and SQL development
3+ years of experience working with cloud data platform, GCP, Azure or AWS
Strong analytical and problem solving skills including the ability to independently troubleshoot issues and determine and develop the best solution while considering all impacts to the production system
Willing to learn and able to adapt to new technologies as needed
Benefits:
Complete insurance coverage starting on first day of employment – medical, dental, vision, life
401(k) with company match
Tuition reimbursement
Bonus opportunity
Vacation and holiday pay
Show more
Show less","Python, SQL, SQL Server, Google Big Query, Google Cloud Composer, API, ETL, GCP, Azure, AWS, Scrum, Jira","python, sql, sql server, google big query, google cloud composer, api, etl, gcp, azure, aws, scrum, jira","api, aws, azure, etl, gcp, google big query, google cloud composer, jira, python, scrum, sql, sql server"
Senior Azure Data Engineer,Codeworks IT Careers,"Milwaukee, WI",https://www.linkedin.com/jobs/view/senior-azure-data-engineer-at-codeworks-it-careers-3756105275,2023-12-17,Waterford,United States,Mid senior,Hybrid,"Codeworks is an IT Services firm in SE Wisconsin, known for our strong commitment to quality and for our direct client relationships.
Who We’re Looking For!...
A Senior Azure Data Engineer! A hybrid flexible role located in Milwaukee, Wisconsin.
Education And Experience Requirements
Bachelor's degree in Computer or Computational Science, Computer Engineering, IT is
preferred.
5-7 years IT experience including 2-3 years as a technical lead in a Big Data or Data
Warehousing Environment.
Significant depth of expertise and track record of accomplishments in multiple IT
technology areas.
Experience utilizing either of the following: Azure (Data Factory, Data Lake, Machine
Learning, SQL Warehouse, Data Bricks, Power BI), SAP Analytics (SAP Business
Warehouse, Business Objects)
Experience engineering data pipelines and logical data models that serve business
analytics and visualization tools such as PowerBI
Experience building algorithm, automation and deployment code using Python or Scala.
Familiarity with Spark is plus
Experience Optimizing Data Loads, Data Extractions, Queries and Stored Procedures
Experience working in a DevOps environment and using CI/CD, Version Control etc as
part of development process
Preferred: Experience working with Streaming data sources such as IoT, Clickstream etc
and building data analytic applications from event driven data sources
Preferred: Experience building and optimizing data pipelines for Machine Learning
Applications
Preferred: Experience in ML Ops processes and Kubernetes based model serving
Preferred: Certifications on Azure Cloud
Good to Have: The ability to read, write, and speak Mandarin or French
Local candidates to Milwaukee area will be given priority. If you feel that you meet the qualifications listed above, please forward your resume in Word format to kristy.harmann@codeworks-inc.com.
About CODEWORKS:
Headquartered in Milwaukee, WI with an office in Madison, WI—Codeworks has over 25+ years of experience serving Fortune 1000 companies in Wisconsin as well as our client's national locations. Our recruiting team is extremely good at evaluating, advising, and connecting IT professionals with new opportunities that will satisfy their expectations both in salary and opportunity for growth.
For more information, please visit our website at: www.codeworks-inc.com.
For priority career/job posting updates, please follow us on Twitter: @CodeworksIT
Show more
Show less","Azure, Data Factory, Data Lake, Machine Learning, SQL Warehouse, Data Bricks, Power BI, SAP Analytics, SAP Business Warehouse, Business Objects, Python, Scala, Spark, DevOps, CI/CD, Version Control, Streaming data, IoT, Clickstream, Machine Learning Applications, ML Ops, Kubernetes, Azure Cloud, Mandarin, French","azure, data factory, data lake, machine learning, sql warehouse, data bricks, power bi, sap analytics, sap business warehouse, business objects, python, scala, spark, devops, cicd, version control, streaming data, iot, clickstream, machine learning applications, ml ops, kubernetes, azure cloud, mandarin, french","azure, azure cloud, business objects, cicd, clickstream, data bricks, data factory, data lake, devops, french, iot, kubernetes, machine learning, machine learning applications, mandarin, ml ops, powerbi, python, sap analytics, sap business warehouse, scala, spark, sql warehouse, streaming data, version control"
Senior Data Engineer,MGIC,"Milwaukee, WI",https://www.linkedin.com/jobs/view/senior-data-engineer-at-mgic-3735207155,2023-12-17,Waterford,United States,Mid senior,Hybrid,"Why work at MGIC?
Are you someone who wants to play a critical role in our company’s success? Do you enjoy solving puzzles and finding a better way to get things done? Are you someone who likes to Take The Lead and make an impact? If so, then imagine yourself at MGIC. At MGIC we are a team of dedicated professionals on a fearless mission. A team that fosters a culture of career development and continuous learning opportunities to help you rise to new heights. We are passionate about providing outstanding customer service and making a difference in our community. #WeAreMGIC
Preferred location: Milwaukee based – hybrid (3 days office, 2 days remote)
Other locations: Madison, Minnesota, Illinois, and Michigan (occasional travel into the office to support team engagements).
How will you make an impact?
We are currently looking for a Senior Data Engineer to deliver high quality, maintainable and scalable data solutions. The Senior Data Engineer will partner with solution architects and other data engineers to develop our enterprise analytics data platform through new and updated data pipelines leveraging shared components and aligning to standards and best practices. This role is focused on collecting data from internal and external sources and transforming it into usable information for the business including data scientists and data analysts.
Deliver high quality data assets to be used by the business to transform business processes and to enable leaders to make data-driven decisions
Continuously improve data solutions to increase quality, speed of delivery and trust of data engineering team’s deliverables to enable business outcomes
Reduce total cost of ownership of solutions by developing shared components and implementing best practices and coding standards
Do you have what it takes?
Ability to translate data engineering designs into working code
Data analysis and data engineering pipeline experience including design, development, and support
Experience with AWS services or cloud data offerings including S3, Lambda, EMR, Dynamo DB, Glue, Snowflake and/or other data technologies.
Experience with coding in Python, PySpark, and Terraform
Experience with DevOps practices including Continuous Integration, Continuous Delivery, and Infrastructure as Code to deliver end-to-end automation of data delivery.
Ability to train and mentor junior data engineers.
Experience with Agile engineering practices including the scrum framework.
Enjoy these benefits from day one:
Competitive Salary & pay-for-performance bonus
Financial Benefits (401k with company match, profit sharing, HSA, wellness rewards program)
On-site Fitness Center and classes (corporate office)
Paid-time off and paid company holidays
Business casual dress
For additional information about MGIC and to apply, please visit our website at www.mgic.com/careers.
Show more
Show less","Data Engineering, Data Analysis, Data Pipelines, AWS, S3, Lambda, EMR, Dynamo DB, Glue, Snowflake, Python, PySpark, Terraform, DevOps, Continuous Integration, Continuous Delivery, Infrastructure as Code, Agile Engineering, Scrum","data engineering, data analysis, data pipelines, aws, s3, lambda, emr, dynamo db, glue, snowflake, python, pyspark, terraform, devops, continuous integration, continuous delivery, infrastructure as code, agile engineering, scrum","agile engineering, aws, continuous delivery, continuous integration, data engineering, dataanalytics, datapipeline, devops, dynamo db, emr, glue, infrastructure as code, lambda, python, s3, scrum, snowflake, spark, terraform"
Senior Cloud Data Engineer,BDO USA,"Milwaukee, WI",https://www.linkedin.com/jobs/view/senior-cloud-data-engineer-at-bdo-usa-3765466996,2023-12-17,Waterford,United States,Mid senior,Hybrid,"Job Description
Job Summary:
This position will work with cutting edge technology, deliver high quality solutions across various industries, and oversee team(s) on engagements that range in size and scope. This position will receive continuous career development opportunities, given the size and potential of client engagements. This role will perform hands-on delivery of data analytics projects, contributing to the development and unit testing of solutions.
Job Duties
Designs and implements best in class data ingestion strategies, data warehouse and data mart structures, semantic layers and models, visualizations, streaming processes, API integrations, and automation (RPA) solutions for end-to-end data analytics solutions on primarily, but not limited to, cloud analytics platforms such as Azure and AWS
Listens to client needs to align solution with business requirements and delivery schedule
Creates written functional and technical designs
Participates in project status and stand meetings, and assists with providing aggregated project status for project and program managers
Assists with SLA compliance of solutions, and performs performance tuning and optimization efforts of end-to-end solutions
Writes code using multiple languages and correctly applies frameworks, architectural patterns, and software development principles
Delivers high-performance, scalable, repeatable, and secure deliverables with broad impact (high throughput and low latency)
Assists with implementation of data governance programs and best practices
Performs the cleaning and transforming of data from source systems into analytics models
Implements models to support data visualizations and integrations
Assists with implementing DevOps, DataOps and MLOps methodologies on projects
Writes custom integration logic in applicable programming languages
Assists project managers with work breakdown structure creation, project estimation, resource staffing, workload planning and adjustments throughout the project lifecycle
Assists clients with licensing, security, and cost estimation of solutions
Performs code reviews to ensure adherence to standards
Works directly with clients and team members to establish secure data analytics platforms and infrastructure
Contributes to successful deployments of developed solutions and integration of DevOps tools
Maintains a broad and current understanding of data analytics and business intelligence strategies, cloud platforms, methodologies, and tools
Builds client relationships during project execution, effectively becoming a trusted advisor of the client
Participates in support activities for existing software solutions
Other duties as assigned
Supervisory Responsibilities
Supervises the day-to-day workload of Associates on assigned engagements to ensure that timelines and deliverables are met, and reviews work product
Education
Qualifications, Knowledge, Skills and Abilities:
High School Diploma or GED equivalent, required
Bachelor’s degree, preferred; focus in Information Systems, Data Science or Computer Science, preferred
Experience
Five (5) or more years of experience within Data Analytics, Business Intelligence, Artificial Intelligence, or Application Development, required
One (1) or more years of experience technically leading development projects, preferred
One (1) or more years of consulting experience or implementation of cloud-based data analytics solutions, preferred
Software
Strong SQL skills including Data Definition Language (DDL), Data Manipulation Language (DML), views, functions, stored procedures, or performance tuning, required
Experience with Data Warehousing, Data Modeling, Semantic Model Definition or Star Schema Construction, required
Hands on delivery experience of end-to-end cloud data analytics solutions within Azure or AWS, preferred
Experience with one (1) or more of the following computer languages, preferred:
C#
Python
Java
Scala
Experience with tabular modeling within Microsoft Fabric, Power BI, or Azure Analysis Services, preferred
Experience with Git and DevOps deployment technologies, preferred
Experience with Linux, preferred
Experience with one (1) or more of the following, preferred:
Data Lake Medallion Architecture
Batch and/or streaming data ingestion into a data lake
AI Algorithms/Machine Learning
Automation tools such as UiPath, Alteryx, etc.
Computer Vision based AI technologies
Other Knowledge, Skills & Abilities
Ability to work with a high degree of professionalism and autonomy
Excellent verbal and written communication skills
Solid organizational skills, especially the ability to meet project deadlines with a focus on details
Ability to successfully multi-task while working independently or within a group environment
Ability to work in a deadline-driven environment, and handle multiple projects simultaneously
Ability to interact effectively with people at all organizational levels of the Firm
Ability to effectively interact with a team of professionals and delegating work assignments, as needed
Ability to build and maintain strong relationships with internal and client personnel
Ability to encourage a team environment on engagements, and contribute to the professional development of assigned personnel
Keywords:
Data Analytics, Business Intelligence, BI, Synapse, IoT, Machine Learning, Data Lake, Stream, Cube, Microsoft, SQL Server, Tableau, .Net, C#, Qlik, Power BI, Machine Learning, Azure Data Factory, RedShift, UiPath, Cloud, RPA, AWS, Redshift, Kinesis, QuickSight, SageMaker, S3, Databricks, AWS Lake Formation, Snowflake, Python, Qlik, Athena, Data Pipeline, Glue, Star Schema, Data Modeling, SQL, SSIS, SSAS, SSRS, PySpark, Microsoft Fabric, dbt, Linux, Terraform, Bicep, Data Ops, Purview, Git, Delta, Pandas, Spark SQL
Individual salaries that are offered to a candidate are determined after consideration of numerous factors including but not limited to the candidate’s qualifications, experience, skills, and geography.
California Range: $111,000 - $152,000
Colorado Range: $111,000 - $152,000
New York City/ Valhalla Range: $111,000 - $152,000
Washington Range: $111,000 - $152,000
About Us
BDO delivers assurance, tax, digital technology solutions and financial advisory services to clients throughout the country and around the globe. We offer numerous industry-specific practices, world-class resources, and an unparalleled commitment to meeting our clients’ needs. We currently serve more than 400 publicly traded domestic and international clients.
Unparalleled partner-involvement
Deep industry knowledge and participation
Geographic coverage across the U.S.
Cohesive global network
Focused capabilities across disciplines
BDO brings world-class resources and exceptional service to each and every one of our clients. BDO USA is a member of BDO International, the world’s fifth largest accounting network.
BDO offers a competitive Total Rewards package that encompass so much more than – “traditional benefits”. Our wide range of rewards and our employees’ ability to customize rewards to their individual needs are two of the reasons why BDO has been honored with so many workplace awards, including 100 Best Companies for Working Parents, Working Mother 100 Best Companies, Top Entry Level Employer, 2022 National Best & Brightest Companies to Work For and more.
Some Examples Of Our Total Rewards Offerings Include
Competitive pay and eligibility for an annual performance bonus.
A 401k plan plus an employer match
Comprehensive, medical, dental, vision, FSA, and prescription insurance from day one
Competitive Paid Time Off with daily accrual from day one of employment, plus paid holidays
Paid Parental Leave
Adoption Assistance
Firm paid life insurance
Wellness programs
Additional offerings include BDO Flex, Group Legal insurance, Pet insurance and Long-Term Care Insurance
Above offerings may be subject to eligibility requirements.
Click here to find out more!
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability or protected veteran status.
""BDO USA, P.A. is an EO employer M/F/Veteran/Disability""
Show more
Show less","Data Analytics, Business Intelligence, Artificial Intelligence, Application Development, SQL, Data Warehousing, Data Modeling, Semantic Model Definition, Star Schema Construction, Cloud Data Analytics, C#, Python, Java, Scala, Tabular Modeling, Git, DevOps, Linux, Data Lake Medallion Architecture, Batch Data Ingestion, Streaming Data Ingestion, AI Algorithms, Machine Learning, Automation Tools, Computer Vision, Tableau, .Net, Qlik, Power BI, Azure Data Factory, Redshift, UiPath, Cloud, RPA, AWS, Redshift, Kinesis, QuickSight, SageMaker, S3, Databricks, AWS Lake Formation, Snowflake, PySpark, Microsoft Fabric, dbt, Terraform, Bicep, Data Ops, Purview, Delta, Pandas, Spark SQL, SSIS, SSAS, SSRS","data analytics, business intelligence, artificial intelligence, application development, sql, data warehousing, data modeling, semantic model definition, star schema construction, cloud data analytics, c, python, java, scala, tabular modeling, git, devops, linux, data lake medallion architecture, batch data ingestion, streaming data ingestion, ai algorithms, machine learning, automation tools, computer vision, tableau, net, qlik, power bi, azure data factory, redshift, uipath, cloud, rpa, aws, redshift, kinesis, quicksight, sagemaker, s3, databricks, aws lake formation, snowflake, pyspark, microsoft fabric, dbt, terraform, bicep, data ops, purview, delta, pandas, spark sql, ssis, ssas, ssrs","ai algorithms, application development, artificial intelligence, automation tools, aws, aws lake formation, azure data factory, batch data ingestion, bicep, business intelligence, c, cloud, cloud data analytics, computer vision, data lake medallion architecture, data ops, dataanalytics, databricks, datamodeling, datawarehouse, dbt, delta, devops, git, java, kinesis, linux, machine learning, microsoft fabric, net, pandas, powerbi, purview, python, qlik, quicksight, redshift, rpa, s3, sagemaker, scala, semantic model definition, snowflake, spark, spark sql, sql, ssas, ssis, ssrs, star schema construction, streaming data ingestion, tableau, tabular modeling, terraform, uipath"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Milwaukee, WI",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3759711064,2023-12-17,Waterford,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto,
San
Francisco or Chicago
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, ML DataOps, Data Pre/Post Processing, Applied ML Solutions, Statistical Analysis, Data Visualization, Pandas, R, Big Data Technologies, SnowFlake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, Public Cloud Environment, AWS, GCP, Azure, Relational Databases, SQL, NoSQL, DynamoDB, ETL, Conversational AI APIs, Recommender Systems, Distributed Systems, Microservices, Kafka, Storm, SparkStreaming, Applied Machine Learning, Data Management Tools, Data Classification, Retention","data engineering, ml dataops, data prepost processing, applied ml solutions, statistical analysis, data visualization, pandas, r, big data technologies, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, public cloud environment, aws, gcp, azure, relational databases, sql, nosql, dynamodb, etl, conversational ai apis, recommender systems, distributed systems, microservices, kafka, storm, sparkstreaming, applied machine learning, data management tools, data classification, retention","airflow, applied machine learning, applied ml solutions, aws, azure, big data technologies, conversational ai apis, data classification, data engineering, data management tools, data prepost processing, distributed systems, docker, dynamodb, etl, gcp, helm, kafka, kubernetes, microservices, ml dataops, nosql, pandas, public cloud environment, r, recommender systems, relational databases, retention, snowflake, spark, sparkstreaming, sql, statistical analysis, storm, visualization"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Milwaukee, WI",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3773089684,2023-12-17,Waterford,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto
or
San
Francisco
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Pandas, R, Python, Java, bash, SQL, Git, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, AWS, GCP, Azure, DynamoDB, ETL, Kafka, Storm, SparkStreaming, Kubernetes","pandas, r, python, java, bash, sql, git, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, dynamodb, etl, kafka, storm, sparkstreaming, kubernetes","airflow, aws, azure, bash, docker, dynamodb, etl, gcp, git, helm, java, kafka, kubernetes, pandas, python, r, snowflake, spark, sparkstreaming, sql, storm"
Marketing Data Analyst,"TSR Consulting Services, Inc.","Naples, FL",https://www.linkedin.com/jobs/view/marketing-data-analyst-at-tsr-consulting-services-inc-3774929341,2023-12-17,Naples,United States,Mid senior,Onsite,"Our client, a leading software company is hiring a
Marketing Data Analyst.
Work Location:
Naples, FL (hybrid, 3-4 days a week onsite)
Main Objective
The Marketing Data Analyst works with Marketing Operations, Data & Analytics Center of Excellence, Product Management, and other stakeholders to provide comprehensive data analysis, unlock opportunities through valuable insights and drive decision support solutions leveraging the best-in-class data & analytics tools.
The analyst will help mature the use of data with user-friendly analytical tools, dashboards, and advanced analytics so that relevant information can be efficiently gathered, shared, and interpreted by our business teams and their partners.
Essential Duties And Responsibilities
Implement and manage Google Analytics to track website traffic, user behavior, and conversion rates.
Utilize Google Analytics to monitor and analyze the performance of digital marketing campaigns, identifying areas for improvement.
Extract data from Google Analytics to provide insights on website user journeys and identify conversion funnel bottlenecks.
Extract and transform data from various sources and load it into BigQuery for in-depth analysis.
Write SQL queries and utilize BigQuery’s data processing capabilities to analyze large datasets effectively.
Collaborate with cross-functional teams to analyze operational processes within the Marketing Department.
Use data-driven insights to streamline marketing operations, reduce costs, and optimize resource allocation.
Identify and recommend process improvements based on operational analytics findings.
Create visually engaging and informative reports and dashboards using data visualization tools (e.g., Tableau, Power BI).
Share regular reports with key stakeholders, translating complex data into actionable insights.
Maintain data security and compliance with relevant regulations, such as GDPR or CCPA, in data handling and analysis practices.
Stay updated with industry trends, tools, and best practices in data analytics, digital marketing, and data warehousing.
Continuously expand skillset in data analysis, statistical modeling, and data visualization.
Collaborate with marketing teams, IT, and other departments to gather data requirements and share insights.
Clearly communicate findings and recommendations to both technical and non-technical stakeholders.
Occasional travel for training, meetings, or trade shows may be required
Additional Duties And Responsibilities
Document current processes and models to understand opportunities for improvement
Problem Solving: Develop solutions to a variety of complex problems. May refer to established precedents and policies.
Discretion/Latitude: Work is performed under general direction. Participates in determining success metrics for work performed.
Education/Experience
Bachelor’s degree required
5+ years of relevant work experience required
Intermediate to advanced level of experience with Google Analytics, Tag Manager required
Intermediate to advanced level of experience with SQL required
Intermediate level of experience using Front-End Data Visualization & Analytical Tools is a must
Specialized Skills
Fundamental understanding of major functions in a global organization
Strong business acumen (in one or more verticals) is preferred
Data literacy is a must
Strong analytics and data analysis skills is preferred
Strong visualization skills is preferred
UX design expertise is a plus
Experience in a Life Sciences – Med Device company is a plus
Data science/Advanced analytical skills is a plus
Incidental Duties
The above statements describe the general nature and level of work being performed in this job. They are not intended to be an exhaustive list of all duties, and indeed additional responsibilities may be assigned, as required, by management.
Machine, Tools, and/or Equipment Skills: Experience working with a variety of software tools including Microsoft Office Suite and industry leading Analytics and Business Intelligence software.
Reasoning Ability: Ability to define problems, collect data, establish facts, and draw valid conclusions. Ability to interpret an extensive variety of technical instructions in mathematical or diagram form and deal with several abstract and concrete variables.
The post Marketing Data Analyst appeared first on TSR Consulting Services.
Show more
Show less","Google Analytics, Tag Manager, SQL, BigQuery, Data visualization tools (e.g. Tableau Power BI), Frontend data visualization tools, Data science, UX design, Microsoft Office Suite, Analytics and Business Intelligence software","google analytics, tag manager, sql, bigquery, data visualization tools eg tableau power bi, frontend data visualization tools, data science, ux design, microsoft office suite, analytics and business intelligence software","analytics and business intelligence software, bigquery, data science, data visualization tools eg tableau power bi, frontend data visualization tools, google analytics, microsoft office suite, sql, tag manager, ux design"
Data Center Engineer - Detroit,DeRisk Technologies,"Detroit, MI",https://www.linkedin.com/jobs/view/data-center-engineer-detroit-at-derisk-technologies-3766683122,2023-12-17,Windsor, Canada,Associate,Onsite,"Job Responsibilities:
Deployment / In-Scope Configuration Items
Servers (Virtual & Physical)
Storage & Backup Devices
Server Appliances
Hyper Converged Infrastructure (E.g. Nutanix, Cisco UCS)
Tape Storage Units
Power Distribution Units rated 3KVA and below
SAN Fabric Switches
Network Switches
KVM Units
WAN Optimization Devices
Firewalls
Access Points
Routers
Physical Cabling
Cable Management
Cables which connect the device to itself, a peripheral, or a power/network port Break-fix / Technical Tasks List
Power Cycling
Running Diagnostics Commands
Conducting whole unit replacement
Inserting/Removing Media
Replacing Defective Components
Assist with fault diagnosis and investigation
Configuring Remote Access
Basic Storage Array Configuration
Replacing faulty cables
Tape Management and Maintenance
Rebooting routers, servers, storage devices or other equipment
Operations Tasks List
Updating and Recording of activities in relevant IT Ticket Management System
Coordinating and agreeing attendance time and date with key stakeholders
Provide support either through phone, remote tools, or in person at onsite
Perform installation as needed either through physical or network medium
Coordinate actual activity date and timings including arrival and departure times
Carry all necessary tools, laptops etc. which might be needed to support issues in the infrastructure environment
Labelling, Patching and Asset Tagging activities
Following specific task instructions and provision necessary reporting as necessary
Requirements
Job Requirements:
Technical Skills
Good general understanding of IT principles such as Networks, Hardware and Domains
Knowledge of Infrastructure (Data Center and Network) hardware architecture as to understand the procedure shared by L3 teams during troubleshooting,H&E support
Knowledge of server/client operations in a domain environment including Active Directory
Understanding of current and legacy hardware Infrastructure platforms
Hands-on experience in installation and troubleshooting Infrastructure (DC & Network) equipment's, Rack and Stack of the equipment/cable
Good Hands-on Experience in IMAC and Break-fix activities related to Infrastructure environment
Ability to identify Excellent the right defective spares and replace them with provided good spares as instructed and by physical observations
Knowledge of TCP/I P standards and networking
Experience with Tape Management activities
Excellent knowledge of best practices around management, control, and monitoring of server infrastructure
Familiarity with backup and recovery software and methodologies
Language skills needed
English Soft Skills
Exceptional customer facing skills
Able to communicate clearly and effectively both with Client and the Customer
Logical and analytical approach to work
Accurate record keeping
Able to work unsupervised
Good timekeeper
Intense focus on quality work
Productive and Efficient Academic Background
Bachelor of Engineering / Technology / Science or Equivalent Work Experience Overall Experience (in yrs.)
5 - 7 years (min
Benefits
Salary and Benefits as per market standard
Show more
Show less","IT principles, Networking, Hardware, Domains, Infrastructure architecture, Server operations, Client operations, Active Directory, Hardware Infrastructure platforms, Installation, Troubleshooting, Infrastructure equipment, Rack and Stack, IMAC, Breakfix, TCP/IP standards, Tape Management, Server infrastructure management, Backup and recovery software, English, Customer service skills, Communication skills, Logical thinking, Analytical thinking, Record keeping, Unsupervised work, Time management, Quality focus, Productivity, Efficiency, Bachelor's degree in Engineering/Technology/Science","it principles, networking, hardware, domains, infrastructure architecture, server operations, client operations, active directory, hardware infrastructure platforms, installation, troubleshooting, infrastructure equipment, rack and stack, imac, breakfix, tcpip standards, tape management, server infrastructure management, backup and recovery software, english, customer service skills, communication skills, logical thinking, analytical thinking, record keeping, unsupervised work, time management, quality focus, productivity, efficiency, bachelors degree in engineeringtechnologyscience","active directory, analytical thinking, bachelors degree in engineeringtechnologyscience, backup and recovery software, breakfix, client operations, communication skills, customer service skills, domains, efficiency, english, hardware, hardware infrastructure platforms, imac, infrastructure architecture, infrastructure equipment, installation, it principles, logical thinking, networking, productivity, quality focus, rack and stack, record keeping, server infrastructure management, server operations, tape management, tcpip standards, time management, troubleshooting, unsupervised work"
Data Developer,Epsilon,"Southfield, MI",https://www.linkedin.com/jobs/view/data-developer-at-epsilon-3782266857,2023-12-17,Windsor, Canada,Associate,Onsite,"Job Description
The Data Engineer position will focus on designing, developing, and supporting our Hadoop data solutions in Spark and Python (PySpark) while working with other components of the Hadoop ecosystem such as HDFS, Hive, Hue, Impala, Zeppelin, Jupyter. A successful candidate will work closely with business and portfolio leads to understand requirements then design and build innovative data solutions.
Job Duties & Responsibilities.
Design and development centered around PySpark, Python and Hadoop Framework.
Working with gigabytes/terabytes of data and must understand the challenges of transforming and enriching such large datasets.
Provide effective solutions to address the business problems – strategic and tactical.
Collaboration with team members, project managers, business analysts and QA teams in conceptualizing, estimating and developing new solutions and enhancements.
Work closely with the stake holders to define and refine the big data platform to achieve sales, product, and strategic objectives.
Collaborate with other technology teams and architects to define and develop cross-function technology stack interactions.
Read, extract, transform, stage and load (ETL) data to multiple targets, including Hadoop and Oracle.
Ingest and streamline incoming files of various layouts/formats as part of Source Prep process.
Develop scripts around Hadoop framework to automate processes and existing flows.
Modify existing programming/code for new requirements.
Estimate work, and track progress through SDLC with JIRA/Confluence
Unit testing and debugging. Perform root cause analysis (RCA) for any failed processes.
Convert business requirements into technical design specifications and execute on them.
Participate in code reviews and keep applications/code base in sync with version control (GIT/Bitbucket).
Effective communication, self-motivation, and ability to work independently while remaining fully aligned within a distributed team environment.
Required Skills
Bachelor’s or Master’s degree in Computer science (or Engineering equivalent).
3+ years of experience with big data ingestion, transformation and staging.
Analysis, design and implementation experience with Hadoop distributed frameworks, including Python & Spark (SparkSQL, PySpark), HDFS, Hive, Impala, Hue, Cloudera Hadoop, Zeppelin, Jupyter, etc.
Extensive experience handling large volumes of data (measured in Terabytes/Billions of Transactions)
Proficient knowledge of SQL with any RDBMS
Familiarity with RDD and Data Frames within Spark
Working knowledge of data analytics
Troubleshooting and complex problem-solving skills
Knowledge of Oracle databases and PL/SQL
Working knowledge of Linux/Unix environments and comfort with Unix Shell scripts (ksh, bash)
Basic Hadoop administration knowledge.
DevOps Knowledge is an advantage
Ability to work within deadlines and effectively prioritize and execute on tasks
Strong communication skills (verbal and written) with ability to communicate across teams, internal and external at all levels
Preferred Skills
Working knowledge of Oracle databases and PL/SQL.
Hadoop Admin & Dev-Ops.
ETL Skills (Familiarity with Talend or other ETL tools a plus.
Good analytical thinking and problem-solving skills.
Ability to diagnose and troubleshoot problems quickly.
Motivated to learn new technologies, applications, and domains.
Possess appetite for learning through exploration and reverse engineering.
Strong time management skills.
Ability to take full ownership of tasks and projects.
Team player with excellent interpersonal skills.
Good verbal and written communication.
Possess Can-Do attitude to overcome any kind of challenges.
Preferred Certifications (Any Of These)
CCA Spark and Hadoop Developer.
MapR Certified Spark Developer (MCSD).
MapR Certified Hadoop Developer (MCHD).
HDP Certified Apache Spark Developer.
HDP Certified Developer.
Additional Information
About Epsilon
Epsilon is a global advertising and marketing technology company positioned at the center of Publicis Groupe. Epsilon accelerates clients’ ability to harness the power of their first-party data to activate campaigns across channels and devices, with an unparalleled ability to prove outcomes. The company’s industry-leading technology connects advertisers with consumers to drive performance while respecting and protecting consumer privacy. Epsilon’s people-based identity graph allows brands, agencies and publishers to reach real people, not cookies or devices, across the open web. For more information, visit epsilon.com.
When you’re one of us, you get to run with the best.
For decades, we’ve been helping marketers from the world’s top brands personalize experiences for millions of people with our cutting-edge technology, solutions and services. Epsilon’s best-in-class identity gives brands a clear, privacy-safe view of their customers, which they can use across our suite of digital media, messaging and loyalty solutions. We process 400+ billion consumer actions each day and hold many patents of proprietary technology, including real-time modeling languages and consumer privacy advancements. Thanks to the work of every employee, Epsilon has been consistently recognized as industry-leading by Forrester, Adweek and the MRC. Positioned at the core of Publicis Groupe, Epsilon is a global company with more than 8,000 employees around the world. Check out a few of these resources to learn more about what makes Epsilon so EPIC
Our Culture https //www.epsilon.com/us/about-us/our-culture-epsilon
Life at Epsilon https //www.epsilon.com/us/about-us/epic-blog
DE&I https //www.epsilon.com/us/about-us/diversity-equity-inclusion
CSR https //www.epsilon.com/us/about-us/corporate-social-responsibility
Great People Deserve Great Benefits
We know that we have some of the brightest and most talented associates in the world, and we believe in rewarding them accordingly. If you work here, expect competitive pay, comprehensive health coverage, and endless opportunities to advance your career.
Epsilon is an Equal Opportunity Employer.
Epsilon’s policy is not to discriminate against any applicant or employee based on actual or perceived race, age, sex or gender (including pregnancy), marital status, national origin, ancestry, citizenship status, mental or physical disability, religion, creed, color, sexual orientation, gender identity or expression (including transgender status), veteran status, genetic information, or any other characteristic protected by applicable federal, state or local law. Epsilon also prohibits harassment of applicants and employees based on any of these protected categories. Epsilon will provide accommodations to applicants needing accommodations to complete the application process.
REF216743S
Show more
Show less","Hadoop, Spark, PySpark, HDFS, Hive, Hue, Impala, Zeppelin, Jupyter, Python, SQL, RDBMS, RDD, Data Frames, Data analytics, Oracle, PL/SQL, Linux, Unix, Unix Shell scripts, ETL, Talend, CCA Spark and Hadoop Developer, MapR Certified Spark Developer (MCSD), MapR Certified Hadoop Developer (MCHD), HDP Certified Apache Spark Developer, HDP Certified Developer","hadoop, spark, pyspark, hdfs, hive, hue, impala, zeppelin, jupyter, python, sql, rdbms, rdd, data frames, data analytics, oracle, plsql, linux, unix, unix shell scripts, etl, talend, cca spark and hadoop developer, mapr certified spark developer mcsd, mapr certified hadoop developer mchd, hdp certified apache spark developer, hdp certified developer","cca spark and hadoop developer, data frames, dataanalytics, etl, hadoop, hdfs, hdp certified apache spark developer, hdp certified developer, hive, hue, impala, jupyter, linux, mapr certified hadoop developer mchd, mapr certified spark developer mcsd, oracle, plsql, python, rdbms, rdd, spark, sql, talend, unix, unix shell scripts, zeppelin"
Junior Data Analyst,Applus+ Laboratories,"Troy, MI",https://www.linkedin.com/jobs/view/junior-data-analyst-at-applus%2B-laboratories-3783922152,2023-12-17,Windsor, Canada,Associate,Onsite,"Applus+ Laboratories
is a leading product testing, inspection, and certification (TIC) organization, providing product safety testing and certification, on-site field evaluation & certification services to a wide range of industries. We are committed to providing our clients with the highest quality services and meeting their needs in a timely and efficient manner.
Job Summary
We are seeking a junior level
Data Analyst
to help lead automation, build power BI dashboards and provide digital solutions where needed with a strong focus on growth and learning different areas of the business.
Responsibilities
Fully transition US and Canadian entities to the Applus+ Commercial Power BI Dashboard
Build a Power BI dashboard for key operational and financial metrics
Lead automation for a highly manual reporting process
Think critically outside the box, analyze current processes, and provide Digital solutions to enhance efficiency and productivity
Strong cross team collaboration with open communication between North America and the senior team in Spain headquarters.
Manage and optimize new digital tools, platforms, and process to ensure smooth workflows
Analyze and evaluate the performance of these tools and identify areas for improvement
Education
Bachelor’s degree: Computer Science, software engineering, business technology or comparable
Qualifications
Required:
1-3 years of relevant experience
mySQL coding: extract and analyze from existing data sources
Power Query raw data transformation
Power BI dashboard creation
Preferred:
Qlikview
Tableau dashboard creation
Data workhouse
Exposure to dashboard and metrics design
Skills:
Project Management skills with deadline reinforcement
Capable of coordinating several teams and to be able to communicate persistently if needed
Posses the necessary social/interpersonal and leadership skills to motivate staff and to communicate effectively at all levels
Prioritize different projects and meet deadlines
Additional Information
This position is preferred in Troy Michigan, but open to locations such as: Punta Gorda Florida, Toronto, or Ithaca New York.
We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.
Show more
Show less","Data Analysis, Power BI, Power Query, MySQL, Qlikview, Tableau, Data Warehouse, Project Management, Deadline Reinforcement, Team Coordination, Communication, Prioritization","data analysis, power bi, power query, mysql, qlikview, tableau, data warehouse, project management, deadline reinforcement, team coordination, communication, prioritization","communication, dataanalytics, datawarehouse, deadline reinforcement, mysql, power query, powerbi, prioritization, project management, qlikview, tableau, team coordination"
Data Engineer,Electrical Components International,"Southfield, MI",https://www.linkedin.com/jobs/view/data-engineer-at-electrical-components-international-3784384152,2023-12-17,Windsor, Canada,Associate,Onsite,"Electrical Components International (ECI) is looking for a
Data Engineer
to join our team in Southfield, MI. This is an on-site position.
Qualified candidates must have:
Bachelor’s degree in computer science, Information Systems, or equivalent work experience; MBA preferred.
3+ years of experience in data management disciplines including data warehousing, data integration, modeling, optimization, and data quality, and/or other areas directly relevant to data engineering responsibilities and tasks in Microsoft Azure Platform.
Experience with implementation and support of Azure cloud services including but not limited to: Azure Data Factory, Azure Data Lake Store, Azure Data Lake Analytics, Azure Analysis Services, Azure SQL, Azure DW, Azure Analytics Services and Synapse Analytics (Required)
Experience developing both multidimensional and tabular models with large and complex datasets.
About Us:
Founded in 1953, Electrical Components International (ECI) is one of the world’s leading suppliers of electrical distribution systems, control box assemblies, and other critical engineered components for diversified markets. With 25,000 employees and 37 global manufacturing locations, ECI is the trusted partner to market leaders with 500+ customers. At ECI, we Power Smart, Connected, And Electrified SolutionsTM that help solve the most complex challenges.
About the Position:
The
Data Engineer
provides strategic, analytical, and technical support to the enterprise data management team. The Data Engineer leverages advanced knowledge on database technologies, operating systems, best practices, and industry trends including database design, configuration, tuning, and support.
Duties Include:
Build data pipelines:
Managed data pipelines consist of a series of stages through which data flows (for example, from data sources or endpoints of acquisition to integration to consumption for specific use cases). Data pipelines must be created, maintained, and optimized as workloads move from development to production for specific use cases. Architecting, creating, and maintaining data pipelines will be the primary responsibility of the data engineer.
Drive Automation through effective metadata management:
The Data Engineer will be responsible for using innovative and modern tools, techniques, and architectures to partially or completely automate the most-common, repeatable and tedious data preparation and integration tasks to minimize manual and error-prone processes and improve productivity. The Data Engineer will also need to assist with renovating the data management infrastructure to drive automation in data integration and management. This will include (but not be limited to):
Learning and using modern data preparation, integration and AI-enabled metadata management tools and techniques.
Tracking data consumption patterns.
Performing intelligent sampling and caching.
Monitoring schema changes.
Recommending/automating existing and future integration flows.
Collaborate across departments:
The newly hired Data Engineer will need strong collaboration skills to work with varied stakeholders within the organization. Work in close relationship with business (data) analysts in refining their data requirements for various data and analytics initiatives and their data consumption requirements.
Educate and train:
Data Engineer should be curious and knowledgeable about new data initiatives and how to address them. This includes applying their data and/or domain understanding in addressing new data requirements. This person will also be responsible for supporting data ingestion, preparation, integration, and operationalization techniques in optimally addressing these data requirements. The data engineer will be required to train counterparts in these data pipelining and preparation techniques, which make it easier for them to integrate and consume the data they need for their own use cases.
Participate in ensuring compliance and governance during data use:
It will be the responsibility of the Data Engineer to ensure that the data users and consumers use the data provisioned to them responsibly through data governance and compliance initiatives. Data engineer should work with data governance teams (and information stewards within these teams) and participate in vetting and promoting content created in the business and by data scientists to the curated data catalog for governed reuse.
Big Data and Analytics:
To handle and analyze enormous volumes of data, Data Engineer should have experience use big data technologies like Azure Databricks and Apache Spark to create data processing workflows and pipelines, to support data analytics, machine learning, and other data-driven applications.
Other duties as assigned.
Qualifications:
Bachelor’s Degree in Computer Science, Information Systems, or equivalent work experience; MBA preferred.
3+ years of experience in data management disciplines including data warehousing, data integration, modeling, optimization, and data quality, and/or other areas directly relevant to data engineering responsibilities and tasks in Microsoft Azure Platform.
Experience with implementation and support of Azure cloud services including but not limited to: Azure Data Factory, Azure Data Lake Store, Azure Data Lake Analytics, Azure Analysis Services, Azure SQL, Azure DW ,Azure Analytics Services and Synapse Analytics (Required)
Experience developing both multidimensional and tabular models with large and complex datasets.
2 years of experience working in cross-functional teams and collaborating with business stakeholders in support of departmental and/or multi-departmental data management and analytics initiatives.
Foundational knowledge of Data Management practices.
Strong experience with various Data Management architectures like Data Warehouse, Data Lake, Data Hub and the supporting processes like Data Integration, Governance, Metadata Management.
Strong ability to design, build and manage data pipelines for data structures encompassing data transformation, data models, schemas, metadata and workload management.
Strong experience in working with large, heterogeneous datasets in building and optimizing data pipelines, pipeline architectures and integrated datasets using traditional data integration technologies. These should include ETL/ELT, data replication/CDC, message-oriented data movement, API design and access and upcoming data ingestion and integration technologies such as stream data integration and data virtualization.
Basic experience in working with data governance/data quality and data security teams and specifically information stewards and privacy and security officers in moving data pipelines into production with appropriate data quality, governance and security standards and certification. Ability to build quick prototypes and to translate prototypes into data products and services in a diverse ecosystem –
Demonstrated success in working with large, heterogeneous datasets to extract business value using popular data preparation tools to reduce or even automate parts of the tedious data preparation tasks.
Strong experience with popular database programming languages including SQL, PL/SQL, others for relational databases.
Exposure to tools like Tools: Microsoft SQL Server Management Studio, SQL Server Analysis Services, SQL Server Integration Services
Strong Experience in use of big data technologies like Azure Databricks and Apache Spark
Strong experience with advanced analytics tools for Object-oriented/object function scripting using languages such as R, Python and others.
Strong experience in working with both open-source and commercial message queuing technologies such as Kafka, Azure Service Bus.
Ability to automate pipeline development –
Strong experience in working with DevOps capabilities like version control, automated builds, testing and release management capabilities using tools like Git.
Ability to collaborate with technical and business personas –
Demonstrated success in working with both IT and business while integrating analytics and data science output into business processes and workflows.
Experience working with popular data discovery, analytics, and BI software tools like Tableau, Qlik, PowerBI and others for semantic-layer-based data discovery.
Adept in agile methodologies and capable of applying DevOps and increasingly DataOps principles to data pipelines to improve the communication, integration, reuse, and automation of data flows between data managers and consumers across an organization.
Strong experience supporting and working with cross-functional teams in a dynamic business environment.
Required to be highly creative and collaborative. An ideal candidate would be expected to collaborate with both the business and IT teams to define the business problem, refine the requirements, and design and develop data deliverables accordingly. The successful candidate will also be required to have regular discussions with data consumers on optimally refining the data pipelines developed in nonproduction environments and deploying them in production.
Why Should You Apply?
Excellent benefits, including medical, dental, vision, 401k match, paid time off and 12 holidays.
Fast-paced and detail-oriented environment where you can have a direct impact on outcomes.
Workplace flexibility
Compensation
:
The salary will be determined based on the candidate's knowledge, skills, and abilities.
Electrical Components International is an Equal Employment Opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, or protected veteran status and will not be discriminated against on the basis of disability.
Show more
Show less","Azure Data Factory, Azure Data Lake Store, Azure Data Lake Analytics, Azure Analysis Services, Azure SQL, Azure DW, Azure Analytics Services, Synapse Analytics, Azure Databricks, Apache Spark, SQL, PL/SQL, R, Python, Kafka, Azure Service Bus, Git, Tableau, Qlik, PowerBI, DevOps, DataOps, Agile methodologies","azure data factory, azure data lake store, azure data lake analytics, azure analysis services, azure sql, azure dw, azure analytics services, synapse analytics, azure databricks, apache spark, sql, plsql, r, python, kafka, azure service bus, git, tableau, qlik, powerbi, devops, dataops, agile methodologies","agile methodologies, apache spark, azure analysis services, azure analytics services, azure data factory, azure data lake analytics, azure data lake store, azure databricks, azure dw, azure service bus, azure sql, dataops, devops, git, kafka, plsql, powerbi, python, qlik, r, sql, synapse analytics, tableau"
Data Center Engineer | Infrastructure Operations,Henry Ford Health,"Rochester Hills, MI",https://www.linkedin.com/jobs/view/data-center-engineer-infrastructure-operations-at-henry-ford-health-3776123670,2023-12-17,Windsor, Canada,Associate,Onsite,"General Summary
As a Data Center Engineer this position will be responsible for performing a wide range of complex technical work within the primary data center as well as smaller remote computer rooms, MDFs, and IDFs throughout the organization. He or she will follow standard procedures to perform duties related to data center cabling, server installation and decommission, and basic power/cooling configuration and maintenance. They will also perform routine data center maintenance including room and cabinet cleanings, server migration and consolidation. This position may include off shift hours, weekends, and holidays and some travel between Henry Ford Health locations.
Principle Duties And Responsibilities
Implement changes, moves, adds, decommissions, or updates in IT rooms
Process visitors to the Data Center to safeguard the integrity of data/equipment\
Review and approve changes to the environment following established Change Management standards and practices
Maintain inventory of spare computer parts and network cables\
Create/print cable labels for supporting IT teams
Create formal recommendations for process improvements
Writes, maintains, and publishes operational documentation
Other duties as required to maintain the Henry Ford Health Data Center complex
Education/Experience Required
High School Diploma or G.E.D. equivalent required.
Associates Degree (IT related field) preferred.
Five plus (5+) years of IT experience.
Extensive knowledge of all types of IT equipment preferred.
Cable management best practices including use of tracing devices preferred.
Equipment rack layout/design/assembly/configuration preferred.
Configure and code devices such as PDUs, CRACs, and ATS preferred.
IT equipment installation and decommission preferred.
Proficient in MS Office products including VISIO preferred.
Ability to occasionally work nights, weekends, and holidays.
Prepared to work at multiple Henry Ford Health facilities as needed.
Certifications/Licensures Required
ITIL certification preferred.
Additional Information
Organization: Corporate Services
Show more
Show less","Data Center Cabling, Server Installation, Power/Cooling Configuration, Server Migration, Server Consolidation, Room and Cabinet Cleaning, Change Management, Inventory Management, Cable Labeling, Process Improvement Recommendations, Operational Documentation, IT Equipment Knowledge, Cable Management, Equipment Rack Layout/Design/Assembly/Configuration, Device Configuration and Coding, MS Office Products, VISIO, ITIL Certification","data center cabling, server installation, powercooling configuration, server migration, server consolidation, room and cabinet cleaning, change management, inventory management, cable labeling, process improvement recommendations, operational documentation, it equipment knowledge, cable management, equipment rack layoutdesignassemblyconfiguration, device configuration and coding, ms office products, visio, itil certification","cable labeling, cable management, change management, data center cabling, device configuration and coding, equipment rack layoutdesignassemblyconfiguration, inventory management, it equipment knowledge, itil certification, ms office products, operational documentation, powercooling configuration, process improvement recommendations, room and cabinet cleaning, server consolidation, server installation, server migration, visio"
Data Analyst,NR Consulting,"Troy, MI",https://www.linkedin.com/jobs/view/data-analyst-at-nr-consulting-3768024112,2023-12-17,Windsor, Canada,Associate,Onsite,"Summary
Ability to analyze existing tools and databases. Demonstrated experience in handling large data sets. Strong numeric and problem-solving capabilities. Ability to multi-task projects in an effective and productive manner. Able to prioritize work by dividing time, attention and effort Must be able to communicate verbally and in technical writing to all levels of the organization in a proactive, contextually appropriate manner. Adjust positively to quickly changing priorities and shifting goals in a fast-paced environment. Good understanding of business process Required hands-on experience with Microsoft Excel Experience in of relational databases and proficiency in writing queries is highly desired (Oracle SQL, PL/SQL) Experience working with ETL tools. (Informatica experience is a plus). Salesforce experience is as an admin/developer is a plus. Experience with various Software Development Methodologies such as Agile, SCRUM, Waterfall, etc.
Show more
Show less","Data analysis, Data mining, Data visualization, SQL, PL/SQL, Informatica, Agile, SCRUM, Waterfall, Microsoft Excel","data analysis, data mining, data visualization, sql, plsql, informatica, agile, scrum, waterfall, microsoft excel","agile, data mining, dataanalytics, informatica, microsoft excel, plsql, scrum, sql, visualization, waterfall"
Jr Data Scientist,Epsilon,"Troy, MI",https://www.linkedin.com/jobs/view/jr-data-scientist-at-epsilon-3778864568,2023-12-17,Windsor, Canada,Associate,Onsite,"Job Description
Summary
The Analytic Consulting Group partners with both internal and external clients, and data providers, leveraging various analytics to drive strategic thought and effective decision making. The Data Analyst 2 is responsible for conducting data analyses using SQL and other tools in support of a variety of analytic solutions.
Roles & Responsibilities
Collaborate with internal/external stakeholders to manage data logistics – including data specifications, transfers, structures, and rules
Access and extract data from a variety of sources of all sizes (including client marketing databases) via Python, Excel, SQL, etc.
Master and perform all steps required to create analysis-ready data sets; including data integration/merging (SAS data step), variable preparation, and quality control (QA/QC)
Develop and execute SQL (or related) programs with detailed direction and supervision
Provide problem solving and data analysis, derived from programming experience
Demonstrate proficiency with desktop and UNIX toolsets (SAS, SAS ODS, SQL, MS Office) to create pivot tables and/or report content such as tables, reports, graphs, etc. (some positions require proficiency in digital analytic tools including Google and/or Adobe Analytics and familiarity with digital data, in addition to or in lieu of SAS/SQL)
Document and articulate steps taken in an analysis to project managers
Answer questions about data sets and analyses
Follow all policies and procedures for programming, project documentation, and system management
Become familiar with…
all offerings outlined in the Insider’s Guide to ACG
various statistical offerings and methods (CHAID, logistic/multiple regression, cluster analysis, factor analysis)
Epsilon data assets
the SAS macro library
Participate in the design, planning & execution of projects
Effectively manage time and resources in order to deliver on time / correctly on a limited number (1-4) of concurrent projects
Proactively communicate with supervisor regarding workload and the status of assignments
Prepare basic report content (Word, Excel, PowerPoint) in support of deliverables
Perform two tasks related to the role of Sr. Data Analyst during the year
Minimum Qualifications
Bachelor’s degree in a quantitative discipline (e.g., Statistics, Economics, Mathematics, Marketing Analytics) or significant relevant coursework
1-2 years of experience in the marketing analytics field
Some positions require a minimum of 1 year of experience conducting digital analytics
Demonstrated proficiency in SQL programming; minimum 2 years of experience
Strong analytic thought process and ability to interpret findings
Acute attention to detail (QA/QC)
Working knowledge of MS Office; including PowerPoint, Word, Excel and Outlook
Ability to work on multiple assignments concurrently
Excellent verbal and written communication skills
Highly motivated and collaborative team player with strong interpersonal skills
Effective organization and time management skills
Desirable Qualifications
Advanced degree (Master’s/PhD) in Statistics, Economics or other quantitative discipline
Database marketing experience/knowledge
Ability to program in newer and emerging languages such as R and Python; working knowledge of Hadoop and other big data technologies
Additional Information
About Epsilon
Epsilon is a global advertising and marketing technology company positioned at the center of Publicis Groupe. Epsilon accelerates clients’ ability to harness the power of their first-party data to activate campaigns across channels and devices, with an unparalleled ability to prove outcomes. The company’s industry-leading technology connects advertisers with consumers to drive performance while respecting and protecting consumer privacy. Epsilon’s people-based identity graph allows brands, agencies and publishers to reach real people, not cookies or devices, across the open web. For more information, visit epsilon.com.
When you’re one of us, you get to run with the best.
For decades, we’ve been helping marketers from the world’s top brands personalize experiences for millions of people with our cutting-edge technology, solutions and services. Epsilon’s best-in-class identity gives brands a clear, privacy-safe view of their customers, which they can use across our suite of digital media, messaging and loyalty solutions. We process 400+ billion consumer actions each day and hold many patents of proprietary technology, including real-time modeling languages and consumer privacy advancements. Thanks to the work of every employee, Epsilon has been consistently recognized as industry-leading by Forrester, Adweek and the MRC. Positioned at the core of Publicis Groupe, Epsilon is a global company with more than 8,000 employees around the world. Check out a few of these resources to learn more about what makes Epsilon so EPIC
Our Culture https //www.epsilon.com/us/about-us/our-culture-epsilon
Life at Epsilon https //www.epsilon.com/us/about-us/epic-blog
DE&I https //www.epsilon.com/us/about-us/diversity-equity-inclusion
CSR https //www.epsilon.com/us/about-us/corporate-social-responsibility
Great People Deserve Great Benefits
We know that we have some of the brightest and most talented associates in the world, and we believe in rewarding them accordingly. If you work here, expect competitive pay, comprehensive health coverage, and endless opportunities to advance your career.
Epsilon is an Equal Opportunity Employer.
Epsilon’s policy is not to discriminate against any applicant or employee based on actual or perceived race, age, sex or gender (including pregnancy), marital status, national origin, ancestry, citizenship status, mental or physical disability, religion, creed, color, sexual orientation, gender identity or expression (including transgender status), veteran status, genetic information, or any other characteristic protected by applicable federal, state or local law. Epsilon also prohibits harassment of applicants and employees based on any of these protected categories. Epsilon will provide accommodations to applicants needing accommodations to complete the application process.
REF218001X
Show more
Show less","Data Analytics, SQL, Python, Excel, SAS, SAS ODS, MS Office, PowerPoint, Word, Outlook, Hadoop, R","data analytics, sql, python, excel, sas, sas ods, ms office, powerpoint, word, outlook, hadoop, r","dataanalytics, excel, hadoop, ms office, outlook, powerpoint, python, r, sas, sas ods, sql, word"
Data Analyst III- Adobe Campaign Specialist,AAA Life Insurance Company,"Livonia, MI",https://www.linkedin.com/jobs/view/data-analyst-iii-adobe-campaign-specialist-at-aaa-life-insurance-company-3784639743,2023-12-17,Windsor, Canada,Associate,Hybrid,"By joining AAA Life, you will have the opportunity to strengthen the name and reputation of the brand that millions have come to rely upon for financial piece of mind. We are company dedicated to our members and our employees. We value the unique attributes and contributions of our associates to build an inclusive, collaborative and innovative workplace where all employees are engaged and feel they belong. Delivering our company’s promise to members is what drives each of our associates every day.
We offer a dynamic work environment, excellent benefits, and competitive compensation, that will allow you will exercise your potential to innovate, finding ways to increase efficiency and enhance our business processes.
Are you a Data Analyst with experience in Adobe Campaign? Our Data Analyst III ensures that AAA Life Insurance makes effective business and operational decisions. As a Data Analyst, you will be at the forefront of transforming data into actionable insights. This role involves extensive use of Adobe Campaign and will regularly extract, analyze, and visualize data to provide valuable recommendations that drive marketing strategies and decision-making.
Responsibilities
Use Adobe Campaign to pull together cross-channel customer data create and customize campaigns.
Set up new Adobe campaign workflows for mail and email
Lead & develop automated, easy to understand reports and ad hoc analyses to address specific marketing questions and provide insights to guide decision making.
Utilize statistical techniques and data analysis tools (i.e., Python, R, SQL) to gather, clean, analyze, and provide recommendations regarding large datasets from various sources, identifying trends, patterns, and key performance metrics.
Collaborate closely with marketing teams to interpret data and provide actionable insights to optimize marketing campaigns, customer segmentation, and overall strategy.
Develop and implement data governance and quality assurance processes.
Create and maintain Power BI reports and dashboards to translate complex data into clear visualizations that marketing staff can easily interpret and use to inform their strategies.
Lead and develop key performance indicators (KPIs), tracking marketing initiatives against established goals, and providing regular updates to stakeholders.
Conduct A/B tests and statistical analyses to evaluate the effectiveness of marketing strategies, making data-driven recommendations for improvements.
Collaborate with marketing teams to segment audiences effectively and personalize marketing approaches based on data-driven insights.
Qualifications
Bachelor in Statistics, Marketing, Economics, Computer Science, or related technical field. Master’s degree is a plus.
A minimum of five years’ experience working as a Data Analyst, Marketing Analyst, or similar role.
Expert in Adobe Campaign experience strongly preferred
Able to set up new Adobe campaign workflows for mail and email
strongly preferred
Understands architecture needed to support very large weekly campaigns
strongly needed
Extensive experience in data analysis tools and programming languages (i.e., Python, R, SQL).
Ability to create and interpret reports and dashboards using Power BI, Tableau, or similar data visualization tools.
Proficiency with marketing analytics tools and platforms (i.e., Google Analytics, Adobe Analytics).
Show more
Show less","Adobe Campaign, Python, R, SQL, Data analysis, Statistics, Power BI, Tableau, Google Analytics, Adobe Analytics, Data governance, Data quality assurance, A/B testing, Marketing analytics, Customer segmentation, Data visualization","adobe campaign, python, r, sql, data analysis, statistics, power bi, tableau, google analytics, adobe analytics, data governance, data quality assurance, ab testing, marketing analytics, customer segmentation, data visualization","ab testing, adobe analytics, adobe campaign, customer segmentation, data governance, data quality assurance, dataanalytics, google analytics, marketing analytics, powerbi, python, r, sql, statistics, tableau, visualization"
Senior Data Engineer (Remote),MMS,"Canton, MI",https://www.linkedin.com/jobs/view/senior-data-engineer-remote-at-mms-3782249669,2023-12-17,Windsor, Canada,Mid senior,Onsite,"MMS is an innovative, data focused CRO that supports the pharmaceutical, biotech, and medical device industries with a proven, scientific approach to complex trial data and regulatory submission challenges. Strong industry experience, technology-enabled services, and a data-driven approach to drug development make MMS a valuable CRO partner, creating compelling submissions that meet rigorous regulatory standards. With a global footprint across four continents, MMS maintains a 97 percent customer satisfaction rating, and the company has been recognized as a leading CRO inGlobal Health & Pharma’sinternational awards programs for the last three consecutive years. For more information, visit www.mmsholdings.com or follow MMS on LinkedIn.
Job Description:
Maintains a strong understanding of regulations and guidance as they pertain to data curation deliverables.
Strong understanding of new methods, tools and solutions to meet the data engineering needs of internal and external stakeholders and teams.
Mentors others and advises on MMS, industry trends and technologies to give the technical and non-technical stakeholders a better understanding of data science methodologies and results.
Maintains a strong understanding of Data Science department methodologies and standard practices.
Proficient in conducting peer reviews for others and validation of project deliverables within the team.
Proficient in developing and delivering training for internal and external stakeholders regarding data engineering processes and deliverables.
Strong understanding of CROs and/or Health Systems and the drug development process.
Proficient in developing requirements and specifications from analysis of business needs.
Proficient in preparing, correcting, modifying and analyzing data sets using complex analytic techniques.
Create reusable, highly parameterized pipelines using Microsoft Azure, driven by project-based configuration files to orchestrate landingdatain thedatalake as well as staging to SQLdatabases for analysis.
Applydatamodeling and architecture best practices to stage and transformdatato a commondatamodel. Incorporatedatawarehouse concepts to support dashboard reporting via star schemas and support auditing viadatalineage concepts.
Ability to write T-SQL stored procedures, master window functions, common table expression, and derived tables, utilize dynamic T-SQL, ability to optimize and tune queries and processes.
Thinks like a software developer. Always looking to refactor code, utilize patterns, think abstractly, and work in ways to encapsulate logic to reduce coding side effects.
Requirements:
College graduate in data engineering discipline or related field, or related experience.
Minimum of 7 years’ experience in data engineering or similar field required or an equivalent combination of education and experience.
Create reusable, highly parameterized Azure data factory pipelines, driven by project-based configuration files to orchestrate landing data in the data lake as well as staging to Microsoft SQL databases for analysis.
Apply data modeling and architecture best practices to stage and transform data to a common data model.Incorporate data warehouse concepts to support dashboard reporting via star schemas and support auditing via data lineage concepts.
Ability to write T-SQL stored procedures, master window functions, common table expression, and derived tables, utilize dynamic T-SQL, ability to optimize and tune queries and processes.
Thinks like a software developer.Always looking to refactor code, utilize patterns, think abstractly, and work in ways to encapsulate logic to reduce coding side effects.
Expert knowledge of data engineering concepts.
Reputation as emerging leader in field with sustained performance and accomplishment.
Hands-on experience with clinical trial and pharmaceutical development preferred.
Good communication skills and willingness to work with others to clearly understand needs and solve problems.
Excellent problem-solving skills.
Good organizational and communication skills.
Familiarity with data privacy and anonymization regulations preferred.
Familiarity with current ISO 9001 and ISO 27001 standards preferred.
Familiarity with 21 CFR Part 11, FDA, and GCP requirements.
Familiarity with industry standard data models (CDISC, FHIR, OMOP) preferred.
Basic understanding of CROs and scientific & clinical data/terminology, & the drug development process
Proficiency with MS Office applications.
Show more
Show less","Data Engineering, Data Science, Azure, SQL, TSQL, Data modeling, Data warehouse, Data lake, Data lineage, Clinical trials, Pharmaceutical development, CDISC, FHIR, OMOP, ISO 9001, ISO 27001, 21 CFR Part 11, FDA, GCP, Microsoft Office","data engineering, data science, azure, sql, tsql, data modeling, data warehouse, data lake, data lineage, clinical trials, pharmaceutical development, cdisc, fhir, omop, iso 9001, iso 27001, 21 cfr part 11, fda, gcp, microsoft office","21 cfr part 11, azure, cdisc, clinical trials, data engineering, data lake, data lineage, data science, datamodeling, datawarehouse, fda, fhir, gcp, iso 27001, iso 9001, microsoft office, omop, pharmaceutical development, sql, tsql"
Senior Data Engineer,Rocket Mortgage,"Detroit, MI",https://www.linkedin.com/jobs/view/senior-data-engineer-at-rocket-mortgage-3739107579,2023-12-17,Windsor, Canada,Mid senior,Onsite,"Rocket Mortgage
, backed by
Rocket Companies®
, means more opportunities for you to carve your own career path forward. From our desire to revolutionize the way people get mortgages to addressing challenges big or small with outside-the-box solutions, we’re not your typical employer. We’ll provide you with everything you need to make sure you’re successful here.
Apply today to join a team that offers career growth, amazing benefits and the chance to work with leading industry professionals.
Minimum Qualifications
Bachelor's degree in computer science, information technology, or a related field or equivalent experience
Preferred Qualifications
3 years of experience working with database tools
3 years of programming experience using Python
3 years of experience working with SQL server integration services or ETL tools
3 years of experience working with AWS
3 years of experience working with data integration tools
Proficiency in the Microsoft Office suite
Experience working with ETL tools
Knowledge of data integration tools
Knowledge of software programming languages, such as Python
Job Summary
As a Senior Data Engineer, you'll work with database engineers to design, develop and maintain the infrastructure of data within the data warehouse, including setting the ETL (extract, transform, load) processes, bringing in new data sources, modifying existing data sources, making sure data is clean, complete and consumable, as well as designing data models within the data warehouse. You'll work as part of one or more project teams and will be responsible for designing and building mechanisms to move, integrate, cleanse and publish large volume datasets. This is a developer role with a specialty in data and requires deep knowledge of a variety of programming languages and design patterns.
Responsibilities
Design and support the new and evolving sources of data being brought into the data warehouse
Work closely with data architects and follow best practices for data management consumption
Work closely with business analysts to work through business requirements and develop processes to provide the needed data visibility via the data warehouse and reporting platform
Assist with application layer and metadata design
Assist with the design and create automated applications and reporting solutions
Work closely with front-end developers to ensure data is being brought in and data integrity is being maintained
Monitor and troubleshoot performance issues on the data warehouse servers
Research and promote new tools and techniques to shape the future of the data environment
Mentor and train other data engineers
Benefits And Perks
Our team members fuel our strategy, innovation and growth, so we ensure the health and well-being of not just you, but your family, too! We go above and beyond to give you the support you need on an individual level and offer all sorts of ways to help you live your best life. We are proud to offer eligible team members perks and health benefits that will help you have peace of mind. Simply put: We’ve got your back. Check out our full list of Benefits and Perks.
Who We Are
Rocket Companies®
is a Detroit-based company made up of businesses that provide simple, fast and trusted digital solutions for complex transactions. The name comes from our flagship business, now known as Rocket Mortgage®, which was founded in 1985. Today, we’re a publicly traded company involved in many different industries, including mortgages, fintech, real estate and more. We’re insistently different in how we look at the world and are committed to an inclusive workplace where every voice is heard. We’re passionate about the work we do, and it shows. We’ve been ranked #1 for Fortune’s Best Large Workplaces in Financial Services and Insurance List in 2022, named #5 on People Magazine’s Companies That Care List in 2022 and recognized as #7 on Fortune’s list of the 100 Best Companies to Work For in 2022.
Disclaimer
This is an outline of the primary responsibilities of this position. As with everything in life, things change. The tasks and responsibilities can be changed, added to, removed, amended, deleted and modified at any time by the leadership group.
We are proud equal opportunity employers and committed to providing an inclusive environment based on mutual respect for all candidates and team members. Employment decisions, including hiring decisions, are not based on race, color, religion, national origin, sex, physical or mental disability, sexual orientation, gender identity or expression, age, military or veteran status or any other characteristic protected by state or federal law. We also provide reasonable accommodation to qualified individuals with disabilities in accordance with state and federal law.
Show more
Show less","Python, SQL, AWS, ETL, Data integration, Microsoft Office, Data warehouse, Data engineering, Cloud computing, Data management, Data architecture, Data analysis, Data mining, Data modeling, Data cleansing, Data governance, Data security, Big data","python, sql, aws, etl, data integration, microsoft office, data warehouse, data engineering, cloud computing, data management, data architecture, data analysis, data mining, data modeling, data cleansing, data governance, data security, big data","aws, big data, cloud computing, data architecture, data engineering, data governance, data integration, data management, data mining, data security, dataanalytics, datacleaning, datamodeling, datawarehouse, etl, microsoft office, python, sql"
Data Engineer General,Global Information Technology,"Dearborn, MI",https://www.linkedin.com/jobs/view/data-engineer-general-at-global-information-technology-3731581790,2023-12-17,Windsor, Canada,Mid senior,Onsite,"Job Title: Data Engineer General
Job Location: Dearborn, MI
Job Type: Contract
Job Description
Experience with Alteryx, Qlikview, SQL, R, or Python a plus.
Experience creating Data Models and Data products.
Experience working with data in Informatica.
Experience working with data in SAP - S4HANA Strong collaboration and influencing skills, and the ability to energize a multi-functional team.
Proven problem formulation with the ability to take complex problems and break them down to build and implement an action plan.
Ability to communicate findings to make data analysis meaningful and understandable by Data Operations team members, business partners, including IT and analytic teams.
Ability to optimally communicate information and ideas in written and verbal formats, including process documentation.
Experience developing data standards.
Experience working in Hadoop, particularly with HDFS and Hive.
Experience Required
3+ years of progressive responsibilities in managing data and data processes
Education Required
Bachelor's degree in Business, Finance, Computer Science, Engineering, Statistics, Economics or equivalent experience.
Interested candidates can send their updated resumes at jobs@global-itech.com
Job Posted by ApplicantPro
Show more
Show less","Alteryx, Qlikview, SQL, R, Python, Data Modelling, Data Products, Informatica, SAP  S4HANA, Hadoop, HDFS, Hive","alteryx, qlikview, sql, r, python, data modelling, data products, informatica, sap s4hana, hadoop, hdfs, hive","alteryx, data modelling, data products, hadoop, hdfs, hive, informatica, python, qlikview, r, sap s4hana, sql"
GCP Data Engineer,Insight Global,"Dearborn, MI",https://www.linkedin.com/jobs/view/gcp-data-engineer-at-insight-global-3778855986,2023-12-17,Windsor, Canada,Mid senior,Onsite,"Position:
Senior Data Engineer (Big Data/GCP)
Location:
Hybrid - Dearborn, MI (3 days/week)
Work Auth:
W2 (can provide sponsorship if needed)
Day to Day:
This position is ideal for someone who has experience doing all things data, including designing data storage solutions, implementing data storage solutions, optimizing data storage solutions, landing data, manipulating data, and providing L2/L3 support. In this role, you will collaborate with multiple organizations to convert business goals into data storage solutions. This team is currently migrating from Hadoop to Google Cloud Platform (GCP). This role will work in small, cross functional teams and embrace lean and agile practices, software best practices, software quality scanning, automated testing, and CI/CD.
Responsibilities Breakdown:
Identify, document, communicate and design per requirements.
Design data stores & Implement data stores (Hadoop, relational DBs, noSQL DBs, etc.).
Perform Extract, Transform and Load (ETL) or variations of this activity.
Create software to perform traditional Create, Read, Update, Delete (CRUD) transactions.
Tune data stores (indexes, SQL queries) to improve performance.
Assist with customer inquiries and incidents/problems.
Lead and support integration projects and discuss status with executives.
Technical Qualifications:
Prior Data Engineering experience in an enterprise environment
Experience migrating data to Google Cloud Platform (GCP)
Experience designing, implementing, and optimizing data storage solutions
Big Data & Data Manipulation experience: Hadoop Hive/HDFS & SQL
Experience with Python or Java
Familiar with Data Architecture, including Data Design, Index Design, Referential Integrity, etc.
Performance Tuning experience (SQL Tuning, database tuning, etc.)
Pluses:
Experience with Google Cloud Platform (GCP)
Show more
Show less","Data Engineering, Cloud Computing, Hadoop, Google Cloud Platform (GCP), Hive, HDFS, SQL, Python, Java, Data Architecture, Data Design, Index Design, Referential Integrity, Performance Tuning, ETL (Extract Transform Load), CRUD (Create Read Update Delete), CI/CD (Continuous Integration / Continuous Delivery)","data engineering, cloud computing, hadoop, google cloud platform gcp, hive, hdfs, sql, python, java, data architecture, data design, index design, referential integrity, performance tuning, etl extract transform load, crud create read update delete, cicd continuous integration continuous delivery","cicd continuous integration continuous delivery, cloud computing, crud create read update delete, data architecture, data design, data engineering, etl extract transform load, google cloud platform gcp, hadoop, hdfs, hive, index design, java, performance tuning, python, referential integrity, sql"
Sr. Data Developer,TechTammina LLC,"Troy, MI",https://www.linkedin.com/jobs/view/sr-data-developer-at-techtammina-llc-3773345457,2023-12-17,Windsor, Canada,Mid senior,Onsite,"Role: Sr. Data Developer
Work Location: Troy, MI (One site, hybrid from day one of the assignment, in office 2-3 days a week and remote rest)
Duration: Long term
Rate: Market
Equipment: Candidates will have to provide their own laptops
Requirements
5-10 years of experience required
Must be proficient in Oracle and SQL coding (with recent experience) - Minimum of 5 years.
Must be proficient in OLTP/3NF (Online transaction processing, 3rd normal form) as well as dimensional/data mart modeling (star schema, snowflake) - Minimum of 5 years.
Must be proficient in data analysis and SQL skills - Minimum of 5 years
Experience with SDLC / Scaled agile delivery frameworks for data warehouse projects minimum of 7 years.
PL/SQL programming of at least 2 years is preferred.
Good communication skills
Complete understanding of ETL tools (Data Stage preferred) and business intelligence reporting, with some exposure to Cognos and PowerBI.
Hands on experience in performance tuning Oracle database and SQL.
Job involves, profiling, creating, advising, and reviewing data models for our data warehouse atomic as well as data mart areas.
Should be able to provide design of the table structures with proper data analysis and mapping capabilities.
Will be responsible for changing, maintaining the data models and be able to work closely with the business team to understand the requirements and provide guidance on the design to the development team.
Should be able to assist both developers and testers to ensure that data processing is accurate, efficient, and complete. Should be able to prioritize work activities to meet the timelines.
Show more
Show less","Oracle, SQL, OLTP, 3NF, Data Modeling, ETL, Data Stage, Cognos, PowerBI, Performance Tuning, Data Structures, Data Analysis, Mapping, SDLC, Agile, PL/SQL","oracle, sql, oltp, 3nf, data modeling, etl, data stage, cognos, powerbi, performance tuning, data structures, data analysis, mapping, sdlc, agile, plsql","3nf, agile, cognos, data stage, data structures, dataanalytics, datamodeling, etl, mapping, oltp, oracle, performance tuning, plsql, powerbi, sdlc, sql"
Senior Data Engineer - Big Data / GCP,Insight Global,Detroit Metropolitan Area,https://www.linkedin.com/jobs/view/senior-data-engineer-big-data-gcp-at-insight-global-3774198109,2023-12-17,Windsor, Canada,Mid senior,Onsite,"Position:
Senior Data Engineer (Big Data/GCP)
Location:
Hybrid - Dearborn, MI (3 days/week)
Work Auth:
W2 (can provide sponsorship if needed)
Day to Day:
This position is ideal for someone who has experience doing all things data, including designing data storage solutions, implementing data storage solutions, optimizing data storage solutions, landing data, manipulating data, and providing L2/L3 support. In this role, you will collaborate with multiple organizations to convert business goals into data storage solutions. This team is currently migrating from Hadoop to Google Cloud Platform (GCP). This role will work in small, cross functional teams and embrace lean and agile practices, software best practices, software quality scanning, automated testing, and CI/CD.
Responsibilities Breakdown:
Identify, document, communicate and design per requirements.
Design data stores & Implement data stores (Hadoop, relational DBs, noSQL DBs, etc.).
Perform Extract, Transform and Load (ETL) or variations of this activity.
Create software to perform traditional Create, Read, Update, Delete (CRUD) transactions.
Tune data stores (indexes, SQL queries) to improve performance.
Assist with customer inquiries and incidents/problems.
Lead and support integration projects and discuss status with executives.
Technical Qualifications:
Prior Data Engineering experience in an enterprise environment
Experience migrating data to Google Cloud Platform (GCP)
Experience designing, implementing, and optimizing data storage solutions
Big Data & Data Manipulation experience: Hadoop Hive/HDFS & SQL
Experience with Python or Java
Familiar with Data Architecture, including Data Design, Index Design, Referential Integrity, etc.
Performance Tuning experience (SQL Tuning, database tuning, etc.)
Pluses:
Experience with Google Cloud Platform (GCP)
Show more
Show less","Data Engineering, GCP, Data Storage Solutions, Hadoop, Relational DBs, NoSQL DBs, ETL, CRUD, Python, Java, Data Architecture, Data Design, Index Design, Referential Integrity, Performance Tuning, SQL Tuning, Database Tuning","data engineering, gcp, data storage solutions, hadoop, relational dbs, nosql dbs, etl, crud, python, java, data architecture, data design, index design, referential integrity, performance tuning, sql tuning, database tuning","crud, data architecture, data design, data engineering, data storage solutions, database tuning, etl, gcp, hadoop, index design, java, nosql dbs, performance tuning, python, referential integrity, relational dbs, sql tuning"
Sr. Data Developer,A-Line Staffing Solutions,"Troy, MI",https://www.linkedin.com/jobs/view/sr-data-developer-at-a-line-staffing-solutions-3760212988,2023-12-17,Windsor, Canada,Mid senior,Onsite,"Title: Senior Data Developer
Location: Troy, MI (Hybrid, will consider remote for the right out of state candidate)
Primary Responsibilities
Research, architect, and develop innovative multi-tiered solutions using modern tools and methodologies with a goal for technical excellence
Work closely with other developers, architects, and stakeholders to provide estimates based on customer experience, features, and envisioned solutions
Ensure architecture and design of the solution is in alignment with overall enterprise architecture
Solve problems and proactively look for ways to improve our products and platform
Required Experience (must Have)
Expert SQL Server Development experience (Minimum 8 years’ experience)
Expert backend designer, capable of managing high-level architecture while adding or optimizing components
Adept at optimizing and refactoring and upgrading legacy systems
Excellent performance tuning skills regarding data models and SQL code
Expert data modeler with strong data analytic skills
Strong ETL experience, with emphasis on complex transformations and large data sets using SSIS or Talend
Relentless troubleshooter and investigator, proficient with SQL Profiler, Extended Events, Query Store etc.
Strong automated testing experience
Strong AWS Experience (Minimum one to two years’ experience)
AWS lambda functions (Python preferred)
AWS databases (Aurora, DynamoDB, or Redshift preferred)
AWS storage services (EC2, S3 preferred)
Technical evangelist who can educate and mentor other SQL developers
Excellent communicator -- articulate, persuasive, professional
Helpful Experience (nice To Have)
AWS Cloud Formation or Rapid solution prototyping
AWS Glue and Scala
AWS Lambda/Step Functions, State Machines, and Control Flow experience very helpful
Data Lake experience or similar (AWS preferred)
TFS, Azure DevOps, or Jira project management/requirements management
Git or TFVC version control
If you think this position is a good fit for you, please reach out to me - feel free e-mail me, or apply to this posting!
Andrew Torchine
Atorchine@alinestaffing.com
Show more
Show less","SQL Server, Backend design, Data modeling, Data analytics, AWS, Python, Aurora, DynamoDB, Redshift, EC2, S3, AWS Cloud Formation, Rapid solution prototyping, AWS Glue, Scala, AWS Lambda, Step Functions, State Machines, Control Flow, Data Lake, TFS, Azure DevOps, Jira, Git, TFVC","sql server, backend design, data modeling, data analytics, aws, python, aurora, dynamodb, redshift, ec2, s3, aws cloud formation, rapid solution prototyping, aws glue, scala, aws lambda, step functions, state machines, control flow, data lake, tfs, azure devops, jira, git, tfvc","aurora, aws, aws cloud formation, aws glue, aws lambda, azure devops, backend design, control flow, data lake, dataanalytics, datamodeling, dynamodb, ec2, git, jira, python, rapid solution prototyping, redshift, s3, scala, sql server, state machines, step functions, tfs, tfvc"
Sr. ETL Data Developer,TechTammina LLC,"Troy, MI",https://www.linkedin.com/jobs/view/sr-etl-data-developer-at-techtammina-llc-3667463150,2023-12-17,Windsor, Canada,Mid senior,Onsite,"Hi,
This is
Nikhil
from
Tech Tammina
, please help me with your updated resume ASAP
Job Title:
Sr. ETL Data Developer
Rate:
Market (Both C2C & W2)
Location:
Troy, MI (Remote Work for EST candidates, but preference will be given to MI local candidates who can work Hybrid)
Visa:
USC or GC Only
Sr. Data Developer
Our Informatica ETL Developer is responsible for developing, enhancing, debugging, maintaining, documenting, and testing software applications that will support various business units using IICS (Informatica Intelligent Cloud Services).
Informatica ETL Developer Responsibilities
Create Informatica mappings with Oracle database objects to incorporate critical business functionality to load the data into the Salesforce application.
Create/Use connections to retrieve data from various sources.
Work with relational databases like Oracle to understand the data and be able to analyze/troubleshoot data issues.
Actively participate in all phases of SDLC from requirements gathering, architecture design, development, testing, and data migration.
Provide documented operation and user procedures, when required, and perform supporting training for respective users.
Provide data flow diagrams describing the solution, when required.
Document implementation instructions for production and lower-environment deployments.
Run jobs and facilitate procurement of test data, when needed, in order to test code to satisfaction.
Be available for occasional off-hours (Eastern time zone) production deployments in order to support dev ops team if questions/issues arise with ETL code.
Import/export objects using IICS and knowing the impacts of all of the options.
Troubleshoot and analyze error logs and job results that do not meet the requirements. Communicate effectively and in a timely, respectful, professional manner with all teammates.
Required Experience
3-5 years of ETL experience using Informatica IICS
3-5 years practical experience with SQL (standard query language) on relational database Effectively communicate in English, both written and spoken language
Preferred Experience
Using Salesforce Developer Console with SOQL (Salesforce Object Query Language)
Maintaining production systems
Data modeling
Creating and maintaining business reports
Intermediate-level expertise in Microsoft Excel (vlookups, pivot tables, etc)
Running Salesforce DataLoader
Promoting code between environments
Working with MSDOS batch files as pre- and post-conditions for IICS jobs
IICS server configurations
Analyzing and retrieving full production error logs from server
Experience Level
5-10 years
Thanks & Regards,
Nikhil Kanchi
Sr Technical Recruiter
Mobile: (832) 862-7261| Direct:(703) 349-1053
Email: nikhil.kanchi@tammina.com
4460 Brookfield Corporate Dr,Suite N,Chantilly,VA 20151
www.TechTammina.com
Web Solutions | Mobility Solutions
MBE Certified | ISO 9001:2008 Certified
Show more
Show less","Informatica, Oracle, Salesforce, SQL, SOQL, Microsoft Excel, MSDOS, ETL, Data modeling, Data migration, IICS, Data flow diagrams, SDLC, Pivot tables, Vlookups","informatica, oracle, salesforce, sql, soql, microsoft excel, msdos, etl, data modeling, data migration, iics, data flow diagrams, sdlc, pivot tables, vlookups","data flow diagrams, data migration, datamodeling, etl, iics, informatica, microsoft excel, msdos, oracle, pivot tables, salesforce, sdlc, soql, sql, vlookups"
Data Analyst,CyberCoders,"Farmington, MI",https://www.linkedin.com/jobs/view/data-analyst-at-cybercoders-3782478929,2023-12-17,Windsor, Canada,Mid senior,Onsite,"If you are a Data Analyst with experience, please read on!
What You Need for this Position
Data Analysis
Tableau
R
Data Modeling
Excel
So, if you are a Data Analyst with experience, please apply today!
Email Your Resume In Word To
Looking forward to receiving your resume through our website and going over the position with you. Clicking apply is the best way to apply, but you may also:
Abby.Ramcharan@CyberCoders.com
Please do NOT change the email subject line in any way. You must keep the JobID: linkedin : AK9-1777996 -- in the email subject line for your application to be considered.***
Abby Ramcharan - Executive Recruiter - CyberCoders
Applicants must be authorized to work in the U.S.
CyberCoders is proud to be an Equal Opportunity Employer
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, protected veteran status, or any other characteristic protected by law.
Your Right to Work
– In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire.
CyberCoders will consider for Employment in the City of Los Angeles qualified Applicants with Criminal Histories in a manner consistent with the requirements of the Los Angeles Fair Chance Initiative for Hiring (Ban the Box) Ordinance.
Show more
Show less","Data Analysis, Tableau, R Programming, Data Modeling, Microsoft Excel","data analysis, tableau, r programming, data modeling, microsoft excel","dataanalytics, datamodeling, microsoft excel, r programming, tableau"
Senior Data Engineer,Valorem Reply,"Detroit, MI",https://www.linkedin.com/jobs/view/senior-data-engineer-at-valorem-reply-3754704346,2023-12-17,Windsor, Canada,Mid senior,Onsite,"Note
This role requires candidates to be US Citizens.
This role requires individuals to be within driving distance from our office locations or willing to relocate (Atlanta, Chicago, Detroit, Kansas City and Philadelphia).
Valorem Reply is an award-winning digital transformation firm focused on delivering data-driven enterprise, IT modernization, customer experience, product transformation and digital workplace. Through the expertise of their people and power of Microsoft technologies, they provide hyper-scale and agile delivery of unique digital business services, strategic business models and design-led user experiences. Their innovative strategies and solutions securely and rapidly transform the way their clients do business.
The Senior Data Engineer will lead the creation of high-value data-driven solutions, leveraging Valorem Reply's proven implementation methodology and solutions for enterprise projects. They will also contribute to technical pre-sales activities as required. The responsibilities include designing solution architecture, defining requirements, and leading the project delivery team. There will be an opportunity to work with and learn about the latest cloud solutions in an exciting work environment. This position will work collaboratively across all of Valorem Reply's sales, service delivery, and account management organizations to serve Valorem Reply's customers.
This position will represent Valorem Reply's approach to advanced data engineering solutions and, as such, must demonstrate proficiency at the architecture level. It will require an understanding of how advanced analytics are positioned to meet business objectives and how data translates to business and enterprise value. The role will also involve implementing the Data Lakehouse solution through people, processes, and technology. This is a hands-on role, leading, coding, and delivering on the most advanced cloud data analytics platforms available. Projects will span from workshops to full enterprise production end-to-end solutions.
The ideal candidate will have extensive experience with Microsoft/Azure data services and Databricks technology. Proficiency with the Databricks platform and the implementation of enterprise Data Lakehouse solutions will be required. Candidates will be expected to contribute to all stages of the data lifecycle, including data ingestion, data modeling, data profiling, data quality, data transformation, data movement, and data curation. The candidate should be familiar with market challenges in multiple industry verticals and have experience with both traditional and modern technologies across the Microsoft technology stack.
Responsibilities
Leading the development of data-driven solutions using Valorem Reply's methodology and enterprise project solutions.
Designing solution architecture and defining project requirements.
Staying up to date with the latest cloud solutions and technologies.
Collaborating across different teams to provide exceptional service to customers.
Demonstrating expertise in data engineering and understanding how it aligns with business objectives.
Managing the entire data lifecycle, from data ingestion to curation, and proficiency in Microsoft/Azure data services and Databricks technology.
Minimum Requirements
Bachelor's/master’s degree in computer science or equivalent with a focus on Azure data engineering solutions
6+ years of data engineering delivery experience
3+ years of Databricks engineering development experience
2+ years of technical team leadership or technical management experience
Candidates must be US Citizens
Show more
Show less","Data engineering, Microsoft Azure, Databricks, Data Lakehouse, Data ingestion, Data modeling, Data profiling, Data quality, Data transformation, Data movement, Data curation, Solution architecture, Project requirements, Cloud solutions, Handson, Coding, Consulting, Customer service, Technical leadership, Technical management","data engineering, microsoft azure, databricks, data lakehouse, data ingestion, data modeling, data profiling, data quality, data transformation, data movement, data curation, solution architecture, project requirements, cloud solutions, handson, coding, consulting, customer service, technical leadership, technical management","cloud solutions, coding, consulting, customer service, data curation, data engineering, data ingestion, data lakehouse, data movement, data profiling, data quality, data transformation, databricks, datamodeling, handson, microsoft azure, project requirements, solution architecture, technical leadership, technical management"
Data Analyst Lead,Dynamic Map Platform North America,"Livonia, MI",https://www.linkedin.com/jobs/view/data-analyst-lead-at-dynamic-map-platform-north-america-3787905060,2023-12-17,Windsor, Canada,Mid senior,Onsite,"Main Duties and Responsibilities
Develop and Maintain Production KPI metrics Dashboard – SQL Queries, areas of interest, KPI’s, PowerBI dash and standards.
Identify quality improvements, areas of interest, write queries to find known issues, create quality dashboard for WSE’s and backlogs.
Production Support - Support queries to identify areas of work to be progressed.
Standard Work Support – support visualization and process improvement on tickets
New business development – processing questions and data needed to support program and sales for RFI / RFQ
Advanced Development – support query generation for shape files, new business PoC etc.
Skills, Qualifications, And Competencies
Bachelor’s Degree in Engineering or appropriate qualification
Experience With Power Bi or similar tool and Python.
Experience with relational database principles including structured query language (SQL).
Experience with GIS, geography, planning, computer science, information technology a plus.
Work Environment
Works in a fast-paced office environment with multiple priorities and competing demands; potential set-backs in project completion due to internal or external issues, resourcing and re-allocation.
Regular office hours with some requirements for additional work during busy times.
May work for long periods reviewing data on computer.
Disclaimer
This job description is not designed to be a complete list of all activities required to be successful in the above position. Ushr retains the right to change or assign other activities to this position.
Powered by JazzHR
yd1VNj7vAT
Show more
Show less","* SQL, * Power BI, * Python, * Relational database, * Structured Query Language (SQL), * GIS, * Geography, * Planning, * Computer science, * Information Technology","sql, power bi, python, relational database, structured query language sql, gis, geography, planning, computer science, information technology","computer science, geography, gis, information technology, planning, powerbi, python, relational database, sql, structured query language sql"
Data Engineer (GCP),Stefanini North America and APAC,"Dearborn, MI",https://www.linkedin.com/jobs/view/data-engineer-gcp-at-stefanini-north-america-and-apac-3778941892,2023-12-17,Windsor, Canada,Mid senior,Remote,"Stefanini Group is hiring!
Stefanini is looking for Data Engineer GCP at Location: Remote
For quick Apply, please reach out to Utkarsh Dutt at 248 263 3997 / Email: utkarsh.dutt@stefanini.com
Open to W2 candidates only!
$5,000 - Sign On and Retention Bonus Offered - Must be willing to take technical assessment within 24 hours of your application.
Position Description:
The successful candidate will be responsible for designing the transformation and modernization of big data solutions on GCP cloud integrating native GCP services and 3rd party data technologies
We are looking for candidates who have a broad set of technology skills across these areas and who can demonstrate an ability to design right solutions with appropriate combination of GCP and 3rd party technologies for deploying on GCP cloud.
Key Responsibilities:
Work as part of an implementation team from concept to operations, providing deep technical subject matter expertise for successfully deployment of Data Platform Implement methods for automation of all parts of the pipeline to minimize labor in development and production.
Identify, develop, evaluate, and summarize Proof of Concepts to prove out solutions
Test and compare competing solutions and report out a point of view on the best solution
Experience with large scale solutioning and operationalization of data warehouses, data lakes and analytics platforms on GCP
Design and build production data engineering solutions to deliver our pipeline patterns using Google Cloud Platform
(GCP) Services: o BigQuery, DataFlow (Apache Beam), Pub/Sub, BigTable, Data Fusion, DataProc, Cloud Composer (Apache Airflow), Cloud SQL, Compute Engine, Cloud Functions, and App Engine • Migrate existing Big Data pipelines into Google Cloud Platform
Skills Required:
Bachelors or masters in required field.
Minimum 3 Years of Experience in Java/python in-depth
Minimum 2 Years of Experience in data engineering pipelines/ building data warehouse systems with ability to understand ETL principles and write complex sql queries.
Minimum 5 Years of GCP experience working in GCP based Big Data deployments (Batch/Real-Time) leveraging Big Query, Big Table, Google Cloud Storage, PubSub, Data Fusion, Dataflow, Dataproc
Minimum 2 years of experience in development using Data warehousing, Big Data Eco System Hive (Hql) & Oozie Scheduler, ETL IBM Data Stage, Informatica IICS with Teradata
1 Year experience of deploying google cloud services using Terraform.
Preferred:
Understands Cloud as being a way to operate and not a place to host systems.
Understands data architectures and design independent of the technology Experience with Python, Shell Script preferred
Exceptional problem solving and communication skills and management of multiple stakeholders.
Experience in working with Agile and Lean methodologies.
Experience with Test-Driven Development
***Listed salary ranges may vary based on experience, qualifications, and local market. Also, some positions may include bonuses or other incentives***
Stefanini takes pride in hiring top talent and developing relationships with our future employees. Our talent acquisition teams will never make an offer of employment without having a phone conversation with you. Those face-to-face conversations will involve a description of the job for which you have applied. We also speak with you about the process including interviews and job offers.
About Stefanini Group
The Stefanini Group is a global provider of offshore, onshore and near shore outsourcing, IT digital consulting, systems integration, application, and strategic staffing services to Fortune 1000 enterprises around the world. Our presence is in countries like the Americas, Europe, Africa, and Asia, and more than four hundred clients across a broad spectrum of markets, including financial services, manufacturing, telecommunications, chemical services, technology, public sector, and utilities. Stefanini is a CMM level 5, IT consulting company with a global presence. We are CMM Level 5 company.
Show more
Show less","Java, Python, Apache Beam, BigQuery, DataFlow, Pub/Sub, BigTable, Data Fusion, DataProc, Cloud Composer (Apache Airflow), Cloud SQL, Compute Engine, Cloud Functions, App Engine, ETL, Hive (Hql), Oozie Scheduler, IBM Data Stage, Informatica IICS, Teradata, Terraform, Agile, Lean, TestDriven Development","java, python, apache beam, bigquery, dataflow, pubsub, bigtable, data fusion, dataproc, cloud composer apache airflow, cloud sql, compute engine, cloud functions, app engine, etl, hive hql, oozie scheduler, ibm data stage, informatica iics, teradata, terraform, agile, lean, testdriven development","agile, apache beam, app engine, bigquery, bigtable, cloud composer apache airflow, cloud functions, cloud sql, compute engine, data fusion, dataflow, dataproc, etl, hive hql, ibm data stage, informatica iics, java, lean, oozie scheduler, pubsub, python, teradata, terraform, testdriven development"
Data Engineer with GCP,"Systems Technology Group, Inc. (STG)","Dearborn, MI",https://www.linkedin.com/jobs/view/data-engineer-with-gcp-at-systems-technology-group-inc-stg-3784590460,2023-12-17,Windsor, Canada,Mid senior,Remote,"Description: STG is a fast-growing Digital Transformation services company providing Fortune 500 companies with Digital Transformation, Mobility, Analytics and Cloud Integration services in both information technology and engineering product lines. STG has a 98% repeat business rate from existing clients and have achieved industry awards and recognition for our services. Crain’s Detroit Business named STG to Michigan’s Fastest Growing Companies list in both 2020 and 2019, Top IT Services Company’s List in 2020, 2019, 2018, and Top Minority Business Enterprise List in 2020, 2019, and 2019.
STG puts company CULTURE at the forefront of every business decision and employees are EMPOWERED and MEASURED for RESULTS. Both TEAMWORK and INDIVIDUAL Performance is recognized and rewarded.
Looking for Data Engineers with GCP exp. No C2C
Position Description:
The GDIA Data Factory Platform covers all business processes and technical components involved in ingesting a wide range of enterprise data into the GDIA Data Factory (Data Lake) and the transformation of that data into consumable data sets in support of analytics.
The Data Factory Enablement Team, as the name suggests enables teams build their solutions in the GCP Data Factory Platform by proving Tools, Guidelines, processes and support.
We are looking for candidates who have a broad set of technology skills across areas and come from a background of DevOps, with exposure to infrastructure and solution monitoring. This person will be expected to provide consultative services to the Software Development and Database Engineering teams.
Skills Required:
· Someone who understands Cloud as being a way to operate and not a place to host systems.
· In-depth understanding of GCP product technology and underlying architectures.
· Experience and very strong with development eco-system such as Git, Jenkins, Terraform and Tekton for CI/CD
· Experience in working with Agile and Lean methodologies.
Experience Required:
· At least 2 years of tekton experience
· At least 5 years of Terraform experience or 3 years with terraform certification.
· At least 3 years of experience in Google cloud and Google cloud professional architect certification
Education Required:
Bachelor’s degree in Computer Science, Information Systems or equivalent work experience
Resume Submittal Instructions: Interested/qualified candidates should email their word formatted resumes to Vasudha – vasudha.lakshminarasimhan(@)stgit.com and/or contact @(Two-Four-Eight) Seven- One-Two – Six-Seven-one-Seven (@248.712.6717). In the subject line of the email please include: First and Last Name - Data Engineer
Interested people please share resume to my email: vasudha.lakshminarasimhan@stgit.com
For more information about STG, please visit us at www.stgit.com
Show more
Show less","GCP, Terraform, Jenkins, SQL, Python, DevOps, Data Factory Platform, Google Cloud Platform, Kubernetes, CI/CD, Git, Lean, Tekton, Agile, Cloud Computing, Data Engineering, Big Data","gcp, terraform, jenkins, sql, python, devops, data factory platform, google cloud platform, kubernetes, cicd, git, lean, tekton, agile, cloud computing, data engineering, big data","agile, big data, cicd, cloud computing, data engineering, data factory platform, devops, gcp, git, google cloud platform, jenkins, kubernetes, lean, python, sql, tekton, terraform"
IT Data Warehousing Analyst - Remote,Get It Recruit - Information Technology,"Detroit, MI",https://www.linkedin.com/jobs/view/it-data-warehousing-analyst-remote-at-get-it-recruit-information-technology-3775427070,2023-12-17,Windsor, Canada,Mid senior,Remote,"We are seeking a highly motivated and skilled individual to join our dynamic team as a Data Warehousing Analyst. In this role, you will play a crucial part in collecting, analyzing, and summarizing data to support key business decisions. Your contributions will be pivotal in identifying opportunities for data reuse across the enterprise, ensuring data integrity, and collaborating with diverse stakeholders to drive innovative solutions.
Key Responsibilities
Data Analysis and Validation:
Identify opportunities for data reuse across the enterprise and validate data sources.
Stay informed about new tools, technologies, and functionalities, contributing to the ongoing enhancement of applications.
Collaboration And Communication
Work closely with customers, vendors, business analysts, and application development teams to resolve information flow and content issues.
Facilitate meetings with clients to gather and document requirements, exploring potential solutions.
Requirements Gathering And Documentation
Gather data and reporting requirements, summarizing patterns and findings through simple reports.
Analyze and document clients' business requirements and processes, constructing conceptual data and process models.
Team Collaboration And Support
Collaborate with work teams, departments, system leadership, staff, and technology vendors to define needs and facilitate solutions.
Provide support and escalation response to production problems, performing root cause analysis and application resolution.
Key Must-Have's
Bachelor's degree required.
5+ years of applicable work experience; 3 years in a Healthcare setting preferred.
Technical experience in state-of-the-art software engineering approaches.
Strong proficiency in SQL and Data Integration.
Experience in the healthcare sector, Azure/cloud experience.
Good understanding of HIPAA data Privacy and Security requirements.
Familiarity with the System Development Life Cycle (SDLC).
Vital Tech Solutions is an Equal Opportunity Affirmative Action employer. We are committed to creating a diverse and inclusive workplace and prohibit discrimination in all aspects of employment.
To apply, please submit your resume along with a cover letter detailing your relevant experience and how you can contribute to our team.
Remote work options are available.
Employment Type: Full-Time
Show more
Show less","Data Warehousing, Data Analysis, Data Validation, Data Reuse, Data Integrity, SQL, Data Integration, Azure, Cloud Computing, HIPAA, System Development Life Cycle (SDLC), Healthcare, Data Privacy, Data Security, Business Intelligence, Data Modeling, Data Visualization, Data Reporting, Data Mining, Data Extraction, Data Transformation, Data Cleaning, Data Warehousing, Data Lakes, Data Governance, Data Quality, Data Security, Data Privacy, Business Intelligence Tools, Data Visualization Tools, Data Mining Tools, Data Extraction Tools, Data Transformation Tools, Data Cleaning Tools, Data Warehousing Tools, Data Lakes Tools, Data Governance Tools, Data Quality Tools, Data Security Tools, Data Privacy Tools","data warehousing, data analysis, data validation, data reuse, data integrity, sql, data integration, azure, cloud computing, hipaa, system development life cycle sdlc, healthcare, data privacy, data security, business intelligence, data modeling, data visualization, data reporting, data mining, data extraction, data transformation, data cleaning, data warehousing, data lakes, data governance, data quality, data security, data privacy, business intelligence tools, data visualization tools, data mining tools, data extraction tools, data transformation tools, data cleaning tools, data warehousing tools, data lakes tools, data governance tools, data quality tools, data security tools, data privacy tools","azure, business intelligence, business intelligence tools, cloud computing, data cleaning, data cleaning tools, data extraction, data extraction tools, data governance, data governance tools, data integration, data integrity, data lakes, data lakes tools, data mining, data mining tools, data privacy, data privacy tools, data quality, data quality tools, data reporting, data reuse, data security, data security tools, data transformation, data transformation tools, data validation, data visualization tools, data warehousing tools, dataanalytics, datamodeling, datawarehouse, healthcare, hipaa, sql, system development life cycle sdlc, visualization"
Data Engineer General,A2Zxperts,"Dearborn, MI",https://www.linkedin.com/jobs/view/data-engineer-general-at-a2zxperts-3683851986,2023-12-17,Windsor, Canada,Mid senior,Hybrid,"Our client in the automotive industry is looking for a Data Engineer General for Dearborn, MI location for a 1-year project.
Title: Data Engineer General
Location: Dearborn, MI, US
Type: Temp Sourcing
Degree: Bachelor's
What are the 3-4 non-negotiable requirements of this position?
Skills Required
Critical thinking and decision making DA/DBA - Data Architecture (Data Design, Index Design, Referential Integrity) DA/DBA - DB Create/Modify (DB Create/Modify tables, DDL export/import, Data export/import) DA/DBA - Performance Tuning (DB-side identification of issues, SQL tuning, DB tuning) Data Manipulation SQL, Hadoop Hive/HDFS, GCP Strong communication both verbal and written
Experience Required
2-5 Years of experience in the field
Education Required
BA/BS in Computer Science or related field, or equivalent experience
For more details, feel free to reach out at kamal.d@a2zxperts.com
Thanks,
KAMAL DHAMIJA
Talent Management Specialist
A2Zxperts | Pioneers in Virtual Consulting
Ph#:
(630) 300-3940 (9 am to 2 pm), (815) 214-9020 (2 pm to 6 pm)
WhatsApp Me:
+1 630 300 3940
Messenger:
m.me/kadhamija.a2z
Kamal.D@A2Zxperts.com www.A2Zxperts.com
Show more
Show less","Data Architecture, Database Design, Index Design, Referential Integrity, Database Creation and Modification, DDL Export and Import, Data Export and Import, Database Performance Tuning, SQL, Hadoop Hive, HDFS, GCP, Communication, Computer Science","data architecture, database design, index design, referential integrity, database creation and modification, ddl export and import, data export and import, database performance tuning, sql, hadoop hive, hdfs, gcp, communication, computer science","communication, computer science, data architecture, data export and import, database creation and modification, database design, database performance tuning, ddl export and import, gcp, hadoop hive, hdfs, index design, referential integrity, sql"
Data Engineer General,A2Zxperts,"Dearborn, MI",https://www.linkedin.com/jobs/view/data-engineer-general-at-a2zxperts-3617896201,2023-12-17,Windsor, Canada,Mid senior,Hybrid,"Our client in the automotive industry is looking for a Data Engineer General for Dearborn, MI location for a 1-year project.
Title: Data Engineer General
Location: Dearborn, MI, US
Type: Temp Sourcing
Degree: Bachelor's
Skills Required
Project Management - Hadoop/Hive/HDFS - RDBMS - SQL - ETL
Experience Required
3+ Years of Project Management experience 5+ years of experience with Data Ingestion, Data analysis, Data transformation, and Data cleansing. 5+ years of experience with Python, PySpark, SQL, Hadoop, Hive, HDFS, RDBMS.
Experience Preferred
5+ year of experience in managing scalable data pipelines that includes batch and real-time streaming process
Critical thinking and proactive decision making
Strong communication both verbal and written
Prior experience coordinating ongoing landing of data
Strong collaboration and influencing skills, and the ability to energize a multi-functional team
Knowledge of GCP: Big Query, Cloud Storage, PubSub etc.
Experience in with Informatica Big Data Management, Enterprise Data Catalog, Enterprise Data Preparation
For more details, feel free to reach out at anita.d@a2zxperts.com
Show more
Show less","Project Management, Hadoop, Hive, HDFS, RDBMS, SQL, ETL, Python, PySpark, GCP: Big Query Cloud Storage PubSub, Informatica Big Data Management, Enterprise Data Catalog, Enterprise Data Preparation","project management, hadoop, hive, hdfs, rdbms, sql, etl, python, pyspark, gcp big query cloud storage pubsub, informatica big data management, enterprise data catalog, enterprise data preparation","enterprise data catalog, enterprise data preparation, etl, gcp big query cloud storage pubsub, hadoop, hdfs, hive, informatica big data management, project management, python, rdbms, spark, sql"
Data Engineer General,A2Zxperts,"Dearborn, MI",https://www.linkedin.com/jobs/view/data-engineer-general-at-a2zxperts-3683858096,2023-12-17,Windsor, Canada,Mid senior,Hybrid,"Our client in the automotive industry is looking for a Data Engineer General for Dearborn, MI location for a 1- year project.
Title: Data Engineer General
Location: Dearborn, MI, US
Type: Temp Sourcing
Degree: Bachelor's
Skills Required
Someone who understands Cloud as being a way to operate and not a place to host systems...
In-depth understanding of GCP product technology and underlying architectures.
Experience with development eco-system such as Git, Jenkins, Terraform and Tekton for CI/CD
Experience in working with Agile and Lean methodologies
Experience Required
Google Cloud Platform (GCP) Certification preferred. -- 5+ years of application development ' Dev Ops experience required, -- 2+ years of GCP experience
For more details, feel free to reach out at Ishan.b@a2zxperts.com
Show more
Show less","Cloud Computing, GCP, Git, Jenkins, Terraform, Tekton, CI/CD, Agile, Lean, GCP Certification, Dev Ops, Application Development","cloud computing, gcp, git, jenkins, terraform, tekton, cicd, agile, lean, gcp certification, dev ops, application development","agile, application development, cicd, cloud computing, dev ops, gcp, gcp certification, git, jenkins, lean, tekton, terraform"
Sr. Data Developer,A-Line Staffing Solutions,"Troy, MI",https://www.linkedin.com/jobs/view/sr-data-developer-at-a-line-staffing-solutions-3683570287,2023-12-17,Windsor, Canada,Mid senior,Hybrid,"ur client is actively seeking a Senior Data Developer. We are looking for someone who can help us acquire, ingest, store, organize, transform, map, relate, process, extract, and package this data using efficient, optimized, state-of-industry tools and methods to help us deliver this content to our customers.
The Data team is undergoing growth and is looking for someone to share our mission to transform our company’s future. With this position, you will have a rare opportunity to use your talents, passions, and expertise to help drive significant change in how we build, organize, and optimize our backend systems and data processes.
This position offers excellent career growth and promotional opportunities, stellar compensation, and an opportunity to work with the world's a world leader in data services.
Required Experience (must Have)
Expert SQL Server Development experience (Minimum 8 years’ experience), in versions MS SQL 2000 through MS SQL 2019.
Expert backend designer, capable of managing high-level architecture while adding or optimizing components
Adept at optimizing and refactoring and upgrading legacy systems
Excellent performance tuning skills regarding data models and SQL code
Good data modeler with data analytic skills
Strong ETL experience, with emphasis on complex transformations and large data sets
SSIS, Talend, or Kettle preferred
Relentless troubleshooter and investigator, proficient with SQL Profiler, Extended Events, Query Store etc.
Strong automated testing experience
Octopus Deploy, Red Gate SQL Change Automation, Red Gate SQL Toolbelt, or equivalent CI/CD
AWS Experience (Minimum one to two years’ experience)
AWS lambda functions (Python preferred)
AWS databases (Aurora, DynamoDB, or Redshift preferred)
AWS storage services (EC2, S3 preferred)
Quick learner: astute and perceptive; keeps up to date on new and emerging technologies
Technical evangelist who can educate and mentor other SQL developers
Excellent communicator -- articulate, persuasive, professional
Helpful Experience (nice To Have)
AWS Cloud Formation or Rapid solution prototyping
AWS Glue and Scala
AWS Step Functions, State Machines, and Control Flow
Data Lake experience or similar (AWS preferred)
Graph Data Base or other NoSQL experience
TFS, Azure DevOps, or Jira project management/requirements management
Git or TFVC version control
Scrum and DevOps experience
Primary Responsibilities
Research, architect, and develop innovative multi-tiered solutions using modern tools and methodologies with a goal for technical excellence
Work closely with other developers, architects, and stakeholders to provide estimates based on customer experience, features, and envisioned solutions
Ensure architecture and design of the solution is in alignment with overall enterprise architecture
Solve problems and proactively look for ways to improve our products and platform
If you think this position is a good fit for you, please reach out to me - feel free to call, text, e-mail, or apply to this posting!
Not Eligible for C2C
Ryan Partlan
586-960-3870
rpartlan@alinestaffing.com
Show more
Show less","SQL Server, SQL Profiler, Extended Events, Query Store, AWS lambda functions, Python, AWS databases, Aurora, DynamoDB, Redshift, AWS storage services, EC2, S3, AWS Cloud Formation, Rapid solution prototyping, AWS Glue, Scala, AWS Step Functions, State Machines, Control Flow, Data Lake, NoSQL, TFS, Azure DevOps, Jira, Git, TFVC, Scrum, DevOps","sql server, sql profiler, extended events, query store, aws lambda functions, python, aws databases, aurora, dynamodb, redshift, aws storage services, ec2, s3, aws cloud formation, rapid solution prototyping, aws glue, scala, aws step functions, state machines, control flow, data lake, nosql, tfs, azure devops, jira, git, tfvc, scrum, devops","aurora, aws cloud formation, aws databases, aws glue, aws lambda functions, aws step functions, aws storage services, azure devops, control flow, data lake, devops, dynamodb, ec2, extended events, git, jira, nosql, python, query store, rapid solution prototyping, redshift, s3, scala, scrum, sql profiler, sql server, state machines, tfs, tfvc"
Senior Data Engineer - Migration,Insight Global,"Dearborn, MI",https://www.linkedin.com/jobs/view/senior-data-engineer-migration-at-insight-global-3770172231,2023-12-17,Windsor, Canada,Mid senior,Hybrid,"Position:
Senior Data Engineer (Big Data/GCP)
Location:
Hybrid - Dearborn, MI (3 days/week)
Work Auth:
W2 (can provide sponsorship if needed)
Day to Day:
This position is ideal for someone who has experience doing all things data, including designing data storage solutions, implementing data storage solutions, optimizing data storage solutions, landing data, manipulating data, and providing L2/L3 support. In this role, you will collaborate with multiple organizations to convert business goals into data storage solutions. This team is currently migrating from Hadoop to Google Cloud Platform (GCP). This role will work in small, cross functional teams and embrace lean and agile practices, software best practices, software quality scanning, automated testing, and CI/CD.
Responsibilities Breakdown:
Identify, document, communicate and design per requirements.
Design data stores & Implement data stores (Hadoop, relational DBs, noSQL DBs, etc.).
Perform Extract, Transform and Load (ETL) or variations of this activity.
Create software to perform traditional Create, Read, Update, Delete (CRUD) transactions.
Tune data stores (indexes, SQL queries) to improve performance.
Assist with customer inquiries and incidents/problems.
Lead and support integration projects and discuss status with executives.
Technical Qualifications:
Prior Data Engineering experience in an enterprise environment
Experience migrating data to Google Cloud Platform (GCP)
Experience designing, implementing, and optimizing data storage solutions
Big Data & Data Manipulation experience: Hadoop Hive/HDFS & SQL
Experience with Python or Java
Familiar with Data Architecture, including Data Design, Index Design, Referential Integrity, etc.
Performance Tuning experience (SQL Tuning, database tuning, etc.)
Pluses:
Experience with Google Cloud Platform (GCP)
Show more
Show less","Data Engineering, Big Data, GCP, Hadoop, Relational DBs, noSQL DBs, ETL, CRUD, SQL, Python, Java, Data Architecture, Data Design, Index Design, Referential Integrity, Performance Tuning","data engineering, big data, gcp, hadoop, relational dbs, nosql dbs, etl, crud, sql, python, java, data architecture, data design, index design, referential integrity, performance tuning","big data, crud, data architecture, data design, data engineering, etl, gcp, hadoop, index design, java, nosql dbs, performance tuning, python, referential integrity, relational dbs, sql"
PostgresSQL Database Engineer,ektello,"Southfield, MI",https://www.linkedin.com/jobs/view/postgressql-database-engineer-at-ektello-3727730575,2023-12-17,Windsor, Canada,Mid senior,Hybrid,"W2 Only please
MSSQL, Oracle, Postgres Architecture -
Seeking candidates with
MSSQL, Oracle, Postgres, Architecture, Security and Production exp.
Must have exp = support/design of enterprise-level infrastructure services focused on MSSQL, Oracle, and Open-source databases like Postgres.
Position Summary:
The qualified candidate for this position will be a member of a Database Engineering team focused on advanced implementation and support of various database technologies. This individual will work closely with various groups including Server Engineering, Enterprise Architecture, Security and Production support teams.
Key Responsibilities:
Assist with advanced support and design of enterprise-level infrastructure services focused on MSSQL, Oracle and other open-source platforms.
Serves as an advanced escalation point for issues that cannot be resolved by tier one support teams.
Create and maintain system documentation for infrastructure technologies, including enterprise standards, installation, configuration, and appropriate troubleshooting steps.
Develop processes to ensure performance, availability, sustainability, and security.
Performance tuning of database systems.
Recommend and implement emerging database technologies.
Create and manage database reports, visualizations, and dashboards.
Create automation for repeating database tasks.
Be available for on-call support as needed.
Basic Qualifications\Professional Skills:
B.S. degree in a computer science, information technology, computer related discipline or 7+ years IT work experience in a multi-site global infrastructure environment.
Must show a progressive advancement in responsibility including deep troubleshooting technical skills.
Team player with proven leadership, communication, organizational, and strong interpersonal skills.
Self-motivated, with keen attention to detail and excellent judgment skills
Technical Skills/Experience:
5+ years of experience in Microsoft SQL Server administration and advanced engineering.
5 years of experience with Oracle Database administration.
Experience with Open-source database like Postgres etc.
Experience with High Availability (HA) tools such as Oracle RAC, SQL Clustering\Mirroring, and 3rd party tools such as DB Visit etc.
Experience with Linux and Windows Server environments
· Experience with cloud services (AWS, Microsoft Azure) a plus
· PowerShell and Unix shell scripting skills
· Advanced knowledge of database security, backup and recovery, and performance monitoring standards
· Strong command of SQL and SQL server tools
Show more
Show less","MSSQL, Oracle, Postgres, Architecture, Security, Production, Server Engineering, Enterprise Architecture, Performance tuning, Automation, SQL, Unix, PowerShell, AWS, Azure, RAC, Mirroring, Linux, Windows Server","mssql, oracle, postgres, architecture, security, production, server engineering, enterprise architecture, performance tuning, automation, sql, unix, powershell, aws, azure, rac, mirroring, linux, windows server","architecture, automation, aws, azure, enterprise architecture, linux, mirroring, mssql, oracle, performance tuning, postgres, powershell, production, rac, security, server engineering, sql, unix, windows server"
Senior Cloud Data Engineer,BDO USA,"Detroit, MI",https://www.linkedin.com/jobs/view/senior-cloud-data-engineer-at-bdo-usa-3765468593,2023-12-17,Windsor, Canada,Mid senior,Hybrid,"Job Description
Job Summary:
This position will work with cutting edge technology, deliver high quality solutions across various industries, and oversee team(s) on engagements that range in size and scope. This position will receive continuous career development opportunities, given the size and potential of client engagements. This role will perform hands-on delivery of data analytics projects, contributing to the development and unit testing of solutions.
Job Duties
Designs and implements best in class data ingestion strategies, data warehouse and data mart structures, semantic layers and models, visualizations, streaming processes, API integrations, and automation (RPA) solutions for end-to-end data analytics solutions on primarily, but not limited to, cloud analytics platforms such as Azure and AWS
Listens to client needs to align solution with business requirements and delivery schedule
Creates written functional and technical designs
Participates in project status and stand meetings, and assists with providing aggregated project status for project and program managers
Assists with SLA compliance of solutions, and performs performance tuning and optimization efforts of end-to-end solutions
Writes code using multiple languages and correctly applies frameworks, architectural patterns, and software development principles
Delivers high-performance, scalable, repeatable, and secure deliverables with broad impact (high throughput and low latency)
Assists with implementation of data governance programs and best practices
Performs the cleaning and transforming of data from source systems into analytics models
Implements models to support data visualizations and integrations
Assists with implementing DevOps, DataOps and MLOps methodologies on projects
Writes custom integration logic in applicable programming languages
Assists project managers with work breakdown structure creation, project estimation, resource staffing, workload planning and adjustments throughout the project lifecycle
Assists clients with licensing, security, and cost estimation of solutions
Performs code reviews to ensure adherence to standards
Works directly with clients and team members to establish secure data analytics platforms and infrastructure
Contributes to successful deployments of developed solutions and integration of DevOps tools
Maintains a broad and current understanding of data analytics and business intelligence strategies, cloud platforms, methodologies, and tools
Builds client relationships during project execution, effectively becoming a trusted advisor of the client
Participates in support activities for existing software solutions
Other duties as assigned
Supervisory Responsibilities
Supervises the day-to-day workload of Associates on assigned engagements to ensure that timelines and deliverables are met, and reviews work product
Education
Qualifications, Knowledge, Skills and Abilities:
High School Diploma or GED equivalent, required
Bachelor’s degree, preferred; focus in Information Systems, Data Science or Computer Science, preferred
Experience
Five (5) or more years of experience within Data Analytics, Business Intelligence, Artificial Intelligence, or Application Development, required
One (1) or more years of experience technically leading development projects, preferred
One (1) or more years of consulting experience or implementation of cloud-based data analytics solutions, preferred
Software
Strong SQL skills including Data Definition Language (DDL), Data Manipulation Language (DML), views, functions, stored procedures, or performance tuning, required
Experience with Data Warehousing, Data Modeling, Semantic Model Definition or Star Schema Construction, required
Hands on delivery experience of end-to-end cloud data analytics solutions within Azure or AWS, preferred
Experience with one (1) or more of the following computer languages, preferred:
C#
Python
Java
Scala
Experience with tabular modeling within Microsoft Fabric, Power BI, or Azure Analysis Services, preferred
Experience with Git and DevOps deployment technologies, preferred
Experience with Linux, preferred
Experience with one (1) or more of the following, preferred:
Data Lake Medallion Architecture
Batch and/or streaming data ingestion into a data lake
AI Algorithms/Machine Learning
Automation tools such as UiPath, Alteryx, etc.
Computer Vision based AI technologies
Other Knowledge, Skills & Abilities
Ability to work with a high degree of professionalism and autonomy
Excellent verbal and written communication skills
Solid organizational skills, especially the ability to meet project deadlines with a focus on details
Ability to successfully multi-task while working independently or within a group environment
Ability to work in a deadline-driven environment, and handle multiple projects simultaneously
Ability to interact effectively with people at all organizational levels of the Firm
Ability to effectively interact with a team of professionals and delegating work assignments, as needed
Ability to build and maintain strong relationships with internal and client personnel
Ability to encourage a team environment on engagements, and contribute to the professional development of assigned personnel
Keywords:
Data Analytics, Business Intelligence, BI, Synapse, IoT, Machine Learning, Data Lake, Stream, Cube, Microsoft, SQL Server, Tableau, .Net, C#, Qlik, Power BI, Machine Learning, Azure Data Factory, RedShift, UiPath, Cloud, RPA, AWS, Redshift, Kinesis, QuickSight, SageMaker, S3, Databricks, AWS Lake Formation, Snowflake, Python, Qlik, Athena, Data Pipeline, Glue, Star Schema, Data Modeling, SQL, SSIS, SSAS, SSRS, PySpark, Microsoft Fabric, dbt, Linux, Terraform, Bicep, Data Ops, Purview, Git, Delta, Pandas, Spark SQL
Individual salaries that are offered to a candidate are determined after consideration of numerous factors including but not limited to the candidate’s qualifications, experience, skills, and geography.
California Range: $111,000 - $152,000
Colorado Range: $111,000 - $152,000
New York City/ Valhalla Range: $111,000 - $152,000
Washington Range: $111,000 - $152,000
About Us
BDO delivers assurance, tax, digital technology solutions and financial advisory services to clients throughout the country and around the globe. We offer numerous industry-specific practices, world-class resources, and an unparalleled commitment to meeting our clients’ needs. We currently serve more than 400 publicly traded domestic and international clients.
Unparalleled partner-involvement
Deep industry knowledge and participation
Geographic coverage across the U.S.
Cohesive global network
Focused capabilities across disciplines
BDO brings world-class resources and exceptional service to each and every one of our clients. BDO USA is a member of BDO International, the world’s fifth largest accounting network.
BDO offers a competitive Total Rewards package that encompass so much more than – “traditional benefits”. Our wide range of rewards and our employees’ ability to customize rewards to their individual needs are two of the reasons why BDO has been honored with so many workplace awards, including 100 Best Companies for Working Parents, Working Mother 100 Best Companies, Top Entry Level Employer, 2022 National Best & Brightest Companies to Work For and more.
Some Examples Of Our Total Rewards Offerings Include
Competitive pay and eligibility for an annual performance bonus.
A 401k plan plus an employer match
Comprehensive, medical, dental, vision, FSA, and prescription insurance from day one
Competitive Paid Time Off with daily accrual from day one of employment, plus paid holidays
Paid Parental Leave
Adoption Assistance
Firm paid life insurance
Wellness programs
Additional offerings include BDO Flex, Group Legal insurance, Pet insurance and Long-Term Care Insurance
Above offerings may be subject to eligibility requirements.
Click here to find out more!
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability or protected veteran status.
""BDO USA, P.A. is an EO employer M/F/Veteran/Disability""
Show more
Show less","Data Analytics, Business Intelligence, Artificial Intelligence, Application Development, Cloud Computing, Data Warehousing, Data Modeling, Semantic Model Definition, Star Schema Construction, SQL, Data Definition Language (DDL), Data Manipulation Language (DML), Views, Functions, Stored Procedures, Performance Tuning, Azure, AWS, C#, Python, Java, Scala, Microsoft Fabric, Power BI, Azure Analysis Services, Git, DevOps, Linux, Data Lake Medallion Architecture, Batch Data Ingestion, Streaming Data Ingestion, Data Lake, AI Algorithms, Machine Learning, Automation Tools, UiPath, Alteryx, Computer Vision, Tableau, .Net, Qlik, Azure Data Factory, RedShift, UiPath, Cloud, RPA, SSIS, SSAS, SSRS, PySpark, dbt, Terraform, Bicep, Data Ops, Purview, Delta, Pandas, Spark SQL, Athena, Data Pipeline, Glue, Star Schema, Data Modeling","data analytics, business intelligence, artificial intelligence, application development, cloud computing, data warehousing, data modeling, semantic model definition, star schema construction, sql, data definition language ddl, data manipulation language dml, views, functions, stored procedures, performance tuning, azure, aws, c, python, java, scala, microsoft fabric, power bi, azure analysis services, git, devops, linux, data lake medallion architecture, batch data ingestion, streaming data ingestion, data lake, ai algorithms, machine learning, automation tools, uipath, alteryx, computer vision, tableau, net, qlik, azure data factory, redshift, uipath, cloud, rpa, ssis, ssas, ssrs, pyspark, dbt, terraform, bicep, data ops, purview, delta, pandas, spark sql, athena, data pipeline, glue, star schema, data modeling","ai algorithms, alteryx, application development, artificial intelligence, athena, automation tools, aws, azure, azure analysis services, azure data factory, batch data ingestion, bicep, business intelligence, c, cloud, cloud computing, computer vision, data definition language ddl, data lake, data lake medallion architecture, data manipulation language dml, data ops, data pipeline, dataanalytics, datamodeling, datawarehouse, dbt, delta, devops, functions, git, glue, java, linux, machine learning, microsoft fabric, net, pandas, performance tuning, powerbi, purview, python, qlik, redshift, rpa, scala, semantic model definition, spark, spark sql, sql, ssas, ssis, ssrs, star schema, star schema construction, stored procedures, streaming data ingestion, tableau, terraform, uipath, views"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Livonia, MI",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3759708640,2023-12-17,Windsor, Canada,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto,
San
Francisco or Chicago
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineer, ML, Data pipelines, Data mining, Data cleaning, Data normalization, Data modeling, Pandas, R, Airflow, KubeFlow, Python, Java, Bash, SQL, Git, Snowflake, Kubernetes, Docker, Spark, pySpark, AWS, GCP, Azure, DynamoDB, ETL, Kafka, Storm, SparkStreaming, Machine learning, Data management, Data classification, Data retention","data engineer, ml, data pipelines, data mining, data cleaning, data normalization, data modeling, pandas, r, airflow, kubeflow, python, java, bash, sql, git, snowflake, kubernetes, docker, spark, pyspark, aws, gcp, azure, dynamodb, etl, kafka, storm, sparkstreaming, machine learning, data management, data classification, data retention","airflow, aws, azure, bash, data classification, data cleaning, data management, data mining, data normalization, data retention, dataengineering, datamodeling, datapipeline, docker, dynamodb, etl, gcp, git, java, kafka, kubeflow, kubernetes, machine learning, ml, pandas, python, r, snowflake, spark, sparkstreaming, sql, storm"
Senior Data Analyst,OneMagnify,"Dearborn, MI",https://www.linkedin.com/jobs/view/senior-data-analyst-at-onemagnify-3773845810,2023-12-17,Windsor, Canada,Mid senior,Hybrid,"Employer Name:
Marketing Associates, LLC dba OneMagnify
Position Title:
Senior Data Analyst
Position Duties:
Work with a centralized team of data scientists, system engineers, and report developers in collaboration with global contact center business customers to design and develop analytical reports and dashboards. Design and develop reports, analytics, dashboards by interpreting business requirements. Text analytics for case notes and survey responses. Ensure the implementation of customer requirements to achieve the final solution. Effectively communicate with technical peers as well as business management to provide effective analytics. Organize and combine data sets to effectively provide business insights to management-level audiences.
Work Location:
777 Woodward Avenue, Ste. 500
Detroit, MI 48226
Work Hours:
8 AM to 5 PM
Minimum Qualifications:
US Master’s degree or foreign equivalent degree in Statistics, Computer Science, or Business Analytics, plus one (1) year of experience as a data/business systems analyst.
Special Skill Requirements:
Must have one (1) year of experience in Hadoop, SQL, Oracle, data mining, and data collection.
Show more
Show less","Hadoop, SQL, Oracle, Data mining, Data collection, Text analytics, Data analytics, Business analytics, Data visualization, Data reporting, Data interpretation, Business intelligence, Communication, Problem solving, Critical thinking","hadoop, sql, oracle, data mining, data collection, text analytics, data analytics, business analytics, data visualization, data reporting, data interpretation, business intelligence, communication, problem solving, critical thinking","business analytics, business intelligence, communication, critical thinking, data collection, data interpretation, data mining, data reporting, dataanalytics, hadoop, oracle, problem solving, sql, text analytics, visualization"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Detroit, MI",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3759709529,2023-12-17,Windsor, Canada,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto,
San
Francisco or Chicago
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data engineering, ML data pipelines, Data pre/post processing, Data mining, Data cleaning, Data normalization, Data modeling, Statistical analysis, Data visualization, Pandas, R, Data platforms, Data frameworks, Big data processing, Realtime data processing, Batch data processing, Text data processing, NLP, Large language models, Automated test suites, Technical documentation, Operational strategy, Python, Java, Bash, SQL, Git, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, AWS, GCP, Azure, Relational databases, NoSQL databases, DynamoDB, ETL, Conversational AI APIs, Recommender systems, Distributed systems, Microservices, Kafka, Storm, SparkStreaming, Applied machine learning, Data management tools, Data classification, Data retention, Complex data projects","data engineering, ml data pipelines, data prepost processing, data mining, data cleaning, data normalization, data modeling, statistical analysis, data visualization, pandas, r, data platforms, data frameworks, big data processing, realtime data processing, batch data processing, text data processing, nlp, large language models, automated test suites, technical documentation, operational strategy, python, java, bash, sql, git, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, relational databases, nosql databases, dynamodb, etl, conversational ai apis, recommender systems, distributed systems, microservices, kafka, storm, sparkstreaming, applied machine learning, data management tools, data classification, data retention, complex data projects","airflow, applied machine learning, automated test suites, aws, azure, bash, batch data processing, big data processing, complex data projects, conversational ai apis, data classification, data cleaning, data engineering, data frameworks, data management tools, data mining, data normalization, data platforms, data prepost processing, data retention, datamodeling, distributed systems, docker, dynamodb, etl, gcp, git, helm, java, kafka, kubernetes, large language models, microservices, ml data pipelines, nlp, nosql databases, operational strategy, pandas, python, r, realtime data processing, recommender systems, relational databases, snowflake, spark, sparkstreaming, sql, statistical analysis, storm, technical documentation, text data processing, visualization"
Geospatial Database Software Engineer,EVONA,San Francisco Bay Area,https://www.linkedin.com/jobs/view/geospatial-database-software-engineer-at-evona-3775700306,2023-12-17,Hayward,United States,Associate,Hybrid,"Senior Geospatial Database Software Engineer
San Fran (Remote)
Your chance to join a thriving start-up, who are pioneering the development of geospatial cloud database and analytics platform. They are looking for experienced Software Engineers to contribute towards developing new features, designing and implementing APIs to enhance their product.
Responsibilities
Design and implement scalable and efficient query capabilities at the heart of our geospatial cloud database.
Introduce new vector and raster functions to expand product feature set for geospatial datasets.
Collaborate with cross-functional teams to define requirements, identify technical solutions, and deliver high-quality software.
Conduct code reviews and offer constructive feedback to ensure code quality and adherence to best practices.
Stay abreast of industry trends and advancements in geospatial and database technologies, and actively contribute to the continuous improvement of our development processes.
Qualifications
Solid technical foundation, with at least 5 years of hands-on experience in implementing scalable, distributed spatial query algorithms and applications.
Proficiency in Java and/or Scala is a prerequisite.
Proven track record of creating high-quality, maintainable, and observable software.
Strong grasp of distributed systems, query engines, DSLs, data structures, and algorithms.
Practical experience in Python programming.
Background in large-scale geospatial data processing and applications.
Apply now!
Show more
Show less","Java, Scala, Python, SQL, GIS, Spatial databases, Cloud computing, Distributed systems, Query engines, DSLs, Data structures, Algorithms, Geospatial data processing","java, scala, python, sql, gis, spatial databases, cloud computing, distributed systems, query engines, dsls, data structures, algorithms, geospatial data processing","algorithms, cloud computing, data structures, distributed systems, dsls, geospatial data processing, gis, java, python, query engines, scala, spatial databases, sql"
Senior Data Engineer,Walmart Global Tech,"Sunnyvale, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-walmart-global-tech-3777302255,2023-12-17,Hayward,United States,Mid senior,Hybrid,"What you'll do...
As a Walmart Data Engineer, you will dig into our mammoth scale of data to help unleash the power of retail data science by imagining, developing, and maintaining data pipelines that our Data Scientists and Analysts can rely on. You will be responsible for contributing to an orchestration layer of complex data transformations, refining raw data from source into targeted, valuable data assets for consumption in a governed way. You will partner with Data Scientists, Analysts, other engineers and business stakeholders to solve complex and exciting challenges so that we can build out capabilities that evolve the retail business model while making a positive impact on our customers’ lives.
About Team:
Our team works closely with our US stores and eCommerce business to better serve customers by empowering team members, stores, and merchants with technological innovation. From groceries and entertainment to sporting goods and crafts, Walmart U.S. offers an extensive selection that our customers value, whether they shop online at Walmart.com, through one of our mobile apps, or in-store. Focus areas include customers, stores and employees, in-store service, merchant tools, merchant data science, and search and personalization.
What you'll do:
Data Modeling: Influences the overarching data strategy and vision for data pipelines and data products. Oversees and governs the expansion of existing data architecture and the optimization of data query performance via best practices. Presents and socializes data models to business and information technology stakeholders.
Data Architecture: Designs, implements and improves processes in data management.
Leads and participates in medium- to large-scale, complex, cross-functional projects by reviewing project requirements. Translating requirements into technical solutions. Gathering requested information, writing and developing the code. Communicating status and issues to team members and stakeholders, collaborating with project team and cross functional teams. Troubleshooting open issues and bug-fixes. Enhancing design to prevent reoccurrences of defects, ensuring on-time delivery and hand-offs. Daily interacting with project manager to provide input on project plan and providing leadership to the project team.
Develops Innovation strategies, processes and best practices by leading internal technical teams. Partnering with cross-functional teams across the business. Developing assessments of key opportunities, documenting project scopes, developing long-range plans and project timelines. Communicating with and influencing decision-makers and executives within the organization and resolving technology differences across teams through informed discussions.
Leads the work of other small groups of six to ten engineers, including offshore associates. Leads assigned Engineering projects by providing pertinent documents, direction, and examples: identifying short- and long- term solutions and timeline. Reviewing and approving proposed solutions, implementing new architectural patterns and performing design and code reviews of changes.
What you'll bring:
Well versed with Hadoop, Hive, Spark using Scala.
You evangelize an extremely high standard of code quality, system reliability, and performance.
You have a proven track record coding with at least one programming language (e.g., Java, Python).
You’re experienced in computing platforms (e.g., GCP, Azure).
You’re skilled in data modeling & data migration protocols.
Experience with ThoughtSpot, Druid, and Big Query is added advantage.
Experience with the integration tools like Automic, Airflow.
Ability to write requirements for ETL and BI developers.
Ability to write designs for data architecture of data warehouse or data lake solutions or end to end pipelines.
Expert in data architecture principles, distributed computing.
Intake prioritization, cost/benefit analysis, decision making of what to pursue across a wide base of users/stakeholders and across products, databases and services.
About Walmart Global Tech
Imagine working in an environment where one line of code can make life easier for hundreds of millions of people. That’s what we do at Walmart Global Tech. We’re a team of software engineers, data scientists, cybersecurity expert's and service professionals within the world’s leading retailer who make an epic impact and are at the forefront of the next retail disruption. People are why we innovate, and people power our innovations. We are people-led and tech-empowered. We train our team in the skillsets of the future and bring in experts like you to help us grow. We have roles for those chasing their first opportunity as well as those looking for the opportunity that will define their career. Here, you can kickstart a great career in tech, gain new skills and experience for virtually every industry, or leverage your expertise to innovate at scale, impact millions and reimagine the future of retail.
Flexible, hybrid work:
We use a hybrid way of working that is primarily in office coupled with virtual when not onsite. Our campuses serve as a hub to enhance collaboration, bring us together for purpose and deliver on business needs. This approach helps us make quicker decisions, remove location barriers across our global team and be more flexible in our personal lives.
Benefits:
Benefits: Beyond our great compensation package, you can receive incentive awards for your performance. Other great perks include 401(k) match, stock purchase plan, paid maternity and parental leave, PTO, multiple health plans, and much more.
Equal Opportunity Employer:
Walmart, Inc. is an Equal Opportunity Employer – By Choice. We believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing diversity- unique styles, experiences, identities, ideas and opinions – while being inclusive of all people.
The above information has been designed to indicate the general nature and level of work performed in the role. It is not designed to contain or be interpreted as a comprehensive inventory of all responsibilities and qualifications required of employees assigned to this job. The full Job Description can be made available as part of the hiring process.
Show more
Show less","Hadoop, Hive, Spark, Scala, Java, Python, GCP, Azure, Data modeling, Data migration, ThoughtSpot, Druid, Big Query, Automic, Airflow, ETL, BI, Data architecture, Distributed computing, Data warehouse, Data lake, Pipelines","hadoop, hive, spark, scala, java, python, gcp, azure, data modeling, data migration, thoughtspot, druid, big query, automic, airflow, etl, bi, data architecture, distributed computing, data warehouse, data lake, pipelines","airflow, automic, azure, bi, big query, data architecture, data lake, data migration, datamodeling, datawarehouse, distributed computing, druid, etl, gcp, hadoop, hive, java, pipelines, python, scala, spark, thoughtspot"
Data Engineer/Big Data Developer,Sigmaways Inc,San Francisco Bay Area,https://www.linkedin.com/jobs/view/data-engineer-big-data-developer-at-sigmaways-inc-3729285817,2023-12-17,Hayward,United States,Mid senior,Hybrid,"As a Developer within the Big Data team, you will contribute to high-quality technology solutions that address business needs by developing data applications for the customer business lines.
You will contribute to the development and ongoing maintenance of a number of strategic
data initiatives and data and analytic applications.
Responsibilities:
The Data Engineer will be responsible for designing, developing, and supporting data applications and platforms with a focus on Big Data/Hadoop, Python/Spark, and other related technologies.
Work with leadership to conceptualize our next-generation product, contribute to the technical architecture of our data platform (including improvements to our cloud infrastructure and DevOps), lead the analysis and resolution of production issues, and help continually improve our processes.
Work closely with product management, business, engineers, cross-functional analysts, and data scientists.
Demonstrates master hands-on capability to drive components to delivery from inception to final product.
Recommends and contributes to software engineering best practices, including those that have enterprise-wide impact.
Takes accountability for the quality, total cost of ownership, maintainability, and security of any component or application produced.
Performs as an expert in all parts of the software development lifecycle (e.g., coding, testing, development) and coaches others around such practices.
Converses in many technologies and learns new technologies quickly.
Qualifications:
Design, Develop, and maintain Big Data platforms including Data Lake, Operational Datamart, and Analytics Data Warehouses.
Extensive experience in managing and optimizing Big Data ecosystems such as Spark, Hadoop/MR, Hive, and distributed systems such as Kafka.
Experience in the design and development of data applications or data platforms with BigData/Hadoop, Python/Spark, etc
Experience in designing, engineering, and managing data lake ingestion, validation, transformation, and consumption services leveraging cloud data tools like Hive, Spark, EMR, Glue ETL and Catalog, Snowflake, etc
Experience with ETL code and tools including Cloudera/Hadoop, Nifi, Spark, etc.
Experience with SQL database systems including PostgreSQL and MySQL/MariaDB.
Strong technical knowledge of AWS, Azure cloud infrastructure, distributed systems, and reliability practices.
Experience working with Terraform, Jenkins, Kubernetes, and Docker.
Experience with API usage and integration.
Programming experience using Python, and shell scripting.
Show more
Show less","Big Data, Hadoop, Python, Spark, Cloud infrastructure, DevOps, Product management, Crossfunctional analysis, Data science, Software engineering, Coding, Testing, Development, Kafka, Hive, Distributed systems, Data Lake, Operational Datamart, Analytics Data Warehouses, Spark, EMR, Glue ETL, Catalog, Snowflake, Cloudera, Nifi, PostgreSQL, MySQL/MariaDB, AWS, Azure, Terraform, Jenkins, Kubernetes, Docker, API integration, Python, Shell scripting","big data, hadoop, python, spark, cloud infrastructure, devops, product management, crossfunctional analysis, data science, software engineering, coding, testing, development, kafka, hive, distributed systems, data lake, operational datamart, analytics data warehouses, spark, emr, glue etl, catalog, snowflake, cloudera, nifi, postgresql, mysqlmariadb, aws, azure, terraform, jenkins, kubernetes, docker, api integration, python, shell scripting","analytics data warehouses, api integration, aws, azure, big data, catalog, cloud infrastructure, cloudera, coding, crossfunctional analysis, data lake, data science, development, devops, distributed systems, docker, emr, glue etl, hadoop, hive, jenkins, kafka, kubernetes, mysqlmariadb, nifi, operational datamart, postgresql, product management, python, shell scripting, snowflake, software engineering, spark, terraform, testing"
Senior Data Engineer,Recruiting from Scratch,San Francisco Bay Area,https://www.linkedin.com/jobs/view/senior-data-engineer-at-recruiting-from-scratch-3778842737,2023-12-17,Hayward,United States,Mid senior,Hybrid,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our next evolution is underway as a newly public company looking to expand and continue to build meaningful experiences for our users. From social issues to original content, we’re blazing innovative paths with impact for our community, all while leveraging the latest tech stacks and striving for engineering excellence. At the heart of our work in this new chapter is a shared set of core values: openness and exploration, a bias for action, and strong support of the LGBTQ community.
With a track record of strong financial performance and plans for continued headcount growth, we’re looking to build a team of talented, passionate, and open-minded people who believe in our mission, align with our values, and are excited to work at the intersection of innovative technology and social impact. Come be a part of this exciting journey with us.
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, deliver analysis to further improve engagement in existing features, and empower our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Solve technical problems of the highest scope and complexity
Exert significant influence on the company’s analytical long-range goals and data architecture
Define and extend our internal standards for style, maintenance, and best practices for a high-scale data platform
Provide mentorship for all on your team to help them grow in their technical responsibilities
Propose ideas to improve the scale, performance, and capabilities of the Data Platform
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What we'll love about you
7+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field
7+ years experience using Python
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention
Location
: We are looking to hire someone in the Palo Alto area.
Salary Range: $180,000 - 250,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, SQL, TDD, Pair Programming, Continuous Integration, Kafka, Storm, SparkStreaming, ETL, Data Warehouses, Data classification","python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, tdd, pair programming, continuous integration, kafka, storm, sparkstreaming, etl, data warehouses, data classification","airflow, continuous integration, data classification, data warehouses, docker, etl, helm, kafka, kubernetes, pair programming, python, snowflake, spark, sparkstreaming, sql, storm, tdd"
Data Science Engineer Internship Summer 2024,CCC Intelligent Solutions,"Chicago, IL",https://www.linkedin.com/jobs/view/data-science-engineer-internship-summer-2024-at-ccc-intelligent-solutions-3738623227,2023-12-17,River Forest,United States,Associate,Onsite,"CCC Intelligent Solutions is a leading technology company helping to improve the insurance claims process for millions of people. Our award-winning SaaS platform connects more than 35,000 businesses, including insurance carriers, repair facilities, automakers, part suppliers, lenders, and others to streamline the process from start to finish.
Our advanced capabilities in AI, IoT, telematics, data, and analytics drive continual innovation across our platform, as we work to advance the multi-trillion-dollar P&C insurance economy’s digital transformation.
At CCC, our mission is to keep people’s lives moving forward when it matters most. Diversity of experience and perspective is key to our pursuit so we can deliver a future of possibilities for our customers.
The Role
Our program is designed to #CCCJumpstart your career! At CCC, you will work and learn alongside innovative and inspiring leaders and gain valuable technical experience while working on real business solutions in a corporate setting.
Key
Responsibilities:
Technology:
Computer Vision
Deep Learning and Machine Learning
Data Exploration, Visualization, Storytelling
ETL
Building machine learning models and deploying them as products for the auto-insurance industry. This can be done using traditional machine learning or techniques specific to massive unstructured data such as deep learning.
Machine learning on structured datasets – starting with data exploration and feasibility tests, design and perform experiments to compare modeling approaches, generate predictions and ensure the solution meets the target metrics specified by business requirements, as well as ensuring scalability of algorithms. Interpret, visualize and communicate results to a general audience.
Applying deep learning to claim image data - using photos of damaged cars to predict what parts that need to be replaced, if a car is a total loss or repairable, finding the severity of the damage using AI and data insights. The amount of property, casualty, car photos, and other data that CCC can harness is one of our biggest competitive advantages.
There are additional projects currently in flight here at CCC, but we wanted to give you an idea of the type of work our team members work on.
This opportunity is a paid, full-time, summer internship. There will be two cohorts for the upcoming CCC Jumpstart class:
May 20th, 2024 – August 9th, 2024
June 17th, 2024 – September 6th, 2024
Requirements:
If you have a Masters, and/or PhD, and have demonstrated excellence in your academic studies, this opportunity is right for you. We are looking for candidates who offer strong collaborative skills and work well with a team. A strong interest in computer science and/or related fields is essential to the success of each intern.
If you have knowledge, through coursework or project work, of Python, Machine Learning, Deep Learning, Mathematics, or Statistics/ Modeling, this opportunity is right for you.
About the company’s commitment to its employees:
CCC Intelligent Solutions employees are part of an inclusive culture that brings together diverse backgrounds and perspectives. Our team is defined by our values of: Integrity, Customer-Focus, Innovation, Diversity & Inclusion, and Tenacity. Together, we help our clients and each other achieve new goals.
CCC is committed to providing employees with opportunities to advance their careers and skillsets. CCC team members receive access to training and education reimbursement is available.
CCC offers competitive compensation and generous benefits. Health insurance, PTO, 401K, are just some of the benefits available to team members.
Each team member plays an important role in the company’s success and each team member has a voice. CCC employee engagement and job satisfaction ratings consistently exceed industry norms – underscoring the value CCC places on its employees.
Explore the Employee Experience at CCC.
Show more
Show less","Computer Vision, Deep Learning, Machine Learning, Data Exploration, Data Visualization, Data Storytelling, ETL, Machine Learning Models, Python, Mathematics, Statistics, Modeling","computer vision, deep learning, machine learning, data exploration, data visualization, data storytelling, etl, machine learning models, python, mathematics, statistics, modeling","computer vision, data exploration, data storytelling, deep learning, etl, machine learning, machine learning models, mathematics, modeling, python, statistics, visualization"
Data Analyst,Scrutton Bland,"Colchester, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-analyst-at-scrutton-bland-3785364327,2023-12-17,Ipswich, United Kingdom,Associate,Onsite,"Full Time: 37.5 Hours
Location: Can be based in either our Colchester or Ipswich office
What you’ll do
You will be a talented and intuitive individual enthusiastic and hungry to learn and develop your knowledge further. We would expect you to be able to manage the data reporting role and undertake all aspects of delivering data to key individuals in the business.
You will be an expert within PowerBI and similar data analytics models, leading from the front on how our data is reported on a day-to-day basis.
You understand the need for change and can adapt your approach accordingly using data to drive and supports this. You will be the go-to person for building new dashboards and working alongside projects to streamline data and use this to enable ongoing business strategy and change.
You will be able to work methodically, but pro-actively address issues and resolve problems. To be able to work under pressure and prioritise, and where needed know when to ask for help or support from your team around you.
Purpose of the role
Interpret business & group reporting needs and turn these into technical reporting requirements.
Understand business requirements in a BI context and design data models to transform raw data into meaningful insights.
Reviewing data collection processes and assessing the validity of the information stored.
Map data flows in the business to standardise and streamline how data is collected, stored, processed and deleted.
Convert business requirements into technical specifications and work with the senior leadership team to decide a timeline to accomplish these.
Develop analytical problem-solving techniques.
Creating functional, interactive, and visually appealing reporting using PowerBI.
Work with users and team members at all levels for performance improvement and suggestions.
Document process, models, designs, and solutions and able to explain and represen
What you’ll get
We offer an attractive reward and benefits package which you can tailor to suit your needs.
Commitment to continued learning & development
Flexible and remote working alongside a culture that promotes work life balance
Salary of up to £40,000 dependant on experience
36 days holiday (including 8 bank holidays and 3 days Christmas closure)
Life Assurance up to 4x salary and contribution towards the Firm’s pension scheme
Regular social and informal events within the organisation
Amazing employee referral scheme, paying up to £5,000 for a successful referral
Hybrid working which allows flexibility and work life balance
Access to mental health support
Employee health and wellbeing program
Opportunity to participate in the Scrutton Bland Foundation, supporting the local community
Every new team member will experience a full induction and training programme where you will get up to speed on SB systems, processes, and ways of working.
What you’ll be part of
At Scrutton Bland, we are committed to empowering our clients and people to make informed decisions and achieve their goals, whether business or personal. Our mission is to be the guardian of our client’s financial futures and our people’s professional aspirations. So, what does that mean for you?
We are looking for individuals who are:
Ambitious, who want to drive the Firm forward.
Compassionate, they have purpose and are proactive in their approach to meeting both client and personal goals.
Agile, they embrace new ideas and demonstrate innovativeness.
And proud, proud to be part of the Scrutton Bland Group.
Diversity Statement
Scrutton Bland values diversity, inclusion and equal opportunities. We don’t discriminate on the basis of race, gender, sexual orientation, gender assignment, age, religion or belief, pregnancy, maternity and paternity status, disability, marital or civil partnership status.
Recruitment and internal career development
We encourage applications from all backgrounds, communities, and industries, and are committed to having a team that is made up of diverse skills, experiences and abilities. We actively encourage ethnic minorities and disabled applicants and value the positive impact different lived experiences have on our teams. We are committed to inclusion and diversity within our organisation. We continue to encourage interest from applicants who require reasonable adjustments within the workplace.
Sounds interesting? We look forward to hearing from you, please apply now
Show more
Show less","PowerBI, Data Analytics, Data Reporting, Business Intelligence, Data Visualization, Dashboard Development, Data Transformation, Data Modeling, Data Quality Assessment, Data Collection, Data Storage, Data Processing, Data Deletion, Analytical ProblemSolving, ProblemSolving, Communication, Teamwork, Collaboration, Adaptability, Proactive Approach","powerbi, data analytics, data reporting, business intelligence, data visualization, dashboard development, data transformation, data modeling, data quality assessment, data collection, data storage, data processing, data deletion, analytical problemsolving, problemsolving, communication, teamwork, collaboration, adaptability, proactive approach","adaptability, analytical problemsolving, business intelligence, collaboration, communication, dashboard development, data collection, data deletion, data processing, data quality assessment, data reporting, data storage, data transformation, dataanalytics, datamodeling, powerbi, proactive approach, problemsolving, teamwork, visualization"
Data Engineering Consultant,Nigel Frank International,"Colchester, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineering-consultant-at-nigel-frank-international-3739799995,2023-12-17,Ipswich, United Kingdom,Mid senior,Onsite,"I am working with a Microsoft Partnered consultancy who are looking for a Data Engineering Consultant to join their growing team. You will have the opportunity to work on a variety of client projects across a number of different sectors such as retail, finance and the public sector.
In this role you will utilise your experience with the latest Azure technologies to provide guidance to clients. As a hands-on consultant you use your experience as a data engineer to ensure the timely delivery on clients data driven projects.
You will be joining a people centred business, who invest heavily in its team. You will be given the opportunity to explore other areas of technologies that may be of interest to you, including data science, machine learning and AI. You will have time designated purely to training and development, with internal development schemes as well as funded Microsoft learning, training courses and certifications.
This is an exciting time to join a hugely successful and growing business who have a number of exciting roadmap of projects planned all the way into 2024!
As part of this role, you will be responsible for some of the following areas.
Offer guidance to clients on data driven projects
Take the lead on data engineering projects focused around the Microsoft and Azure tech stack
Deliver on all aspects of the project including requirements gathering, testing, implementation and maintenance
This is a salaried role paying up to £55,000 per annum depending on experience and a company benefits package. This is a home based role with occasional, fully expensed visits to client sites as and when required. You will also need to commute to company retreats once per quarter to meet other members of the team.
To be successful in the role you will have.
Experience working with the Azure tech stack including Azure Data Factory, Synapse and Azure Data Lake.
Experience designing and implementing ETL solutions.
Coding experience with languages such as SQL and Python.
Strong knowledge of Databricks for data ingestion and transformation would be beneficial
Excellent communication skills
This is just a brief overview of the role. For the full information, simply apply to the role with your CV, and I will call you to discuss further. My client is looking to begin the interview process ASAP, so don't miss out, APPLY now! To do so please email me at a.pinkerton@nigelfrank.com or call me on 0191 3387487.
Nigel Frank International are the go-to recruiter for Power BI and Azure Data Platform roles in the UK offering more opportunities across the country than any other recruitment agency. We're the proud sponsor and supporter of SQLBits, Power Platform World Tour, the London Power BI User Group, Newcastle Power BI User Group and Newcastle Data Platform and Cloud User Group. We are the global leaders in Microsoft recruitment.
Show more
Show less","Azure, Azure Data Factory, Azure Synapse, Azure Data Lake, ETL, Python, SQL, Databricks, Data Ingestion, Data Transformation","azure, azure data factory, azure synapse, azure data lake, etl, python, sql, databricks, data ingestion, data transformation","azure, azure data factory, azure data lake, azure synapse, data ingestion, data transformation, databricks, etl, python, sql"
Data Analyst/Business Analyst,Ttgtalentsolutions,"Colchester, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-analyst-business-analyst-at-ttgtalentsolutions-3732668636,2023-12-17,Ipswich, United Kingdom,Mid senior,Remote,"DESCRIPTION: you will contribute to the efficient handling of essential case-related information. This role offers an opportunity to work in a fast-paced environment while ensuring accuracy and attention to detail in document processing.
RESPONSIBILITIES AND DUTIES:
Utilize Microsoft Outlook, Word, Excel, Adobe Acrobat, and PSISafe for efficient document management.
Perform data entry tasks to input necessary information for case management.
Proactively follow up on required documentation for ongoing cases.
Execute additional duties as needed to support case management and administrative functions.
REQUIREMENTS:
A high school diploma or GED.
Strong customer service skills, enabling effective communication with clients and co-workers.
Preferred bilingual proficiency in English and Spanish for enhanced client interaction.
Previous legal experience, including working with clients and understanding case-related documentation.
Proficiency in Microsoft Office programs (Word, Excel, Outlook) and familiarity with tools like DocuSign.
Ability to thrive in a fast-paced environment and manage a heavy workload efficiently.
Excellent communication skills to effectively collaborate with clients and colleagues.
Exceptional organizational skills, multitasking abilities, and the capacity to prioritize tasks effectively.
BENEFITS:
Paid Time Off, Holiday, Bereavement, and Sick Time
401K Retirement Savings Plan
Group Medical/Dental/Vision Plans
Employer-Covered Supplemental Benefits
Voluntary Supplemental Benefits
Annual Performance Review
Powered by Webbtree
Show more
Show less","Microsoft Office Suite (Outlook Word Excel), Adobe Acrobat, PSISafe, DocuSign","microsoft office suite outlook word excel, adobe acrobat, psisafe, docusign","adobe acrobat, docusign, microsoft office suite outlook word excel, psisafe"
Junior Data Engineer,Pepper Mill,"Ipswich, England, United Kingdom",https://uk.linkedin.com/jobs/view/junior-data-engineer-at-pepper-mill-3783936981,2023-12-17,Ipswich, United Kingdom,Mid senior,Hybrid,"About Sparta Global
Embark on a transformative career journey with Sparta Global, where diversity, innovation, and passion for technology converge. We welcome individuals from all backgrounds, offering not just jobs, but dynamic careers in the tech industry. You'll work alongside enthusiastic professionals, receiving top-tier training and mentorship to hone your skills for success in both public and private sectors. Our commitment to designing impactful careers and coaching future leaders is evident in our over 10 prestigious awards in Learning & Development and Equality, Diversity & Inclusion. As a Top 20 Employer for Social Mobility and a proud B-Corp certified organisation, we're not just advancing careers; we're fostering a more diverse, equitable, and inclusive tech landscape. Join us in shaping the future of technology - where your growth is our mission, and your success, our pride. Apply now to be part of our award-winning team at Sparta Global.
About This Role
You'll become versatile in a wide array of tools across topics covering data focused coding, data visualisation, Cloud Services and Big Data.
You'll be designing, building, maintaining, and troubleshooting the data pipelines that enable organizations to store, process, and analyse their data, and ensuring the data is reliable through testing and debugging. Looking for efficiencies and optimising the data pipelines for scalability and performance will be a focus.
You'll be handling and working with large sets of structured and unstructured data and will be responsible for ensuring that the data is organized and available for data scientists and analysts to use.
Working with others is key, you could be working with other engineers, developers, data scientists, analysts and even stakeholders to understand their data needs.
We're not expecting you to have the proficiencies right away - that's where our award-winning Academy comes in. We are the experts in building skills and confidence in a fun and supportive environment that will not only challenge you but also develop your specialist capabilities ready to work on our clients' projects.
What we're looking for.
To be successful for this role you will demonstrate a level of ability in Python or similar. You will be passionate about technology and eager to learn programme development to an advanced level.
We're Seeking Candidates Who Can Exemplify Our Values
Empathy and Diversity: Integrity, respect, and a commitment to inclusivity.
Drive: A goal-oriented mindset with pride in exceeding targets.
Collaboration: A team-focused approach, fostering positive relationships.
Innovation: Curiosity, creativity, and openness to diverse ideas.
Flexibility: Adaptability and composure in the face of change.
As a national organisation with clients across the UK, we require flexibility and a willingness to relocate post-remote training. Deployment locations vary and cannot be guaranteed. We encourage applications from diverse backgrounds and experience levels. Eligibility to work in the UK by the start of employment is mandatory.
Why You Should Apply
Our environment is designed to nurture your talents and skills, your hard work and progress are not just appreciated - they're tangibly rewarded. We conduct performance-based reviews every six months, offering you the chance to increase your earning potential twice a year. This regular appraisal system is our way of ensuring that your efforts and achievements are consistently recognised and rewarded.
We Also Provide
20 days annual leave + bank holidays.
An extra day off for your birthday.
Pension.
Discounted gym membership.
Eye care.
Death in service cover.
Cycle to work scheme.
Season ticket loan.
Employee assistance program.
Yearly budget for personal development.
Access to alumni and community networks.
Opportunities to be brand ambassadors.
Being employed by Sparta Global is an investment in your future that pays dividends along the way. We give you breadth of experience and skills, along with increasing opportunities to develop further and earn more. No two career paths look the same at Sparta.
Our Recruitment Process
Begin your journey via our supportive recruitment process. Apply online and our team will promptly review your application, contacting successful candidates within 48 hours to initiate the next steps. If you pass our initial screening, candidates will proceed to online assessments which vary depending on the opportunity you are applying for. The final stage is a competency interview, here you'll have the opportunity to impress us with your ability to communicate effectively and exhibit behavioural competencies through relevant examples. We're looking for candidates who can demonstrate a collaborative spirit and a growth mindset.
Your dedicated Talent Team member will be with you every step of the way to support and answer any questions you have. You can also visit our YouTube channel to gain valuable insights and expert advice on virtual interviews, strategies to manage nerves, and tips on nonverbal communication.
We look forward to receiving your application - good luck!
Show more
Show less","Python, Data visualization, Cloud services, Big data, Data pipelines, Data storage, Data processing, Data analysis, Data testing, Data debugging, Data scalability, Data performance, Structured data, Unstructured data, Data organization, Data availability, Data science, Data analytics, Teamwork, Collaboration, Communication, Behavioural competencies, Growth mindset","python, data visualization, cloud services, big data, data pipelines, data storage, data processing, data analysis, data testing, data debugging, data scalability, data performance, structured data, unstructured data, data organization, data availability, data science, data analytics, teamwork, collaboration, communication, behavioural competencies, growth mindset","behavioural competencies, big data, cloud services, collaboration, communication, data availability, data debugging, data organization, data performance, data processing, data scalability, data science, data storage, data testing, dataanalytics, datapipeline, growth mindset, python, structured data, teamwork, unstructured data, visualization"
Junior Data Engineer,Sparta Global,"Ipswich, England, United Kingdom",https://uk.linkedin.com/jobs/view/junior-data-engineer-at-sparta-global-3783935750,2023-12-17,Ipswich, United Kingdom,Mid senior,Hybrid,"About Sparta Global
Embark on a transformative career journey with Sparta Global, where diversity, innovation, and passion for technology converge. We welcome individuals from all backgrounds, offering not just jobs, but dynamic careers in the tech industry. You'll work alongside enthusiastic professionals, receiving top-tier training and mentorship to hone your skills for success in both public and private sectors. Our commitment to designing impactful careers and coaching future leaders is evident in our over 10 prestigious awards in Learning & Development and Equality, Diversity & Inclusion. As a Top 20 Employer for Social Mobility and a proud B-Corp certified organisation, we're not just advancing careers; we're fostering a more diverse, equitable, and inclusive tech landscape. Join us in shaping the future of technology - where your growth is our mission, and your success, our pride. Apply now to be part of our award-winning team at Sparta Global.?
About This Role
You'll become versatile in a wide array of tools across topics covering data focused coding, data visualisation, Cloud Services and Big Data.
You'll be designing, building, maintaining, and troubleshooting the data pipelines that enable organizations to store, process, and analyse their data, and ensuring the data is reliable through testing and debugging. Looking for efficiencies and optimising the data pipelines for scalability and performance will be a focus.
You'll be handling and working with large sets of structured and unstructured data and will be responsible for ensuring that the data is organized and available for data scientists and analysts to use.
Working with others is key, you could be working with other engineers, developers, data scientists, analysts and even stakeholders to understand their data needs.
We're not expecting you to have the proficiencies right away - that's where our award-winning Academy comes in. We are the experts in building skills and confidence in a fun and supportive environment that will not only challenge you but also develop your specialist capabilities ready to work on our clients' projects.
What we're looking for.
To be successful for this role you will demonstrate a level of ability in Python or similar. You will be passionate about technology and eager to learn programme development to an advanced level.
We're seeking candidates who can exemplify our values:??
Empathy and Diversity: Integrity, respect, and a commitment to inclusivity.??
Drive: A goal-oriented mindset with pride in exceeding targets.??
Collaboration: A team-focused approach, fostering positive relationships.??
Innovation: Curiosity, creativity, and openness to diverse ideas.??
Flexibility: Adaptability and composure in the face of change.
As a national organisation with clients across the UK, we require flexibility and a willingness to relocate post-remote training. Deployment locations vary and cannot be guaranteed. We encourage applications from diverse backgrounds and experience levels. Eligibility to work in the UK by the start of employment is mandatory.
Why you should apply:?
Our environment is designed to nurture your talents and skills, your hard work and progress are not just appreciated - they're tangibly rewarded. We conduct performance-based reviews every six months, offering you the chance to increase your earning potential twice a year. This regular appraisal system is our way of ensuring that your efforts and achievements are consistently recognised and rewarded.?
We also provide:??
20 days annual leave + bank holidays.??
An extra day off for your birthday.??
Pension.??
Discounted gym membership.??
Eye care.??
Death in service cover.??
Cycle to work scheme.??
Season ticket loan.??
Employee assistance program.??
Yearly budget for personal development.??
Access to alumni and community networks.??
Opportunities to be brand ambassadors.??
Being employed by Sparta Global is an investment in your future that pays dividends along the way. We give you breadth of experience and skills, along with increasing opportunities to develop further and earn more. No two career paths look the same at Sparta.??
Our Recruitment Process:?
Begin your journey via our supportive recruitment process. Apply online and our team will promptly review your application, contacting successful candidates within 48 hours to initiate the next steps. If you pass our initial screening, candidates will proceed to online assessments which vary depending on the opportunity you are applying for. The final stage is a competency interview, here you'll have the opportunity to impress us with your ability to communicate effectively and exhibit behavioural competencies through relevant examples. We're looking for candidates who can demonstrate a collaborative spirit and a growth mindset.??
Your dedicated Talent Team member will be with you every step of the way to support and answer any questions you have. You can also visit our YouTube channel to gain valuable insights and expert advice on virtual interviews, strategies to manage nerves, and tips on nonverbal communication.???
We look forward to receiving your application - good luck!???
Show more
Show less","Python, Data pipelines, Cloud services, Big data, Data science, Data analysis, Software development, Programming","python, data pipelines, cloud services, big data, data science, data analysis, software development, programming","big data, cloud services, data science, dataanalytics, datapipeline, programming, python, software development"
"Role: Lead Data Developer/Data Architect - Contract / Full-Time - Phoenix, AZ",Futran Solutions,"Phoenix, AZ",https://www.linkedin.com/jobs/view/role-lead-data-developer-data-architect-contract-full-time-phoenix-az-at-futran-solutions-3660535707,2023-12-17,Phoenix,United States,Mid senior,Onsite,"Role: Data Architect
Location: Phoenix, AZ
Duration: Full time/ Contract
Note: Kindly share the profile at mjaved@futransolutions.com
Skills
10+ Years of experience in creating data pipelines with ETL, Ingestion and Sanitization (preferably on AWS Cloud for at least 3 years)
Proficiency in Data Modelling, Managing data in Structured, Unstructured & Semi-Structured form.
Working proficiency with big data and data pipelines On-Prem, On-Cloud (AWS)
Expertise in Java, Java Spark (Good to have pySpark)
Exposure to Apache Airflow for Data Workflows
Exposure AWS Data Services like, AWS Redshift, EMR Serverless, Amazon S3, LakeFormation, Glue, Athena
Data Integration experience with data Streaming (Kafka, AWS Kinesis)
Good understanding of Data Security
Exposure to Data Analytics with Power BI, Grafana.
Good to have:
Exposure to Talend
Python
Sagemaker
Tablueau
WebFocus
Mainframes with Cobol exposure
Show more
Show less","Data Engineering, Data Modeling, Structured Data, Unstructured Data, SemiStructured Data, Big Data, Data Pipelines, ETL, Ingestion, Sanitization, AWS Cloud, Java, Java Spark, PySpark, Apache Airflow, AWS Redshift, EMR Serverless, Amazon S3, LakeFormation, Glue, Athena, Data Integration, Data Streaming, Kafka, AWS Kinesis, Data Security, Data Analytics, Power BI, Grafana, Talend, Python, Sagemaker, Tablueau, WebFocus, Mainframes, COBOL","data engineering, data modeling, structured data, unstructured data, semistructured data, big data, data pipelines, etl, ingestion, sanitization, aws cloud, java, java spark, pyspark, apache airflow, aws redshift, emr serverless, amazon s3, lakeformation, glue, athena, data integration, data streaming, kafka, aws kinesis, data security, data analytics, power bi, grafana, talend, python, sagemaker, tablueau, webfocus, mainframes, cobol","amazon s3, apache airflow, athena, aws cloud, aws kinesis, aws redshift, big data, cobol, data engineering, data integration, data security, data streaming, dataanalytics, datamodeling, datapipeline, emr serverless, etl, glue, grafana, ingestion, java, java spark, kafka, lakeformation, mainframes, powerbi, python, sagemaker, sanitization, semistructured data, spark, structured data, tablueau, talend, unstructured data, webfocus"
Entry Level Data Analyst/Management Consultant - Nationwide (US Based Candidates Only),Arcadis,"Phoenix, AZ",https://www.linkedin.com/jobs/view/entry-level-data-analyst-management-consultant-nationwide-us-based-candidates-only-at-arcadis-3701463917,2023-12-17,Phoenix,United States,Mid senior,Onsite,"Arcadis is the world's leading company delivering sustainable design, engineering, and consultancy solutions for natural and built assets.
We are more than 36,000 people, in over 70 countries, dedicated to improving quality of life. Everyone has an important role to play. With the power of many curious minds, together we can solve the world’s most complex challenges and deliver more impact together.
Role description:
Note: See below regarding the nature of this position being a prospecting position.
Arcadis is currently seeking Analysts and Junior Management Consultants to join our world-class Business Advisory practice nationwide.
We are looking for candidates who want to apply technical know-how, combined with business principles, to the water, wastewater, and stormwater industry. We want dedicated, creative, and energetic candidates interested in tackling challenges and developing sustainable solutions to address water issues like renewal and replacement of aging infrastructure, funding of capital improvements, water supply, workforce retention and development, and emergency preparedness. Collaborating with our experienced consulting professionals, you will support and contribute to project outcomes; interact, and work with clients, and develop your technical capabilities.
We are a People First company, industry thought leaders, and drivers and allies of utility innovation.
Our passion: to Improve Quality of Life.
Our approach: to delight our clients by developing successful long-term partnerships and supporting them to address existing and emerging challenges.
Arcadis provides multiple onboarding and development programs created for young professionals that support professional growth and help drive creativeness, innovation, and greater integration within our local, National and global teams.
Role accountabilities:
What will you do?
Assess, develop, and support a variety of management consultant projects including performing data analytics, financial analysis, operational and organizational assessments, condition assessments, vulnerability, and mitigation assessments, as well as planning and development for utilities, municipalities, and cities’ (primarily water/wastewater/stormwater utilities).
Utilize strong analytical skills and ability to apply logic to solve problems.
Support teams in tasks ranging from general fieldwork to technical office-based analysis.
Assist in technical writing which may include preparation of technical reports, business development support, presentations, and other audiovisual materials.
Work independently and as part of a team, with the flexibility to accommodate collaboration with team members across the U.S. and internationally.
Manage multiple concurrent projects with multiple deadlines, ensuring completion per project budgets and timelines.
What skills will you need?
Reliable, client-focused, and capable of working independently under the supervision of project managers.
Exceptional analytical and problem-solving skills, strong attention to detail, organization skills, and work ethic.
Self-motivated and team-oriented, with the ability to work successfully both independently and within a team.
Ability to balance and address new challenges as they arise and an eagerness to take ownership of tasks.
Knowledge of engineering concepts, theories, and practices related to water/wastewater/stormwater.
Drive to succeed and grow a career in the utility industry
Qualifications & Experience:
Required Qualifications:
Masters of Science degree in Civil or Environmental Engineering, or closely related STEM discipline; or business analytics/MBA, MS in data science or related business discipline.
For those with engineering degrees, ability to obtain the EIT within six months of start date
Preferred Qualifications:
Previous relevant consulting or utility experience, either internship or full-time.
Experience applying programming languages and analytics to problem-solving is a plus
SharePoint, Building Information Modeling (BIM), Power BI, Excel, PowerPoint, Visio, Change Management skills, and/or Augmented Reality experience
This is a general job posting and not tied to a specific current open position. Please make sure you create a search agent to be alerted of specific opportunities of interest. Candidates who submit their resume to this posting may be considered for all future openings as they arise.
Why Arcadis?
We can only achieve our goals when everyone is empowered to be their best. We believe everyone's contribution matters. It’s why we are pioneering a skills-based approach, where you can harness your unique experience and expertise to carve your career path and maximize the impact we can make together.
You’ll do meaningful work, and no matter what role, you’ll be helping to deliver sustainable solutions for a more prosperous planet. Make your mark, on your career, your colleagues, your clients, your life and the world around you.
Together, we can create a lasting legacy.
Join Arcadis. Create a Legacy.
Our Commitment to Equality, Diversity, Inclusion & Belonging
We want you to be able to bring your best self to work every day which is why we take equality and inclusion seriously and hold ourselves to account for our actions. Our ambition is to be an employer of choice and provide a great place to work for all our people. We are an equal opportunity and affirmative action employer. Women, minorities, people with disabilities and veterans are strongly encouraged to apply. We are dedicated to a policy of non-discrimination in employment on any basis including race, creed, color, religion, national origin, sex, age, disability, marital status, sexual orientation, gender identity, citizenship status, disability, veteran status, or any other basis prohibited by law.
Arcadis offers benefits for full time and part time positions. These benefits include medical, dental, and vision, EAP, 401K, STD, LTD, AD&D, life insurance, paid parental leave, reward & recognition program and optional benefits including wellbeing benefits, adoption assistance and tuition reimbursement. We offer seven paid holidays and potentially up to two floating holidays per calendar year depending on start date, and 15 days PTO that accrue per year. The salary range for this position is $52000 - 89700 / year.
#ANACollege
Show more
Show less","Engineering, Civil Engineering, Environmental Engineering, Water/Wastewater/Stormwater Utilities, Data Analytics, Financial Analysis, Operational and Organizational Assessments, Condition Assessments, Vulnerability and Mitigation Assessments, Planning and Development, Microsoft SharePoint, Building Information Modeling (BIM), Power BI, Microsoft Excel, Microsoft PowerPoint, Microsoft Visio, Change Management, Augmented Reality","engineering, civil engineering, environmental engineering, waterwastewaterstormwater utilities, data analytics, financial analysis, operational and organizational assessments, condition assessments, vulnerability and mitigation assessments, planning and development, microsoft sharepoint, building information modeling bim, power bi, microsoft excel, microsoft powerpoint, microsoft visio, change management, augmented reality","augmented reality, building information modeling bim, change management, civil engineering, condition assessments, dataanalytics, engineering, environmental engineering, financial analysis, microsoft excel, microsoft powerpoint, microsoft sharepoint, microsoft visio, operational and organizational assessments, planning and development, powerbi, vulnerability and mitigation assessments, waterwastewaterstormwater utilities"
Senior Data Warehouse Developer,arrivia,"Scottsdale, AZ",https://www.linkedin.com/jobs/view/senior-data-warehouse-developer-at-arrivia-3776691940,2023-12-17,Phoenix,United States,Mid senior,Onsite,"Job Details
Description
As a Senior Data Warehouse Developer, you will design and maintain integration services packages, while helping us evolve to a modern, real-time data pipeline. You are a product-minded, customer-focused, data platform engineer who can help take ideas from prototype to launch. You propose, prototype, design and implement various components of the platform in collaboration with teams. As an important member of the team, you will provide the thought leadership and technical expertise needed to overcome hard problems. By now, you would have built and supported critical data integrations at scale. You will work in a highly collaborative, fast-paced environment that crosses multiple technology domains, business teams, and offices.
Responsibilities
Architect, design, and develop data integrations using integration services, while helping us move toward a real-time, event based data pipeline.
Create and maintain database objects in our data warehouse.
Problem-solve issues around data integration, unusable data elements, unstructured data sets, and other data management incidents
Research opportunities for data acquisition and new uses for existing data
Integrate new data management technologies and software engineering tools into existing structures
Create and maintain data management documentation, such as data dictionary.
Recommend ways to ensure continuous improvement around data reliability, efficiency and quality
Collaborate with data architects, modelers and IT team members on project requirements and goals
Revitalize services and architectures which have outlasted their original implementations, through refactoring, migration, and cloud-enabled distributed architectural design
Contribute to improving development and operations of an increasingly distributed architecture
Establish and help us meet backend and system SLOs to protect us through future growth
Requirements
Bachelor’s degree in Computer Science, Computer Engineering, or equivalent experience
6+ years general experience designing, building, and ingesting data into data warehouse systems
6+ years experience working with data pipelines, ingesting data from various sources into a data warehouse
3+ years experience with Microsoft Integration Services (SSIS)
3+ years experience ingesting data from event driven architecture
Experience with cloud architecture and data repositories
Experience processing streaming data
Experience in data cleansing and optimization for data consumption
Experience with Jira, ADO, Git
Experience with agile methodologies
Welcome to
arrivia
. We specialize in making brands better through the power of travel. With more than 55 years of combined experience, we’re a merger of three powerhouse brands (in case you’ve heard of us in the travel industry) combining ICE, SOR Technology and WMPH Vacations. With offices on both coasts of the US and around the world, we embrace diversity and a passion for travel across our global staff.
We’re focused on building a customer-first culture, fueled by the best travel experiences for all our members at every point in their journey. Grow with us, as we continue our path to deliver innovative solutions and take charge of change. The adventure is only beginning. We’re on a mission to help people around the world travel better and experience more. Our team members bring world-class skills to the table to create extraordinary memories for our partners and members.
Here at
arrivia
we…
Stay Curious - Explore new challenges and make space to learn, grow and improve
Keep it Real - Earn trust through open, honest and clear communication
Own it - Seek ways to make an impact and take action.
Win Together - Create a culture of connection and inclusion where everyone can be their best service, or other non-merit factor.
Show more
Show less","Data Warehousing, Data Integration, EventDriven Architecture, Cloud Architecture, Data Repositories, Data Cleansing, Data Optimization, Agile Methodologies, Microsoft Integration Services (SSIS), Jira, ADO, Git, Data Dictionary, Data Acquisition, Data Consumption","data warehousing, data integration, eventdriven architecture, cloud architecture, data repositories, data cleansing, data optimization, agile methodologies, microsoft integration services ssis, jira, ado, git, data dictionary, data acquisition, data consumption","ado, agile methodologies, cloud architecture, data acquisition, data consumption, data dictionary, data integration, data optimization, data repositories, datacleaning, datawarehouse, eventdriven architecture, git, jira, microsoft integration services ssis"
Staff Data Engineer,Recruiting from Scratch,"Phoenix, AZ",https://www.linkedin.com/jobs/view/staff-data-engineer-at-recruiting-from-scratch-3744394703,2023-12-17,Phoenix,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our client is a dating app.
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, deliver analysis to further improve engagement in existing features, and empower our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Solve technical problems of the highest scope and complexity
Exert significant influence on the company’s analytical long-range goals and data architecture
Define and extend our internal standards for style, maintenance, and best practices for a high-scale data platform
Provide mentorship for all on your team to help them grow in their technical responsibilities
Propose ideas to improve the scale, performance, and capabilities of the Data Platform
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We'll Love About You
7+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field
7+ years experience using Python
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention
Location
: This role will be located in office twice a week minimum in Palo Alto, San Francisco or Chicago.
Salary Range: $180,000 - 250,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data engineering, Business intelligence, Data science, Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, SQL, TDD, Pair Programming, Continuous Integration, Kafka, Storm, SparkStreaming, Data Warehouses, ETL, Data classification, Data retention","data engineering, business intelligence, data science, python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, tdd, pair programming, continuous integration, kafka, storm, sparkstreaming, data warehouses, etl, data classification, data retention","airflow, business intelligence, continuous integration, data classification, data engineering, data retention, data science, data warehouses, docker, etl, helm, kafka, kubernetes, pair programming, python, snowflake, spark, sparkstreaming, sql, storm, tdd"
Lead ML Data engineer,Diamondpick,"Phoenix, AZ",https://www.linkedin.com/jobs/view/lead-ml-data-engineer-at-diamondpick-3647232550,2023-12-17,Phoenix,United States,Mid senior,Onsite,"Job Title - Bigdata Engineer
Location - Phoenix, Arizona
Job Description
:
Responsible for designing system solutions, developing custom applications, and modifying existing applications to meet distinct and changing business requirements. Handle coding, debugging, and documentation, as well working closely with SRE team. Provide post implementation and ongoing production support
Develop and design software applications, translating user needs into system architecture. Assess and validate application performance and integration of component systems and provide process flow diagrams. Test the engineering resilience of software and automation tools.
You will be challenged with identifying innovative ideas and proof of concept to deliver against the existing and future needs of our customers. Software Engineers who join our Loyalty Technology team will be assigned to one of several exciting teams that are developing a new, nimble, and modern loyalty platform which will support the key element of connecting with our customers where they are and how they choose to interact with American Express.
Be part of an enthusiastic, high performing technology team developing solutions to drive engagement and loyalty within our existing cardmember base and attract new customers to the Amex brand.
The position will also play a critical role partnering with other development teams, testing and quality, and production support, to meet implementation dates and allow smooth transition throughout the development life-cycle.
The successful candidate will be focused on building and executing against a strategy and roadmap focused on moving from monolithic, tightly coupled, batch-based legacy platforms to a loosely coupled, event-driven, microservices-based architecture to meet our long-term business goals.
Must Have Qualifications:
Bachelors degree in Engineering or Computer Science or equivalent OR Masters in Computer Applications or equivalent.
7-10+ years of software development experience and leading teams of engineers and scrum teams
4+ years of experience in applying Statistics along with end to end ML engineering (design, development & implementation of end-to-end AI/ML models)
Hands-on experience on writing and understanding complex SQL(Hive/PySpark-dataframes), optimizing joins while processing huge amount of data
Good understanding of various AI/ML models including Classification, Clustering, Regression in detecting Product anomalies and building early warning systems
Expertise with data structures, data modeling, and software architecture
Good to have Qualifications:
Expert on Hadoop and Spark Architecture and its working principle
Experience in UNIX shell scripting
Ability to design and develop optimized Data pipelines for batch and real time data processing
Should have experience in analysis, design, development, testing, and implementation of system applications
Demonstrated ability to develop and document technical and functional specifications and analyze software and system processing flows
Aptitude for learning and applying programming concepts.
Ability to effectively communicate with internal and external business partners. Preferred Additional:
Experience in cloud platforms like GCP/AWS, building Microservices and scalable solutions is highly desired
Knowledge of Financial reporting ecosystem will be a plus
2+ years of experience in designing and building solutions using Kafka streams or queues
Experience with GitHub and leveraging CI/CD pipelines
Experience with NoSQL i.e., HBase, Couchbase, MongoDB
Show more
Show less","Data pipelines, Data structures, Data modeling, Software architecture, Hadoop, Spark, UNIX shell scripting, SQL, Hive, PySparkdataframes, Artificial intelligence, Machine learning, Classification, Clustering, Regression, Microservices, GCP, AWS, Kafka streams, Queues, CI/CD pipelines, GitHub, NoSQL, HBase, Couchbase, MongoDB, Cloud platforms","data pipelines, data structures, data modeling, software architecture, hadoop, spark, unix shell scripting, sql, hive, pysparkdataframes, artificial intelligence, machine learning, classification, clustering, regression, microservices, gcp, aws, kafka streams, queues, cicd pipelines, github, nosql, hbase, couchbase, mongodb, cloud platforms","artificial intelligence, aws, cicd pipelines, classification, cloud platforms, clustering, couchbase, data structures, datamodeling, datapipeline, gcp, github, hadoop, hbase, hive, kafka streams, machine learning, microservices, mongodb, nosql, pysparkdataframes, queues, regression, software architecture, spark, sql, unix shell scripting"
Senior Database Engineer,Carvana,"Tempe, AZ",https://www.linkedin.com/jobs/view/senior-database-engineer-at-carvana-3780233754,2023-12-17,Phoenix,United States,Mid senior,Onsite,"About Carvana
If you like disrupting the norm and are looking to join a company revolutionizing an industry then you will LOVE what Carvana has done for the car buying experience. Buying a car the old fashioned way sucks and we are working hard to make it NOT suck. At Carvana, our customers can hop online to...
Search and browse our inventory of over 40,000 vehicles that we own and certify
Narrow down search results using highly intelligent filtering tools/components
View vehicle details, Carfax reports and 360 rotating studio images for every vehicle
Secure financing in minutes using Carvana’s in house service or their own bank
Interact with GUI components to easily customize loan length, down payment and monthly payment
Generate, upload and eSign all documents online (no ink necessary)
Schedule front door delivery or pick up at one of our vending machines
Trade in their existing vehicle or just sell it to Carvana (no purchase necessary)
About The Team And Position
We are seeking an experienced and highly skilled Senior Database Engineer to join our innovative Engineering Data Platform team.
In this role, you will manage both Elasticsearch and Azure Databricks environments, including self-managed and Elastic Cloud clusters. Your responsibilities will encompass creating, administering, maintaining, troubleshooting, and managing users for both systems. You will also provide consultation to our search and data developers. If you have a strong background in Azure deployments, cluster management, and have a sharp problem-solving mindset, we invite you to apply.
What You’ll Be Doing
Help architect, design, and prototype solutions to support business strategies and deliver business value.
Recommend and implement Azure Cloud High Availability and Disaster Recovery configurations and procedures.
Become familiar with the multitude of ways data flows through our various systems in order to contribute creative problem-solving solutions.
Utilize development, test and production environments, adhering to change management requirements for system implementations.
Communicate regularly with technical, application and operational staff to ensure database integrity and security.
Assist teams with their database provisioning by design, establishing, administering, support and maintaining both Elasticsearch and Azure Databricks environments, including self-managed and Elastic Cloud clusters.
Collaborate closely with our search and data developers to address technical requirements, provide consultation, and ensure optimal performance of both systems.
Manage user access, rights, and security settings across both systems in accordance with company standards and policies.
Minimize downtime for Databricks by promptly troubleshooting and resolving system issues. Optimize job execution, use data warehousing features effectively, automate workflows, create dynamic dashboards, encourage collaboration, implement Delta Lake benefits, and explore streaming analytics for efficient responsibilities.
Implement effective alerting strategies to detect system anomalies and potential issues across both environments.
Provide comprehensive support for Azure deployments, including planning, setup, and ongoing management.
Develop and maintain documentation and guidelines related to the Databricks solution, including architecture diagrams, standards, and processes.
Participate in lunch and learns, design sessions, and code reviews.
Other duties as assigned.
Requirements
What you should know
Bachelor's degree in Management Information Systems, Engineering, Computer Science, or related field.
5 years of experience in Data Engineering, Analytics, Administration or similar occupation.
Proven experience as an Elasticsearch and Azure Databricks Administrator, with demonstrable expertise in managing both self-managed and Elastic Cloud clusters.
Strong knowledge of Azure deployments and familiarity with cluster management tools.
Experience with relational and non-relational databases
Experience/proficiency with and reviewing query plans
Understand basic indexing concepts and difference between clustered/non-clustered
Experience with performance tuning and process streamlining and system troubleshooting, issue resolution, and implementing robust alerting systems.
Excellent communication skills to collaborate effectively and provide consultative services to the development teams.
A problem-solving mindset, with the ability to diagnose and fix issues promptly to ensure system reliability and uptime.
What We’ll Offer In Return
Full-Time Salary Position with a competitive salary.
Medical, Dental, and Vision benefits.
401K with company match.
A multitude of perks including student loan payments, discounts on vehicles, benefits for your pets, and much more.
A great wellness program to keep you healthy and happy both physically and mentally.
Access to opportunities to expand your skill set and share your knowledge with others across the organization.
A company culture of promotions from within, with a start-up atmosphere allowing for varied and rapid career development.
A seat in one of the fastest-growing companies in the country.
Other Requirements
To be able to do your job at Carvana, there are some basic requirements we want to share with you.
Must be able to read, write, speak and understand English.
Of course, we’ll make any reasonable accommodations for those with disabilities to perform the essential functions of their jobs.
Legal stuff
Hiring is contingent on passing a complete background check. This role is eligible for visa sponsorship.
Carvana is an equal employment opportunity employer. All applicants receive consideration for employment without regard to race, color, religion, gender, sexual orientation, gender identity or expression, marital status, national origin, age, mental or physical disability, protected veteran status, or genetic information, or any other basis protected by applicable law. Carvana also prohibits harassment of applicants or employees based on any of these protected categories.
Please note this job description is not designed to contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.
Show more
Show less","Elasticsearch, Azure Databricks, SQL, Azure Cloud, Data warehousing, Delta Lake, Streaming analytics, Data Engineering, Data Analytics, Data Administration, Index, Cluster management, NoSQL, GUI, Frontend development, Backend development, Change management, System troubleshooting, Issue resolution, Alerting systems, Performance tuning, Process streamlining","elasticsearch, azure databricks, sql, azure cloud, data warehousing, delta lake, streaming analytics, data engineering, data analytics, data administration, index, cluster management, nosql, gui, frontend development, backend development, change management, system troubleshooting, issue resolution, alerting systems, performance tuning, process streamlining","alerting systems, azure cloud, azure databricks, backend development, change management, cluster management, data administration, data engineering, dataanalytics, datawarehouse, delta lake, elasticsearch, frontend development, gui, index, issue resolution, nosql, performance tuning, process streamlining, sql, streaming analytics, system troubleshooting"
Data center electrical design engineer,Ascendion,"Seattle, WA",https://www.linkedin.com/jobs/view/data-center-electrical-design-engineer-at-ascendion-3774849574,2023-12-17,Tacoma,United States,Mid senior,Remote,"About the job
About Ascendion
Ascendion is a full-service digital engineering solutions company. We make and manage software platforms and products that power growth and deliver captivating experiences to consumers and employees. Our engineering, cloud, data, experience design, and talent solution capabilities accelerate transformation and impact for enterprise clients. Headquartered in New Jersey, our workforce of 6,000+ Ascenders delivers solutions from around the globe. Ascendion is built differently to engineer the next.
Ascendion | Engineering to elevate life
We have a culture built on opportunity, inclusion, and a spirit of partnership. Come, change the world with us:
Build the coolest tech for world’s leading brands
Solve complex problems - and learn new skills
Experience the power of transforming digital engineering for Fortune 500 clients
Master your craft with leading training programs and hands-on experience
Experience a community of change makers!
Join a culture of high-performing innovators with endless ideas and a passion for tech. Our culture is the fabric of our company, and it is what makes us unique and diverse. The way we share ideas, learning, experiences, successes, and joy allows everyone to be their best at Ascendion.
About the Role:
We are seeking a skilled and knowledgeable Electrical Engineer to join our team. As an Electrical Engineer, you will be responsible for installing, maintaining, and repairing electrical systems. You will also be required to troubleshoot and diagnose electrical problems, as well as ensure compliance with electrical codes and standards.
Job Title: Data Center Electrical Design Engineer
Responsibilities:
Read and interpret blueprints, schematics, and diagrams.
Use electrical test equipment proficiently.
Troubleshoot and diagnose electrical problems.
Install, maintain, and repair electrical systems.
Ensure compliance with electrical codes and standards.
Have knowledge of power distribution systems.
Install and maintain UPS systems.
Have knowledge of data center cooling systems.
Follow electrical safety procedures.
Required Skills:
Knowledge of electrical engineering principles and practices.
Ability to read and interpret blueprints, schematics, and diagrams.
Proficiency in the use of electrical test equipment.
Familiarity with electrical codes and standards.
Ability to troubleshoot and diagnose electrical problems.
Knowledge of power distribution systems.
Ability to install, maintain, and repair electrical systems.
Knowledge of data center cooling systems.
Ability to install and maintain UPS systems.
Knowledge of electrical safety procedures.
Desirable Skills:
Experience working in a similar role.
Strong problem-solving skills.
Excellent attention to detail.
Effective communication skills.
Ability to work well in a team.
Location:
Remote,
prefereble in Seattle and herndon, VA
Salary Range:
The salary for this position is between
110K - 150K
annually. Factors that may affect pay within this range may include geography/market, skills, education, experience, and other qualifications of the successful candidate.
Benefits
: The Company offers the following benefits for this position, subject to applicable eligibility requirements: [medical insurance] [dental insurance] [vision insurance] [401(k) retirement plan] [long-term disability insurance] [short-term disability insurance] [5 personal days accrued each calendar year. The Paid time off benefits meet the paid sick and safe time laws that pertain to the City/ State] [10-15 days of paid vacation time] [6 paid holidays and 1 floating holiday per calendar year] [Ascendion Learning Management System]
Want to change the world? Let us know.
Tell us about your experiences, education, and ambitions. Bring your knowledge, unique viewpoint, and creativity to the table. Let’s talk!
Show more
Show less","Electrical engineering, Electrical test equipment, UPS systems, Data center cooling, Troubleshooting, Electrical safety, Microsoft Office Suite, AutoCAD, Power distribution systems","electrical engineering, electrical test equipment, ups systems, data center cooling, troubleshooting, electrical safety, microsoft office suite, autocad, power distribution systems","autocad, data center cooling, electrical engineering, electrical safety, electrical test equipment, microsoft office suite, power distribution systems, troubleshooting, ups systems"
"Data Conversion Developer, Senior Associate",PwC,"Seattle, WA",https://www.linkedin.com/jobs/view/data-conversion-developer-senior-associate-at-pwc-3749938489,2023-12-17,Tacoma,United States,Mid senior,Remote,"Specialty/Competency:
Functional & Industry Technologies
Industry/Sector:
Not Applicable
Time Type:
Full time
Travel Requirements:
Up to 80%
A career within Functional and Industry Technologies services will provide you with the opportunity to build secure and new digital experiences for customers, employees, and suppliers. We focus on improving apps or developing new apps for traditional and mobile devices as well as conducting usability testing to find ways to improve our clients’ user experience. Our team helps clients transform their business through enabling technologies across marketing, finance and operations in the functional areas such as Maximo and PowerPlant.
To really stand out and make us fit for the future in a constantly changing world, each and every one of us at PwC needs to be a purpose-led and values-driven leader at every level. To help us achieve this we have the PwC Professional; our global leadership development framework. It gives us a single set of expectations across our lines, geographies and career paths, and provides transparency on the skills we need as individuals to be successful and progress in our careers, now and in the future.
Responsibilities
As a Senior Associate, you'll work as part of a team of problem solvers, helping to solve complex business issues from strategy to execution. PwC Professional skills and responsibilities for this management level include but are not limited to:
Use feedback and reflection to develop self awareness, personal strengths and address development areas.
Delegate to others to provide stretch opportunities, coaching them to deliver results.
Demonstrate critical thinking and the ability to bring order to unstructured problems.
Use a broad range of tools and techniques to extract insights from current industry or sector trends.
Review your work and that of others for quality, accuracy and relevance.
Know how and when to use tools available for a given situation and can explain the reasons for this choice.
Seek and embrace opportunities which give exposure to different situations, environments and perspectives.
Use straightforward communication, in a structured way, when influencing and connecting with others.
Able to read situations and modify behavior to build quality relationships.
Uphold the firm's code of ethics and business conduct.
Basic Qualifications
Minimum Degree Required:
Bachelor Degree
Minimum Years Of Experience
4 years
Preferred Qualifications
Degree Preferred:
Master Degree
Certification(s) Preferred
Azure Data Engineer Associate
Databricks Certified Data Engineer Associate
Preferred Fields Of Study
Computer and Information Science, Computer Engineering, Computer Management, Management Information Systems, Information Technology
Preferred Knowledge/Skills
Demonstrates a thorough level of abilities with, and/or a proven record of success as both an individual contributor and team member, identifying and addressing client needs:
Supports in data analysis techniques to assess source data structures, identify mapping requirements, and define transformation rules for data conversion into Maximo;
Leads Maximo's modules and functionalities related to Asset Management and Work Order Management and IBM Maximo, including its data structures, configuration settings, and integration capabilities;
Identifies relational databases, preferably experience in working with databases commonly used in Maximo, such as IBM DB2, Oracle, Microsoft SQL Server, along with familiarity with Maximo's Integration Framework (MIF) and its capabilities for data integration and conversion;
Supports in designing and implementing data extraction, transformation, and loading processes for Maximo data conversion;
Showcases understanding in SQL and database querying languages to extract and manipulate data from source systems along with understanding ETL tools and methodologies commonly used in Maximo data conversion;
Identifies technologies commonly used with Maximo, including web services (SOAP, RESTful APIs), XML, JSON, and other relevant data exchange formats;
Supports in pipeline architecture and development using one of the tools such as Azure ADF, AWS Glue, SSIS, DataBricks (multiple preferred);
Utilizes data cleansing techniques and methodologies to ensure the integrity and accuracy of converted data in Maximo;
Developes data cleansing functional business rules as per Maximo Business Object (MBO) definitions for source to Maximo conversion requirements;and comprehensive testing plans and executing validation processes to verify the accuracy and integrity of converted data in Maximo;
Customizes Maximo options, such as Automation Scripts, Java Customizations, Database Configuration, or Application Designer, to support data conversion requirements;
Identifies integrations within Maximo with other enterprise systems, such as ERP systems, GIS systems, or asset management systems. Knowledge of integration patterns, data synchronization, and data exchange protocols; and,
Showcases work experience as a Data Engineer, Data Architect or similar role, along with experience in programming languages including Python, PySpark, Scala, SQL.
Learn more about how we work: https://pwc.to/how-we-work
PwC does not intend to hire experienced or entry level job seekers who will need, now or in the future, PwC sponsorship through the H-1B lottery, except as set forth within the following policy: https://pwc.to/H-1B-Lottery-Policy.
All qualified applicants will receive consideration for employment at PwC without regard to race; creed; color; religion; national origin; sex; age; disability; sexual orientation; gender identity or expression; genetic predisposition or carrier status; veteran, marital, or citizenship status; or any other status protected by law. PwC is proud to be an affirmative action and equal opportunity employer.
For positions based in San Francisco, consideration of qualified candidates with arrest and conviction records will be in a manner consistent with the San Francisco Fair Chance Ordinance.
Applications will be accepted until the position is filled or the posting is removed, unless otherwise set forth on the following webpage. Please visit this link for information about anticipated application deadlines: https://pwc.to/us-application-deadlines
For positions in California, Colorado, Hawaii, Nevada, New York State, or Washington State, or for opportunities that will report to a supervisor, office or other work site in New York State, please visit the following link for pay range information: https://pwc.to/payrange-v1-advisoryseniorassociate
Show more
Show less","Maximo, PowerPlant, Azure Data Engineer Associate, Databricks Certified Data Engineer Associate, Computer and Information Science, Computer Engineering, Computer Management, Management Information Systems, Information Technology, Data analysis techniques, Maximo's modules and functionalities, Asset Management, Work Order Management, IBM Maximo, Data structures, Configuration settings, Integration capabilities, Relational databases, IBM DB2, Oracle, Microsoft SQL Server, Maximo's Integration Framework (MIF), Data integration, Data conversion, SQL, Database querying languages, ETL tools, Methodologies, Maximo data conversion, Web services (SOAP RESTful APIs), XML, JSON, Data exchange formats, Pipeline architecture, Development, Azure ADF, AWS Glue, SSIS, DataBricks, Data cleansing techniques, Methodologies, Data integrity, Data accuracy, Maximo Business Object (MBO) definitions, Source to Maximo conversion requirements, Comprehensive testing plans, Validation processes, Maximo options, Automation Scripts, Java Customizations, Database Configuration, Application Designer, Data conversion requirements, Integrations, Enterprise systems, ERP systems, GIS systems, Asset management systems, Integration patterns, Data synchronization, Data exchange protocols, Data Engineer, Data Architect, Programming languages, Python, PySpark, Scala","maximo, powerplant, azure data engineer associate, databricks certified data engineer associate, computer and information science, computer engineering, computer management, management information systems, information technology, data analysis techniques, maximos modules and functionalities, asset management, work order management, ibm maximo, data structures, configuration settings, integration capabilities, relational databases, ibm db2, oracle, microsoft sql server, maximos integration framework mif, data integration, data conversion, sql, database querying languages, etl tools, methodologies, maximo data conversion, web services soap restful apis, xml, json, data exchange formats, pipeline architecture, development, azure adf, aws glue, ssis, databricks, data cleansing techniques, methodologies, data integrity, data accuracy, maximo business object mbo definitions, source to maximo conversion requirements, comprehensive testing plans, validation processes, maximo options, automation scripts, java customizations, database configuration, application designer, data conversion requirements, integrations, enterprise systems, erp systems, gis systems, asset management systems, integration patterns, data synchronization, data exchange protocols, data engineer, data architect, programming languages, python, pyspark, scala","application designer, asset management, asset management systems, automation scripts, aws glue, azure adf, azure data engineer associate, comprehensive testing plans, computer and information science, computer engineering, computer management, configuration settings, data accuracy, data analysis techniques, data architect, data cleansing techniques, data conversion, data conversion requirements, data exchange formats, data exchange protocols, data integration, data integrity, data structures, data synchronization, database configuration, database querying languages, databricks, databricks certified data engineer associate, dataengineering, development, enterprise systems, erp systems, etl tools, gis systems, ibm db2, ibm maximo, information technology, integration capabilities, integration patterns, integrations, java customizations, json, management information systems, maximo, maximo business object mbo definitions, maximo data conversion, maximo options, maximos integration framework mif, maximos modules and functionalities, methodologies, microsoft sql server, oracle, pipeline architecture, powerplant, programming languages, python, relational databases, scala, source to maximo conversion requirements, spark, sql, ssis, validation processes, web services soap restful apis, work order management, xml"
"Principal Software Engineer, Data Platform - Remote",Clari,"Seattle, WA",https://www.linkedin.com/jobs/view/principal-software-engineer-data-platform-remote-at-clari-3740549932,2023-12-17,Tacoma,United States,Mid senior,Remote,"Clari’s Revenue platform gives forecasting accuracy and visibility from the sales rep to the board room on revenue performance - helping them spot revenue leak to answer if they will meet, beat, or miss their sales goals. With insights like this, no wonder leading companies worldwide, including Okta, Adobe, Workday, and Zoom use Clari to drive revenue accuracy and precision. We never get tired of our customers singing our praises because it fuels us to help them continue to achieve remarkable. The next generation of revenue excellence is here…are you ready to achieve remarkable with us?
About The Team
The Engineering team at Clari is an Agile shop that practices Scrum across all of our teams. We layer in coordination practices such as Big Room Planning to stay aligned to Clari’s KPIs quarterly across sites and teams. If you love working in an Agile environment that values collaboration and continuous improvement then we can’t wait to meet you.
About The Role
We are looking for a talented Principal Software Engineer to join our Query Manager team. Query Manager is a part of Clari’s Data Platform team, and is the interface that allows application and API developers to easily and efficiently retrieve data across hundreds of databases and billions of rows of data that comprise our ever-evolving Data Platform.
You will work with remarkable colleagues to architect, build and optimize the query layer to derive exceptional performance from our data warehouse built on top of AWS Aurora Postgres. You will collaborate closely with the product management, architecture, application and infrastructure teams to build the data services that power our best-in-class enterprise product suite. Most of Clari’s application and API queries are processed through the query manager layer. The products you build are used and loved by many of the most well-known companies in the world. Don’t believe us? Hear what our customers have to say
Come join this fluid, dynamic, and growing team to learn, teach, and make a big, measurable impact every day. We work in an open, collaborative environment and seek exceptional developers who enjoy problem-solving and straying outside their routine.
This is a fully remote opportunity and can be worked from any location in the United States.
Responsibilities
Design and evolve the architecture for the query layer that powers Clari’s product suite and platform
Learn and contribute to all aspects of the data platform, from extracting and ingesting data from external systems to modeling, transforming, and managing large volumes of data at rest and in motion
Mentor junior engineers to set and maintain high standards of engineering excellence while helping to grow their careers
Write scalable, robust, and fully tested software for deployment in mission-critical production environments
Create and improve tooling and processes to help reduce development friction and enable greater productivity across the development organization
Contribute to the growth of Clari by being a brand ambassador and assisting in the hiring of great talent
Qualifications
10+ years of software development experience using Java or similar object-oriented languages for backend development
Deep expertise with relational database skills and concepts
Experience having led multiple projects from inception through deployment, maintenance, and support
Experience with Postgres and non-relational databases like MongoDB
Experience with AWS
Experience building scalable systems and architectures
Experience with database performance tuning
Perks and Benefits @ Clari
Remote-first with opportunities to work and celebrate in person
Medical, dental, vision, short & long-term disability, Life insurance, and EAP
Mental health support provided by Modern Health
Pre-IPO stock options
Well-being and professional development funds
Retirement 401(k) plan
100% paid parental leave, plus fertility and family planning support provided by Maven
Discretionary paid time off, monthly ‘take a break’ days, and Focus Fridays
Focus on culture: Charitable giving match, plus in-person and virtual events
It is Clari’s intent to pay all Clarians competitive wages and salaries that are motivational, fair, and equitable. The goal of Clari’s compensation program is to be transparent, attract potential employees, meet the needs of all current employees and encourage employees to stay and grow at Clari.
Actual compensation packages are based on several factors that are unique to each candidate, including but not limited to specific work location, skill set, depth of experience, education and certifications.
The salary range for this position is $191,300 to $286,900. The compensation package for this position also includes stock options and company-paid benefits, including well-being and professional development stipends.
#BI-Remote
You’ll often hear our CEO talk about being remarkable. To Clari, remarkable means many things. We believe in providing interesting and meaningful work in a nurturing and inclusive environment. One that is free from discrimination for everyone without regard to race, color, religion, sex, sexual orientation, national origin, age, disability, gender identity, or veteran status. Efforts have to be recognized. Voices have to be heard. And work-life balance has to be baked into the very fiber of the company. We are honored to be recognized by Inc. Magazine and Bay Area News Group as a best place to work for several years running. We’d love to have you join us on our journey to remarkable!
If you feel you don’t meet 100% of the qualifications outlined above, we want you to apply! Clari believes in hiring people, not just skills. If you are passionate about learning and excited about what we are doing, then we want to hear from you.
Clari focuses on culture add, not culture fit. One of our values is One with Customers, and we know we can serve them better when we involve as many different perspectives as possible. Our team is made stronger by what makes you unique, so we hope you’ll bring your whole self to the job.
Show more
Show less","Agile, Scrum, Big Room Planning, AWS Aurora Postgres, Java, Objectoriented programming, Relational database skills, Postgres, MongoDB, AWS, Scalable systems, Database performance tuning, Software development, Backend development","agile, scrum, big room planning, aws aurora postgres, java, objectoriented programming, relational database skills, postgres, mongodb, aws, scalable systems, database performance tuning, software development, backend development","agile, aws, aws aurora postgres, backend development, big room planning, database performance tuning, java, mongodb, objectoriented programming, postgres, relational database skills, scalable systems, scrum, software development"
Senior Data Engineer,Methods,"Sheffield, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-at-methods-3676187544,2023-12-17,Sheffield, United Kingdom,Mid senior,Onsite,"Methods Analytics
is currently recruiting for a
Senior
Data Engineer
to join our teamon a
permanent
basis. This role will require flexibility to travel to client sites, and is expected to be based in Sheffield or Bristol.
About Methods Analytics:
Methods Analytics exists to improve society by helping people make better decisions with data. Combining passionate people, sector-specific insight and technical excellence to provide our customers an end-to-end data service. We use a collaborative, creative and user centric approach data to do good and solve difficult problems. And ensure that our outputs are transparent, robust and transformative.
We value discussion and debate as part of our approach. We will question assumptions, ambition and process – but do so with respect and humility. We relish difficult problems, and overcome them with innovation, creativity and technical freedom to help us design optimum solutions. Ethics, privacy and quality are at the heart of our work and we will not sacrifice these for outcomes. We treat data with respect and use it only for the right purpose. Our people are positive, dedicated and relentless. Data is a vast topic, but we strive for interactions that are engaging, informative and fun in equal measure. But maintain a steely focus on outcomes and delivering quality products for our customers.
Requirements
The Senior Data Engineer will be:
Working with other members of the delivery team across a mix of large and small projects and be responsible for translating data into valuable insights that inform decisions for small to large transformation projects and programmes
Working with other members of the delivery team you will be responsible for identifying and using the most appropriate analytical techniques, developing fit-for-purpose, resilient, scalable and future-proof data services that meet user needs and design and write and iterate code from prototype to production-ready.
Communicating effectively across organisational, technical and political boundaries to understand the context and how to make complex and technical information and language simple and accessible for non-technical audiences.
Creating effective data visualisation, appropriate to the audience
Working with the Lead Data Engineer to support the growth and development of the team
Aware of, and keep up to date with advances in digital analytics tools and data manipulation products.
Collecting, collating, cleansing, synthesising and interpreting data to derive meaningful and actionable insights
Understanding of how to expose data from systems (for example, through APIs), link data from multiple systems and deliver streaming services
Producing data models and understand where to use different types of data models.
Provide training, support and mentoring to a team of Data Engineers and helping to grow the Engineering team
Ensuring that risks associated with deployment are adequately understood and documented
Ideal Candidates will demonstrate:
Experience of creating PowerBI solutions and dashboards from row level data including data structure optimisation through to visualisation and dashboard creation
Strong T-SQL Development including debugging & troubleshooting
Experience in SQL Server Integration Services (SSIS)
Solid Relational Database design skills with an eye for performance optimisation
an ability to translate business requirements into technical specifications
Attention to detail and ability to QA own and other team member's work
Good experience with ETL
Infrastructure, Azure Data Factory / SSIS, SQL (On-prem/Cloud),
Good experience with Analytical/Reporting
SQL (On-prem/Cloud), Analysis Services / Tabular Data Model, Power BI/Tableau/T-SQL
Understanding of analytical tools; you are numerate.
Desirable Skills & Experience;
Experience with NoSQL type environments, Data Lakes, Lake-Houses (Cassandra, MongoDB or Neptune)
Have experience with Python, Scala or Java
Have cloud based experience, preferably with AWS and/or Azure
Knowledge of statistics principles necessary to interpret data and apply models. For example, knowledge of errors and confidence intervals to understand whether a relation seen in the data is real
Exposure to high performing, low latency or large volume data systems (i.e. 1 billion+ records, terabyte size database)
Working within a continuous integration environment with automated builds, deployment and unit testing
Exposure to iterative/agile development methodologies such as SCRUM
Experience of working in a consultancy
Knowledge of Healthcare data, specifically NHS Data sources such as HES, SUS PbR
This role will require you to have or be willing to go through Security Clearance. As part of the onboarding process candidates will be asked to complete a Baseline Personnel Security Standard; details of the evidence required to apply may be found on the government website Gov.UK. If you are unable to meet this and any associated criteria, then your employment may be delayed, or rejected . Details of this will be discussed with you at interview
Benefits
Methods Analytics is passionate about its people; we want our colleagues to develop the things they are good at and enjoy.
By joining us you can expect
Autonomy to develop and grow your skills and experience
Be part of exciting project work that is making a difference in society
Strong, inspiring, and thought-provoking leadership
A supportive and collaborative environment
As well as this, we offer:
Development
access to LinkedIn Learning, a management development programme and training
Wellness
24/7 Confidential employee assistance programme
Social -
office parties, pizza Friday and commitment to charitable causes
Time off
25 days a year
Pension
Salary Exchange Scheme with 4% employer contribution and 5% employee contribution
Discretionary Company Bonus
based on company and individual performance
Life Assurance
of 4 times base salary
Private Medical Insurance
which is non-contributory
(spouse and dependants included)
Worldwide Travel Insurance
which is non-contributory
(spouse and dependants included)
Benefits Platform
offering various retail and leisure discounts
Show more
Show less","Data Engineering, Data Analytics, Data Manipulation, Data Visualization, PowerBI, TSQL, SQL Server Integration Services (SSIS), Relational Database Design, ETL, Azure Data Factory, SQL, Analytical/Reporting, Analysis Services / Tabular Data Model, Power BI, Tableau, Python, Scala, Java, AWS, Azure, Statistics, SCRUM, Healthcare Data, NHS Data, NoSQL, Data Lakes, LakeHouses, Cassandra, MongoDB, Neptune","data engineering, data analytics, data manipulation, data visualization, powerbi, tsql, sql server integration services ssis, relational database design, etl, azure data factory, sql, analyticalreporting, analysis services tabular data model, power bi, tableau, python, scala, java, aws, azure, statistics, scrum, healthcare data, nhs data, nosql, data lakes, lakehouses, cassandra, mongodb, neptune","analysis services tabular data model, analyticalreporting, aws, azure, azure data factory, cassandra, data engineering, data lakes, data manipulation, dataanalytics, etl, healthcare data, java, lakehouses, mongodb, neptune, nhs data, nosql, powerbi, python, relational database design, scala, scrum, sql, sql server integration services ssis, statistics, tableau, tsql, visualization"
Data Engineer - Arcadis Gen,Arcadis,"Sheffield, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-arcadis-gen-at-arcadis-3770620498,2023-12-17,Sheffield, United Kingdom,Mid senior,Onsite,"Role: Data Engineer
Location: UK, Remote
As an Arcadian, you already help us deliver world leading sustainable design, engineering, and consultancy solutions for natural and built assets. You are part of our global business comprising 36,000 people, in over 70 countries, dedicated to improving quality of life.
Everyone has an important role to play. With the power of many curious minds, together we can continue to solve the world’s most complex challenges and deliver more impact together.
Role description:
The Data Engineer is responsible for migrating the data systems, delivery of the technical leadership in data migration, support of data migration strategies, tools, design and build. As well as formulating various methods to transform raw data into useful data systems. Overall, you’ll strive for efficiency by aligning data systems with business goals. This includes data ETL and modelling pipelines, hosting environments, automated testing and deployment, information storage, and retrieval and support enabling infrastructure across multiple client projects.
The Data Engineer will execute migration projects and may be a coach to other Junior Engineers within the Service Engineering Group (SEG). They will work together with Data architects, Data Consultants, and QA Consultants to enable technical solutions throughout the project lifecycle, including assessments, tool evaluations, demonstrations, requirements gathering, results presentation.
Role accountabilities:
Support and own the end-to-end migration journey on cloud-based products.
Technical delivery of ETL data migration with framework agreed.
Execute data migration runs.
Perform data migration using Snowflake, Dbt and Airflow or similar toolset.
Analyse and organize raw data; build data systems and pipelines.
Assess the compatibility with your cloud destination.
Ensure that the design fully meets the client’s needs.
Manage outputs and quality throughout delivery of a project ensuring all work packages align to the project scope.
Qualifications & Experience:
The ideal candidate will have a background in computer science and/or data engineering, ideally within a data migration consultancy or physical asset owning company context. They will have good leadership, communication, and organisational skills. They will have a strong understanding of how to deploy and scale machine learning and advance analytics techniques in a modern cloud environment. A keen interest in the latest technologies is important, to help ensure that the product (as well as Gen in general) stays on the cutting edge.
More importantly than these technical skills, however, we are looking for someone to be a data solution leader, pushing forward the boundaries of what we do, and the standards expected of the wider Gen data architecture. The successful candidate will need to be resilient and maintain a positive attitude when faced with challenges, be an excellent problem-solver, and be happy to be out of their comfort zone. They will need to have excellent communication skills, not just at a technical level, but also with more business-focused colleagues. They will not shy away from a challenge and will stand up and be counted. They will never be satisfied, and always look to find the next improvement in what Gen does, and how we do it.
We are looking for someone who possesses:
Must haves:
Experience in cloud-based data migration tools and validations
Experience using AWS and/or Azure
Experience using the following tools: Snowflake & DBT
Excellent understanding of SQL
Familiarity with DevOps processes/best practices
Part of assessment, tool options and data migration enablement
Experience of delivery of client facing projects, ideally in a data migration consultancy
Knowledge of programming languages (e.g., Java and Python)
You have experience in the use of automated testing, CI/CD, the use and creation of APIs and of virtual package environments to ensure repeatability of code.
Demonstrable skills in database management
Desirable:
Experience using Airflow
Experience using Azure Data Factory
Continue your career journey as an Arcadian.
We can only achieve our goals when everyone is empowered to be their best. We believe everyone's contribution matters. It’s why we are pioneering a skills based approach, where you can harness your unique experience and expertise to carve your career path and maximize the impact we can make together.
You do meaningful work, and no matter where your next role in Arcadis takes you, you’ll be helping to deliver sustainable solutions for a more prosperous planet. Make your mark; on your career, your colleagues, your clients, your life and the world around you.
Together, we can create a lasting legacy.
Our Commitment to Equality, Diversity, Inclusion & Belonging
We want you to be able to bring your best self to work every day which is why we take equality and inclusion seriously and hold ourselves to account for our actions. Our ambition is to be an employer of choice and provide a great place to work for all our people. We believe that by working together diverse people with different experiences develop the most innovative ideas. Equality, diversity and inclusion is at the heart of how we improve quality of life and we work closely with our people across six ED&I Workstreams: Age, Disability, Faith, Gender, LGBT+ and Race. A diverse and skilled workforce is essential to our success.
Show more
Show less","Data engineering, Data migration, ETL pipelines, Data modelling, Cloudbased data migration, Snowflake, DBT, Airflow, DevOps, Automated testing, CI/CD, API creation, Virtual package environments, Database management, SQL, Java, Python, AWS, Azure, Azure Data Factory","data engineering, data migration, etl pipelines, data modelling, cloudbased data migration, snowflake, dbt, airflow, devops, automated testing, cicd, api creation, virtual package environments, database management, sql, java, python, aws, azure, azure data factory","airflow, api creation, automated testing, aws, azure, azure data factory, cicd, cloudbased data migration, data engineering, data migration, data modelling, database management, dbt, devops, etl pipelines, java, python, snowflake, sql, virtual package environments"
"Senior Cloud Data Engineer - GBP70,000",Nigel Frank International,"Sheffield, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-cloud-data-engineer-gbp70-000-at-nigel-frank-international-3763121061,2023-12-17,Sheffield, United Kingdom,Mid senior,Onsite,"Senior Cloud Data Engineer - £70,000
I am working with a market leading manufacturer of specialist medical equipment who are looking for a Senior Cloud Data Engineer at the start of a number of exciting data driven projects. You would be joining at the start of a greenfield project where you will be responsible for the data migration, development and maintenance of the brand new Azure Data Platform!
The overall goal of the business and ultimately this role is to deliver data to healthcare professionals to aid with rehabilitation of patients. You will join a close-knit team, with talented professionals who specialise in a number of areas including software development and data science. You will be the in house expert for the team on all things data engineering and will take the lead on the creation, development and maintenance of the Azure data platform.
As part of this role, you will be responsible for some of the following areas.
Design, maintain and optimize both on premise and cloud based database solutions.
Develop and maintain robust ETL pipelines.
Work with other specialists within the team to make technical decisions that will benefit the overall business.
Take the lead of migration of data to the new Azure data platform.
This is a salaried role paying up to £70,000 per annum depending on experience and a company benefits package. This role would be largely a remote opportunity with occasional visits to the office in Stafford. These visits would be as and when needed in line with project requirements.
To be successful in the role you will have.
Experience working with the Azure tech stack including Azure Data Factory, Synapse and Azure Data Lake.
Experience designing and implementing ETL solutions.
Strong coding experience with languages such as Python or C#.
Experience working as a DBA or completing database administration tasks
This is just a brief overview of the role. For the full information, simply apply to the role with your CV, and I will call you to discuss further. My client is looking to begin the interview process ASAP, so don't miss out, APPLY now! To do so please email me at a.pinkerton@nigelfrank.com or call me on 0191 3387487.
Nigel Frank International are the go-to recruiter for Power BI and Azure Data Platform roles in the UK offering more opportunities across the country than any other recruitment agency. We're the proud sponsor and supporter of SQLBits, Power Platform World Tour, the London Power BI User Group, Newcastle Power BI User Group and Newcastle Data Platform and Cloud User Group. We are the global leaders in Microsoft recruitment.
Show more
Show less","Azure, Data Factory, Synapse, Azure Data Lake, ETL, Python, C#, DBA, SQL, Power BI","azure, data factory, synapse, azure data lake, etl, python, c, dba, sql, power bi","azure, azure data lake, c, data factory, dba, etl, powerbi, python, sql, synapse"
Senior Data & Process Analyst,Pendragon PLC,"Derbyshire, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-process-analyst-at-pendragon-plc-3775625620,2023-12-17,Sheffield, United Kingdom,Mid senior,Onsite,"Pendragon PLC
Hybrid working - requirements to work in the office once every 2 weeks with additional adhoc travel as required
Salary of up to £40,000, pension, critical illness, life assurance, 23 days holiday, plus bank holidays
As a Senior Data & Process Analyst, your role entails the design, development, and maintenance of reports, dashboards, and data visualizations. Additionally, you will collaborate closely with business units, providing data products and toolkits that support informed decision-making and data-driven insights. Reporting directly to the BI Manager, your expertise in data analysis, reporting, and data quality assurance will play a crucial role in enhancing our business operations and strategy.
Your Journey As a Senior BI & Process Analyst
If you’re looking for a role that’s a blend of being fast-paced and challenging, with a culture that enables you to be yourself and enjoy the ride, you’ll feel right at home:
Create, develop, and maintain reports, dashboards, and data visualisations to deliver actionable insights that steer our strategic direction
Collaborate closely with business teams to ensure reporting solutions are tailored to meet their specific information requirements and to unearth data-driven insights and opportunities, driving our organisational strategy forward
Partner with business stakeholders, providing data products and toolkits that elevate operational efficiency and decision-making capabilities
Guarantee the quality, consistency, and accessibility of our data assets, safeguarding the integrity of our information
Implement robust data quality checks, validation processes, and error handling mechanisms to ensure sustained data accuracy
Are You Ready to Embrace the Challenge? We’re looking for someone who has:
Strong Microsoft proficiency, encompassing Power Automate and Power Apps, along with advanced Excell skills including VBA and Microsoft scripts
Extensive experience in data analysis and reporting, particularly with Power BI, demonstrating a proven track record in BI development
Proficiency in data visualisation tools and techniques, complemented by a good understanding of SQL and data querying
Sound knowledge of data modelling, database design and ETL processes
Excellent communication skills with the ability to work collaboratively and is open to new ideas whilst appreciating the difference each person makes
Exceptional project management skills adept at handling complex data projects
At Pendragon, Together, We’re Unstoppable
Here at Pendragon, together, we’re unstoppable. Collaboration is at the heart of our culture, and it’s the power of our group coming together that unlocks our potential to transform automotive retail. We thrive in a fast-paced environment where the doors are open, if you have the attitude and drive to walk through them. We’ll empower you with the opportunities to bring out the best in yourself, and in return you’ll support and bring out the best in others.
Embrace The Gift Of Movement
We are dedicated to igniting the spark of movement and exploration. Our teams empower millions to experience the thrill of discovering new horizons, connecting with others, and embracing life’s fullest potential. Join us and seize the chance to make a real impact whilst embracing change and adapting to a world of endless possibilities.
Diversity Is Our Strength
Pendragon recognises the value that diversity brings to the workforce. This is why we positively welcome applications from all walks of life, backgrounds, and communities. If you have the motivation, skills and talent potential that we are looking for then get in touch. We are an equal opportunities employer.
Show more
Show less","Data Analysis, Reporting, Data Visualization, Power BI, Power Automate, Power Apps, Microsoft Excel, VBA, SQL, Data Modeling, Database Design, ETL, Project Management, Collaboration, Communication","data analysis, reporting, data visualization, power bi, power automate, power apps, microsoft excel, vba, sql, data modeling, database design, etl, project management, collaboration, communication","collaboration, communication, dataanalytics, database design, datamodeling, etl, microsoft excel, power apps, power automate, powerbi, project management, reporting, sql, vba, visualization"
Data Integrity Manager,Ardeta Search,"Derbyshire, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-integrity-manager-at-ardeta-search-3780458982,2023-12-17,Sheffield, United Kingdom,Mid senior,Onsite,"Ardeta Search is partnering with a Global Manufacturing Business based in Derbyshire to recruit a Data Integrity Manager on a permanent basis.
This is an exceptional opportunity for individuals who are passionate about safeguarding the accuracy, reliability, and security of data within the SAP Business One ecosystem. The ideal candidate thrives in a dynamic environment, possessing a keen eye for detail and a commitment to maintaining data integrity within SAP Business One.
Key Responsibilities:
Oversee and implement robust processes to ensure the accuracy and reliability of data across all platforms.
Develop and execute data integrity strategies that align with organizational goals and regulatory requirements.
Work closely with cross-functional teams to establish and enforce data integrity best practices.
Drive initiatives to enhance data quality, proactively identifying and resolving issues to maintain the highest standards.
Champion data security measures, ensuring compliance with industry regulations and safeguarding sensitive information.
Assist in introduction and maintenance of Service Engineer scheduling software.
Assist in development and maintenance of a “Net Store” solution for automated quoting of products
Skills
Proficient in project management with a proven track record of leading projects to successful completion.
Demonstrates effective stakeholder and budget management skills
Comprehensive understanding of various business processes and the relevance of IT systems
Expertise in testing and documentation.
Ability to instruct and influence others in these areas
This is an exciting role that works in the office on a hybrid basis paying circa £45,000 + Benefits.
Show more
Show less","SAP Business One, Data Integrity Management, Project Management, Stakeholder Management, Budget Management, Business Process Analysis, Testing, Documentation, Data Quality Management, Data Security, Service Engineer Scheduling","sap business one, data integrity management, project management, stakeholder management, budget management, business process analysis, testing, documentation, data quality management, data security, service engineer scheduling","budget management, business process analysis, data integrity management, data quality management, data security, documentation, project management, sap business one, service engineer scheduling, stakeholder management, testing"
Data Engineer,BJSS,"Sheffield, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-bjss-3629564981,2023-12-17,Sheffield, United Kingdom,Mid senior,Hybrid,"About Us
We’re an award-winning innovative tech consultancy - a team of creative problem solvers. Since 1993 we’ve been finding better, more sustainable ways to solve complex technology problems for some of the world’s leading organisations and delivered solutions that millions of people use every day.
In the last 30 years we won several awards, including a prestigious Queen’s Award for Enterprise in the Innovation category for our Enterprise Agile delivery approach.
Operating from 26 locations across the world, we bring together teams of creative experts with diverse backgrounds and experiences, who enjoy working and learning in our collaborative and open culture and are committed to world-class delivery.
We want to continue to grow our team with people just like you!
About the Role
We are DataOps advocates and use software engineering best practices to build scalable and re-usable data solutions to help clients use their data to gain insights, drive decisions and deliver business value. Clients don’t engage BJSS to do the straightforward things, they ask us to help on their biggest challenges which means we get to work with a wide range of tools and technologies and there are always new things to learn.
BJSS data engineers are specialist software engineers that build, optimise and maintain data applications, systems and services. This role combines the discipline of software engineering with the knowledge and experience of building data solutions in order to deliver business value.
As a BJSS data engineer you’ll help our clients deploy data pipelines and processes in a production-safe manner, using the latest technologies and with a DataOps culture.
You’ll work in a fast moving, agile environment, within multi-disciplinary teams of highly skilled consultants, delivering modern data platforms into large organisations.
You can expect to get involved in variety of projects in the cloud (AWS, Azure, GCP), learning about and using data services such as Databricks, Data Factory, Synapse, Kafka, Redshift, Glue, Athena, BigQuery, S3, Cloud Data Fusion etc.
About You
You're an engineer at heart and enjoy the challenge of building reliable, efficient data applications systems, services and platforms.
You have a good understanding of coding best practices and design patterns and experience with code and data versioning, dependency management, code quality and optimisation, error handling, logging, monitoring, validation and alerting.
You have experience in writing well tested object-oriented Python.
You have experience with using CI/CD tooling to analyse, build, test and deploy your code.
You have a good understanding of design choices for data storage and data processing, with a particular focus on cloud data services.
You have experience in using parallel computing to process large datasets and to optimise computationally intensive tasks.
You have experience in programmatically deploying, scheduling and monitoring components in a workflow.
You have experience in writing complex queries against relational and non-relational data stores.
Some of the Perks
Flexible benefits allowance – you choose how to spend your allowance (additional pension contributions, healthcare, dental and more)
Industry leading health and wellbeing plan - we partner with several wellbeing support functions to cater to each individual's need, including 24/7 GP services, mental health support, and other
Life Assurance (4 x annual salary)
25 days annual leave plus bank holidays
Hybrid working - Our roles are not fully remote as we take pride in the tight knit communities we have created at our local offices. But we offer plenty of flexibility and you can split your time between the office, client site and WFH
Discounts – we have preferred rates from dozens of retail, lifestyle, and utility brands
An industry-leading referral scheme with no limits on the number of referrals
Flexible holiday buy/sell option
Electric vehicle scheme
Training opportunities and incentives – we support professional certifications tooling like Airflow and Databricks.
Experience in supporting API Gateways and building and consuming REST APIs along with other distribution technologies.
Familiarity with Financial Systems architecture/ecosystems, Real Time Market Data messaging and FIX Protocol a huge plus.
Foundational knowledge of data structures, algorithms, and designing for performance.
Competent in one of the following programming languages: Java, C# or Python (preferred) and willingness to learn and adopt new languages as necessary
Experience in database technology like MSSQL and one of key value and document databases like MongoDb, Redis, Dynamo Db, Casandra.
Monitoring/Observability concepts and tooling: APM, Distributed Tracing, Grafana, Splunk, Prometheus.
Excellent communications skills and the ability to work with subject matter expert to extract critical business concepts.
Ability to work and potentially lead in an Agile methodology environment.
Show more
Show less","Cloud computing, Azure, Docker, Kubernetes, Microservices, Distributed systems architecture, Concurrency, Memory management, ETL tooling, Airflow, Databricks, REST APIs, Financial systems architecture, Realtime market data messaging, FIX Protocol, Data structures, Algorithms, Designing for performance, Java, C#, Python, MSSQL, MongoDB, Redis, DynamoDB, Cassandra, APM, Distributed tracing, Grafana, Splunk, Prometheus, Agile methodology","cloud computing, azure, docker, kubernetes, microservices, distributed systems architecture, concurrency, memory management, etl tooling, airflow, databricks, rest apis, financial systems architecture, realtime market data messaging, fix protocol, data structures, algorithms, designing for performance, java, c, python, mssql, mongodb, redis, dynamodb, cassandra, apm, distributed tracing, grafana, splunk, prometheus, agile methodology","agile methodology, airflow, algorithms, apm, azure, c, cassandra, cloud computing, concurrency, data structures, databricks, designing for performance, distributed systems architecture, distributed tracing, docker, dynamodb, etl tooling, financial systems architecture, fix protocol, grafana, java, kubernetes, memory management, microservices, mongodb, mssql, prometheus, python, realtime market data messaging, redis, rest apis, splunk"
Data Engineer IV - Max Digital (Data Engineering),ACV Auctions,"Alabama, United States",https://www.linkedin.com/jobs/view/data-engineer-iv-max-digital-data-engineering-at-acv-auctions-3762880107,2023-12-17,Alabama,United States,Mid senior,Hybrid,"If you are looking for a career at a dynamic company with a people-first mindset and a deep culture of growth and autonomy, ACV is the right place for you! Competitive compensation packages and learning and development opportunities, ACV has what you need to advance to the next level in your career. We will continue to raise the bar every day by investing in our people and technology to help our customers succeed. We hire people who share our passion, bring innovative ideas to the table, and enjoy a collaborative atmosphere.
Who we are:
ACV is a technology company that has revolutionized how dealers buy and sell cars online. We are transforming the automotive industry. ACV Auctions Inc. (ACV), has applied innovation and user-designed, data driven applications and solutions. We are building the most trusted and efficient digital marketplace with data solutions for sourcing, selling and managing used vehicles with transparency and comprehensive insights that were once unimaginable. We are disruptors of the industry and we want you to join us on our journey. ACV’s network of brands includes ACV Auctions, ACV Transportation, ClearCar, MAX Digital and ACV Capital within its Marketplace Products, as well as, True360 and Data Services.
At ACV we focus on the Health, Physical, Financial, Social and Emotional Wellness of our Teammates and to support this we offer:
Multiple medical plans including a high deductible health plan that costs $0 out of your paycheck
Company-sponsored (paid) Short-Term Disability, Long-Term Disability, and Life Insurance
Comprehensive optional benefits such as Dental, Vision, Supplemental Life/AD&D, Legal/ID Protection, and Accident and Critical Illness Insurance
Generous paid time off options, including vacation time, sick days, Company holidays, floating holidays, parental leave, bereavement leave, jury duty leave, voting leave, and other forms of paid leave as required by applicable law or regulation
Employee Stock Purchase Program with additional opportunities to earn stock in the Company
Retirement planning through the Company’s 401(k)
Who we are looking for:
We are seeking a highly skilled Engineer IV in Data Engineering with a strong foundation in computer science and excellent problem-solving skills. You will be responsible for maintaining and extending our database operations, optimizing SQL queries, and designing scalable data services.
What you will do:
Actively and consistently support all efforts to simplify and enhance the customer experience.
Maintain and extend (as required) existing database operations solution for backups, index defragmentation, data retention, etc.
Troubleshoot any SQL Server or ETL stack outages during our operational support window.
Triage any issues with data stack (SSIS, C#, Web APIs).
Support development, integration, and stage SQL Server environments for application development and data science teams.
Ensure that new database development meets company standards for readability, reliability, and performance. Work with internal teams on transactional and analytical schema design.
Collaborate with software and DevOps engineers to design scalable services, plan feature roll-out, and ensure high reliability and performance of our products.
Architect and build entire services including but not limited to; data modeling, storage, message brokers, protocols and interfaces.
Design, build and maintain complex systems that can scale rapidly with little maintenance.
Conduct code reviews, develop high-quality documentation, and build robust test suites.
Own the overall performance of products and services within a defined area of focus.
Be empowered to lead and complete software projects with minimal guidance from managers.
Lead team discussions to define technical requirements on new and current products.
Respond-to and troubleshoot highly complex problems quickly, efficiently, and effectively.
Mentor junior engineers.
Perform additional duties as assigned.
What you will need:
Bachelor's degree in Computer Science, Information Technology, Computer Information Systems, Management Information Systems, or similar
5 years' building & supporting the database-tier of SaaS web applications.
Ability to read, write, speak, and understand English.
Expert understanding of SQL query execution fundamentals and query optimization principles.
Experience maintaining and extending an existing codebase, adapting to pre-existing patterns and tracing the code’s path of execution.
ETL workflow implementation (SSIS, Airflow, C#, Python)
Experience working with Cloud Services (AWS RDS, S3, SQS, SNS)
Experience working with NoSQL data stores (MongoDB)
Experience writing unit and integration testing (DBT, C#)
Expert SQL and data-layer development experience; OLTP schema design.
Experience integrating 3rd-party APIs, implementing authentication & authorization and developing asynchronous data flows.
Nice to Have
OLAP schema design experience.
Experience with Airflow, Snowflake, etc.
Experience with DBT
Our Values
Trust & Transparency | People First | Positive Experiences | Calm Persistence | Never Settling
At ACV, we are committed to an inclusive culture in which every individual is welcomed and empowered to celebrate their true selves. We achieve this by fostering a work environment of acceptance and understanding that is free from discrimination. ACV is committed to being an equal opportunity employer regardless of sex, race, creed, color, religion, marital status, national origin, age, pregnancy, sexual orientation, gender, gender identity, gender expression, genetic information, disability, military status, status as a veteran, or any other protected characteristic. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you have a disability or special need that requires reasonable accommodation, please let us know.
For information on our collection and use of your personal information, please see our Privacy Notice.
Apply Now
Show more
Show less","SQL, ETL, Data Modeling, Data Engineering, Python, C#, SSIS, AWS, NoSQL, DBT, Data Integration, Cloud Services, Agile Development, Authentication, Authorization, Data Warehousing, Data Science, Software Development, Software Design, OLAP, Scalable Services, Testing, Troubleshooting, Communication, Problem Solving, Leadership","sql, etl, data modeling, data engineering, python, c, ssis, aws, nosql, dbt, data integration, cloud services, agile development, authentication, authorization, data warehousing, data science, software development, software design, olap, scalable services, testing, troubleshooting, communication, problem solving, leadership","agile development, authentication, authorization, aws, c, cloud services, communication, data engineering, data integration, data science, datamodeling, datawarehouse, dbt, etl, leadership, nosql, olap, problem solving, python, scalable services, software design, software development, sql, ssis, testing, troubleshooting"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Huntsville, AL",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3759710401,2023-12-17,Alabama,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto,
San
Francisco or Chicago
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","ML, Data engineering, Data mining, Data cleaning, Data normalization, Pandas, R, Airflow, KubeFlow, NLP, Git, Python, Java, bash, SQL, Snowflake, Kubernetes, Docker, Helm, Spark, pySpark, AWS, GCP, Azure, DynamoDB, ETL, Kafka, Storm, SparkStreaming, Machine learning, Legal compliance, Data classification, Data retention, 401K, Equity, Genderaffirming offerings, HRT, Flexible vacation, Cell phone stipend, Internet stipend, Wellness stipend, Food stipend, Homeoffice setup stipend","ml, data engineering, data mining, data cleaning, data normalization, pandas, r, airflow, kubeflow, nlp, git, python, java, bash, sql, snowflake, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, dynamodb, etl, kafka, storm, sparkstreaming, machine learning, legal compliance, data classification, data retention, 401k, equity, genderaffirming offerings, hrt, flexible vacation, cell phone stipend, internet stipend, wellness stipend, food stipend, homeoffice setup stipend","401k, airflow, aws, azure, bash, cell phone stipend, data classification, data cleaning, data engineering, data mining, data normalization, data retention, docker, dynamodb, equity, etl, flexible vacation, food stipend, gcp, genderaffirming offerings, git, helm, homeoffice setup stipend, hrt, internet stipend, java, kafka, kubeflow, kubernetes, legal compliance, machine learning, ml, nlp, pandas, python, r, snowflake, spark, sparkstreaming, sql, storm, wellness stipend"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Huntsville, AL",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3773091269,2023-12-17,Alabama,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto
or
San
Francisco
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, ML Data OPs, ML models, Python, Java, bash, SQL, Git, Airflow, KubeFlow, Snowflake, Kubernetes, Docker, Helm, Spark, pySpark, AWS, GCP, Azure, DynamoDB, ETL, Kafka, Storm, SparkStreaming, Machine learning, Data classification, Data retention, Data management tools","data engineering, ml data ops, ml models, python, java, bash, sql, git, airflow, kubeflow, snowflake, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, dynamodb, etl, kafka, storm, sparkstreaming, machine learning, data classification, data retention, data management tools","airflow, aws, azure, bash, data classification, data engineering, data management tools, data retention, docker, dynamodb, etl, gcp, git, helm, java, kafka, kubeflow, kubernetes, machine learning, ml data ops, ml models, python, snowflake, spark, sparkstreaming, sql, storm"
26NOS - SysAd - Database Admin - Sr - (#109b),"SMS Data Products Group, Inc.","Montgomery, AL",https://www.linkedin.com/jobs/view/26nos-sysad-database-admin-sr-%23109b-at-sms-data-products-group-inc-3578828899,2023-12-17,Alabama,United States,Mid senior,Hybrid,"SMS is seeking a Senior DBA to support the Air Force 26 NOS.
As a dynamic systems integrator, SMS offers proven solutions in engineering, operations, cybersecurity, and digital transformation. With expertise in modernizing and optimizing legacy infrastructure and systems, ensuring operational efficiency, and designing, implementing, and managing secure environments, SMS supports business and mission goals with proficiency, quality, and integrity.
SMS has been serving the advanced information technology needs of the federal government since 1976, delivering talented teams and innovative, cost-effective solutions and services to support our customers’ missions for more than 40 years. SMS is headquartered in McLean, Virginia, with offices and on-site operations at customer locations throughout the United States. For additional information on SMS, visit www.sms.com.
Submit your resume today!
Responsibilities
Duties & Responsibilities
Work within data center operations supporting Windows servers, desktops, Linux systems and various application patching, software version updates, network and security configurations
Administer SQL (Structured Query Language) database instances or PostgreSQL to manage and interact with hosted software applications such as SharePoint sites, NetQoS, Cacti, SolarWinds, etc.
Provide database administration including design, install, configuration, upgrades, consolidation, monitoring, tuning, and maintenance; Manage database backup and recovery processes
Strong skills designing, developing and maintaining stored procedures, views, tables, T-SQL scripts, creating associated documentation; Perform data conversion and server migrations
Maintain detailed SQL server environment technical documentation and system security plans
Interact with Microsoft SQL database stored procedures, and desktop applications
Aide in effective provisioning, installation, configuration, operation, certificate management and maintenance of high availability production and disaster recovery (DR) sites
Responsible for database implementation, security, privileges and regular maintenance tasks
Build scripts automating daily, weekly, monthly, or quarterly operations of database management
Detect, craft and sustain server Windows automation processes with PowerShell to improve or enhance day-to-day operations to create team efficiencies & reduce manual processes
Provide a periodic report of system status to include uptime, incidents, problems or open issues
Respond to system faults, analyzing and correcting applications or errors to reestablish service
Aid to identify technical environment issues and recommend solutions to mitigate problems
Perform analytical, technical and admin work using and installing new and existing physical or web-based database application systems in a smart card Public Key Infrastructure (PKI) environment
Proactively monitor and evaluate performance issues and requests for local and remote data centers
Mentor junior system administrators to impart knowledge, share methods and teach processes
Preferred exposure to virtualized server offerings, especially those based on VMware or Nutanix
Provide in depth systems analysis, system administration, security, compliance and maintenance
Able to audit, remediate, and maintain DISA Security Technical Implementation Guide (STIG) checklist documentation on supported systems utilizing technical procedures, best practices, hardening guidance, vendor manuals, user guides and runbooks;
Experience in evaluating audit scan report data from ACAS Tenable.SC or Tenable Nessus Scanners; participate in the review and response phases of the Vulnerability Management (VM) life cycle
Aptitude to support provisional installation, configuration, operation and maintenance of server systems hardware (Dell PowerEdge, HPe ProLiant, etc.), and utilize related infrastructure devices such as KVMs, out-of-band management (OOBM) tools like integrated lights-out (ILO), iLO Amplifier, integrated Dell Remote Access Controller (iDRAC) or Open Managed Enterprise (OME) appliances
Develop and/or implement technical solutions based on defined mission scope and using approved software tools such as PuTTY, SecureCRT, Remote Desktop Protocol (RDP), NotePad++, etc.
Aptitude to test/interact with new software technologies in environment with 3 rd party monitoring tools to track overall health and availability of apps: Splunk Enterprise, SolarWinds Orion platform, McAfee Endpoint Security Systems (ESS), Microsoft Endpoint Configuration Manager (MECM), WSUS, ForeScout, NAGIOS, Veritas NetBackup, etc.
Ensures networks receive periodic updates from AFCYBER-released software, patches, firmware, and upgrades via Time Compliance Technical Orders (TCTO), Time Compliance Network Orders (TCNO), Maintenance Tasking Order (MTO) and Notices to Airman (NOTAMs)
Assist AF Cyber personnel with Information Assurance Vulnerability Management (IAVM) programs, cybersecurity toolsets, and Operation Order (OPORD)/Fragmentary Order (FRAGO) support
Candidate will report to the 26 th NOS Systems Administration (SA) team leadership
Qualifications/Requirements:
Experience within DoD environment or enterprise data center desired
Experience with supporting database servers preferably VMware infrastructure, and/or backups
Must have a minimum of 5+ years of experience with database technologies
Prefer a minimum of 3+ years of experience with Windows server environment
Knowledge of network protocols and technologies: DHCP, ICMP, LDAPS, NetFlow, Polling, SCP, SMTP, SNMP, Syslog, TCP/IP, VLANs, WMI
Exposure to cluster management, high availability methodologies, load balancing traffic management technologies; virtualized environments such as VMware vSphere’s vCenter Server Appliance (vCSA), ESXi hosts, virtual machines (VMs), VMTools, Horizon View, and/or VM/Host distributed resource schedules (DRS) groups/rules
Support day-to-day technical communication systems, alerts and incident tickets queue in support of operations; Open/track change requests and interact with external customers or vendors
Experience with networking, desktops / laptops, middleware, client/server software, Office products, server operating systems, virtualization, managing data, applications, websites, or database supporting a multi-site, multi-domain Windows forest
Responsible for shared 24x7 coverage of the Windows physical and/or virtual support
Competencies
Qualifications
Interact well with all levels of management across multiple teams consisting of Government civilians, contractors, and military personnel
Self-motivated self-starter, able to work autonomously and as part of a team, sharing knowledge to help others in a fast-paced cyber environment
Possess refined critical thinking skills and thrive in fast-paced multi-task environment
Good communication and interpersonal skills; Ability to follow policies and procedures
Approach work tasks as diplomatic, adaptive to a dynamic environment, dependable and reliable
Aptitude to address negative situations and resolve them in a positive manner
Experience maintaining, overseeing and troubleshooting computers and related peripheral equipment performing preventative maintenance in accordance with department policies
Proactively stay current with the latest server infrastructure and storage technologies releases
Ability to lift, rack and provision government furnished equipment (GFE)
Education/Certification(s):
Technical degree, Associates or, Bachelor’s degree in Computer Science/Information Systems, S.T.E.M. or 2-4 years’ relevant experience in Information Technology preferably within system administration is acceptable
Required IA Certification: CompTIA Security+ CE or higher
Minimum one or more of the following technical CE/OS certifications:
Microsoft based certification ( current ): Microsoft Certified: Azure Data Fundamentals ( Database/DBA Role ); | Microsoft 365 Certified: Fundamentals; Microsoft Certified: Security, Compliance, and Identity Fundamentals; Microsoft 365 Certified: Security Administrator Associate; Microsoft Certified: Azure Security Engineer Associate; Microsoft Certified: Identity and Access Administrator Associate
Senior Level preferred : Microsoft Certified: Azure Database Administrator Associate -or- Administering Microsoft Azure SQL Solutions -or- Oracle Database SQL Certified Associate ( Database/DBA Role ); | Microsoft 365 Certified: Enterprise Administrator Expert; Microsoft Certified: Windows Server Hybrid Administrator Associate;
Microsoft based certification ( retired, will be considered ): Microsoft Technology Associate (MTA) Database Fundamentals, Windows Operating System Fundamentals, Windows Server Administration Fundamentals, Security Fundamentals; or Microsoft Certified Solutions Associate (MCSA) SQL 2016 Database Administration, SQL Server 2012/2014; or Microsoft Certified Solutions Expert (MCSE) Data Management and Analytics, Server Infrastructure
ITIL certification a plus
Clearance:
Active DoD
Secret
required or ability to complete investigation process for interim with potential to upgrade to Top Secret clearance preferred
Show more
Show less","Windows, Linux, SQL, PostgreSQL, SharePoint, NetQoS, Cacti, SolarWinds, TSQL, PowerShell, VMware, Nutanix, DISA Security Technical Implementation Guide (STIG), ACAS Tenable.SC, Tenable Nessus, Dell PowerEdge, HPe ProLiant, PuTTY, SecureCRT, Remote Desktop Protocol (RDP), NotePad++, Splunk Enterprise, SolarWinds Orion platform, McAfee Endpoint Security Systems (ESS), Microsoft Endpoint Configuration Manager (MECM), WSUS, ForeScout, NAGIOS, Veritas NetBackup, AFCYBER, Time Compliance Technical Orders (TCTO), Time Compliance Network Orders (TCNO), Maintenance Tasking Order (MTO), Notices to Airman (NOTAMs), Information Assurance Vulnerability Management (IAVM), Operation Order (OPORD), Fragmentary Order (FRAGO), DHCP, ICMP, LDAPS, NetFlow, Polling, SCP, SMTP, SNMP, Syslog, TCP/IP, VLANs, WMI, VMware vSphere’s vCenter Server Appliance (vCSA), ESXi, virtual machines (VMs), VMTools, Horizon View, VM/Host distributed resource schedules (DRS) groups/rules, Office, Windows forest, CompTIA Security+ CE, Microsoft Certified: Azure Data Fundamentals ( Database/DBA Role ), Microsoft 365 Certified: Fundamentals, Microsoft Certified: Security Compliance and Identity Fundamentals, Microsoft 365 Certified: Security Administrator Associate, Microsoft Certified: Azure Security Engineer Associate, Microsoft Certified: Identity and Access Administrator Associate, Microsoft Certified: Azure Database Administrator Associate, Administering Microsoft Azure SQL Solutions, Oracle Database SQL Certified Associate ( Database/DBA Role ), Microsoft 365 Certified: Enterprise Administrator Expert, Microsoft Certified: Windows Server Hybrid Administrator Associate, Microsoft Technology Associate (MTA) Database Fundamentals, Windows Operating System Fundamentals, Windows Server Administration Fundamentals, Security Fundamentals, Microsoft Certified Solutions Associate (MCSA) SQL 2016 Database Administration, SQL Server 2012/2014, Microsoft Certified Solutions Expert (MCSE) Data Management and Analytics, Server Infrastructure, ITIL","windows, linux, sql, postgresql, sharepoint, netqos, cacti, solarwinds, tsql, powershell, vmware, nutanix, disa security technical implementation guide stig, acas tenablesc, tenable nessus, dell poweredge, hpe proliant, putty, securecrt, remote desktop protocol rdp, notepad, splunk enterprise, solarwinds orion platform, mcafee endpoint security systems ess, microsoft endpoint configuration manager mecm, wsus, forescout, nagios, veritas netbackup, afcyber, time compliance technical orders tcto, time compliance network orders tcno, maintenance tasking order mto, notices to airman notams, information assurance vulnerability management iavm, operation order opord, fragmentary order frago, dhcp, icmp, ldaps, netflow, polling, scp, smtp, snmp, syslog, tcpip, vlans, wmi, vmware vspheres vcenter server appliance vcsa, esxi, virtual machines vms, vmtools, horizon view, vmhost distributed resource schedules drs groupsrules, office, windows forest, comptia security ce, microsoft certified azure data fundamentals databasedba role, microsoft 365 certified fundamentals, microsoft certified security compliance and identity fundamentals, microsoft 365 certified security administrator associate, microsoft certified azure security engineer associate, microsoft certified identity and access administrator associate, microsoft certified azure database administrator associate, administering microsoft azure sql solutions, oracle database sql certified associate databasedba role, microsoft 365 certified enterprise administrator expert, microsoft certified windows server hybrid administrator associate, microsoft technology associate mta database fundamentals, windows operating system fundamentals, windows server administration fundamentals, security fundamentals, microsoft certified solutions associate mcsa sql 2016 database administration, sql server 20122014, microsoft certified solutions expert mcse data management and analytics, server infrastructure, itil","acas tenablesc, administering microsoft azure sql solutions, afcyber, cacti, comptia security ce, dell poweredge, dhcp, disa security technical implementation guide stig, esxi, forescout, fragmentary order frago, horizon view, hpe proliant, icmp, information assurance vulnerability management iavm, itil, ldaps, linux, maintenance tasking order mto, mcafee endpoint security systems ess, microsoft 365 certified enterprise administrator expert, microsoft 365 certified fundamentals, microsoft 365 certified security administrator associate, microsoft certified azure data fundamentals databasedba role, microsoft certified azure database administrator associate, microsoft certified azure security engineer associate, microsoft certified identity and access administrator associate, microsoft certified security compliance and identity fundamentals, microsoft certified solutions associate mcsa sql 2016 database administration, microsoft certified solutions expert mcse data management and analytics, microsoft certified windows server hybrid administrator associate, microsoft endpoint configuration manager mecm, microsoft technology associate mta database fundamentals, nagios, netflow, netqos, notepad, notices to airman notams, nutanix, office, operation order opord, oracle database sql certified associate databasedba role, polling, postgresql, powershell, putty, remote desktop protocol rdp, scp, securecrt, security fundamentals, server infrastructure, sharepoint, smtp, snmp, solarwinds, solarwinds orion platform, splunk enterprise, sql, sql server 20122014, syslog, tcpip, tenable nessus, time compliance network orders tcno, time compliance technical orders tcto, tsql, veritas netbackup, virtual machines vms, vlans, vmhost distributed resource schedules drs groupsrules, vmtools, vmware, vmware vspheres vcenter server appliance vcsa, windows, windows forest, windows operating system fundamentals, windows server administration fundamentals, wmi, wsus"
"SR. ANALYST, DATA-HYBRID",Independence Health System - Westmoreland Area,"Greensburg, PA",https://www.linkedin.com/jobs/view/sr-analyst-data-hybrid-at-independence-health-system-westmoreland-area-3770864770,2023-12-17,Blairsville,United States,Mid senior,Onsite,"Job Details
Description
Job Summary
As a key member of the Independence Health System Business Intelligence team, the Senior Data Analyst leads expert understanding in design, modeling, and delivery of complex data analyses. The Senior Analyst will operate as a champion of using data to make decisions. By utilizing data derived from multiple disparate data sources and the strategic imperatives identified from leadership, the Senior Analyst will conceptualize, develop, and implement data strategies to drive appropriate results. The Senior Analyst will work closely with key stakeholders to identify needs and deliver effective solutions to model data in innovative ways. To successfully perform the role, the Senior Analyst must understand data modeling, data normalization and standardization, outlier identification, and data cleaning, advanced data practices. Further, the Senior Data Analyst is responsible for understanding data visualization techniques to make the outcomes of data analytics consumable to the operational end user.
Essential Job Functions
Develops industry standard techniques to model and aggregate data.
Investigates variation of data and derives solutions to solve data quality issues.
Articulate in the operations of many different disparate data sources to identify the right choice for data analysis.
Work collaboratively with IT, Quality, Nursing Informatics, Finance, and other members of the Business Intelligence team to model appropriately.
Develop and implement data collection systems, data analytics, and other strategies that optimize analytical report out.
Create data models ready for interactive, user-friendly visualizations to convey data in an exploratory way using tools such as advanced Excel, Tableau, or other reporting capabilities.
Leads data dissemination projects. Adheres to project deadlines and desired deliverables.
Creates collaboratively the standard for data modeling with other Business Intelligence team members.
Manages overall data usage structure for Independence Health.
Ensure adherence to compliance requirements, regulations, and policies.
Design, create, test, and deploy ETL queries and reports.
Monitors data model performance, usage, and communicating functional and technical issues.
Researches, suggests, and implements industry trends in healthcare analytics.
Identify, analyze, and interpret results, trends, or patterns in complex data sets.
Practices advanced analytical and design skills, including the ability to abstract information from real-world processes to understand information and operational flows.
Participate in improvement initiatives and develops data extracts needed to identify risks, weaknesses, opportunities or other relevant data.
Communicates status of tasks/issues and results in an efficient and effective manner.
Demonstrates a working understanding of operations and pertinent data flows that allows for design and implementation of analytics.
Demonstrates ability to produce and deliver products or presentations to diverse audience of technical and non-technical stakeholders.
Works with management to prioritize business information and needs.
Demonstrates advanced knowledge of healthcare concepts to assist with advanced analytics.
Seeks to continuously learn how analytics integrate with operational decisions and advocates for major operational issues to be solved with data-driven decisions.
Works with end users to optimize data processes to streamline manual analytics.
Leads initiatives to continually improve the quality of data in organization.
Other related duties as assigned.
Qualifications
Bachelor’s Degree in Data Sciences, Mathematics, Statistics, Biostatistics, Healthcare Management, Business, or related field and 4-6 years of data analysis, data visualization, reporting, and/or analytics experience.
Superior computer skills with knowledge of SQL and relevant software experience.
Excellent Microsoft Office skills in all applicable programs, including Excel.
Business Intelligence, data visualization and delivery experience, such as Tableau, Qlikview, Excel advanced reporting, etc.
Strong leadership ability, good organizational skills, independent and critical thinking skills, sound judgment, and knowledge of legal aspects and liability of nursing practice.
Strong ability to communicate complex and/or controversial topics and concepts to a wide and diverse audience such as Leadership, Management, or departmental teams.
Preferred Qualifications
Master’s Degree in related field preferred.
Tableau, Data Warehousing, SQL, or other analytical related certifications preferred.
Experience with healthcare softwares such as Meditech Expanse, Cerner, Allscripts, Meditech Business Clinical Analytics, etc.
Experience in Healthcare preferred.
Strong experience in Tableau or other data visualization software.
Strong experience with data modeling and mining, including blending, combining and marrying disparate data solutions.
Enterprise data warehouse or other data aggregation experience.
Familiarity with highly complex logical data design, mapping and conversion.
Familiarity with R or Python coding.
License, Certification & Clearances
Act 34-PA Criminal Record Check from the PA State Police system
Supervisory Responsibilities
This position has no direct supervisory responsibilities but could serve as a coach and mentor for other positions in the department.
Position Type/Expected Hours of Work
Incumbent will be scheduled based on operational need (rotate shifts, standby, on-call, etc.).
AAP/EEO
Excela Health is an Equal Opportunity Employer. It is the policy of Excela Health to prohibit discrimination of any type and to afford equal employment opportunities to employees and applicants, without regard to race, color, religion, sex, national origin, age, marital status, non-job related disability, veteran status, or genetic information, or any other protected class. Excela Health will conform to the spirit as well as the letter of all applicable laws and regulations.
Ability to perform the Essential Functions listed on the Physical Conditions and ability to perform the Essential Functions on the Working Condition chart below.
Actively promotes a Lean work culture by performing team member duties to encourage consistent use of LEAN principles and processes, including continually seeking work process improvements. Recognizes the necessity of taking ownership of one’s own motivation, morale, performance and professional development. Strives for behavior consistent with being committed to Excela’s missions, vision and values.
Work Environment
Effective March 2020 or during pandemic: goggles, face shield and mask are required according to CDC guidelines
When lift requirement is in excess of 50#, lift assistance (2 person) and/or transfer device is required.
Essential – Absolute Necessity.
Marginal – Minimal Necessity.
Constantly – 5.5 to 8 hours or more or 200 reps/shift.
Frequently – 2.5 to 5.5 hours or more or 32-200 reps/shift.
Occasionally – 0.25 to 2.5 hours or 2-32 reps/shift.
Rarely – Less than 0.25 hours or less than 2 reps/shift.
Physical Condition
Essential
Marginal
Constantly
Frequently
Occasionally
Rarely
Never
Extreme Heat
X
Extreme Cold
X
Heights
X
Confined Spaces
X
Extreme Noise(>85dB)
X
Mechanical Hazards
X
Use of Vibrating Tools
X
Operates Vehicle
X
Operates Heavy Equipment
X
Use of Lifting/Transfer Devices
X
Rotates All Shifts
X
8 Hours Shifts
X
X
10-12 Hours Shifts
X
On-Call
X
Overtime(+8/hrs/shift; 40/hr/wk)
X
X
Travel Between Sites
X
Direct Patient Care
X
Respirator Protective Equipment
X
Eye Protection
X
Head Protection (hard hat)
X
Hearing Protection
X
Hand Protection
X
Feet, Toe Protection
X
Body Protection
X
Latex Exposure
X
X
Solvent Exposure
X
Paint (direct use) Exposure
X
Dust (sanding) Exposure
X
Ethylene Oxide Exposure
X
Cytotoxic (Chemo) Exposure
X
Blood/Body Fluid Exposure
X
Chemicals (direct use) Exposure
X
Mist Exposure
X
Wax Stripper (direct use)
X
Non-Ionizing Radiation Exposure
X
Ionizing Radiation Exposure
X
Laser Exposure
X
Physical Demand
When lift requirement is in excess of 50#, lift assistance (2 person) and/or transfer device is required.
Essential
Marginal
Constantly
Frequently
Occasionally
Rarely
Never
Bending (Stooping)
X
X
Sitting
X
X
Walking
X
X
Climbing Stairs
X
X
Climbing Ladders
X
Standing
X
X
Kneeling
X
Squatting (Crouching)
X
Twisting/Turning
X
Keyboard/Computer Operation
X
X
Gross Grasp
X
X
Fine Finger Manipulation
X
X
Hand/Arm Coordination
X
X
Pushing/Pulling(lbs. of force)
X
<10#
Carry
X
<10#
Transfer/Push/Pull Patients
X
Seeing Near w/Acuity
X
X
Feeling (Sensation)
X
Color Vision
X
Hearing Clearly
X
X
Pulling/Pushing Objects Overhead
Reaching Above Shoulder Level
Reaching Forward
X
X
Lifting Floor to Knuckle
X
<10#
Lifting Seat Pan to Knuckle
X
<10#
Lifting Knuckle to Shoulder
X
Lifting Shoulder to Overhead
X
When lift requirement is in excess of 50#, lift assistance (2 person) and/or transfer device is required. Indep
Show more
Show less","Data modeling, Data analytics, Data visualization, Data mining, Data governance, Data aggregation, Data quality, Data normalization, Data standardization, Data cleansing, Data cleaning, Data analysis, Data interpretation, Data reporting, Data dissemination, Data warehousing, Data integration, Data warehousing, Data warehousing, ETL, SQL, Tableau, Qlikview, Excel, Power BI, Python, R, R or Python coding, Healthcare analytics, Healthcare data, Meditech Expanse, Cerner, Allscripts, Meditech Business Clinical Analytics","data modeling, data analytics, data visualization, data mining, data governance, data aggregation, data quality, data normalization, data standardization, data cleansing, data cleaning, data analysis, data interpretation, data reporting, data dissemination, data warehousing, data integration, data warehousing, data warehousing, etl, sql, tableau, qlikview, excel, power bi, python, r, r or python coding, healthcare analytics, healthcare data, meditech expanse, cerner, allscripts, meditech business clinical analytics","allscripts, cerner, data aggregation, data cleaning, data dissemination, data governance, data integration, data interpretation, data mining, data normalization, data quality, data reporting, data standardization, dataanalytics, datacleaning, datamodeling, datawarehouse, etl, excel, healthcare analytics, healthcare data, meditech business clinical analytics, meditech expanse, powerbi, python, qlikview, r, r or python coding, sql, tableau, visualization"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Hartford, CT",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3748826679,2023-12-17,Wallingford,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our next evolution is underway as a newly public company looking to expand and continue to build meaningful experiences for our users. From social issues to original content, we’re blazing innovative paths with impact for our community, all while leveraging the latest tech stacks and striving for engineering excellence. At the heart of our work in this new chapter is a shared set of core values: openness and exploration, a bias for action, and strong support of the LGBTQ community.
With a track record of strong financial performance and plans for continued headcount growth, we’re looking to build a team of talented, passionate, and open-minded people who believe in our mission, align with our values, and are excited to work at the intersection of innovative technology and social impact. Come be a part of this exciting journey with us.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location: Palo Alto, San Francisco or Chicago
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Realtime Streaming Technologies, TDD, Automation, Continuous Delivery, Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, SQL, Agile Engineering Practices, Pair Programming, Continuous Integration, Automated Testing, Deployment, StreamProcessing Systems, Kafka, Storm, SparkStreaming, Dimensional Data Modeling, Schema Design, Data Warehouses, ETL, Data Management Tools, Data Classification, Retention","data engineering, realtime streaming technologies, tdd, automation, continuous delivery, python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, agile engineering practices, pair programming, continuous integration, automated testing, deployment, streamprocessing systems, kafka, storm, sparkstreaming, dimensional data modeling, schema design, data warehouses, etl, data management tools, data classification, retention","agile engineering practices, airflow, automated testing, automation, continuous delivery, continuous integration, data classification, data engineering, data management tools, data warehouses, deployment, dimensional data modeling, docker, etl, helm, kafka, kubernetes, pair programming, python, realtime streaming technologies, retention, schema design, snowflake, spark, sparkstreaming, sql, storm, streamprocessing systems, tdd"
"Operational Data Analysis Specialist_ Chattanooga, TN 37402 USA",eStaffing Inc.,"Chattanooga, TN",https://www.linkedin.com/jobs/view/operational-data-analysis-specialist-chattanooga-tn-37402-usa-at-estaffing-inc-3747464871,2023-12-17,Tennessee,United States,Associate,Onsite,"Job Title - Operational Data Analysis Specialist
Job Location - Chattanooga, TN 37402
Shift - 1 SHIFT- 8:00 am to 5:00 pm
Duration - 12 Months
Positions - 1
Job Description
Understands the decision-making processes, workflows, business, and information needs of programs/processes for the
Operations leadership team.
Translates business needs into analytics/reporting requirements to support executive decisions and workflows with required
information.
Coordinates with program/process owners and end users to understand their information needs and identify ways to visualize
and present information in a user-friendly manner.
Proactively mines data from various application sources to identify trends and patterns to generate insights for the leadership team
and program/process owners. Illustrate key business trends through effective data interpretation and visualization.
Develops frameworks and processes to analyze unstructured information collected through various applications. Develops
standards for data, reporting, and visualization.
Interacts with T&I and application owners to understand how information is stored, modeled, and tagged.
Collaborates with Operations and T&I staff to develop and maintain self-service technology solutions.
Supports leadership team, program/process owners, and end users to enhance information visualization through the development
of dashboards and user interfaces.
Works closely with the leadership team, program/process owners, end users, and business units for knowledge sharing, mentoring,
and training.
Maintains organizational SharePoint pages.
Provide recommendations based on daily activities to update, simplify, and enhance processes, procedures, and technologies.
Engages with T&I Centers of Excellence to stay informed on new technology and best practices.
Advise and fulfill ad-hoc requests for data and analysis to support senior management.
Minimum Requirements
Education: Bachelor's degree in computer science, data analytics, information technology, engineering, business, finance,
mathematics, statistics, or related fields. Or equivalent education, training & experience.
Experience:
Minimum of one year experience in the electric utility industry with a focus on data analysis, IT applications, or technical
program and performance analysis. Preferred: working knowledge of project and work management fundamentals and experience
with Microsoft Office Software and Power Platform applications (Power BI, Power Automate, etc.) and/or Tableau.
Certification/License, etc?
Knowledge/Skills/Abilities:
Must have good written and verbal communication skills, especially concerning technical concepts
and results. Able to work with co-workers, peer teams, senior management, and various BUs. Strong organizational skills and
proven ability to prioritize work, apply critical thinking, meet deadlines, and work on multiple assignments simultaneously with
minimal supervision. May be required to obtain and maintain NERC CIP and *** Sensitive security clearances based on position /
access requirements and essential job functions. Ability and willingness to travel to operations sites as required when performing
assigned tasks
Show more
Show less","Data analysis, Data visualization, Data mining, Data interpretation, Data management, Data reporting, Power BI, Power Automate, Tableau, SharePoint, Microsoft Office, Project management, IT applications, NERC CIP, Sensitive security clearances","data analysis, data visualization, data mining, data interpretation, data management, data reporting, power bi, power automate, tableau, sharepoint, microsoft office, project management, it applications, nerc cip, sensitive security clearances","data interpretation, data management, data mining, data reporting, dataanalytics, it applications, microsoft office, nerc cip, power automate, powerbi, project management, sensitive security clearances, sharepoint, tableau, visualization"
Data Engineer,Verinext,Nashville Metropolitan Area,https://www.linkedin.com/jobs/view/data-engineer-at-verinext-3773730890,2023-12-17,Tennessee,United States,Mid senior,Remote,"**** PLEASE NOTE - WE CAN NOT WORK WITH THIRD PARTIES OR OTHER AGENCIES ON THIS ROLE. APPLICANTS MUST BE U.S. CITIZENS OR GREEN CARD HOLDERS.****
Verinext, headquartered in Blue Bell, PA, is a leader in digital business transformation. We specialize in empowering our clients to expand their customer base and enhance workforce efficiency through visionary, development, and operational excellence in next-generation technology solutions. Our core expertise lies in digital applications, analytics, and hybrid IT, enabling businesses to undergo rapid transformation. Clients choose Verinext to support the full lifecycle of their next-generation digital business endeavors. We employ cutting-edge network engineering technologies to safeguard and preserve sensitive data, and we are steadfast in our commitment to ensuring excellence in network security. Join our team of network engineering experts and contribute to shaping the future of network security.
Verinext is seeking a talented and experienced Data Engineer who will play a pivotal role in data engineering and integrations. The ideal candidate will demonstrate a deep understanding of ERP, integrations, and database systems.
Key Responsibilities:
Must have integration development experience for enterprise-level implementation projects
5+ years in data engineering/integrations development
Functional/Technical Competencies
Technical Mapping Document Creation
Unit Test Documentation & Execution
Moderate foundation in Java
Strong foundation in Python
Ability to understand and build data structures
Advanced SQL Proficiency (PL/SQL, procedures, and functions; Oracle database experience preferred)
Working knowledge of shell scripting
Ability to understand/learn business issues and propose technical solutions
Ability to perform impact analysis for new requirements
Experience with source control/repository (SVN/CVS/GitHub)
Ability to prioritize and provide timely resolution to issues and requests
Experience with defect tracking tools like JIRA
Behavioral Competencies
Must be well-organized, efficient, and detail-oriented
Able to work independently and collaborate as a member of a team
Ability to understand priorities and expedite work in high-pressure situations
Proactive and self-directed
Goal-driven with the ability to be flexible
Excellent verbal and written communication skills
Show more
Show less","Data Engineering, ERP, Integrations, Database Systems, Java, Python, Data Structures, SQL, PL/SQL, Procedures, Functions, Oracle, Shell Scripting, Business Issues, Technical Solutions, Impact Analysis, Source Control/Repository, SVN, CVS, GitHub, Defect Tracking Tools, JIRA","data engineering, erp, integrations, database systems, java, python, data structures, sql, plsql, procedures, functions, oracle, shell scripting, business issues, technical solutions, impact analysis, source controlrepository, svn, cvs, github, defect tracking tools, jira","business issues, cvs, data engineering, data structures, database systems, defect tracking tools, erp, functions, github, impact analysis, integrations, java, jira, oracle, plsql, procedures, python, shell scripting, source controlrepository, sql, svn, technical solutions"
Data Engineer - Scala(U.S. remote),Railroad19,"Athens, GA",https://www.linkedin.com/jobs/view/data-engineer-scala-u-s-remote-at-railroad19-3783321739,2023-12-17,Athens,United States,Mid senior,Remote,"Railroad19, Inc.
is hiring
remote
Senior Data Engineers
to be a solid technical resource on a dynamic and growing team of accomplished engineers.
Our ideal candidate is passionate about creating well-architected solutions containing thoroughly tested code. The ability to communicate effectively and develop relationships by empathizing with client goals is a highly valued skill within our company culture.
Core Responsibilities:
Develop new and enhance existing application services
Writing tests to maintain code quality
Understand and adapt to our client's evolving business requirements within the television advertising domain.
Participate in detailed technical design sessions to understand client needs and provide productive feedback
Identify new opportunities, tools, and services to enhance the software platform
Support and troubleshoot issues, identify the root cause, and proactively recommend corrective actions
Skills & Experience:
Scala 2.12 + development experience
Passionate about developing clean and maintainable code with little or no side-effects
Experience building Restful APIs in Scala using Spark 2.4
Strong hands-on experience with AWS running spark jobs on ephemeral EMR clusters on AWS and S3.
Experience with relational and non-relational databases
Willingness to learn new technologies and takes pride in keeping up with the latest technologies and practices within the Scala and Spark development community
Excellent oral and written communication skills
Strong analytical and problem-solving skills
Self-directed and can effectively deliver solutions with little oversight
A bachelor's or master's degree in computer science, computer engineering, or other technical disciplines or equivalent work experience is preferred but not required.
$120,000 - $160,000 a year
Salary is commensurate with experience.
We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender, gender identity or expression, or veteran status. We are proud to be an equal-opportunity workplace.
#Hiringnow
We are actively hiring (Data Engineers)
#remote #Scala #DataEngineering #ApacheSpark
Show more
Show less","Scala 2.12, Spark 2.4, Restful APIs, AWS, EMR, S3, Relational databases, Nonrelational databases, EMR clusters on AWS, Computer science, Computer engineering","scala 212, spark 24, restful apis, aws, emr, s3, relational databases, nonrelational databases, emr clusters on aws, computer science, computer engineering","aws, computer engineering, computer science, emr, emr clusters on aws, nonrelational databases, relational databases, restful apis, s3, scala 212, spark 24"
Senior Data Analyst,Rockwell Automation,"South Portland, ME",https://www.linkedin.com/jobs/view/senior-data-analyst-at-rockwell-automation-3785893600,2023-12-17,Biddeford,United States,Mid senior,Remote,"Rockwell Automation is a global technology leader focused on helping the world’s manufacturers be more productive, sustainable, and agile. With more than 28,000 employees who make the world better every day, we know we have something special. Behind our customers - amazing companies that help feed the world, provide life-saving medicine on a global scale, and focus on clean water and green mobility - our people are energized problem solvers that take pride in how the work we do changes the world for the better.
We welcome all makers, forward thinkers, and problem solvers who are looking for a place to do their best work. And if that’s you we would love to have you join us!
Job Description
The Data Analyst will partner with Business stakeholders to understand and align business strategies, requirements and align them to their priorities. You will collaborate with the business and technology partners to translate high-level business requirements into functional, non-functional, and data requirements that will be used by the design and development teams to build the technical solutions to fulfills the business needs. You will report to the Senior Data Product Manager.
Key Responsibilities
Gather and validate business / technology / data requirements to establish scope and define project impact, outcome criteria, and metrics.
Translate conceptual user requirements into technical requirements.
Lead Program Increment sessions with Partners to identify 3-6-month priorities.
Ensure the quality, accuracy and efficiency of data, supporting data governance. Guide metadata population and approvals with knowledge of business purpose.
Communicate with Project team to guide proposed technical solutions. Direct Data engineers on implementation tasks. Analyze suggestions in any production support issues with Data/Analytics Engineers in problem resolution.
Partner with Enterprise Architect to create data models in line with organization's data architecture.
Apply use of technology modeling tools (ie Power BI) to build working prototypes.
Help map out team-to-team dependencies and collaborate with capability teams accordingly.
Basic Qualifications
Bachelor's Degree.
Legal authorization to work in the US is required. We will not sponsor individuals for employment visas, now or in the future, for this job opening.
Preferred Qualifications
Typically requires a minimum of 8 years of related experience.
Bachelor's degree in computer science, management information systems or related field.
Experience with basic system engineering, object-oriented design, information risk and security guidelines and architecture standards.
Well versed in Analytics & Insights methodologies and tools – example Azure Data Factory Azure Data Lake storage G2, Azure SQL DW, AAS, Power BI, Python, Data exploration and mining.
Understanding of Finance processes is desired; ie General Ledger, Cost Center Accounting, Profitability Analysis.
Understanding of SAP transactional systems such as ECC, CRM, Hybris is desirable
Knowledge of Planning applications such as Hyperion, Enterprise Planning Management
This position is part of a job family. Experience will be the determining factor for position level and compensation.
What We Offer
Health Insurance including Medical, Dental and Vision
401k
Paid Time off
Parental and Caregiver Leave.
This job is remote friendly.
To learn more about our benefits package, please visit at www. raquickfind.com .
We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender, gender identity or expression, or veteran status. We are proud to be an equal opportunity workplace.
At Rockwell Automation we are dedicated to building a diverse, inclusive and authentic workplace, so if you're excited about this role but your experience doesn't align perfectly with every qualification in the job description, we encourage you to apply anyway. You may be just the right person for this or other roles.
We are an Equal Opportunity Employer including disability and veterans.
If you are an individual with a disability and you need assistance or a reasonable accommodation during the application process, please contact our services team at +1 (844) 404-7247.
Show more
Show less","Power BI, Python, Azure Data Factory, Azure Data Lake storage G2, Azure SQL DW, AAS, SQL, SAP, ECC, CRM, Hybris, Hyperion, Enterprise Planning Management, Python, Finance, General Ledger, Cost Center Accounting, Profitability Analysis, Objectoriented design, Information risk, Security guidelines, Architecture standards, Analytics, Insights, Data Governance, Metadata, Data modeling","power bi, python, azure data factory, azure data lake storage g2, azure sql dw, aas, sql, sap, ecc, crm, hybris, hyperion, enterprise planning management, python, finance, general ledger, cost center accounting, profitability analysis, objectoriented design, information risk, security guidelines, architecture standards, analytics, insights, data governance, metadata, data modeling","aas, analytics, architecture standards, azure data factory, azure data lake storage g2, azure sql dw, cost center accounting, crm, data governance, datamodeling, ecc, enterprise planning management, finance, general ledger, hybris, hyperion, information risk, insights, metadata, objectoriented design, powerbi, profitability analysis, python, sap, security guidelines, sql"
Config/Data Mgt Analyst 3 (Army Metering Program),COLSA,"Huntsville, AL",https://www.linkedin.com/jobs/view/config-data-mgt-analyst-3-army-metering-program-at-colsa-3750653548,2023-12-17,Huntsville,United States,Mid senior,Onsite,"General Summary
Performs somewhat complex to complex Army Metering Program (AMP) configuration and data management assignments, requiring use of applied concepts, principles, theories, practices, and techniques.
Principal Duties and Responsibilities (*Essential functions)
Reviews, analyzes, and updates AMP packages and/or documentation for technical content, completeness, proper authorizations, and adherence to specific requirements, policies, and theories.
Establishes baselines on hardware and software systems.
Reviews AMP drawings, changes, and documentation.
Records, tracks, and maintains status of documents and action items.
Develops documentation for AMP such as Standard Operating Procedure and Tactics Techniques and Procedures (TTP)s.
Develops and generates special AMP reports.
May develop and prepare specific requirements, standards, plans, and procedures.
May provide AMP technical advice and guidance on methods, procedures, and requirements.
May participate in AMP special studies and projects.
At COLSA, people are our most valuable resource and centered at our core value. We invite you to unite your talents with opportunity and be a part of our “Family of Professionals!” Learn about our employee-centric culture and benefits here.
Show more
Show less","Army Metering Program, AMP, Baselines, Standard Operating Procedure, Tactics Techniques and Procedures","army metering program, amp, baselines, standard operating procedure, tactics techniques and procedures","amp, army metering program, baselines, standard operating procedure, tactics techniques and procedures"
Configuration / Data / Records Manager with Security Clearance,ClearanceJobs,"Huntsville, AL",https://www.linkedin.com/jobs/view/configuration-data-records-manager-with-security-clearance-at-clearancejobs-3753446712,2023-12-17,Huntsville,United States,Mid senior,Onsite,"Responsibilities PeopleTec is currently seeking a Configuration / Data / Records Manager to support our Huntsville, AL location. Duties:
Support the Program Manager and Chief Engineer with various programmatic tasks, actions, and meetings.
Process taskers from BC Front Office that have been assigned to the JEMINI Program Office and delegate tasks to appropriate JEMINI team leads as applicable; staff final tasker responses with JEMINI Program Manager and Chief Engineer as needed.
Serve as staff officer to coordinate program documents / products internally and externally, consolidate comments, and coordinate with appropriate team members for comment adjudication.
Document meeting minutes and action items during government and prime contractor meetings associated with various programmatic, budgetary, contractual, and technical topics.
Consolidate / track JEMINI Program Manager / Executive Level action items, coordinate status updates with the team, and provide the JEMINI Program Manager with periodic updates; ensure action item response pre-briefs / briefs are scheduled by the Action Officers.
Support the JEMINI PM and CE to define the Engineering Review process for the program; provide technical review of engineering deliverables, coordinate with others on the team and distribute material for review as part of the engineering review process, consolidate comments, review with the JEMINI CE, and provide to the JEMINI PM for final concurrence. Capture notes/actions during engineering reviews and ensure comments are incorporated and actions closed prior to approval
Maintain a consolidated list of program action items with updated status and track to closure; notify Chief Engineer and Program Manager of any upcoming critical or Executive / Senior Level action items that are due.
Coordinate with other team members to obtain action item status updates assist with prioritization of tasks associated with action item closure.
Assist with maintaining program documentation, records, and products to support various Programmatic and Engineering Reviews and program gates.
Develop processes and structure to organize program data; maintain repositories including web portals and shared folders as the program matures over time
Ensure critical program data is stored for historical purposes and coordinate with security as needed for storage of various media and based on classification / security procedures Qualifications Required Skills/Experience :
Travel: 10 %
Must be a U.S. Citizen
An active DoD Top Secret clearance is required to perform this work. Candidates are required to have an active Top Secret clearance upon hire, and the ability to maintain this level of clearance during their employment. * . Overview People First. Technology Always. PeopleTec, Inc. is an employee-owned small business founded in Huntsville, AL that provides exceptional customer support by employing and retaining a highly skilled workforce. Culture: The name ""PeopleTec"" was deliberately chosen to remind us of our core value system - our people. Our company's foundation was built on placing our employees and customers first. With an award-winning atmosphere, we have matured into a company that boasts the best and brightest across multiple technical fields. Career: At PeopleTec, we value your long-term goals. Whether it's through our continuing-education opportunities, our robust training programs, or our ""People First"" benefits package, PeopleTec truly believes that our best investments are our people. Come Experience It. #cjpost #dpost EEO Statement PeopleTec, Inc. is an Equal Employment Opportunity employer and provides reasonable accommodation for qualified individuals with disabilities and disabled veterans in its job application procedures. If you have any difficulty using our online system and you need an accommodation due to a disability, you may use the following email address, and/or phone number (256.319.3800) to contact us about your interest in employment with PeopleTec, Inc. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, genetic information, citizenship, ancestry, marital status, protected veteran status, disability status or any other status protected by federal, state, or local law. PeopleTec, Inc. participates in E-Verify.
Show more
Show less","DoD Top Secret Clearance, Microsoft Office Suite, Engineering Review Process, Security Procedures, Technical Writing, Data Consolidation, Action Item Tracking, Program Management, Records Management, Task Management, Team Coordination, Communication, Document Control, Meeting Minutes, Engineering Deliverables, Web Portals, Shared Folders, Classification/Security Procedures","dod top secret clearance, microsoft office suite, engineering review process, security procedures, technical writing, data consolidation, action item tracking, program management, records management, task management, team coordination, communication, document control, meeting minutes, engineering deliverables, web portals, shared folders, classificationsecurity procedures","action item tracking, classificationsecurity procedures, communication, data consolidation, document control, dod top secret clearance, engineering deliverables, engineering review process, meeting minutes, microsoft office suite, program management, records management, security procedures, shared folders, task management, team coordination, technical writing, web portals"
Data Recovery Engineer - Macintosh and iOS Platforms,DriveSavers Data Recovery,"Novato, CA",https://www.linkedin.com/jobs/view/data-recovery-engineer-macintosh-and-ios-platforms-at-drivesavers-data-recovery-3787791429,2023-12-17,Napa,United States,Mid senior,Onsite,"Seeking a candidate with 2–4 years of IT / Desktop Support and troubleshooting experience on the Macintosh platform (iOS) who is excited to learn the art of data recovery. Apple Genius and/or Apple Expert a plus.
Education
Associate/Bachelor Degree or equivalent work experience
Required Skills And Experience
ACMT preferred, but not required
2–4 years experience in an IT / Help Desk / Desktop Support / Apple Genius / Apple Expert environment
Familiarity with all native Apple applications
Maintains knowledge of current technology
Experience and understanding of computer hardware components for troubleshooting and upgrading purposes
Demonstrated experience in information technology operations
Familiarity with all Apple products including iPhone, iPad, and Time Capsule
Using Terminal to complete tasks
Must have passion for staying ahead of current and future technologies
Excellent interpersonal, organizational, and communication skills
Great customer service skills are required as you will need to work with customers over the phone on some projects
Strong attention to detail
Able to work independently as well as part of a team
Able to analyze and evaluate customer needs
Able to prioritize and meet deadlines in a fast-paced environment
If you have some of the qualifications but not all, please tell us why you think you should be considered.
Benefits
Competitive Salary
Monthly Bonuses
401K Retirement Plan
Medical Insurance
Dental and Vision Plan
Ongoing Training
Paid Holidays
Maternity/Paternity Leave
Events and Celebrations
Subsidized Stocked Refrigerator
Friendly Workspace
Green Business
From Employees on Glassdoor
Fun work environment, rewarding knowing that we get irreplaceable data back for customers.
Amazing work environment. Excellent incentives that encourage us all.
Friendly, inviting, and supportive work environment.
Great training, room for growth.
Engaging and manageable workload.
Each day brings new challenges with new and older technology.
Snacks and beverages are stocked regularly.
You feel valued as an employee. Comfortable work environment and strong incentives.
Drawings for various concerts, regular season and championship playoff (NHL, NFL, MLB, NBA) tickets, and other cool attractions are commonplace.
It’s been great to work as an individual to make a difference to customers and work next to others with that same goal.
Small business with a work environment that feels like a family, while also being known as the leader in data recovery.
Powered by JazzHR
mexrC0DUSf
Show more
Show less","ACMT, IT, Desktop Support, Troubleshooting, Apple Macintosh, iOS, Apple Genius, Apple Expert, Terminal, iPhone, iPad, Time Capsule","acmt, it, desktop support, troubleshooting, apple macintosh, ios, apple genius, apple expert, terminal, iphone, ipad, time capsule","acmt, apple expert, apple genius, apple macintosh, desktop support, ios, ipad, iphone, it, terminal, time capsule, troubleshooting"
Senior Health Data Analyst I,Partnership HealthPlan of California,"Fairfield, CA",https://www.linkedin.com/jobs/view/senior-health-data-analyst-i-at-partnership-healthplan-of-california-3693053486,2023-12-17,Napa,United States,Mid senior,Onsite,"Overview
The Senior Healthcare Data Analyst I contributes to the overall success of the organization by
developing analytic solutions that support activities related to health services utilization
management, care coordination, quality improvement and population health. Through analyzing
patient claims, member enrollment, and other data, the Senior Healthcare Data Analyst
participates in identifying progress, performance and opportunities for improvement on
programs, quality of care, patient experience, and other metrics. This position requires a
thorough understanding of healthcare data and workflows, combined with an extensive
experience working with large data sets, conducting data analysis, including standard statistical
software (SAS), and creating reports using Tableau.
Responsibilities
Works collaboratively with business partners, other analysts, and IT to gather and integrate data from disparate sources.
Responds to ad hoc data requests from business units and leadership
Assists in design and development of data collection strategies, aggregation, analysis, and reporting to ensure data integrity and enhance information value.
Participates in design and interpretation of data analyses; provides recommendations for improvement of data quality and reporting.
Helps build, manage, and/or enhance predictive models
Assesses reporting and automation requirements and develops appropriate solutions.
Maintains in-depth knowledge of health plan operations, including claims processing, utilization management, quality improvement activities and pay for performance programs.
Critically analyzes data, draws conclusions and effectively articulates results.
Presents data and conclusions to non-technical audience; uses data visualizations and summaries to highlight key findings.
Creates and maintains thorough and consistent documentation of programs used to create reports.
Manages and prioritizes workload while meeting deliverables and expectations.
Works autonomously and collaboratively with report requestors, providing guidance to define report requirements and validate results.
Works collaboratively across departments to understand and meet the organization’s analytic needs.
Secondary Duties And Responsibilities
Performs other assigned or needed activities required to assure success of the organization.
Participates in special projects as needed.
Performs other duties as assigned.
General Traits
Passionate about data, willing to acquire new skills and knowledge, flexible, selfmotivated, and very curious.
Creative problem-solver, critical thinker, independent worker, data-driven mentality.
Communicates clearly and directly, relates well to others, engages people, provides and seeks feedback, articulates clearly, actively listens.
Qualifications
Education and Experience
Bachelor’s degree with concentration in health informatics, health administration, public health, computing, epidemiology, statistics or related field, Master’s degree preferred. Minimum four (4) years of
experience in data analysis and reporting. Knowledge of major health plan operations: healthcare claims processing, membership, provider, and benefits; or equivalent combination of education and experience.
Excellent knowledge of data collection, analysis, statistics and data presentation with experience in data mining techniques and procedures. Experience using statistical packages for analyzing large data sets, SAS and/or SQL a plus. Experience working with administrative data, ideally health care data (Medicaid data a plus). Understanding of health data formats including claims, lab and pharmacy. Knowledge of clinical coding systems (e.g., ICD9, ICD10, CPT).
Special Skills, Licenses and
Certifications
Proficiency in inferential and predictive statistical analysis. MS Office, Excel, SQL, SAS, Tableau.
Ability to present complex information in an understandable and compelling manner.
Performance Based Competencies
Ability to quickly acquire in-depth knowledge of various systems related to claims processing, membership, provider, and benefits at PHC. Strong written and oral communication skills with ability to interpret and
understand technical requirements. Excellent analytical skills to troubleshoot and resolve data issues. Must be highly organized and proficient at multi-tasking. Must be willing and able to provide gracious assistance to users, providers, and other constituents of PHC.
Work Environment And Physical Demands
More than 50% of work time is spent at a video display terminal.
All HealthPlan employees are expected to:
Provide the highest possible level of service to clients;
Promote teamwork and cooperative effort among employees;
Maintain safe practices; and
Abide by the HealthPlan’s policies and procedures, as they may from time to time be updated.
HIRING RANGE:
$ 91,163.24 - $ 118,514.23
IMPORTANT DISCLAIMER NOTICE
The job duties, elements, responsibilities, skills, functions, experience, educational factors and the requirements and conditions listed in this job description are representative only and not exhaustive or definitive of the tasks that an employee may be required to perform. The employer reserves the right to revise this job description at any time and to require employees to perform other tasks as circumstances or conditions of its business, competitive considerations, or work environment change.
Show more
Show less","Healthcare Data Analytics, Data Integration, Ad Hoc Data Requests, Data Collection Strategies, Data Analysis, Statistical Software (SAS), Tableau, Data Visualization, Report Creation, Predictive Modeling, Health Plan Operations, Claims Processing, Utilization Management, Quality Improvement, Pay for Performance Programs, Inferential and Predictive Statistical Analysis, MS Office, Excel, SQL, Tableau","healthcare data analytics, data integration, ad hoc data requests, data collection strategies, data analysis, statistical software sas, tableau, data visualization, report creation, predictive modeling, health plan operations, claims processing, utilization management, quality improvement, pay for performance programs, inferential and predictive statistical analysis, ms office, excel, sql, tableau","ad hoc data requests, claims processing, data collection strategies, data integration, dataanalytics, excel, health plan operations, healthcare data analytics, inferential and predictive statistical analysis, ms office, pay for performance programs, predictive modeling, quality improvement, report creation, sql, statistical software sas, tableau, utilization management, visualization"
Data and Communication Analyst,Professional Diversity Network,"Martinez, CA",https://www.linkedin.com/jobs/view/data-and-communication-analyst-at-professional-diversity-network-3786829622,2023-12-17,Napa,United States,Mid senior,Onsite,"Job Description And Duties
This position supports the California Department of Public Health's (CDPH) mission and strategic plan by ensuring quality standards in clinical and public health laboratories, tissue and blood banks, the production of biologics, and laboratory scientists through licensing, examination, inspection, education, and proficiency testing.
The incumbent works under the general supervisor of the Staff Services Manager I (SSM I).
The attached duty statement indicates whether this position is eligible for telework. All employees who telework are required to be California residents in accordance with Government Code 14200, and may be required to report to a CDPH office, when needed. Candidates who reside outside of the state of California may be interviewed; however, the selected candidate must have a primary residency in the state of California prior to appointment (and continue to maintain California residency) as a condition of employment. Failure to meet this requirement may result in the job offer being rescinded.
Please let us know how you heard about our position by taking this brief survey:
https://www.surveymonkey.com/r/CDPHRecruitment
You will find additional information about the job in the
Duty Statement
.
Minimum Requirements
You will find the Minimum Requirements in the Class Specification.
STAFF SERVICES ANALYST
Additional Documents
Job Application Package Checklist
Duty Statement
Position Details
Job Code #:
JC-407094
Position #(s):
580-750-5157-718
Working Title:
Data and Communication Analyst
Classification:
STAFF SERVICES ANALYST $3,534.00 - $4,428.00 A $3,826.00 - $4,789.00 B $4,588.00 - $5,744.00 C
# of Positions:
1
Work Location:
Contra Costa County
Telework:
Telework
Job Type:
Permanent, Full Time
Department Information
At the California Department of Public Health (CDPH), equity, diversity, and inclusion are at the core of our mission to advance the health and well-being of California's diverse people and communities. We are genuinely and strongly committed to cultivating and preserving a culture of inclusion and connectedness where we can grow and learn together with a diverse team of employees. In recruiting for team members, we welcome the unique contributions that you can bring to us and the work we do.
The Center for Laboratory Sciences (CLS) is based on the Richmond Campus which employs more than 1,200 people. The Richmond Campus is a flat, 29-acre property located in Richmond's Marina Bay. The CLS serves to protect and promote the health of all Californians through innovative and collaborative infectious disease and environmental testing, including provision of investigation and surveillance activities which form the basis of disease response and prevention. The CLS works to bring together emerging scientific capabilities in testing, analytics, and communications reflecting a new level of coordination, support, and leadership for the public health laboratory system at the state, local and national level. In addition, the Center relies on a continuous improvement infrastructure as a core value in developing solutions that emphasize a culture of quality and performance through performance management success.
Richmond campus offers FREE parking and has FREE charging stations!
Special Requirements
For experience/education to qualify during the application screening process, and to ensure that minimum qualifications can be determined, applicants should include all employment history on the Employment Application (STD 678) and/or Resume, including detailed job descriptions, hours worked per week, and start/end dates (MM/DD/YYYY). Application packages without this information will experience delayed processing times and your eligibility for this position may be impacted.
A completed State application (STD. 678) and any other relevant documents (e.g. unofficial transcript, copy of degree, resume, etc.) should be submitted electronically via your CalCareers Account. Please reference Job Control # (407094) and indicate the basis of your eligibility in the Examination(s) or Job Title(s) section. SROA and surplus candidates should submit a copy of their letter with their application. Please remove any confidential information (i.e. social security number, date of birth) from your documents prior to submission.
Complete Application Packages (including your Examination/Employment Application (STD 678) and applicable or required documents) must be submitted to apply for this Job Posting. Application Packages may be submitted electronically through your CalCareers Account at www.CalCareers.ca.gov. Submitting an electronic application through your CalCareers account is strongly recommended since electronic applications will be received/processed faster than other methods of filing. Please submit only one application.
If you are unable to submit your application electronically through your CalCareers account, please email apply@cdph.ca.gov for assistance and a CDPH Human Resources Division staff member will contact you to assist with the online application process or, a hard copy application package may be submitted through an alternative method as explained in the How to Apply section below. When submitting your application in hard copy, a completed copy of the Application Package listing must be included.
Application Instructions
Completed applications and all required documents must be received or postmarked by the Final Filing Date in order to be considered. Dates printed on Mobile Bar Codes, such as the Quick Response (QR) Codes available at the USPS, are not considered Postmark dates for the purpose of determining timely filing of an application.
Final Filing Date: 12/27/2023
Who May Apply
Individuals who are currently in the classification, eligible for lateral transfer, eligible for reinstatement, have list eligibility, are in the process of obtaining list eligibility, or have SROA and/or Surplus eligibility (please attach your letter, if available). SROA and Surplus candidates are given priority; therefore, individuals with other eligibility may be considered in the event no SROA or Surplus candidates apply. Individuals who are eligible for a Training and Development assignment may also be considered for this position(s).
Applications will be screened and only the most qualified applicants will be selected to move forward in the selection process. Applicants must meet the Minimum Qualifications stated in the Classification Specification(s).
How To Apply
Complete Application Packages (including Your Examination/Employment Application (STD 678) And Applicable Or Required Documents) Must Be Submitted To Apply For This Job Posting. Application Packages May Be Submitted Electronically Through Your CalCareer Account At Www.CalCareers.ca.gov. When Submitting Your Application In Hard Copy, a Completed Copy Of The Application Package Listing Must Be Included. If You Choose To Not Apply Electronically, a Hard Copy Application Package May Be Submitted Through An Alternative Method Listed Below:
Address for Mailing Application Packages
You may submit your application and any applicable or required documents to:
Department of Public Health Attn: Classification & Certification Unit P.O. Box 997378 MS 1700-1702 Sacramento, CA 95899-7378
Address for Drop-Off Application Packages
You may drop off your application and any applicable or required documents at:
Department of Public Health Classification & Certification Unit 1615 Capitol Avenue Suite 73.430 Sacramento, CA 95814 08:00 AM - 05:00 PM
Required Application Package Documents
The Following Items Are Required To Be Submitted With Your Application. Applicants Who Do Not Submit The Required Items Timely May Not Be Considered For This Job:
Current version of the State Examination/Employment Application STD Form 678 (when not applying electronically), or the Electronic State Employment Application through your Applicant Account at www.CalCareers.ca.gov. All Experience and Education relating to the Minimum Qualifications listed on the Classification Specification should be included to demonstrate how you meet the Minimum Qualifications for the position.
Resume is required and must be included.
Other - A Cover Letter is required and must be submitted with your application package.
Applicants requiring reasonable accommodations for the hiring interview process must request the necessary accommodations if scheduled for a hiring interview. The request should be made at the time of contact to schedule the interview. Questions regarding reasonable accommodations may be directed to the EEO contact listed on this job posting.
Desirable Qualifications
In addition to evaluating each candidate's relative ability, as demonstrated by quality and breadth of experience, the following factors will provide the basis for competitively evaluating each candidate:
Critical thinking skills
Demonstrate ability to communicate effectively, both orally and in writing
Balance multiple priorities simultaneously in a fast-paced environment
Ability to work under pressure, execute sound judgement and exercise a high degree of confidentiality
Ability to organize and prioritize multiple assignments and meet deadlines
Ability to work independently and/or collaboratively in a team environment
Experience working with Databases is a plus but not required
Benefits
Benefit information can be found on the CalHR website and the CalPERS website.
Contact Information
The Human Resources Contact is available to answer questions regarding the application process. The Hiring Unit Contact is available to answer questions regarding the position.
Human Resources Contact:
Human Resources Division
(916) 445-0983
apply@cdph.ca.gov Hiring Unit Contact:
Najib Gul
(510) 620-3815
najib.gul@cdph.ca.gov
Please direct requests for Reasonable Accommodations to the interview scheduler at the time the interview is being scheduled. You may direct any additional questions regarding Reasonable Accommodations or Equal Employment Opportunity for this position(s) to the Department's EEO Office.
EEO Contact:
EEO Office
(916) 445-0938
California Relay Service: 1-800-735-2929 (TTY), 1-800-735-2922 (Voice) TTY is a Telecommunications Device for the Deaf, and is reachable only from phones equipped with a TTY Device.
Cover Letter Instructions
Cover Letter Required: No Specific
Arial Font 12, Single Space
PLEASE NOTE: Resumes, letters, Statement of Qualifications, transcripts, degrees, your state application, writing sample and other materials will not take the place of the Cover Letter. Simply copying and pasting your resume will also not be accepted. Applications received without a CL may be rejected.
Equal Opportunity Employer
The State of California is an equal opportunity employer to all, regardless of age, ancestry, color, disability (mental and physical), exercising the right to family care and medical leave, gender, gender expression, gender identity, genetic information, marital status, medical condition, military or veteran status, national origin, political affiliation, race, religious creed, sex (includes pregnancy, childbirth, breastfeeding and related medical conditions), and sexual orientation.
It is an objective of the State of California to achieve a drug-free work place. Any applicant for state employment will be expected to behave in accordance with this objective because the use of illegal drugs is inconsistent with the law of the State, the rules governing Civil Service, and the special trust placed in public servants.
PDN-9ad7cd3a-b19c-4bdd-a2a1-52b1d4f31feb
Show more
Show less","Data Analysis, Data Communication, Critical Thinking, Written Communication, Verbal Communication, Teamwork, Prioritization, Time Management, Confidentiality, Databases","data analysis, data communication, critical thinking, written communication, verbal communication, teamwork, prioritization, time management, confidentiality, databases","confidentiality, critical thinking, data communication, dataanalytics, databases, prioritization, teamwork, time management, verbal communication, written communication"
Senior Data Analyst/ Scientist,Boehringer Ingelheim,"Bracknell, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-analyst-scientist-at-boehringer-ingelheim-3776937252,2023-12-17,Basingstoke, United Kingdom,Mid senior,Onsite,"Purpose of the job
To work with structured data to solve tangible business problems using tools like SQL, R or Python programming languages, data visualization software including Tableau & MicroStrategy, and statistical analysis.
Identify patterns and trends to highlight problems or leverage opportunities for UK/Ireland Human Pharma Business Unit (HPBU), its stakeholders, and cross-functional teams. Supporting UK & Ireland HP business by simplifying data management and consolidate business vision behind technical topics
Accountabilities
Deliver standardised reporting, ensuring that all the basic needs are covered, incorporating analytics within the standard reports to facilitate the proper interpretation of customer level data. Design, development, and maintenance of reports and/or dashboards that stay current with ICP KPIs and support the plans of all levels of sales and marketing internal customers. Utilise best practice on reporting concepts, automation, data visualisation and methodology within the BI defined reporting environment. Work in conjunction with EUCAN A&I team to continuously enhance skills and knowledge
We are looking for someone who has;
Strong data analysis and general analytic skills in support of business programs.
Strong analytical skills with the ability to collect, organise, analyse, and disseminate significant amounts of information with attention to detail and accuracy. - Ability to handle big data
Strong knowledge in pharma environment and matrix.
Ability to convey complex information in business terms; interpret actionable insights
Able to manage timelines and complexity effectively.
Data and analytic systems and tools strong heavy user (SQL, Excel, Access, Adobe Analytics, SAS, R, Python, Tableau, MicroStrategy or similar)
Technical expertise regarding data models
Coding knowledge to complement data management (XML, R, Phyton, Javascript…)
Empathetic and dynamic
Proactive and able to reach out across functions for new ideas or to help solve problems
Business vision with a hybrid mindset to be able to work with IT teams
High level of commitment with BI values.
Influence skills and credibility strong impact. Demonstrating ability to build solid argumentations and convictions
Digital skills - Understanding Omnichannel Environment, Medium knowledge in digital channels as websites and email and A/B or Multivariant testing
Expert in tagging content for business purposes
Customer and customer facing oriented
We are also looking for someone who is;
Passionate to work cross functional
Team player
Self-motivated and flexible
Project Management leadership
AAI (behaviours)
Show more
Show less","SQL, R, Python, Tableau, MicroStrategy, Data visualization, Statistical analysis, Business intelligence, Reporting, Data management, Data analysis, Big data, Pharma environment, SAS, XML, Javascript, Coding, Data models, Omnichannel Environment, Digital channels, A/B testing, Multivariant testing, Tagging, Project Management","sql, r, python, tableau, microstrategy, data visualization, statistical analysis, business intelligence, reporting, data management, data analysis, big data, pharma environment, sas, xml, javascript, coding, data models, omnichannel environment, digital channels, ab testing, multivariant testing, tagging, project management","ab testing, big data, business intelligence, coding, data management, data models, dataanalytics, digital channels, javascript, microstrategy, multivariant testing, omnichannel environment, pharma environment, project management, python, r, reporting, sas, sql, statistical analysis, tableau, tagging, visualization, xml"
Customer Service Representative/Data Analyst/Data Entry Clerk,Northwellhealth,"Reading, England, United Kingdom",https://uk.linkedin.com/jobs/view/customer-service-representative-data-analyst-data-entry-clerk-at-northwellhealth-3744232276,2023-12-17,Basingstoke, United Kingdom,Mid senior,Onsite,"Summary:
The Data Analyst will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : application@jobsolutionsai.online
Powered by Webbtree
Show more
Show less","Data Analysis, Data Interpretation, Data Insights, Statistical Techniques, Performance Metrics, Business Questions, Business Processes, A/B Testing, Data Quality, Data Collection, Data Cleansing, Data Manipulation, DataDriven Reports, SQL, R, Python, Tableau, Power BI, Statistical Modeling, Hypothesis Testing, Data Management, ETL","data analysis, data interpretation, data insights, statistical techniques, performance metrics, business questions, business processes, ab testing, data quality, data collection, data cleansing, data manipulation, datadriven reports, sql, r, python, tableau, power bi, statistical modeling, hypothesis testing, data management, etl","ab testing, business processes, business questions, data collection, data insights, data interpretation, data management, data manipulation, data quality, dataanalytics, datacleaning, datadriven reports, etl, hypothesis testing, performance metrics, powerbi, python, r, sql, statistical modeling, statistical techniques, tableau"
Senior Data Engineer (Security Cleared),Energy Jobline,"Andover, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-security-cleared-at-energy-jobline-3772985143,2023-12-17,Basingstoke, United Kingdom,Mid senior,Onsite,"Security Cleared Senior Data Engineer
4 Months initially
Andover, Hampshire- hybrid. Must be willing to be in the office to start, then 1 day per week on average once up to speed
Apply online only) (Inside IR35)
My client, a high security government based organisation are looking for a Security Cleared Senior Data Engineer to join their fast paced team on an initial 4 month contract.
Additional Information-
This work is still in progress and aims to improve the granularity/richness of the Demand signal by building activity stacks from an event level perspective, ""bottom up"", with eight stacks already built and another three in development. Having a more granular event level view of the future activity enables a more accurate and informed support solution. The stacks are built from manual ""data cuts"" from various existing applications and sources. This process is very time consuming and therefore requires automation from a data point of view; this will enable the full breadth of equipment to be captured. The work to automate the data feeds from OPUS/Churchill/TAFMIS is already underway.
Experience/knowledge of MOD IT systems Essential- CHURCHILL and TAFMIS and OPUS
Solid background in data analysis & manipulation
Knowledge of equipment support planning within the armed forces- beneficial/desirable
Disclaimer
This vacancy is being advertised by either Advanced Resource Managers Limited, Advanced Resource Managers IT Limited or Advanced Resource Managers Engineering Limited (""ARM""). ARM is a specialist talent acquisition and management consultancy. We provide technical contingency recruitment and a portfolio of more complex resource solutions. Our specialist recruitment divisions cover the entire technical arena, including some of the most economically and strategically important industries in the UK and the world today. We will never send your CV without your permission. Where the role is marked as Outside IR35 in the advertisement this is subject to receipt of a final Status Determination Statement from the end Client and may be subject to change
Show more
Show less","Data Engineering, Data Analysis, Data Manipulation, MOD IT systems (CHURCHILL TAFMIS OPUS), Equipment support planning","data engineering, data analysis, data manipulation, mod it systems churchill tafmis opus, equipment support planning","data engineering, data manipulation, dataanalytics, equipment support planning, mod it systems churchill tafmis opus"
Senior Software Engineer Team Leader - Data Acquisition,Diamond Light Source,"Harwell, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-software-engineer-team-leader-data-acquisition-at-diamond-light-source-3780694271,2023-12-17,Basingstoke, United Kingdom,Mid senior,Onsite,"Senior Software Engineer Team Leader - Data Acquisition
Full Time Salary: £44,666 - £60,430
Post Type: Full time / Permanent
Provisional Interview Dates: 26/01/2024
Closing Date: 07/01/2024
Ref No: 11268
Based onsite at Diamond Light Source- some home working available
About Us
Diamond Light Source is the UK’s national synchrotron; a huge scientific facility designed to produce very intense beams of X-rays, infrared, and ultraviolet light. Our scientists use the light to study a vast range of subject matter, from new medicines and treatments for disease to innovative engineering and cutting-edge technology.
Diamond is one of the most advanced scientific facilities in the world, and its pioneering capabilities and talented staff are helping to keep the UK at the forefront of scientific research.
About the Data Acquisition Group
The Data Acquisition (DAQ) group develops software to interface with and control the hardware which allows experiments to be performed and monitored as well as guiding the end user to tools for data reduction and analysis. The group works closely with scientists, motion, and controls engineers to continuously develop and improve software solutions for scientific data acquisition and on-the-fly monitoring and data analysis.
The current DAQ software platform is a client-server application called GDA (Generic Data Acquisition) which is written in Java and uses the Eclipse RCP framework. To facilitate the advanced scientific capabilities that are expected from Diamond-II (the planned Synchrotron upgrade) the DAQ software is being modernised and will result in a service-based architecture. The new Acquisition Platform, Athena, will be configured to deliver advanced capabilities for a new collection of state of-the-art Flagship Beamlines. Over the next few years existing beamline software will be migrated from GDA to Athena.
About the Role
We now have an opportunity for a senior software engineer within the DAQ group to lead the team supporting and developing DAQ software for Diamond’s Soft Condensed Matter (SCM) and Crystallography science groups.
This role involves technically guiding and leading the team developing DAQ solutions for these science groups as well as contributing to those solutions.
As a team leader in the DAQ Group you will work alongside other software engineers supporting a range of experimental techniques and developing DAQ capabilities in an open and collaborative environment. You will be required to interact with scientists and translate their specifications for scientific capabilities into software requirements.
You will liaise with controls, data analysis and information management engineers to put these requirements into action. You will collaborate with the beamline scientists and science group leaders to prioritise the work of your team and you will support the DAQ Core Team in the modernisation of the Acquisition Platform. You will also potentially participate in international collaborations within this domain, sharing best practice and supporting other facilities and synchrotrons within the collaboration.
About You
You will be qualified to degree level in a STEM subject, with experience in the full Software Development Lifecycle, using a modern high-level language, and with an understanding of good software design principles and design for usability. Knowledge and experience of developing software to control and monitor scientific or industrial equipment or IoT is preferable.
Experience of working with large and complex code bases would be an advantage, as would experience of Python, Java, Spring, JMS message-oriented middleware, Java RMI, the HDF5 file format, the Eclipse RCP platform, REST and Kubernetes.
You should display good communication, interpersonal and analytical skills, with a personal interest or experience in science.
You should additionally be able to demonstrate the following essential capabilities:
Strong leadership and organisation.
Proven experience in managing small teams of software engineers.
Proven experience in providing technical leadership.
Benefits
Diamond offers an exceptional benefits package to support staff in achieving a positive work/life balance. This includes 25 days annual leave plus 13 days of statutory and company holidays, along with flexible working hours and an excellent pension scheme. Staff also have access to a range of amenities on site including a nursery, cafes, a restaurant and sports and leisure facilities.
To Apply
Please use the online application process to apply and tell us why you believe you are suitable for this role.
Show more
Show less","Java, Python, Spring, Java RMI, Eclipse RCP, Kubernetes, HDF5, JMS, REST, Software Development Lifecycle, Software Design, Good communication, Interpersonal skills, Analytical skills, Strong leadership, Team management, Technical leadership","java, python, spring, java rmi, eclipse rcp, kubernetes, hdf5, jms, rest, software development lifecycle, software design, good communication, interpersonal skills, analytical skills, strong leadership, team management, technical leadership","analytical skills, eclipse rcp, good communication, hdf5, interpersonal skills, java, java rmi, jms, kubernetes, python, rest, software design, software development lifecycle, spring, strong leadership, team management, technical leadership"
Data Analyst,RED Global,"Newbury, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-analyst-at-red-global-3780638752,2023-12-17,Basingstoke, United Kingdom,Mid senior,Hybrid,"Data Analyst Inside IR35 / Hybrid from Newbury / 6 months / Start ASAP
Rate: £300 - £330
MUST HAVE:
BPSS (Baseline Personnel Security Standard) certified
Some travel required between UK offices, as required
Key accountabilities and decision ownership:
· Manipulate, analyse, and interpret complex operational data sets.
· Mine and analyse large datasets, draw valid inferences, and present them successfully to stakeholders.
· Identify areas to increase efficiency and automation of processes.
· Produce and track key performance indicators and reporting processes.
· Assessing the data's performance on an ongoing basis
· Identifying and mitigating risks involved with the transference of data.
· Work with Business Process Architect and Process Owner by in creating data models and interpretation of the data.
· Work with the Benefit tracking manager to ensure the evidence is available to justify the improved efficiency and productivity. Core competencies, knowledge, and experience:
· 3+ years of experience in data modelling, data cleansing and data enrichment techniques
· Excellent numerical and analytical skills
· Strong presentation and communication skills
· Ability to test hypotheses from raw data sets and draw meaningful conclusions.
· Experience in statistical methodologies and data analysis techniques.
· Capacity to develop and document procedures and workflows.
· Ability to produce clear graphical representations and data visualizations.
· Exceptional attention to detail and problem-solving skills
Must have technical / professional qualifications:
· You have studied Statistics, Physics, Maths, Engineering, Econometrics, Macroeconomics or any other relevant field
· Proficient in R, Python and SQL, Excel, SAS, SPSS as well as business intelligence platforms such as Tableau, D3, Qlik Sense
· Proven abilities in mathematics, computer science and analysis to handle vast amounts of data
· Understanding of data modelling and data based solutions
· Certifications – Six Sigma, Lean, ITIL, Agile desirable
Show more
Show less","Baseline Personnel Security Standard (BPSS), R, Python, SQL, Excel, SAS, SPSS, Tableau, D3, Qlik Sense, Agile, Six Sigma, Lean, ITIL, Data modelling, Data visualization, Data analysis, Statistics, Physics, Math, Engineering, Econometrics, Macroeconomics","baseline personnel security standard bpss, r, python, sql, excel, sas, spss, tableau, d3, qlik sense, agile, six sigma, lean, itil, data modelling, data visualization, data analysis, statistics, physics, math, engineering, econometrics, macroeconomics","agile, baseline personnel security standard bpss, d3, data modelling, dataanalytics, econometrics, engineering, excel, itil, lean, macroeconomics, math, physics, python, qlik sense, r, sas, six sigma, spss, sql, statistics, tableau, visualization"
Data Analyst,E-Solutions,"Newbury, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-analyst-at-e-solutions-3785509669,2023-12-17,Basingstoke, United Kingdom,Mid senior,Hybrid,"Role: Data Analyst
Location: Newbury, UK (Hybrid)
Type: Permanent
Note: Looking for someone who is residing in UK for last 5 years.
Key accountabilities and decision ownership:
· Manipulate, analyze, and interpret complex operational data sets.
· Mine and analyze large datasets, draw valid inferences, and present them successfully to stakeholders.
· Identify areas to increase efficiency and automation of processes.
· Produce and track key performance indicators and reporting processes.
· Assessing the data's performance on an ongoing basis
· Identifying and mitigating risks involved with the transference of data.
· Work with Business Process Architect and Process Owner by in creating data models and interpretation of the data.
· Work with the Benefit tracking manager to ensure the evidence is available to justify the improved efficiency and productivity. Core competencies, knowledge, and experience:
· 3+ years of experience in data modelling, data cleansing and data enrichment techniques
· Excellent numerical and analytical skills
· Strong presentation and communication skills
· Ability to test hypotheses from raw data sets and draw meaningful conclusions.
· Experience in statistical methodologies and data analysis techniques.
· Capacity to develop and document procedures and workflows.
· Ability to produce clear graphical representations and data visualizations.
· Exceptional attention to detail and problem-solving skills
Must have technical / professional qualifications:
· You have studied Statistics, Physics, Maths, Engineering, Econometrics, Macroeconomics or any other relevant field
· Proficient in R, Python and SQL, Excel, SAS, SPSS as well as business intelligence platforms such as Tableau, D3, Qlik Sense
· Proven abilities in mathematics, computer science and analysis to handle vast amounts of data
· Understanding of data modelling and data based solutions
· Certifications – Six Sigma, Lean, ITIL, Agile desirable Location
Show more
Show less","Data Analysis, Data Visualization, R, Python, SQL, Excel, SAS, SPSS, Tableau, D3, Qlik Sense, Statistics, Physics, Maths, Engineering, Econometrics, Macroeconomics, Six Sigma, Lean, ITIL, Agile","data analysis, data visualization, r, python, sql, excel, sas, spss, tableau, d3, qlik sense, statistics, physics, maths, engineering, econometrics, macroeconomics, six sigma, lean, itil, agile","agile, d3, dataanalytics, econometrics, engineering, excel, itil, lean, macroeconomics, maths, physics, python, qlik sense, r, sas, six sigma, spss, sql, statistics, tableau, visualization"
Quantitative Data Engineer,Jobs for Humanity,"Worcester, MA",https://www.linkedin.com/jobs/view/quantitative-data-engineer-at-jobs-for-humanity-3786353517,2023-12-17,Fitchburg,United States,Mid senior,Onsite,"Company Description
Jobs for Humanity is partnering with MassMutual to build an inclusive and just employment ecosystem. Therefore, we prioritize individuals coming from the following communities: Refugee, Neurodivergent, Single Parent, Blind or Low Vision, Deaf or Hard of Hearing, Black, Hispanic, Asian, Military Veterans, the Elderly, the LGBTQ, and Justice Impacted individuals. This position is open to candidates who reside in and have the legal right to work in the country where the job is located.
Company Name: MassMutual
Job Description
Quantitative Data Engineer - Full-time Position
Location:
Boston, MA, Springfield, MA or New York, NY
The Opportunity:
We are seeking a Data Engineer to join our fast-paced and collaborative team. As a Data Engineer, you will work on exciting technology initiatives and utilize both technical skills and emotional intelligence to solve complex problems. You should have a strong background in Data Engineering, workflow orchestration, CI/CD pipelines, cloud platforms, and system architectures.
The Team:
Join our Quantitative Investment & Developer Operations team, which is part of MassMutual's Investment Management organization. This team consists of highly skilled professionals who provide collaborative and portfolio risk solutions to our portfolio managers. We value resilience, accountability, agility, continuous improvement, and work/life balance.
The Impact:
As a Quant Data Engineer, you will have the opportunity to make a meaningful impact by deploying data solutions and workflow automations. You will be responsible for designing, building, and maintaining complex ETL jobs, translating business requirements, ingesting data from various sources, ensuring data quality, and providing insight to guide the future development of our data platform. Additionally, you will collaborate with other developers, provide mentorship, evaluate tools and technologies, and work in an Agile development environment.
Minimum Qualifications:
Bachelor's degree in Computer Science, Finance, Business, or a related field
2+ years of experience in the IT and/or finance industry
Understanding of ETL methodologies and proficiency in Python
Hands-on experience with Python, advanced data processing using Python libraries, and working in a cloud environment
Experience with relational databases (SQL Server/PostgreSQL) and NoSQL databases (MongoDB)
Experience with GIT, code review/deployment, and Agile/SCRUM methodology
Ideal Qualifications:
Master's degree in Computer Science, Engineering, or a related field
4+ years of experience in the IT, quantitative, investment, or finance industry
Experience with troubleshooting, root cause analysis, orchestration and scheduling tools, and data reporting
Entrepreneurial mindset and experience with mobile app development
Excellent communication, problem-solving, organizational, and analytical skills
Data-driven mindset and ability to adapt to changing business priorities
Curiosity regarding emerging digital and technology trends
What to Expect:
Regular meetings with project teams
One-on-one meetings with your manager
Mentorship opportunities
Networking opportunities through Business Resource Groups
Access to learning content and resources
A company that values ethics, integrity, and offers industry-leading pay and benefits
MassMutual is an Equal Employment Opportunity Employer
We welcome all persons to apply, including those from diverse backgrounds such as the elderly, refugees, people with visible and invisible disabilities, LGBTQIA+, veterans, etc. Veterans are encouraged to apply, regardless of their discharge status. If you need accommodations during the application process, please contact us.
Show more
Show less","Python, SQL, MongoDB, GIT, Agile, SCRUM, NoSQL, ETL, Cloud Computing, Data Engineering, Data Processing, Relational Databases","python, sql, mongodb, git, agile, scrum, nosql, etl, cloud computing, data engineering, data processing, relational databases","agile, cloud computing, data engineering, data processing, etl, git, mongodb, nosql, python, relational databases, scrum, sql"
Data Engineer - Mastery,Jobs for Humanity,"Worcester, MA",https://www.linkedin.com/jobs/view/data-engineer-mastery-at-jobs-for-humanity-3786351820,2023-12-17,Fitchburg,United States,Mid senior,Onsite,"Company Description
Jobs for Humanity is partnering with MassMutual to build an inclusive and just employment ecosystem. Therefore, we prioritize individuals coming from the following communities: Refugee, Neurodivergent, Single Parent, Blind or Low Vision, Deaf or Hard of Hearing, Black, Hispanic, Asian, Military Veterans, the Elderly, the LGBTQ, and Justice Impacted individuals. This position is open to candidates who reside in and have the legal right to work in the country where the job is located.
Company Name: MassMutual
Job Description
At MassMutual, our mission is to help people protect their families, support their communities, and support one another. We believe in inspiring people to Live Mutual and working together to make a positive impact. We value diversity and believe that every voice should be heard. We are committed to creating an inclusive environment where all individuals, including elderly, refugees, people with visible and invisible disabilities, LGBTQIA+, and veterans, can thrive. We are currently seeking a Data Engineer to join our team. In this role, you will have the opportunity to work alongside exceptional individuals and contribute to the development of robust data solutions that meet the highest standards. You will collaborate with business partners to understand requirements and deliver valuable insights. Main responsibilities: - Design, build, and maintain complex data projects that have real business value - Translate high-level business requirements into technical specifications - Ingest data from various sources into our data lake and data warehouse - Cleanse and enrich data, ensuring data quality controls are in place - Provide guidance and contribute to the development of our data platform - Develop tools to streamline project delivery - Collaborate with other developers and provide mentorship - Evaluate and recommend tools, technologies, and processes - Work in an Agile development environment, delivering incremental improvements Basic qualifications: - Bachelor's degree in computer science, engineering, or a related field - 8+ years of experience in data analytics and warehousing - Deep knowledge of SQL and query optimization - Understanding of ELT methodologies and tools - Experience with troubleshooting and root cause analysis - Strong communication, problem-solving, and organizational skills - Ability to work independently and provide leadership to small teams Preferred qualifications: - Master's degree in computer science, engineering, or a related field - Experience working in a cloud environment (e.g., AWS) - Hands-on experience with Python - Familiarity with data processing technologies, such as Apache Spark or Kafka - Knowledge of orchestration and scheduling tools, like Apache Airflow - Experience with data reporting tools, such as Microstrategy, Tableau, Looker, and data cataloging tools like Alation We are an Equal Employment Opportunity employer, welcoming applicants from all backgrounds and walks of life. We encourage veterans to apply, regardless of their discharge status. If you need any accommodations in the application process, please reach out to us and let us know how we can assist you.
Show more
Show less","SQL, ELT, Python, Apache Spark, Kafka, Apache Airflow, Microstrategy, Tableau, Looker, Alation, Data Analytics, Data Warehousing, Agile","sql, elt, python, apache spark, kafka, apache airflow, microstrategy, tableau, looker, alation, data analytics, data warehousing, agile","agile, alation, apache airflow, apache spark, dataanalytics, datawarehouse, elt, kafka, looker, microstrategy, python, sql, tableau"
MDM Data Engineer,Jobs for Humanity,"Worcester, MA",https://www.linkedin.com/jobs/view/mdm-data-engineer-at-jobs-for-humanity-3786354359,2023-12-17,Fitchburg,United States,Mid senior,Onsite,"Company Description
Jobs for Humanity is partnering with MassMutual to build an inclusive and just employment ecosystem. Therefore, we prioritize individuals coming from the following communities: Refugee, Neurodivergent, Single Parent, Blind or Low Vision, Deaf or Hard of Hearing, Black, Hispanic, Asian, Military Veterans, the Elderly, the LGBTQ, and Justice Impacted individuals. This position is open to candidates who reside in and have the legal right to work in the country where the job is located.
Company Name: MassMutual
Job Description
Job Advertisement: Data Engineer
About Us
We are a diverse and inclusive company committed to providing equal employment opportunities to individuals from all walks of life.
We value diversity and welcome applications from elderly individuals, refugees, people with visible and invisible disabilities, LGBTQIA+ individuals, veterans, and anyone who brings unique perspectives and experiences.
Objectives of the Role
Design, build, and measure complex data processes using Informatica Power Center and MDM to improve data quality across various domains in our organization.
Work on a variety of projects to develop efficient data pipelines and solutions, addressing specific business needs.
Daily And Monthly Responsibilities
Design, construct, and assess data integration tasks to consolidate diverse data sources into a reliable data repository.
Create Master Data Management (MDM) solutions for different domains within our organization.
Provide support for MDM solutions, including data ingestion, master record creation, outbound data extracts, and integration with APIs.
Implement data modeling policies, processes, and standards, while also contributing feedback.
Document technical data, system flows, and database designs.
Ensure data accuracy and quality through the establishment of benchmarks and the development of tools/processes.
Analyze source system data and collaborate with various departments to understand emerging data patterns.
Translate business needs into technical specifications.
Contribute to the development and implementation of MDM solutions and other Information Management initiatives.
Basic Qualifications
Bachelor's degree in computer science or engineering.
5+ years of experience with Informatica Power Center.
5+ years of experience with data analytics, data modeling, and database design.
3+ years of coding and scripting experience (Python, Java, Scala).
3+ years of experience with Informatica MDM platform.
Experience with ELT methodologies and tools.
Proficiency in SQL tuning and troubleshooting.
A strong focus on data integrity and analytical skills.
Experience with Oracle database and AWS.
Knowledge of basic UNIX commands and shell scripts.
Experience with 3rd party job schedulers like Maestro.
Experience with RESTful APIs and data profiling tools.
Willingness to provide support outside business hours if required.
Excellent communication, problem-solving, organizational, and analytical skills.
Ability to work independently and legally authorized to work in the USA.
Preferred Qualifications
Master's degree in computer science or engineering.
Familiarity with agile project delivery process.
Knowledge of SQL for data access and analysis.
Ability to manage diverse projects impacting multiple roles and processes.
Strong troubleshooting and problem-solving skills.
Ability to adapt in a fast-changing environment.
Experience with Python and Kafka.
Basic knowledge of database technologies (Vertica, Redshift, etc.).
Experience in designing and implementing automated ETL processes.
We Encourage Applications from All Individuals
We are an Equal Employment Opportunity employer, welcoming applications from individuals of all backgrounds, including minority groups, females, sexual orientation/gender identity, individuals with disabilities, and protected veterans.
Veterans are welcome to apply, regardless of their discharge status.
Accommodation Assistance
If you require accommodation to complete the application process, please contact us and provide the details of the assistance you need. We are committed to ensuring equal opportunities for all applicants.
Show more
Show less","Informatica Power Center, Informatica MDM, ETL methodologies and tools, SQL tuning and troubleshooting, Oracle database, AWS, UNIX commands and shell scripts, 3rd party job schedulers, RESTful APIs, Data profiling tools, Python, Java, Scala, Data analytics, Data modeling, Database design, Data integration, Master Data Management, Data integrity, Analytical skills","informatica power center, informatica mdm, etl methodologies and tools, sql tuning and troubleshooting, oracle database, aws, unix commands and shell scripts, 3rd party job schedulers, restful apis, data profiling tools, python, java, scala, data analytics, data modeling, database design, data integration, master data management, data integrity, analytical skills","3rd party job schedulers, analytical skills, aws, data integration, data integrity, data profiling tools, dataanalytics, database design, datamodeling, etl methodologies and tools, informatica mdm, informatica power center, java, master data management, oracle database, python, restful apis, scala, sql tuning and troubleshooting, unix commands and shell scripts"
Senior Big Data Developer / Hadoop/ fullstack,Prorsum Technologies,"Worcester, MA",https://www.linkedin.com/jobs/view/senior-big-data-developer-hadoop-fullstack-at-prorsum-technologies-3766960677,2023-12-17,Fitchburg,United States,Mid senior,Onsite,"eligible for candidate who are in USA only
Only W2 candidates
who are willing to re-locate any where in USA
who can work on any employer
specification OPT / H1b Transfer / GC / GC EAD / TN visa / H4 EAD
who are willing to file H1b sponsorship can apply for my job post
Company Description
Prorsum Technologies, founded in 2016, is a technology solutions provider, offering high-quality professionals and services. Prorsum consultants are certified and skilled at providing mission-critical solutions that help clients adapt to the shifting demands of their environment, quicken their digital transformation activities, and stimulate creativity. Prorsum provides exceptional talent and deep understanding of the latest technologies to help clients remain competitive in areas such as Digital transformation, Cloud and On-Premise, SAP, Big data Analytics, AI/ML/Data sciences, Cybersecurity, and Devops. Headquartered in Houston, Texas, and with offices in Chicago, Illinois; Bay Area, California; and a Global Delivery Center in Hyderabad, India providing around-the-clock solutions for our customers.
Role: Sr. Big Data Engineer / Big Data Developer (10+ Years)
Location: Open / Onsite
Long Term Role
Salary: $60-70/Hr on W2 depends on Experience
Job Description:
This is a full-time, on-site role located in Worcester, MA for a Big Data Developer/Hadoop. The Big Data Developer/Hadoop will be responsible for developing, maintaining, and analyzing large-scale data processing systems using the Hadoop ecosystem, and creating scalable, high-performance data pipelines that support data ingestion, transformation, and dissemination.
Key Skills:
4+ years of Cloudera Admin/Pyspark /Big Data /Impala Experience – Must have.
Kafka & AWS Redshift experience – Good to have.
Experience in working with large teams in an Onsite/Offshore model.
Good Communication Skills.
Job Description:
We are seeking a highly skilled and experienced Big Data Engineer to join our dynamic team. The ideal candidate will have a strong background in Information Technology, with specific expertise in Cloudera Administration, Pyspark, Big Data, and Impala, totaling a minimum of 4 years of hands-on experience in these areas.
Overall, 9+ Years of experience, with hands-on Bigdata development, production Exp.
Experience with AWS - CI/CD
Experience with Linux/Unix scripting
Experience troubleshooting long running Unix/Linux processes, Analyzing query plans, etc.
Exposure to
Hadoop / Spark
Experience with
Stored Procedures
Experience with RDBMS (Good to have)
Hands-on experience writing / supporting Java/Spring boot applications.
Good communication skills
Big data, Spark, Kafka. ML Models, NLP, LLMs, exp in building, executing & deploying AI/ ML models, decision trees etc.
Qualifications
Bachelor’s degree in Computer Science, Information Systems, or related field
Proficiency with Big Data Technologies, particularly Hadoop, Spark, Hive, HBase, and MapReduce
Expertise in data modeling, data analysis, and database design, including SQL/NOSQL databases
Experience with programming languages such as Java, Python, Scala, or R
Experience in building data pipelines, processing huge data sets, ETL development, and using workflow/scheduling tools
Knowledge of tools and technologies such as Apache Storm, Kafka, and Flink is desirable
Experience working with AWS cloud services such as EC2, EMR, S3, RDS or Redshift
Excellent analytical, communication, and team collaboration skills
Strong understanding of statistical analysis, data mining techniques, and machine learning algorithms
Interested can email resume at
Keerthi@prorsum-tech.com.
Show more
Show less","Big Data Development, Hadoop, Cloudera Administration, Pyspark, Impala, AWS Redshift, Kafka, Linux/Unix Scripting, Stored Procedures, Java, Spring Boot, AI/ML Models, Decision Trees, Hadoop, Spark, Hive, HBase, MapReduce, SQL, NOSQL, Java, Python, Scala, R, Data Pipelines, ETL Development, workflow/scheduling tools, Apache Storm, Flink, AWS, EC2, EMR, S3, RDS, Redshift","big data development, hadoop, cloudera administration, pyspark, impala, aws redshift, kafka, linuxunix scripting, stored procedures, java, spring boot, aiml models, decision trees, hadoop, spark, hive, hbase, mapreduce, sql, nosql, java, python, scala, r, data pipelines, etl development, workflowscheduling tools, apache storm, flink, aws, ec2, emr, s3, rds, redshift","aiml models, apache storm, aws, aws redshift, big data development, cloudera administration, datapipeline, decision trees, ec2, emr, etl development, flink, hadoop, hbase, hive, impala, java, kafka, linuxunix scripting, mapreduce, nosql, python, r, rds, redshift, s3, scala, spark, spring boot, sql, stored procedures, workflowscheduling tools"
"Software Data Engineer, Java",Jobs for Humanity,"Worcester, MA",https://www.linkedin.com/jobs/view/software-data-engineer-java-at-jobs-for-humanity-3786349985,2023-12-17,Fitchburg,United States,Mid senior,Onsite,"Company Description
Jobs for Humanity is partnering with MassMutual to build an inclusive and just employment ecosystem. Therefore, we prioritize individuals coming from the following communities: Refugee, Neurodivergent, Single Parent, Blind or Low Vision, Deaf or Hard of Hearing, Black, Hispanic, Asian, Military Veterans, the Elderly, the LGBTQ, and Justice Impacted individuals. This position is open to candidates who reside in and have the legal right to work in the country where the job is located.
Company Name: MassMutual
Job Description
Job Description
At MassMutual, we have been committed to helping people protect their families, support their communities, and assist one another since 1851. We believe in inspiring people to Live Mutual and we are all about people helping people. We value diversity and welcome individuals from all walks of life to join our team.
What success looks like in this role
We are looking for someone who enjoys creating and delivering complex systems. You love coding and enjoy finding smart solutions to difficult problems. You understand the challenges of handling large amounts of data and are always exploring ways to use open-source tools to speed up development. You are passionate about learning new technologies, work well in a team, and communicate effectively.
Objectives of the role
Design, develop, and deliver scalable, reliable, and reusable components using technologies like Python, Java, AWS serverless (Lambda, Glue), Apache Spark, Apache Kafka, and REST.
Participate in all aspects of development, taking on the roles of both a developer and a component lead.
Closely collaborate with data users, including data engineers and data scientists, to understand and refine requirements.
Write code, create unit tests, and conduct code reviews.
Identify and resolve issues in code and data pipelines.
Evaluate and recommend tools, technologies, and processes for improvement and automation.
Collaborate with other developers and provide mentorship as needed.
Work closely with other teams to prevent and resolve technical issues.
Work in an Agile development environment, attending daily stand-up meetings and delivering incremental improvements.
Basic Qualifications
Minimum of 4 years of Java development experience.
Strong understanding of algorithms, design patterns, and writing efficient code.
Good knowledge of data and data processing tools (e.g. Spark, Kafka, SQL), relational databases, and analytics databases (e.g. Redshift, Vertica, Snowflake).
Experience with source control and CI/CD tools.
Proficiency in writing unit, integration, and load tests.
Excellent communication, problem-solving, organizational, and analytical skills.
Able to work independently and provide leadership to small developer teams.
Bachelor’s degree or equivalent work experience.
Preferred Qualifications
Experience building and deploying to cloud platforms like AWS, and using serverless architectures (e.g. Lambda, Glue).
At least 2 years of experience with big data and/or streaming technologies (e.g. Apache Spark, Apache Kafka, Apache Flink).
MassMutual is an equal opportunity employer that welcomes applications from all individuals, regardless of their race, gender, sexual orientation, gender identity, disability status, or veteran status. We value diversity and inclusivity.
If you require an accommodation to complete the application process, please contact us and let us know how we can assist you.
Show more
Show less","Python, Java, AWS Lambda, AWS Glue, Apache Spark, Apache Kafka, REST, CI/CD, Unit testing, Integration testing, Load testing, SQL, Redshift, Vertica, Snowflake, Cloud platforms, Serverless architectures","python, java, aws lambda, aws glue, apache spark, apache kafka, rest, cicd, unit testing, integration testing, load testing, sql, redshift, vertica, snowflake, cloud platforms, serverless architectures","apache kafka, apache spark, aws glue, aws lambda, cicd, cloud platforms, integration testing, java, load testing, python, redshift, rest, serverless architectures, snowflake, sql, unit testing, vertica"
"Principal Data Engineer, MS&T Robustness & Digital Strategies",Bristol Myers Squibb,"Devens, MA",https://www.linkedin.com/jobs/view/principal-data-engineer-ms-t-robustness-digital-strategies-at-bristol-myers-squibb-3782857319,2023-12-17,Fitchburg,United States,Mid senior,Onsite,"Working with Us
Challenging. Meaningful. Life-changing. Those aren’t words that are usually associated with a job. But working at Bristol Myers Squibb is anything but usual. Here, uniquely interesting work happens every day, in every department. From optimizing a production line to the latest breakthroughs in cell therapy, this is work that transforms the lives of patients, and the careers of those who do it. You’ll get the chance to grow and thrive through opportunities uncommon in scale and scope, alongside high-achieving teams rich in diversity. Take your career farther than you thought possible.
Bristol Myers Squibb recognizes the importance of balance and flexibility in our work environment. We offer a wide variety of competitive benefits, services and programs that provide our employees with the resources to pursue their goals, both at work and in their personal lives. Read more careers.bms.com/working-with-us.
Working with Us
Challenging. Meaningful. Life-changing. Those aren’t words that are usually associated with a job. But working at Bristol Myers Squibb is anything but usual. Here, uniquely interesting work happens every day, in every department. From optimizing a production line to the latest breakthroughs in cell therapy, this is work that transforms the lives of patients, and the careers of those who do it. You’ll get the chance to grow and thrive through opportunities uncommon in scale and scope, alongside high-achieving teams rich in diversity. Take your career farther than you thought possible.
Bristol Myers Squibb recognizes the importance of balance and flexibility in our work environment. We offer a wide variety of competitive benefits, services and programs that provide our employees with the resources to pursue their goals, both at work and in their personal lives. Read more careers.bms.com/working-with-us
Position Summary
At BMS, digital innovation and Information Technology are central to our vision of transforming patients’ lives through science. To accelerate our ability to innovate and guarantee supply to our patients around the world, we must unleash the power of technology. We are committed to being at the forefront of transforming the way medicine is made by harnessing the power of computer and data science, artificial intelligence, and other technologies to promote robust products and processes, faster decision making, and more efficient manufacturing and supply.
We are seeking an experienced and highly motivated data engineer to join the Robustness & Digital Strategies team within the Manufacturing Sciences & Technology (MS&T) organization. In this role, the Principal Data Engineer will be responsible for designing, building, and maintaining manufacturing data assets and data products to enable rapid investigation resolution and advanced multivariate model development for real-time process monitoring and control.
The ideal candidate will have exceptional background in data engineering, data systems, and data governance and will be comfortable working with both structured and unstructured data. Experience in Biopharma manufacturing processes and data types is a plus, but not required.
If you want an exciting and rewarding career that is meaningful and directly helps deliver lifesaving medicines to patients, consider joining our diverse team!
Key Responsibilities
Work as a member of the MS&T Robustness & Digital Strategies team to develop and implement data engineering solutions that deliver high-quality, contextualized datasets as an enabler of advanced process modelling and other analytics
Design and establish a scalable framework for engineering new features and processing modular datasets across different subject areas into modelling-ready data
Collaborate with Data & Supply Technology Excellence (DSTE) team within GPS IT to shape data and technology strategy and drive towards synergistic outcomes
Optimize or redesign existing data engineering solutions to improve efficiency or scalability
Devise and implement data engineering best practices across the team with a focus on short-term deliverables and strategic capabilities
Partner with and guide offshore data partner team who provides support in implementing, maintaining, and supporting data engineering pipeline
Mentor provide guidance to fellow Data Engineers where required
Leverage the latest advances in data engineering and analytics to design innovative solutions
Learn new technologies and lead proof-of-concepts to further innovate and optimize data engineering approaches
Acquire and maintain thorough understanding of internal and external manufacturing data landscape, including enterprise and site systems, data warehouses, and data lakes
Qualifications & Experience
Expected 9 years, 4 years with Ph.D., of experience in data engineering or DevOps environment
Minimum Bachelor’s degree in computer science, information systems, computer engineering, or equivalent experience
Advanced knowledge of Python or similar data engineering focused programming language
Hands-on experience implementing and operating cloud-based data ingestion, integration, transformation, storage, and virtualization solutions using company approved technologies such as AWS (Amazon Web Services) native services (S3, Glue, Athena, Redshift, RDS, Aurora, Lambda, etc.), Cloudera Data Platform (CDP), and Domino
In-depth experience with distributed processing systems like Apache Spark
Experience in DataOps workflow orchestration tools such as Apache Airflow, dbt, etc.
Deep experience and knowledge of
Software engineering principles testing (definition of unit tests, integration tests), setting up CI/CD pipelines in collaboration with DevOps teams, experience with Docker containers and kubernetes, experience developing or interacting with APIs
Data quality and validation principles experience with libraries like great-expectations, pandera, pydantic, pandas profiler
Data architecture principles data modeling, SQL query optimization, data warehouse design patterns
Security principles data encryption, access control, authentication and authorization
Team management skills strong track record of leading teams in the technical development of analytical solutions
Experience integrating with Spotfire or other visual analytics platforms like Tableau
Deep experience in definition and implementation of feature engineering
Good experience with agile/scrum development processes and concepts and with leveraging project management tools like Jira and Confluence
Experience managing multiple priorities and working in fast-paced, constantly evolving environment with a variety of cross-functional teams
Evaluates complex issues through analytical thinking and previous experience to consider short- and long-term implications and interdependencies
Excellent interpersonal, collaborative, team building, and communication skills to ensure effective collaborations within matrix teams. Demonstrated performance against cooperation principles and enterprise mindset.
Experience working in life sciences/biopharmaceutical industry is a plus
If you come across a role that intrigues you but doesn’t perfectly line up with your resume, we encourage you to apply anyway. You could be one step away from work that will transform your life and career.
Uniquely Interesting Work, Life-changing Careers
With a single vision as inspiring as “Transforming patients’ lives through science™ ”, every BMS employee plays an integral role in work that goes far beyond ordinary. Each of us is empowered to apply our individual talents and unique perspectives in an inclusive culture, promoting diversity in clinical trials, while our shared values of passion, innovation, urgency, accountability, inclusion and integrity bring out the highest potential of each of our colleagues.
On-site Protocol
Physical presence at the BMS worksite or physical presence in the field is a necessary job function of this role, which the Company deems critical to collaboration, innovation, productivity, employee well-being and engagement, and it enhances the Company culture.
BMS is dedicated to ensuring that people with disabilities can excel through a transparent recruitment process, reasonable workplace accommodations/adjustments and ongoing support in their roles. Applicants can request a reasonable workplace accommodation/adjustment prior to accepting a job offer. If you require reasonable accommodations/adjustments in completing this application, or in any part of the recruitment process, direct your inquiries to adastaffingsupport@bms.com. Visit careers.bms.com/eeo-accessibility to access our complete Equal Employment Opportunity statement.
BMS cares about your well-being and the well-being of our staff, customers, patients, and communities. As a result, the Company strongly recommends that all employees be fully vaccinated for Covid-19 and keep up to date with Covid-19 boosters.
BMS will consider for employment qualified applicants with arrest and conviction records, pursuant to applicable laws in your area.
Any data processed in connection with role applications will be treated in accordance with applicable data privacy policies and regulations.
If you come across a role that intrigues you but doesn’t perfectly line up with your resume, we encourage you to apply anyway. You could be one step away from work that will transform your life and career.
Uniquely Interesting Work, Life-changing Careers
With a single vision as inspiring as “Transforming patients’ lives through science™ ”, every BMS employee plays an integral role in work that goes far beyond ordinary. Each of us is empowered to apply our individual talents and unique perspectives in an inclusive culture, promoting diversity in clinical trials, while our shared values of passion, innovation, urgency, accountability, inclusion and integrity bring out the highest potential of each of our colleagues.
On-site Protocol
Physical presence at the BMS worksite or physical presence in the field is a necessary job function of this role, which the Company deems critical to collaboration, innovation, productivity, employee well-being and engagement, and it enhances the Company culture.
BMS is dedicated to ensuring that people with disabilities can excel through a transparent recruitment process, reasonable workplace accommodations/adjustments and ongoing support in their roles. Applicants can request a reasonable workplace accommodation/adjustment prior to accepting a job offer. If you require reasonable accommodations/adjustments in completing this application, or in any part of the recruitment process, direct your inquiries to adastaffingsupport@bms.com. Visit careers.bms.com/eeo-accessibility to access our complete Equal Employment Opportunity statement.
BMS cares about your well-being and the well-being of our staff, customers, patients, and communities. As a result, the Company strongly recommends that all employees be fully vaccinated for Covid-19 and keep up to date with Covid-19 boosters.
BMS will consider for employment qualified applicants with arrest and conviction records, pursuant to applicable laws in your area.
Any data processed in connection with role applications will be treated in accordance with applicable data privacy policies and regulations.
Show more
Show less","Data Engineering, DevOps, Python, AWS, Cloudera Data Platform (CDP), Apache Spark, Apache Airflow, dbt, Docker, Kubernetes, greatexpectations, pandera, Pydantic, Pandas Profiler, SQL, Jira, Confluence, Spotfire, Tableau","data engineering, devops, python, aws, cloudera data platform cdp, apache spark, apache airflow, dbt, docker, kubernetes, greatexpectations, pandera, pydantic, pandas profiler, sql, jira, confluence, spotfire, tableau","apache airflow, apache spark, aws, cloudera data platform cdp, confluence, data engineering, dbt, devops, docker, greatexpectations, jira, kubernetes, pandas profiler, pandera, pydantic, python, spotfire, sql, tableau"
Senior Data Analyst,EG America,"Westborough, MA",https://www.linkedin.com/jobs/view/senior-data-analyst-at-eg-america-3759722389,2023-12-17,Fitchburg,United States,Mid senior,Onsite,"Description
Position Summary:
The Sr. Data Analyst at EG America will play a crucial role in transforming data into actionable insights that drive informed decision-making and enhance our business operations. As we look to expand EG's Data team, we are seeking a talented and detail-oriented individual with a strong analytical mindset to join our dynamic team. You should be curious about business requirements and coming up with inventive BI solutions to solve for business challenges.
You will be for leading our Data CoE (Center of Excellence). You will be providing guidance, support, and inspiration to Junior analysts by being an enthusiastic mentor. You will be responsible for collecting, analyzing, and transforming data to provide valuable insights and recommendations to various stakeholders.
EG America is a convenience store operator with nearly 1,700 company owned and operated retail and restaurant locations across the United States. EG America’s corporate support center is located in Westborough, MA.
Responsibilities
Data Collection and Cleansing:
Gather and validate data from multiple sources, ensuring data accuracy and completeness.
Clean, preprocess, and transform raw data into usable formats for analysis in collaboration with the Data Engineering team.
Data Analysis
Perform in-depth data analysis to identify trends, patterns, and anomalies using R, M, DAX & Python.
Develop and apply statistical models and algorithms to extract meaningful insights.
Create and maintain data dashboards and reports for monitoring and reporting on key performance indicators.
Apply Generative AI (Artificial intelligence) techniques to analyze and extract valuable insights from complex datasets.
Data Visualization
Visualize data using tools such as Power BI, Tableau or similar, to communicate findings effectively.
Collaborate with the BI team to refine standards for actionable dashboard reporting and standardizing design of reports.
Design and deliver clear, informative visualizations that support decision-making processes.
Reporting And Communication
Prepare and present findings to stakeholders, translating complex data into understandable insights.
Collaborate with cross-functional teams to provide data-driven recommendations.
Continuous Improvement
Stay updated on industry best practices and emerging data analysis techniques.
Suggest process improvements and implement efficiency-enhancing measures.
Working Relationships:
Sr. Data Analyst will be directly reporting to Group VP of Application Development.
Minimum Education: Computer Science or related Bachelor's Degree
Preferred Education: Masters in Computer Science;
Minimum Experience
7 years of hands-on Data Analyst experience.
Proficiency in data analysis tools and programming languages (e.g., Python, R, SQL).
Strong knowledge of data visualization tools.
Excellent problem-solving skills and attention to detail.
Ability to work independently and collaboratively in a team.
Knowledge of statistical analysis and machine learning is a plus.
Preferred Experience
10 years of experience as a Data Analyst or in a related role in the Retail industry.
Licenses/Certifications
Soft Skills: Strong communication skills, Desire to continue to learn and stay abreast of new technologies
Hours & Conditions: 40+ hrs/week on-site at our office in Westborough, MA
Driving Requirement: None
Travel: Minimal
Physical: None
Other: Other / Miscellaneous
Show more
Show less","Data Analysis, Data Visualization, Data Preparation, Business Intelligence, Data Warehousing, Data Mining, Data Modeling, Machine Learning, Statistical Analysis, Artificial Intelligence, Generative AI, R, Python, SQL, Power BI, Tableau, DAX","data analysis, data visualization, data preparation, business intelligence, data warehousing, data mining, data modeling, machine learning, statistical analysis, artificial intelligence, generative ai, r, python, sql, power bi, tableau, dax","artificial intelligence, business intelligence, data mining, data preparation, dataanalytics, datamodeling, datawarehouse, dax, generative ai, machine learning, powerbi, python, r, sql, statistical analysis, tableau, visualization"
Python Data Engineer with Angular,Compunnel Inc.,"Merrimack, NH",https://www.linkedin.com/jobs/view/python-data-engineer-with-angular-at-compunnel-inc-3783653760,2023-12-17,Fitchburg,United States,Mid senior,Remote,"Position: Data Engineer - W2 only
Location: Merrimack, NH/Boston, MA/Durham, NC/Westlake, TX
Duration: Long Term
MUST HAVE SKILLS:
- Python
- SQL
- Angular (order is important)
NICE TO HAVE SKILLS:
- Container experiences
- knowledge of Java
- Reporting Tools experience
The Expertise and Skills You Bring
Experience handling various forms of unstructured and semi structured data and develop solutions to process these complex data.
Strength in building data pipelines, API’s, back-end services using Python.
Expertise in building containerized applications using docker, Kubernetes.
Experience in processing and exposing data using AWS technologies like ec2, s3, Lambda, API Gateway, Load Balancers, Auto scaling etc.
Expertise in building data ingestion tools using technologies like Python to extract data from Relational Databases/Web Scraping/External API’s
Experience in Snowflake or any MPP and columnar database on the Cloud.
Experience in CI/CD release automation and deployment (Jenkins, Concourse, CloudFormation etc.)
Good understanding of overall AWS security services like KMS, IAM, Security groups etc.
Knowledge of Angular, Java and reporting tools like Power BI will be an added advantage.
5 years of software development experience with at least 3 years working on Cloud/Big Data technologies.
BS in Computer Science or related degree, or equivalent experience
Show more
Show less","Python, SQL, Angular, Container experience, Java, Reporting Tools, Data pipelines, APIs, Backend services, Docker, Kubernetes, AWS, EC2, S3, Lambda, API Gateway, Load Balancers, Auto Scaling, Data ingestion tools, Snowflake, MPP, CI/CD release automation, Deployment, Jenkins, Concourse, CloudFormation, AWS security services, KMS, IAM, Security groups, Big Data technologies","python, sql, angular, container experience, java, reporting tools, data pipelines, apis, backend services, docker, kubernetes, aws, ec2, s3, lambda, api gateway, load balancers, auto scaling, data ingestion tools, snowflake, mpp, cicd release automation, deployment, jenkins, concourse, cloudformation, aws security services, kms, iam, security groups, big data technologies","angular, api gateway, apis, auto scaling, aws, aws security services, backend services, big data technologies, cicd release automation, cloudformation, concourse, container experience, data ingestion tools, datapipeline, deployment, docker, ec2, iam, java, jenkins, kms, kubernetes, lambda, load balancers, mpp, python, reporting tools, s3, security groups, snowflake, sql"
Senior Cloud Data Engineer,BDO USA,"Gardner, MA",https://www.linkedin.com/jobs/view/senior-cloud-data-engineer-at-bdo-usa-3765472148,2023-12-17,Fitchburg,United States,Mid senior,Hybrid,"Job Description
Job Summary:
This position will work with cutting edge technology, deliver high quality solutions across various industries, and oversee team(s) on engagements that range in size and scope. This position will receive continuous career development opportunities, given the size and potential of client engagements. This role will perform hands-on delivery of data analytics projects, contributing to the development and unit testing of solutions.
Job Duties
Designs and implements best in class data ingestion strategies, data warehouse and data mart structures, semantic layers and models, visualizations, streaming processes, API integrations, and automation (RPA) solutions for end-to-end data analytics solutions on primarily, but not limited to, cloud analytics platforms such as Azure and AWS
Listens to client needs to align solution with business requirements and delivery schedule
Creates written functional and technical designs
Participates in project status and stand meetings, and assists with providing aggregated project status for project and program managers
Assists with SLA compliance of solutions, and performs performance tuning and optimization efforts of end-to-end solutions
Writes code using multiple languages and correctly applies frameworks, architectural patterns, and software development principles
Delivers high-performance, scalable, repeatable, and secure deliverables with broad impact (high throughput and low latency)
Assists with implementation of data governance programs and best practices
Performs the cleaning and transforming of data from source systems into analytics models
Implements models to support data visualizations and integrations
Assists with implementing DevOps, DataOps and MLOps methodologies on projects
Writes custom integration logic in applicable programming languages
Assists project managers with work breakdown structure creation, project estimation, resource staffing, workload planning and adjustments throughout the project lifecycle
Assists clients with licensing, security, and cost estimation of solutions
Performs code reviews to ensure adherence to standards
Works directly with clients and team members to establish secure data analytics platforms and infrastructure
Contributes to successful deployments of developed solutions and integration of DevOps tools
Maintains a broad and current understanding of data analytics and business intelligence strategies, cloud platforms, methodologies, and tools
Builds client relationships during project execution, effectively becoming a trusted advisor of the client
Participates in support activities for existing software solutions
Other duties as assigned
Supervisory Responsibilities
Supervises the day-to-day workload of Associates on assigned engagements to ensure that timelines and deliverables are met, and reviews work product
Education
Qualifications, Knowledge, Skills and Abilities:
High School Diploma or GED equivalent, required
Bachelor’s degree, preferred; focus in Information Systems, Data Science or Computer Science, preferred
Experience
Five (5) or more years of experience within Data Analytics, Business Intelligence, Artificial Intelligence, or Application Development, required
One (1) or more years of experience technically leading development projects, preferred
One (1) or more years of consulting experience or implementation of cloud-based data analytics solutions, preferred
Software
Strong SQL skills including Data Definition Language (DDL), Data Manipulation Language (DML), views, functions, stored procedures, or performance tuning, required
Experience with Data Warehousing, Data Modeling, Semantic Model Definition or Star Schema Construction, required
Hands on delivery experience of end-to-end cloud data analytics solutions within Azure or AWS, preferred
Experience with one (1) or more of the following computer languages, preferred:
C#
Python
Java
Scala
Experience with tabular modeling within Microsoft Fabric, Power BI, or Azure Analysis Services, preferred
Experience with Git and DevOps deployment technologies, preferred
Experience with Linux, preferred
Experience with one (1) or more of the following, preferred:
Data Lake Medallion Architecture
Batch and/or streaming data ingestion into a data lake
AI Algorithms/Machine Learning
Automation tools such as UiPath, Alteryx, etc.
Computer Vision based AI technologies
Other Knowledge, Skills & Abilities
Ability to work with a high degree of professionalism and autonomy
Excellent verbal and written communication skills
Solid organizational skills, especially the ability to meet project deadlines with a focus on details
Ability to successfully multi-task while working independently or within a group environment
Ability to work in a deadline-driven environment, and handle multiple projects simultaneously
Ability to interact effectively with people at all organizational levels of the Firm
Ability to effectively interact with a team of professionals and delegating work assignments, as needed
Ability to build and maintain strong relationships with internal and client personnel
Ability to encourage a team environment on engagements, and contribute to the professional development of assigned personnel
Keywords:
Data Analytics, Business Intelligence, BI, Synapse, IoT, Machine Learning, Data Lake, Stream, Cube, Microsoft, SQL Server, Tableau, .Net, C#, Qlik, Power BI, Machine Learning, Azure Data Factory, RedShift, UiPath, Cloud, RPA, AWS, Redshift, Kinesis, QuickSight, SageMaker, S3, Databricks, AWS Lake Formation, Snowflake, Python, Qlik, Athena, Data Pipeline, Glue, Star Schema, Data Modeling, SQL, SSIS, SSAS, SSRS, PySpark, Microsoft Fabric, dbt, Linux, Terraform, Bicep, Data Ops, Purview, Git, Delta, Pandas, Spark SQL
Individual salaries that are offered to a candidate are determined after consideration of numerous factors including but not limited to the candidate’s qualifications, experience, skills, and geography.
California Range: $111,000 - $152,000
Colorado Range: $111,000 - $152,000
New York City/ Valhalla Range: $111,000 - $152,000
Washington Range: $111,000 - $152,000
About Us
BDO delivers assurance, tax, digital technology solutions and financial advisory services to clients throughout the country and around the globe. We offer numerous industry-specific practices, world-class resources, and an unparalleled commitment to meeting our clients’ needs. We currently serve more than 400 publicly traded domestic and international clients.
Unparalleled partner-involvement
Deep industry knowledge and participation
Geographic coverage across the U.S.
Cohesive global network
Focused capabilities across disciplines
BDO brings world-class resources and exceptional service to each and every one of our clients. BDO USA is a member of BDO International, the world’s fifth largest accounting network.
BDO offers a competitive Total Rewards package that encompass so much more than – “traditional benefits”. Our wide range of rewards and our employees’ ability to customize rewards to their individual needs are two of the reasons why BDO has been honored with so many workplace awards, including 100 Best Companies for Working Parents, Working Mother 100 Best Companies, Top Entry Level Employer, 2022 National Best & Brightest Companies to Work For and more.
Some Examples Of Our Total Rewards Offerings Include
Competitive pay and eligibility for an annual performance bonus.
A 401k plan plus an employer match
Comprehensive, medical, dental, vision, FSA, and prescription insurance from day one
Competitive Paid Time Off with daily accrual from day one of employment, plus paid holidays
Paid Parental Leave
Adoption Assistance
Firm paid life insurance
Wellness programs
Additional offerings include BDO Flex, Group Legal insurance, Pet insurance and Long-Term Care Insurance
Above offerings may be subject to eligibility requirements.
Click here to find out more!
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability or protected veteran status.
""BDO USA, P.A. is an EO employer M/F/Veteran/Disability""
Show more
Show less","Data Analytics, Business Intelligence, Artificial Intelligence, Cloud Analytics, Machine Learning, SQL, Data Warehousing, Data Modeling, Star Schema Construction, Azure, AWS, C#, Python, Java, Scala, Power BI, Git, Linux, Data Lake Medallion Architecture, Batch Data Ingestion, Streaming Data Ingestion, AI Algorithms, Automation Tools, Computer Vision, Tableau, .Net, Qlik, Azure Data Factory, RedShift, UiPath, RPA, SSIS, SSAS, SSRS, PySpark, Microsoft Fabric, dbt, Terraform, Bicep, Data Ops, Purview, Delta, Pandas, Spark SQL, Athena, Glue, Snowflake","data analytics, business intelligence, artificial intelligence, cloud analytics, machine learning, sql, data warehousing, data modeling, star schema construction, azure, aws, c, python, java, scala, power bi, git, linux, data lake medallion architecture, batch data ingestion, streaming data ingestion, ai algorithms, automation tools, computer vision, tableau, net, qlik, azure data factory, redshift, uipath, rpa, ssis, ssas, ssrs, pyspark, microsoft fabric, dbt, terraform, bicep, data ops, purview, delta, pandas, spark sql, athena, glue, snowflake","ai algorithms, artificial intelligence, athena, automation tools, aws, azure, azure data factory, batch data ingestion, bicep, business intelligence, c, cloud analytics, computer vision, data lake medallion architecture, data ops, dataanalytics, datamodeling, datawarehouse, dbt, delta, git, glue, java, linux, machine learning, microsoft fabric, net, pandas, powerbi, purview, python, qlik, redshift, rpa, scala, snowflake, spark, spark sql, sql, ssas, ssis, ssrs, star schema construction, streaming data ingestion, tableau, terraform, uipath"
SAP Master Data Analyst,"TSR Consulting Services, Inc.","Devens, MA",https://www.linkedin.com/jobs/view/sap-master-data-analyst-at-tsr-consulting-services-inc-3742718296,2023-12-17,Fitchburg,United States,Mid senior,Hybrid,"Our client, a leading pharmaceutical company, is hiring a SAP Master Data Analyst on a contract basis.
Work Location:
Devens, MA– 100% on-site for the first 4-6 weeks for training, then hybrid.
Summary
This position will be responsible for establishing and following policies and procedures that ensure accurate and timely administration of Master Data in SAP. They will support the enterprise data operation business team and serve as a key data custodian for GMS Operations. They will support data entry and audit accuracy requirements of Supply Chain, Procurement, Operations, Logistics, Finance, Sales/Operations and Quality Assurance groups.
This position enters and maintains key data according to policy, procedures and templates defined for SAP master data maintenance. This position works cross functionally with members of several different departments in multiple locations including Finance, Quality, Operations, Planning, Distribution, Customer Service and Procurement.
Responsibilities
Ensure Master Data is entered and maintained with accuracy and integrity.
Define/Maintain SAP master data templates.
Perform routine data cleansing operations.
Monthly/Quarterly review of master data with functional groups.
Coordinates Master Data activities with functional groups.
Entry and maintenance activities for the following Master Data elements (including but not limited to):
Material Master
Bills of Materials
Routings
Recipes
Customer Master
Customer Contracts
Vendor Master
General Ledger
Cost Center
The post SAP Master Data Analyst appeared first on TSR Consulting Services.
Show more
Show less","SAP, Master Data, Data Entry, Data Auditing, Data Cleansing, Data Analysis, Data Management, Data Governance, Data Warehousing, Data Integration, Data Mining, Data Visualization, Data Security, Data Quality, Data Architecture, ERP, CRM, SCM, PLM, MES, BI, DW, AI, ML, DL, RPA, IoT, Blockchain, Cloud Computing, Big Data, Analytics, Visualization, Modeling, Simulation, Optimization, Machine Learning, Deep Learning, Natural Language Processing, Computer Vision, Robotics, Automation, Software Development, Programming Languages, Databases, Operating Systems, Networking, Security, DevOps, Agile, Scrum, Kanban","sap, master data, data entry, data auditing, data cleansing, data analysis, data management, data governance, data warehousing, data integration, data mining, data visualization, data security, data quality, data architecture, erp, crm, scm, plm, mes, bi, dw, ai, ml, dl, rpa, iot, blockchain, cloud computing, big data, analytics, visualization, modeling, simulation, optimization, machine learning, deep learning, natural language processing, computer vision, robotics, automation, software development, programming languages, databases, operating systems, networking, security, devops, agile, scrum, kanban","agile, ai, analytics, automation, bi, big data, blockchain, cloud computing, computer vision, crm, data architecture, data auditing, data entry, data governance, data integration, data management, data mining, data quality, data security, dataanalytics, databases, datacleaning, datawarehouse, deep learning, devops, dl, dw, erp, iot, kanban, machine learning, master data, mes, ml, modeling, natural language processing, networking, operating systems, optimization, plm, programming languages, robotics, rpa, sap, scm, scrum, security, simulation, software development, visualization"
Engineering Data Analyst,Lycoming Engines,"Williamsport, PA",https://www.linkedin.com/jobs/view/engineering-data-analyst-at-lycoming-engines-3769525934,2023-12-17,Lock Haven,United States,Mid senior,Onsite,"About this Role
We believe people should love what they do – in this role, you will be responsible for:
• Designing and delivering new analytic solutions (model, algorithms, tools) leveraging algorithms,
• Software application capabilities and a strong engineering & analytics backgrounds
• Exposure to working with large-scale data that includes analysis of data to a conclusive level.
• Provide recommendations and lead solutions to mitigate risks and abate pressures.
• Engage key technical & subject matter experts (SMEs) cross-functionally for knowledge capture transformed into the solutions and deploy those solutions to the field for usage.
• Communicate complex data insights to non-technical stakeholders in a clear and concise manner, and work to build a data-driven culture within the organization.
• Participate as a presenter in both technical and program reviews.
Qualifications
BSc/MSc in Engineering, Physics or Mathematics
Technical knowledge of piston engine architecture
Advanced statistical and analytical knowledge
Experience with scripting and data analysis packages like Python, R is a plus.
Knowledge of design for six σ
Experience designing analytics utilizing Machine Learning algorithms or heuristics.
You will not now, or in the future require sponsorship (i.e. H-1B visa, etc.) to legally work in the U.S
Desired Characteristics
Strong oral and written communication skills
Strong interpersonal and project management skills
Reporting on a timely manner
Growth oriented and open to learn mindset
Show more
Show less","Python, R, Statistics, Analytics, Machine Learning algorithms, Heuristics, Design for six σ","python, r, statistics, analytics, machine learning algorithms, heuristics, design for six","analytics, design for six, heuristics, machine learning algorithms, python, r, statistics"
Engineering Data Analyst,Textron,"Williamsport, PA",https://www.linkedin.com/jobs/view/engineering-data-analyst-at-textron-3709805441,2023-12-17,Lock Haven,United States,Mid senior,Onsite,"About This Role
We believe people should love what they do – in this role, you will be responsible for:
Designing and delivering new analytic solutions (model, algorithms, tools) leveraging algorithms,
Software application capabilities and a strong engineering & analytics backgrounds
Exposure to working with large-scale data that includes analysis of data to a conclusive level.
Provide recommendations and lead solutions to mitigate risks and abate pressures.
Engage key technical & subject matter experts (SMEs) cross-functionally for knowledge capture transformed into the solutions and deploy those solutions to the field for usage.
Communicate complex data insights to non-technical stakeholders in a clear and concise manner, and work to build a data-driven culture within the organization.
Participate as a presenter in both technical and program reviews.
Qualifications
BSc/MSc in Engineering, Physics or Mathematics
Technical knowledge of piston engine architecture
Advanced statistical and analytical knowledge
Experience with scripting and data analysis packages like Python, R is a plus.
Knowledge of design for six σ
Experience designing analytics utilizing Machine Learning algorithms or heuristics.
You will not now, or in the future require sponsorship (i.e. H-1B visa, etc.) to legally work in the U.S
Desired Characteristics
Strong oral and written communication skills
Strong interpersonal and project management skills
Reporting on a timely manner
Growth oriented and open to learn mindset
Textron is committed to providing Equal Opportunity in Employment, to all applicants and employees regardless of, race, color, religion, gender, age, national origin, military status, veteran status, handicap, physical or mental disability, sexual orientation, gender identity, genetic information or any other characteristic protected by law.
Recruiting Company
Textron Systems: Lycoming Engines
Primary Location
US-Pennsylvania-Williamsport
Job Field
Engineering
Schedule
Full-time
Job Level
Individual Contributor
Job Type
Experienced
Shift
First Shift
Relocation
Available
Worksite
Onsite
Job Posting
08/31/2023, 3:09:02 PM
Show more
Show less","Data Analytics, Machine Learning, Algorithms, Python, R, Statistics, Data Analysis, Design for Six Sigma, Software Application Capabilities, Engineering, Analytics, Communication, Project Management","data analytics, machine learning, algorithms, python, r, statistics, data analysis, design for six sigma, software application capabilities, engineering, analytics, communication, project management","algorithms, analytics, communication, dataanalytics, design for six sigma, engineering, machine learning, project management, python, r, software application capabilities, statistics"
Business Data Analyst,Voyage Advisory,"Chicago, IL",https://www.linkedin.com/jobs/view/business-data-analyst-at-voyage-advisory-3787796169,2023-12-17,Niles,United States,Mid senior,Remote,"Our consultancy is looking for a highly motivated and talented Business Data Analyst. We are looking for peak performers who have a passion for business, a joy in solving problems and a work ethic that doesn't rest until the job is done.
The Business Data Analyst will be instrumental in analyzing and interpreting customer engagement and interaction data across various channels such as chat, voice, CRM, surveys, social media, and email. The role requires a blend of analytical skills and business acumen to develop insightful dashboards and reports that drive decision-making and improve customer experience.
The role will be working on a dedicated data analytics project, harnessing the strategic use of enterprise data, by leveraging cross-platform data mining, identification, preparation and integration of complex data sources, and provide multifaceted analysis to assist the technology & data modernization efforts of the department.
The Business Data Analyst will be responsible for developing data analytic models in coordination with the government lead, identifying relevant data sources for highlighted strategic efforts, providing strong verbal and written feedback to government and program leadership on analytic capabilities and emerging opportunities based on new data integrations and business requirements.
The ideal candidate has experience with iterative analytic methodologies working with stakeholders from multiple different backgrounds incl. strategic planning, operations, communications, and overall technology management. Furthermore, the candidate will have experience with customer service operations aimed at identifying issues in a Customer Experience (CX) delivery & reporting organization, preferably in healthcare, finance or insurance related settings.
Requireed Skills
Bachelor’s degree in Computer Science, Statistics or related quantitative field.
5+ years experience mining & analyzing data as a business data analyst.
Proven analytic skills, including mining, evaluation, analysis, and visualization preferably in a customer engagement context.
Proficiency in data analysis tools (e.g., SQL, Python, R) and business intelligence platforms (e.g., Tableau, Power BI).
Strong understanding of data warehousing and database management.
Experience working with data platforms such as Databricks, AWS and MS Azure Data Lakes.
Strong understanding of data models, database design development and data mining.
Strong analytical skills with the ability to collect, organize, analyze and disseminate significant amounts of information with attention to detail and accuracy.
Strong ability to manage several complex tasks in a timely, effective manner.
Demonstrated ability to clearly communicate complex data insights to the business community.
Demonstrated ability to be proactive, think independently, and work collaboratively with stakeholders of various backgrounds.
Additiional Qualifications
Prior experience with database and model design and segmentation techniques.
Prior experience with Power BI for reporting to both technical and leadership level recipients.
Strong programming experience with ETL tools/approaches, incl. Informatica, Java, Python, Ruby etc.
Practical experience in statistical analysis through the use of statistical packages including Excel, SPSS, and SAS.
Experience with Machine Learning (ML) and Natural Language Processing/Understanding (NLP/U) methodologies and approaches.
Demonstrated experience with guiding the development of analytical approaches and overall data modelling methodologies with stakeholders from varying backgrounds.
Proven success in a collaborative, team-oriented environment with clear, direct and courteous communication.
LOCATION
This position is 100% remote.
Compensation
The position is full time and will include a base salary of $80K + bonus, and full benefits, including health, dental, vision, life, disability, vacation, and 401(k) with company match.
Powered by JazzHR
ylfgK1Hz2v
Show more
Show less","SQL, Python, R, Tableau, Power BI, Databricks, AWS, MS Azure Data Lakes, Data warehousing, Database management, Data mining, Data models, Database design, Data segmentation, Informatica, Java, Ruby, Excel, SPSS, SAS, Machine Learning, Natural Language Processing, Natural Language Understanding, ETL","sql, python, r, tableau, power bi, databricks, aws, ms azure data lakes, data warehousing, database management, data mining, data models, database design, data segmentation, informatica, java, ruby, excel, spss, sas, machine learning, natural language processing, natural language understanding, etl","aws, data mining, data models, data segmentation, database design, database management, databricks, datawarehouse, etl, excel, informatica, java, machine learning, ms azure data lakes, natural language processing, natural language understanding, powerbi, python, r, ruby, sas, spss, sql, tableau"
Senior Principal Consultant – Data and Analytics,Genesys,"Delaware, United States",https://www.linkedin.com/jobs/view/senior-principal-consultant-%E2%80%93-data-and-analytics-at-genesys-3781016465,2023-12-17,Delaware,United States,Mid senior,Remote,"Build something new with a world-class team.
At Genesys, we allow our employees to make their mark by entrusting them to make decisions and do what they’ve been hired to do: their very best. Your potential is waiting; why are you?
About Genesys:
GENESYS® powers more than 25 billion of the world’s best customer experiences each year.
In GENESYS, Innovations team builds solutions that enable our customers to get more value from their Genesys platforms. The Innovations team’s primary job is to build new, innovative applications using AI and analytics, with the goal of helping our customers effectively and efficiently deliver experience as a service (XaaS).
Summary: We are seeking a highly skilled and experienced Contact Center Data and Analytics Consultant to join our team. The ideal candidate will have a strong background in data analysis, data visualization, data engineering, and business intelligence. This is a customer facing role and requires the ability to present to both technical and business executive audiences.
Responsibilities:
Present Genesys data and analytics technology and vision to executives and technical contributors at prospects and customers
Work hands-on with prospects and customers to demonstrate and communicate the value of Genesys data and analytics apps throughout the sales cycle, from demo to proof of concept to design and implementation
Immerse yourself in the ever-evolving industry of contact center and experience management SaaS, maintaining a deep understanding of competitive and complementary technologies and vendors and how to position our analytics apps in relation to them
Collaborate with Product Management, Engineering, and Saloes to continuously improve Genesys data and analytics apps and marketing
Requirements:
8+ years direct experience with Genesys Engage Infomart or GCXI or NICE CX Analytics; strong understanding of Contact Center and Customer Experience data and metrics
Strong proficiency in SQL, data modeling, and data engineering
Familiarity with scripting languages for data engineering
Outstanding presenting skills to both technical and executive audiences, whether impromptu on a whiteboard or using presentations and demos.
Hands-on expertise with SQL and SQL analytics
Ability to connect a customer’s specific business problems to Genesys analytics solutions
Experience with data visualization tools such as Tableau or Power BI
Preferred Qualifications:
Experience with Snowflake
Experience with Elastic (ELK stack)
Experience with data governance and data management best practices.
Experience working with remote engineering teams
Familiar with software development and project management frameworks
Experience in consulting or developing products and solutions with contact center technologies like Genesys Engage, Genesys Cloud, NICE, Cisco, Avaya, etc.
Compensation:
This role has a market-competitive salary with an anticipated base compensation range listed below. Actual salaries will vary depending on a candidate’s experience, qualifications, skills, and location. This role might also be eligible for a commission or performance-based bonus opportunities.
$121,500.00 - $238,400.00
Benefits:
Medical, Dental, and Vision Insurance.
Telehealth coverage
Flexible work schedules and work from home opportunities
Development and career growth opportunities
Open Time Off in addition to 10 paid holidays
401(k) matching program
Adoption Assistance
Fertility treatments
More details about our company benefits can be found at the following link: https://mygenesysbenefits.com
If a Genesys employee referred you, please use the link they sent you to apply.
About Genesys:
Every year, Genesys orchestrates billions of remarkable customer experiences for organizations in more than 100 countries. Through the power of our cloud, digital and AI technologies, organizations can realize Experience as a Service™ our vision for empathetic customer experiences at scale. With Genesys, organizations have the power to deliver proactive, predictive, and hyper personalized experiences to deepen their customer connection across every marketing, sales, and service moment on any channel, while also improving employee productivity and engagement. By transforming back-office technology to a modern revenue velocity engine Genesys enables true intimacy at scale to foster customer trust and loyalty. Visit www.genesys.com.
Reasonable Accommodations:
If you require a reasonable accommodation to complete any part of the application process or are limited in the ability or unable to access or use this online application process and need an alternative method for applying, you or someone you know may complete the Reasonable Accommodations Form for assistance. Please use the Candidate field in the dropdown menu to ensure a timely response.
This form is designed to assist job seekers who seek reasonable accommodation for the application process. Submissions entered for non-accommodation-related issues, such as following up on an application or submitting a resume, may not receive a response.
Genesys is an equal opportunity employer committed to equity in the workplace. We evaluate qualified applicants without regard to race, color, age, religion, sex, sexual orientation, gender identity or expression, marital status, domestic partner status, national origin, genetics, disability, military and veteran status, and other protected characteristics.
Please note that recruiters will never ask for sensitive personal or financial information during the application phase.
Show more
Show less","Data visualization, Data analysis, Data engineering, Business intelligence, SQL, Data modeling, Scripting languages, Contact center, Customer experience, Software development, Project management, Genesys Engage, Genesys Cloud, Snowflake, Elastic (ELK stack), Data governance, Data management, Avaya, Cisco","data visualization, data analysis, data engineering, business intelligence, sql, data modeling, scripting languages, contact center, customer experience, software development, project management, genesys engage, genesys cloud, snowflake, elastic elk stack, data governance, data management, avaya, cisco","avaya, business intelligence, cisco, contact center, customer experience, data engineering, data governance, data management, dataanalytics, datamodeling, elastic elk stack, genesys cloud, genesys engage, project management, scripting languages, snowflake, software development, sql, visualization"
Enterprise Data Architect,ITS Logistics,"Reno, NV",https://www.linkedin.com/jobs/view/enterprise-data-architect-at-its-logistics-3780818289,2023-12-17,Sparks,United States,Mid senior,Onsite,"About ITS Logistics
Are you ready to unleash your potential and be a part of one of the fastest growing, exciting, logistics companies in the US? ITS Logistics is a premier Third-Party Logistics company that provides creative supply chain solutions with an asset-lite transportation division ranked #21 in North America , the #11 drayage and intermodal provider, a top-tier asset-based dedicated fleet , and innovative omnichannel distribution and fulfillment services. With the highest level of service, unmatched industry experience and work ethic, and a laser focus on innovation and technology–our purpose is to improve the quality of life by delivering excellence in everything we do.
At ITS, we invest in your personal and professional growth, providing the tools, resources, and support you need to unleash your full potential, collaborate with like-minded teammates, and seize limitless opportunities. By joining our all-star team, you will be part of an organization that values your unique skills, encourages your drive for excellence, and recognizes your unwavering commitment to achieving our shared goals.
We empower our team members to become champions in their respective fields by nurturing a culture of collaboration, competition, and unyielding resilience. We believe that together, we can conquer any challenge and achieve remarkable victories.
Want to learn more about ITS Logistics? Check out our website! www.its4logistics.com
About Our IT Team
At ITS we see Information technology as a core enabler in delivering complex third-party logistics services at scale. ITS continues to invest in transforming its enterprise data architecture to support business growth and set us apart in the marketplace. An increasing percent of ITS IT spending is now allocated to innovation and transformation initiatives with the goal to rapidly leverage the following significant technology shifts to maximize business gain:
Leverage public cloud to deliver an enterprise data platform for increased business agility, scalability, and resiliency.
Use of the latest data analytics platform for informed decision-making, driving business outcomes, and uncover new opportunities with data-driven insights.
AI and automation to improve efficiency and to speed up business processes and results.
Position Summary
We are seeking an experienced hands-on enterprise data architect with the capability to work in cross functional groups. You will work with business data owners and a team of senior development engineers to develop high quality data solutions using industry leading technologies with an Agile approach. This is a great opportunity to work for a dynamic, fast-paced growing company. This position reports to the Director of Enterprise Data.
Essential Job Functions
Essential activities include but are not limited to:
Strong knowledge of database management systems, data warehousing, data lake/lakehouse, data modeling, and business intelligence tools.
Experience with ETL/ELT processes, and data integration tools. Proficiency in programming languages such as SQL, or Snowflake.
Experience with structured and unstructured data in a cloud, on-premises, and hybrid environment
The ability to analyze complex data sets and identify patterns and trends to drive business decisions.
Effective communication and interpersonal skills are essential for collaborating with business stakeholders, project managers, and technical teams.
The ability to lead cross-functional teams, manage projects, and provide guidance on best practices in data management.
The ability to identify and solve complex data management problems and optimize data structures for maximum efficiency.
The ability to think creatively and outside the box to develop innovative solutions to complex data challenges.
Basic Qualifications
Bachelor’s or master’s degree in computer science, information technology, or relevant experience.
8 – 10 years of experience in data management, database design, and data architecture.
Other Qualifications
Familiarity with logistics industry, regulations, compliance requirements, and data privacy laws is nice to have.
Data governance experience is also nice to have.
The ability to continuously learn and stay up to date on modern technologies and data management trends.
Working knowledge of requirements gathering methodologies and processes.
Familiarity with working in a collaborative environment.
Good judgment skills (e.g., when to escalate technical problems and issues).
Strong attention to detail.
Demonstrated analytical and critical thinking skills.
Benefits
At ITS Logistics, We Value Our Employees And Offer a Supportive And Rewarding Work Environment. As a Mid-level Software Engineer In Logistics And Transportation, You Can Expect
Competitive compensation packages based on your experience and qualifications.
Comprehensive health benefits, including medical, dental, and vision coverage.
Flexible working hours to promote work-life balance.
Continuous learning opportunities through workshops, conferences, and certifications in logistics and transportation technology.
Access to the latest tools and technologies to enhance your skills and drive innovation.
Opportunities for career growth within the company as we expand our footprint in the logistics and transportation industry.
Show more
Show less","Enterprise data architecture, Agile approach, Data management systems, Data warehousing, Data lake, Data modeling, Business intelligence tools, ETL/ELT processes, Data integration tools, Programming languages (SQL Snowflake), Structured data, Unstructured data, Cloud computing, Onpremises, Hybrid environment, Data analysis, Datadriven decision making, Communication skills, Interpersonal skills, Crossfunctional teams, Project management, Best practices in data management, Data governance, Modern technologies, Data management trends, Requirements gathering methodologies, Collaborative environment, Analytical thinking skills, Critical thinking skills","enterprise data architecture, agile approach, data management systems, data warehousing, data lake, data modeling, business intelligence tools, etlelt processes, data integration tools, programming languages sql snowflake, structured data, unstructured data, cloud computing, onpremises, hybrid environment, data analysis, datadriven decision making, communication skills, interpersonal skills, crossfunctional teams, project management, best practices in data management, data governance, modern technologies, data management trends, requirements gathering methodologies, collaborative environment, analytical thinking skills, critical thinking skills","agile approach, analytical thinking skills, best practices in data management, business intelligence tools, cloud computing, collaborative environment, communication skills, critical thinking skills, crossfunctional teams, data governance, data integration tools, data lake, data management systems, data management trends, dataanalytics, datadriven decision making, datamodeling, datawarehouse, enterprise data architecture, etlelt processes, hybrid environment, interpersonal skills, modern technologies, onpremises, programming languages sql snowflake, project management, requirements gathering methodologies, structured data, unstructured data"
"Staff Software Engineer, Data Frameworks",Ridgeline,"Reno, NV",https://www.linkedin.com/jobs/view/staff-software-engineer-data-frameworks-at-ridgeline-3675316377,2023-12-17,Sparks,United States,Mid senior,Onsite,"As a Ridgeline Staff Software Engineer on Data Frameworks, you’ll have the unique opportunity to champion and build high-quality features leveraging our unified data model framework in a fast-moving, progressive work environment. You’ll think outside the box and add your own genius, passion, and interests to the software development lifecycle.
This role requires you to be work authorized in the United States without the need for employer sponsorship.
What will you do?
Contribute business insight, design skills, and technical expertise to a team where design, strategy, and engineering collaborate closely
Be involved in the entire software development process, from requirements and design reviews through the implementation of a new product
Impact an evolving tech stack based on modern front-end frameworks and cost-efficient utilization of AWS back-end services
Participate in the creation and construction of developer-based automation that leads to scalable, high-quality applications customers will depend on to run their businesses
Coach, mentor, and inspire teams of software engineers that are responsible for delivering high performing, secure enterprise applications
Think creatively, own problems, seek solutions, and communicate clearly along the way
Contribute to a collaborative environment deeply rooted in learning, teaching, and transparency
Demonstrate a strong dedication to code quality, automation and operational excellence: unit/integration/component tests and workflows
Desired Skills And Experiences
8-10+ years in a software engineering position or similar function, with a history of designing and developing new products and using new technologies
A degree in Computer Science, Information Science, or a related discipline
Mastery of data structures, algorithms, and architectural patterns
Experience working with relational databases and/or enterprise frameworks
Understanding of building back-end infrastructure using AWS offerings such as Aurora, Lambda, and S3
Expertise in one or more of the following languages: Python, Java
Willingness to learn about new technologies while simultaneously developing expertise in a business domain/problem space
Ability to focus on short-term deliverables while maintaining a big-picture perspective
An aptitude for problem solving
Ability to communicate effectively with colleagues at all levels and all audiences
Serious interest in having fun at work
Nice-to-Haves
Experience building responsive Web interfaces using frameworks such as React and Redux
Experience with agile development methodologies
Background in design, economics, or business
About Ridgeline
Ridgeline is the industry cloud platform for investment management. It was founded in 2017 by visionary entrepreneur Dave Duffield (co-founder of both PeopleSoft and Workday) to address the unique technology challenges of an industry in need of new thinking. We are building a modern platform in the public cloud, purpose-built for the investment management industry to empower business like never before.
Headquartered in Lake Tahoe with offices in Reno, Manhattan, and the Bay Area, Ridgeline is proud to have built a fast-growing, people-first company that has been recognized by Fast Company as a “Best Workplace for Innovators,” by LinkedIn as a “Top U.S. Startup,” and by The Software Report as a “Top 100 Software Company.”
Ridgeline is proud to be a community-minded, discrimination-free equal opportunity workplace.
Ridgeline processes the information you submit in connection with your application in accordance with the Ridgeline Applicant Privacy Statement. Please review the Ridgeline Applicant Privacy Statement in full to understand our privacy practices and contact us with any questions.
Compensation And Benefits
[For New York and California Based Only]
The typical starting salary range for new hires in this role is listed below. In select locations (including, the San Francisco Bay Area, CA, and the New York City Metro Area), an alternate range may apply as specified below.
The typical starting salary range for this role is: $165,000-$200,000.
The typical starting salary range for this role in the select locations listed above is: $175,000-$215,000.
Final compensation amounts are determined by multiple factors, including candidate experience and expertise, and may vary from the amount listed above.
As an employee at Ridgeline, you’ll have many opportunities for advancement in your career and can make a true impact on the product.
In addition to the base salary, 100% of Ridgeline employees can participate in our Company Stock Plan subject to the applicable Stock Option Agreement. We also offer rich benefits that reflect the kind of organization we want to be: one in which our employees feel valued and are inspired to bring their best selves to work. These include unlimited vacation, educational and wellness reimbursements, and $0 cost employee insurance plans. Please check out our Careers page for a more comprehensive overview of our perks and benefits.
Show more
Show less","Software Engineering, AWS, Aurora, Lambda, S3, Python, Java, React, Redux, Relational Databases, Enterprise Frameworks, Data Structures, Algorithms, Architectural Patterns","software engineering, aws, aurora, lambda, s3, python, java, react, redux, relational databases, enterprise frameworks, data structures, algorithms, architectural patterns","algorithms, architectural patterns, aurora, aws, data structures, enterprise frameworks, java, lambda, python, react, redux, relational databases, s3, software engineering"
Staff Data Engineer,Recruiting from Scratch,"Carmel Valley, CA",https://www.linkedin.com/jobs/view/staff-data-engineer-at-recruiting-from-scratch-3744396237,2023-12-17,Monterey,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our client is a dating app.
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, deliver analysis to further improve engagement in existing features, and empower our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Solve technical problems of the highest scope and complexity
Exert significant influence on the company’s analytical long-range goals and data architecture
Define and extend our internal standards for style, maintenance, and best practices for a high-scale data platform
Provide mentorship for all on your team to help them grow in their technical responsibilities
Propose ideas to improve the scale, performance, and capabilities of the Data Platform
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We'll Love About You
7+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field
7+ years experience using Python
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention
Location
: This role will be located in office twice a week minimum in Palo Alto, San Francisco or Chicago.
Salary Range: $180,000 - 250,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, SQL, TDD, Pair Programming, Continuous Integration, Automated Testing, Deployment, Kafka, Storm, SparkStreaming, Data Warehousing, ETL, Data Classification, Data Retention","python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, tdd, pair programming, continuous integration, automated testing, deployment, kafka, storm, sparkstreaming, data warehousing, etl, data classification, data retention","airflow, automated testing, continuous integration, data classification, data retention, datawarehouse, deployment, docker, etl, helm, kafka, kubernetes, pair programming, python, snowflake, spark, sparkstreaming, sql, storm, tdd"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Carmel Valley, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3748828485,2023-12-17,Monterey,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our next evolution is underway as a newly public company looking to expand and continue to build meaningful experiences for our users. From social issues to original content, we’re blazing innovative paths with impact for our community, all while leveraging the latest tech stacks and striving for engineering excellence. At the heart of our work in this new chapter is a shared set of core values: openness and exploration, a bias for action, and strong support of the LGBTQ community.
With a track record of strong financial performance and plans for continued headcount growth, we’re looking to build a team of talented, passionate, and open-minded people who believe in our mission, align with our values, and are excited to work at the intersection of innovative technology and social impact. Come be a part of this exciting journey with us.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location: Palo Alto, San Francisco or Chicago
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Business Intelligence, Data Science, Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, SQL, TDD, Pair Programming, Continuous Integration, Automated Testing, Kafka, Storm, SparkStreaming, Dimensional Data Modeling, Schema Design, ETL, Data Warehouses, Legal Compliance, Data Management Tools, Data Classification, Data Retention","data engineering, business intelligence, data science, python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, tdd, pair programming, continuous integration, automated testing, kafka, storm, sparkstreaming, dimensional data modeling, schema design, etl, data warehouses, legal compliance, data management tools, data classification, data retention","airflow, automated testing, business intelligence, continuous integration, data classification, data engineering, data management tools, data retention, data science, data warehouses, dimensional data modeling, docker, etl, helm, kafka, kubernetes, legal compliance, pair programming, python, schema design, snowflake, spark, sparkstreaming, sql, storm, tdd"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Carmel Valley, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3744392699,2023-12-17,Monterey,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
They are a profitable dating application.
Location: This is a hybrid role based in their Chicago office and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location
: This role is twice a week in a hybrid role minimum.
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Python, SQL, Snowflake, Kafka, Spark, Docker, Kubernetes, Airflow, Helm, TDD, Pair Programming, Continuous Integration, Automated Testing, ETL, Dimensional Data Modeling, Schema Design, Data Warehouses, StreamProcessing Systems, Data Governance, Data Classification, Data Retention, Legal Compliance","data engineering, python, sql, snowflake, kafka, spark, docker, kubernetes, airflow, helm, tdd, pair programming, continuous integration, automated testing, etl, dimensional data modeling, schema design, data warehouses, streamprocessing systems, data governance, data classification, data retention, legal compliance","airflow, automated testing, continuous integration, data classification, data engineering, data governance, data retention, data warehouses, dimensional data modeling, docker, etl, helm, kafka, kubernetes, legal compliance, pair programming, python, schema design, snowflake, spark, sql, streamprocessing systems, tdd"
Lead Master Data Analyst,Driscoll's,"Watsonville, CA",https://www.linkedin.com/jobs/view/lead-master-data-analyst-at-driscoll-s-3666667401,2023-12-17,Monterey,United States,Mid senior,Onsite,"Location: US - Watsonville, California
Job Family: IT
Time Type: Full time
Job Requisition ID: R0004918
Experience Years: 5-10 Years of Experience
Employee Type: Regular
About the Opportunity
Lead Master Data Management Analyst will be responsible for operations and analysis of multi-domain master data across different systems. The responsibilities involve master data operations including timely setup, updates, deactivation of master data, analyzing and documenting master data processes and business requirements, collaborating with different stakeholders across departments and geographies to consolidate and curate master data. The role will be involved in executing MDM Operations and projects across systems, partner across multiple functions to analyze and document MDM standards and governance processes. This role is will also be involved in the process and governance for the global Master Data Management process and working with business units and global support functions to execute and govern Master Data across systems. This role will help analyze and document standards and processes for Master Data as well as ensure compliance to maintain data integrity.
Responsibilities
Designs and develops a framework for global Master Data standardization and governance, including owning process flows, data maps, and standard definitions for governance across a variety of cross-functional systems and processes.
Create, maintain, and deprecate item, customer, vendor, grower, ranch, facility, pricing, Bill of Material data in ERP Systems to facilitate smooth and accurate transaction flow through the ERP.
Define and publish data definitions for the organization as data elements are added or modified
Serve as master data SME on all new system integrations or enhancements requiring master data to ensure proper usage and assess process impact
Analyze and document process and data points as necessary to support seamless integration as business models change or exceptions arise
Ensure data quality and MDM adherence through systems & processes to ensure data integrity.
Research and document Master Data processes and governance best practices.
Facilitate change management initiatives with stakeholders
Review Master Data requests that affect financial reports or mapping and ensure they follow the established definitions and guidelines
Ensure policies, procedures, and SLAs relating to corporate master data are met across business units and global support functions
Provide issue resolution or escalate as needed to appropriate level or parties
Candidate Profile
Designs and develops a framework for global Master Data standardization and governance, including owning process flows, data maps, and standard definitions for governance across a variety of cross-functional systems and processes.
Create, maintain, and deprecate item, customer, vendor, grower, ranch, facility, pricing, Bill of Material data in ERP Systems to facilitate smooth and accurate transaction flow through the ERP.
Define and publish data definitions for the organization as data elements are added or modified
Serve as master data SME on all new system integrations or enhancements requiring master data to ensure proper usage and assess process impact
Analyze and document process and data points as necessary to support seamless integration as business models change or exceptions arise
Ensure data quality and MDM adherence through systems & processes to ensure data integrity.
Research and document Master Data processes and governance best practices.
Facilitate change management initiatives with stakeholders
Review Master Data requests that affect financial reports or mapping and ensure they follow the established definitions and guidelines
Ensure policies, procedures, and SLAs relating to corporate master data are met across business units and global support functions
Provide issue resolution or escalate as needed to appropriate level or parties
The following information is provided in good faith as a general description of the salary range and benefits for the position posted. The actual compensation offered to the successful candidate is dependent upon experience, skills, education, work location, internal pay equity, and other objective job-related factors.
Salary Range estimated for
SR SQL Developer $97,000 - $150,000 USD.
About Driscolls
Driscoll's is the global market leader for fresh strawberries, blueberries, raspberries and blackberries. With more than 100 years of farming heritage and hundreds of independent growers around the world, Driscoll's is passionate about growing fresh, beautiful and delicious berries. Our values of humility, passion and trustworthiness have guided our mission to delight consumers around the world.
Driscoll's exclusive patented berry varieties are developed through years of research using only natural breeding methods – meaning, no GMOs. From farm-to-table, we focus on delivering a high quality, premium berry experience with our many supply chain partners.
Driscoll's is the trusted brand for Only the Finest Berries™.
Driscoll's es el proveedor líder en la comercialización de Fresas, Frambuesas, Zarzamoras y Arándanos Azules en el mundo. Con más de 100 años de herencia en agricultura y cientos de productores independiente alrededor del mundo, Driscoll's es una compañía apasionada por cultivar Berries frescas, hermosas y deliciosas. Nuestros valores de humildad, pasión y confianza han guiado nuestra misión de deleitar a los consumidores en el mundo.
Las variedades exclusivas de Driscoll's han sido desarrolladas a través de años de investigación usando sólo métodos de mejoramiento natural. De la granja a la mesa, nos enfocamos en ofrecer una experiencia de alta calidad con apoyo de nuestros socios de la cadena de suministro.
Driscoll's es la marca de confianza de sólo las mejores Berries.
Apply Return to Job List
Show more
Show less","MDM, Master Data Management, Operations, Analysis, Framework, Governance, Standards, Data Definitions, Data Quality, Data Integrity, ERP Systems, Data Maps, Data Standardization, Data Governance, Data Mapping, Data Integration, Data Migration, Business Intelligence, Reporting, Data Modeling, Data Warehousing, Data Mining, Data Analytics, Data Visualization, Change Management","mdm, master data management, operations, analysis, framework, governance, standards, data definitions, data quality, data integrity, erp systems, data maps, data standardization, data governance, data mapping, data integration, data migration, business intelligence, reporting, data modeling, data warehousing, data mining, data analytics, data visualization, change management","analysis, business intelligence, change management, data definitions, data governance, data integration, data integrity, data mapping, data maps, data migration, data mining, data quality, data standardization, dataanalytics, datamodeling, datawarehouse, erp systems, framework, governance, master data management, mdm, operations, reporting, standards, visualization"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Carmel Valley, CA",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3759706903,2023-12-17,Monterey,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto,
San
Francisco or Chicago
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Machine Learning, Python, Java, SQL, Git, Snowflake, Airflow, Kubernetes, Docker, Spark, AWS, GCP, Azure, NoSQL, ETL, Kafka, Storm, SparkStreaming, Hadoop, Linux, Bash","data engineering, machine learning, python, java, sql, git, snowflake, airflow, kubernetes, docker, spark, aws, gcp, azure, nosql, etl, kafka, storm, sparkstreaming, hadoop, linux, bash","airflow, aws, azure, bash, data engineering, docker, etl, gcp, git, hadoop, java, kafka, kubernetes, linux, machine learning, nosql, python, snowflake, spark, sparkstreaming, sql, storm"
Staff Data Engineer,Recruiting from Scratch,"South Venice, FL",https://www.linkedin.com/jobs/view/staff-data-engineer-at-recruiting-from-scratch-3744395462,2023-12-17,Sarasota,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our client is a dating app.
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, deliver analysis to further improve engagement in existing features, and empower our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Solve technical problems of the highest scope and complexity
Exert significant influence on the company’s analytical long-range goals and data architecture
Define and extend our internal standards for style, maintenance, and best practices for a high-scale data platform
Provide mentorship for all on your team to help them grow in their technical responsibilities
Propose ideas to improve the scale, performance, and capabilities of the Data Platform
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We'll Love About You
7+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field
7+ years experience using Python
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention
Location
: This role will be located in office twice a week minimum in Palo Alto, San Francisco or Chicago.
Salary Range: $180,000 - 250,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data engineering, Business intelligence, Data science, Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, SQL, TDD, Pair programming, Continuous integration, Automated testing, Kafka, Storm, SparkStreaming, Data modeling, Schema design, Data warehouses, ETL, Data management tools, Data classification, Data retention","data engineering, business intelligence, data science, python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, tdd, pair programming, continuous integration, automated testing, kafka, storm, sparkstreaming, data modeling, schema design, data warehouses, etl, data management tools, data classification, data retention","airflow, automated testing, business intelligence, continuous integration, data classification, data engineering, data management tools, data retention, data science, data warehouses, datamodeling, docker, etl, helm, kafka, kubernetes, pair programming, python, schema design, snowflake, spark, sparkstreaming, sql, storm, tdd"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"South Venice, FL",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3748833054,2023-12-17,Sarasota,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our next evolution is underway as a newly public company looking to expand and continue to build meaningful experiences for our users. From social issues to original content, we’re blazing innovative paths with impact for our community, all while leveraging the latest tech stacks and striving for engineering excellence. At the heart of our work in this new chapter is a shared set of core values: openness and exploration, a bias for action, and strong support of the LGBTQ community.
With a track record of strong financial performance and plans for continued headcount growth, we’re looking to build a team of talented, passionate, and open-minded people who believe in our mission, align with our values, and are excited to work at the intersection of innovative technology and social impact. Come be a part of this exciting journey with us.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location: Palo Alto, San Francisco or Chicago
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Data Science, Business Intelligence, Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, SQL, TDD, Pair Programming, Continuous Integration, Automated Testing, Deployment, Kafka, Storm, SparkStreaming, Dimensional Data Modeling, Schema Design, ETL, Data Warehouses, Legal Compliance, Data Management Tools, Data Classification, Data Retention","data engineering, data science, business intelligence, python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, tdd, pair programming, continuous integration, automated testing, deployment, kafka, storm, sparkstreaming, dimensional data modeling, schema design, etl, data warehouses, legal compliance, data management tools, data classification, data retention","airflow, automated testing, business intelligence, continuous integration, data classification, data engineering, data management tools, data retention, data science, data warehouses, deployment, dimensional data modeling, docker, etl, helm, kafka, kubernetes, legal compliance, pair programming, python, schema design, snowflake, spark, sparkstreaming, sql, storm, tdd"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"South Venice, FL",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3744397546,2023-12-17,Sarasota,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
They are a profitable dating application.
Location: This is a hybrid role based in their Chicago office and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location
: This role is twice a week in a hybrid role minimum.
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, SQL, TDD, Pair Programming, Continuous Integration, Automated Testing, Kafka, Storm, SparkStreaming, Dimensional Data Modeling, Data Warehouses, ETL, Data Management Tools, Data Classification, Data Retention","data engineering, python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, tdd, pair programming, continuous integration, automated testing, kafka, storm, sparkstreaming, dimensional data modeling, data warehouses, etl, data management tools, data classification, data retention","airflow, automated testing, continuous integration, data classification, data engineering, data management tools, data retention, data warehouses, dimensional data modeling, docker, etl, helm, kafka, kubernetes, pair programming, python, snowflake, spark, sparkstreaming, sql, storm, tdd"
Sr Data Engineer,Pella Corporation,"Pella, IA",https://www.linkedin.com/jobs/view/sr-data-engineer-at-pella-corporation-3724372295,2023-12-17,Oskaloosa,United States,Mid senior,Hybrid,"JOB SUMMARY:
This role will be embedded on a cross-functional product Agile scrum team as the primary data engineer. Work with Data Architect(s) and other DAI personnel and takes the lead to develop and maintain data flows, data workflows and other code/logic to gather, create and deliver high quality reliable data to meet the needs of Pella’s business. These efforts will support Pella’s analytics and transactional needs. Additionally, this position will support Pella’s Data Enablement Technologies and Data Users with on-call responsibilities, direct end-user tickets and addressing performance or quality issues.
ESSENTIAL RESPONSIBILITIES:
Design and develop data pipelines to manage how data flows between disparate systems
Build data pipelines to feed analytics use cases, KPI or enterprise apps.
Develop data quality metrics and performs QC tests (system and visual) to verify data integrity.
Interface with architects, product managers/SMEs and product analysts to understand data needs and support the implementation of the business rules into transformation.
Document the data blending process along with the specifications and workflow/data lineage.
Perform continuous integration to ensure that every step of the pipeline is testable and automated
Lead cloud data migration, transformation and modeling projects, developing project plans and communicating project status through Agile process and in Jira for the cross-functional team
Collaborate with the business to understand backlog and refine use cases related to data management, BI reporting and data science deliverables. Research source system data, architecture and transactions.
Takes lead to perform detailed design (the Physical Data Model and transformations), based on understanding of the Logical Data Model (the business requirements)
Create design documents for data integration or data reporting projects
Develop new and improve existing processes to ensure service levels are being met
Support development of new or modify existing analytical reports
Analyze data integration problems, provide solutions and recommend corrective actions.
Analyze source system data structures and map them to target data warehouse schemas.
Must have excellent skills in requirements analysis, logical/physical modeling, data transformation and data modeling and technical governance design concepts.
Serve as a technical expert to data warehouse project teams and key business individuals for support of applications, tools, data integration, and ad-hoc analytics.
Participate in design and code reviews, documentation of design, and implementation of methodologies to ensure high quality deployments
Analyze application and data integration problems, provide solutions and recommend corrective actions.
Education/Experience
BS degree in Computer Science, Data Engineering, Software Engineering, or a related field. MBA beneficial.
5+ years' experience in data engineering / software development
QUALIFICATIONS:
Technical / functional skills (includes computer skills):
Expertise in Azure cloud technologies specifically Synapse, ADF, Delta Lake, Databricks or comparable technology experience within AWS, Snowflake and/or GCP
Experience working with architectural fabric of Salesforce or comparable CRM applications
Understanding of architecture of Data Quality, Metadata Management and Master Data Management
Understanding of Data Governance and Data Stewardship concepts
Understanding of dimensional data modeling and design as well as data population techniques for target structures such as Star Schemas.
Skilled in Python, SPARK and SQL to build production-grade data pipelines and tools
Experience navigating a modern data environment and working between on-prem & cloud technologies
Knowledge of the data science process and understanding of/experience with Data Engineering support for Data Science
Strong grasp of CI/CD operating practices
Experience operating within a Product Scrum Agile team
Experience with MS Office, Outlook, Jira
Leadership Skills:
Strong communication and collaboration skills, good project management methodology
Certifications or licenses: None
Travel Expected:
5-10% of time
Show more
Show less","Data Engineering, Agile, Data Pipelines, Data Quality, Data Integration, Data Transformation, Data Modeling, Cloud Technologies, Azure, Synapse, ADF, Delta Lake, Databricks, Snowflake, AWS, GCP, Salesforce, Python, SPARK, SQL, Data Governance, Data Stewardship, Dimensional Modeling, Star Schemas, CI/CD, Product Scrum Agile, MS Office, Outlook, Jira, Linux, Bash, Docker, Kubernetes","data engineering, agile, data pipelines, data quality, data integration, data transformation, data modeling, cloud technologies, azure, synapse, adf, delta lake, databricks, snowflake, aws, gcp, salesforce, python, spark, sql, data governance, data stewardship, dimensional modeling, star schemas, cicd, product scrum agile, ms office, outlook, jira, linux, bash, docker, kubernetes","adf, agile, aws, azure, bash, cicd, cloud technologies, data engineering, data governance, data integration, data quality, data stewardship, data transformation, databricks, datamodeling, datapipeline, delta lake, dimensional modeling, docker, gcp, jira, kubernetes, linux, ms office, outlook, product scrum agile, python, salesforce, snowflake, spark, sql, star schemas, synapse"
Clinical Flow Cytometry Data Analyst TEMP,Magnit,"Tarrytown, NY",https://www.linkedin.com/jobs/view/clinical-flow-cytometry-data-analyst-temp-at-magnit-3703018754,2023-12-17,Tarrytown,United States,Mid senior,Onsite,"Clinical Flow Cytometry Data Analyst TEMP
Pay Range: 36.39 - 59.38
We are seeking a Clinical Flow Cytometry Data Analyst to join our Precision Medicine team. Precision Medicine, a part of Early Clinical Development & Experimental Sciences leads biomarker strategy and execution from early concept studies through late development and is supported by Precision Medicine Strategy Leads, Precision Medicine Operations specialists, as well as quantitative analytical and companion diagnostics scientists.
Flow cytometry continues to gain importance with regulators as an essential method in the development of personalized medicine. In this newly created role that resides within Precision Medicine, the individual would be primarily responsible for supporting analysis and review of clinical flow cytometry data acquired across various clinical studies and therapeutic areas. The role requires a solid, hands-on understanding of flow cytometry and will support data characterization from conventional and spectral cytometry platforms often from large multi-color panels. In this role, the individual would develop a pipeline to ensure high data quality, working closely with key internal stakeholders and vendors. The selected individual would be an important contributor to the Biomarker Assays and Quality team. This is an onsite role with participation in meetings and activities in Tarrytown, NY.
As a Clinical Flow Cytometry Data Analyst, a typical day may include the following:
Review and analyze raw data (using various gating strategies) to identify irregularities and negative trends
Contribute independently and in matrix teams to improve data quality from flow cytometry experiments
Meet with key stakeholders and develop KPIs for assay quality, and periodically review assay quality data from various sources on relevant clinical trials
Support clinical biomarker strategy leads through data upload, workflow sharing, and other activities performed especially through cloud platforms
Support initiatives to reduce instrument and technical variation across assays and vendors
Support the development of business process documents related to these activities
Manage limited number of critical internal reagents especially supporting cytometry assays with monitoring of consumption and forecasting future needs
Participate in cross functional meetings to support flow cytometry and other biomarker assay development, validation and assay quality initiatives
This role may be for you if:
Candidate must be detail-oriented, self-starter and possess both strong organizational and communication skills
Able to multi-task and critically analyze and troubleshoot technical/scientific problems of various complexity levels
Ability to collaborate cross-functionally with members of the Flow Cytometry Core and research therapeutic areas
To be considered for this role, you must have a B.S. with at least 2-3 years’ experience performing and analyzing flow cytometry assays or M.S. with at least 1 year experience. Experience and coursework in Immunology and/or Oncology setting preferred. Experience independently using at least 1 or more flow cytometry software programs (FlowJo, FCS Express, etc) is required. Knowledge of OMIQ analysis platform a plus. Experience in coding (R, Python, etc), Data Science or willingness to learn in support of role and tool building. Proficiency with Microsoft Word, Excel, PowerPoint; experience with MS Project is helpful. Role may involve infrequent travel to meet with key vendors.
This is a contract position at Regeneron with Magnit Global being the Employer.
To do our best work we need different viewpoints. Therefore, we celebrate diversity and embrace inclusion. As an equal opportunity employer, we are dedicated to building a team that represents a variety of backgrounds, perspectives, and skills. We strive to ensure that we maintain a positive and enriching work environment for all.
​
Show more
Show less","Flow cytometry, Data analysis, Data quality, Statistical analysis, Oncology, Immunology, Gating strategies, Assay validation, Assay development, Troubleshooting, Microsoft Office Suite, MS Project, R, Python, Data Science, FlowJo, FCS Express, OMIQ analysis platform","flow cytometry, data analysis, data quality, statistical analysis, oncology, immunology, gating strategies, assay validation, assay development, troubleshooting, microsoft office suite, ms project, r, python, data science, flowjo, fcs express, omiq analysis platform","assay development, assay validation, data quality, data science, dataanalytics, fcs express, flow cytometry, flowjo, gating strategies, immunology, microsoft office suite, ms project, omiq analysis platform, oncology, python, r, statistical analysis, troubleshooting"
Data Analyst/ Modeler,NYC Department of Finance,New York City Metropolitan Area,https://www.linkedin.com/jobs/view/data-analyst-modeler-at-nyc-department-of-finance-3766976360,2023-12-17,Tarrytown,United States,Mid senior,Onsite,"NYC Department of Finance (DOF) is responsible for administering the tax revenue laws of the city fairly, efficiently, and transparently to instill public confidence and encourage compliance while providing exceptional customer service.
The Tax Policy & Data Analytics Division reviews, analyzes, and evaluates existing and proposed policies and legislation that affect the Department of Finance and New York City in general. This includes analyzing the revenue and distributional impacts of proposed changes to the tax system, monitoring and reporting on tax and parking revenues, working with local and state public agencies and private sector interests to promote improved tax administration, and advising the commissioner, Executive Office, and the New York City Office of Management and Budget on revenue and budgetary issues. The Tax Policy and Data Analytics Division is also responsible for all modeling and data mining for DOF’s Audit Division. Tax Policy also prepares briefing and position papers on tax policy and issues a variety of public reports and newsletters on tax-related issues.
Tax Policy and Data Analytics Division seeks an individual with strong quantitative and qualitative research skills to serve as Data Analyst/Modeler in the Tax Policy and Data Analytics Division's Data Intelligence Group (DIG). The selected candidate will assist with the development of computer programs and data models, using SAS, for the purposes of audit-candidate selection, model and audit result tracking, and related research.
Duties will include, but are not limited to:
- Write code for and support the development of enhanced matching programs using a variety of data sources in order to identify non-filers and under-reporters as candidates for audit.
- Provide technical computer programming and data analysis support to the agency's modeling and computer assisted audit functions.
- Use SAS to merge and manipulate large datasets and critically analyze statistical findings.
- Liaise with staff from Tax Policy and other divisions to successfully implement and track results.
- Conduct analysis on the expected impact of proposed changes to tax law.
- Use taxpayer transactional data in various formats to construct statistical samples for audit purposes.
- Manage responses to ad-hoc data and reporting requests.
Minimum Qualifications
Level I
1. A master’s degree from an accredited college or university in social science, economics, statistics, computer science, data analysis, geography, sciences, technology, engineering, mathematics (STEM), or a closely related field, with at least 12 credits or five courses in economics, public policy, econometrics, statistics, mathematics, engineering, geography or computer science.
2. A baccalaureate degree from an accredited college or university as described in “1” above and two years of full-time, professional experience performing statistical analysis and programming work in any of the areas described in “1” above.
Level II
1. A master’s degree from an accredited college or university in social science, economics, statistics, computer science, data analysis, geography, sciences, technology, engineering, mathematics (STEM), or a closely related field, with at least 12 credits or five courses in economics, public policy, econometrics, statistics, mathematics, engineering, geography or computer science.
2. A baccalaureate degree from an accredited college or university as described in “1” above and two years of full-time, professional experience performing statistical analysis and programming work in any of the areas described in “1” above.
Special Note:
To be eligible for placement in Assignment Level II, individuals must have, in addition to meeting the minimum requirements, at least one year full-time work experience in a related field, or a master’s degree from an accredited college or university, in the areas described in “1” above, with at least 12 credits or three advanced courses in economics, public policy, econometrics, statistics, mathematics, engineering, geography, or computer science.""
Level III
1. A master’s degree from an accredited college or university in social science, economics, statistics, computer science, data analysis, geography, sciences, technology, engineering, mathematics (STEM), or a closely related field, with at least 12 credits or five courses in economics, public policy, econometrics, statistics, mathematics, engineering, geography or computer science.
2. A baccalaureate degree from an accredited college or university as described in “1” above and two years of full-time, professional experience performing statistical analysis and programming work in any of the areas described in “1” above.
Special Note:
To be eligible for placement in Assignment Level III, individuals must have, in addition to meeting the minimum requirements of Level II, at least three years full-time work experience in a related field, or a Doctorate degree from an accredited college or university, in the areas described above, with at least 12 credits or three advanced courses in economics, public policy, econometrics, statistics, mathematics, engineering, geography or computer science.
Preferred Skills
- Strong programming, data analysis, statistical sampling and/or data mining experience. - Excellent written and verbal communication. - Strong qualitative and quantitative analysis skills.
Public Service Loan Forgiveness
As a prospective employee of the City of New York, you may be eligible for federal loan forgiveness programs and state repayment assistance programs. For more information, please visit the U.S. Department of Education’s website at https://studentaid.gov/pslf/.
Residency Requirement
New York City residency is generally required within 90 days of appointment. However, City Employees in certain titles who have worked for the City for 2 continuous years may also be eligible to reside in Nassau, Suffolk, Putnam, Westchester, Rockland, or Orange County. To determine if the residency requirement applies to you, please discuss with the agency representative at the time of interview.
Additional Information
The City of New York is an inclusive equal opportunity employer committed to recruiting and retaining a diverse workforce and providing a work environment that is free from discrimination and harassment based upon any legally protected status or protected characteristic, including but not limited to an individual's sex, race, color, ethnicity, national origin, age, religion, disability, sexual orientation, veteran status, gender identity, or pregnancy.
Show more
Show less","SAS, Data analysis, Statistical sampling, Data mining, Programming, Statistics, Computer science, Econometrics, Public policy, Mathematics, Economics, Engineering, Geography","sas, data analysis, statistical sampling, data mining, programming, statistics, computer science, econometrics, public policy, mathematics, economics, engineering, geography","computer science, data mining, dataanalytics, econometrics, economics, engineering, geography, mathematics, programming, public policy, sas, statistical sampling, statistics"
Data Analyst,Valera Health,"Manhattan, NY",https://www.linkedin.com/jobs/view/data-analyst-at-valera-health-3770515030,2023-12-17,Tarrytown,United States,Mid senior,Onsite,"Valera Health is a leading tele-mental health service provider, utilizing cutting-edge digital technology and carefully cultivated teams to deliver top-tier, behavioral healthcare. We believe that mental health care should be simple, accessible, and affordable.
Why Valera:
Our Purpose:
to provide compassionate mental health care to those who need it the most, when they need it the most.
Mission Driven:
We know that behavioral healthcare isn't one-size-fits-all, and we've tailored our complete care model to meet the needs of the individual. We believe that no one should be excluded from receiving the care they need, and our diverse team reflects the change we bring to the space.
Culture:
At the core of our culture lies a profound commitment to fostering a spirit of teamwork, dedication to continuous learning, a devotion to integrity, and a pursuit of delivering the highest quality mental healthcare services. Our ethos is rooted in nurturing your personal and professional growth, as we champion the principles of inclusivity and equity in all that we do.
Watch to learn more about Valera Health here!
About You:
We are seeking a hybrid New York City based Data Analyst to join our rapidly growing organization in a strategic capacity. This is a hybrid role working from both our coworking office space in Williamsburg, Brooklyn and from home. Be part of a compassionate care team dedicated to elevating mental health care. We provide the technology and systems you need to work efficiently and effectively.
Responsibilities:
Work with large data sets and build metrics to provide meaningful insights to leadership around Clinical Outcomes, Revenue Cycle Management, Finance, Business Development, and Marketing
Identify key insights to further leverage claims and other business measure data
Build and maintain internal facing, self service dashboards to support day to day decisions for department leads
Develop end to end processes around the collection, integration and reporting of data from new and existing vendors and partnerships
Partner with department leads to understand and improve current clinical and operational practices and workflows
Communicate technical information in an accessible executive and partner facing manner in both verbal presentations to an organization's leadership to communicate key findings and updates on business process adjustments
Perform technical tasks such as creating business requirements documents, user training manuals and guides, and requirements traceability
Conduct in-depth data analysis using SQL, AWS tools, Python, and spreadsheets
Periodically review the progress of any recommended and implemented changes to see if they're still on track, and repeating the above steps as needed to ensure continuous improvement
Ad hoc projects as requested by the manager
Qualifications:
Required:
Bachelor in Computer Science, Finance, or related field
1-5 years of Analytics experience at a technology-focused organization, high-growth startup, healthcare company, or similar background
Hands on experience working with non-technical end-business users, not limited to crafting requirements, designing dashboards and communicating technical terms and limitations
Advanced in SQL (AWS Athena, Presto, PostgreSQL or other SQL)
Python experience, specifically relating to external API integrations and simple data processing using AWS Lambda or other serverless cloud platforms
Proficient in Excel (data manipulation, vlookup, index match)
Experience building impactful BI dashboards (Quicksight, Tableau, Looker, Excel, etc.)
Experience working in full end-to-end analytics cycle - data management, cleaning/processing, analysis, visualization
Preferred:
Experience working with EHR, claims, marketing data from both tabular and JSON data structures
Familiarity with clinical coding (ICD-10, CPT), medication (NDC, RxNorm), and health insurance components
Prior experience or eagerness to learn Amazon Web Service tools (S3, Athena, Lambda, and Cloudwatch, among others)
Collaborative working style with the ability to work independently in a remote environment
Interested in exploring and implementing new approaches or tools, and is highly adaptable
Brings a forward thinking, self-service and automation first mindset to improve reporting capabilities
Strong organizational skills, keen eye for details and innate problem-solving skills
Understanding of version control processes using GitHub in a shared codebase
Interest in developing & implementing of advanced analytical models
Experience working with Google Ads
Familiarity with Atlassian services (Confluence, JIRA) and/or Agile-based methods
Fair work deserves fair wages. At Valera, we value this role at between 85k-120k with the potential for an annual performance bonus based on experience.
Benefits include but not limited to:
Health Insurance
Vision/Dental
401k
Paid Time Off
STD
Life Insurance
And many more
Be part of our mission!
We are very proud of the work that we do and it takes a great team to make it happen! If you are interested in one of our open positions, we'd love to start the conversation.
We hire people from all backgrounds because that's what it takes to build a team that can reach and support those in need of high-quality behavioral healthcare. We are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, gender, sexual orientation, gender identity, national origin, disability status, protected veteran status, or any other characteristic protected by law.
Check us out on Linkedin!
Show more
Show less","SQL, AWS Athena, AWS Lambda, Python, Presto, PostgreSQL, Tableau, Looker, Quicksight, EHR, ICD10, CPT, NDC, RxNorm, S3, Cloudwatch, GitHub, Confluence, JIRA, Agile","sql, aws athena, aws lambda, python, presto, postgresql, tableau, looker, quicksight, ehr, icd10, cpt, ndc, rxnorm, s3, cloudwatch, github, confluence, jira, agile","agile, aws athena, aws lambda, cloudwatch, confluence, cpt, ehr, github, icd10, jira, looker, ndc, postgresql, presto, python, quicksight, rxnorm, s3, sql, tableau"
Data Analyst,Valera Health,"Queens, NY",https://www.linkedin.com/jobs/view/data-analyst-at-valera-health-3770516003,2023-12-17,Tarrytown,United States,Mid senior,Onsite,"Valera Health is a leading tele-mental health service provider, utilizing cutting-edge digital technology and carefully cultivated teams to deliver top-tier, behavioral healthcare. We believe that mental health care should be simple, accessible, and affordable.
Why Valera:
Our Purpose:
to provide compassionate mental health care to those who need it the most, when they need it the most.
Mission Driven:
We know that behavioral healthcare isn't one-size-fits-all, and we've tailored our complete care model to meet the needs of the individual. We believe that no one should be excluded from receiving the care they need, and our diverse team reflects the change we bring to the space.
Culture:
At the core of our culture lies a profound commitment to fostering a spirit of teamwork, dedication to continuous learning, a devotion to integrity, and a pursuit of delivering the highest quality mental healthcare services. Our ethos is rooted in nurturing your personal and professional growth, as we champion the principles of inclusivity and equity in all that we do.
Watch to learn more about Valera Health here!
About You:
We are seeking a hybrid New York City based Data Analyst to join our rapidly growing organization in a strategic capacity. This is a hybrid role working from both our coworking office space in Williamsburg, Brooklyn and from home. Be part of a compassionate care team dedicated to elevating mental health care. We provide the technology and systems you need to work efficiently and effectively.
Responsibilities:
Work with large data sets and build metrics to provide meaningful insights to leadership around Clinical Outcomes, Revenue Cycle Management, Finance, Business Development, and Marketing
Identify key insights to further leverage claims and other business measure data
Build and maintain internal facing, self service dashboards to support day to day decisions for department leads
Develop end to end processes around the collection, integration and reporting of data from new and existing vendors and partnerships
Partner with department leads to understand and improve current clinical and operational practices and workflows
Communicate technical information in an accessible executive and partner facing manner in both verbal presentations to an organization's leadership to communicate key findings and updates on business process adjustments
Perform technical tasks such as creating business requirements documents, user training manuals and guides, and requirements traceability
Conduct in-depth data analysis using SQL, AWS tools, Python, and spreadsheets
Periodically review the progress of any recommended and implemented changes to see if they're still on track, and repeating the above steps as needed to ensure continuous improvement
Ad hoc projects as requested by the manager
Qualifications:
Required:
Bachelor in Computer Science, Finance, or related field
1-5 years of Analytics experience at a technology-focused organization, high-growth startup, healthcare company, or similar background
Hands on experience working with non-technical end-business users, not limited to crafting requirements, designing dashboards and communicating technical terms and limitations
Advanced in SQL (AWS Athena, Presto, PostgreSQL or other SQL)
Python experience, specifically relating to external API integrations and simple data processing using AWS Lambda or other serverless cloud platforms
Proficient in Excel (data manipulation, vlookup, index match)
Experience building impactful BI dashboards (Quicksight, Tableau, Looker, Excel, etc.)
Experience working in full end-to-end analytics cycle - data management, cleaning/processing, analysis, visualization
Preferred:
Experience working with EHR, claims, marketing data from both tabular and JSON data structures
Familiarity with clinical coding (ICD-10, CPT), medication (NDC, RxNorm), and health insurance components
Prior experience or eagerness to learn Amazon Web Service tools (S3, Athena, Lambda, and Cloudwatch, among others)
Collaborative working style with the ability to work independently in a remote environment
Interested in exploring and implementing new approaches or tools, and is highly adaptable
Brings a forward thinking, self-service and automation first mindset to improve reporting capabilities
Strong organizational skills, keen eye for details and innate problem-solving skills
Understanding of version control processes using GitHub in a shared codebase
Interest in developing & implementing of advanced analytical models
Experience working with Google Ads
Familiarity with Atlassian services (Confluence, JIRA) and/or Agile-based methods
Fair work deserves fair wages. At Valera, we value this role at between 85k-120k with the potential for an annual performance bonus based on experience.
Benefits include but not limited to:
Health Insurance
Vision/Dental
401k
Paid Time Off
STD
Life Insurance
And many more
Be part of our mission!
We are very proud of the work that we do and it takes a great team to make it happen! If you are interested in one of our open positions, we'd love to start the conversation.
We hire people from all backgrounds because that's what it takes to build a team that can reach and support those in need of high-quality behavioral healthcare. We are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, gender, sexual orientation, gender identity, national origin, disability status, protected veteran status, or any other characteristic protected by law.
Check us out on Linkedin!
Show more
Show less","AWS, Athena, SQL, Python, Tableau, Lambda, Cloudwatch, Looker, JIRA, Confluence, Presto, PostgreSQL, RxNorm, ICD10, EHR, GitHub, Atlassian, Quicksight, Excel","aws, athena, sql, python, tableau, lambda, cloudwatch, looker, jira, confluence, presto, postgresql, rxnorm, icd10, ehr, github, atlassian, quicksight, excel","athena, atlassian, aws, cloudwatch, confluence, ehr, excel, github, icd10, jira, lambda, looker, postgresql, presto, python, quicksight, rxnorm, sql, tableau"
Senior Data Engineer,Insight Global,New York City Metropolitan Area,https://www.linkedin.com/jobs/view/senior-data-engineer-at-insight-global-3773996461,2023-12-17,Tarrytown,United States,Mid senior,Hybrid,"Must Haves
4+ years as a Data Engineer - data science, machine learning, advanced analytics experience in high velocity, high-growth companies
Experience with
Databricks
, Snowflake, Amazon Web Services, or related cloud platforms
Strong coding skills in general purpose languages like Scala or Python, and familiarity with software engineering principles around testing, code reviews and deployment
Experience with
distributed data processing systems like Spark, and proficiency in SQL
Experience owning and managing a full pipeline within a large project, including interfacing with business stakeholders, architects, and senior leadership to effectively communicate status and concerns.
Familiarity with ML and MLOps concepts and technologies, such as model training, deployment, and monitoring.
Plusses
Experience working within the media, entertainment, television, and/or streaming industry.
Day-to-day
Insight Global is seeking out a Senior Data Engineer to sit hybrid in New York City for a large media/entertainment client. This Sr. Engineer will be joining the Playback Analytics Engineering team who is responsible for building out data pipelines and applications for the operations of current products and all new product launches. This person will be owning full segments of a data engineering project, managing expectations, and driving delivery with the appropriate stakeholders while maintaining the consensus for engineering architecture. As part of the Global Video Engineering organization, this team collects client-side and server-side requirements and prepares it for analytics workloads. This engineer will be a major contributor to the overall data product roadmap by working closely with business partners to understand their challenges and develop analytical tools to help drive business decisions. They will design, build, and scale data pipelines across a variety of source systems and streams, distributed/elastic environments, and downstream applications and implement the appropriate design patterns while optimizing performance, cost, security, and scale and end user experience. This person will be expected to build and manage relationships with supporting IT teams to effectively deliver work products to production and participate in development sprints, demos, and retrospectives, as well as release and deployment. This person must be action-orientated as they will be constantly figuring out new problems and regularly showing results with a positive attitude by displaying ethical behavior, integrity, and building trust. The ideal candidate has dealt with ambiguity and can make quality decisions in a dynamic, fast-paced environment.
Show more
Show less","Databricks, Snowflake, Amazon Web Services, Cloud platforms, Scala, Python, Spark, SQL, Distributed data processing systems, Machine learning, MLOps, Model training, Model deployment, Model monitoring","databricks, snowflake, amazon web services, cloud platforms, scala, python, spark, sql, distributed data processing systems, machine learning, mlops, model training, model deployment, model monitoring","amazon web services, cloud platforms, databricks, distributed data processing systems, machine learning, mlops, model deployment, model monitoring, model training, python, scala, snowflake, spark, sql"
Market Data Engineer,Selby Jennings,New York City Metropolitan Area,https://www.linkedin.com/jobs/view/market-data-engineer-at-selby-jennings-3780670334,2023-12-17,Tarrytown,United States,Mid senior,Hybrid,"Our client is a New York based hedge fund with elite engineering teams across the globe in Hong Kong, London, and Singapore. Their most urgent need is a Senior Market Data Engineering that is open to being one of the first hires on this team. The role covers many area around core data engineering, interacting with the C Level Suite, and core technology support on the trading floor.
Responsibilities
Architect and Develop the firm's core market data platform
Provide technical support to the trading floor
Meet with C level stakeholders on platform upgrades
Background
5+ years of experience working in a market data environment, mainly using vendors like Bloomberg, FactSet, or Reuters
Past experience working as a Software Engineering using languages like Python, C#, or C++
Show more
Show less","Market Data Engineering, Software Engineering, Python, C#, C++, Bloomberg, FactSet, Reuters","market data engineering, software engineering, python, c, c, bloomberg, factset, reuters","bloomberg, c, factset, market data engineering, python, reuters, software engineering"
Senior Data Center Engineer - Leading Hedge Fund,Xcede,New York City Metropolitan Area,https://www.linkedin.com/jobs/view/senior-data-center-engineer-leading-hedge-fund-at-xcede-3763310390,2023-12-17,Tarrytown,United States,Mid senior,Hybrid,"We are looking for a
Senior Data Center Engineer
to work at a
top-tier hedge fund
based in NYC with regional travel expected to co-location New Jersey based centers. This is a hands-on position that will be responsible for
performing the daily upkeep of cutting-edge data center equipment, designing, building, installing, configuring and cabling.
We expect this individual to have an expert background in ‘racking & stacking’ while being able to communicate with key stakeholders (internal and 3rd party vendors & service providers) to design and
build data center best practices and implement them across multiple facilities.
Those applying should have experience in:
Designing plans for new data center equipment deployment
Logistic management including procurement, shipping, moving, and keeping inventory
Installing, cabling, configuring, and troubleshooting different types of network and server equipment (can address Layer 2 and 3 connectivity issues)
Install and configure server OS in a PXE environment.
Proficient in a Linux environment (Scripting skills in Bash or Python are a plus)
Experienced with server and network hardware (e.g., Dell, Cisco, Arista, Ciena, OTDR testing tools like EXFO and VIAVI)
Experience in the design, deploy, test, and troubleshooting both copper and fiber cabling
A valid driver license and ability to climb ladders for equipment installation is essential.
This is an excellent opportunity to work in a leading organization to help design & orchestrate the management of a set of data centers using market leading technology and work with stakeholders across the business to identify enhancements to workflow, capacity management, data center builds, liquid cooling and power balancing among others.
Show more
Show less","Data Center Design, Equipment Installation, Configuration, Cabling, Network and Server Troubleshooting, Linux Environment, Bash Scripting, Python Scripting, Server and Network Hardware, Copper and Fiber Cabling, PXE Environment, Layer 2 and 3 Connectivity, OTDR Testing Tools, Procurement, Shipping, Inventory Management, Driver's License, Liquid Cooling, Power Balancing","data center design, equipment installation, configuration, cabling, network and server troubleshooting, linux environment, bash scripting, python scripting, server and network hardware, copper and fiber cabling, pxe environment, layer 2 and 3 connectivity, otdr testing tools, procurement, shipping, inventory management, drivers license, liquid cooling, power balancing","bash scripting, cabling, configuration, copper and fiber cabling, data center design, drivers license, equipment installation, inventory management, layer 2 and 3 connectivity, linux environment, liquid cooling, network and server troubleshooting, otdr testing tools, power balancing, procurement, pxe environment, python scripting, server and network hardware, shipping"
Senior Data Engineer (Architect),Storm2,New York City Metropolitan Area,https://www.linkedin.com/jobs/view/senior-data-engineer-architect-at-storm2-3769500384,2023-12-17,Tarrytown,United States,Mid senior,Hybrid,"Senior Data Architect (Tech Lead)
Multinational Brokerage Firm 🚀
US - NY - 3 days in office🌎
$200,000 - 230,000 base pay (flexible) + up to 40% bonus
We are currently working with a leading multinational brokerage firm. Being pioneers in the space, they have office presence in 27 global locations, serving just under 2 million customers.
They are looking for a well-versed data architect to help modernize their existing data systems.
Responsibilities:
Develop and support the implementation of the company’s data architecture strategy
Lead software engineers who are implementing the projects
Drive excellence and implement industry-standard data architecture principles and data standards-compliant with policies and regulatory requirements
Requirements
:
5+ years in data architecture
5+ years in cloud engineering
Experienece with Data Lake is required
Experience with Big Data tech such as Spark, Hadoop, Kafka etc.
Familiarity with the AWS tech such as Athena, Glue, Lambda, S3, DynamoDB, NoSQL, Relational Database Service (RDS), EMR and Redshift
Why Apply
:
Competitive salary, performance-based bonus and stock grant
Be part of a leading multinational company
Fast-tracked career progression opportunities
🌎 United States
📧 Interested in applying? Please click on the ‘Easy Apply’ button
⚡ Storm2 is a FinTech recruitment firm with clients across London, Europe, and North America. To discuss open opportunities or career options, please visit our website www.storm2.com and follow the Storm2 Linked In page for the latest jobs and int
el
Show more
Show less","Data Architecture, Software Engineering, Data Standards, Data Lake, Spark, Hadoop, Kafka, AWS, Athena, Glue, Lambda, S3, DynamoDB, NoSQL, Relational Database Service (RDS), EMR, Redshift","data architecture, software engineering, data standards, data lake, spark, hadoop, kafka, aws, athena, glue, lambda, s3, dynamodb, nosql, relational database service rds, emr, redshift","athena, aws, data architecture, data lake, data standards, dynamodb, emr, glue, hadoop, kafka, lambda, nosql, redshift, relational database service rds, s3, software engineering, spark"
senior data software engineer,"The Randy Neuringer Co., LLC.",New York City Metropolitan Area,https://www.linkedin.com/jobs/view/senior-data-software-engineer-at-the-randy-neuringer-co-llc-3784399173,2023-12-17,Tarrytown,United States,Mid senior,Hybrid,"Role
The Senior Software Engineer position in the Data Strategy teampresents a chance to create and execute data products for the world's biggest and most reputable reinsurance brokerage. Data Strategy has a “start-up style” mandate (within a $2 billion company) to enhance the acquisition, storage, analysis, fidelity, and monetization of client, internal, and third-party data across the organization.
This innovation spans our petabyte-scale insured assets, including property, business, marine, and aviation entities, and their associated risks, such as hurricanes, wildfires, cyber-attacks, and wars, in a financial and economic context.
As a member of the Data Strategy group, the Senior Software Engineer will work with fellow data and web engineers, data scientists, product managers, business analysts, and stakeholders from other internal groups to design and improve data-centric projects with the dual mandate of (1) increasing the efficiency of the data collection and analysis process across and (2) driving the monetization of data via newly designed and existing products for reinsurance clients. The Senior Software Engineer will be the head facilitator on multiple innovative initiatives and will have ownership over the design, development, and delivery of projects requiring direct reporting to senior-level management in both business and technical groups.
Leadership Responsibilities
Work with a product manager as technical lead of a team of ~5 engineers, data scientists, and analysts to design, scope, and oversee work in an Agile environment.
Manage junior data and web engineers, focusing on productivity, quality, and professional development.
Partner with the head of Data Strategy and other senior engineers to create and evangelize best-in-class engineering competency and tooling within the organization.
Enforce strong development standards across the team through code reviews, automated testing, and monitoring.
Establish strong relationships with internal clients as an engineering representative for data strategy.
Contribute to the overall Data Strategy vision and execution via quarterly planning and executive committee reporting.
Partner regularly improving engineering recruiting process for the required skillsets and resourcing demands.
Learn the complex business of reinsurance to coach data technologists and execute the team's initiatives more effectively.
Software Engineer Responsibilities
Develop, implement, and deploy custom data pipelines powering machine learning algorithms, insights generation, client benchmarking tools, business intelligence dashboards, reporting, and new data products.
Innovate new ways to leverage large and small datasets to drive revenue via the development of new products with the Data Strategy team, as well as the enhancement of existing products.
Architect engineering solutions using the latest cloud technologies in a process that spans hypothesis-validating prototypes to large-scale production data products, ensuring internal security and regulatory compliance.
Design solutions that account for unstructured data and document management system(s), including ingesting, tracking, parsing, analyzing, and summarizing documents at scale.
Perform exploratory and goal-oriented data analyses to understand and validate the requirements of data products and help create product roadmaps.
Develop, implement, and deploy front-ends and APIs, which may involve business intelligence dashboards, data pipelines, machine learning algorithms, and file ingestion mechanisms.
Work closely with data scientists, data engineers, web engineers, PMs, and other stakeholders to design & develop products.
Keep current on the latest trends and innovations in data technology and how these trends apply to GC's business and data strategy.
Required Qualifications
5-8+ years of relevant experience in data-focused software engineering
Master’s Degree, Ph.D., or equivalent experience in data science, computer science, or related quantitative field such as applied mathematics, statistics, engineering or operations research
Experience in Python and familiarity with OOP and functional programming principles
Strong knowledge of SQL and familiarity with the high-level properties of modern data stores.
Strong understanding of the contemporary SDLC, including dev/QC/prod environments, unit/integration/UA testing, CI/CD, etc.
Experience building and maintaining CI/CD pipelines with tools such as Azure DevOps, GitLab, Travis, Jenkins, etc.
At least two and ideallyall of the following sets of experience:
Data Engineering
2+ years’ experience with data engineering
Extensive experience with (py)Spark, Python, JSON, and SQL
Experience integrating data from semi-structured and unstructured sources
Knowledge of various industry-leading SQL and NoSQL database systems
Backend Web
2+ years of backend/full-stack web engineering
Experience working with Python-based server-side web frameworks like FastAPI or Django
Experience with complex backends involving multiple data stores, asynchronous worker queues, pub-sub messaging, and the like
Knowledge of cloud-based web deployments (AWS/Azure/GCP, Kubernetes, auto-scaling, etc.)
Experience with one or more major frontend frameworks (React strongly preferred)
Data Science/Analytics
2+ years of data analysis, AI, or data science work
Experience with data cleaning, enrichment, and reporting to business users
Experience selecting, training, validating, and deploying machine-learning models
Experience with or strong interest in learning about LLMs in a productized context
Experience working in an Agile environment to facilitate the quick and effective fulfillment of group goals
Good interpersonal and communication skills for establishing and maintaining sound internal relationships, working well as part of a team, and for presentations and discussions
Strong analytical skills and intellectual curiosity (interest in the meaning and usefulness of the data), as demonstrated through academic experience or work assignments
Excellent English verbal and writing skills for complex communications with GC colleagues in all departments and levels of the organization, including communicating technical concepts to a non-technical audience
Good ability to prioritize workload according to volume, urgency, etc., and to deliver on required projects in a timely fashion
Preferred Qualifications
Strong understanding of entity resolution, streaming technologies, and ELT/ETL frameworks
Experience with web scraping and crowdsourcing technologies
Experience with Databricks and optimizing Spark clusters
Experience architecting web ecosystems from the ground up, including monolith vs. microservice decisions, caching technologies, security integrations, etc.
Experience working with data visualization dashboarding tools (PowerBI, Tableau)
Insurance domain knowledge or strong interest in developing it
Experience with the MS Azure cloud environment
Show more
Show less","Data Strategy, Data Engineering, Backend Web, Data Science/Analytics, Data Products, Data Pipelines, Machine Learning Algorithms, Business Intelligence Dashboards, Data Analysis, Data Warehousing, Cloud Computing, Azure, DevOps, Python, SQL, OOP, Functional Programming, Agile, NoSQL, Kubernetes, React, FastAPI, Django, Spark, JSON, Hadoop, Tableau, PowerBI, LLMs, Entity Resolution, Streaming Technologies, ELT/ETL Frameworks, Web Scraping, Crowdsourcing Technologies, Databricks, Microservices, Caching, Security Integrations, Data Visualization, Dashboarding Tools","data strategy, data engineering, backend web, data scienceanalytics, data products, data pipelines, machine learning algorithms, business intelligence dashboards, data analysis, data warehousing, cloud computing, azure, devops, python, sql, oop, functional programming, agile, nosql, kubernetes, react, fastapi, django, spark, json, hadoop, tableau, powerbi, llms, entity resolution, streaming technologies, eltetl frameworks, web scraping, crowdsourcing technologies, databricks, microservices, caching, security integrations, data visualization, dashboarding tools","agile, azure, backend web, business intelligence dashboards, caching, cloud computing, crowdsourcing technologies, dashboarding tools, data engineering, data products, data scienceanalytics, data strategy, dataanalytics, databricks, datapipeline, datawarehouse, devops, django, eltetl frameworks, entity resolution, fastapi, functional programming, hadoop, json, kubernetes, llms, machine learning algorithms, microservices, nosql, oop, powerbi, python, react, security integrations, spark, sql, streaming technologies, tableau, visualization, web scraping"
Database Reliability Engineer,"Infusive Solutions, Inc","Manhattan, NY",https://www.linkedin.com/jobs/view/database-reliability-engineer-at-infusive-solutions-inc-3783270965,2023-12-17,Tarrytown,United States,Mid senior,Hybrid,"One of the top hedge funds in world that has been around for over 50 years is knee-deep in revamping their systems and they have an opportunity for a Database / Data Engineer who enjoys working in the cloud and welcomes the challenge of doing DevOps as well.
I’ve known the new Head of Infrastructure for over 15 years, and he loves working here. The firm has over 20 billion dollars under management, and only 200+ employees. This makes for an incredibly stable environment that provides you with all the resources you need to design, implement and support cutting edge solutions while affording you the opportunity to work with a lot of different technologies. People in the firm are extremely bright, work well together, and believe it or not, enjoy a good work life balance with regular hours.
Technology:
You will be working heavily with:
Snowflake and SQL Sequel Server
AWS and Azure
ETL / ELT
You will also get to use:
Various Scripting languages; Python, PowerShell, Bash…
DevOps tools like Docker, Terraform, Git, Jira…
Show more
Show less","Snowflake, SQL Server, AWS, Azure, ETL / ELT, Python, PowerShell, Bash, Docker, Terraform, Git, Jira","snowflake, sql server, aws, azure, etl elt, python, powershell, bash, docker, terraform, git, jira","aws, azure, bash, docker, etl elt, git, jira, powershell, python, snowflake, sql server, terraform"
Data Analyst,Cybernetic Search,New York City Metropolitan Area,https://www.linkedin.com/jobs/view/data-analyst-at-cybernetic-search-3778207552,2023-12-17,Tarrytown,United States,Mid senior,Hybrid,"Cybernetic Search has partnered exclusively with a leading financial advisory firm to recruit a Data Analyst. While this company may be one of the oldest investment banks in the US, they have modernized by investing in fintech, infrastructure, power, and renewable energy. This is a great opportunity to join a diverse M&A advisory and financing platform as it builds out its international influence. The role has a fantastic work-life balance in midtown, NYC with their hybrid model to work from home 2-3 times a week. Clear career progression is a key benefit. We cannot sponsor at this time.
We are looking for a Data analyst with experience in (SSRS/SSIS, Power Bi, and Python/R) to join a collaborative work environment with a boutique wealth management firm. You will work with the Data Solutions team increasingly; helping prepare material for meetings and investment board committees. You will also help with new strategies and collaborate with other team members.
Your Background:
· Minimum 4-6 years of experience in IT.
· Experience with Python/R.
· Experience with Report Builder/SSRS
· Experience with DAX
· Experience with SQL Database
· Needs to be a great team player.
· Experience in supporting Financial Service firms.
While not required, it is desirable to have:
· Experience with Python/R
· Experience with Power BI
· Firm understanding of the systems development life cycle.
We have a $1000 referral bonus in place for anyone you refer whom we place. For further information please send your resume to: swadhawan@cyberneticsearch.com
Show more
Show less","Python, R, SSRS, Report Builder, DAX, SQL, Power BI, Financial Services","python, r, ssrs, report builder, dax, sql, power bi, financial services","dax, financial services, powerbi, python, r, report builder, sql, ssrs"
Specification Analyst III (SAP Data Coordination/Data Analysis/Open to Recent Grads),Cube Hub Inc.,"Dubuque, IA",https://www.linkedin.com/jobs/view/specification-analyst-iii-sap-data-coordination-data-analysis-open-to-recent-grads-at-cube-hub-inc-3659954464,2023-12-17,Benton,United States,Mid senior,Onsite,"Office Work 40 hrs/wk, No PPE Required
No driving
Prefer recent Graduate of 4 year degree
Monday - Friday Day hours flexible No Weekends, No Overtime 40 Hours
Duties
As a Specification Analyst, located in Dubuque, IA, you will be an integral part of the engineering product development change process. You will have the following duties:
Complete task based, SAP data coordination and updates for design changes to complex parts and assemblies related to major programs and product improvements by analyzing engineering data and other related data for the Product Delivery Process (PDP)
Use the engineering change process to create, coordinate and maintain complex Bills of Material, Material Masters and part attribute data in SAP and PDMLink Enterprise systems
Audit and process various request forms from the business related to specifications
Use knowledge and experience to serve as liaison and consultant providing feedback that affects specification and decision activities; serves as specification representative for project teams
Complete various specification audits and monitors Engineering changes to ensure data integrity and uniformity
Coordinate implementation decisions of inter-factory decision changes
Facilitate required multi-discipline meetings, records and publishes meeting minutes and works closely with other affected stake holders to establish a consistent pattern for adopting specifications.
Exp in engineer background or data analytics background is helpful
Willingness to learn; SAP exp is beneficial
Show more
Show less","SAP, PDMLink Enterprise, Data coordination, Bills of Material, Material Masters, Part attribute data, Audit, Specification audits, Engineering changes, Data integrity, Multidiscipline meetings, Meeting minutes","sap, pdmlink enterprise, data coordination, bills of material, material masters, part attribute data, audit, specification audits, engineering changes, data integrity, multidiscipline meetings, meeting minutes","audit, bills of material, data coordination, data integrity, engineering changes, material masters, meeting minutes, multidiscipline meetings, part attribute data, pdmlink enterprise, sap, specification audits"
Master Data Analyst,SSM Health,"Jefferson City, MO",https://www.linkedin.com/jobs/view/master-data-analyst-at-ssm-health-3742532785,2023-12-17,Jefferson City,United States,Mid senior,Onsite,"It's more than a career, it's a calling.
MO-REMOTE
Worker Type:
Regular
Job Highlights:
SSM Health — a nationally recognized Catholic, not-for-profit integrated health system serving Illinois, Missouri, Oklahoma, and Wisconsin — is seeking a Master Data Analyst.
To confidentially submit your interest or nominate a fellow colleague, please contact:
Donald R. Schlag
Executive and Professional Recruiter
Donald.Schlag@SSMHealth.com
Job Summary:
Participates on collaborative team to create and maintain master data elements including item file, contract data and price lists, and ensures system data integrity in accordance with guidelines and internal controls.
Job Responsibilities and Requirements:
Primary Responsibilities
Builds and maintains contracts, item files, vendor files, transmissions and pricing in database.
Follows system-wide processes to minimize non-file purchases through reporting and item file management.
Reviews and analyzes materials data for consistency and accuracy. Investigates, reconciles and determines reason for pricing and other irregularities. Identifies results not meeting standard thresholds.
Investigates pricing discrepancies by reviewing contracts and initiating credit requests with vendors.
Serves as key member of team to coordinate data elements that integrate with clinical and financial information systems.
Provides support for process owners, health ministries and their associates; answers questions, resolves issues or errors, and escalates issues that cannot be resolved.
Collaborates with leadership to improve process efficiency.
Performs other duties as assigned.
EDUCATION
Bachelor's degree or equivalent years of experience or education
Experience
Five years' experience
Department:
8725030033 Purchasing
Work Shift:
Day Shift (United States of America)
Scheduled Weekly Hours:
40
SSM Health is an equal opportunity employer. SSM Health does not discriminate on the basis of race, color, religion, national origin, age, disability, sex, sexual orientation, gender identity, pregnancy, veteran status
,
or any other characteristic protected by applicable law. Click here to learn more.
Show more
Show less","Data Analysis, Data Management, Database Management, Contract Management, Pricing Analysis, Data Integrity, Data Quality, System Integration, Clinical Information Systems, Financial Information Systems, Process Improvement, Collaboration, Leadership","data analysis, data management, database management, contract management, pricing analysis, data integrity, data quality, system integration, clinical information systems, financial information systems, process improvement, collaboration, leadership","clinical information systems, collaboration, contract management, data integrity, data management, data quality, dataanalytics, database management, financial information systems, leadership, pricing analysis, process improvement, system integration"
Master Data Analyst,SSM Health,"Jefferson City, MO",https://www.linkedin.com/jobs/view/master-data-analyst-at-ssm-health-3742536117,2023-12-17,Jefferson City,United States,Mid senior,Onsite,"It's more than a career, it's a calling.
MO-REMOTE
Worker Type:
Regular
Job Highlights:
SSM Health — a nationally recognized Catholic, not-for-profit integrated health system serving Illinois, Missouri, Oklahoma, and Wisconsin — is seeking a Master Data Analyst.
To confidentially submit your interest or nominate a fellow colleague, please contact:
Donald R. Schlag
Executive and Professional Recruiter
Donald.Schlag@SSMHealth.com
Job Summary:
Participates on collaborative team to create and maintain master data elements including item file, contract data and price lists, and ensures system data integrity in accordance with guidelines and internal controls.
Job Responsibilities and Requirements:
Primary Responsibilities
Builds and maintains contracts, item files, vendor files, transmissions and pricing in database.
Follows system-wide processes to minimize non-file purchases through reporting and item file management.
Reviews and analyzes materials data for consistency and accuracy. Investigates, reconciles and determines reason for pricing and other irregularities. Identifies results not meeting standard thresholds.
Investigates pricing discrepancies by reviewing contracts and initiating credit requests with vendors.
Serves as key member of team to coordinate data elements that integrate with clinical and financial information systems.
Provides support for process owners, health ministries and their associates; answers questions, resolves issues or errors, and escalates issues that cannot be resolved.
Collaborates with leadership to improve process efficiency.
Performs other duties as assigned.
EDUCATION
Bachelor's degree or equivalent years of experience or education
Experience
Five years' experience
Department:
8725030033 Purchasing
Work Shift:
Day Shift (United States of America)
Scheduled Weekly Hours:
40
SSM Health is an equal opportunity employer. SSM Health does not discriminate on the basis of race, color, religion, national origin, age, disability, sex, sexual orientation, gender identity, pregnancy, veteran status
,
or any other characteristic protected by applicable law. Click here to learn more.
Show more
Show less","Data Management, Data Analysis, Data Integrity, Contract Management, Item Management, Vendor Management, Pricing Analysis, Database Management, Reporting, Data Entry, Data Validation, Data Reconciliation, Troubleshooting, Issue Resolution, Process Improvement, Collaboration, Communication","data management, data analysis, data integrity, contract management, item management, vendor management, pricing analysis, database management, reporting, data entry, data validation, data reconciliation, troubleshooting, issue resolution, process improvement, collaboration, communication","collaboration, communication, contract management, data entry, data integrity, data management, data reconciliation, data validation, dataanalytics, database management, issue resolution, item management, pricing analysis, process improvement, reporting, troubleshooting, vendor management"
Associate Research/Data Analyst - 5034196,State of Missouri,"Jefferson City, MO",https://www.linkedin.com/jobs/view/associate-research-data-analyst-5034196-at-state-of-missouri-3781637863,2023-12-17,Jefferson City,United States,Mid senior,Hybrid,"Associate Research/Data Analyst
Department of Revenue – Taxation Division – Income Tax Bureau
Annual Salary: $ 45,006.24
Location: 301 West High Street, Jefferson City, MO
DOR’s Vision: To provide every customer the best experience every time.
How This Position Supports The Department’s Vision
This position is the Department of Revenue’s Federal and State Coordinator and the liaison with the Internal Revenue Service and state tax agencies. The main responsibility of this position is to ensure federal tax information that the Department of Revenue receives from the Internal Revenue Service is safeguarded to the requirements in IRS Publication 1075. Create procedures and conduct educational sessions on IRS Pub 1075 requirements and on other Income Tax Bureau processes and functions. Advise Department of Revenue team members on IRS Pub 1075 requirements as needed. Analyze tax data. Work with other Departments to create agreements between the Department of Revenue and other agencies.
This position requires the candidate to work independently, have good organizational, written and verbal communication skills, and be able to prioritize duties and adjust as priorities change.
Duties Performed To Support The Department’s Vision
Self-Directed
Attention to Detail
Organized
Effective Writing
Professional
Excellent Time Management
Safeguarding Federal Tax Information
Communication between the Internal Revenue Service and the Missouri Department of Revenue
Core Compentencies Needed
Computer Literacy Effective Writing Excellent Customer Service
Self-directed Attention to Detail Analytical Thinking
Clear Communication Organizational Abilities
Qualifications
Knowledge of basic research methods and analysis, computer information systems, and statistical software.
Ability to perform basic queries and analysis of data.
Bachelor’s degree and 0-2 years of relevant experience and/or appropriate certification. (Substitutions may be allowed.)
More Reasons To Love This Position
The State of Missouri offers an excellent benefits package that includes a defined pension plan, generous amounts of leave and holiday time, and eligibility for health insurance coverage. Your total compensation is more than the dollars you receive in your paycheck. To help demonstrate the value of working for the State of Missouri, we have created an interactive Total Compensation Calculator. This tool provides a comprehensive view of benefits and more that are offered to prospective employees. The Total Compensation Calculator and other applicant resources can be found here .
Please Direct Any Questions About This Position To
The Missouri Department of Revenue Human Resources and Total Rewards office at (573) 751-1291.
We celebrate diversity and are committed to creating an inclusive environment for all employees
The State of Missouri is an equal opportunity employer.
Show more
Show less","Computer Literacy, Organizing, Statistical Software, Queries, Data Analysis, Analytical Thinking, Information Systems, Computer Programming","computer literacy, organizing, statistical software, queries, data analysis, analytical thinking, information systems, computer programming","analytical thinking, computer literacy, computer programming, dataanalytics, information systems, organizing, queries, statistical software"
Medicaid Birth Data Analyst,Oklahoma State Department of Health,"Oklahoma County, OK",https://www.linkedin.com/jobs/view/medicaid-birth-data-analyst-at-oklahoma-state-department-of-health-3746812612,2023-12-17,Arcadia,United States,Associate,Onsite,"Job Posting Title
Medicaid Birth Data Analyst
Agency
340 OKLAHOMA STATE DEPARTMENT OF HEALTH
Supervisory Organization
340 Maternal & Child Health Serv
Job Posting End Date (Continuous if Blank)
Note: Applications will be accepted until 11:59 PM on the day prior to the posting end date above.
Estimated Appointment End Date (Continuous if Blank)
Full/Part-Time
Full time
Job Type
Regular
Compensation
The annual salary is based on education and experience.
Level I: Up to $61,600.00
Level II: Up to $72,600.00
Level III: Up to $83,600.00
Job Description
Medicaid Birth Data Analyst
Location: 123 Robert S Kerr, Central office
Salary: $61,600 - $83,600
Full Time /Part Time: Full Time
Work Schedule: Monday-Friday
Primary Hours: 8:00 am - 5:00 pm
Why you’ll love it here!
RESPECT. COLLABORATION. SERVICE. The Oklahoma State Department of Health (OSDH) is committed to leading Oklahoma to prosperity through health. Our mission is to protect and promote health, prevent disease and injury, and cultivate conditions by which Oklahomans can thrive. Check out why we are passionate about public health and believe it is the career for you!!! What is Public Health?
Oh yeah, did we mention perks? We know that benefits matter and that is why we offer a competitive benefits package for all eligible employees.
Generous state paid benefit allowance to help cover insurance premiums.
A wide choice of insurance plans with no pre-existing condition exclusions or limitations.
Flexible spending accounts for health care expenses and/or dependent care.
Retirement Savings Plan with a generous match.
15 days of vacation and 15 days of sick leave the first year for full time employees.
11 paid holidays a year.
Student Loan repayment options & tuition reimbursement.
Employee discounts with a variety of companies and venders.
Longevity Bonus for years of service
Position Description: This position has responsibility for linking and matching Oklahoma Health Care Authority (OHCA) Medicaid records with Oklahoma State Department of Health (OSDH) vital records and program service records. The Medicaid Data Analyst will assess the processes and linking algorithms to assure that methodologies used are valid, replicable, and yield linked results of high caliber.
Duties
Coordinating the data linkages of OHCA Medicaid records with OSDH vital statistics and program services data.
Using computer software and matching guidelines to conduct probabilistic and deterministic matching of records from datasets that do not have common identification fields.
Perform analyses of linked Medicaid/birth data to support planning and evaluation of Maternal and Child Health programs, OHCA service delivery, and other OSDH/OHCA program needs.
Conduct descriptive and complex statistical analyses of MCH-related data.
Collect, manage, analyze, interpret, and display data using visualization tools.
Prepare analytic results to be used in agency and program area planning, including written reports and publications.
Assure data integrity through the course of its usage to include collection, storage, editing, analysis, and reporting.
Support the completion of the MCH Title V Block Grant Annual Report, Grant Application, and the Title V 5-Year Needs Assessment.
Design data collection, assessment, and analytic tools that comply with acceptable statistical and epidemiologic practices.
Present written and oral reports of analyses, assuring that presentations are suitable for diverse audiences.
Minimum Qualifications
Level I: master's degree in public health, biological, medical, or health science which includes 6 semester hours in epidemiological methods and 6 semester hours of other epidemiological coursework and 9 semester hours in statistical methods and analysis.
Level II: master’s degree in biostatistics, epidemiology, or other closely related field. One year experience preferred. Working experience with SAS® statistical software essential. Experience with complex data file manipulation and matching techniques, including previous experience in linking large datasets, is preferred. Experience with Business Objects software is preferred.
Level III: Requirements consist of those identified in Level II and three years of experience in a field involving the use of epidemiological techniques and analysis; or a doctorate in epidemiology and one year of qualifying experience.
Preferred Qualifications: Use SAS® statistical software and other software matching programs to perform deterministic and probabilistic linking. Ability to analyze data using basic statistical and probability concepts, including expected values and confidence intervals. Thoroughness and attention to accuracy is fundamental. Strong communication and interpersonal skills. Ability to complete linkages and deliver results in a timely manner.
There are two additional requirements. The Oklahoma State Department of Health is tobacco free. Additionally, we consider every employee to be a public health employee; therefore, this position requires possible response to public health emergencies including, but not limited to, natural disasters, disease outbreaks, or catastrophic events.
Physical Demands And Work Environment
This position works at a personal computer up to 90% of the workday in a home or open office environment with noise, distractions, and interruptions. Required to be self-directed and manage multiple and often competing priorities. The incumbent must maintain a high level of confidentiality. Occasional travel required for meeting with stakeholders, vendors, or offsite personnel/management. Out-of-state travel is unlikely.
FOR ADDITIONAL INFORMATION ABOUT WORKING AT THE OKLAHOMA STATE DEPARTMENT OF HEALTH CLICK HERE
The current URL is: https://www.ok.gov/health/Careers_HR/Career_Opportunities/index.html
The correct URL is: https://oklahoma.gov/health/about-us/careers.html
Telework: Telework and is subject to OSDH policy and supervisor’s discretion.
Application Requirements
If education, certification or licensure is required to meet qualifications, applicants must provide documentation by the time of interview.
All applicants are subject to a background check and must be legally authorized to work in the United States without visa sponsorship.
Equal Opportunity Employment
The State of Oklahoma is an equal opportunity employer and does not discriminate on the basis of genetic information, race, religion, color, sex, age, national origin, or disability.
Current State of Oklahoma employees must apply for open positions through their Workday account. Go to Careers app on WD home screen>Click on 'Find Jobs-Internal State of Oklahoma'.
Show more
Show less","Public Health, SAS, Statistical Analysis, Epidemiological Methods, Data Linkage, Data Matching, Data Management, Data Analysis, Data Visualization, Report Writing, Data Interpretation, Statistical Software, Probabilistic Matching, Deterministic Matching, Business Objects, Statistical Concepts, Accuracy, Communication Skills, Interpersonal Skills, Multitasking, Time Management, Confidentiality, Telework","public health, sas, statistical analysis, epidemiological methods, data linkage, data matching, data management, data analysis, data visualization, report writing, data interpretation, statistical software, probabilistic matching, deterministic matching, business objects, statistical concepts, accuracy, communication skills, interpersonal skills, multitasking, time management, confidentiality, telework","accuracy, business objects, communication skills, confidentiality, data interpretation, data linkage, data management, data matching, dataanalytics, deterministic matching, epidemiological methods, interpersonal skills, multitasking, probabilistic matching, public health, report writing, sas, statistical analysis, statistical concepts, statistical software, telework, time management, visualization"
Geospatial Health Data Analyst,Oklahoma State Department of Health,"Oklahoma County, OK",https://www.linkedin.com/jobs/view/geospatial-health-data-analyst-at-oklahoma-state-department-of-health-3759325661,2023-12-17,Arcadia,United States,Associate,Onsite,"Job Posting Title
Geospatial Health Data Analyst
Agency
340 OKLAHOMA STATE DEPARTMENT OF HEALTH
Supervisory Organization
340 Center for Health Statistics
Job Posting End Date (Continuous if Blank)
Note: Applications will be accepted until 11:59 PM on the day prior to the posting end date above.
Estimated Appointment End Date (Continuous if Blank)
Full/Part-Time
Full time
Job Type
Regular
Compensation
The annual salary is up to $83,600.00 based on education and experience.
Job Description
Location: 123 Robert S Kerr OKC, OK 73012
Salary: $83,600
Full Time /Part Time: Full Time
Work Schedule: Monday-Friday
Primary Hours: 8:00 am - 5:00 pm
Why you’ll love it here!
RESPECT. COLLABORATION. SERVICE. The Oklahoma State Department of Health (OSDH) is committed to leading Oklahoma to prosperity through health. Our mission is to protect and promote health, prevent disease and injury, and cultivate conditions by which Oklahomans can thrive. Check out why were are passionate about public health and believe it is the career for you!!! What is Public Health?
Oh yeah, did we mention perks? We know that benefits matter and that is why we offer a competitive benefits package for all eligible employees.
Generous state paid benefit allowance to help cover insurance premiums.
A wide choice of insurance plans with no pre-existing condition exclusions or limitations.
Flexible spending accounts for health care expenses and/or dependent care.
Retirement Savings Plan with a generous match.
15 days of vacation and 15 days of sick leave the first year for full time employees.
11 paid holidays a year.
Student Loan repayment options & tuition reimbursement.
Employee discounts with a variety of companies and venders.
Longevity Bonus for years of service
Position Description
The Geospatial Health Data Analyst will primarily be responsible for conducting spatial analyses such as: disease mapping, geographic correlation studies and disease clustering/surveillance. This position would interpret, evaluate and design public health spatial research projects in collaboration with internal and external partners meeting geospatial data needs. The Geospatial Data Analyst will also conduct small-area studies using data sources (BRFSS, Cancer Registry, Vital Statistics, and Hospital Discharge) within Center for Health Statistics, providing stakeholders data visualization on Oklahoma high-need areas. He/she will demonstrate a high proficiency with ArcGIS suite acting as Oklahoma State Department of Health (OSDH) subject matter expert. The position will also be responsible for the development and maintenance of the OSDH geodatabases
Duties
Duties include, but are not limited to:
Analyze public health and geographical data to aid understanding of complex relationships between environment and health.
Conduct spatial analyses with quality data using ArcGIS software, display data using maps, reports, charts and tables.
Produce data visualizations of spatiotemporal and health services data.
Investigate the environmental, social, and behavioral factors underlying geographic variations in disease rates.
Complete individual GIS mapping projects as required and provide technical support for GIS projects created by end-users at OSDH
Assess, coordinate, and determine GIS needs throughout the agency.
Conduct trainings and resources on GIS to build epidemiologic capacity in mapping and spatial analyses.
Provide technical advice and expertise for the OSDH geospatial database environment including the development and organization of databases, and assessment and implementation of related new technologies.
Develop standards, policies and procedures for ArcGIS use.
Represent OSDH on external committees, work groups, and contribute spatial epidemiologic data to statewide clearinghouse.
Other Duties
Demonstrates knowledge of and supports mission, vision, value statements, standards, policies and procedures, operating instructions, confidentiality standards, and the code of ethical behavior.
Works effectively in team environment, participating and assisting their peers.
Minimum Qualifications: (MUST upload transcripts with application)
Requirements consist of a Master’s degree in Public health (Biostatistics/Epidemiology) or related Master’s level degree and three years of experience in a field involving the use of epidemiological techniques and analysis; or a doctorate in epidemiology and one year of qualifying experience; or 10 years of directly applicable experience. Proficient in ArcGIS software.
Valued Knowledge, Skills And Abilities
Knowledge of epidemiology methods, environmental analyses interpretation; of statistical analysis, and demography; of epidemiological or statistical software; of social and economic conditions; of current epidemiological developments and techniques; of grant writing. Ability to establish and maintain effective working relationships with others; to meet with the public and conduct public information programs; to write technical and non-technical information material; to use a personal computer and applicable software; to manipulate large databases; and to develop and implement intervention and prevention strategies as identified through data analysis.
Physical Demands And Work Environment
This position is set in an office environment and computer based. While performing the duties of the job, employees are frequently required to stand, walk, lift, and reach. Applicants must be willing to perform all job-related travel associated with this position.
Telework: This position is Hybrid and is subject to OSDH policy and supervisor’s discretion.
Application Requirements
If education, certification or licensure is required to meet qualifications, applicants must provide documentation at the time of application .
All applicants are subject to a background check and must be legally authorized to work in the United States without visa sponsorship.
Equal Opportunity Employment
The State of Oklahoma is an equal opportunity employer and does not discriminate on the basis of genetic information, race, religion, color, sex, age, national origin, or disability.
Current State of Oklahoma employees must apply for open positions through their Workday account. Go to Careers app on WD home screen>Click on 'Find Jobs-Internal State of Oklahoma'.
Show more
Show less","ArcGIS, Visual analysis, Data visualization, Programming, Epidemiological analysis, Mapping, Spatial analysis, Statistical analysis, Research, Collaboration, Reporting, GIS, Database administration, Risk assessment, Public health, Data interpretation, Environmental health","arcgis, visual analysis, data visualization, programming, epidemiological analysis, mapping, spatial analysis, statistical analysis, research, collaboration, reporting, gis, database administration, risk assessment, public health, data interpretation, environmental health","arcgis, collaboration, data interpretation, database administration, environmental health, epidemiological analysis, gis, mapping, programming, public health, reporting, research, risk assessment, spatial analysis, statistical analysis, visual analysis, visualization"
Sr. Business Intelligence Data Analyst,MidFirst Bank,"Oklahoma City, OK",https://www.linkedin.com/jobs/view/sr-business-intelligence-data-analyst-at-midfirst-bank-3734880656,2023-12-17,Arcadia,United States,Mid senior,Onsite,"The Business Intelligence Analyst will collaborate with internal departments to provide valuable reporting and analysis to support strategic initiatives and help determine the future reporting direction of the company. The position is responsible for multiple stages of the business intelligence cycle: understanding business objectives, collecting business requirements, translating business requirements into data requirements, mapping data assets, and designing reports or dashboards.
Collaborate with management to understand business unit needs and develop solutions
Develops and maintains a thorough understanding of multiple business processes
Works with subject matter experts within various departments of the business to understand processes, define requirements for data integration, and develop business intelligence solutions
Translates business requirements into technical requirements, and helps identify and develop business rules to be applied against the data
Identifies source systems and data required in the data warehouse environment to address the business solution
Develops the logical solution design, identifying data gaps and quality issues, from specifications identified with the subject matter experts
Identifies the key facts and dimensions necessary in partnership with technical experts to support the business
Model data for BI migration/integration projects
Map source data to destinations and document any transformations or calculations
Design and maintain BI reporting criteria to support business requirements
Performs ad hoc analysis to meet internal and external reporting requirements of the business
This position is onsite in the Oklahoma City office not offering hybrid/remote
QUALIFICATIONS:
4-6 years of applicable work experience
This position requires a bachelor's degree in Management Information Systems, Computer Science, Mathematics, or other business related field with a minimum GPA of 3.0 or 6-10 years of commensurate experience in lieu of specific degree required
In-depth knowledge of BI and reporting concepts and analysis techniques
Experience managing large amounts of data from various sources
Knowledge of general principles of data warehouse environments and enterprise BI tools regardless of platform
Excellent analytical, problem solving and decision-making skills
Excellent interpersonal, verbal, and written communication skills, including strong listening skills
Excellent attention to detail
Ability to turn complex concepts into understandable content for non-technical users
Ability to work independently and within a team environment
A strong willingness to learn, adapt, and actively contribute to the success of the organization
#MM
Show more
Show less","Business Intelligence, Data Analysis, Reporting, Data Management, Data Warehousing, Data Integration, Business Process Analysis, Data Requirements, Technical Requirements, Data Modeling, Data Migration, Data Transformation, Data Quality, Data Visualization, Business Analytics, Analytical Skills, Problem Solving, Decision Making, Communication Skills, Interpersonal Skills, Written Communication, Verbal Communication, Listening Skills, Attention to Detail, Complex Concept Simplification, Independent Work, Teamwork, Learning and Adaptability, Organizational Contribution","business intelligence, data analysis, reporting, data management, data warehousing, data integration, business process analysis, data requirements, technical requirements, data modeling, data migration, data transformation, data quality, data visualization, business analytics, analytical skills, problem solving, decision making, communication skills, interpersonal skills, written communication, verbal communication, listening skills, attention to detail, complex concept simplification, independent work, teamwork, learning and adaptability, organizational contribution","analytical skills, attention to detail, business analytics, business intelligence, business process analysis, communication skills, complex concept simplification, data integration, data management, data migration, data quality, data requirements, data transformation, dataanalytics, datamodeling, datawarehouse, decision making, independent work, interpersonal skills, learning and adaptability, listening skills, organizational contribution, problem solving, reporting, teamwork, technical requirements, verbal communication, visualization, written communication"
Senior Data Analyst (Permian),Continental Resources,"Oklahoma City, OK",https://www.linkedin.com/jobs/view/senior-data-analyst-permian-at-continental-resources-3703656730,2023-12-17,Arcadia,United States,Mid senior,Onsite,"Job Summary
The Senior Data Analyst is primarily responsible for leading projects to automate manual processes and produce data driven insights for the team and other stakeholders across the organization. A successful candidate will also actively seek out and generate new and innovative ways to retrieve, visualize and analyze data across a variety of operational, geological and financial data sources.
Serves as a resource within functional area. Guides others in resolving complex issues; may lead functional teams or projects. Advanced knowledge of fundamental theories, principles and practices in the area of specialization. Solves complex problems without precedent and/or structure; takes a broad perspective to identify solutions. Works independently, with guidance in complex situations.
Duties And Responsibilities
Leads large-scale data & analytics projects within team and across the organization
Discovers and troubleshoots data issues across sources and drives issues to resolution
Create highly complex datasets, visualizations, and tools
Mentors and trains junior staff; creates training content and executes across the team, department and company.
Collaborate with varying levels of engineers, geologists, field personnel, and management to effectively deliver datasets and analytical tools per reporting requirements
Own, maintain and enhance critical Spotfire Tools
Implement automated solutions to streamline manual team processes
Perform data capture processes as necessary to support analytics efforts
Personal commitment to inclusion and diversity
Other duties as assigned.
Skills And Competencies
Use SQL to retrieve data from systems, create integrated datasets using CTEs and/or subqueries, update data within systems and create data tables and views within CLR’s data warehouse.
Use Spotfire to enhance existing projects and create new projects from scratch using data from a variety of sources across the enterprise.
Use UiPath, Grooper or Alteryx to implement automated solutions for manual workflows
Understands or is able to learn the data models of Continental’s primary data sources including Aries, WellView, ProCount, SAP, Planning Analytics, eGIS, WLM, IHS and Enverus.
Understands basic data modeling and data integration principles and techniques
Detail oriented with ability to organize and synthesize large amounts of data to generate high-quality, executive-level quality presentations in an efficient manner
Understands E&P operations, drilling, completions, production, reservoir and geoscience data types
Self-starter, with ability to work as part of a team or individually
Advanced written and verbal communication skills
Ability to quickly adapt and exemplify flexibility around process changes and shifting priorities
Advanced skills with data & analytics applications
Advanced knowledge of data modeling and data integration principles and techniques
Advanced programming skills capable of using all available techniques
Action oriented - Taking on new opportunities and tough challenges with a sense of urgency, high energy, and enthusiasm.
Instills trust - Gaining the confidence and trust of others through honesty, integrity, and authenticity.
Ensures accountability - Holding self and others accountable to meet commitments.
Drives results - Consistently achieving results, even under tough circumstances.
Nimble learning - Actively learning through experimentation when tackling new problems, using both successes and failures as learning fodder.
Required Qualifications
High School Diploma or GED
Minimum of eight (8) years’ experience within the Oil and Gas industry working as a Data Analyst, Engineering Tech, Engineering Specialist, Business Analyst, Systems Analyst, IT Analyst or related role
Advanced skills within Spotfire with the ability to use existing projects, inherit and enhance existing projects and create new projects from scratch leveraging multiple data sources.
Advanced skills within SQL with the ability to retrieve data from systems, create integrated datasets using CTEs and/or subqueries, update data within systems and create data tables and views within CLR’s data warehouse.
An acceptable pre-employment background and drug test.
Preferred Qualifications
Bachelor’s Degree from an accredited college or university in Management Information Systems, Data Analytics, Information Science/Management, Economics, Finance, Accounting or related degree.
Intermediate skills developing automated workflows using UiPath, Grooper or Alteryx
Advanced coding skills in R or Python
Physical Requirements And Working Conditions
Requires prolonged sitting, some bending and stooping.
Occasional lifting up to 25 pounds.
Manual dexterity sufficient to operate a computer keyboard and calculator.
Continental Resources, Inc. provides equal employment and affirmative action opportunities to applicants and employees without regard to race, color, religion, sex, age, sexual orientation, gender identity, national origin, protected veteran status, or disability.
atus.
Show more
Show less","SQL, Spotfire, UiPath, Grooper, Alteryx, Aries, WellView, ProCount, SAP, Planning Analytics, eGIS, WLM, IHS, Enverus, Data modeling, Data integration, Programming, R, Python","sql, spotfire, uipath, grooper, alteryx, aries, wellview, procount, sap, planning analytics, egis, wlm, ihs, enverus, data modeling, data integration, programming, r, python","alteryx, aries, data integration, datamodeling, egis, enverus, grooper, ihs, planning analytics, procount, programming, python, r, sap, spotfire, sql, uipath, wellview, wlm"
Sr. Data Analyst-Quality Consultant,CVS Health,"Oklahoma City, OK",https://www.linkedin.com/jobs/view/sr-data-analyst-quality-consultant-at-cvs-health-3742292220,2023-12-17,Arcadia,United States,Mid senior,Onsite,"Bring your heart to CVS Health. Every one of us at CVS Health shares a single, clear purpose: Bringing our heart to every moment of your health. This purpose guides our commitment to deliver enhanced human-centric health care for a rapidly changing world. Anchored in our brand — with heart at its center — our purpose sends a personal message that how we deliver our services is just as important as what we deliver.
Our Heart At Work Behaviors™ support this purpose. We want everyone who works at CVS Health to feel empowered by the role they play in transforming our culture and accelerating our ability to innovate and deliver solutions to make health care more personal, convenient and affordable.
Position Summary
Aetna Better Health of Oklahoma, a CVS Health company, is a trusted health partner in the local Oklahoma communities we serve. We provide a full array of innovative services that enhance overall wellness and improve everyday life for our members. At Aetna Better Health of Oklahoma, we value professional development and career growth. You will work along other colleagues who align on Heart at Work behaviors and bringing your heart to every moment of health. We will support you all the way!
This is a fulltime teleworker opportunity in Oklahoma. Instate travel may be required based on business needs; including travel to the Oklahoma City office.
The Sr. Data Analyst, Quality Consultant works closely with business partners to identify and improve key business processes and improve member experience. Serves as an operations champion through measuring and monitoring the KPIs and effectiveness of operational processes that impact customer satisfaction, cost management, and operational efficiency. You will evaluate reporting and management dashboards, perform data analysis, develop workflows, and offer support for assigned initiatives that impact the delivery of products and services to internal and external customers.
Applies critical and analytical methods and procedures to identify root causes to recommend, assist in implementing and measuring durable solutions.
Ability to understand healthcare data operations and systems
Experience with data manipulation, data visualization, and presentation.
Act as liaison between internal business units to facilitate new business process plans.
Manage multiple initiatives with multiple customer priorities and successfully meet targeted deadlines.
Communication with all levels of management and relevant stakeholders to encourage internal problem solving.
Collaborate with management to create new processes and document them for future use.
Drive process improvements and effectively manage change with demonstrated ability to challenge status quo.
Proactively identify inefficiencies and process improvement opportunities.
Presentation of findings to both small groups and larger audiences.
Performs other duties as assigned.
Required Qualifications
Must reside in Oklahoma.
3+ years of project management skills and experience.
3+ years’ experience working with data, data analytics or data management systems.
Demonstrated ability to facilitate cross-functional process improvement teams.
Ability to manage multiple initiatives with multiple customer priorities and successfully meet targeted deadlines.
Strong capability to communicate with all levels of management and large groups to achieve desired outcomes.
Must possess strong presentation and communication skills, verbal and written.
Ability to proactively identify inefficiencies and process improvement opportunities.
Ability to collaborate with small teams and cross functionally across business units.
2+ years’ experience using personal computer, keyboard navigation, navigating multiple systems and applications; and using MS Office Suite applications (Teams, Outlook, Word, Excel, PowerPoint, SharePoint, etc.).
Must possess reliable transportation and be willing and able to travel in-state up to 10% of the time. Mileage is reimbursed per our company expense reimbursement policy.
Preferred Qualifications
Experience with Quickbase, or PowerBI.
Healthcare or managed care experience.
Medicaid experience.
Knowledge of data management systems.
Bachelor’s degree preferred.
Education
Bachelor’s Degree in health informatics, information technology, computer science, statistics, applied mathematics, or equivalent relevant work experience.
Pay Range
The typical pay range for this role is:
$43,700.00 - $90,000.00
This pay range represents the base hourly rate or base annual full-time salary for all positions in the job grade within which this position falls. The actual base salary offer will depend on a variety of factors including experience, education, geography and other relevant factors. This position is eligible for a CVS Health bonus, commission or short-term incentive program in addition to the base pay range listed above.
In addition to your compensation, enjoy the rewards of an organization that puts our heart into caring for our colleagues and our communities. The Company offers a full range of medical, dental, and vision benefits. Eligible employees may enroll in the Company’s 401(k) retirement savings plan, and an Employee Stock Purchase Plan is also available for eligible employees. The Company provides a fully-paid term life insurance plan to eligible employees, and short-term and long term disability benefits. CVS Health also offers numerous well-being programs, education assistance, free development courses, a CVS store discount, and discount programs with participating partners. As for time off, Company employees enjoy Paid Time Off (“PTO”) or vacation pay, as well as paid holidays throughout the calendar year. Number of paid holidays, sick time and other time off are provided consistent with relevant state law and Companypolicies.
For more detailed information on available benefits, please visitjobs.CVSHealth.com/benefits
CVS Health requires certain colleagues to be fully vaccinated against COVID-19 (including any booster shots if required), where allowable under the law, unless they are approved for a reasonable accommodation based on disability, medical condition, religious belief, or other legally recognized reasons that prevents them from being vaccinated.
You are required to have received at least one COVID-19 shot prior to your first day of employment and to provide proof of your vaccination status or apply for a reasonable accommodation within the first 10 days of your employment. Please note that in some states and roles, you may be required to provide proof of full vaccination or an approved reasonable accommodation before you can begin to actively work.
CVS Health is committed to recruiting, hiring, developing, advancing, and retaining individuals with disabilities. As such, we strive to provide equal access to the benefits and privileges of employment, including the provision of a reasonable accommodation to perform essential job functions. CVS Health can provide a request for a reasonable accommodation, including a qualified interpreter, written information in other formats, translation or other services throughColleagueRelations@CVSHealth.comIf you have a speech or hearing disability, please call 7-1-1 to utilize Telecommunications Relay Services (TRS). We will make every effort to respond to your request within 48 business hours and do everything we can to work towards a solution.
Show more
Show less","Project management, Data analytics, PowerBI, Quickbase, MS Office Suite, Medicaid, Data management systems, Presentation skills, Communication skills, Critical thinking, Analytical methods, Healthcare data operations, Data visualization, KPI analysis, Process improvement, Change management, Collaboration, Problem solving, Time management, Multitasking, Travel","project management, data analytics, powerbi, quickbase, ms office suite, medicaid, data management systems, presentation skills, communication skills, critical thinking, analytical methods, healthcare data operations, data visualization, kpi analysis, process improvement, change management, collaboration, problem solving, time management, multitasking, travel","analytical methods, change management, collaboration, communication skills, critical thinking, data management systems, dataanalytics, healthcare data operations, kpi analysis, medicaid, ms office suite, multitasking, powerbi, presentation skills, problem solving, process improvement, project management, quickbase, time management, travel, visualization"
Senior Data Engineer,BambooHR,"Oklahoma City, OK",https://www.linkedin.com/jobs/view/senior-data-engineer-at-bamboohr-3760295697,2023-12-17,Arcadia,United States,Mid senior,Remote,"About Us
Our mission is simple: we want to set people free to do meaningful work. People love our software—and it turns out that people love working here too. We've been recognized as a ""Best Company to Work For"" and we're proud of our team for creating software that makes an impact in the lives of HR pros and employees all over the world.
What You'll Do
As a Senior Data Engineer on the data platform team, we'll rely on your expertise across multiple disciplines to develop, deploy and support data systems, data pipelines, data lakes, and lakehouses. Your ability to automate, performance tune, and scale the data platform will be key to your success.
Your initial areas of focus will include:
Collaborate with stakeholders to make effective use of core data assets
With Spark and Pyspark libraries, load both streaming and batched data
Engineer lakehouse models to support defined data patterns and use cases
Leverage a combination of tools, engines, libraries, and code to build scalable data pipelines
Work within an IT managed AWS account and VPC to stand up and maintain data platform development, staging, and production environments
Documentation of data pipelines, cloud infrastructure, and standard operating procedures
Express data platform cloud infrastructure, services, and configuration as code
Automate load, scaling, and performance testing of data platform pipelines and infrastructure
Monitor, operate, and optimize data pipelines and distributed applications
Help ensure appropriate data privacy and security
Automate continuous upgrades and testing of data platform infrastructure and services
Build data pipeline unit, integration, quality, and performance tests
Participate in peer code reviews, code approvals, and pull requests
Identify, recommend, and implement opportunities for improvement in efficiency, resilience, scale, security, and performance
What You Need to Get the Job Done (if you don't have all, apply anyway!)
Experience developing, scaling, and tuning data pipelines in Spark with PySpark
Understanding of data lake, lakehouse, and data warehouse systems, and related technologies
Knowledge and understanding of data formats, data patterns, models, and methodologies
Experience storing data objects in hadoop or hadoop like environments such as S3
Demonstrated ability to deploy, configure, secure, performance tune, and scale EMR and Spark
Experience working with streaming technologies such as Kafka and Kinesis
Experience with the administration, configuration, performance tuning, and security of database engines like Snowflake, Databricks, Redshift, Vertica, or Greenplum
Ability to work with cloud infrastructure including resource scaling, S3, RDS, IAM, security groups, AMIs, cloudwatch, cloudtrail, and secrets manager
Understanding of security around cloud infrastructure and data systems
Git-based team coding workflows
Bonus Skills (Not Required, So Apply Anyway!)
Experience deploying and implementing lakehouse technologies such as Hudi, Iceberg, and Delta
Experience with Flink, Presto, Dremio, Databricks, or Kubernetes
Experience with expressing infrastructure as code leveraging tools like Terraform
Experience and understanding of a zero trust security framework
Experience developing CI/CD pipelines for automated testing and code deployment
Experience with QA and test automation
Exposure to visualization tools like Tableau
Beyond the technical skills, we're looking for individuals who are:
Clear communicators with team members and stakeholders
Analytical and perceptive of patterns
Creative in coding
Detail-oriented and persistent
Productive in a dynamic setting
If you love to learn, you'll be in good company. You'll likely have a Bachelor's degree in computer science, information systems, or equivalent working experience.
An Equal Opportunity Employer--M/F/D/V
Because our team members are trusted to handle sensitive information, we require all candidates that receive and accept employment offers to complete a background check before being hired.
For information on our Privacy Policy, click here.
Show more
Show less","Spark, PySpark, Data Lake, Lakehouse, Data Warehouse, Hadoop, S3, EMR, Kafka, Kinesis, Snowflake, Databricks, Redshift, Vertica, Greenplum, AWS, RDS, IAM, Security Groups, AMIs, Cloudwatch, Cloudtrail, Secrets Manager, Git, Terraform, Hudi, Iceberg, Delta, Flink, Presto, Dremio, Kubernetes, Tableau","spark, pyspark, data lake, lakehouse, data warehouse, hadoop, s3, emr, kafka, kinesis, snowflake, databricks, redshift, vertica, greenplum, aws, rds, iam, security groups, amis, cloudwatch, cloudtrail, secrets manager, git, terraform, hudi, iceberg, delta, flink, presto, dremio, kubernetes, tableau","amis, aws, cloudtrail, cloudwatch, data lake, databricks, datawarehouse, delta, dremio, emr, flink, git, greenplum, hadoop, hudi, iam, iceberg, kafka, kinesis, kubernetes, lakehouse, presto, rds, redshift, s3, secrets manager, security groups, snowflake, spark, tableau, terraform, vertica"
Data Analyst,NikSoft Systems Corporation,"Memphis, TN",https://www.linkedin.com/jobs/view/data-analyst-at-niksoft-systems-corporation-3769050150,2023-12-17,West Memphis,United States,Associate,Hybrid,"Job Location:
Memphis, TN (2 days onsite, 3 days remote)
Overview:
NikSoft Systems Corporation is a recognized Information Technology solutions provider. Founded in 1998 and based in Reston, Virginia, NikSoft is a CMMI Level 3 Certified company with an established reputation for excellence and on-time delivery with a consistently high customer satisfaction rating from its Federal Government and private consulting contracts.
NikSoft is currently conducting a search for a
Data Analyst
to add to its team in support of the United States Postal Service. The successful candidate will experience an unparalleled large-scale enterprise environment with over 800 Information Technology systems, supporting billions of dollars in annual revenue, supporting a diverse user base spread across the entire US. Join the NikSoft team to scale up your career to the next level.
Responsibilities:
Perform data analysis for an enterprise big data and analytics platform that utilizes emerging technology to ingest data in real-time at extreme volumes and high velocity; Support data mining, data analysis, and data visualization.
Versatile data analyst - able to support data analysis needs in a software development lifecycle and in a business support capacity to find data insights to solve a business problem.
In a software development lifecycle - Provide inputs into data arch design, analyze data outputs to validate data quality and communicate gaps to the development team; Validate SIT and CAT results to ensure the product meets acceptance criteria; Support production data validation and investigations.
In a business support role - Triage and investigate user inquiries; query and analyze data to identify business insights; Create data visualizations and PowerPoint decks to communicate insights to client management/leadership; Develop canned data visualizations using Qlik.
Required Qualifications:
Bachelor's Degree in IT or a related field.
2-5 years of hands-on experience in data and business analysis, preferably in a large-scale enterprise environment.
Detail-oriented with strong analytical and problem-solving skills
Proven data analysis experience.
Strong knowledge of SQL or other structured query languages
Communications skills (both verbal & written) - Ability to work and communicate with all levels in a team structure
Team player with the ability to prioritize and multi-task, work in a fast-paced environment, and effectively manage time.
Desired Qualifications:
Data Visualization, Qlik experience
Experience with querying and exploring data within a NoSQL data platform (e.g. Hadoop/Hive)
Postal or mailing industry experience
****Candidates must be able to obtain a Postal Sensitive Clearance (US Citizenship or Green Card required). Additionally, candidates must not have traveled outside of the USA for a combined period not to exceed 6 months within the last 5 years.***
Show more
Show less","Data Analysis, Data Mining, Data Visualization, Data Warehousing, Software Development Lifecycle, Business Intelligence, Business Analytics, SQL, NoSQL, Hadoop, Hive, Qlik, PowerPoint, Team Player, Communication Skills, ProblemSolving Skills","data analysis, data mining, data visualization, data warehousing, software development lifecycle, business intelligence, business analytics, sql, nosql, hadoop, hive, qlik, powerpoint, team player, communication skills, problemsolving skills","business analytics, business intelligence, communication skills, data mining, dataanalytics, datawarehouse, hadoop, hive, nosql, powerpoint, problemsolving skills, qlik, software development lifecycle, sql, team player, visualization"
Data Scientist,"Inventory Locator Service,® LLC",Memphis Metropolitan Area,https://www.linkedin.com/jobs/view/data-scientist-at-inventory-locator-service-%C2%AE-llc-3778591618,2023-12-17,West Memphis,United States,Associate,Hybrid,"CAMP Systems is the leading provider of aircraft compliance and health management services to the global business aviation industry. CAMP is the pre-eminent brand in its industry and is the exclusive recommended service provider for nearly all business aircraft manufacturers in the world. Our services are delivered through a “SaaS plus” model and we support over 20,000 aircraft on our maintenance tracking platform and over 31,000 engines on our engine health monitoring platform. Additionally, CAMP provides shop floor management ERP systems to over 1,300 aircraft maintenance facilities and parts suppliers around the world. CAMP has grown from a single location company in 2001, to over 1,300 employees in 13 locations around the world.
Inventory Locator Service (ILS), a division of CAMP has helped customers by collecting data about parts available in the marketplace and organizing them into one user-friendly database. The new and used parts locator service developed by ILS has helped numerous customers in the aviation, marine, and defense sectors find the parts they need, streamline procurement, sell their parts inventory, improve their MRO services, and automate their supply chain operations.
CAMP’s relationships with business aircraft manufacturers, aircraft maintenance facilities, and parts suppliers place it in a unique position to understand how current offline information flows in the business aviation industry to introduce friction to the global market for business aviation parts and services. CAMP is building a digital business that will streamline the exchange of parts and services and create substantial value for both CAMP and the aviation industry at large.
CAMP is an exciting company to work for, not only because of its future growth prospects, but also because of its culture. Smart, motivated people, who want to take initiative, are given the opportunity and freedom to make things happen. CAMP is part of the Hearst Business Media portfolio.
Job Summary
As a Data Scientist, you will report to the Manager of Database Services and will primarily participate in planning, designing and implementation of architecture to support Machine Learning (ML) and Statistical modeling to enhance existing intelligence to a varied set of customers as well as improve decision making across the organization and enhance sales. The ideal candidate will have advanced skills and experience in ML and mathematical and statistical analysis, along with a natural curiosity and a creative mind. The candidate will be responsible for collection, cleansing, mining and interpreting data for hidden insights and opportunities to help realize the data’s full potential and transform it into monetized products for the Aviation industry.
Responsibilities
Must have excellent verbal and written communication skills.
Take ownership and follow through with tasks until completion.
Manage relationships and expectations with multiple stakeholders.
Collaborate with cross-functional teams to develop new and enhance existing products and services.
Drive process improvements to optimize solution delivery, operations, and cost efficiencies.
Guiding project teams in area of expertise.
Ensure data architecture and standards meet the current and future needs of the business.
Produce high quality documentation to share with other Data Architects and Database Administrators to ensure good data governance practices.
Research and devise innovative statistical models for data analysis.
Communicate findings to all stakeholders.
Enable smarter business processes by using analytics for meaningful insights.
Keep current with technical and industry developments.
Serve as lead data strategist to identify and integrate new datasets that can be leveraged through our product capabilities, and work closely with the engineering team in the development of data products.
Identify relevant data sources to mine for client/business needs, extending company’s internal data with third party sources where required.
Automate collection process where required.
Devise and utilize algorithms and models to mine big-data stores; perform data and error analysis to improve models; clean and validate data for uniformity and accuracy.
Select features and build predictive models, ensemble models, and ML algorithms.
Perform ad-hoc analysis and present findings using data visualization techniques.
Create and improve automated anomaly detection.
Requirements
3+ years experience developing AI or ML solutions.
3+ years experience with R, SQL and Python or other ML suite.
3+ years experience with Power BI, SQL, and other programming languages.
3+ years of experience with Agile development.
Proficiency with data mining and statistical analysis.
Strong math and statistical analysis skills.
Problem-solving aptitude.
Excellent communication and presentation skills.
Bachelors degree in Computer Science, Engineering, Mathematics, Statistics or relevant field
Preferred
Aviation Industry Experience.
Graduate degree in Data Science or other quantitative field.
Active in local technology user groups.
Experience with structured and unstructured data.
Experience with NoSQL.
Business Intelligence and Analytics Experience.
Linux command line and scripting.
Cloud based database technologies like Snowflake, AWS, or Azure.
Experience with Oracle PL/SQL.
CAMP is committed to creating a diverse environment and is proud to be an affirmative action and equal opportunity employer. We understand the value of diversity and its impact on a high-performance culture. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, disability, age, sexual orientation, gender identity, national origin, veteran status, or genetic information.
CAMP is committed to providing access, equal opportunity, and reasonable accommodation for individuals with disabilities in employment, its services, programs, and activities. To request reasonable accommodation, please contact hr@campsystems.com.
All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, national origin, age, sexual orientation, gender identity, disability or veteran status EOE.
Show more
Show less","Data mining, Machine learning, Statistical analysis, Data architecture, Data visualization, Cloud based database technologies, Apache Airflow, SQL, R, Python, Power BI, BI softwares, Data integration, Data governance, Agile development, NoSQL, Linux command line, Data modeling, Product development, Big data stores, Data collection, Data cleansing, Data interpretation, Oracle PL/SQL, Snowflake, AWS, Azure, Predictive modeling, Data analytics","data mining, machine learning, statistical analysis, data architecture, data visualization, cloud based database technologies, apache airflow, sql, r, python, power bi, bi softwares, data integration, data governance, agile development, nosql, linux command line, data modeling, product development, big data stores, data collection, data cleansing, data interpretation, oracle plsql, snowflake, aws, azure, predictive modeling, data analytics","agile development, apache airflow, aws, azure, bi softwares, big data stores, cloud based database technologies, data architecture, data collection, data governance, data integration, data interpretation, data mining, dataanalytics, datacleaning, datamodeling, linux command line, machine learning, nosql, oracle plsql, powerbi, predictive modeling, product development, python, r, snowflake, sql, statistical analysis, visualization"
Staff Data Engineer,Recruiting from Scratch,"Memphis, TN",https://www.linkedin.com/jobs/view/staff-data-engineer-at-recruiting-from-scratch-3744397324,2023-12-17,West Memphis,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our client is a dating app.
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, deliver analysis to further improve engagement in existing features, and empower our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Solve technical problems of the highest scope and complexity
Exert significant influence on the company’s analytical long-range goals and data architecture
Define and extend our internal standards for style, maintenance, and best practices for a high-scale data platform
Provide mentorship for all on your team to help them grow in their technical responsibilities
Propose ideas to improve the scale, performance, and capabilities of the Data Platform
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We'll Love About You
7+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field
7+ years experience using Python
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention
Location
: This role will be located in office twice a week minimum in Palo Alto, San Francisco or Chicago.
Salary Range: $180,000 - 250,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Python, SQL, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, Kafka, Storm, SparkStreaming, TDD, Pair Programming, Continuous Integration, Automated testing, Deployment, ETL, Big data, Data engineering, Data science, Business intelligence, Data Warehouses, Relational databases, Data governance, Data architecture, Distributed Databases","python, sql, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, kafka, storm, sparkstreaming, tdd, pair programming, continuous integration, automated testing, deployment, etl, big data, data engineering, data science, business intelligence, data warehouses, relational databases, data governance, data architecture, distributed databases","airflow, automated testing, big data, business intelligence, continuous integration, data architecture, data engineering, data governance, data science, data warehouses, deployment, distributed databases, docker, etl, helm, kafka, kubernetes, pair programming, python, relational databases, snowflake, spark, sparkstreaming, sql, storm, tdd"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Memphis, TN",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3748830451,2023-12-17,West Memphis,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our next evolution is underway as a newly public company looking to expand and continue to build meaningful experiences for our users. From social issues to original content, we’re blazing innovative paths with impact for our community, all while leveraging the latest tech stacks and striving for engineering excellence. At the heart of our work in this new chapter is a shared set of core values: openness and exploration, a bias for action, and strong support of the LGBTQ community.
With a track record of strong financial performance and plans for continued headcount growth, we’re looking to build a team of talented, passionate, and open-minded people who believe in our mission, align with our values, and are excited to work at the intersection of innovative technology and social impact. Come be a part of this exciting journey with us.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location: Palo Alto, San Francisco or Chicago
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, SQL, TDD, Pair Programming, Continuous Integration, Kafka, Storm, SparkStreaming, ETL, Data Warehouses, Data Governance, S3","python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, tdd, pair programming, continuous integration, kafka, storm, sparkstreaming, etl, data warehouses, data governance, s3","airflow, continuous integration, data governance, data warehouses, docker, etl, helm, kafka, kubernetes, pair programming, python, s3, snowflake, spark, sparkstreaming, sql, storm, tdd"
Sr Business Data Analyst,DHL Supply Chain,"Memphis, TN",https://www.linkedin.com/jobs/view/sr-business-data-analyst-at-dhl-supply-chain-3733085734,2023-12-17,West Memphis,United States,Mid senior,Onsite,"Are you a passionate leader looking for autonomy and exciting career possibilities?Do you take an energetic and resourceful approach to problem-solving while bringing innovative ideas and analytics to life on behalf of your team and your customers?Do you enjoy effectively translating requirements into an efficient process and/or system solution? If so, DHL Supply Chain has the opportunity for you.
Job Description
To apply knowledge and analytics to develop and communicate timely, accurate, and actionable insight to the business through the use of modeling, visualization, and optimization. Responsible for the reporting, analyzing, and predicting of operational processes, performance, and Key Performance Indicators. Communication with site leadership, operations, and finance on efficiency, customer requirements, account specific issues, and insight into to the business, operations, and customer.
Applies hindsight, insight, and foresight techniques to communicate complex findings and recommendations to influence others to take action
Uses knowledge of business and data structure to discover and/or anticipate problems where data can be used to solve the problem
Uses spreadsheets, databases, and relevant software to provide ongoing analysis of operational activities
Applies data visualization for discovery and timely insights to decrease Cycle Time to Action (CTA)
Assists site operations in identifying areas for improving service levels, reducing operational costs, and providing other operational enhancements
Supports account start-up analysis and/or report implementation as needed
Develop standardized and ad hoc site and/or customer reporting
Streamlines and/or automates internal and external reporting
May investigate and recommend new technologies and information systems
May conduct feasibility analyses on various processes and equipment to increase efficiency of operations
Partners with Finance to develop financial models to analyze productivity and payroll; calculates cost benefits and business impact and proposes solutions
Develops predictive models to help drive decision making
Designs, develops, and implements data gathering and reporting methods and procedures for Operations
Coordinates with Operations Systems group to ensure technical issues and problems are being identified, addressed, and resolved in a timely manner
May coordinate with ILD group on issues related to modeling customer solutions, including providing data and relevant insight for customer pursuits
Responsible for assisting finance and senior leadership in modeling yearly labor budget based on operational and profile changes
Required Education And Experience
Undergraduate degree in business, logistics, mathematics, statistics, related field, or equivalent experience, required
1+ years of analytics experience, required
Our Organization has a business casual environment and focuses on teamwork, associate development, training, and continuous improvement. We offer competitive wages, excellent affordable insurance benefits (including health, dental, vision and life), 401K plan, paid vacation and holidays.
Our Organization is an equal opportunity employer.
Show more
Show less","Analytics, Data visualization, Databases, Spreadsheets, Modeling, Optimization, Financial modeling, Reporting, Automation, Machine learning, Data mining, Statistics, Business intelligence, Data analysis, Data science, Predictive modeling, Process improvement, Costbenefit analysis, Decision making, Data gathering, Data integration, Data management","analytics, data visualization, databases, spreadsheets, modeling, optimization, financial modeling, reporting, automation, machine learning, data mining, statistics, business intelligence, data analysis, data science, predictive modeling, process improvement, costbenefit analysis, decision making, data gathering, data integration, data management","analytics, automation, business intelligence, costbenefit analysis, data gathering, data integration, data management, data mining, data science, dataanalytics, databases, decision making, financial modeling, machine learning, modeling, optimization, predictive modeling, process improvement, reporting, spreadsheets, statistics, visualization"
Data Analyst I,St. Jude Children's Research Hospital,"Memphis, TN",https://www.linkedin.com/jobs/view/data-analyst-i-at-st-jude-children-s-research-hospital-3779268501,2023-12-17,West Memphis,United States,Mid senior,Onsite,"Program Description Summary
A diverse biomedical workforce is essential to providing high quality patient care. Yet, women and people of color remain underrepresented in the biomedical workforce. The STEMM Education and Outreach Program aims to increase the diversity of the biomedical workforce by ensuring that every child has the opportunity to see themselves as a scientist.
Job Description Summary
The St. Jude STEMM Education and Outreach Program is looking for a Data Analyst to join our team! The right candidate will provide critical analysis of both quantitative and qualitative STEMM education and outreach data from various data sources in support of programmatic decisions and design and maintain longitudinal datasets. This person will bring significant value
to both the technical and non-technical sides of the program by employing quantitative methods in consistent ways with the tenants of critical social theories to make sense of our data. Specifically they will develop high quality, impactful dashboards using charts, graphs, drilldown, navigation and other data visualization techniques to help us understand the impacts of our
programs and how this relates to STEMM education. You will also be responsible for the day-today design, development, monitoring, documentation and support of all program outcomes as well as mentoring and guiding junior staff members. If this sounds like the role for you, apply today!
The World’s Best Stop at Nothing
There’s a reason St. Jude Children’s Research Hospital is consistently ranked on Fortune Magazine’s “100 Best Places to Work For” list. Actually, there are more than 3,600 reasons. At our world-class pediatric research hospital, every one of our 3,600+ professionals shares our commitment to making a difference in the lives of the children we serve. There’s a unique bond when you’re part of a team that will stop at nothing to advance the treatments and cures of pediatric catastrophic diseases. The result is a collaborative, positive environment where all employees, regardless of their role, receive the resources, support, and encouragement to advance and grow their careers.
Better at Work. Better at Life.
When you work at St. Jude, you’ll join a highly-collaborative work culture that inspires you every day to be your best. With opportunities for learning and growth, you can shape a career path that is right for you while also enjoying all the benefits and stability of working for a world-class institution. This includes work-life balance with generous paid time-off and on-campus conveniences that make life a little easier. Join us and you’ll quickly see why St. Jude is consistently ranked by our employees on Glassdoor as a “Best Place to Work.”
Have More in Memphis
Get the best of big-city amenities mixed with all the charm of a small-town feel when you live in Memphis. From our world-renowned music scene to our eclectic mix of food and people, Memphis is a place our employees are proud to call home. Our region enjoys a cost of living more than 20% lower than the national average, and our state places no personal income tax on wages. Plus, Memphians spend five hours less per year in their daily commute compared to the national average. Learn how you can ""Have More in Memphis.""
Job Responsibilities:
Assist in/execute entry-level design and coding of databases/dashboards and/or data validation and loading processes.
Monitor the performance of databases, dashboards, ETL processes, and system data access transactions.
Assist in preparing reports and technical documentation of databases, ETL processes, data management, and enterprise reports/dashboards as applicable.
Communicate effectively with team members, customers, and management, keeping them updated on status as needed.
Proactively develop deeper knowledge of data concepts, processes, and methodologies.
Use data modeling and/or data processing and ETL tools for data load pipelines.
Perform other duties as assigned to meet the goals and objectives of the department and institution.
Maintains regular and predictable attendance.
Minimum Education and/or Training:
Bachelor's degree in computer science, data science, information science, business, or related field required.
Master's degree preferred.
Minimum Experience:
Knowledge of programming languages, databases, and software development lifecycle.
Some experience with enterprise reporting tools for development of reports preferred as applicable.
Some experience working with data loading processes preferred.
St. Jude Children’s Research Hospital has a diverse, global patient population and workforce, built on the principles of diversity, equity and inclusion. Our founder Danny Thomas envisioned a hospital that would treat children of the world—regardless of race, religion or a family’s ability to pay. Learn more about our history and commitment.
Today, we continue the mission to advance cures and means of prevention for pediatric catastrophic diseases through research and treatment. As we accelerate this progress globally, we believe our legacy of diversity, equity and inclusion is foundational to success. With the commitment of leaders at all levels of the organization, we strive to ensure the St. Jude culture, leadership approaches and talent processes are equitable and culturally responsive. View our Diversity, Equity and Inclusion Report to learn about the hospital’s roots in diversity, equity and inclusion, where we are today and our aspirations for an even better future.
Other Information
St. Jude is an Equal Opportunity Employer
No Search Firms
St. Jude Children's Research Hospital does not accept unsolicited assistance from search firms for employment opportunities. Please do not call or email. All resumes submitted by search firms to any employee or other representative at St. Jude via email, the internet or in any form and/or method without a valid written search agreement in place and approved by HR will result in no fee being paid in the event the candidate is hired by St. Jude.
Show more
Show less","Data Analysis, Data Visualization, Data Modeling, Data Processing, ETL Tools, Enterprise Reporting, Programming Languages, Databases, Software Development Lifecycle, Dashboards, Data Validation, Data Loading","data analysis, data visualization, data modeling, data processing, etl tools, enterprise reporting, programming languages, databases, software development lifecycle, dashboards, data validation, data loading","dashboard, data loading, data processing, data validation, dataanalytics, databases, datamodeling, enterprise reporting, etl tools, programming languages, software development lifecycle, visualization"
CRM Data Analyst - Remote,Terminix,"Memphis, TN",https://www.linkedin.com/jobs/view/crm-data-analyst-remote-at-terminix-3763004942,2023-12-17,West Memphis,United States,Mid senior,Remote,"Benefits Start Day 1 for Full-Time Colleagues - No Waiting Period!
Job Summary
In this role you will work together with CRM strategists and a data manager to help produce, test and report upon complex cross-platform digital customer journeys. Your work will help develop new tests and continually work to optimize customer communications, so we ensure we’re giving our customers what they need, when they want, how they want. You are a highly motivated problem solver with a curious mind and exceptional attention to detail and organization. You have extensive experience collecting large sets of data from multiple sources and cleaning for analysis. You are fluent in Excel, Power BI, Tableau, or a similar platform. You are not afraid to ask questions, provide solutions, and you’re quick to learn from your mistakes.
Principal Duties And Responsibilities
Help construct, perform, and analyze testing for pilots, campaigns, and tactics
Report on cross-platform marketing and loyalty initiative performance, pulling results from disparate online platforms
Provide insights to colleagues and leadership questions on an ad hoc, rolling basis
Proficiency in data analysis and reporting using CRM tools or other data analysis software
Effective communication and interpersonal skills to interact with a diverse group of stakeholders
Project management experience with the ability to prioritize tasks and manage deadlines effectively
Make recommendations to improve marketing performance based on the campaign results
Key Relationships
North American managers and colleagues
Functional teams: Marketing & Innovation, IT, Finance, Legal, and M&A, etc.
Key suppliers, partners, and consultants.
Key Performance Indicators
Customer retention rate
Customer Lifetime Value
Deliverability and email metrics
Monthly active users
Repeat purchase rate
Required Experience
3+ years experience in marketing data analysis
A/B testing cohort segmentation
Experience using enterprise-level ESP, like Salesforce Marketing Cloud
Detail-oriented
Highly organized
Previous customer service experience a plus
Power BI or Google Analytics experience a plus
Strong collaboration and interpersonal skills to work with teams from Product, IT, Data Architecture, and Finance
Required Leadership Traits And Characteristics
Strives towards making a positive impact on society and the environment across the Marketing spectrum, establishing Rentokil Terminix as a leader in this space
Able to demonstrate high levels of drive, work ethic and personal accountability with the ability to work under pressure while maintaining sound judgement and a rigorous focus on the details.
Formal Education, Qualifications Or Training
3 years of hands-on marekting analytics experience
Why Choose Rentokil ?
A career with Rentokil can be a professional trajectory filled with opportunity. We pride ourselves on being a world-class team that rewards high performance, and we love to promote from within. We offer competitive pay and many of our roles offer performance incentives.
Below you'll find information about some of what Rentokil has to offer. All Full-Time Colleagues qualify for the following and Part-Time Colleagues qualify for most benefits after they meet certain criteria.
Click here to read more about our Total Rewards Program which includes:
Professional and Personal Growth
Multiple avenues to grow your career
Rentokil Terminix is a Drug Free workplace
Training and development programs available
Tuition Reimbursement benefits (for FT Colleagues)
Health and Wellness
Full-time colleagues are eligible to begin enrollment immediately upon hire with benefits starting on day 1
Health benefits including Medical, Dental, Vision, Disability, and Life Insurance plus much more
Savings and Retirement
401(k) retirement plan with company-matching contributions
Work-Life Balance
Vacation days & sick days
Company-paid holidays & floating holidays
A company mindset that prioritizes health, safety, and flexibility
We are looking for individuals who want to make a difference where our customers live and work.  Is that you?
Our companies are proud to be Affirmative Action (AA) and Equal Opportunity Employers (EOE) inclusive of veterans and those with disabilities.
California residents click here to review your privacy rights.
By applying to this position, you consent to receive an initial text message to collect your communication preferences. Message and data rates may apply. You can opt-out any time.
Show more
Show less","Salesforce Marketing Cloud, Tableau, Power BI, CRM, Excel, Google Analytics, A/B testing, Project management, Data analysis, Data collection, Data cleaning, Communication, Collaboration, Organization, Attention to detail, Problem solving","salesforce marketing cloud, tableau, power bi, crm, excel, google analytics, ab testing, project management, data analysis, data collection, data cleaning, communication, collaboration, organization, attention to detail, problem solving","ab testing, attention to detail, collaboration, communication, crm, data cleaning, data collection, dataanalytics, excel, google analytics, organization, powerbi, problem solving, project management, salesforce marketing cloud, tableau"
Data Engineer,"IDR, Inc.","Memphis, TN",https://www.linkedin.com/jobs/view/data-engineer-at-idr-inc-3776253741,2023-12-17,West Memphis,United States,Mid senior,Hybrid,"IDR is seeking a
Data Engineer
to join one of our top clients in Memphis, TN! If you are looking for an opportunity to join a large organization and excited about marketing tech, please apply today!
Position Overview For The Data Engineer
Responsible for working with large data sets and developing data pipelines that move data from source systems to segmentation systems, advertising platforms, data warehouses, data lakes, and other data storage and processing systems. The data engineer will prepare data for synthesis and analysis by CDP and other marketing tech systems.
Required Skills For The Data Engineer
Solid programming skills (J2EE, Python)
Large Data Set Experience / Big Data Exposure
Understanding of Salesforce & Adobe Marketing Stacks
What’s in it for you?
Competitive compensation package
Full Benefits; Medical, Vision, Dental, and more!
Opportunity to get in with an industry leading organization
Close-knit and team-oriented culture
Why IDR?
20+ Years of Proven Industry Experience in 4 major markets
Employee Stock Ownership Program
Dedicated Engagement Manager who is committed to you and your success
Medical, Dental, Vision, and Life Insurance
ClearlyRated’s Best of Staffing® Client and Talent Award winner 10 years in a row
Show more
Show less","J2EE, Python, Big Data, Salesforce, Adobe Marketing Stacks, CDP, Data Lakes, Data Warehouses, Data Pipelines, Data Storage, Data Processing","j2ee, python, big data, salesforce, adobe marketing stacks, cdp, data lakes, data warehouses, data pipelines, data storage, data processing","adobe marketing stacks, big data, cdp, data lakes, data processing, data storage, data warehouses, datapipeline, j2ee, python, salesforce"
Senior Cloud Data Engineer,BDO USA,"Memphis, TN",https://www.linkedin.com/jobs/view/senior-cloud-data-engineer-at-bdo-usa-3765466994,2023-12-17,West Memphis,United States,Mid senior,Hybrid,"Job Description
Job Summary:
This position will work with cutting edge technology, deliver high quality solutions across various industries, and oversee team(s) on engagements that range in size and scope. This position will receive continuous career development opportunities, given the size and potential of client engagements. This role will perform hands-on delivery of data analytics projects, contributing to the development and unit testing of solutions.
Job Duties
Designs and implements best in class data ingestion strategies, data warehouse and data mart structures, semantic layers and models, visualizations, streaming processes, API integrations, and automation (RPA) solutions for end-to-end data analytics solutions on primarily, but not limited to, cloud analytics platforms such as Azure and AWS
Listens to client needs to align solution with business requirements and delivery schedule
Creates written functional and technical designs
Participates in project status and stand meetings, and assists with providing aggregated project status for project and program managers
Assists with SLA compliance of solutions, and performs performance tuning and optimization efforts of end-to-end solutions
Writes code using multiple languages and correctly applies frameworks, architectural patterns, and software development principles
Delivers high-performance, scalable, repeatable, and secure deliverables with broad impact (high throughput and low latency)
Assists with implementation of data governance programs and best practices
Performs the cleaning and transforming of data from source systems into analytics models
Implements models to support data visualizations and integrations
Assists with implementing DevOps, DataOps and MLOps methodologies on projects
Writes custom integration logic in applicable programming languages
Assists project managers with work breakdown structure creation, project estimation, resource staffing, workload planning and adjustments throughout the project lifecycle
Assists clients with licensing, security, and cost estimation of solutions
Performs code reviews to ensure adherence to standards
Works directly with clients and team members to establish secure data analytics platforms and infrastructure
Contributes to successful deployments of developed solutions and integration of DevOps tools
Maintains a broad and current understanding of data analytics and business intelligence strategies, cloud platforms, methodologies, and tools
Builds client relationships during project execution, effectively becoming a trusted advisor of the client
Participates in support activities for existing software solutions
Other duties as assigned
Supervisory Responsibilities
Supervises the day-to-day workload of Associates on assigned engagements to ensure that timelines and deliverables are met, and reviews work product
Education
Qualifications, Knowledge, Skills and Abilities:
High School Diploma or GED equivalent, required
Bachelor’s degree, preferred; focus in Information Systems, Data Science or Computer Science, preferred
Experience
Five (5) or more years of experience within Data Analytics, Business Intelligence, Artificial Intelligence, or Application Development, required
One (1) or more years of experience technically leading development projects, preferred
One (1) or more years of consulting experience or implementation of cloud-based data analytics solutions, preferred
Software
Strong SQL skills including Data Definition Language (DDL), Data Manipulation Language (DML), views, functions, stored procedures, or performance tuning, required
Experience with Data Warehousing, Data Modeling, Semantic Model Definition or Star Schema Construction, required
Hands on delivery experience of end-to-end cloud data analytics solutions within Azure or AWS, preferred
Experience with one (1) or more of the following computer languages, preferred:
C#
Python
Java
Scala
Experience with tabular modeling within Microsoft Fabric, Power BI, or Azure Analysis Services, preferred
Experience with Git and DevOps deployment technologies, preferred
Experience with Linux, preferred
Experience with one (1) or more of the following, preferred:
Data Lake Medallion Architecture
Batch and/or streaming data ingestion into a data lake
AI Algorithms/Machine Learning
Automation tools such as UiPath, Alteryx, etc.
Computer Vision based AI technologies
Other Knowledge, Skills & Abilities
Ability to work with a high degree of professionalism and autonomy
Excellent verbal and written communication skills
Solid organizational skills, especially the ability to meet project deadlines with a focus on details
Ability to successfully multi-task while working independently or within a group environment
Ability to work in a deadline-driven environment, and handle multiple projects simultaneously
Ability to interact effectively with people at all organizational levels of the Firm
Ability to effectively interact with a team of professionals and delegating work assignments, as needed
Ability to build and maintain strong relationships with internal and client personnel
Ability to encourage a team environment on engagements, and contribute to the professional development of assigned personnel
Keywords:
Data Analytics, Business Intelligence, BI, Synapse, IoT, Machine Learning, Data Lake, Stream, Cube, Microsoft, SQL Server, Tableau, .Net, C#, Qlik, Power BI, Machine Learning, Azure Data Factory, RedShift, UiPath, Cloud, RPA, AWS, Redshift, Kinesis, QuickSight, SageMaker, S3, Databricks, AWS Lake Formation, Snowflake, Python, Qlik, Athena, Data Pipeline, Glue, Star Schema, Data Modeling, SQL, SSIS, SSAS, SSRS, PySpark, Microsoft Fabric, dbt, Linux, Terraform, Bicep, Data Ops, Purview, Git, Delta, Pandas, Spark SQL
Individual salaries that are offered to a candidate are determined after consideration of numerous factors including but not limited to the candidate’s qualifications, experience, skills, and geography.
California Range: $111,000 - $152,000
Colorado Range: $111,000 - $152,000
New York City/ Valhalla Range: $111,000 - $152,000
Washington Range: $111,000 - $152,000
About Us
BDO delivers assurance, tax, digital technology solutions and financial advisory services to clients throughout the country and around the globe. We offer numerous industry-specific practices, world-class resources, and an unparalleled commitment to meeting our clients’ needs. We currently serve more than 400 publicly traded domestic and international clients.
Unparalleled partner-involvement
Deep industry knowledge and participation
Geographic coverage across the U.S.
Cohesive global network
Focused capabilities across disciplines
BDO brings world-class resources and exceptional service to each and every one of our clients. BDO USA is a member of BDO International, the world’s fifth largest accounting network.
BDO offers a competitive Total Rewards package that encompass so much more than – “traditional benefits”. Our wide range of rewards and our employees’ ability to customize rewards to their individual needs are two of the reasons why BDO has been honored with so many workplace awards, including 100 Best Companies for Working Parents, Working Mother 100 Best Companies, Top Entry Level Employer, 2022 National Best & Brightest Companies to Work For and more.
Some Examples Of Our Total Rewards Offerings Include
Competitive pay and eligibility for an annual performance bonus.
A 401k plan plus an employer match
Comprehensive, medical, dental, vision, FSA, and prescription insurance from day one
Competitive Paid Time Off with daily accrual from day one of employment, plus paid holidays
Paid Parental Leave
Adoption Assistance
Firm paid life insurance
Wellness programs
Additional offerings include BDO Flex, Group Legal insurance, Pet insurance and Long-Term Care Insurance
Above offerings may be subject to eligibility requirements.
Click here to find out more!
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability or protected veteran status.
""BDO USA, P.A. is an EO employer M/F/Veteran/Disability""
Show more
Show less","Data Analytics, Business Intelligence, Artificial Intelligence, Application Development, SQL, Data Warehousing, Data Modeling, Semantic Model Definition, Star Schema Construction, Cloud Data Analytics Solutions, C#, Python, Java, Scala, Tabular Modeling, Microsoft Fabric, Power BI, Azure Analysis Services, Git, DevOps, Linux, Data Lake Medallion Architecture, Batch Data Ingestion, Streaming Data Ingestion, AI Algorithms, Machine Learning, Automation Tools, Computer Vision, Professionalism, Verbal Communication, Written Communication, Organizational Skills, Multitasking, Teamwork, DeadlineDriven Environment, Relationship Building, Data Pipeline, Glue, Star Schema, Data Modeling, SSIS, SSAS, SSRS, PySpark, Microsoft Fabric, dbt, Linux, Terraform, Bicep, Data Ops, Purview, Git, Delta, Pandas, Spark SQL","data analytics, business intelligence, artificial intelligence, application development, sql, data warehousing, data modeling, semantic model definition, star schema construction, cloud data analytics solutions, c, python, java, scala, tabular modeling, microsoft fabric, power bi, azure analysis services, git, devops, linux, data lake medallion architecture, batch data ingestion, streaming data ingestion, ai algorithms, machine learning, automation tools, computer vision, professionalism, verbal communication, written communication, organizational skills, multitasking, teamwork, deadlinedriven environment, relationship building, data pipeline, glue, star schema, data modeling, ssis, ssas, ssrs, pyspark, microsoft fabric, dbt, linux, terraform, bicep, data ops, purview, git, delta, pandas, spark sql","ai algorithms, application development, artificial intelligence, automation tools, azure analysis services, batch data ingestion, bicep, business intelligence, c, cloud data analytics solutions, computer vision, data lake medallion architecture, data ops, data pipeline, dataanalytics, datamodeling, datawarehouse, dbt, deadlinedriven environment, delta, devops, git, glue, java, linux, machine learning, microsoft fabric, multitasking, organizational skills, pandas, powerbi, professionalism, purview, python, relationship building, scala, semantic model definition, spark, spark sql, sql, ssas, ssis, ssrs, star schema, star schema construction, streaming data ingestion, tabular modeling, teamwork, terraform, verbal communication, written communication"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Memphis, TN",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3759709541,2023-12-17,West Memphis,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto,
San
Francisco or Chicago
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","ML Data Engineer, ML Data OPs, Airflow, KubeFlow, SnowFlake, Spark, Kubernetes, DynamoDB, Docker, Helm, SQL, PySpark, AWS, GCP, Azure, ETL, Kafka, Storm, SparkStreaming, Python, Java, SQL, Bash, Git, Pandas, R, NLP, LLMs","ml data engineer, ml data ops, airflow, kubeflow, snowflake, spark, kubernetes, dynamodb, docker, helm, sql, pyspark, aws, gcp, azure, etl, kafka, storm, sparkstreaming, python, java, sql, bash, git, pandas, r, nlp, llms","airflow, aws, azure, bash, docker, dynamodb, etl, gcp, git, helm, java, kafka, kubeflow, kubernetes, llms, ml data engineer, ml data ops, nlp, pandas, python, r, snowflake, spark, sparkstreaming, sql, storm"
Senior Analyst - Data & Analytics,Louis Dreyfus Company,"Memphis, TN",https://www.linkedin.com/jobs/view/senior-analyst-data-analytics-at-louis-dreyfus-company-3774986379,2023-12-17,West Memphis,United States,Mid senior,Hybrid,"Company Description
Louis Dreyfus Company is a leading merchant and processor of agricultural goods. Our activities span the entire value chain from farm to fork, across a broad range of business lines, we leverage our global reach and extensive asset network to serve our customers and consumers around the world. Structured as a matrix organization of six geographical regions and ten platforms, Louis Dreyfus Company is active in over 100 countries and employs approximately 15,000 people globally.
Job Description
The DT&A Department of Louis Dreyfus Company is currently looking for a Senior Analyst for its Data & Analytics Service, to be based in Cordova.
As part of a dynamic IT organization currently undergoing a transformational change, you will act as change booster and play a key role on business process transformation to enable innovative and faster decision making processes driven by analytic application. We invite you to join our team if you aim to make the difference with your contribution and establish yourself as a key member of our IT organization.
What will be your mission
:
Establish a relationship with your business partners to analyze and understand their data and analytics needs, searching for opportunities to improve and transform business processes
You will be a key contributor for the successful deployment of various Data & Analytics products for the region/platform
You will be the change agent in charge of reviewing and deploying the process transformation on various Data & Analytics products for the region/platform
What will be your duties & responsibilities
:
Management of your solutions/products’ scope, processes, and stakeholders
Monitor adoption measures for processes and applications in your area and propose corrective actions when necessary
Ensure agile product backlog, roll outs, continuous improvement plan and deployments are defined and constantly updated in sync with the global Business Intelligence strategy
Ensure support structure is in place for your products and become the “go to person” for trader’s users’ community driving adoption and removing roadblocks to effectively embed analytics solutions in business processes
Responsible for gathering and translating business requirements into detailed function specifications and managing requests for changes
Own the delivery of parts of the projects or programs in your scope and its associated metrics on time and on budget
Ensure products’ deliverables are properly documented and with adequate reference material
Highlight to solution manager implementation constraints you may sport as well as dependency with other solutions
Provide management with accurate and timely status reports of all projects/scope as assigned
Assist business partners in determining how business requirements are integrated into business processes and generate expected outcomes
Work closely with the business owners of your application/solution being the intermediary between them and our external delivery partners to develop or support your data and analytics products
Support business partners in the test phases and organize training sessions as required
Learn all core technologies in the data and analytics portfolio and how to build reports and analytics applications starting from data using them proficiently
What Technical Skills are required
:
Overall understanding and knowledge of global enterprise data management systems
Good knowledge of front end and data visualization tools for analytics applications implementation (dashboard, cockpits, reports) with preferably previous experience in one of:
Microsoft Power BI, Tableau
Design and developments skills on data structure and data flow for Backend implementation, ETL, Data warehousing, data modeling with preferably previous experience in one of:
MSSQL or similar technologies
Microsoft Azure Framework
Business rules coding, scripting and BI systems architecture knowledge previous experiences will be considered a plus
What Soft Skills will make you thrive in this role
:
Ability to move easily across different projects and initiatives, to prioritize and manage many open cases at one time
Curiosity and eagerness for exploring and understanding complex business processes
Quick learner on new domains, ramp up on knowledge curve easily, ability to work independently and with little prompting
Excellent teamwork, communication & collaboration skills
Strong analysis, synthesis, and communication capacity (written and oral), ability to negotiate and influence both internally and externally
Language
:
Fluent in English, oral and written is a must
Education
:
Higher Education in IT, Engineering, Math, Physics (others only if with previous proven IT experience
Experience
Previous Experiences required
:
5-7 years of proven experience in a business analyst type of role in IT projects implementation
Project management experience in small to medium size projects being able to report status and potential risks
Previous experience in commodities trading domain on middle office or controlling processes will be considered a strong plus (Position Reporting, Daily P&L, Execution, Risk Exposure etc.)
Previous experience in KPIs definition and storytelling visual analytics implementation will be considered a strong plus
Previous experience in a client facing role will be considered a plus
Experience in a multi-cultural environment
Additional Information
Why you should apply for this role
:
You will have the possibility to learn about latest cutting edge technologies in the Analytics domains: Microsoft Azure Platform and Microsoft Power BI
You will be the reference contact point for LDC North America’s senior leaders and decision makers
You will be part of
What We Offer
We provide a dynamic and stimulating international environment, which will stretch and develop your abilities and channel your skills and expertise with outstanding career development opportunities in one of the largest and most solid private companies in the world.
Comprehensive benefits program including medical, dental and vision care coverage, flexible spending account plans, employee assistance program, life insurance and disability coverage
401k with Company Match
Family Friendly Benefits including childbirth and parental leave, fertility and family building benefits
Paid Time Off (PTO) and Paid Holidays
Hybrid work available (not applicable to all roles)
Diversity & Inclusion
LDC is driven by a set of shared values and high ethical standards, with diversity and inclusion being part of our DNA. LDC is an equal opportunity employer committed to providing a working environment that embraces and values diversity, equity and inclusion.
LDC encourages diversity, supports local communities and environmental initiatives. We encourage people of all backgrounds to apply.
Equal employment opportunity (EEO)
Louis Dreyfus Company provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.
This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.”
Sustainability
Sustainable value is at the heart of our purpose as a company.
We are passionate about creating fair and sustainable value, both for our business and for other value chain stakeholders: our people, our business partners, the communities we touch and the environment around us
Show more
Show less","Microsoft Power BI, Tableau, MSSQL, Microsoft Azure Framework, Data visualization tools, Data warehousing, Data modeling, ETL, BI systems architecture, Business rules coding, Scripting, Data management systems","microsoft power bi, tableau, mssql, microsoft azure framework, data visualization tools, data warehousing, data modeling, etl, bi systems architecture, business rules coding, scripting, data management systems","bi systems architecture, business rules coding, data management systems, data visualization tools, datamodeling, datawarehouse, etl, microsoft azure framework, microsoft power bi, mssql, scripting, tableau"
"Credit and Collections Analyst, Data Strategy & Analytics",Enercare Inc.,"Markham, Ontario, Canada",https://ca.linkedin.com/jobs/view/credit-and-collections-analyst-data-strategy-analytics-at-enercare-inc-3783998451,2023-12-17,Oshawa, Canada,Associate,Hybrid,"Enercare Inc. is one of Canada’s largest home and commercial services companies, providing leading products and services in heating, cooling, plumbing, electrical, water heating and water purification. Our purpose is to provide energy-efficient products and services to our customers, so together we can take action for a greener tomorrow, starting in our homes and buildings.
We take pride in continually striving to make a positive impact in the communities we operate in. Enercare is a company that believes strongly in the health, safety and wellness of our people. It’s a place where careers are made. A place where people care about the communities we operate in, and who are inspired everyday by our purpose. In our values and our ambitions, we embrace change, and support our team members along the way.
Nationally, Enercare Inc. operates under several brands including Enercare, Service Experts, HydroSolution, Pioneer Plumbing & Heating, and Syles Mechanical Services, servicing Canadians in Ontario, Manitoba, Saskatchewan, Alberta, British Columbia, Quebec and New Brunswick. We are united through our joint commitment to excellent customer service to the Canadians we service every day, and our mission to contribute to a resilient, sustainable future. Enercare Inc. Is wholly owned by Brookfield Infrastructure Partners LP (“Brookfield”), a global leader in the management of alternative assets across real estate, infrastructure, renewable power, and private equity.
Summary:
Reporting to the Collections Analytics Lead, you will play a key role in a highly visible team within the organization to advance our business as we execute our data strategy. This is a great opportunity for an individual with an appetite for using data to drive insights, presenting key learnings, and delivering transformational outcomes to our business.
You are an inquisitive and results-driven individual with advanced analytics experience, a keen business sense, and a passion for telling stories with data. You contribute to the success of the Collections function by digging deep into Enercare’s data architecture and providing our team with high performing datasets, ensuring continuous high levels of service through development and oversight of KPIs, and researching emerging trends and technologies in the Credit and Collections space.
Responsibilities:
Leverage your strong analytical background in order to collaborate with various business partners to:
Lead collections analytics, strategy and portfolio performance assessment
Manipulate and maintain high quality datasets to facilitate precise and efficient analysis
Generate analytical recommendations to influence stakeholders in their actions to improve collections revenue and streamline operations
Research and help implement modern collections practices and emerging technologies
Ensure credit risks are quantified with effective management reporting, across all components of collections (People, Data, Process, Technology, and Strategy)
Manage processes to ensure clean and accurate data in a form readily available for analysis
Provide oversight of collection efforts by developing performance dashboards/reports and early warning triggers
Qualifications:
An undergraduate degree in a quantitative discipline and/or a strong background in quantitative analysis
Strong experience with statistical software packages such as SQL, Python, or R
Experience working with business intelligence tools such as Power BI, Tableau, etc.
Strong judicious relationship-building and negotiation skills to articulate sound recommendations and influence stakeholders
Excellent communication and presentation skills: can convey messages in a clear, and succinct manner
A results-driven individual with a high level of curiosity and the ability to dive into details without losing sight of the big picture
Exceptional organization skills to prioritize, manage, and implement a variety of contesting initiatives in a fast-paced environment
Prior experience in credit/collections or call center operations considered an asset
Enercare is an equal opportunity employer. We are committed to equal employment opportunity regardless of race, colour, ancestry, national origin, religion, sex, age, sexual orientation, gender identity, citizenship, marital status, disability, pregnancy, military status, protected veteran status or other characteristics protected by applicable law. Enercare’s recruitment process includes accommodation for applicants with disabilities in accordance with applicable provincial accessibility laws and regulations. All accommodations will take into account the applicant’s accessibility needs due to disability and are available upon request.
Show more
Show less","Statistics, SQL, Python, R, Power BI, Tableau, Data Analytics","statistics, sql, python, r, power bi, tableau, data analytics","dataanalytics, powerbi, python, r, sql, statistics, tableau"
(Electronic Data Interchange) Data Analyst - Contract,"Kelly Science, Engineering, Technology & Telecom","Markham, Ontario, Canada",https://ca.linkedin.com/jobs/view/electronic-data-interchange-data-analyst-contract-at-kelly-science-engineering-technology-telecom-3778848141,2023-12-17,Oshawa, Canada,Mid senior,Onsite,"We are currently recruiting for our pharma manufacturing client based out of Thornhill for a contract position.
Duration:
10
months
Role:
Reporting to the Oracle Project Manager in Canada, the Data Analyst will be responsible for collecting, analyzing, and visualizing data to uncover insights, communicate findings, and create data-driven solutions for the decision-making processes. This role requires strong knowledge of Oracle Cloud systems, Veeva CRM, EDI implementation, along with proficiency in data analysis tools and data visualization techniques. The successful candidate will collaborate closely with stakeholders from the cross functional departments to understand their data needs and translate complex data into intuitive visualizations and interactive dashboards.
Our ideal candidate will be a driven individual with high attention to detail and organization, and a positive attitude with a focus on exceptional execution.
Responsibilities:
• Collect, clean, and preprocess large datasets from Oracle Cloud system and Veeva CRM to ensure data integrity and accuracy.
• Conduct exploratory data analysis to identify trends, patterns, and correlations using statistical techniques and data visualization tools.
• Expert knowledge of EDI, script / scenario writing, and process work flows for phased implementation
• Collaborate with stakeholders to define data requirements and objectives and develop analytical solutions that address their specific needs.
• Design and create visually appealing and interactive data visualizations, charts, graphs, and dashboards using industry-standard tools (e.g., Power BI, Oracle Transactional Business Intelligence (OTBI), Oracle Business Intelligence Publisher).
• Analyze and interpret data to extract meaningful insights and present findings to both technical and non-technical audiences.
• Collaborate with cross-functional teams, including customer service specialists, BIAs, and IT professionals, to ensure efficient data integration and accessibility.
• Create and collaborate with cross-functional teams for script writing / scenario capturing for EDI setup, data testing, and preparation for CSV review.
Job Qualifications:
• Bachelor's degree in a relevant field such as Data Science, Statistics, Computer Science, or a related discipline.
• Minimum 2 years experience with Oracle Cloud / ERP systems,1 year experience with Veeva CRM beneficial but not required, minimum 2 years experience with EDI implementation (specifically, with Canadian retailers ie. 850 / 810 / 856 / 855).
• Strong knowledge of Oracle ERP systems, including the data model, data extraction, transformation, and analysis.
• Strong knowledge and experience in Oracle Transactional Business Intelligence (OTBI).
• Strong knowledge in Fusion technology is preferred specifically FBDi.
• SME in data analysis tools (e.g., SQL, Python, R) and data visualization tools (e.g., Power BI).
• Expert understanding of Microsoft Suite, specifically Excel and Word.
• Solid understanding of statistical concepts and techniques, with the ability to apply them to data analysis and interpretation.
• Strong analytical and problem-solving skills, with attention to detail and the ability to work with large and complex datasets.
• Experience in dashboard development, interactive visualization techniques, and user interface design principles.
Core Competencies:
• Ability to deal with sensitive topics in a tactful, diplomatic and professional manner at all times
• High-level of integrity and dependability with a strong sense of urgency and results-oriented
• Proactive self-starter with an ’owner's mindset’
• Consistently delivers quality work on time, with strong attention to detail
• Strong analytical, negotiation and creative problem-solving skills
• Exceptional oral, written and communication skills
• Excellent telephone and email etiquette
• Motivated, methodical, and optimistic
Show more
Show less","Oracle Cloud systems, Veeva CRM, EDI implementation, Oracle Transactional Business Intelligence (OTBI), Power BI, SQL, Python, R, Excel, Word, Fusion technology, FBDi","oracle cloud systems, veeva crm, edi implementation, oracle transactional business intelligence otbi, power bi, sql, python, r, excel, word, fusion technology, fbdi","edi implementation, excel, fbdi, fusion technology, oracle cloud systems, oracle transactional business intelligence otbi, powerbi, python, r, sql, veeva crm, word"
Data Center (Lab) Engineer,AMD,"Markham, Ontario, Canada",https://ca.linkedin.com/jobs/view/data-center-lab-engineer-at-amd-3767236614,2023-12-17,Oshawa, Canada,Mid senior,Onsite,"Overview
WHAT YOU DO AT AMD CHANGES EVERYTHING
We care deeply about transforming lives with AMD technology to enrich our industry, our communities, and the world. Our mission is to build great products that accelerate next-generation computing experiences – the building blocks for the data center, artificial intelligence, PCs, gaming and embedded. Underpinning our mission is the AMD culture. We push the limits of innovation to solve the world’s most important challenges. We strive for execution excellence while being direct, humble, collaborative, and inclusive of diverse perspectives.
AMD together we advance_
Responsibilities
SENIOR SOFTWARE DEVELOPMENT ENGINEER
The Role
AI GPU Software (AGS) Datacenter group is looking for dynamic and skilled individuals that can contribute to the bring up, support and debug of complex computing systems. Individual will be part of a growing lab team and is required to do hands on experiments related to issue management as well as hardware level reworks. The position involves a wide range of activities including deployments of pre-production platforms, test equipment to enable silicon to bring up and validations, contributing to the triage, debug and rework to resolve complicated system level issues. As a key contributor, individual will be part of a leading AI team to drive and enhance AMD’s abilities to deliver the highest quality, industry-leading technologies to market.
The Person
The ideal candidate possesses an innovative and problem-solving mindset, has a keen eye for system engineering support and development, and is diligent and passionate about Technology..
Key Responsibilities
Setup hardware (CPU/APU, GPU, Memory cards) in server/workstation computing systems to facilitate user defined workloads in the AGS data center labs.
Own initial trouble shooting, and debug of a wide variety of systems, firmware, or software issues encountered while maintaining the integrity of large number development equipment.
Triage, debug and reproduce issues and validate fixes identified by AGS development teams.
Installation, configuration of various OS Distros from console.
Setup Network gear, perform and validate Network configurations.
Configure file systems using VG/LV/Partitions
Integrate automated testing in CI/CD environment (e.g. Jenkins, ansible)
Provide logs and statistics that will help in further debug of issues.
Own inventory database management and administration through a managed system.
Participate in the Agile method of planning, delivery, and collaboration with internal and scaled agile teams.
Work with a managed ticketing system and communicate clearly on activities and steps.
Preferred Experience
Demonstrated experience working in data center and managing all aspects of lab infrastructure.
Thorough understanding of server, workstation hardware architecture.
Able to read and interpret board schematics.
Demonstrated experience in PC/Server environment H/W and S/W setup and administration.
Comfortable working in different operating system environments including Linux and Windows.
Excellent Hardware and OS Debug, troubleshoot skills.
Familiarity with Networking setup and configuration.
Hands on experience with various storage solutions (NAS,SAN. etc.) and form factors.
Knowledge on Cobbler, Open stack, foreman or any other provisioning automation tool is a big plus.
Experience with power supplies monitoring and sequencing.
Proficient at documenting experimental results in a structured manner for ease of reference.
Demonstrated ability to work with JIRA and Confluence project management and documentation tools.
Team player with strong communication, analytical and problem-solving skills.
Must be a self-starter capable of working in a dynamic environment with minimal supervision and driving tasks to completion with utmost quality.
Academic Credentials
Bachelor’s or Master’s degree in Computer/Electrical Engineering, or related technical discipline.
LOCATION:
Markham, Ontario, Canada
Qualifications
Benefits offered are described:
AMD benefits at a glance.
AMD does not accept unsolicited resumes from headhunters, recruitment agencies, or fee-based recruitment services. AMD and its subsidiaries are equal opportunity, inclusive employers and will consider all applicants without regard to age, ancestry, color, marital status, medical condition, mental or physical disability, national origin, race, religion, political and/or third-party affiliation, sex, pregnancy, sexual orientation, gender identity, military or veteran status, or any other characteristic protected by law. We encourage applications from all qualified candidates and will accommodate applicants’ needs under the respective laws throughout all stages of the recruitment and selection process.
Show more
Show less","Technical Support, System Engineering, System Administration, Hardware Troubleshooting, Linux, Windows, Server Hardware, Workstation Hardware, CPU, APU, GPU, Memory Cards, Network Configuration, OS Distros, File Systems, VG/LV, Partitions, CI/CD Environment, Jenkins, Ansible, Logs, Statistics, Agile Methods, JIRA, Confluence, Power Supplies, NAS, SAN, Cobbler, Open Stack, Foreman, Provisioning Automation","technical support, system engineering, system administration, hardware troubleshooting, linux, windows, server hardware, workstation hardware, cpu, apu, gpu, memory cards, network configuration, os distros, file systems, vglv, partitions, cicd environment, jenkins, ansible, logs, statistics, agile methods, jira, confluence, power supplies, nas, san, cobbler, open stack, foreman, provisioning automation","agile methods, ansible, apu, cicd environment, cobbler, confluence, cpu, file systems, foreman, gpu, hardware troubleshooting, jenkins, jira, linux, logs, memory cards, nas, network configuration, open stack, os distros, partitions, power supplies, provisioning automation, san, server hardware, statistics, system administration, system engineering, technical support, vglv, windows, workstation hardware"
Data Analyst - Canada Office,Children Believe,"Markham, Ontario, Canada",https://ca.linkedin.com/jobs/view/data-analyst-canada-office-at-children-believe-3765653397,2023-12-17,Oshawa, Canada,Mid senior,Hybrid,"Careers That Empower Young Dreamers
When you make a decision to work with Children Believe you’re not just doing something great for your career, you’re helping empower children around the world to access education, pursue their dreams and be a voice for change. We know there are lots of career options for talented, top performers like you, but not many offer the opportunity to help children live and dream fearlessly. Children Believe is currently recruiting for the following position:
Data Analyst
Reporting to the Sr. Project Manager, the Data Analyst will be responsible for full lifecycle data analysis including requirements gathering and analysis, design, development and testing of reporting and analytics capabilities for the organization.
The Data Analyst will identify, design and execute analytical functions to support the decision-making processes, strategic direction and project execution of functional teams (Corporate Services, Fund Development and Communication and Programs & Policy) and cross-functional special project groups. This includes monthly management reporting, marketing campaign analysis, donor profile analysis, program reach and effectiveness reporting and other ad hoc special projects. The position will serve areas across the enterprise, supporting and enabling collaboration, new insights and data-driven decisions.
This role requires a detail-oriented, dedicated and driven individual who is passionate about data and transforming it into actionable insights.
This role includes, but not limited to:
Work with management and key stakeholders to prioritize business and information needs.
Partner with cross-functional teams including Fund Development and Communication, Corporate Services, Programs and Policy, to answer high-level business questions.
Gather and document the business requirements and then develop actionable insights.
Autonomously prioritize and execute Analytics projects and tasks in a fast-paced environment.
Contribute to various phases of campaign management, from pre-analytics to list generation and post measurement.
Effectively develop and maintain highly accurate reports, dashboards and data analysis to support the various teams as required.
Perform UAT and regression testing.
Provide ongoing reports to stakeholders, interpret data and analyze results using statistical techniques.
Be able to handle and merge different data sources and respond rapidly to one-off requests;
Effectively communicate and/or present results and recommendations based on data analysis.
Work with internal stakeholders, fulfilling ad hoc reporting requests for information by interpreting requests, extracting, compiling, interpreting and presenting data.
Ensure the integrity of key organization-wide data and the safeguarding of sensitive information. This includes overseeing relevant data hygiene practice, quality control standards, records maintenance and cross-platform data integrity.
Develop, manage and enhance KPI dashboard reporting – working with relevant stakeholders to identify and prioritize needs, establish and test metrics, pilot test and improve ongoing reporting.
Act as subject-matter expert in matters related to data integrity and analysis. Research and provide guidance on policy changes and the establishment of best practices.
Continuously educate end-users and identify opportunities for enhancing analytics data, tools and processes to enable them for easy, quick and accurate self-serve reporting on Power BI.
Collaborate with digital and marketing teams on challenges and provide fact-based analytical advice for better acquisition and supporter retention.
Document operational procedures, dashboards and reports
Comply with all Children Believe’s policies.
Perform other related duties pro-actively or as assigned.
The ideal candidate possesses:
Bachelors degree in Statistics, Computer Science, Information Technology, Business, Marketing, Analytics or a related discipline.
Minimum of 3 years’ relevant work experience in reporting and analytics.
Comfortable generating, manipulating, and interpreting both qualitative and quantitative data.
Exceptional ability to gather, analyze and document information requirements from business units and cross-functional groups and work with multiple stakeholders.
Adept at queries, report writing and presenting findings.
Proven confidentiality processing sensitive data and information in accordance with security and privacy legislation and best practices.
Self-driven with a strong desire to collaborate and leverage the knowledge and skills of those around them.
Strong knowledge of Power BI, DAX, Power Query, SQL, Excel.
Familiarity with data loading methods, data modeling, ETL processes and reporting tools.
Certification in Data Analytics is an asset.
Strong interest in or established track record working with non-governmental international. Development organizations.
Demonstrated commitment to Children Believe’s Vision, Mission and Values.
Attributes, Traits, Behaviours:
Accountability for results.
Strong analytical and problem solving skills with a keen attention to detail.
Strong verbal and written communication skills.
Ability to manage multiple deliverables that have overlapping completion date.
Solutions-oriented, flexible, adaptable.
Ability to filter and prioritize opposing and concurrent requests and balance the needs of various functions.
Curiosity, fast learning.
Excellent presentation skills
Type: Hybrid/Remote
Our Hybrid Workplace Program is a flexible work arrangement that permits employees to work remotely while also occasionally being required to come into the Organization’s office. If approved to work from home as part of our Hybrid workplace program; the incumbent must be available to work from the Organization’s office from time to time as determined by the supervisor or as otherwise directed by the Organization.
Compensation: Hiring range between $55,000 to $60,000 annually based on experience.
Children Believe offers competitive paid time-off benefits and a comprehensive group benefits plan, including health/dental, life insurance, disability, EFAP and company-match RRSP from the first day of employment.
Application Process:
If you believe you have that unique combination of a not-for-profit heart combined with the skills and interest for the “Data Analyst” position, then click the “Apply” button found at the bottom of this screen.
Applicants must be legally eligible to work in Canada.
Consistent with our Child Protection/Safe-Guarding Policy the successful candidate must receive clearance by a criminal record check.
Children Believe is committed to diversity in the workplace and is an equal opportunity employer.
Children Believe is committed to providing workplace accommodations. If you require an accommodation, inform us and we will work with you to meet your needs.
We thank all applicants for applying, however, only candidates selected for an interview will be contacted.
Show more
Show less","Data Analysis, Reporting, Analytics, Data Integration, Data Extraction, Data Management, Data Cleaning, Data Modeling, Data Warehousing, ETL Processes, Statistical Analysis, Business Intelligence, Data Visualization, Dashboarding, Power BI, DAX, Power Query, SQL, Excel, Python, R, Tableau, Alteryx, Data Mining, Machine Learning, Artificial Intelligence, Cloud Computing, Big Data","data analysis, reporting, analytics, data integration, data extraction, data management, data cleaning, data modeling, data warehousing, etl processes, statistical analysis, business intelligence, data visualization, dashboarding, power bi, dax, power query, sql, excel, python, r, tableau, alteryx, data mining, machine learning, artificial intelligence, cloud computing, big data","alteryx, analytics, artificial intelligence, big data, business intelligence, cloud computing, dashboard, data cleaning, data extraction, data integration, data management, data mining, dataanalytics, datamodeling, datawarehouse, dax, etl, excel, machine learning, power query, powerbi, python, r, reporting, sql, statistical analysis, tableau, visualization"
Data Migration Analyst,WELL Health Technologies Corp. (TSX: WELL),"Whitby, Ontario, Canada",https://ca.linkedin.com/jobs/view/data-migration-analyst-at-well-health-technologies-corp-tsx-well-3767968430,2023-12-17,Oshawa, Canada,Mid senior,Hybrid,"Entity: WELL Health Technologies Corp, WELL EMR Group
Position: Data Migration Analyst
Type: Full Time Permanent
Location: Toronto, Ontario
Company Overview
WELL is an innovative technology enabled healthcare company whose overarching objective is to positively impact health outcomes by leveraging technology to empower and support healthcare practitioners and their patients.
WELL’s practitioner enablement platform includes comprehensive end-to-end practice management tools, including virtual care and digital patient engagement capabilities and Electronic Medical Records (EMR), Revenue Cycle Management (RCM) and data protection services. WELL uses this platform to power healthcare practitioners both inside and outside of WELL’s own omni-channel patient services offerings.
WELL owns and operates Canada's largest network of outpatient medical clinics serving primary and specialized healthcare services and is the provider of a leading multi-national multi-disciplinary telehealth offering. WELL is publicly traded on the Toronto Stock Exchange under the symbol ""
WELL
"". To learn more about the company, please visit:www.well.company.
About The Team
The WELL EMR Group is responsible for the development and support of the OSCAR platform, a leading Electronic Medical Record (EMR) software in Canada that supports thousands of providers and millions of Canada patients across the country. Primarily located in the greater Toronto area and the greater Vancouver area, the team currently works remotely due to COVID with expectations to remain remote for the near future. We are a remote first with hybrid support where desired.
Position Overview
Reporting to the Director of Accounts, the Data Migration Specialist will be responsible to develop, test and execute data conversions including extracting healthcare data from various EMR systems, performing transformations on the data and importing it into the target EMR. The Data Migration Specialist is responsible for converting data from multiple data sources, creation and usage of complex scripts, with limited assistance.
Working alongside the rest of your team, the Data Migration Specialist is responsible for facilitating meetings with both internal and external stakeholders to gather requirements, identify data conversion needs, provide ongoing external and internal updates to ensure high customer satisfaction.
You will provide quality assurance on data conversion work; verifying data integrity and identify and remedy of issues in a timely manner.
You will be expected to work and manage migration support tickets independently and develop and update technical and business process documentation for data conversions for both internal and externals stakeholders.
This is an exciting opportunity to play a role in shaping health care technology that supports tens of thousands of health care providers and millions of Canadians just like yourself. It is a requirement that employees work in a distraction free workplace.
Mission
Delight new customers through exceptional onboarding experience with a focus on data migration and accuracy. Make customers feel at home in OSCAR on day 1.
Outcomes
Deliver 50 Customer Onboarding/Migrations in the 6 Months of Employment
Reduce Number of Customers Migration Issues By 15% in First Year of Employment
Recommend and Implement Process Changes to Accelerate Throughput by 15% in First Year of Employment
Significantly Improve Documentation Around Migration/Onboarding Processes
Implement New PDF Export Offboarding Process
Implement New Onboarding Process for Importing Telus Local Customers
Competencies
Technical You are very confident with data systems (excel, myslq, postgres, etc) and have a strong background in extracting and modifying data.
Honest Honest with an ability to speak truthfully in all scenarios and with all stakeholders to properly represent their position and customer’s interests.
Smart The role requires individuals that have a generally high level of intelligence and a mental toughness for solving problems.
Communicator Able to communicate clearly and thoughtfully to both internal (employees) and external (customers) parties. Provides context (background) and clarity (details) as required and appropriately.
Integrity They have high level of personal standards around their integrity.
Accountability Doesn’t shy away from responsibility. Takes work to the finish line with regularity.
More About You
You are able to analyze and convert healthcare data in HL7, CDS, CSV and SQL formats for extraction, transformation and import.
The Successful Candidate Will Also
Possess 4 years of data migration experience with appropriate supporting education.
Take pride in delivering great results to our customers.
Possess strong communication skills and communicate with precision and clarity.
Be comfortable working in a fast-paced environment and prioritize your time and focus effectively.
Have exacting standards and a deep understanding of best practices and industry standards.
WELL is committed to supporting a diverse, inclusive, and accessible workplace. We welcome and celebrate the diversity of applicants and team members across ability, race, gender identity, sexual orientation, and perspective. We strive to create an inclusive workplace where differences are celebrated and fuels our success – this is the WELL Way!
Show more
Show less","Data Migration, Data Extraction, Data Transformation, Data Import, HL7, CDS, CSV, SQL, Excel, MySQL, PostgreSQL, OSCAR, EMR, RCM, Telehealth","data migration, data extraction, data transformation, data import, hl7, cds, csv, sql, excel, mysql, postgresql, oscar, emr, rcm, telehealth","cds, csv, data extraction, data import, data migration, data transformation, emr, excel, hl7, mysql, oscar, postgresql, rcm, sql, telehealth"
Data Scientist,Allstate Canada,"Markham, Ontario, Canada",https://ca.linkedin.com/jobs/view/data-scientist-at-allstate-canada-3752894073,2023-12-17,Oshawa, Canada,Mid senior,Hybrid,"Who is Allstate:
Allstate Insurance Company of Canada is a leading home and auto insurer focused on providing its customers prevention and protection products and services for every stage of life. The company is proud to have been named a Best Employer in Canada for nine consecutive years and prioritizes supporting employees and fostering an inclusive, welcoming corporate culture. Allstate is committed to making a positive difference in the communities in which it operates through partnerships with charitable organizations, employee giving and volunteerism. Serving Canadians since 1953, Allstate strives to provide reassurance with its ""You’re in Good Hands®"" promise.
Through our Employee Value Proposition, Opportunity, Flexibility, Community, Diversity and Family, we have worked hard to develop and nurture a culture where employees feel valued, experience personal growth, have career options and truly enjoy the work they do.
Role Designation:
Hybrid
Benefits to joining Allstate
Flexible Work Arrangements
Employee discounts (15% on auto and property insurance, plus many other products and services)
Good Office program (receive up to 400$ back after purchasing office equipment)
Student Loan Payment Matching Program for Government Student loans
Comprehensive Retirement Savings Program with employer matched contributions
Annual Wellness allowance to support employees with improving health and wellbeing
Personal reflection day
Tuition Reimbursement
Working within the community and giving back!
Job description:
Our team is growing and we are looking to hire an exceptional Data Scientist to synthesize our extensive policyholder and claims data into business enhancing predictive models that will ultimately impact the company’s bottom line. This is a unique opportunity to join a new, multidisciplinary team who work on high-impact projects to bring efficiencies to all functional areas of our business. The team works in a fast-paced environment, using techniques and algorithms best suited for solving challenging problems of the insurance industry.
Accountabilities:
The successful candidate will gain exposure to all aspects of insurance operations with respect to claims, underwriting and pricing, as well as general areas of business operations such as human resources and marketing, working on high impact actionable projects. You will learn the business knowledge of the prospective stakeholder through their information datasets. Combining quantitative data with business domain knowledge, you will transform them into predictive models that provide insights that ultimately produce new and creative analytic solutions as part of our core deliverables. This role will work hand in hand with data scientists, business stakeholders in developing models and identifying strategies to improve the company’s bottom line.
Qualifications:
Minimum of 5 years of Machine Learning and analytics experience within a professional services firm or similar industrial environment.
Bachelor's degree or higher from an accredited college in a Mathematics, Statistics, Actuarial Sciences, Computer Science, Physics, Engineering or a related field .
Strong programming skills, ability to write production level code in Python.
Working experience in Machine learning algorithms, model implementation and optimization to solve Regression, Classification and Segmentation problems.
Experience with ML frameworks/libraries e.g. Scikit-learn, Pandas, Matplotlib, Seaborn, Tensor flow or Pytorch.
Business and analytics acumen to run and interpret the results of models - turn large amounts of complex, detailed information into clear summaries and business recommendations.
Excellent collaboration skills and the ability to work in a team environment and across multiple sites and business units.
Excellent communication and interpersonal skills - be able to effectively communicate complex results to a business audience not familiar with complex data and analytics.
Allstate Canada Group has policies and practices that provide workplace accommodations. If you require accommodation, please let us know and we will work with you to meet your needs.
Show more
Show less","Machine Learning, Python, Pandas, Matplotlib, Seaborn, Tensor flow, Pytorch, Scikitlearn, Business acumen, Analytics, Collaboration, Communication","machine learning, python, pandas, matplotlib, seaborn, tensor flow, pytorch, scikitlearn, business acumen, analytics, collaboration, communication","analytics, business acumen, collaboration, communication, machine learning, matplotlib, pandas, python, pytorch, scikitlearn, seaborn, tensor flow"
E-Commerce and Master Data Analyst,Johnson & Johnson,"Markham, Ontario, Canada",https://ca.linkedin.com/jobs/view/e-commerce-and-master-data-analyst-at-johnson-johnson-3781128984,2023-12-17,Oshawa, Canada,Mid senior,Hybrid,"Johnson & Johnson is recruiting for an E-Commerce & Master Data Analyst (EMDA), located in Markham, ON.
Johnson & Johnson MedTech has been working to make surgery better for more than a century. With substantial breadth and depth in surgical technologies, orthopedic and interventional solutions, we aspire to improve and enhance medical care for people worldwide. Together, we are working to shape the future of health through differentiated products and services.
Johnson & Johnson thrives on a diverse company culture, celebrates the uniqueness of our employees and is committed to inclusion. We are proud to be an equal opportunity employer.
As the EMDA, you will be responsible for increasing efficiency through automation, by leading e-commerce initiatives from implementation through operational support, and problem solving with a focus on continuous improvement.
You will also have responsibility for maintaining cross-sector master data to enable business needs while adhering to compliance requirements.
The work environment is dynamic and will provide you with the opportunity to go behind the scenes and learn the skills to enable a smooth electronic order journey for our customers. Your attention to detail, partnering and collaboration skills and e-commerce expertise will be crucial to success in this role.
Key Responsibilities
Drives electronic adoption using e-commerce expertise to deliver outstanding customer support.
Leads the onboarding and testing of new EDI partners and new transactions and leads the troubleshooting of errors through focus on root cause
Owns the resolution of issues with GHX transactions and reducing error rates by partnering with Customer Logistics representatives, GHX representatives, and our customers.
As the Deliver Department Data Lead, you will own the Deliver fields within the cross-sector Material and Customer master workflow. This requires partnering with the Regional Master Data team, and other functional teams to ensure the accuracy and timely update of data.
Owns the maintenance and update of critical functional master data processes that enable the customer order journey.
Responsible for promoting, training and administration of our customer ordering portal
Partners with Customer Operations Leads to prioritize efficiencies and identify solutions. Provides subject matter expertise and testing support within our SAP and JJCC environments.
Uses analytical skills to develop, prepare and analyze reporting dashboards for the team. QUALIFICATIONS :
Education
University degree or equivalent experience
Required
EXPERIENCE AND SKILLS
:
2-3 plus years proven expertise with SAP in Master Data or Sales and Distribution (SD)- R
EDI experience strongly preferred- P
Strong attention to detail and written and verbal communication skills- R
Ability to successfully prioritize multiple deliverables in a rapidly changing, dynamic environment and work independently while demonstrating initiative
Ability to apply proven problem solving and analytical experience to interpret data with a strong proficiency in Excel
Bilingual (English/French) required
Johnson & Johnson is an Affirmative Action and Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, age, national origin, or protected veteran status and will not be discriminated against on the basis of disability.
Show more
Show less","SAP, EDI, Data Analysis, Problem Solving, Communication Skills, Excel, Bilingual (English/French)","sap, edi, data analysis, problem solving, communication skills, excel, bilingual englishfrench","bilingual englishfrench, communication skills, dataanalytics, edi, excel, problem solving, sap"
Senior Data Analyst,The BayTech Group,"Timonium, MD",https://www.linkedin.com/jobs/view/senior-data-analyst-at-the-baytech-group-3781317079,2023-12-17,Catonsville,United States,Mid senior,Hybrid,"Senior Data Analyst
We are seeking a Senior Data Analyst with strength in SQL for a dynamic and innovative company dedicated to modernizing their data environment. We are seeking a highly skilled and experienced Senior SQL Analyst to join our client's growing team and play a crucial role in shaping their data analytics and business intelligence capabilities.
Responsibilities:
Develop and optimize complex SQL queries and stored procedures for efficient data retrieval and analysis in an enterprise environment.
Design and implement ETL processes to ensure seamless data integration across multiple sources.
Utilize experience in SSRS or SSIS to streamline and automate data workflows.
Create and maintain interactive and visually appealing dashboards and reports using tools such as Power BI, Tableau, or other data visualization technologies.
Ensure data accuracy, integrity, and security through rigorous testing and validation processes.
Stay abreast of industry trends and advancements in SQL and data analytics, providing recommendations for continuous improvement.
Qualifications:
Bachelor's degree in a relevant field (e.g., Computer Science, Information Systems, Statistics).
Proven experience as a Senior SQL Analyst
Expertise in SQL Development, queries, stored procedures
Strong proficiency in writing and optimizing complex SQL queries.
Expertise in SSRS or SSIS for data integration and automation.
Familiarity with data visualization tools such as Power BI, Tableau, or equivalent.
Excellent problem-solving skills and attention to detail.
Effective communication skills to collaborate with cross-functional teams and present insights to stakeholders.
Preferred Qualifications:
Experience with data visualization and dashboarding technologies.
Advanced certifications in SQL or related fields.
For more information on this position, please apply with your resume. Be sure that your resume highlights your extensive SQL experience.
Only candidates open to a hybrid schedule of 1-3 days/week onsite will be considered.
Show more
Show less","SQL, Data Analytics, Stored Procedures, ETL Processes, SSRS, SSIS, Data Visualization, Power BI, Tableau, Data Integrity, Data Security, Data Modeling, Data Mining, Data Warehousing, Business Intelligence, Reporting, Communication","sql, data analytics, stored procedures, etl processes, ssrs, ssis, data visualization, power bi, tableau, data integrity, data security, data modeling, data mining, data warehousing, business intelligence, reporting, communication","business intelligence, communication, data integrity, data mining, data security, dataanalytics, datamodeling, datawarehouse, etl, powerbi, reporting, sql, ssis, ssrs, stored procedures, tableau, visualization"
Data Analyst Part Time,Voxmediallc,"Sault Ste. Marie, Ontario, Canada",https://ca.linkedin.com/jobs/view/data-analyst-part-time-at-voxmediallc-3757203914,2023-12-17,Sault Sainte Marie, Canada,Mid senior,Onsite,"Summary:
Data Analyst Part Time will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : hiring@jobsai.live
Powered by Webbtree
Show more
Show less","Data Analysis, Statistical Techniques, SQL, R, Python, Datadriven decisionmaking, Business Process Optimization, A/B Testing, Data Quality Management, Data Cleansing, Data Manipulation, Data Visualization, Tableau, Power BI, Statistical Modeling, Hypothesis Testing, ETL Processes","data analysis, statistical techniques, sql, r, python, datadriven decisionmaking, business process optimization, ab testing, data quality management, data cleansing, data manipulation, data visualization, tableau, power bi, statistical modeling, hypothesis testing, etl processes","ab testing, business process optimization, data manipulation, data quality management, dataanalytics, datacleaning, datadriven decisionmaking, etl, hypothesis testing, powerbi, python, r, sql, statistical modeling, statistical techniques, tableau, visualization"
Sr. Business Intelligence Analyst (Data Scientist),Corebridge Financial,"Orange County, CA",https://www.linkedin.com/jobs/view/sr-business-intelligence-analyst-data-scientist-at-corebridge-financial-3772698681,2023-12-17,Irvine,United States,Mid senior,Hybrid,"Who We Are
Corebridge Financial is an outstanding franchise that brings together a broad portfolio of life insurance, retirement and institutional products offered through an extensive, multichannel distribution network. We hold long-standing, leading market positions in many of the markets we serve. With our strong capital position, customer orientation, breadth of product expertise and deep distribution relationships across various channels, we are well positioned to serve growing market needs.
We have a legacy of working to make the world a better place, and that begins with our most important asset, our employees. We’re proud to offer a range of employee benefits and resources that help you protect what matters most – your health care, savings, financial protection, and wellbeing.
Sr. Business Intelligence Analyst (Data Scientist)
About The Role
We are looking for an analytically savvy individual to join the Business Intelligence and Analytics team. The work product of this role informs decisioning across the Life & Retirement business. To thrive in this position you, will be curiosity-driven, technically adept, communicative, and accomplished at navigating all stages of data science projects. You would:
Apply a full spectrum of data science techniques to projects and to structure analyses
Work directly with business owners to build relevant use cases
Analyze internal and industry data to find insights, formulate recommendations, and deliver findings to non-technical and executive stakeholders
Collaborate with business, reporting, and IT partners across the organization to operationalize results
Consult with team members to inform parallel projects
Be a leader in furthering team practices and capabilities
What You Need To Know
The Business Intelligence Analyst will lead complex data-driven analyses to support executive decision making and drive growth across the Life & Retirement business.
The ideal candidate is a critical thinker who balances technical skills with business-focus and has an established analytics foundation.
Works with stakeholders to assess needs, structure analyses, and apply analytical methodologies to derive actionable insights.
Communicates impacts and applicability of results to the business through visuals and storytelling.
Implements analytical best-practices and knowledge sharing with the team.
What We’re Looking For
In-depth understanding and expertise applying statistical models, predictive models, clustering approaches, and regression techniques
A minimum of 5 years of experience working in analytics or as a data scientist in a business setting. An advanced degree in a quantitative field or an MBA is a plus
Strong interpretation skills and the ability to distill results into business-focused scenarios and recommendations
Aptitude navigating all classifications of data, assessing relevance and limitations. This includes extensive experience integrating data and building models
Strong SQL skills, proficiency in Python, R, or at least one scripting/statistical programming language
Skilled at producing compelling visuals with tools such as Tableau, PowerBI, or packages such as Matplotlib or Plotly.
Work in cloud-based environments (Snowflake, AWS)
Ability to prioritize across individual and team deliverables
Project management experience & experience with Machine learning, a plus
Knowledge of financial services or life insurance, a plus
What Our Employees Like Most About Working
We care about your professional development. Our career progression program will provide you with the opportunity to develop your skills, strengthen your productivity and be eligible to progressively advance to positions with an increased responsibility and increased compensation.
Our “Giving Back” policy is at the core of our daily operations and guides our future progress. We offer up to 16 hours a year paid time off to volunteer in the community.
Our people are our most important asset therefore we provide a generous benefits plan; Medical, Dental and Vision, 401(k) and company match, paid time off (PTO) plus company paid holidays. We’re proud to offer a range of competitive benefits, a summary of which can be viewed here: Benefits Overview.
For positions based in Woodland Hills California, the base salary range is $100,000 - $130,000 and the position is eligible for a bonus in accordance with the terms of the applicable incentive plan. In addition, we’re proud to offer a range of competitive benefits.
Remote Role, Work from Home
We are an Equal Opportunity Employer
Corebridge Financial, Inc., its subsidiaries and affiliates are committed to be an Equal Opportunity Employer and its policies and procedures reflect this commitment. We provide equal opportunity to all qualified individuals regardless of race, color, religion, age, gender, gender expression, national origin, veteran status, disability or any other legally protected categories such as sexual orientation. At Corebridge Financial, we believe that diversity and inclusion are critical to our future and our mission – creating a foundation for a creative workplace that leads to innovation, growth, and profitability. Through a wide variety of programs and initiatives, we invest in each employee, seeking to ensure that our people are not only respected as individuals, but also truly valued for their unique perspectives.
To learn more please visit: www.corebridgefinancial.com
Corebridge Financial is committed to working with and providing reasonable accommodations to job applicants and employees with physical or mental disabilities. If you believe you need a reasonable accommodation in order to search for a job opening or to complete any part of the application or hiring process, please send an email to TalentandInclusion@corebridgefinancial.com. Reasonable accommodations will be determined on a case-by-case basis.
We consider qualified applicants with criminal histories, consistent with applicable law.
Functional Area
DT - Data
Estimated Travel Percentage (%): Up to 25%
Relocation Provided: No
American General Life Insurance Company
Show more
Show less","Statistical models, Predictive models, Clustering approaches, Regression techniques, Data science, SQL, Python, R, Scripting/statistical programming languages, Tableau, PowerBI, Matplotlib, Plotly, Machine learning, Snowflake, AWS, Project management","statistical models, predictive models, clustering approaches, regression techniques, data science, sql, python, r, scriptingstatistical programming languages, tableau, powerbi, matplotlib, plotly, machine learning, snowflake, aws, project management","aws, clustering approaches, data science, machine learning, matplotlib, plotly, powerbi, predictive models, project management, python, r, regression techniques, scriptingstatistical programming languages, snowflake, sql, statistical models, tableau"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Cypress, CA",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3773088738,2023-12-17,Irvine,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto
or
San
Francisco
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Java, bash, SQL, Apache Airflow, KubeFlow, Helm, Git, Kubernetes, Spark, Apache Kafka, Apache Hadoop, Distributed Systems, Big Data, Microservices, Docker, Machine Learning, Data Engineering, Data Mining, Data Normalization, Data Modeling, ETL, Data Governance, AWS, GCP, Azure, Snowflake, DynamoDB, Pandas, R, NLP, LLMs","python, java, bash, sql, apache airflow, kubeflow, helm, git, kubernetes, spark, apache kafka, apache hadoop, distributed systems, big data, microservices, docker, machine learning, data engineering, data mining, data normalization, data modeling, etl, data governance, aws, gcp, azure, snowflake, dynamodb, pandas, r, nlp, llms","apache airflow, apache hadoop, apache kafka, aws, azure, bash, big data, data engineering, data governance, data mining, data normalization, datamodeling, distributed systems, docker, dynamodb, etl, gcp, git, helm, java, kubeflow, kubernetes, llms, machine learning, microservices, nlp, pandas, python, r, snowflake, spark, sql"
"Sr. Analyst, Brand and Customer Data Process Management",Nutrawise Health and Beauty (Youtheory),"Irvine, CA",https://www.linkedin.com/jobs/view/sr-analyst-brand-and-customer-data-process-management-at-nutrawise-health-and-beauty-youtheory-3753009162,2023-12-17,Irvine,United States,Mid senior,Hybrid,"Overall Responsibilities
This role is responsible to provide cross functional coordination to develop and facilitate the data collection processes for all required information needed for the master data creation, maintenance, and the subsequent maintenance in all of the core applications, including customer databases, like Walmart 360, IX-One, ReposiTtrak, and the like. This role will ensure the accuracy and completeness of all system data and processes, identifying and resolving discrepancies to resolve and eliminate errors with the integrated downstream systems. We are seeking a dynamic and results-driven individual with a focus on Business Analytics and Master Data. In this role, you will play a pivotal part in driving our business success by ensuring the accuracy and efficiency of our data management processes. If you have a passion for data-driven decision-making and a background in business, marketing, sales, or economics, we encourage you to explore this exciting opportunity.
Specific Key Responsibilities & Duties
A) Cross-Functional Data and Process Management:
Lead data-driven decision-making processes across various departments, with a focus on Sales and Marketing.
Analyze data to provide valuable insights and collaborate with organizational leaders to influence data and process management strategies.
Implement and enforce data and process management best practices.
Provide training and education as needed to enhance data and process management skills.
B) Data and Process Governance and Accuracy:
Ensure governance, oversight, and maintenance of critical data in core systems, including Item Master, Customer Master, and Vendor Master information.
Collaborate with business data owners to maintain accurate and consistent data.
Enforce processes and guidelines to uphold master data best practices.
C) Core Systems:
Ensure the accuracy of Youtheory customer records, particularly in customer databases like Walmart 360, IX-One, ReposiTtrak, and others.
Drive data and process initiatives such as data process cleansing, refresh, and creation across core systems.
Oversee data transitions to new systems, ensuring proper formatting and characteristics.
Implement standardized master data processes company-wide.
Create and manage documentation of master data business processes and format guidelines.
D) Sales Team Related Tasks:
Manage Sales Teams Site.
Coordinate and execute Sales Samples effectively.
Knowledge, Skills & Abilities Requirements
Bachelor's degree in business, sales, marketing or economics or equivalent experience in a related field.
3-5 years of experience in data analytics and process improvement
5+ years of experience in a communications or administration role.
Exceptional communication skills for collaboration with departmental data owners to develop ERP data and process requirements.
Supply chain planning experience in ERP systems in a manufacturing environment is an asset.
Proficiency in Microsoft Office applications (Outlook, Word, Excel, etc.).
Strong analytical and problem-solving skills, including customer-facing data analysis and insights.
Strong attention to detail and ability to follow processes.
Ability to prioritize deliverables and multitask effectively.
Domain knowledge in the Consumer Packaged Goods (CPG) industry.
Join our team and be a part of an organization that values data and process-driven decision-making, collaboration, and continuous improvement. Your expertise will be instrumental in shaping our data management processes and contributing to our business growth.
Show more
Show less","Data integration, Data analysis, Business intelligence, ERP data management, Master data management, Data governance, Data accuracy, Data cleansing, Data migration, Sales management, Microsoft Office Suite, Supply chain planning, Customer relationship management, Consumer packaged goods","data integration, data analysis, business intelligence, erp data management, master data management, data governance, data accuracy, data cleansing, data migration, sales management, microsoft office suite, supply chain planning, customer relationship management, consumer packaged goods","business intelligence, consumer packaged goods, customer relationship management, data accuracy, data governance, data integration, data migration, dataanalytics, datacleaning, erp data management, master data management, microsoft office suite, sales management, supply chain planning"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Cypress, CA",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3759712046,2023-12-17,Irvine,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto,
San
Francisco or Chicago
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Java, Bash, SQL, Git, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, AWS, GCP, Azure, DynamoDB, ETL, Kafka, Storm, SparkStreaming, Pandas, R, Machine learning, Data mining, Data cleaning, Data modeling, Data enrichment, Data monitoring, Data governance, Data compliance, Data management, Data pipelines, Data engineering, Data ops, ML models, ML datasets, NLP/large language models, Conversational AI APIs, Recommender systems, Distributed systems, Microservices","python, java, bash, sql, git, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, dynamodb, etl, kafka, storm, sparkstreaming, pandas, r, machine learning, data mining, data cleaning, data modeling, data enrichment, data monitoring, data governance, data compliance, data management, data pipelines, data engineering, data ops, ml models, ml datasets, nlplarge language models, conversational ai apis, recommender systems, distributed systems, microservices","airflow, aws, azure, bash, conversational ai apis, data cleaning, data compliance, data engineering, data enrichment, data governance, data management, data mining, data monitoring, data ops, datamodeling, datapipeline, distributed systems, docker, dynamodb, etl, gcp, git, helm, java, kafka, kubernetes, machine learning, microservices, ml datasets, ml models, nlplarge language models, pandas, python, r, recommender systems, snowflake, spark, sparkstreaming, sql, storm"
Customer Service Representative/Data Analyst/Data Entry Clerk,Dooleyboyer,"Bundaberg, Queensland, Australia",https://au.linkedin.com/jobs/view/customer-service-representative-data-analyst-data-entry-clerk-at-dooleyboyer-3742321859,2023-12-17,Bundaberg, Australia,Mid senior,Hybrid,"Customer Service Representative/Data Analyst/Data Entry Clerk /Office will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.If you are interested in this position, please send your resume, contact information and salary requirements to : hrteam@jobsolutionsai.online
Powered by Webbtree
Show more
Show less","Data analysis, Statistical techniques, Data interpretation, Data visualization, Data management, ETL processes, SQL, R, Python, Tableau, Power BI, Hypothesis testing, A/B testing","data analysis, statistical techniques, data interpretation, data visualization, data management, etl processes, sql, r, python, tableau, power bi, hypothesis testing, ab testing","ab testing, data interpretation, data management, dataanalytics, etl, hypothesis testing, powerbi, python, r, sql, statistical techniques, tableau, visualization"
Senior Data Engineer,Extend Information Systems Inc.,"New Boston, ME",https://www.linkedin.com/jobs/view/senior-data-engineer-at-extend-information-systems-inc-3645414549,2023-12-17,Maine,United States,Mid senior,Onsite,"Hello,
Greetings!!
We are actively hiring Senior Data Engineer. It's a Contract/ Full Time Position. Please have a look of below job description and let me know if you are interested. Zeenat@extendinfosys.com.
Role: Senior Data Engineer
Location: Boston MA, Durham North Carolina, Westlake Texas
Full-time/Contract
(Day 1 Onsite)
Must Have Skills
Expertise in designing and developing analytics / operational insights and real time dashboards using Python, Java both on-premise and cloud (AWS EC2/RDS & Azure SQL MI).
Strong Database experience on one or more DBMS such as Oracle, DB2, Netezza, SQL server, PostgreSQL, Mongo etc.). SQL Queries, Joins and performance monitoring/tuning.
Excellent communication skills and experience in Agile methodologies Kanban/Scrum.
Good To Have Skills
Knowledge on DevSecOps, CI/CD, Automation, & AI OPS/ML OPS.
Knowledge on Informatica powercenter Admin support, Snowflake SysDBA.
Knowledge on Nifi for Database Operations, Optim for Data Masking, Kubernetes for cluster management on AWS and Real Time platform such Aerospike for ATIM & UID.
Thank You,
Zeenat Nayer | Technical Recruiter | Extend Information Systems
Cell:
571-800-0882
Email:
zeenat@extendinfosys.com
Address:
44355 Premier Plaza UNIT 220, Ashburn, VA, USA - 20147
Web:
WWW.extendinfosys.com
Show more
Show less","Python, Java, SQL, Data Warehousing, Data Analytics, Machine Learning, Cloud Computing (AWS Azure), Database Systems (Oracle DB2 Netezza PostgreSQL MongoDB), Data Visualization, Agile Methodologies (Kanban Scrum), DevSecOps, Continuous Integration/Continuous Delivery (CI/CD), Data Masking, Kubernetes, RealTime Data Processing, Informatica PowerCenter, Snowflake, Nifi, Aerospike, Agile Methodology (Kanban/Scrum), DevSecOps, CI/CD","python, java, sql, data warehousing, data analytics, machine learning, cloud computing aws azure, database systems oracle db2 netezza postgresql mongodb, data visualization, agile methodologies kanban scrum, devsecops, continuous integrationcontinuous delivery cicd, data masking, kubernetes, realtime data processing, informatica powercenter, snowflake, nifi, aerospike, agile methodology kanbanscrum, devsecops, cicd","aerospike, agile methodologies kanban scrum, agile methodology kanbanscrum, cicd, cloud computing aws azure, continuous integrationcontinuous delivery cicd, data masking, dataanalytics, database systems oracle db2 netezza postgresql mongodb, datawarehouse, devsecops, informatica powercenter, java, kubernetes, machine learning, nifi, python, realtime data processing, snowflake, sql, visualization"
"Senior Data Engineer, Corporate Strategy Advanced Analytics",IDEXX,"Westbrook, ME",https://www.linkedin.com/jobs/view/senior-data-engineer-corporate-strategy-advanced-analytics-at-idexx-3751810778,2023-12-17,Maine,United States,Mid senior,Onsite,"As a
Senior Data Engineer
in the Corporate Strategy Advanced Analytics team, you will partner with others across the organization to enable access to enriched data for analysis and visualization for internal and external stakeholders. You will be responsible for helping define processes that allow consumers (both individual and software) to efficiently utilize appropriate data assets while ensuring compliance with defined standards.
Department
The Corporate Strategy Advanced Analytics is a high-visibility team that plays a vital role in identifying growth opportunities. The group supports essential business strategy projects and performs critical analyses for internal and external partners using a wide range of data assets.
Our Tech Stack
AWS, Snowflake, Hadoop, DataBricks, Spark, R, Python, SQL
In this role
You will design and implement scalable, reliable distributed data processing frameworks and analytical infrastructure using multiple technologies, including data sets or data warehouses, data virtualization and services, and repositories of semi-structured data sets.
You will define, design, and implement data management, storage, backup, and recovery solutions that ensure the high performance of the organization's enterprise data.
You will design automated software deployment functionality that efficiently manages applications across distributed platforms.
You will monitor structural performance and utilization, identify problems, and implement solutions.
You will lead the creation of standards, best practices, and new processes for the operational integration of new technology solutions.
You will ensure environments are compliant with defined standards and operational procedures.
You will implement measures to ensure data accuracy and accessibility, constantly monitoring and refining the performance of data management systems.
What do you need to succeed?
5+ years of related work experience in a business environment.
You have experience owning a technology product and assuming a technical lead role.
You understand structural requirements and can define standards for storing, consuming, integrating, and managing data.
You are proficient in coding and programming languages such as Structured Query Language (SQL) and Python. Familiarity with R will be an advantage.
You are familiar with cloud platforms such as Amazon Web Services (AWS).
You have experience or a good understanding of:
-Hadoop-based technologies like MapReduce, Spark, Hive, Presto and Pig
-SQL-based technologies like Oracle, PostgreSQL and MySQL
-NoSQL technologies like Cassandra and MongoDB
-data warehousing solutions and relational database theory
-industry-standard software APIs.
You have good verbal and written communication skills and can translate technical subject matter to non-technical audiences.
You take the initiative in resolving problems and can balance conflicting requirements in partnership with others.
You excel at customer service and building relationships.
You have experience building distributed and cloud-based data pipelines.
Why IDEXX
We’re proud of the work we do because our work matters. An innovation leader in every industry we serve, we follow our Purpose and Guiding Principles to help pet owners worldwide keep their companion animals healthy and happy, to ensure safe drinking water for billions, and to help farmers protect livestock and poultry from diseases. We have customers in over 175 countries and a global workforce of over 9,000 talented people.
So, what does that mean for you? We enrich the livelihoods of our employees with a positive and respectful work culture that embraces challenges and encourages learning and discovery. At IDEXX, you will be supported by competitive compensation, incentives, and benefits while enjoying purposeful work that drives improvement.
Let’s pursue what matters together.
IDEXX values a diverse workforce and workplace and strongly encourages women, people of color, LGBT individuals, people with disabilities, members of ethnic minorities, foreign-born residents, and veterans to apply.
IDEXX is an equal opportunity employer. Applicants will not be discriminated against because of race, color, creed, sex, sexual orientation, gender identity or expression, age, religion, national origin, citizenship status, disability, ancestry, marital status, veteran status, medical condition, or any protected category prohibited by local, state, or federal laws.
Show more
Show less","AWS, Snowflake, Hadoop, Databricks, Spark, R, Python, SQL, Data warehouses, Data virtualization, Semistructured datasets, Data management, Data storage, Data backup, Data recovery, Automated software deployment, Performance monitoring, Standards development, Best practices, Operational integration, Cloud platforms, Hadoopbased technologies, MapReduce, Spark, Hive, Presto, Pig, SQLbased technologies, Oracle, PostgreSQL, MySQL, NoSQL technologies, Cassandra, MongoDB, Data warehousing solutions, Relational database theory, Software APIs, Data pipelines, Distributed systems, Cloud computing","aws, snowflake, hadoop, databricks, spark, r, python, sql, data warehouses, data virtualization, semistructured datasets, data management, data storage, data backup, data recovery, automated software deployment, performance monitoring, standards development, best practices, operational integration, cloud platforms, hadoopbased technologies, mapreduce, spark, hive, presto, pig, sqlbased technologies, oracle, postgresql, mysql, nosql technologies, cassandra, mongodb, data warehousing solutions, relational database theory, software apis, data pipelines, distributed systems, cloud computing","automated software deployment, aws, best practices, cassandra, cloud computing, cloud platforms, data backup, data management, data recovery, data storage, data virtualization, data warehouses, data warehousing solutions, databricks, datapipeline, distributed systems, hadoop, hadoopbased technologies, hive, mapreduce, mongodb, mysql, nosql technologies, operational integration, oracle, performance monitoring, pig, postgresql, presto, python, r, relational database theory, semistructured datasets, snowflake, software apis, spark, sql, sqlbased technologies, standards development"
Data Engineer,Jobs for Humanity,"Springfield, MA",https://www.linkedin.com/jobs/view/data-engineer-at-jobs-for-humanity-3786356207,2023-12-17,East Hartford,United States,Mid senior,Onsite,"Company Description
Jobs for Humanity is partnering with MassMutual to build an inclusive and just employment ecosystem. Therefore, we prioritize individuals coming from the following communities: Refugee, Neurodivergent, Single Parent, Blind or Low Vision, Deaf or Hard of Hearing, Black, Hispanic, Asian, Military Veterans, the Elderly, the LGBTQ, and Justice Impacted individuals. This position is open to candidates who reside in and have the legal right to work in the country where the job is located.
Company Name: MassMutual
Job Description
At MassMutual, we believe in helping everyone achieve financial freedom. We are looking to hire data engineers to join our Data Management and Engineering team. We welcome people from all backgrounds, including the elderly, refugees, people with disabilities (visible and invisible), LGBTQIA+ individuals, and veterans.
What we are looking for
We are seeking passionate individuals who love working with data. As a Data Engineer, you will be involved in building data projects from scratch. You will collaborate with business partners to understand their requirements and develop robust solutions that meet high standards. We value individuals who are eager to learn new technologies and enjoy working in the cloud. Additionally, being a team player and a strong communicator is essential.
Key responsibilities
Design, build, and maintain complex data jobs that provide business value.
Translate high-level business requirements into technical specifications.
Ingest data from multiple sources into the data lake and data warehouse.
Cleanse, enrich, and ensure good data quality.
Provide insights and guide future development.
Support existing data warehouse applications and consumers, serving as a technical leader for the offshore team.
MassMutual's data platform
Develop reusable tools to streamline project delivery.
Collaborate closely with other developers and provide mentorship.
Evaluate and recommend tools, technologies, processes, and architectures.
Work in an Agile development environment, attending daily stand-up meetings and delivering incremental improvements.
Basic Qualifications
Bachelor’s degree in computer science, engineering, or a related field.
5+ years of experience with data warehousing and analytics.
Strong knowledge of SQL programming and query optimization.
Good understanding of ETL/ELT methodologies and tools.
Experience with troubleshooting and root cause analysis.
Excellent communication, problem-solving, organizational, and analytical skills.
Ability to work independently and lead small teams of developers.
Preferred Qualifications
Master’s degree in computer science, engineering, or a related field.
Experience working in a cloud environment (e.g., AWS).
Hands-on experience with Python.
Experience with data processing technologies like Apache Spark or Kafka.
Good knowledge of orchestration and scheduling tools (e.g., Apache Airflow).
Experience with data reporting tools (e.g., Microstrategy, Tableau, Looker) and data cataloging tools (e.g., Alation).
MassMutual is an equal opportunity employer. We welcome all individuals to apply, including those from minority groups, females, individuals with disabilities, LGBTQIA+ individuals, and veterans, regardless of discharge status. If you need assistance or accommodation during the application process, please contact us.
Show more
Show less","Data Engineering, Data Warehousing, Analytics, SQL, ETL/ELT, Cloud Computing, Python, Apache Spark, Kafka, Orchestration, Scheduling Tools, Apache Airflow, Data Reporting Tools, Data Cataloging Tools","data engineering, data warehousing, analytics, sql, etlelt, cloud computing, python, apache spark, kafka, orchestration, scheduling tools, apache airflow, data reporting tools, data cataloging tools","analytics, apache airflow, apache spark, cloud computing, data cataloging tools, data engineering, data reporting tools, datawarehouse, etlelt, kafka, orchestration, python, scheduling tools, sql"
Data Engineer,VLink Inc,"Springfield, MA",https://www.linkedin.com/jobs/view/data-engineer-at-vlink-inc-3775971599,2023-12-17,East Hartford,United States,Mid senior,Onsite,"Job Title: Data Engineer
Location: Springfield, MA ( Local ) preferred
Employment Type:
(Full-time or contract) Contract
Duration: 6+ Months
About VLink:
Started in 2006 and headquartered in Connecticut, VLink is one of the fastest growing digital technology services and consulting companies. Since its inception, our innovative team members have been solving the most complex business, and IT challenges of our global clients.
Job Description
Strong AWS Data Engineer who can bring in expertise and industry best practices define better development and Eng approaches. Should be able to accept new challenge and want to make a difference in the Healthcare Industry.
Responsibilities
Work with technical development team and team lead to understand desired application capabilities.
Continuously improve software engineering practices.
Work within and across Agile teams to test and support technical solutions across a full stack of development tools and technologies.
Develop applications in AWS - data and analytics technologies including but not limited to Glue, EMR, Lambda, Step Functions, CloudTrail, CloudWatch, SNS, SQS, S3, VPC, EC2, RDS, IAM.
Application development by lifecycles, & continuous integration/deployment practices.
Working to integrate open-source components into data-analytic solutions.
Working with vendors to enhance tool capabilities to meet enterprise needs.
Willingness to continuously learn & share learnings with others.
Requirements / Qualifications:
You will expand your experience in various areas of mental & behavioral health.
Candidate should have hands on experience with AWS services such as Glue, EMR, Lambda, Step Functions, CloudTrail, CloudWatch, SNS, SQS, S3, VPC, EC2, RDS, IAM.
Proficient in at least one programming language (Python, Scala, Java etc)
Experience in ETL / Data application development and version control systems such as Git.
Knowledge of application development lifecycles, & continuous integration/deployment practices.
5 - 7 years' experience delivering and operating large scale, highly visible distributed systems.
Knowledge of IAC using terraform is preferred.
Snowflake MPP and graph database experience is preferred but not mandatory.
Employment Practices
EEO, ADA, FMLA Compliant
VLink is an equal opportunity employer. At VLink, we are committed to embracing diversity, multiculturalism, and inclusion. VLink does not discriminate on the basis of race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law. All aspects of employment including the decision to hire, promote, or discharge, will be decided on the basis of qualifications, merit, performance, and business needs.
Show more
Show less","AWS, Glue, EMR, Lambda, Step Functions, CloudTrail, CloudWatch, SNS, SQS, S3, VPC, EC2, RDS, IAM, Python, Scala, Java, Git, Terraform, Snowflake, MPP, Graph database","aws, glue, emr, lambda, step functions, cloudtrail, cloudwatch, sns, sqs, s3, vpc, ec2, rds, iam, python, scala, java, git, terraform, snowflake, mpp, graph database","aws, cloudtrail, cloudwatch, ec2, emr, git, glue, graph database, iam, java, lambda, mpp, python, rds, s3, scala, snowflake, sns, sqs, step functions, terraform, vpc"
MDM Data Engineer,Jobs for Humanity,"Springfield, MA",https://www.linkedin.com/jobs/view/mdm-data-engineer-at-jobs-for-humanity-3786351877,2023-12-17,East Hartford,United States,Mid senior,Onsite,"Company Description
Jobs for Humanity is partnering with MassMutual to build an inclusive and just employment ecosystem. Therefore, we prioritize individuals coming from the following communities: Refugee, Neurodivergent, Single Parent, Blind or Low Vision, Deaf or Hard of Hearing, Black, Hispanic, Asian, Military Veterans, the Elderly, the LGBTQ, and Justice Impacted individuals. This position is open to candidates who reside in and have the legal right to work in the country where the job is located.
Company Name: MassMutual
Job Description
Job Title: Data Integration Specialist
About The Company
We are a diverse and inclusive company that values all individuals. At MassMutual, we are committed to equal employment opportunity and welcome applicants from all backgrounds and walks of life. We also provide accommodations for applicants who require assistance during the application process.
Job Description
We are looking for a Data Integration Specialist to join our team. In this role, you will be responsible for designing and building complex Informatica Power Center and MDM processes to master data in various domains. You will be working on projects that involve data collection, processing, modeling, and master data solutions to meet specific business needs.
Responsibilities
Design, build, and evaluate complex ELT/ETL tasks to integrate data from different sources into a reliable data repository.
Develop Master Data Management (MDM) solutions for various domains.
Provide comprehensive support for MDM solutions, including data ingestion, master record creation, data extracts, and API integration.
Implement data modeling standards and processes.
Assist in documenting technical data and system information.
Establish data quality benchmarks and develop tools/processes for data accuracy.
Perform data profiling and analysis.
Collaborate with different departments to understand data patterns.
Translate business needs into technical specifications.
Contribute to the development and implementation of MDM Solutions and other Information Management initiatives.
Requirements
Basic Qualifications:
Bachelor’s degree in computer science or engineering.
5+ years of experience with Informatica Power Center.
5+ years of experience with data analytics, data modeling, and database design.
3+ years of coding and scripting experience (Python, Java, Scala).
3+ years of experience with Informatica MDM platform in customer/party subject area.
Experience with ELT methodologies and tools.
Expertise in tuning and troubleshooting SQL.
Strong data integrity, analytical, and multitasking skills.
Experience with Oracle database.
Experience with AWS.
Knowledge of basic UNIX commands and shell scripts.
Experience with 3rd party job schedulers like Maestro.
Experience with RESTful APIs.
Experience with near real-time mastering via SIF.
Experience with data profiling tools.
Willingness to provide support for batch processing cycles outside business hours.
Excellent communication, problem-solving, organizational, and analytical skills.
Able to work independently.
Authorized to work in the USA with or without sponsorship.
Preferred Qualifications:
Master’s degree in computer science or engineering.
Experience with agile project delivery process.
Knowledge of SQL for data access and analysis.
Ability to manage diverse projects impacting multiple roles and processes.
Able to identify and troubleshoot data gaps and issues.
Ability to adapt to a fast-changing environment.
Experience with Python.
Experience with Kafka.
Basic knowledge of database technologies (Vertica, Redshift, etc.).
Experience designing and implementing automated ETL processes.
How To Apply
If you are interested in this position, please submit your application through our online application portal. If you require any accommodations during the application process, please contact us and let us know how we can assist you.
Show more
Show less","Informatica Power Center, MDM, ELT/ETL, Python, Java, Scala, Informatica MDM, Oracle, AWS, UNIX, Shell scripts, Maestro, RESTful APIs, SIF, Data profiling tools, SQL","informatica power center, mdm, eltetl, python, java, scala, informatica mdm, oracle, aws, unix, shell scripts, maestro, restful apis, sif, data profiling tools, sql","aws, data profiling tools, eltetl, informatica mdm, informatica power center, java, maestro, mdm, oracle, python, restful apis, scala, shell scripts, sif, sql, unix"
"Software Data Engineer, Java",Jobs for Humanity,"Springfield, MA",https://www.linkedin.com/jobs/view/software-data-engineer-java-at-jobs-for-humanity-3786353531,2023-12-17,East Hartford,United States,Mid senior,Onsite,"Company Description
Jobs for Humanity is partnering with MassMutual to build an inclusive and just employment ecosystem. Therefore, we prioritize individuals coming from the following communities: Refugee, Neurodivergent, Single Parent, Blind or Low Vision, Deaf or Hard of Hearing, Black, Hispanic, Asian, Military Veterans, the Elderly, the LGBTQ, and Justice Impacted individuals. This position is open to candidates who reside in and have the legal right to work in the country where the job is located.
Company Name: MassMutual
Job Description
Job Description
At MassMutual, we are committed to helping people protect their families, support their communities, and assist one another. We believe in inspiring people to come together and live in mutual support. We welcome a diverse range of individuals from all backgrounds and perspectives, including the elderly, refugees, people with disabilities, LGBTQIA+ individuals, and veterans. Our goal is to create an inclusive environment where everyone can thrive.
What we are looking for in this role
We are seeking a candidate who enjoys designing, building, and delivering complex systems. You should have a passion for coding and solving challenging problems with innovative solutions. You should also have experience working with large-scale data and an eagerness to explore open-source tools. Collaboration and effective communication skills are essential as well.
Objectives of the role
Design, develop, and deliver scalable and robust components using technologies such as Python, Java, AWS serverless (Lambda, Glue), Apache Spark, Apache Kafka, and REST
Participate in all aspects of development from design to delivery, taking on roles as a developer and component lead
Interact closely with data users, such as data engineers and data scientists, to understand and refine requirements
Develop, test, and review code
Troubleshoot and debug issues in code and data pipelines
Evaluate and recommend tools, technologies, and processes to improve efficiency and enhance existing systems
Collaborate with other developers and provide mentorship when needed
Work closely with other teams to prevent and resolve technical issues
Contribute to an Agile development environment, attending daily stand-up meetings and delivering incremental improvements
Basic Qualifications
4+ years of Java development experience
Strong knowledge of algorithms, design patterns, and writing performant code
Good understanding of data processing tools like Spark, Kafka, and SQL, as well as relational and analytics databases
Experience with source control and CI/CD tools
Proficiency in writing unit, integration, and load tests
Excellent communication, problem-solving, organizational, and analytical skills
Ability to work independently and provide leadership to small development teams
Bachelor's degree or equivalent work experience
Preferred Qualifications
Experience building and deploying on cloud platforms like AWS and working with serverless architectures (Lambda, Glue)
2+ years of experience working with big data and/or streaming technologies like Spark, Kafka, and Flink
MassMutual is an equal opportunity employer. We welcome applicants regardless of race, gender identity, sexual orientation, disability, or veteran status. We value diversity and strive to create a workplace where everyone feels included and supported. Veterans are encouraged to apply, regardless of their discharge status. If you require any accommodations during the application process, please let us know so we can assist you accordingly.
Show more
Show less","Python, Java, AWS, Lambda, Glue, Apache Spark, Apache Kafka, REST, SQL, Relational databases, Analytics databases, Source control, CI/CD, Unit testing, Integration testing, Load testing, Cloud platforms, Serverless architectures, Big data, Streaming technologies","python, java, aws, lambda, glue, apache spark, apache kafka, rest, sql, relational databases, analytics databases, source control, cicd, unit testing, integration testing, load testing, cloud platforms, serverless architectures, big data, streaming technologies","analytics databases, apache kafka, apache spark, aws, big data, cicd, cloud platforms, glue, integration testing, java, lambda, load testing, python, relational databases, rest, serverless architectures, source control, sql, streaming technologies, unit testing"
Data Engineer -W2 role,VLink Inc,"Springfield, Massachusetts Metropolitan Area",https://www.linkedin.com/jobs/view/data-engineer-w2-role-at-vlink-inc-3778707812,2023-12-17,East Hartford,United States,Mid senior,Hybrid,"Job Description:
Strong AWS Data Engineer who can bring in expertise and industry best practices define better development and Eng approaches. Should be able to accept new challenge and want to make a difference in the Healthcare Industry.
Responsibilities:
Work with technical development team and team lead to understand desired application capabilities.
Continuously improve software engineering practices.
Work within and across Agile teams to test and support technical solutions across a full stack of development tools and technologies.
Develop applications in AWS - data and analytics technologies including but not limited to Glue, EMR, Lambda, Step Functions, CloudTrail, CloudWatch, SNS, SQS, S3, VPC, EC2, RDS, IAM.
Application development by lifecycles, & continuous integration/deployment practices.
Working to integrate open-source components into data-analytic solutions.
Working with vendors to enhance tool capabilities to meet enterprise needs.
Willingness to continuously learn & share learnings with others.
Requirements / Qualifications:
You will expand your experience in various areas of mental & behavioral health.
Candidate should have hands on experience with AWS services such as Glue, EMR, Lambda, Step Functions, CloudTrail, CloudWatch, SNS, SQS, S3, VPC, EC2, RDS, IAM.
Proficient in at least one programming language (Python, Scala, Java etc)
Experience in ETL / Data application development and version control systems such as Git.
Knowledge of application development lifecycles, & continuous integration/deployment practices.
5 - 7 years' experience delivering and operating large scale, highly visible distributed systems.
Knowledge of IAC using terraform is preferred.
Snowflake MPP and graph database experience is preferred but not mandatory.
Show more
Show less","AWS, Glue, EMR, Lambda, Step Functions, CloudTrail, CloudWatch, SNS, SQS, S3, VPC, EC2, RDS, IAM, Python, Scala, Java, Git, Terraform, Snowflake, MPP, Graph database","aws, glue, emr, lambda, step functions, cloudtrail, cloudwatch, sns, sqs, s3, vpc, ec2, rds, iam, python, scala, java, git, terraform, snowflake, mpp, graph database","aws, cloudtrail, cloudwatch, ec2, emr, git, glue, graph database, iam, java, lambda, mpp, python, rds, s3, scala, snowflake, sns, sqs, step functions, terraform, vpc"
"Data Scientist with Python (Hybrid- Hartford, Connecticut)",Cognizant,"Hartford, CT",https://www.linkedin.com/jobs/view/data-scientist-with-python-hybrid-hartford-connecticut-at-cognizant-3774225087,2023-12-17,East Hartford,United States,Mid senior,Hybrid,"We are Cognizant Artificial Intelligence
Digital technologies, including analytics and AI, give companies a once-in-a-generation opportunity to perform orders of magnitude better than ever before. However, clients need new business models built from analyzing customers and business operations at every angle to really understand them.
With the power to apply artificial intelligence and data science to business decisions via enterprise data management solutions, we help leading companies prototype, refine, validate and scale the most desirable products and delivery models to enterprise scale within weeks.
*You must be legally authorized to work in United States without the need of employer sponsorship, now or at any time in the future*
Python Data Scientist (Hybrid- Hartford, Connecticut)
10 to 14 years of experience
Technical Skills
Python
Nice to have skills.
SAS
PySpark
Machine Learning
Roles/Responsibilities:
Anyone with data scientist experience in the range of 2 to 4 years
Develops validates and executes algorithms and predictive models to investigate problems detect patterns and recommend solutions.
Explores examines and interprets large volumes of data in various forms.
Performs analyses of structured and unstructured data to solve moderately complex business problems. Utilizing advanced statistical techniques and mathematical analyses. Develops data structures and pipelines to organize collect and standardize data that helps generate insights and addresses reporting needs.
Uses data visualization techniques to effectively communicate analytical results and support business decisions.
Creates and evaluates the data needs of assigned projects and assures the integrity of the data. Explores existing data and recommends additional sources of data for improvements.
Documents projects including business objectives data gathering and processing detailed set of results and analytical metrics.
Primary Location
Hybrid 2 to 3 days per week at Hartford, Connecticut
Benefits:
Cognizant offers the following benefits for this position, subject to applicable eligibility requirements:
Medical/Dental/Vision/Life Insurance
Paid holidays plus Paid Time Off
401(k) plan and contributions
Long-term/Short-term Disability
Paid Parental Leave
Employee Stock Purchase Plan
Disclaimer:
The salary, other compensation, and benefits information is accurate as of the date of this posting. Cognizant reserves the right to modify this information at any time, subject to applicable law.
Show more
Show less","Python, Data Science, Machine Learning, SAS, PySpark, Data Visualization, Statistical Techniques, Mathematical Analyses, Data Structures, Data Pipelines, Data Integrity, Data Gathering, Data Processing, Analytical Metrics","python, data science, machine learning, sas, pyspark, data visualization, statistical techniques, mathematical analyses, data structures, data pipelines, data integrity, data gathering, data processing, analytical metrics","analytical metrics, data gathering, data integrity, data processing, data science, data structures, datapipeline, machine learning, mathematical analyses, python, sas, spark, statistical techniques, visualization"
Senior Data Scientist,Chubb,"Simsbury, CT",https://www.linkedin.com/jobs/view/senior-data-scientist-at-chubb-3756335183,2023-12-17,East Hartford,United States,Mid senior,Hybrid,"Job Description
This position offers the opportunity to leverage the entire spectrum of analytics tools towards solving a wide variety of business problems. The focus is on applying a combination of industry standard practices and innovative ideas using structured and unstructured data from multiple sources and applying AI, machine learning and statistical techniques to build, evaluate and implement models. Our team focus’ on creating analytical solutions that solve real world business problems for all lines of business. Analytics empowered technology platforms are changing the way we operationalize model outputs at Chubb. This role will work closely with engineering teams to deploy models as APIs into various cutting-edge platforms. This is a great opportunity to create analytic solutions for multiple lines of business, deepening your insurance product knowledge.
Responsibilities
Collaborate with business partners and peers in the organization to understand and scope the problem, gather business requirements, and plan projects tasks and timelines.
Partner with Product Owners and other data scientists to drive technical execution of modeling projects (from scoping, model development & review to implementation).
Execute all aspects of analytics initiatives including exploratory data analysis, machine learning model development, model evaluation and benefit estimation.
Build modeling frameworks that can be leveraged for multiple projects. High focus on code reproducibility and creating generalizable technology assets for analytics teams.
Create excellent working relationships with business partners across the Chubb organization, IT and analytics peer groups.
Effectively communicate results in written, oral and presentation formats.
Monitor performance of models and report findings to leadership.
Qualifications
7+ years analytics background with programming experience in Python or R required.
3+ years experience with predictive modelling in a P&C environment.
Experienced with development and implementation of machine learning and statistical models.
Experience hosting machine learning models as APIs preferred (i.e. experience with Docker, MLFlow, or Flask)
Collaboration, knowledge sharing and keeping up to date on emerging industry trends.
Passion for solving business problems through analytics.
A good understanding of databases and approaches to extracting data.
Desire to work in a fast passed, team-oriented environment.
Education
Bachelor’s or Master’s degree in a computational science or statistical field preferred.
The pay range for the role is $80,500 to $137,000. The specific offer will depend on an applicant’s skills and other factors. This role may also be eligible to participate in a discretionary annual incentive program. Chubb offers a comprehensive benefits package, more details on which can be found on our careers website . The disclosed pay range estimate may be adjusted for the applicable geographic differential for the location in which the position is filled.
About Us
Chubb is the world’s largest publicly traded property and casualty insurer. With operations in 54 countries, Chubb provides commercial and personal property and casualty insurance, personal accident and supplemental health insurance, reinsurance, and life insurance to a diverse group of clients. The company is distinguished by its extensive product and service offerings, broad distribution capabilities, exceptional financial strength, underwriting excellence, superior claims handling expertise and local operations globally.
At Chubb, we are committed to equal employment opportunity and compliance with all laws and regulations pertaining to it. Our policy is to provide employment, training, compensation, promotion, and other conditions or opportunities of employment, without regard to race, color, religious creed, sex, gender, gender identity, gender expression, sexual orientation, marital status, national origin, ancestry, mental and physical disability, medical condition, genetic information, military and veteran status, age, and pregnancy or any other characteristic protected by law. Performance and qualifications are the only basis upon which we hire, assign, promote, compensate, develop and retain employees. Chubb prohibits all unlawful discrimination, harassment and retaliation against any individual who reports discrimination or harassment.
Show more
Show less","Python, R, Predictive modeling, Machine learning, Statistical models, Docker, MLFlow, Flask, Databases, Data extraction, Collaboration, Knowledge sharing, Emerging industry trends, Analytics, Bachelor's degree, Master's degree, Computational science, Statistics","python, r, predictive modeling, machine learning, statistical models, docker, mlflow, flask, databases, data extraction, collaboration, knowledge sharing, emerging industry trends, analytics, bachelors degree, masters degree, computational science, statistics","analytics, bachelors degree, collaboration, computational science, data extraction, databases, docker, emerging industry trends, flask, knowledge sharing, machine learning, masters degree, mlflow, predictive modeling, python, r, statistical models, statistics"
Analyst of Ecommerce & Data Science,Vuori,"Carlsbad, CA",https://www.linkedin.com/jobs/view/analyst-of-ecommerce-data-science-at-vuori-3753079762,2023-12-17,Escondido,United States,Associate,Onsite,"Company Description
Vuori is re-defining what athletic apparel looks like: built to move and sweat in but designed with a casual aesthetic to transition into everyday life. We draw inspiration from an active coastal California lifestyle; an integration of fitness, creative expression and life. Our high energy fast paced retail environment is reflected in the clothes we make. We aim to inspire others to take on all aspects of their lives with clarity, enthusiasm and purpose…while having a lot of fun along the way. We are proud to be an outlet for opportunity and for personal growth and success.
Job Description
This position will play a key role on Vuori’s Ecommerce Team as we look to build the analytics infrastructure necessary to accelerate growth, optimize customer acquisition and retention strategies and develop proprietary measurement capabilities. The Analyst of Ecommerce & Data Science will be expected to take a hands-on approach and become a master of Vuori’s Ecommerce Data, develop data frameworks and reporting tables that will be utilized by our Analysts and partner closely with our Data Enterprise Team to develop data pipelines to fuel our reporting needs.
In the short/medium term (6 months to 1 year), the Analyst will primarily be responsible for the tactical movement of data, data cleansing, data integrity management and data preparation in support of our Analysts and Data Modeling initiatives. The Analyst will be expected to work closely with our first-party customer and sales data, third party vendor data (such as Facebook and Google Ads) and session data to develop unified tables and schemas. Projects that her/his work will support include but are not limited to:
Ecommerce KPI Ownership
Customer Cohort and Omni Channel Customer Analysis
Media Mix Modeling
Market Basket Analyses
CDP (Customer Data Platform)
Data Activation and Automation.
The Analyst will collaboration extensively with our Central Data Enterprise & Analytics Team from conceptualization to execution of new analytic ideas, data source integrations and data pipeline development and management. The Analyst will be expected to be hands-on with Vuori’s data and spend the majority of their first in in data preparation and management.
In the long term (1 year+), the Analyst will pivot to manage endeavors that will directly impact the data-driven decision-making and strategies that drive Ecommerce. They will work directly on and own analyses that drive insights into Ecommerce Channel Optimization and Customer Strategies (both acquisition and retention). Additionally, rather than just preparing data for modeling initiatives, the Analyst will play a key role in model ownership and application across all Ecommerce Channels.
The Ecommerce Analytics team is cross-functional and collaborates regularly with a variety of teams both within ecommerce (Performance Marketing, Retention Marketing, Media and Site Experience) as well as teams outside of ecommerce (Retail, Real Estate, Finance, Product, Planning, Merchandising). It is essential that all candidates be team players, eager to collaborate and excited to build solutions to drive the business forward.
Short/Mid Term (6 Months to 1 Year):
This position will be a hands-on lead who empowers the Ecommerce Analytics Team with the necessary date for Analytics, Insights and Data Science/Modeling
This position will own the tactical movement of and structuring of Ecommerce Data
Areas of responsibilities will include data integrity management, data cleansing, reporting support, data preparation and data pipeline development and maintenance (in partnership with Central Data Enterprise Team).
Short/Mid Term primary focus will be on preparing and maintaining the data necessary to support our Analysts, Stakeholders and Modeling Initiatives to ensure we have accurate and actionable data.
Short/Mid Term secondary focus will be on developing automations to assist the broader Ecommerce Team on minimizing manual tasks and data preparation
Short/Mid Term tertiary focus will be on supporting stakeholders with reporting and dashboarding development (as needed on ad hoc basis)
This role will partner closely with our Central Data Enterprise Team (VP = Himanshu Shekhar) on supporting engineering development for Ecommerce and utilization of core reporting tables (managed by Central Data Enterprise Team).
Long Term (1 year+)
After the framework for Ecommerce Data has been developed and executed and a clear partnership has been established with the Central Data Enterprise Team, this role will pivot more into business analytics and insights.
In the short term, the majority of their time was spent in data preparation, but in the long-term the role will shift into running Ecommerce Data Models, managing our Customer Data Model and surfacing key insights on acquisition, retention and channel performance to our stakeholders (both internal and external).
Responsibilities include but are not limited to:
Collaborate with ecommerce, analytics, engineering and technology teams to build data models and transformations to enhance our reporting capabilities.
Serve as a subject matter expert for all ecommerce and demand-based data pipelines
Design solutions that support the integrity of ecommerce data across our ecosystem of platforms and reports
Lead QA initiatives on Ecommerce Data, KPIs and Reporting
Pioneer automation efforts that enable the Ecommerce Analytics and broader Ecommerce team to spend less time procuring and validating data and spend more time analyzing
Prepare and clean first-party and cross channel data for utilization in Media Mix Models
Develop solutions to democratize data to the broader ecommerce team with self-service options and capabilities
Develop documentation for all relevant reporting and data models to ensure accuracy, consistency, and visibility
Build and maintain daily weekly, monthly and quarterly reporting for the E-Commerce channel
Support ecommerce forecasting initiatives and partner with team leads on monthly forecast updates
Qualifications
Expertise in SQL and data warehousing (Snowflake)
Proficient in data manipulation and data-wrangling
Expertise in ETL solutions including analyzing, cleansing, joining and automating data sets
Experience in working with Data Engineering Teams on developing and building data pipelines
Experience with the Google Marketing Cloud (BigQuery)
Understanding of the core concepts of Digital Marketing, Social Ad Platforms, Web Analytics Tools (Google Analytics) and other advertising technologies
Experience in apparel or omni-channel commerce business preferred
Attention to detail and strong desire to vet all inputs and outputs for accuracy and reliability
Ability to manage projects independently and set deliverables and expectations with stakeholders
Proven ability to work and communicate cross-functionally between business and technical stakeholders
Ability to develop accurate, user-friendly and scalable data visualizations (Tableau, Looker, PowerBI)
Google Sheets Power User
Experience with Tracking (Google Tag Manager) and data privacy (GDPR + CCPA) an added plus
Additional Information
Pay Range:
$100,000 - $120,000
Benefits:
Health Insurance
Paid Time Off
Employee Discount
401(k)
All your information will be kept confidential according to EEO guidelines.
Show more
Show less","SQL, Data warehousing, Snowflake, Data wrangling, ETL, Data pipelines, Google Marketing Cloud, BigQuery, Digital Marketing, Social Ad Platforms, Web Analytics Tools, Google Analytics, Retail, Fashion, Tableau, Looker, PowerBI, Data visualizations, Google Sheets, Google Tag Manager, GDPR, CCPA","sql, data warehousing, snowflake, data wrangling, etl, data pipelines, google marketing cloud, bigquery, digital marketing, social ad platforms, web analytics tools, google analytics, retail, fashion, tableau, looker, powerbi, data visualizations, google sheets, google tag manager, gdpr, ccpa","bigquery, ccpa, data visualizations, data wrangling, datapipeline, datawarehouse, digital marketing, etl, fashion, gdpr, google analytics, google marketing cloud, google sheets, google tag manager, looker, powerbi, retail, snowflake, social ad platforms, sql, tableau, web analytics tools"
Business Data Analyst,Airspace,"Carlsbad, CA",https://www.linkedin.com/jobs/view/business-data-analyst-at-airspace-3785138359,2023-12-17,Escondido,United States,Mid senior,Onsite,"Company Introduction:
Airspace is a tech-enabled freight forwarder that's redefining how the world's most critical packages are delivered. Headquartered in Carlsbad, California, Airspace has employees who are based around the world. Our European headquarters is in Amsterdam, The Netherlands.
As a recognized leader in AI and machine learning, our team leverages data and patented technology to coordinate logistics across a global network of drivers and airlines. Our goal is to deliver those packages that are truly mission-critical in a way that is faster, more transparent, more secure, and more accountable than ever before. The items we deliver range from organs for transplant, to parts for critical machinery including grounded aircraft and highly sensitive components such as semiconductors.
Airspace has been rated one of America's best Startup Employers, listed as one of CNBC's Disruptor 50 companies, and featured as an Innovation and Disruption leader by CBS News. Airspace has the support of leading investors such as Telstra Ventures, HarbourVest Partners, Defy Partners, DBL Partners, and Scale Ventures. To date the company has raised more than $140m.
The company is growing rapidly and serving more places around the world than ever before. We are looking for passionate, motivated individuals who want to make an IMPACT every day to help us execute on our mission of reshaping the world of time-critical logistics.
What we do:
Airspace is a tech-enabled freight forwarder that's redefining how the world's most critical packages are delivered. Headquartered in Carlsbad, California, Airspace has employees who are based around the world. Our European headquarters is in Amsterdam, The Netherlands.
As a recognized leader in AI and machine learning, our team leverages data and patented technology to coordinate logistics across a global network of drivers and airlines. Our goal is to deliver those packages that are truly mission-critical in a way that is faster, more transparent, more secure, and more accountable than ever before. The items we deliver range from organs for transplant to parts for critical machinery, including grounded aircraft and highly sensitive components such as semiconductors.
Airspace has been rated one of America's best Startup Employers, listed as one of CNBC's Disruptor 50 companies, and featured as an Innovation and Disruption leader by CBS News. Airspace has the support of leading investors such as Telstra Ventures, HarbourVest Partners, Defy Partners, DBL Partners, and Scale Ventures. To date the company has raised more than $140m.
The company is growing rapidly and serving more places around the world than ever before. We are looking for passionate, motivated individuals who want to make an IMPACT every day to help us execute our mission of reshaping the world of time-critical logistics.
Join Our Team:
At Airspace, the Data & Analytics team is a pivotal force, steering the course of our product and business decisions with insights and intelligence. We're on the hunt for passionate data enthusiasts who thrive on uncovering vital insights from complex data sets to revolutionize the world of time-critical logistics.
Your Role and Responsibilities:
Strategic Liaison: Act as a key connector between our Analytics and Revenue teams, applying your analytical skills to harmonize data insights with business objectives. Immerse yourself in understanding our business and client needs and devise innovative data-driven solutions to maximize value for ourselves and our clients.
Storyteller with Data: Use your analytical acumen to craft compelling narratives that highlight Airspace's value to customers. Your insights will be essential in conducting impactful Quarterly Business Reviews with our top clients.
Insight Generator: Apply your skills in data processing, quantitative analysis, and visualization to create and communicate influential insights to stakeholders.
KPI Monitor and Advisor: Develop dashboards, reports, and models to keep a pulse on customer-centric KPIs, understand the dynamics behind them, and propose actionable recommendations.
Qualifications:
Experience: At least 2 years in a role focused on quantitative analysis and reporting.
Technical Proficiency: Strong skills in SQL, Excel/GSheets, and familiarity with Looker, Tableau, or similar BI tools. An understanding of data visualization best practices is key.
Analytical Translator: Ability to turn complex, vague data into clear, actionable insights for non-technical audiences.
Proactive and Detail-Oriented: A self-motivated individual with a keen eye for detail.
Continuous Learner: Eager to learn new technologies, methods, and approaches to problem-solving. Demonstrated experience quickly picking up and mastering new tools.
Data Modeling Skills: Experience with Python or R for data manipulation and modeling is a valuable plus.
Compensation:
Competitive salary.
High-quality health, vision, and dental care plans.
Unlimited PTO.
401K company contribution program.
Professional training and education reimbursements.
Generous employee perks
Why Choose Airspace?
Join us at Airspace and be part of a team that not only pioneers in logistics with cutting-edge analysis but also treasures the power of collaboration. In our dynamic environment, your work goes beyond data and numbers; it directly impacts the fast-paced world of time-critical logistics. Here, every analysis you perform and every insight you provide helps shape innovative solutions that revolutionize the industry.
We pride ourselves on a culture that is grounded in teamwork and mutual support. As a valued member of our team, you will collaborate closely with diverse groups, sharing ideas and combining expertise to achieve common goals. This collaborative spirit ensures that your work is not just a solitary pursuit but a shared journey towards making a tangible difference in a vital industry.
Salary Range:
$75,000 - $100,000
Core Values:
We are One Team. We believe we all accomplish more when we are working together.
We make an Impact. We are determined to have a positive influence on our environment, our customers, our industry, and our world.
We are Passionate. We care deeply about our mission and are not afraid to raise the bar.
We are Transparent. We pride ourselves on having open, honest, and sincere communication with our team and customers.
We are Innovative. We never settle and are always striving to improve our product, service, and ourselves.
About Airspace:
From life-saving organs to essential machinery components, Airspace is trusted by the world's largest companies and most critical healthcare organizations to move their most time-sensitive shipments on time, every time. Our proprietary AI-powered platform is the most advanced of its kind- awarded and protected by multiple patents, it provides speed, reliability, and transparency unrivaled in time-critical logistics. We are thinkers, builders, and doers; from building and deploying AI in the world to assembling a world-class operations team, Airspace is on a hypergrowth trajectory while remaining hyper-focused on the needs of our customers and team members.
With offices in the United States in Carlsbad, CA, Dallas, TX, and in Europe in Amsterdam, Frankfurt, Stockholm, and London, we are rapidly scaling into new markets and industries while continuing to innovate and maximize value for our customers. Backed by leading investors including Telstra, HarbourVest, Prologis, Qualcomm, Defy, and others, Airspace has raised $140M to date.
Join our team of 300+ technologists, futurists, and industry veterans as we work as One Team to revolutionize time-critical logistics.
Airspace is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. Additionally, Airspace participates in the E-Verify program for all locations.
For this role the acquisition of recruitment agencies is not appreciated, thank you for your understanding.
Show more
Show less","SQL, Data Visualization, Looker, Tableau, BI Tools, Python, R, Data Modeling, Excel, GSheets","sql, data visualization, looker, tableau, bi tools, python, r, data modeling, excel, gsheets","bi tools, datamodeling, excel, gsheets, looker, python, r, sql, tableau, visualization"
Senior Engineer - Data Management & MDM,Energy Jobline,"Carlsbad, CA",https://www.linkedin.com/jobs/view/senior-engineer-data-management-mdm-at-energy-jobline-3783596811,2023-12-17,Escondido,United States,Mid senior,Onsite,"Job Description
We are seeking a highly skilled and experienced Senior Engineer specializing in Data Management and Master Data Management (MDM). In this role, you will be responsible for designing, implementing, and maintaining robust data management solutions, with a focus on ensuring the integrity, quality, and accessibility of organizational data. Your expertise in MDM will be crucial in establishing and maintaining a centralized and consistent view of master data across the enterprise. Experience in Retail apparel space would be valuable in driving industry specific governance and data management techniques.
Responsibilities Include But Are Not Limited To
Data Architecture and Design:
Collaborate with cross-functional teams to understand data requirements and design scalable and efficient data architectures.
Develop and implement data models, ensuring compatibility and integration with existing systems.
Master Data Management (MDM)
Lead the implementation and maintenance of MDM solutions to create and manage a single, accurate, and consistent view of master data.
Define and enforce data governance policies and procedures related to master data.
Data Quality Assurance
Establish and implement data quality standards and procedures to ensure the accuracy and completeness of data.
Develop and maintain data quality monitoring processes and resolve data quality issues.
Data Integration
Design and implement data integration solutions to ensure seamless flow of data between different systems and platforms.
Work on ETL (Extract, Transform, Load) processes to integrate data from various sources.
Data Security And Compliance
Implement data security measures to safeguard sensitive information.
Ensure compliance with relevant data protection and privacy regulations.
Collaboration And Communication
Collaborate with business stakeholders, analysts, and other IT teams to understand data requirements and deliver effective solutions.
Communicate complex technical concepts to non-technical stakeholders.
Documentation
Create and maintain comprehensive documentation for data management processes, data models, and MDM solutions.
Continuous Improvement
Stay abreast of industry trends and emerging technologies in data management and MDM.
Identify opportunities for process improvement and implement best practices.
Show more
Show less","Data Architecture, Data Design, Data Modeling, Master Data Management (MDM), Data Governance, Data Quality Assurance, Data Quality Monitoring, Data Integration, Data Warehousing, ETL (Extract Transform Load), Data Security, Data Privacy, Collaboration, Communication, Documentation, Continuous Improvement","data architecture, data design, data modeling, master data management mdm, data governance, data quality assurance, data quality monitoring, data integration, data warehousing, etl extract transform load, data security, data privacy, collaboration, communication, documentation, continuous improvement","collaboration, communication, continuous improvement, data architecture, data design, data governance, data integration, data privacy, data quality assurance, data quality monitoring, data security, datamodeling, datawarehouse, documentation, etl extract transform load, master data management mdm"
Staff Data Engineer,Recruiting from Scratch,"San Diego, CA",https://www.linkedin.com/jobs/view/staff-data-engineer-at-recruiting-from-scratch-3744395273,2023-12-17,Escondido,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our client is a dating app.
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, deliver analysis to further improve engagement in existing features, and empower our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Solve technical problems of the highest scope and complexity
Exert significant influence on the company’s analytical long-range goals and data architecture
Define and extend our internal standards for style, maintenance, and best practices for a high-scale data platform
Provide mentorship for all on your team to help them grow in their technical responsibilities
Propose ideas to improve the scale, performance, and capabilities of the Data Platform
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We'll Love About You
7+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field
7+ years experience using Python
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention
Location
: This role will be located in office twice a week minimum in Palo Alto, San Francisco or Chicago.
Salary Range: $180,000 - 250,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, SQL, TDD, Pair Programming, Continuous Integration, Automated Testing, Kafka, Storm, SparkStreaming, Dimensional Data Modeling, Schema Design, Data Warehouses, ETL, Data Classification, Data Retention","python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, tdd, pair programming, continuous integration, automated testing, kafka, storm, sparkstreaming, dimensional data modeling, schema design, data warehouses, etl, data classification, data retention","airflow, automated testing, continuous integration, data classification, data retention, data warehouses, dimensional data modeling, docker, etl, helm, kafka, kubernetes, pair programming, python, schema design, snowflake, spark, sparkstreaming, sql, storm, tdd"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"San Diego, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3744396135,2023-12-17,Escondido,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
They are a profitable dating application.
Location: This is a hybrid role based in their Chicago office and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location
: This role is twice a week in a hybrid role minimum.
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Business Intelligence, Data Science, Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, SQL, TDD, Pair Programming, Continuous Integration, Automated Testing, Deployment, Kafka, Storm, SparkStreaming, Dimensional Data Modeling, Schema Design, Data Warehouses, ETL, Data Management, Data Classification, Data Retention","data engineering, business intelligence, data science, python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, tdd, pair programming, continuous integration, automated testing, deployment, kafka, storm, sparkstreaming, dimensional data modeling, schema design, data warehouses, etl, data management, data classification, data retention","airflow, automated testing, business intelligence, continuous integration, data classification, data engineering, data management, data retention, data science, data warehouses, deployment, dimensional data modeling, docker, etl, helm, kafka, kubernetes, pair programming, python, schema design, snowflake, spark, sparkstreaming, sql, storm, tdd"
Sr Master Data Systems Analyst,Callaway Golf,"Carlsbad, CA",https://www.linkedin.com/jobs/view/sr-master-data-systems-analyst-at-callaway-golf-3771192467,2023-12-17,Escondido,United States,Mid senior,Onsite,"Topgolf Callaway Brands is an unrivaled tech-enabled Modern Golf and active lifestyle company while also being a great place to work. With a portfolio of global brands including Topgolf, Callaway Golf, TravisMathew, Toptracer, Odyssey, OGIO, Jack Wolfskin, and World Golf Tour (“WGT”), we have a team of passionate individuals who dare to be great while acting with integrity and respect. We stay hungry, yet humble. All while having fun and making golf accessible and enjoyable for everyone!
Our company is a blend of experience and diverse backgrounds, and our leaders have a strong history of building and selling successful initiatives. We are working to build a truly groundbreaking company, and we want top-notch people to join us in that mission.
Job Overview
Under general supervision, the Sr. Master Data Systems Analyst is responsible for gathering all relevant Master Data information related to product lines from R&D, Sales, Manufacturing, Finance, and other departments, analyzing the business process and system requirement impacts, and preparing and entering the appropriate data accurately into the SAP Master Data system for various product lines both domestic and international. Incumbent is responsible for insuring data accuracy through continuous audits and analysis of existing data, and conducting and facilitating project work in conjunction with Master Data processes and product accuracy.
Roles And Responsibilities
Responsible for working with other business areas to define requirements needed for new business processes, as well as train all new personnel on department policies and procedures.
Responsible for the accuracy of system data through process evaluation and conducting regular audits. Identify and implement corrective measures for data or process errors based on audit results and process evaluations.
Coordinate efforts among the data owners to deliver their data in a timely fashion to the Master Data team so part numbers and material master records can be created efficiently and in a timely fashion.
Work on projects with various business areas to streamline processes and eliminate waste. These projects would be moderately complex and affect a wide range of cross-functional areas.
Ensure thorough, complete, and timely communication with all necessary departments to verify completion of commitments, notify all parties of any slippage issues, and provide updates on timelines and responsibilities.
Prepare appropriate data entry worksheets based on analysis of product specifications, pricing requirements, materials flows, and other relevant factors and understanding of business process and data relationships in SAP.
Enter data into SAP Master Data and verify 100% accuracy for the following: Material Masters, BOMs, Routings, Purchasing information records, Global extensions, Product groups, and other related data.
Assess the need for any changes to SAP configuration or processes to accommodate required business data.
Execute SAP Cockpit program to generate data for new product lines.
Demonstrate effective and collaborative communication with internal team members and Business Partners.
Apply standard department audits and comply with Corporate Audit standards.
Actively work with peers and manager to develop and document all business process flows and department training documentation.
Assist in design & development of training procedures and tools.
TECHNICAL COMPETENCIES (Knowledge, Skills & Abilities)
Advanced SAP Master Data skills and knowledge of all relevant SAP Master Data setup to include Material Master, Customer Master, Purchasing Master, Storage Locations, Intercompany Purchase Info Records, Bills Of Material, Routings, Material Determination Codes, etc.
Knowledge of running mass changes in SAP (MM17) including developing CATTSCRIPTS, table lookups, and analysis tools in SAP for performing data audits.
Thorough and complete knowledge of industry practices, standards, and techniques, and all Master Data components and business system impacts, preferably in a medium to large manufacturing environment. Will be required to regularly apply the full array of all relevant principles and concepts in execution of duties.
Thorough systems expertise in desktop & MRP/ERP applications.
High degree of initiative and ownership, as well as a proven history of delivering results while working with several different departments in a fast-paced environment.
Excellent communication skills both oral and written, including ability to work with business area representatives across all levels to translate business requirements into technical needs.
Proficiency in MS Excel at an advanced level, including spread sheet creation, editing and exporting to other applications utilizing macros and pivot tables.
Excellent organizational, analytical, and problem solving skills.
Able to work extended hours as needed.
Solid project management skills.
Education And Experience
Bachelors degree or equivalent additional relevant work experience, required.
Minimum 5 years experience in relevant field, preferably in a Manufacturing environment.
Relevant professional certifications (APICS or other as appropriate), or certification in progress a plus.
Callaway Golf is an Equal Opportunity Employer. If your experience is close to what we’re looking for, please consider applying. Experience comes in many forms, skills are transferable, and passion goes a long way. We know that diverse backgrounds and experiences make for the best problem-solving and creative thinking, which is why we’re dedicated to adding new perspectives to the team and encourage everyone to apply. We look forward to learning more about you.
Show more
Show less","SAP Master Data, Material Master, Customer Master, Purchasing Master, Storage Locations, Intercompany Purchase Info Records, Bills Of Material, Routings, Material Determination Codes, CATTSCRIPTS, Table lookups, Analysis tools, MRP/ERP applications, MS Excel, Spread sheet creation, Editing, Exporting to other applications, Macros, Pivot tables, Project management","sap master data, material master, customer master, purchasing master, storage locations, intercompany purchase info records, bills of material, routings, material determination codes, cattscripts, table lookups, analysis tools, mrperp applications, ms excel, spread sheet creation, editing, exporting to other applications, macros, pivot tables, project management","analysis tools, bills of material, cattscripts, customer master, editing, exporting to other applications, intercompany purchase info records, macros, material determination codes, material master, mrperp applications, ms excel, pivot tables, project management, purchasing master, routings, sap master data, spread sheet creation, storage locations, table lookups"
"Senior Engineer, Data Management",Energy Jobline,"Carlsbad, CA",https://www.linkedin.com/jobs/view/senior-engineer-data-management-at-energy-jobline-3786096987,2023-12-17,Escondido,United States,Mid senior,Hybrid,"Job Description
As a Senior Engineer – Data Management, you will be responsible for designing and implementing the data architecture, infrastructure, and tools to support the organization's data processing and analysis needs. You will work closely with data scientists, analysts, and other stakeholders to understand data requirements and ensure the efficient flow of data from various sources to data repositories. Your role will involve optimizing data systems and building scalable solutions to enable advanced analytics and business intelligence.
Data Architecture Design
Responsibilities include but are not limited to:
Design and implement scalable, high-performance data pipelines and data architecture.
Collaborate with data scientists and analysts to understand data requirements and design solutions accordingly.
Ensure data architecture meets the organization's security, compliance, and performance standards.
Data Integration And Processing
Develop and maintain ETL (Extract, Transform, Load) processes for ingesting and processing structured and unstructured data.
Integrate data from various sources, ensuring data quality and accuracy.
Optimize data processing systems for performance and reliability.
Database Management
Manage and maintain databases, ensuring data integrity, security, and performance.
Implement data partitioning, indexing, and optimization strategies.
Troubleshoot and resolve database-related issues.
Data Technologies
Work with data technologies such as Spark, Snowflake, Azure Data Factory etc
Implement and optimize distributed computing and storage solutions.
Support
Enable best in class, reliable and quality driven solutions that are automated and monitored
Provide support for issues with data pipes, restart-ability and data issues from sources
Work with Analytics and Application teams to ensure end to end data integrity & reliability
Collaboration
Collaborate with cross-functional teams, including data scientists, analysts, and software engineers.
Provide technical leadership and mentorship to junior team members.
Documentation
Document data engineering processes, architectures, and solutions.
Create and maintain documentation for data workflows and systems.
Show more
Show less","Data Architecture Design, Data Integration And Processing, ETL (Extract Transform Load), Data Quality, Data Security, Database Management, Data Partitioning, Indexing, Optimization, Spark, Snowflake, Azure Data Factory, Distributed Computing, Data Storage, Data Warehousing, Data Analytics, Business Intelligence, Data Engineering, Data Lifecycle Management, Data Governance, Data Lineage, Data Visualization, Collaboration, Communication, Problem Solving, Analytical Thinking","data architecture design, data integration and processing, etl extract transform load, data quality, data security, database management, data partitioning, indexing, optimization, spark, snowflake, azure data factory, distributed computing, data storage, data warehousing, data analytics, business intelligence, data engineering, data lifecycle management, data governance, data lineage, data visualization, collaboration, communication, problem solving, analytical thinking","analytical thinking, azure data factory, business intelligence, collaboration, communication, data architecture design, data engineering, data governance, data integration and processing, data lifecycle management, data lineage, data partitioning, data quality, data security, data storage, dataanalytics, database management, datawarehouse, distributed computing, etl extract transform load, indexing, optimization, problem solving, snowflake, spark, visualization"
Senior Staff AI Data Engineer,Recruiting from Scratch,"San Diego, CA",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3773086820,2023-12-17,Escondido,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto
or
San
Francisco
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Airflow, Apache Spark, Bash, Big data analytics, Cloud computing (AWS GCP Azure), Data engineering, Data governance, Data mining, Data modeling, Data pipelines, Data visualization, DataOps, Docker, DynamoDB, Git, Hadoop, Java, Kafka, Kubernetes, Legal compliance, Machine learning, Microservices, Natural language processing (NLP), NoSQL, Pandas, Python, R, Relational databases, SQL, Snowflake, Storm, Text data processing","airflow, apache spark, bash, big data analytics, cloud computing aws gcp azure, data engineering, data governance, data mining, data modeling, data pipelines, data visualization, dataops, docker, dynamodb, git, hadoop, java, kafka, kubernetes, legal compliance, machine learning, microservices, natural language processing nlp, nosql, pandas, python, r, relational databases, sql, snowflake, storm, text data processing","airflow, apache spark, bash, big data analytics, cloud computing aws gcp azure, data engineering, data governance, data mining, datamodeling, dataops, datapipeline, docker, dynamodb, git, hadoop, java, kafka, kubernetes, legal compliance, machine learning, microservices, natural language processing nlp, nosql, pandas, python, r, relational databases, snowflake, sql, storm, text data processing, visualization"
Senior Staff AI Data Engineer,Recruiting from Scratch,"San Diego, CA",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3759708639,2023-12-17,Escondido,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto,
San
Francisco or Chicago
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Machine Learning, Data Mining, Data Cleaning, Data Normalization, Data Modeling, Pandas, R, Airflow, KubeFlow, Spark, PySpark, Snowflake, Kubernetes, Docker, Helm, AWS, GCP, Azure, SQL, NoSQL, DynamoDB, ETL, Kafka, Storm, SparkStreaming, Python, Java, Bash, Git","data engineering, machine learning, data mining, data cleaning, data normalization, data modeling, pandas, r, airflow, kubeflow, spark, pyspark, snowflake, kubernetes, docker, helm, aws, gcp, azure, sql, nosql, dynamodb, etl, kafka, storm, sparkstreaming, python, java, bash, git","airflow, aws, azure, bash, data cleaning, data engineering, data mining, data normalization, datamodeling, docker, dynamodb, etl, gcp, git, helm, java, kafka, kubeflow, kubernetes, machine learning, nosql, pandas, python, r, snowflake, spark, sparkstreaming, sql, storm"
Data Center Engineer - Buffalo,DeRisk Technologies,"Buffalo, NY",https://www.linkedin.com/jobs/view/data-center-engineer-buffalo-at-derisk-technologies-3766678468,2023-12-17,Fort Erie, Canada,Associate,Onsite,"Job Responsibilities:
Deployment / In-Scope Configuration Items
Servers (Virtual & Physical)
Storage & Backup Devices
Server Appliances
Hyper Converged Infrastructure (E.g. Nutanix, Cisco UCS)
Tape Storage Units
Power Distribution Units rated 3KVA and below
SAN Fabric Switches
Network Switches
KVM Units
WAN Optimization Devices
Firewalls
Access Points
Routers
Physical Cabling
Cable Management
Cables which connect the device to itself, a peripheral, or a power/network port Break-fix / Technical Tasks List
Power Cycling
Running Diagnostics Commands
Conducting whole unit replacement
Inserting/Removing Media
Replacing Defective Components
Assist with fault diagnosis and investigation
Configuring Remote Access
Basic Storage Array Configuration
Replacing faulty cables
Tape Management and Maintenance
Rebooting routers, servers, storage devices or other equipment
Operations Tasks List
Updating and Recording of activities in relevant IT Ticket Management System
Coordinating and agreeing attendance time and date with key stakeholders
Provide support either through phone, remote tools, or in person at onsite
Perform installation as needed either through physical or network medium
Coordinate actual activity date and timings including arrival and departure times
Carry all necessary tools, laptops etc. which might be needed to support issues in the infrastructure environment
Labelling, Patching and Asset Tagging activities
Following specific task instructions and provision necessary reporting as necessary
Requirements
Job Requirements:
Technical Skills
Good general understanding of IT principles such as Networks, Hardware and Domains
Knowledge of Infrastructure (Data Center and Network) hardware architecture as to understand the procedure shared by L3 teams during troubleshooting,H&E support
Knowledge of server/client operations in a domain environment including Active Directory
Understanding of current and legacy hardware Infrastructure platforms
Hands-on experience in installation and troubleshooting Infrastructure (DC & Network) equipment's, Rack and Stack of the equipment/cable
Good Hands-on Experience in IMAC and Break-fix activities related to Infrastructure environment
Ability to identify Excellent the right defective spares and replace them with provided good spares as instructed and by physical observations
Knowledge of TCP/I P standards and networking
Experience with Tape Management activities
Excellent knowledge of best practices around management, control, and monitoring of server infrastructure
Familiarity with backup and recovery software and methodologies
Language skills needed
English Soft Skills
Exceptional customer facing skills
Able to communicate clearly and effectively both with Client and the Customer
Logical and analytical approach to work
Accurate record keeping
Able to work unsupervised
Good timekeeper
Intense focus on quality work
Productive and Efficient Academic Background
Bachelor of Engineering / Technology / Science or Equivalent Work Experience Overall Experience (in yrs.)
5 - 7 years (min
Benefits
Salary and Benefits as per market standard
Show more
Show less","Servers, Storage, Backup, Hyper Converged Infrastructure, Tape Storage, Power Distribution Units, SAN Fabric Switches, Network Switches, KVM Units, WAN Optimization Devices, Firewalls, Access Points, Routers, Physical Cabling, Cable Management, Diagnostics, Whole Unit Replacement, Media Insertion/Removal, Component Replacement, Fault Diagnosis, Remote Access Configuration, Storage Array Configuration, Cable Replacement, Tape Management, Equipment Rebooting, IT Ticket Management, Stakeholder Coordination, Onsite Support, Installation, Asset Tagging, Task Instruction Following, Reporting, Networking, Hardware, Domains, Server/Client Operations, Active Directory, Legacy Hardware, Infrastructure Equipment Installation, Rack and Stack, IMAC, Breakfix, Spare Identification, TCP/IP, Tape Management, Server Infrastructure Management, Backup/Recovery Software, English, Customer Service, Communication, Logical Thinking, Analytical Thinking, Record Keeping, Unsupervised Work, Time Management, Quality Focus, Productivity, Efficiency, Engineering, Technology, Science","servers, storage, backup, hyper converged infrastructure, tape storage, power distribution units, san fabric switches, network switches, kvm units, wan optimization devices, firewalls, access points, routers, physical cabling, cable management, diagnostics, whole unit replacement, media insertionremoval, component replacement, fault diagnosis, remote access configuration, storage array configuration, cable replacement, tape management, equipment rebooting, it ticket management, stakeholder coordination, onsite support, installation, asset tagging, task instruction following, reporting, networking, hardware, domains, serverclient operations, active directory, legacy hardware, infrastructure equipment installation, rack and stack, imac, breakfix, spare identification, tcpip, tape management, server infrastructure management, backuprecovery software, english, customer service, communication, logical thinking, analytical thinking, record keeping, unsupervised work, time management, quality focus, productivity, efficiency, engineering, technology, science","access points, active directory, analytical thinking, asset tagging, backup, backuprecovery software, breakfix, cable management, cable replacement, communication, component replacement, customer service, diagnostics, domains, efficiency, engineering, english, equipment rebooting, fault diagnosis, firewalls, hardware, hyper converged infrastructure, imac, infrastructure equipment installation, installation, it ticket management, kvm units, legacy hardware, logical thinking, media insertionremoval, network switches, networking, onsite support, physical cabling, power distribution units, productivity, quality focus, rack and stack, record keeping, remote access configuration, reporting, routers, san fabric switches, science, server infrastructure management, serverclient operations, servers, spare identification, stakeholder coordination, storage, storage array configuration, tape management, tape storage, task instruction following, tcpip, technology, time management, unsupervised work, wan optimization devices, whole unit replacement"
Senior Lead Data Engineer (Remote-Eligible),Jobs for Humanity,Buffalo-Niagara Falls Area,https://www.linkedin.com/jobs/view/senior-lead-data-engineer-remote-eligible-at-jobs-for-humanity-3766263048,2023-12-17,Fort Erie, Canada,Mid senior,Onsite,"Job Description
Job Title: Senior Lead Data Engineer (Remote-Eligible) Do you enjoy working in technology and solving complex business problems? At Capital One, we are a diverse and inclusive group of individuals who love to create, innovate, and meet the needs of our customers. We are looking for passionate Data Engineers who are skilled in merging data with emerging technologies. As a Senior Lead Data Engineer at Capital One, you will play a vital role in driving a major transformation within our company. What You'll Do: - Manage and develop a Java-based pipeline and query tools using technologies such as Hive Metastore, AWS S3, Kafka, and ORC. - Create analytics tools to solve business problems that come with growth and international expansion. - Optimize configurations for analytics tools to support our growing business and organization. Basic Qualifications: - Bachelor's Degree. - At least 8 years of experience in application development (Internship experience does not apply). - At least 2 years of experience in big data technologies. - At least 1 year of experience with cloud computing (AWS, Microsoft Azure, Google Cloud). Preferred Qualifications: - 9+ years of experience in application development including Python, Javascript, or Java. - 4+ years of experience with AWS. - 5+ years of experience with Distributed data/computing tools (Trino, Hive, Kafka, or Spark). - 4+ years of experience working on real-time data and streaming applications. - 4+ years of experience with NoSQL implementation (Cassandra). - 4+ years of experience with UNIX/Linux, including basic commands and shell scripting. - 2+ years of experience with Agile engineering practices. Capital One is open to hiring remote employees for this opportunity. Compensation: - New York City (Hybrid On-Site): $230,100 - $262,700 for Sr. Lead Data Engineer. - San Francisco, California (Hybrid On-Site): $243,800 - $278,200 for Sr. Lead Data Engineer. - Remote (Regardless of Location): $195,000 - $222,600 for Sr. Lead Data Engineer. Please note that salaries for part-time roles will be prorated based on agreed-upon hours. Benefits: Capital One offers a comprehensive set of health, financial, and other benefits that support your total well-being. Learn more at the Capital One Careers website. Eligibility varies based on employment status and management level. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. We do not discriminate based on sex, race, age, disability, genetic information, sexual orientation, gender identity, citizenship, veteran status, or any other basis prohibited by law. We also promote a drug-free workplace and consider applicants with a criminal history in compliance with applicable laws and regulations. If you require accommodation during the application process, please contact Capital One Recruiting at 1-800-304-9102 or email RecruitingAccommodation@capitalone.com. All provided information will be kept confidential and used only to provide necessary accommodations. For technical support or questions about our recruiting process, email Careers@capitalone.com. Please note that Capital One Financial includes multiple entities. Positions posted in Canada are for Capital One Canada, positions in the UK are for Capital One Europe, and positions in the Philippines are for Capital One Philippines Service Corp. (COPSSC).
Show more
Show less","Java, Hive Metastore, AWS S3, Kafka, ORC, Python, Javascript, Linux, Agile engineering, Git, Trino, Hive, Spark, Cassandra, UNIX/Linux, Shell scripting","java, hive metastore, aws s3, kafka, orc, python, javascript, linux, agile engineering, git, trino, hive, spark, cassandra, unixlinux, shell scripting","agile engineering, aws s3, cassandra, git, hive, hive metastore, java, javascript, kafka, linux, orc, python, shell scripting, spark, trino, unixlinux"
Virtual Data Analyst Part Time,Toyandsons,"Niagara Falls, Ontario, Canada",https://ca.linkedin.com/jobs/view/virtual-data-analyst-part-time-at-toyandsons-3758723665,2023-12-17,Fort Erie, Canada,Mid senior,Onsite,"Summary:
The Virtual Data Analyst Part Time will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : hiring@jobsai.live
Powered by Webbtree
Show more
Show less","Data Analysis, Statistical Techniques, Data Mining, Data Visualization, SQL, R, Python, Tableau, Power BI, Statistical Modeling, Hypothesis Testing, A/B Testing, Data Management, ETL, Data Quality Management, Data Cleansing, Data Manipulation, Performance Metrics, Reporting, CrossFunctional Collaboration, Communication","data analysis, statistical techniques, data mining, data visualization, sql, r, python, tableau, power bi, statistical modeling, hypothesis testing, ab testing, data management, etl, data quality management, data cleansing, data manipulation, performance metrics, reporting, crossfunctional collaboration, communication","ab testing, communication, crossfunctional collaboration, data management, data manipulation, data mining, data quality management, dataanalytics, datacleaning, etl, hypothesis testing, performance metrics, powerbi, python, r, reporting, sql, statistical modeling, statistical techniques, tableau, visualization"
Customer Service Representative/Data Analyst/Data Entry Clerk Full Time,Louisvuitton,"St. Catharines, Ontario, Canada",https://ca.linkedin.com/jobs/view/customer-service-representative-data-analyst-data-entry-clerk-full-time-at-louisvuitton-3751470205,2023-12-17,Fort Erie, Canada,Mid senior,Onsite,"Summary:
The Data Analyst will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : hiring@jobsai.live
Powered by Webbtree
Show more
Show less","Data analysis, Data interpretation, Statistical techniques, Data visualization, Data collection, Data cleansing, Data manipulation, Data integrity, Data accuracy, Data completeness, SQL, R, Python, Tableau, Power BI, ETL processes, Statistical modeling, Hypothesis testing, A/B testing, Machine learning, Artificial intelligence","data analysis, data interpretation, statistical techniques, data visualization, data collection, data cleansing, data manipulation, data integrity, data accuracy, data completeness, sql, r, python, tableau, power bi, etl processes, statistical modeling, hypothesis testing, ab testing, machine learning, artificial intelligence","ab testing, artificial intelligence, data accuracy, data collection, data completeness, data integrity, data interpretation, data manipulation, dataanalytics, datacleaning, etl, hypothesis testing, machine learning, powerbi, python, r, sql, statistical modeling, statistical techniques, tableau, visualization"
Senior Data Engineer (BioPharma and/or Life Sciences),Ambit Inc.,"Boston, NY",https://www.linkedin.com/jobs/view/senior-data-engineer-biopharma-and-or-life-sciences-at-ambit-inc-3773459613,2023-12-17,Fort Erie, Canada,Mid senior,Remote,"About Ambit:
Ambit is a healthcare technology and services company that provides data, analytics, consulting, and platform-based offerings to biopharma companies with a focus on rare and specialty diseases. Ambit capabilities include the areas of Strategy, Digital, Analytics, Digital Transformation, and Patient Identification & Activation.
About Ambit Leadership:
Ambit is led by a team of entrepreneurs with decades of rare and specialty disease experience and a passion to deliver data-driven innovation for our Biopharma partners.
Overview:
At Ambit we are on a mission to transform the world of rare and specialty disease. We are looking for people who excel in their work and are able to manage ambiguity with resourcefulness and creativity. Our core values are:
Set high standards
Deliver on ambitious goals
Be relentless and innovative
Be responsible and trusted
Job Summary:
We are seeking an experienced and motivated Senior Data Engineer to join our dynamic team. In this role, you will have the unique opportunity to work at the intersection of Data Engineering and DevOps, playing a vital role in developing and maintaining our data infrastructure while ensuring the efficient and reliable delivery of our data analytics products. As a Data Engineer, you will work closely with cross-functional teams, including data scientists, software developers, and product teams, to enable the seamless integration of data-driven solutions with our software products.
As Data Engineer, you will:
Design, develop, and maintain the data infrastructure and systems to support Ambit products, solutions, and projects.
Build and optimize data pipelines for extracting, transforming, and loading (ETL) large volumes of data from various sources into our data environment in a structured format suitable for analysis.
Collaborate with data scientists and analysts to understand data requirements and design efficient and scalable data processing workflows.
Develop algorithms and workflows to clean, transform, and integrate data from multiple sources, ensuring its quality and consistency.
Implement data governance policies, access controls, and security measures to protect sensitive data and ensure compliance with relevant regulations.
Optimize data processing and retrieval performance to ensure efficient and timely access to data.
Work closely with product teams, especially developers, to integrate data-driven solutions with our software products, ensuring seamless functionality and performance.
Design, build, and manage CI/CD pipelines for software build, testing, and deployment, ensuring efficient and reliable software delivery.
Implement infrastructure-as-code principles to automate the provisioning and management of infrastructure resources.
Set up monitoring and logging systems to ensure the availability, performance, and security of our software systems.
Manage system configurations, ensuring consistency across different environments, and employ orchestration tools to automate deployment and scaling.
Collaborate with cross-functional teams to identify areas for process improvement and implement DevOps best practices.
Stay updated with emerging technologies, industry trends, and best practices in Data Engineering and DevOps to drive innovation within the organization.
Qualifications:
Bachelor's degree in Computer Science, Engineering, or a related field. Master's degree preferred.
Minimum of 5 years of professional experience in a Data Engineering role.
Minimum of 3 years of experience in a biopharmaceutical or life sciences company, having knowledge in industry processes, commonly available/used data sources as well as data management, and regulatory compliance requirements.
Strong experience in data engineering, including data pipeline development, data integration, and data management.
Experience and proficiency in handling secondary data sources such as claims data (835/837), EMR/EHR, EDI 852/867, sell-in data, Rx dispense, formulary data, as well as digital and sales force execution data.
Life sciences-specific knowledge in master data management for entities such as patient, HCP, HCO, and MCO; as well as reference data management including medical codes such as ICD, CPT, HCPCS, NDC, etc. and clinical/medical taxonomies such as HPO and SNOMED CT.
Proficiency in programming languages such as Python, Java, or Scala, with experience in SQL and database technologies.
Demonstrated expertise and experience in working with Microsoft Azure cloud platform, including knowledge of Azure services such as Azure Data Factory, Azure Databricks, Azure Storage, Azure SQL Database, and Azure Kubernetes Service (AKS).
Hands-on experience in deploying and managing data and application workloads on Azure, leveraging Azure services for data engineering and DevOps tasks.
Solid understanding of DevOps principles, CI/CD practices, and experience with tools like Jenkins, Git, and configuration management systems.
Experience in defining and deploying infrastructure resources programmatically using infrastructure-as-code tools and techniques in Azure environments, including resource grouping, tagging, and parameterization for scalable and reusable deployments.
Understanding of Azure security and compliance practices, including experience implementing data governance, access controls, and security measures in Azure environments.
Strong problem-solving skills and the ability to work in a fast-paced, high-growth environment with tight deadlines.
Base Salary Range: $126,000 – $170,000
Qualities that we seek:
In addition to strong educational background and relevant work experience, we evaluate the following:
Problem solving abilities
Attention to detail
Communication skills
Organization and planning skills
Interested in growing and taking on additional responsibilities as part of a fast-paced entrepreneurial work environment
Work ethic
We treat our team right:
Competitive compensation is just the beginning. As part of our team, you can expect:
An early-stage growth company filled with passionate and fun teammates, dedicated to changing every life touched by rare disease
Flexible vacation policy with company wide shutdowns to unplug, rest, and recharge
401(k) (full-time employees)
Medical, dental, vision & disability insurance (full-time employees)
Parental leave (full-time employees)
Show more
Show less","Data Engineering, DevOps, Data Infrastructure, Data Analytics, Data Warehousing, Data Mining, Data Integration, Data Governance, Data Security, Python, Java, Scala, SQL, Azure Data Factory, Azure Databricks, Azure Storage, Azure SQL Database, Azure Kubernetes Service (AKS), Jenkins, Git, InfrastructureasCode, Continuous Integration (CI), Continuous Delivery (CD), ProblemSolving, Attention to Detail, Communication Skills, Organization, Planning","data engineering, devops, data infrastructure, data analytics, data warehousing, data mining, data integration, data governance, data security, python, java, scala, sql, azure data factory, azure databricks, azure storage, azure sql database, azure kubernetes service aks, jenkins, git, infrastructureascode, continuous integration ci, continuous delivery cd, problemsolving, attention to detail, communication skills, organization, planning","attention to detail, azure data factory, azure databricks, azure kubernetes service aks, azure sql database, azure storage, communication skills, continuous delivery cd, continuous integration ci, data engineering, data governance, data infrastructure, data integration, data mining, data security, dataanalytics, datawarehouse, devops, git, infrastructureascode, java, jenkins, organization, planning, problemsolving, python, scala, sql"
Data Engineer IV - Max Digital (Data Engineering),ACV Auctions,"Buffalo, NY",https://www.linkedin.com/jobs/view/data-engineer-iv-max-digital-data-engineering-at-acv-auctions-3762877414,2023-12-17,Fort Erie, Canada,Mid senior,Hybrid,"If you are looking for a career at a dynamic company with a people-first mindset and a deep culture of growth and autonomy, ACV is the right place for you! Competitive compensation packages and learning and development opportunities, ACV has what you need to advance to the next level in your career. We will continue to raise the bar every day by investing in our people and technology to help our customers succeed. We hire people who share our passion, bring innovative ideas to the table, and enjoy a collaborative atmosphere.
Who we are:
ACV is a technology company that has revolutionized how dealers buy and sell cars online. We are transforming the automotive industry. ACV Auctions Inc. (ACV), has applied innovation and user-designed, data driven applications and solutions. We are building the most trusted and efficient digital marketplace with data solutions for sourcing, selling and managing used vehicles with transparency and comprehensive insights that were once unimaginable. We are disruptors of the industry and we want you to join us on our journey. ACV’s network of brands includes ACV Auctions, ACV Transportation, ClearCar, MAX Digital and ACV Capital within its Marketplace Products, as well as, True360 and Data Services.
At ACV we focus on the Health, Physical, Financial, Social and Emotional Wellness of our Teammates and to support this we offer:
Multiple medical plans including a high deductible health plan that costs $0 out of your paycheck
Company-sponsored (paid) Short-Term Disability, Long-Term Disability, and Life Insurance
Comprehensive optional benefits such as Dental, Vision, Supplemental Life/AD&D, Legal/ID Protection, and Accident and Critical Illness Insurance
Generous paid time off options, including vacation time, sick days, Company holidays, floating holidays, parental leave, bereavement leave, jury duty leave, voting leave, and other forms of paid leave as required by applicable law or regulation
Employee Stock Purchase Program with additional opportunities to earn stock in the Company
Retirement planning through the Company’s 401(k)
Who we are looking for:
We are seeking a highly skilled Engineer IV in Data Engineering with a strong foundation in computer science and excellent problem-solving skills. You will be responsible for maintaining and extending our database operations, optimizing SQL queries, and designing scalable data services.
What you will do:
Actively and consistently support all efforts to simplify and enhance the customer experience.
Maintain and extend (as required) existing database operations solution for backups, index defragmentation, data retention, etc.
Troubleshoot any SQL Server or ETL stack outages during our operational support window.
Triage any issues with data stack (SSIS, C#, Web APIs).
Support development, integration, and stage SQL Server environments for application development and data science teams.
Ensure that new database development meets company standards for readability, reliability, and performance. Work with internal teams on transactional and analytical schema design.
Collaborate with software and DevOps engineers to design scalable services, plan feature roll-out, and ensure high reliability and performance of our products.
Architect and build entire services including but not limited to; data modeling, storage, message brokers, protocols and interfaces.
Design, build and maintain complex systems that can scale rapidly with little maintenance.
Conduct code reviews, develop high-quality documentation, and build robust test suites.
Own the overall performance of products and services within a defined area of focus.
Be empowered to lead and complete software projects with minimal guidance from managers.
Lead team discussions to define technical requirements on new and current products.
Respond-to and troubleshoot highly complex problems quickly, efficiently, and effectively.
Mentor junior engineers.
Perform additional duties as assigned.
What you will need:
Bachelor's degree in Computer Science, Information Technology, Computer Information Systems, Management Information Systems, or similar
5 years' building & supporting the database-tier of SaaS web applications.
Ability to read, write, speak, and understand English.
Expert understanding of SQL query execution fundamentals and query optimization principles.
Experience maintaining and extending an existing codebase, adapting to pre-existing patterns and tracing the code’s path of execution.
ETL workflow implementation (SSIS, Airflow, C#, Python)
Experience working with Cloud Services (AWS RDS, S3, SQS, SNS)
Experience working with NoSQL data stores (MongoDB)
Experience writing unit and integration testing (DBT, C#)
Expert SQL and data-layer development experience; OLTP schema design.
Experience integrating 3rd-party APIs, implementing authentication & authorization and developing asynchronous data flows.
Nice to Have
OLAP schema design experience.
Experience with Airflow, Snowflake, etc.
Experience with DBT
Compensation:
$124,000.00 - $152,000.00 annually. Please note that final compensation will be determined based upon the applicant's relevant experience, skillset, location, business needs, market demands, and other factors as permitted by law.
Our Values
Trust & Transparency | People First | Positive Experiences | Calm Persistence | Never Settling
At ACV, we are committed to an inclusive culture in which every individual is welcomed and empowered to celebrate their true selves. We achieve this by fostering a work environment of acceptance and understanding that is free from discrimination. ACV is committed to being an equal opportunity employer regardless of sex, race, creed, color, religion, marital status, national origin, age, pregnancy, sexual orientation, gender, gender identity, gender expression, genetic information, disability, military status, status as a veteran, or any other protected characteristic. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you have a disability or special need that requires reasonable accommodation, please let us know.
For information on our collection and use of your personal information, please see our Privacy Notice.
Apply Now
Show more
Show less","Data Engineering, SQL, Data Retention, ETL, SSIS, C#, Web APIs, Python, AWS, MongoDB, DBT, OLTP, APIs, Airflow, Snowflake","data engineering, sql, data retention, etl, ssis, c, web apis, python, aws, mongodb, dbt, oltp, apis, airflow, snowflake","airflow, apis, aws, c, data engineering, data retention, dbt, etl, mongodb, oltp, python, snowflake, sql, ssis, web apis"
Data Engineer IV - Max Digital (Data Operations),ACV Auctions,"Buffalo, NY",https://www.linkedin.com/jobs/view/data-engineer-iv-max-digital-data-operations-at-acv-auctions-3762879090,2023-12-17,Fort Erie, Canada,Mid senior,Hybrid,"If you are looking for a career at a dynamic company with a people-first mindset and a deep culture of growth and autonomy, ACV is the right place for you! Competitive compensation packages and learning and development opportunities, ACV has what you need to advance to the next level in your career. We will continue to raise the bar every day by investing in our people and technology to help our customers succeed. We hire people who share our passion, bring innovative ideas to the table, and enjoy a collaborative atmosphere.
Who we are:
ACV is a technology company that has revolutionized how dealers buy and sell cars online. We are transforming the automotive industry. ACV Auctions Inc. (ACV), has applied innovation and user-designed, data driven applications and solutions. We are building the most trusted and efficient digital marketplace with data solutions for sourcing, selling and managing used vehicles with transparency and comprehensive insights that were once unimaginable. We are disruptors of the industry and we want you to join us on our journey. ACV’s network of brands includes ACV Auctions, ACV Transportation, ClearCar, MAX Digital and ACV Capital within its Marketplace Products, as well as, True360 and Data Services.
At ACV we focus on the Health, Physical, Financial, Social and Emotional Wellness of our Teammates and to support this we offer:
Multiple medical plans including a high deductible health plan that costs $0 out of your paycheck
Company-sponsored (paid) Short-Term Disability, Long-Term Disability, and Life Insurance
Comprehensive optional benefits such as Dental, Vision, Supplemental Life/AD&D, Legal/ID Protection, and Accident and Critical Illness Insurance
Generous paid time off options, including vacation time, sick days, Company holidays, floating holidays, parental leave, bereavement leave, jury duty leave, voting leave, and other forms of paid leave as required by applicable law or regulation
Employee Stock Purchase Program with additional opportunities to earn stock in the Company
Retirement planning through the Company’s 401(k)
Who we are looking for:
We are seeking a highly skilled Engineer IV in Data Engineering with a strong foundation in computer science and excellent problem-solving skills. You will be responsible for maintaining and extending our database operations, optimizing SQL queries, and designing scalable data services.
What you will do:
Actively and consistently support all efforts to simplify and enhance the customer experience.
Maintain and extend (as required) existing database operations solution for backups, index defragmentation, data retention, etc.
Troubleshoot any SQL Server or ETL stack (SSIS, C#, Web APIs) outages during our operational support window.
Leverage monitoring tools to ensure high performance and availability; work with operations and engineering to improve as required.
Leverage DMVs and monitoring tools to ensure system performance; work with data operations and engineering to improve as required.
Ensure existing HADR (availability groups) solution is functional and meets requirements.
Support development, integration, and stage SQL Server environments for application development and data science teams.
Ensure that new database development meets company standards for readability, reliability, and performance. Work with internal teams on transactional and analytical schema design.
Collaborate with software and DevOps engineers to design scalable services, plan feature roll-out, and ensure high reliability and performance of our products.
Conduct code reviews, develop high-quality documentation, and build robust test suites.
Own the overall performance of products and services within a defined area of focus.
Respond-to and troubleshoot highly complex problems quickly, efficiently, and effectively. This may include being part of the emergency after-hours on-call rotation.
Mentor junior data engineers.
Perform additional duties as assigned.
What you will need:
Bachelor's degree in Computer Science, Information Technology, Computer Information Systems, Management Information Systems, or similar
5 years' building & supporting the database-tier of SaaS web applications.
Ability to read, write, speak, and understand English.
Expert in SQL Query optimization
ETL workflow implementation (SSIS, Airflow, C#, Python)
Experience working with Cloud Services (AWS RDS, S3, SQS, SNS)
Experience working with NoSQL data stores (e.g., MongoDB)
Experience developing Windows services in C#
Experience writing unit and integration testing
Expert SQL and data-layer development experience; OLTP schema design.
Experience using and integrating with cloud services, specifically: AWS RDS, S3, SQS, SNS.
Nice to Have
Experience with Airflow
Experience with DBT
Compensation:
$127,000.00 - $152,000.00 annually. Please note that final compensation will be determined based upon the applicant's relevant experience, skillset, location, business needs, market demands, and other factors as permitted by law.
Our Values
Trust & Transparency | People First | Positive Experiences | Calm Persistence | Never Settling
At ACV, we are committed to an inclusive culture in which every individual is welcomed and empowered to celebrate their true selves. We achieve this by fostering a work environment of acceptance and understanding that is free from discrimination. ACV is committed to being an equal opportunity employer regardless of sex, race, creed, color, religion, marital status, national origin, age, pregnancy, sexual orientation, gender, gender identity, gender expression, genetic information, disability, military status, status as a veteran, or any other protected characteristic. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you have a disability or special need that requires reasonable accommodation, please let us know.
For information on our collection and use of your personal information, please see our Privacy Notice.
Apply Now
Show more
Show less","SQL, AWS RDS, AWS S3, AWS SQS, AWS SNS, MongoDB, NoSQL, SSIS, Airflow, C#, Python, DBT, OLTP, ETL","sql, aws rds, aws s3, aws sqs, aws sns, mongodb, nosql, ssis, airflow, c, python, dbt, oltp, etl","airflow, aws rds, aws s3, aws sns, aws sqs, c, dbt, etl, mongodb, nosql, oltp, python, sql, ssis"
"CFD Data Centers, Lead Product Engineer",Cadence Design Systems,Buffalo-Niagara Falls Area,https://www.linkedin.com/jobs/view/cfd-data-centers-lead-product-engineer-at-cadence-design-systems-3683134195,2023-12-17,Fort Erie, Canada,Mid senior,Hybrid,"At Cadence, we hire and develop leaders and innovators who want to make an impact on the world of technology.
The Lead Application Engineer role offers an opportunity to join an exciting and highly motivated team. You will work towards creating and implementing Digital Twin models of Data Centers, helping customer design and efficiently operate their data centers using Computational Fluid Dynamics (CFD).
Using Cadence’s suite of DCX tools, you will develop creative and innovative solutions for customers, whether for new data center design, planning expansions for existing sites, or designing critical power and cooling infrastructures.
Duties Include
Implement Professional Services offerings that focus on accelerating implementations, creating, and evolving best practices for rolling out Digital Twin services.
Present Digital twin service offerings aligned to enterprise, hyperscale, co-location, and edge market segments, including value positioning, the scope of service, collateral, and pricing.
Understand customer requirements and work with them to develop and manage project scope.
Provide project delivery leadership on small to large, complex, and highly strategic solutions involving single to integrated solution deals.
Collaborate with sales and development teams to build project and deployment plans to understand and document customers’ expectations.
Work to understand how our clients use our products and help identify growth opportunities within existing accounts
Work with clients and educate them on product features and functionality through training/webinars and workshops to improve adoption and usage
Gather client feedback on product challenges to help improve the product.
Serve as the internal escalation point for complex technical client issues.
Coordinate project schedules and tasks and manage progress to ensure on-time, quality delivery of business requirements.
Develop project metrics, such as time-to-value, customer satisfaction, and on-time completion of projects.
Provide regular project status updates to clients and internal teams.
Train and mentor junior engineers and channel partners in the practices and procedures for selling and delivering Digital Twin projects.
Identify and mitigate risks.
Provide regular KPI reports of professional services work effort and status, from quoting to delivery, including resource utilization and project health.
The Ideal Candidate Should Have The Following
BS required in mechanical, aerospace, or equivalent, MS Preferred
At least 3-5 years of experience in Data center cooling and/or power technologies
Experience with Datacenter simulation tools like 6SigmaDCX or similar
Experience with DCIM tools and Databases like MongoDB desired
Customer service oriented with strong problem-solving skills
Strong attention to detail with the ability to organize and prioritize
Team player with a positive attitude, willing to offer and start, run ideas and solutions to enhance processes within an evolving environment
Prepared to travel and work at customer sites throughout North America
Must be in California or New York and be authorized to work in the US
The annual salary range for California is $87,500 to $162,500. You may also be eligible to receive incentive compensation: bonus, equity, and benefits. Sales positions generally offer a competitive On Target Earnings (OTE) incentive compensation structure. Please note that the salary range is a guideline and compensation may vary based on factors such as qualifications, skill level, competencies and work location. Our benefits programs include: paid vacation and paid holidays, 401(k) plan with employer match, employee stock purchase plan, a variety of medical, dental and vision plan options, and more.
We’re doing work that matters. Help us solve what others can’t.
Show more
Show less","Mechanical Engineering, Aerospace Engineering, Computational Fluid Dynamics (CFD), Data center cooling technologies, Data center power technologies, Datacenter simulation tools, DCIM tools, MongoDB, Customer service, Problemsolving, Attention to detail, Organization, Prioritization, Teamwork, Creativity, Innovation, Communication, Leadership, 6SigmaDCX","mechanical engineering, aerospace engineering, computational fluid dynamics cfd, data center cooling technologies, data center power technologies, datacenter simulation tools, dcim tools, mongodb, customer service, problemsolving, attention to detail, organization, prioritization, teamwork, creativity, innovation, communication, leadership, 6sigmadcx","6sigmadcx, aerospace engineering, attention to detail, communication, computational fluid dynamics cfd, creativity, customer service, data center cooling technologies, data center power technologies, datacenter simulation tools, dcim tools, innovation, leadership, mechanical engineering, mongodb, organization, prioritization, problemsolving, teamwork"
Data Analyst,Empyreal Logistics (We're Hiring!!),"Englewood, CO",https://www.linkedin.com/jobs/view/data-analyst-at-empyreal-logistics-we-re-hiring%21%21-3782270608,2023-12-17,Aurora,United States,Associate,Onsite,"We are a growing Cash Logistics company looking for enthusiastic, responsible, and reliable individuals to join our team. Empyreal Logistics' mission is to provide and deliver a broader and deeper level of support to our customers by tailoring our logistics and cash handling services to meet the unique needs of our customers. We are building a culture of dependable, flexible, and trustworthy employees that are committed to service and excellence.
We have an opening for a
Data Analyst
in Denver, CO. The
Data Analyst
reports to the
Logistics Manager
.
Position Summary:
The Data Analyst supervises the day-to-day client profile builds and data input into CIT software and Currency Controller software. Manages all client profiles nationwide and verifies data accuracy while providing quick and consistent customer support as consistent with Empyreal's goals and objectives, customers' expectations, and regulatory requirements.
Responsibilities and Duties include, but are not limited to:
Provides accurate data input into customer profiles and ensures compatibility between software systems
Verifies all account numbers with client or FI, prior to profile completion
Actively communicates and provides customer support for service request corrections, billing information updates, service day changes and alternative branch servicing adjustments
Researching anomalies in billing or service-related issues within CIT and Currency Controller software
Researching and assisting Empyreal clients with servicing issues and changes
Acts as a liaison between the Onboarding Department and Operations Department
Builds Financial Institution profiles and verifies data
Proactively working with vault employees to ensure all transactions are reported correctly, as required during client complaint investigations
Provides additional support to the Onboarding Department by actively communicating with new and existing customers and sorting issues to correct departments
Ability to support the Logistics Analyst and Logistics Manager during high volume routing, as needed
Performs periodic audits of client SA's to software system data
Supports the Vault Services Department in the communication, set up and data input of all new Federal Rerserve Bank Cash Services activations for FI's
Models and supports the company's vision, values, and desired culture• Performs other duties and projects as assigned
Qualifications:
A minimum of 1-2 years' experience in an administrative role
In-depth knowledge of the cash logistics industry
Solid understanding of logistics management software
Outstanding analytical, problem solving and organizational abilities
Creative thinking skills
Good time management skills
Exceptional verbal and written communication skills
Physical Requirements:
Must be able to lift 30 pounds and sometimes up to 50 pounds, as needed.
If any items for lifting purposes, are more than 50 lbs., seek support for lifting from a team member
Empyreal Logistics Culture:
Acts in a manner that portrays Empyreal's vision, values, and WOW culture.
Establishes respect for everyone, fosters an environment that builds and sustains productive relationships.
Addresses conflict respectively and constructively.
Works in partnership within and across workgroups.
Represents Empyreal as a transportation and customer service professional.
Places value not only on what work gets done, but also how it gets done.
Show more
Show less","Data analysis, Software systems, Customer support, CIT software, Currency Controller software, Financial Institution profiles, Federal Reserve Bank Cash Services, Logistics management software, Analytical skills, Problem solving skills, Organizational skills, Creative thinking, Time management skills, Verbal communication skills, Written communication skills, Physical strength","data analysis, software systems, customer support, cit software, currency controller software, financial institution profiles, federal reserve bank cash services, logistics management software, analytical skills, problem solving skills, organizational skills, creative thinking, time management skills, verbal communication skills, written communication skills, physical strength","analytical skills, cit software, creative thinking, currency controller software, customer support, dataanalytics, federal reserve bank cash services, financial institution profiles, logistics management software, organizational skills, physical strength, problem solving skills, software systems, time management skills, verbal communication skills, written communication skills"
Data Engineer,Kforce Inc,"Brooklyn, NY",https://www.linkedin.com/jobs/view/data-engineer-at-kforce-inc-3773542616,2023-12-17,Middletown,United States,Associate,Onsite,"Responsibilities
Kforce has a client in need of a Data Engineer in Brooklyn, NY. Responsibilities:
The Data Engineer will collect entity metadata and lineage for unified data catalog
Develop metrics, tools, and dashboards to better understand users' access patterns
Advise data producers on how to record metadata at the system of record so that it is discoverable in the data catalog
The Data Engineer will work closely with data producers and consumers to understand their needs and help guide our roadmap
Requirements
5-7 years of experience with Python
Experience working with Google BigQuery and SQL
Familiarity with cloud platform such as AWS, GCP or Azure, validation
The pay range is the lowest to highest compensation we reasonably in good faith believe we would pay at posting for this role. We may ultimately pay more or less than this range. Employee pay is based on factors like relevant education, qualifications, certifications, experience, skills, seniority, location, performance, union contract and business needs. This range may be modified in the future.
We offer comprehensive benefits including medical/dental/vision insurance, HSA, FSA, 401(k), and life, disability & ADD insurance to eligible employees. Salaried personnel receive paid time off. Hourly employees are not eligible for paid time off unless required by law. Hourly employees on a Service Contract Act project are eligible for paid sick leave.
Note: Pay is not considered compensation until it is earned, vested and determinable. The amount and availability of any compensation remains in Kforce's sole discretion unless and until paid and may be modified in its discretion consistent with the law.
This job is not eligible for bonuses, incentives or commissions.
Kforce is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.
Salary: $70 - $85 per hour
Show more
Show less","Data Engineering, Data Catalog, Python, Google BigQuery, SQL, AWS, GCP, Azure","data engineering, data catalog, python, google bigquery, sql, aws, gcp, azure","aws, azure, data catalog, data engineering, gcp, google bigquery, python, sql"
Senior Data Engineer,Kforce Inc,"Brooklyn, NY",https://www.linkedin.com/jobs/view/senior-data-engineer-at-kforce-inc-3777072396,2023-12-17,Middletown,United States,Associate,Onsite,"Responsibilities
Kforce's client in NYC is seeking a experiened Senior Data Engineer to join their team on a contract basis. Summary: The Senior Data Engineer will be building a BigQuery-backed data warehouse for employee and organizational analytics, using existing data processing frameworks. They will work closely with the resident Staff Engineers, other developers, data scientists, and analysts to write efficient pipelines aggregating data from various sources. This new position will play a ground-breaking role in driving more informed and objective decisions by modernizing the way organizational information is collected managed and consumed. In this day-to-day role, the Senior Data Engineer will be working on building ETL pipelines, data warehouses, data marts, and aggregate tables.
Requirements
General experience building ETL pipelines from start to finish with a focus on data aggregation and rollups in SQL
Experience with BigQuery
SQL/analytics background
Very comfortable with Python
Experience with Dataform or dbt
Experience with Airflow
The pay range is the lowest to highest compensation we reasonably in good faith believe we would pay at posting for this role. We may ultimately pay more or less than this range. Employee pay is based on factors like relevant education, qualifications, certifications, experience, skills, seniority, location, performance, union contract and business needs. This range may be modified in the future.
We offer comprehensive benefits including medical/dental/vision insurance, HSA, FSA, 401(k), and life, disability & ADD insurance to eligible employees. Salaried personnel receive paid time off. Hourly employees are not eligible for paid time off unless required by law. Hourly employees on a Service Contract Act project are eligible for paid sick leave.
Note: Pay is not considered compensation until it is earned, vested and determinable. The amount and availability of any compensation remains in Kforce's sole discretion unless and until paid and may be modified in its discretion consistent with the law.
This job is not eligible for bonuses, incentives or commissions.
Kforce is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.
Salary: $70 - $78 per hour
Show more
Show less","BigQuery, SQL, Python, Dataform, Airflow, ETL pipelines, Data warehouse, Data marts, Data aggregation, Rollups","bigquery, sql, python, dataform, airflow, etl pipelines, data warehouse, data marts, data aggregation, rollups","airflow, bigquery, data aggregation, data marts, dataform, datawarehouse, etl pipelines, python, rollups, sql"
Data Analyst,First Soft Solutions LLC,"Piscataway, NJ",https://www.linkedin.com/jobs/view/data-analyst-at-first-soft-solutions-llc-3709604697,2023-12-17,Middletown,United States,Associate,Onsite,"We are actively hiring for
Sr.Data Analyst
C2C
Required Skills: Data analysis, FHIR, HL7
Job Summary : Managing master data, including creation, updates, and deletion.
Managing users and user roles.
Provide quality assurance of imported data, working with quality assurance analysts if necessary.
Commissioning and decommissioning of data sets.
Processing confidential data and information according to guidelines
Helping develop reports and analysis.
Managing and designing the reporting environment, including data sources, security, and metadata.
Managing master data, including creation, updates, and deletion.
Managing users and user roles.
Provide quality assurance of imported data, working with quality assurance analysts if necessary.
Commissioning and decommissioning of data sets.
Processing confidential data and information according to guidelines.
Helping develop reports and analysis
Managing and designing the reporting environment, including data sources, security, and metadata.
Supporting the data warehouse in identifying and revising reporting requirements.
Supporting initiatives for data integrity and normalization.
Assessing tests and implementing new or upgraded software and assisting with strategic decisions on new systems
Generating reports from single or multiple systems.
Troubleshooting the reporting database environment and reports.
Providing technical expertise in data storage structures, data mining, and data cleansing
Please share profiles to sony@firstsoftsolutions.net
Show more
Show less","Data analysis, FHIR, HL7, Data management, User management, Quality assurance, Data commissioning, Data decommissioning, Data security, Data metadata, Reporting, Data warehouse, Data integrity, Data normalization, Software testing, Software implementation, Data storage, Data mining, Data cleansing","data analysis, fhir, hl7, data management, user management, quality assurance, data commissioning, data decommissioning, data security, data metadata, reporting, data warehouse, data integrity, data normalization, software testing, software implementation, data storage, data mining, data cleansing","data commissioning, data decommissioning, data integrity, data management, data metadata, data mining, data normalization, data security, data storage, dataanalytics, datacleaning, datawarehouse, fhir, hl7, quality assurance, reporting, software implementation, software testing, user management"
Senior Data Engineer,Kforce Inc,"Brooklyn, NY",https://www.linkedin.com/jobs/view/senior-data-engineer-at-kforce-inc-3782233038,2023-12-17,Middletown,United States,Associate,Onsite,"Responsibilities
Kforce's client in Brooklyn, NY is seeking an experienced Senior Data Engineer to join their team on a contract basis! The Data Engineer will be focused on migrating data pipelines from legacy infrastructure and frameworks to more modern infrastructure.
Requirements
The ideal candidate is a Data Engineer with considerable experience in migrations and Big Data frameworks
Scala programming language expertise
Spark framework expertise
Experience working with BigQuery
Familiarity scheduling jobs in Airflow
Fluency with Google Cloud Platform, in particular GCS and Dataproc
The pay range is the lowest to highest compensation we reasonably in good faith believe we would pay at posting for this role. We may ultimately pay more or less than this range. Employee pay is based on factors like relevant education, qualifications, certifications, experience, skills, seniority, location, performance, union contract and business needs. This range may be modified in the future.
We offer comprehensive benefits including medical/dental/vision insurance, HSA, FSA, 401(k), and life, disability & ADD insurance to eligible employees. Salaried personnel receive paid time off. Hourly employees are not eligible for paid time off unless required by law. Hourly employees on a Service Contract Act project are eligible for paid sick leave.
Note: Pay is not considered compensation until it is earned, vested and determinable. The amount and availability of any compensation remains in Kforce's sole discretion unless and until paid and may be modified in its discretion consistent with the law.
This job is not eligible for bonuses, incentives or commissions.
Kforce is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.
Salary: $75 - $85 per hour
Show more
Show less","Scala, Spark, BigQuery, Airflow, Google Cloud Platform, GCS, Dataproc","scala, spark, bigquery, airflow, google cloud platform, gcs, dataproc","airflow, bigquery, dataproc, gcs, google cloud platform, scala, spark"
Data Center Engineer,Leaseweb,"Edison, NJ",https://www.linkedin.com/jobs/view/data-center-engineer-at-leaseweb-3783128915,2023-12-17,Middletown,United States,Associate,Onsite,"The Leaseweb USA team is growing quickly and looking for talented individuals to join us.
Are you a hands-on and technology-driven engineer? Do you have extensive knowledge of the hosting and data center landscape? Are you energetic and able to get things done? Then you should apply with us!
Everybody at Leaseweb is responsible for high customer satisfaction. We give our customers royal treatment. Our team of Data Center Operations Engineers is involved in the daily management of our 12 data center facilities throughout the US.
This includes the deployment and support of hardware and hosting services for our end users.
You are responsible for the configuration and installation of the servers during the provisioning process in our data center in Edison, New Jersey, and other domestic data centers as needed. You also assist customers with the installation of servers and other equipment. The work environment is dynamic and has an international character.
As a Data Center Operations Engineer, you will work Monday to Friday, 8:30 a.m. to 5:30 p.m. And be on call when needed. You will receive a flat EOD (Engineer on Duty) payment once a month. In addition, to your EOD flat rate, you will get paid 180% of your regular rate, for any hours worked while on call (before and after hours, or on weekends).
Key responsibilities:
Installation, relocation, and removal of server equipment at the data center
The assistance of customers within data centers (for customers with their own racks/servers)
Upgrade/downgrade of server equipment (memory/hard disks/etc.)
Replacement of hardware and handling of all relevant administrative matters (RMA handling)
Testing servers and server equipment in accordance with standards
Requirements:
At least one year of hands-on experience working in a data center with servers
Knowledge of various operating systems, especially Linux
Knowledge of Hardware (Servers, Switches, KVM, Hard Drives, Memory, etc.)
Basic network knowledge (adjusting DNS/IP/Netmask/gateway/etc.)
Valid driver’s license
Certificate or degree in Computer Science or similar discipline preferably with an emphasis in Technology
Strong Work Ethic
Professional and effective communication
Time Management
Team Player
Great attention to detail
Ability to travel domestically and internationally to other data centers, as needed.
Travel <10%
Who are you?
We’re looking for a team player with hands-on experience, working in a data center environment. You should be a highly dependable and responsible individual.
About Leaseweb
Work for Leaseweb and help make the internet’s heartbeat.
Leaseweb is a leading cloud hosting provider that offers a broad range of products and services including, private cloud, dedicated servers, hybrid hosting, colocation, CDN, and cybersecurity. Working with companies ranging from SMBs to enterprises, Leaseweb has the flexibility, market knowledge, and technical expertise to provide the perfect balance of price and performance. The Leaseweb platform resides on one of the largest, most reliable networks in the world, boasting 5.5 Tbps bandwidth capacity and 99.999% core uptime, low latency, and fast response time from 25 data centers throughout the world.
What we offer:
Competitive salary + annual bonus
The possibility to develop yourself and build your career
Free Medical, Dental, Vision, Short Term Disability, Long Term Disability, and Life Insurance
Competitive time-off policy
Flexible Spending Account for Dependant Care and Medical expenses
100% Match on 401k
A great working environment in a dynamic, international, fast-growing organization
Show more
Show less","Hardware, Servers, Linux, Network knowledge, DNS, IP, Netmask, Gateway, Computer Science, Attention to detail, Travel, Data center, Customer service, Installation, Relocation, Removal, Upgrade, Downgrade, RMA handling, Testing, Valid driver’s license","hardware, servers, linux, network knowledge, dns, ip, netmask, gateway, computer science, attention to detail, travel, data center, customer service, installation, relocation, removal, upgrade, downgrade, rma handling, testing, valid drivers license","attention to detail, computer science, customer service, data center, dns, downgrade, gateway, hardware, installation, ip, linux, netmask, network knowledge, relocation, removal, rma handling, servers, testing, travel, upgrade, valid drivers license"
"Data Analyst | Brooklyn, NY | Long term Contract",Spanco Solutions,"Brooklyn, NY",https://www.linkedin.com/jobs/view/data-analyst-brooklyn-ny-long-term-contract-at-spanco-solutions-3707517515,2023-12-17,Middletown,United States,Associate,Onsite,"Job Title: Data Analyst with Python, SQL, SAS, Excel and Access
Position Type: Long term Contract
Location: Brooklyn, NY
Required Skills
Minimum 4 Years' Experience with Development and Maintenance of PL/SQL Packages, Stored Procedures, Functions, Triggers, SQL statements in Oracle 12c.
Minimum 4 Years' Experience with Performance Tuning including tuning/optimizing SQL, PL/SQL code.
Minimum 4 Years' Experience working with XML Processing, Object oriented programming with large objects.
Minimum 4 Years' Experience with data structures, data manipulation, databases, design, programming, testing and implementation.
Minimum 4 Years' Experience with technical and user documentation, software conversion.
Show more
Show less","Python, SQL, SAS, Excel, Access, PL/SQL, Oracle 12c, SQL statements, XML Processing, Objectoriented programming, Data structures, Data manipulation, Databases, Design, Programming, Testing, Implementation, Technical documentation, User documentation, Software conversion","python, sql, sas, excel, access, plsql, oracle 12c, sql statements, xml processing, objectoriented programming, data structures, data manipulation, databases, design, programming, testing, implementation, technical documentation, user documentation, software conversion","access, data manipulation, data structures, databases, design, excel, implementation, objectoriented programming, oracle 12c, plsql, programming, python, sas, software conversion, sql, sql statements, technical documentation, testing, user documentation, xml processing"
"Functional Data Analyst (SQL, EXCEL, ACCESS )-US citizens only","ACS Consultancy Services, Inc","New York, NY",https://www.linkedin.com/jobs/view/functional-data-analyst-sql-excel-access-us-citizens-only-at-acs-consultancy-services-inc-3707634657,2023-12-17,Middletown,United States,Associate,Onsite,"Role:
Functional Data Analyst (SQL, EXCEL, ACCESS )
Location:
NYC, NY, 10003
Duration:
36
Position #:
DSS-FDA-RF-36-37-131
Rate: Open
Mandatory Requirements
Resume(s) MUST BE restricted to a maximum of 2 pages
2 references for each consultant
Must have 8+ Years of Experience.
Tasks & Duties
Conduct analysis, measure various areas of performance, and develop data flow process improvement initiatives.
Perform functional data analysis to assess the quality of data.
Analyze findings from the data analytics group, create and document technical specifications, facilitate complex programming tasks involving qualitative data corrections.
Prepare analytical reports for stakeholder's review, enabling them to guide the team with decisions making based on timely and accurate information.
Collaborate with data analyst, data architect, solution architect and programmers to identify process improvement opportunities, propose system modifications, and devise data governance strategies.
Required Skills
Minimum 8 Years Conducting Analysis, creating technical workflow diagrams pertaining to complex data mapping & flow.
Minimum 8 Years Hands on using process mapping tools like Microsoft Visio.
Minimum 8 Years Defining & validating system integration test cases pertaining to data fixes.
Minimum 8 Years Excellent Organizational, Interpersonal, And Communication Skills
Minimum 8 Years Proficient in analyzing and understanding highly technical information.
Minimum 8 Years Ability to adapt, grasp & perform in a fast-paced environment subject to challenges in identifying bad data & bringing qualitative techniques to improve data quality.
Minimum 8 Years Expertise in data Visualization techniques
Minimum 8 Years Expertise in Advanced Microsoft Excel and Access
Minimum 8 Years SQL Programming Experience
Minimum 8 Years Experience in creating executive summary data reporting
Show more
Show less","SQL, Access, Data analysis, Data quality assessment, Data corrections, Data governance strategies, Data mapping, Data visualization, Excel, Microsoft Visio, Process mapping, System integration testing, Data reporting, Organizational skills, Communication skills","sql, access, data analysis, data quality assessment, data corrections, data governance strategies, data mapping, data visualization, excel, microsoft visio, process mapping, system integration testing, data reporting, organizational skills, communication skills","access, communication skills, data corrections, data governance strategies, data mapping, data quality assessment, data reporting, dataanalytics, excel, microsoft visio, organizational skills, process mapping, sql, system integration testing, visualization"
"HXGN EAM DATA ANALYST || New York, Hybrid - Local Candidates only of NY",Steneral Consulting,"New York, NY",https://www.linkedin.com/jobs/view/hxgn-eam-data-analyst%C2%A0-new-york-hybrid-local-candidates-only-of-ny-at-steneral-consulting-3707361949,2023-12-17,Middletown,United States,Associate,Hybrid,"HXGN EAM DATA ANALYST
New York, Hybrid - Local Candidates only of NY
Summary Of The Role
Two temporary consultants to serve as HxGN EAM Data Analysts.
This position is to provide critical HxGN EAM project roll-out support for DOS Enterprise Asset Management (EAM) implementation.
Responsibilities
Under the direction of the System Data Lead, extract, cleanse and load data, including the use of Hexagon EAM Import/Upload Utilities based on project requirements
Review data loads for accuracy and completeness
Review legacy system data and provide mapping to EAM data model
Develop Power BI and/or GIS solutions
Support training and develop communication materials
Support other project activities, as needed
Task-Based Deliverables
For each project, the deliverables will be determined by Subways EAM Program Management at the initiation of each project. Most deliverables will fall within the following high-level groupings:
Project management documentation
Technical design, configuration and specifications documentation
System configuration and administration
Training development and delivery
QUALIFICATIONS EXPERIENCE & EDUCATION
A minimum of 3-4 years HxGN EAM (or other EAM/enterprise data management software) experience
Strong analytical and problem-solving skills
Proficiency with the Microsoft Office suite of products, including Visio
Excellent interpersonal and communication skills
Working knowledge in Rail industry is preferred.
Show more
Show less","Hexagon EAM Import/Upload Utilities, Power BI, GIS, Visio, HxGN EAM, EAM, Microsoft Office Suite, Rail industry","hexagon eam importupload utilities, power bi, gis, visio, hxgn eam, eam, microsoft office suite, rail industry","eam, gis, hexagon eam importupload utilities, hxgn eam, microsoft office suite, powerbi, rail industry, visio"
Associate Data Engineer,Vivanti Consulting,"New York, NY",https://www.linkedin.com/jobs/view/associate-data-engineer-at-vivanti-consulting-3641784622,2023-12-17,Middletown,United States,Mid senior,Onsite,"About Vivanti
At Vivanti, we are building a better consultancy, one consultant at a time. We believe very strongly in the ability of empowered technically-minded individuals to collaborate on solutions to business problems. We are a modern data and cloud consultancy focusing on the rising demands of cloud-first, data-driven enterprises. We provide services across Data & Analytics, Cloud Technologies, Customer Engagement, and Artificial Intelligence.
About The Role
You will be working on exciting projects identifying our client's needs and providing solutions across many major data platforms (Snowflake, Databricks, and more) atop all major cloud platforms.
As a Data Consultant you will:
Deliver solutions to our customers' pressing data problems and most urgent data needs.
Work on a variety of projects involving data architecture, data models, data migration, data integration, data analysis, and visualization.
Develop end-to-end data pipelines.
Implement solutions for the establishment of data management capabilities including data models and structures, database and data storage infrastructure, master and metadata management, data quality, data warehousing, data transformation, data analysis and data governance.
About You
We are looking for talented technologists who would like to further enhance their data engineering, analytics and AI/ML skills. Your background may be in
data engineering
or
application development
(with strong SQL skills). We are open to graduates who have done a good amount of work with SQL as well as handling data sets. You should be willing to learn & master the leading cloud data platforms (Snowflake, Databricks, AWS, GCP, Azure) and engineering techniques.
Our consultants are adaptable, natural problem solvers, and driven technical people who want to build a better consultancy based on empowerment, autonomy, and doing right by the customer. You bring the technical chops and we will teach you how to consult, how to weave creative solutions to on-the-ground problems, and how to propel your career to the next level. We invest extensively in growth and development providing our consultants with a genuine opportunity for career growth.
What are the technical chops?
We don't expect all of the following skills, but experience in some of the areas below is desirable:
Strong SQL skills
Experience in any of the programming languages (Python, Java, Scala, Go, etc.)
Working knowledge of supporting data structures - SQL or NoSQL databases
Understanding of one of the data platforms ( Snowflake, Databricks, MS SQL Server, Oracle, GCP BigQuery, Azure Synapse)
Working knowledge of Cloud Platforms
Soft Skills
Ability to learn quickly
Ability to communicate effectively across teams, and build positive relationships
Ability to work on teams, collaborate on projects, and contribute to the overall successful delivery of projects
Proactiveness to get things done, be solutions–oriented, and enjoy solving complex problems
Resourcefulness, curiosity, creativity, courage, critical thinking, and a desire to use technology to search for new and innovative solutions to problems
Ability to execute, manage competing priorities, and deliver to deadlines
US Citizenship
Must be based in one of the following locations: New York, Washington DC, or Atlanta.
What about the stock standard benefits that everybody wants?
Medical / Dental / Vision plans picked specifically for both young single professionals and those with families to support.
401(k) match, dollar-for-dollar, up to 4% of your salary.
Flexible paid time-off, designed for adults — take time as you need it when you need it (and you do need at least two weeks off–preferably contiguous–per year!).
Top-of-the-line hardware, whether you want to optimize for horsepower or mobility (mostly MacBook M1s right now).
Vivanti Institute of Technology – an expansive and pervasive learning platform to keep the skills sharp and the mind keen. Learn more about your chosen specialization, whether that's data science, dev/ops, cloud automation, or software engineering. Branch out and pick up a new specialization, or even just dabble in a lot of different arcane technical topics. Whatever you do, we want you to learn and grow.
Sponsored certifications.
We are Vivanti. You can be too.
Above all, we're looking for fun people, with fresh perspectives, and a complete inability to pass up an interesting puzzle or problem to solve. We offer you the opportunity to make a workplace that suits you, with colleagues you love and celebrate, support and encourage. Come help us build a better consultancy, and have a blast along the way!
Show more
Show less","Data Engineering, Analytics, AI/ML, Application Development, SQL, Python, Java, Scala, Go, SQL Databases, NoSQL Databases, Snowflake, Databricks, MS SQL Server, Oracle, GCP BigQuery, Azure Synapse, Cloud Platforms, MacBook M1s","data engineering, analytics, aiml, application development, sql, python, java, scala, go, sql databases, nosql databases, snowflake, databricks, ms sql server, oracle, gcp bigquery, azure synapse, cloud platforms, macbook m1s","aiml, analytics, application development, azure synapse, cloud platforms, data engineering, databricks, gcp bigquery, go, java, macbook m1s, ms sql server, nosql databases, oracle, python, scala, snowflake, sql, sql databases"
Data Engineer,Prismagic Solutions Inc.,"New York, NY",https://www.linkedin.com/jobs/view/data-engineer-at-prismagic-solutions-inc-3768025409,2023-12-17,Middletown,United States,Mid senior,Onsite,"As part of our diverse tech team, you can architect, code and ship software that makes us an essential part of our customers' digital lives. Here, you can work alongside talented engineers in an open, supportive, inclusive environment where your voice is valued, and you make your own decisions on what tech to use to solve challenging problems. Client offers a range of opportunities to work with the latest technologies and encourages you to back the broader engineering community through open source. And because we understand the importance of keeping your skills fresh and relevant, we give you dedicated time to invest in your professional development.
Up to 8 years of software development experience in a professional environment and/or comparable experience such as:
Bachelor's or master's degree in computer science, computer engineering, or other technical discipline, or equivalent work experience, is preferred
5+ years of software development experience in big data technologies such as Python, Spark, PySpark, Spark SQL, Shell Scripting & Hive
Preferred - 5+ Years of hands-on experience in Data Ingestion, Data Organization & Data Consumption frame works using AXP Enterprise Data Platform
Good understanding of big data technologies such as Spark, Mapreduce, YARN, Hive, Zookeeper etc. Preferably, with some real-world experience.
Experience in design and development for batch, streaming and real-time big data applications using AXP EDP platform
Experience in hierarchical data structures in json/ xml
Experience with AXP CI/CD frameworks for code management and deployment like GitHub, Jenkins, XLR
Experience with NoSQL technologies (column-family, key-value or document datastores)
Experience (preferably GCP) and exposure to cloud native big data technologies would be a plus
Working knowledge of message queuing, stream processing, and highly scalable 'big data' data stores
REST API design and implementation experience
Good understanding of NoSQL technologies such as HBase, Cassandra, Redis, Memcached etc. Preferably, with some real-world experience.
Ability to effectively interpret technical and business objectives and challenges and articulate solutions
Understanding of SOA, microservices and containerized application concepts would be a plus
Strong analytical skills and programming skills, in production environment.
Hands on Experience on GraphQL a plus
Experience with Production Support/Dev-Ops would be a plus
Willingness to learn new technologies and exploit them to their optimal potential
Show more
Show less","* Python, * Spark, * PySpark, * Spark SQL, * Shell Scripting, * Hive, * Hadoop, * AXP Enterprise Data Platform, * MapReduce, * YARN, * Zookeeper, * Batch Processing, * Streaming Processing, * RealTime Processing, * JSON, * XML, * GitHub, * Jenkins, * XLR, * NoSQL, * REST API, * HBase, * Cassandra, * Redis, * Memcached, * Apache Flink, * Kafka, * SOA, * Microservices, * Docker, * Kubernetes, * GraphQL, * Production Support, * DevOps","python, spark, pyspark, spark sql, shell scripting, hive, hadoop, axp enterprise data platform, mapreduce, yarn, zookeeper, batch processing, streaming processing, realtime processing, json, xml, github, jenkins, xlr, nosql, rest api, hbase, cassandra, redis, memcached, apache flink, kafka, soa, microservices, docker, kubernetes, graphql, production support, devops","apache flink, axp enterprise data platform, batch processing, cassandra, devops, docker, github, graphql, hadoop, hbase, hive, jenkins, json, kafka, kubernetes, mapreduce, memcached, microservices, nosql, production support, python, realtime processing, redis, rest api, shell scripting, soa, spark, spark sql, streaming processing, xlr, xml, yarn, zookeeper"
Senior Data Engineer (Remote),MMS,"Jersey City, NJ",https://www.linkedin.com/jobs/view/senior-data-engineer-remote-at-mms-3782261658,2023-12-17,Middletown,United States,Mid senior,Onsite,"MMS is an innovative, data focused CRO that supports the pharmaceutical, biotech, and medical device industries with a proven, scientific approach to complex trial data and regulatory submission challenges. Strong industry experience, technology-enabled services, and a data-driven approach to drug development make MMS a valuable CRO partner, creating compelling submissions that meet rigorous regulatory standards. With a global footprint across four continents, MMS maintains a 97 percent customer satisfaction rating, and the company has been recognized as a leading CRO inGlobal Health & Pharma’sinternational awards programs for the last three consecutive years. For more information, visit www.mmsholdings.com or follow MMS on LinkedIn.
Job Description:
Maintains a strong understanding of regulations and guidance as they pertain to data curation deliverables.
Strong understanding of new methods, tools and solutions to meet the data engineering needs of internal and external stakeholders and teams.
Mentors others and advises on MMS, industry trends and technologies to give the technical and non-technical stakeholders a better understanding of data science methodologies and results.
Maintains a strong understanding of Data Science department methodologies and standard practices.
Proficient in conducting peer reviews for others and validation of project deliverables within the team.
Proficient in developing and delivering training for internal and external stakeholders regarding data engineering processes and deliverables.
Strong understanding of CROs and/or Health Systems and the drug development process.
Proficient in developing requirements and specifications from analysis of business needs.
Proficient in preparing, correcting, modifying and analyzing data sets using complex analytic techniques.
Create reusable, highly parameterized pipelines using Microsoft Azure, driven by project-based configuration files to orchestrate landingdatain thedatalake as well as staging to SQLdatabases for analysis.
Applydatamodeling and architecture best practices to stage and transformdatato a commondatamodel. Incorporatedatawarehouse concepts to support dashboard reporting via star schemas and support auditing viadatalineage concepts.
Ability to write T-SQL stored procedures, master window functions, common table expression, and derived tables, utilize dynamic T-SQL, ability to optimize and tune queries and processes.
Thinks like a software developer. Always looking to refactor code, utilize patterns, think abstractly, and work in ways to encapsulate logic to reduce coding side effects.
Requirements:
College graduate in data engineering discipline or related field, or related experience.
Minimum of 7 years’ experience in data engineering or similar field required or an equivalent combination of education and experience.
Create reusable, highly parameterized Azure data factory pipelines, driven by project-based configuration files to orchestrate landing data in the data lake as well as staging to Microsoft SQL databases for analysis.
Apply data modeling and architecture best practices to stage and transform data to a common data model.Incorporate data warehouse concepts to support dashboard reporting via star schemas and support auditing via data lineage concepts.
Ability to write T-SQL stored procedures, master window functions, common table expression, and derived tables, utilize dynamic T-SQL, ability to optimize and tune queries and processes.
Thinks like a software developer.Always looking to refactor code, utilize patterns, think abstractly, and work in ways to encapsulate logic to reduce coding side effects.
Expert knowledge of data engineering concepts.
Reputation as emerging leader in field with sustained performance and accomplishment.
Hands-on experience with clinical trial and pharmaceutical development preferred.
Good communication skills and willingness to work with others to clearly understand needs and solve problems.
Excellent problem-solving skills.
Good organizational and communication skills.
Familiarity with data privacy and anonymization regulations preferred.
Familiarity with current ISO 9001 and ISO 27001 standards preferred.
Familiarity with 21 CFR Part 11, FDA, and GCP requirements.
Familiarity with industry standard data models (CDISC, FHIR, OMOP) preferred.
Basic understanding of CROs and scientific & clinical data/terminology, & the drug development process
Proficiency with MS Office applications.
Show more
Show less","Data engineering, Data science methodologies, Peer reviews, Data pipelines, Data modeling, Data architecture, Common data model, Data warehouse concepts, Star schemas, Data lineage concepts, TSQL stored procedures, Master window functions, Common table expression, Derived tables, Dynamic TSQL, Microsoft Azure, SQL databases, Data privacy, Anonymization regulations, ISO 9001, ISO 27001, 21 CFR Part 11, FDA, GCP, CDISC, FHIR, OMOP, MS Office applications","data engineering, data science methodologies, peer reviews, data pipelines, data modeling, data architecture, common data model, data warehouse concepts, star schemas, data lineage concepts, tsql stored procedures, master window functions, common table expression, derived tables, dynamic tsql, microsoft azure, sql databases, data privacy, anonymization regulations, iso 9001, iso 27001, 21 cfr part 11, fda, gcp, cdisc, fhir, omop, ms office applications","21 cfr part 11, anonymization regulations, cdisc, common data model, common table expression, data architecture, data engineering, data lineage concepts, data privacy, data science methodologies, data warehouse concepts, datamodeling, datapipeline, derived tables, dynamic tsql, fda, fhir, gcp, iso 27001, iso 9001, master window functions, microsoft azure, ms office applications, omop, peer reviews, sql databases, star schemas, tsql stored procedures"
"Senior Data Engineer: New York, NY, San Francisco, CA",eStaffing Inc.,"New York, NY",https://www.linkedin.com/jobs/view/senior-data-engineer-new-york-ny-san-francisco-ca-at-estaffing-inc-3741122928,2023-12-17,Middletown,United States,Mid senior,Onsite,"Job Title -
Senior Data Engineer :
New York, NY,
San Francisco, CA
Location -
New York, NY,
San Francisco, CA
Job Type- Full Time
Shift- Day
Job Requirements-
Experience: 4+ years of experience as a Data Engineer
Experience: hands-on experience building or maintaining data pipelines with modern orchestration tools like Airflorw/Prefect/Dragster
Technical Skills: strong programming skills in languages like Python, Scala or Javascript, with a solid understanding of software engineering principles
Technical Skills: strong expertise with SQL and non-SQL DBs, cloud-based data platforms (e.g., AWS, GCP) and their related services (S3, BigQuery, etc.)
SQL: Expert level writing of SQL for data manipulation, transformation and for analytics
Dashboarding: Experience building and maintaining interactive and intuitive dashboards using tools like Looker
Development: Comfortable with tool development, including building and enhancing internal data tools and frameworks
Apply for full job position and will share full job details in your email id.
#DataEngineer #SeniorDataEngineer #DataPipelines #DataProcessing #DataTransformation #DataAnalytics #DataDashboards #ToolDevelopment #DataModels #DataInfrastructure #DataVisualization #DataDriven #DataTools #DataFrameworks #SQLExpert #OrchestrationTools #PythonProgramming #ScalaProgramming #JavaScriptProgramming #SoftwareEngineering #CloudPlatforms #AWS #GCP #S3 #BigQuery #Looker #DataMentor
Show more
Show less","Data Engineering, Data Pipelines, Data Processing, Data Transformation, Data Analytics, Data Dashboards, Tool Development, Data Models, Data Infrastructure, Data Visualization, DataDriven, Data Tools, Data Frameworks, SQL Expert, Orchestration Tools, Python, Scala, Javascript, Software Engineering, Cloud Platforms, AWS, GCP, S3, BigQuery, Looker","data engineering, data pipelines, data processing, data transformation, data analytics, data dashboards, tool development, data models, data infrastructure, data visualization, datadriven, data tools, data frameworks, sql expert, orchestration tools, python, scala, javascript, software engineering, cloud platforms, aws, gcp, s3, bigquery, looker","aws, bigquery, cloud platforms, data dashboards, data engineering, data frameworks, data infrastructure, data models, data processing, data tools, data transformation, dataanalytics, datadriven, datapipeline, gcp, javascript, looker, orchestration tools, python, s3, scala, software engineering, sql expert, tool development, visualization"
ML Data Engineer,First Soft Solutions LLC,"Piscataway, NJ",https://www.linkedin.com/jobs/view/ml-data-engineer-at-first-soft-solutions-llc-3712848910,2023-12-17,Middletown,United States,Mid senior,Onsite,"We are actively hiring ML Data Engineer
Remote
7+ years of development experience in big data development with good work experience in Hadoop, Hive, Spark, PySpark and GCP OR AWS with experience in health care domain and ML.
Should have strong knowledge on building hive pipelines and prior project experience in Agile methodology.
Knowledge on tools like JIRA, Github is desired.
Technical Skills : Apache Hadoop ,Hive ,PySpark ,Google Cloud Platform, ML
Data Engineer with health care knowledge, good analytical skill, development experience in Hadoop, Hive, Spark, PySpark and GCP OR AWS with ability to develop Hive pipeline, develop pyspark code in GCP OR AWS environment Configuration Management-Maintain versions of the code or consolidate version maintained by the Developers Provide support as required to the Administrators during configuration, code back-ups, deployment etc.Deployment-Assess and create deployment/ roll back plan Validate if all the components have been migrated and the right version is checked in Perform sanity checks post deployment to ensure smooth production Share activity status with supervisor and highlight concerns if anyProject Execution Monitoring & closure (Support to Project Management activities)-Monitor work of the developers and share work achieved with them Provide guidance through SDLC Provide status of progress to leads Knowledge Management-Post release participate in project review call and discuss points on what went well and what didnt Create and update knowledge articles (case studies, lessons learnt) in the knowledge management repository Guide developers in creating such documents Publish white papers/ blogs/ articles (if required)People Management-Conduct training through academy or internally within the team Conduct technical, face to face interviews for internal transfer or external hiring Provide feedback on Developers form technical /domain standpoint to the module lead
Please share updated profiles suriya@firstsoftsolutions.net
Show more
Show less","Apache Hadoop, Hive, PySpark, Google Cloud Platform, ML, Jira, Github, Configuration Management, Deployment, Project Execution Monitoring & Closure, Knowledge Management, People Management","apache hadoop, hive, pyspark, google cloud platform, ml, jira, github, configuration management, deployment, project execution monitoring closure, knowledge management, people management","apache hadoop, configuration management, deployment, github, google cloud platform, hive, jira, knowledge management, ml, people management, project execution monitoring closure, spark"
GOOGLE CLOUD DATA ENGINEER,First Soft Solutions LLC,"Piscataway, NJ",https://www.linkedin.com/jobs/view/google-cloud-data-engineer-at-first-soft-solutions-llc-3693718502,2023-12-17,Middletown,United States,Mid senior,Onsite,"We are actively hiring #GCP Data Engineer
W2 position
Job title GOOGLE CLOUD DATA ENGINEER
Experience 7+
Required Skill: Hive, Apache Spark, BigQuery
Nice to have skills ,Unix Shell Scripting, Python, PySpark, Cloud Dataproc, Cloud SQL, Google BigQuery
Please share profiles to sony@firstsoftsolutions.com
732 609 2952
Show more
Show less","GCP, Data Engineer, Hive, Apache Spark, BigQuery, Unix Shell Scripting, Python, PySpark, Cloud Dataproc, Cloud SQL, Google BigQuery","gcp, data engineer, hive, apache spark, bigquery, unix shell scripting, python, pyspark, cloud dataproc, cloud sql, google bigquery","apache spark, bigquery, cloud dataproc, cloud sql, dataengineering, gcp, google bigquery, hive, python, spark, unix shell scripting"
GCP-DATA ENGINEER,Workcog Inc,"Brooklyn, NY",https://www.linkedin.com/jobs/view/gcp-data-engineer-at-workcog-inc-3755543811,2023-12-17,Middletown,United States,Mid senior,Onsite,"Role: GCP DATA ENGINEER
Contract
Primary Skills
BITBUCKET , SPARK , BIG QUERY , PERL , ENGINEER , SHELL , GITFLOW , APACHE AIRFLOW , RDBMS , JENKINS , TFS , SCALA , APACHE HIVE , JIRA , HADOOP , PYTHON , APACHE SPARK , JAVA
Description
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Extensive Experience on BigQuery, DataProc and DataFlow platforms on Google Cloud platform. Having experience on Azure Databricks is an added advantage (not mandatory).
Experience on Cluster capacity configurations and cloud optimization to meet application demand.
Programming experience on Python, Shell scripting, PySpark and other data programming languages.
Programming experience on Apache Beam Java SDK for building effective heavy data pipelines and deploying them in GCP DataFlow. CICD process to deploy these pipelines in GCP.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with Data Visualization Dashboard, Metrics, etc.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable 'big data' data stores.
Familiar with Deployment tool like Docker and building CI/CD pipelines.
Experience supporting and working with cross-functional teams in a dynamic environment.
8+ years' experience in software development, Data engineering, and
Bachelor's degree in computer science, Statistics, Informatics, Information Systems or another quantitative field. Postgraduate/master's degree is preferred.
Experience in Machine Learning and Data Modeling is a plus
Show more
Show less","BigQuery, DataProc, DataFlow, SQL, Python, Shell scripting, PySpark, Apache Beam Java SDK, GCP DataFlow, CICD, Data Visualization Dashboard, Docker, CI/CD pipelines, Machine Learning, Data Modeling","bigquery, dataproc, dataflow, sql, python, shell scripting, pyspark, apache beam java sdk, gcp dataflow, cicd, data visualization dashboard, docker, cicd pipelines, machine learning, data modeling","apache beam java sdk, bigquery, cicd, cicd pipelines, data visualization dashboard, dataflow, datamodeling, dataproc, docker, gcp dataflow, machine learning, python, shell scripting, spark, sql"
Senior Data Engineer,Shaped,"New York, NY",https://www.linkedin.com/jobs/view/senior-data-engineer-at-shaped-3625989554,2023-12-17,Middletown,United States,Mid senior,Onsite,"We are looking for a data engineer to design, build and optimize Shaped's real-time and batch streaming infrastructure. You will be a founding engineer working to reliably ingest customer data (both with batch and real-time processing) into our our state-of-the-art AI discovery engine. As one of Shaped’s early employees you will help shape our product, culture and vision.
Skills should include Python, Data Warehouses (such as Clickhouse, Snowflake, or BigQuery)
Nice-to-have skills should include DBT, Meltano, Airflow, and Apache Flink (or other stream processing frameworks)
We’re excited to work with you. Come build the future of AI with us!
Show more
Show less","Python, Data Warehouses, Apache Flink, SQL, DBT, Meltano, Airflow, BigQuery, Clickhouse, Snowflake, Stream processing frameworks","python, data warehouses, apache flink, sql, dbt, meltano, airflow, bigquery, clickhouse, snowflake, stream processing frameworks","airflow, apache flink, bigquery, clickhouse, data warehouses, dbt, meltano, python, snowflake, sql, stream processing frameworks"
Senior MS Azure Data Engineer,Spanco Solutions,"New York, NY",https://www.linkedin.com/jobs/view/senior-ms-azure-data-engineer-at-spanco-solutions-3745399751,2023-12-17,Middletown,United States,Mid senior,Onsite,"Position: Senior MS Azure Data Engineer
Location: Newyork – Onsite
Long Term contract
Job Description
Must have 4+ years of experience working for US companies in the US.
Must have 4+ years of Experience working in a Banking or Brokerage environment.
Must have 4+ years of experience with Azure SQL and Azure Cloud Environments versions 2012 - 2023.Must have experience with Blob Storage, Queue Storage, table storage, and files.
Must have minimum of 5 years scripting and automation experience with PowerShell, Spark, Scala, Lambda, Python, and Python/R.
Must have experience with USQL and experience in building pipeline from multiple sources.
Must have experience with Azure Data Lake Store(ADLS), Integration Run Time(IR), Azure Data Factory(ADF),File System Data Ingestion, Relational Data Ingestion.
Experience designing and building production data pipelines from ingestion to consumption
Must have experience with Data Lake, Data Factory experience.
Experience in building a data pipeline.
Experience in designing and implementing data engineering, ingestion and curation functions on Azure cloud using Azure native or custom programming
Must have 5 years of experience in design, development, and maintenance of large scale database projects.
Must have 5 years of experience in database maintenance and release operations.
Must have experience with Database Code Promotion and Release Process.
Must have experience with Microsoft SQL, ETL processes using SQL Server Integration Services (SSIS) for large-scale OLTP and data warehouse systems.
Must have experience with evangelizing database best practices across company.
Must have experience with emerging technologies to data management such as Hadoop, MongoDB, and other NoSQL platforms.
Expert knowledge of SQL Server environments for designing and developing databases.
Familiarity with Microsoft .NET framework and C# is preferred.
Exposure to Service Oriented Architecture (SOA) and web services is preferred.
Experience working in an Agile/Scrum environment.
Familiarity with K-12 data warehousing and analytics is preferred.
Bachelor's degree in Computer Science or equivalent degree in a related field is required.
The JOB
Client is looking for Senior Azure Database developers to join their growing software development team to:
The Data Developer will be working with our software development team to produce high quality work, participate in development meetings and database architecture meetings, and adhere to project schedules.
The Database Developer will develop and maintain data downloads and processing data transfer utilities.
Will work with the Azure Cloud Environment.
Expected to have excellent communication skills, teamwork, collaboration, and bias for action.
Design and develop optimal database solutions that cater to the variety of application and business requirements.
Create, maintain, and execute SQL Server jobs.
Create, maintain, and deploy SSIS packages using BIDS.
Maintain several databases in Azure Cloud Environment and SQL Server 2019 on-premise.
Ensure performance, security, and availability of databases
Experience working in an Agile/SCRUM model.
Show more
Show less","MS Azure, SQL, Cloud Environments, Blob Storage, Queue Storage, Table Storage, Files, PowerShell, Spark, Scala, Lambda, Python, Python/R, USQL, Azure Data Lake Store (ADLS), Integration Run Time (IR), Azure Data Factory (ADF), File System Data Ingestion, Relational Data Ingestion, Data Lake, Data Factory, Data Pipeline, Azure Cloud, Database Code Promotion and Release Process, SQL Server Integration Services (SSIS), Hadoop, MongoDB, NoSQL, SQL Server, .NET Framework, C#, Service Oriented Architecture (SOA), Web Services, Agile/Scrum, K12 Data Warehousing, Analytics, Computer Science","ms azure, sql, cloud environments, blob storage, queue storage, table storage, files, powershell, spark, scala, lambda, python, pythonr, usql, azure data lake store adls, integration run time ir, azure data factory adf, file system data ingestion, relational data ingestion, data lake, data factory, data pipeline, azure cloud, database code promotion and release process, sql server integration services ssis, hadoop, mongodb, nosql, sql server, net framework, c, service oriented architecture soa, web services, agilescrum, k12 data warehousing, analytics, computer science","agilescrum, analytics, azure cloud, azure data factory adf, azure data lake store adls, blob storage, c, cloud environments, computer science, data factory, data lake, data pipeline, database code promotion and release process, file system data ingestion, files, hadoop, integration run time ir, k12 data warehousing, lambda, mongodb, ms azure, net framework, nosql, powershell, python, pythonr, queue storage, relational data ingestion, scala, service oriented architecture soa, spark, sql, sql server, sql server integration services ssis, table storage, usql, web services"
Lead Data Software Engineer,Talener,"New York, NY",https://www.linkedin.com/jobs/view/lead-data-software-engineer-at-talener-3774888230,2023-12-17,Middletown,United States,Mid senior,Onsite,"Our client is a a global risk and reinsurance specialist based in New York, they are a subsidiary of one of the largest professional services firm in the world and they have recently created a state of the art Data Strategy start-up division to focus on the acquisition, storage, analysis, fidelity, and monetization of data. They are seeking a Lead Engineer to co-lead their team as they scale and grow, it is impetrative that this person not only have a Data Engineering and ML/AI background but also the ability to write high-level production grade software code in Python. Ideal candidates will have began their career in software engineering and transitioned into the world of data. This is a critical hire at a Lead level so interested applicants must have 6+ years of experience and ideally will be coming out of technology focused product companies. This position requires hybrid work in NYC and only local candidates who are legally authorized to work in the United States without sponsorship will be considered.
Title:
Lead Data Software Engineer
Location:
New York, NY (hybrid work required - up to 3x / week in office - local candidates only)
Required Skills And Responsibilities
6 + years of overall engineering experience, ideally with 2+ years spent in each of the following functions; Python Engineer, Data Engineer, AI / ML Engineer
Backend Web: Experience working with Python-based server-side web frameworks like FastAPI or Django
Data Engineering: Extensive experience with (py)Spark, Python, JSON, and SQL
ML / AI & Data Science: Experience selecting, training, validating, and deploying machine-learning models
Experience building and maintaining CI/CD pipelines with tools such as Azure DevOps, GitLab, Travis, Jenkins, etc.
Knowledge of cloud-based web deployments (AWS/Azure/GCP, Kubernetes, auto-scaling, etc.)
Experience architecting web ecosystems from the ground up
Additional Skills
Experience with the MS Azure cloud environment
Experience with Databricks and optimizing Spark clusters
Experience working with data visualization dashboarding tools (PowerBI, Tableau)
Insurance domain knowledge or related industry experience
Prior experience leading teams and projects at an enterprise level
Compensation And Benefits
Base Salary in the range of $200,000 - $230,000
Lucrative annual bonus potential
Comprehensive benefits packages and plans
For more information or to apply, reach out to John Higgins at jhiggins@talener.com
Show more
Show less","Data Engineering, Machine Learning, Artificial Intelligence, Python, FastAPI, Django, PySpark, JSON, SQL, Azure DevOps, GitLab, Travis, Jenkins, AWS, Azure, GCP, Kubernetes, MS Azure, Databricks, PowerBI, Tableau, Insurance, Team Leadership, Project Management","data engineering, machine learning, artificial intelligence, python, fastapi, django, pyspark, json, sql, azure devops, gitlab, travis, jenkins, aws, azure, gcp, kubernetes, ms azure, databricks, powerbi, tableau, insurance, team leadership, project management","artificial intelligence, aws, azure, azure devops, data engineering, databricks, django, fastapi, gcp, gitlab, insurance, jenkins, json, kubernetes, machine learning, ms azure, powerbi, project management, python, spark, sql, tableau, team leadership, travis"
"Junior Data Engineer - Jersey City, NJ",NTT DATA Services,"Jersey City, NJ",https://www.linkedin.com/jobs/view/junior-data-engineer-jersey-city-nj-at-ntt-data-services-3694435936,2023-12-17,Middletown,United States,Mid senior,Onsite,"Req ID:
247480
NTT DATA Services strives to hire exceptional, innovative and passionate individuals who want to grow with us. If you want to be part of an inclusive, adaptable, and forward-thinking organization, apply now.
We are currently seeking a Junior Data Engineer - Jersey City, NJ to join our team in Jersey City, New Jersey (US-NJ), United States (US).
NTT Data Americas is Hiring!
This is a permanent, fulltime, salaried position with benefits. Not open to C2C or C2H.
Job Description
Data Engineer
Responsibilities:
Develop, maintain, and optimize data pipelines to extract, transform, and load large datasets from diverse sources into our data ecosystem.
Design and implement efficient and scalable data models that align with business requirements, ensuring data integrity and performance.
Collaborate with cross-functional teams to understand data needs and deliver solutions that meet those requirements.
Work closely with data scientists, analysts, and software engineers to ensure seamless integration of data solutions into larger systems.
Identify and resolve data quality issues, ensuring accuracy, reliability, and consistency of the data infrastructure.
Continuously monitor and improve data pipelines and processes, identifying opportunities for automation and optimization.
Stay updated with emerging trends, technologies, and best practices in data engineering, data modeling, and backend Java engineering.
Provide technical guidance and mentorship to junior team members, fostering their growth and development.
Requirements:
Bachelor’s or Master’s degree in Computer Science, Engineering, or a related field.
5+years of hands-on experience as a Data Engineer, working on complex data projects and implementing data modeling solutions.
Data Engineering
Must have
Solid understanding of SQL and expertise in working with relational databases (e.g., PostgreSQL, MySQL).
In-depth knowledge of data modeling techniques and experience with data modeling tools.
Proficiency in designing and optimizing data pipelines using ETL/ELT frameworks and tools (e.g., Informatica, Apache Spark, Airflow, AWS Glue).
Working knowledge on Data warehousing
Familiarity with cloud-based data platforms and services (e.g., Snowflake, AWS, Google Cloud, Azure).
Experience with version control systems (e.g., Git) and agile software development methodologies.
Strong communication skills to effectively convey technical concepts to both technical and non-technical stakeholders.
Excellent problem-solving skills and the ability to work independently and collaboratively in a fast-paced environment.
Good to Have
JAVA 8, REST APIs, and microservices, Spring Boot framework
Alteryx
UNIX scripting
Primary Skillset: Data Engineering
Solid understanding of SQL and expertise in working with relational databases (e.g., DB2, MySQL).
Data Modelling knowledge
cloud-based data platforms like Snowflake
Working knowledge on Data warehousing
Cloud-based data platforms and services (e.g., AWS, Google Cloud, Azure)
Alteryx (good to have)
ETL/ELT tools like Informatica, Apache Spark
UNIX scripting
Good to have: GS specific tools like Alloy Registry, LEGEND tool for Alloy data modelling, PURE, Data Browser, Data Lake etc
About NTT DATA Services
NTT DATA Services is a recognized leader in IT and business services, including cloud, data and applications, headquartered in Texas. As part of NTT DATA, a $30 billion trusted global innovator with a combined global reach of over 80 countries, we help clients transform through business and technology consulting, industry and digital solutions, applications development and management, managed edge-to-cloud infrastructure services, BPO, systems integration and global data centers. We are committed to our clients’ long-term success. Visit nttdata.com or LinkedIn to learn more.
NTT DATA Services is an equal opportunity employer and considers all applicants without regarding to race, color, religion, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other characteristic protected by law. We are committed to creating a diverse and inclusive environment for all employees. If you need assistance or an accommodation due to a disability, please inform your recruiter so that we may connect you with the appropriate team.
#INDFSINS
Show more
Show less","Java, Rest APIs, Microservices, Spring Boot, SQL, Relational databases, Data modeling, Cloudbased data platforms, Data warehousing, ETL/ELT, Informatica, Apache Spark, Airflow, AWS Glue, Snowflake, Azure, Git, Agile software development, Alteryx, Unix scripting","java, rest apis, microservices, spring boot, sql, relational databases, data modeling, cloudbased data platforms, data warehousing, etlelt, informatica, apache spark, airflow, aws glue, snowflake, azure, git, agile software development, alteryx, unix scripting","agile software development, airflow, alteryx, apache spark, aws glue, azure, cloudbased data platforms, datamodeling, datawarehouse, etlelt, git, informatica, java, microservices, relational databases, rest apis, snowflake, spring boot, sql, unix scripting"
Senior SQL Data Developer,Accord Technologies Inc,"New York, NY",https://www.linkedin.com/jobs/view/senior-sql-data-developer-at-accord-technologies-inc-3688002118,2023-12-17,Middletown,United States,Mid senior,Onsite,"Senior SQL Data Developer
Location: Nyc, NY
Duration: 6 months
Hybrid in NY for 3 days a week
Position Overview
The Senior Data Developer role is responsible for supporting Business Intelligence and Reporting initiatives. Main responsibilities are creating data structures for reporting systems and developing data exchanges and integrations between various platforms. Important aspect of this role is optimizing existing and new data processes.
Basic Requirements
Expert knowledge of SQL
Extensive hands on experience on MS SQL Server platform
Deep understanding of RDBMS concepts
Hands On Experience Developing Stored Procedures, Functions And Scripts
Expertise in database/query performance optimization (Indexing, query tuning, troubleshooting performance problems)
Expertise in developing ETL and batch processes to support data movement
Experience in performing source to target data mapping exercises
Strong knowledge of Data Warehousing concepts
Exposure to data modeling/database design
Curious, relentless with excellent written and verbal communication and interpersonal skills
Desired Skills
Experience developing/deploying reports on SSRS/Cognos/PowerBI platform
Experience working with BIG data and unstructured data sources (AWS)
Experience with streaming data pipelines (Streamsets, Kafka)
Financial Services industry experience
Familiarity with Salesforce
Show more
Show less","SQL, MS SQL Server, RDBMS, Stored Procedures, Functions, Scripts, Database/Query Performance Optimization, ETL, Batch Processes, Data Warehousing Concepts, Data Modeling/Database Design, SSRS, Cognos, PowerBI, BIG Data, Unstructured Data Sources, AWS, Streaming Data Pipelines, Streamsets, Kafka, Financial Services Industry Experience, Salesforce","sql, ms sql server, rdbms, stored procedures, functions, scripts, databasequery performance optimization, etl, batch processes, data warehousing concepts, data modelingdatabase design, ssrs, cognos, powerbi, big data, unstructured data sources, aws, streaming data pipelines, streamsets, kafka, financial services industry experience, salesforce","aws, batch processes, big data, cognos, data modelingdatabase design, data warehousing concepts, databasequery performance optimization, etl, financial services industry experience, functions, kafka, ms sql server, powerbi, rdbms, salesforce, scripts, sql, ssrs, stored procedures, streaming data pipelines, streamsets, unstructured data sources"
"Looking for Sr. Data Engineer - NYC, NY, USA (Onsite)- Contract",Extend Information Systems Inc.,"New York, NY",https://www.linkedin.com/jobs/view/looking-for-sr-data-engineer-nyc-ny-usa-onsite-contract-at-extend-information-systems-inc-3750875634,2023-12-17,Middletown,United States,Mid senior,Onsite,"Hi,
I hope you are doing well!
We have an opportunity for
Sr. Data Engineer
with one of our clients for
NYC, NY, USA (Onsite)
Please see the job details below and let me know if you would be interested in this role.
If interested, please send me a copy of your resume, contact details, availability, and a good time to connect with you.
Title:
Sr. Data Engineer
Location:
NYC, NY, USA (Onsite)
Terms:
Contract
Job Details
8+ years of data engineering experience
Experience building streaming pipelines using Scala, Spark, and SQL
Must have good exposure in AWS technology
Excellent communication and ownership skills
Thanks & Regards
Monika Singh
Extend Information System Inc
Email: monika@extendinfosys.com
44258 Mercure Circle, UNIT 102 A, Sterling VA, USA – 20166
Show more
Show less","Scala, Spark, SQL, AWS, Data Engineering","scala, spark, sql, aws, data engineering","aws, data engineering, scala, spark, sql"
Data Engineer - Consultant,Vivanti Consulting,"New York, NY",https://www.linkedin.com/jobs/view/data-engineer-consultant-at-vivanti-consulting-3641780934,2023-12-17,Middletown,United States,Mid senior,Onsite,"Please note this is a full-time permanent opportunity and you must be located and authorized to work in the US*
About Vivanti
At Vivanti, we are building a better consultancy, one consultant at a time. We believe very strongly in the ability of empowered technically-minded individuals to collaborate on solutions to business problems. We are a modern data and cloud consultancy focusing on the rising demands of cloud-first, data-driven enterprises. We provide services across Data & Analytics, Cloud Technologies, Customer Engagement, and Artificial Intelligence.
About The Role
You will be working on exciting projects identifying our client's needs and providing solutions across many major data platforms (Snowflake, Databricks, and more) atop all major cloud platforms.
As a Data Consultant you will:
Deliver solutions to our customers' pressing data problems and most urgent data needs.
Work on a variety of projects involving data architecture, data models, data migration, data integration, data analysis, and visualization.
Develop end-to-end data pipelines.
Implement solutions for the establishment of data management capabilities including data models and structures, database and data storage infrastructure, master and metadata management, data quality, data warehousing, data transformation, data analysis and data governance.
Support pre-sales activity to promote Vivanti's services and capabilities to current and prospective clients.
About You
We are looking for a strong technologist who would like to further enhance their data engineering, analytics and AI/ML skills. Your background may be
data engineering
or
application development
(with strong SQL skills). Your experience may be in legacy data platforms (MS SQL, Oracle, DB2, Informix etc.), but should be willing to learn & master the leading cloud data platforms (Snowflake, Databricks, AWS, GCP, Azure) and engineering techniques.
Our consultants are adaptable, natural problem solvers, and driven technical people who want to build a better consultancy based on empowerment, autonomy, and doing right by the customer. You bring the technical chops and we will teach you how to consult, how to weave creative solutions to on-the-ground problems, and how to propel your career to the next level. We invest extensively in growth and development providing our consultants with a genuine opportunity for career growth.
What are the technical chops?
We don't expect all of the following skills, but experience in some of the areas below is desirable:
SQL. Lots of it. Everything is SQL. (at least 2 years SQL coding)
Experience (at least 2 years) in Data Engineering - designing and building data pipelines.
Exposure to modern cloud data warehouses like Snowflake, BigQuery and Databricks.
Data Lakes. Know what they are, when to use them, and how to play to their strengths.
Experience (at least 2 years) with ETL / ELT processes and tools, preferably hands-on in live production environments.
Certifications to be attained within 3 months – Snowflake, dbt, Databricks.
More important than technical skills, we are looking for individuals with a positive attitude and willingness to learn and experiment. We are happy to build on your passion for technology and invest in training to make you a successful consultant.
What about the stock standard benefits that everybody wants?
Medical / Dental / Vision plans picked specifically for both young single professionals and those with families to support.
401(k) match, dollar-for-dollar, up to 4% of your salary.
Flexible paid time-off, designed for adults — take time as you need it, when you need it (and you do need at least two weeks off–preferably contiguous–per year!).
Top-of-the-line hardware, whether you want to optimize for horsepower or mobility (mostly MacBook M1s right now).
Vivanti Institute of Technology – an expansive and pervasive learning platform to keep the skills sharp and the mind keen. Learn more about your chosen specialization, whether that's data science, dev/ops, cloud automation, or software engineering. Branch out and pick up a new specialization, or even just dabble in a lot of different arcane technical topics. Whatever you do, we want you to learn and grow.
Sponsored certifications.
We are Vivanti. You can be too.
Above all, we're looking for fun people, with fresh perspectives, and a complete inability to pass up an interesting puzzle or problem to solve. We offer you the opportunity to make a workplace that suits you, with colleagues you love and celebrate, support and encourage. Come help us build a better consultancy, and have a blast along the way!
Show more
Show less","SQL, Data Engineering, Data Pipelines, Cloud Data Warehouses, Snowflake, BigQuery, Databricks, Data Lakes, ETL / ELT, dbt, AWS, GCP, Azure, MacBook M1, Data Science, Dev/Ops, Cloud Automation, Software Engineering","sql, data engineering, data pipelines, cloud data warehouses, snowflake, bigquery, databricks, data lakes, etl elt, dbt, aws, gcp, azure, macbook m1, data science, devops, cloud automation, software engineering","aws, azure, bigquery, cloud automation, cloud data warehouses, data engineering, data lakes, data science, databricks, datapipeline, dbt, devops, etl elt, gcp, macbook m1, snowflake, software engineering, sql"
Senior Data Analytics Engineer,Fastly,"New York, NY",https://www.linkedin.com/jobs/view/senior-data-analytics-engineer-at-fastly-3775559402,2023-12-17,Middletown,United States,Mid senior,Onsite,"Fastly helps people stay better connected with the things they love. Fastly’s edge cloud platform enables customers to create great digital experiences quickly, securely, and reliably by processing, serving, and securing our customers’ applications as close to their end-users as possible — at the edge of the Internet. The platform is designed to take advantage of the modern internet, to be programmable, and to support agile software development. Fastly’s customers include many of the world’s most prominent companies, including Vimeo, Pinterest, The New York Times, and GitHub.
We're building a more trustworthy Internet. Come join us.
Senior Data Analytics Engineer
The Analytics team is building a modern data stack to cultivate data-driven decision-making across our high growth business. We are looking for a Senior Analytics Engineer to join our Analytics team, who is inquisitive, curious, and passionate about solving complex business problems with analysis. This role will provide the opportunity to work closely with business partners and build KPIs, dashboards, pipelines, and exploratory analysis to empower a data-driven culture across the company.
What You'll Do
You’ll collaborate and partner with executives, stakeholders, and decision-makers to prioritize, design, and fulfill their data analytics needs
You’ll identify KPIs driving business growth and performance and work closely with the business teams to define them
You’ll work across the data stack to design and build various data products (data models, dashboards, analysis) that enable the business to make data backed decisions
You’ll help garner trust in data from the business by documenting analytics best practices, assisting with data discoverability, governance, and accuracy
You’ll promote and drive a self-service data culture by building easy to use analytics products and teaching the business users how to use them
You’ll communicate progress, risks, and completion of projects to stakeholders
What We're Looking For
You have 5+ years of hands-on experience in BI or Analytics teams.
You have hands-on experience in building and managing data transformations (dbt experience is a plus)
You are able to craft functional and user-friendly data products (data models, dashboards, etc..) for both technical and non-technical stakeholders
You have experience with cloud data warehouse technologies (BigQuery is a plus)
You have experience with a data visualization tools (ie. Looker, Tableau, Mode)
You are a SQL expert, capable of complex, efficient, and logical SQL to accomplish data analysis goals
You are driven by logical and analytical thinking to solve problems. And have an eye for detail-oriented analysis and design
You have functional knowledge of financial, sales, or other core business metrics
You are able to manage professional relationships across a variety of stakeholders, including executives
You are able to identify and prioritize multiple requirements and manage expectations accordingly
You are collaborative and willing to traverse and contribute to the entire data stack, from front-end to back-end as needed
Work Hours:
This position will require you to be available during core business hours.
Work Location(s) & Travel Requirements:
This position is open to the following preferred office locations:
San Francisco, CA USA
Denver, CO USA
New York, NY USA
Culver City, CA USA
Fastly currently embraces a largely hybrid model for most roles which allows employees flexibility to split their time between the office and home.
This position may require travel as required by your role or requested by your manager.
Salary
The estimated salary range for this position is $143,860 to $179,820.
Starting salary may vary based on permissible, non-discriminatory factors such as experience, skills, qualifications, and location.
This role may be eligible to participate in Fastly’s equity and discretionary bonus programs.
Benefits
We care about you. Fastly works hard to create a positive environment for our employees, and we think your life outside of work is important too. We support our teams with great benefits that start on the first day of your employment with Fastly. Curious about our offerings?
We offer a comprehensive benefits package including medical, dental, and vision insurance. Family planning, mental health support along with Employee Assistance Program, Insurance (Life, Disability, and Accident), an open vacation policy and up to 18 days of accrued paid sick leave are there to help support our employees. We also offer 401(k) (including company match) and an Employee Stock Purchase Program. For 2023, we offer 10 paid local holidays, 11 paid company wellness days.
Why Fastly?
We have a huge impact. Fastly is a small company with a big reach. Not only do our customers have a tremendous user base, but we also support a growing number of open source projects and initiatives. Outside of code, employees are encouraged to share causes close to their heart with others so we can help lend a supportive hand.
We love distributed teams. Fastly’s home-base is in San Francisco, but we have multiple offices and employees sprinkled around the globe. As a new hire, you will be able to attend our IN-PERSON new hire orientation in our San Francisco office! It is an exciting week-long experience that we offer to new employees to build connections with colleagues across Fastly, participate in hands-on learning opportunities, and immerse yourself in our culture firsthand.
We value diversity. Growing and maintaining our inclusive and diverse team matters to us. We are committed to being a company where our employees feel comfortable bringing their authentic selves to work and have the ability to be successful -- every day.
We are passionate. Fastly is chock full of passionate people and we’re not ‘one size fits all’. Fastly employs authors, pilots, skiers, parents (of humans and animals), makeup geeks, coffee connoisseurs, and more. We love employees for who they are and what they are passionate about.
We’re always looking for humble, sharp, and creative folks to join the Fastly team. If you think you might be a fit please apply!
A fully completed application and resume or CV are required when applying.
Fastly is committed to ensuring equal employment opportunity and to providing employees with a safe and welcoming work environment free of discrimination and harassment. Our employment decisions are based on business needs, job requirements and individual qualifications.
All qualified applicants will receive consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, family or parental status, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by applicable laws, regulations and ordinances.
Consistent with the Americans with Disabilities Act (ADA) and federal or state disability laws, Fastly will provide reasonable accommodations for applicants and employees with disabilities. If reasonable accommodation is needed to participate in the job application or interview process, to perform essential job functions, and/or to receive other benefits and privileges of employment, please contact your Recruiter, or the Fastly Employee Relations team at
candidateaccommodations@fastly.com
or 501-287-4901.
Fastly collects and processes personal data submitted by job applicants in accordance with our Privacy Policy. Please see our privacy notice for job applicants.
Show more
Show less","Data Analytics, Business Intelligence, dbt, Data Visualization, Looker, Tableau, Mode, SQL, BigQuery, Financial Metrics, Sales Metrics, Business Metrics, Stakeholder Management, Data Warehousing, Data Modeling, Data Pipelines, Data Governance, Data Security, Data Analysis, Data Mining, Data Exploration, Reporting","data analytics, business intelligence, dbt, data visualization, looker, tableau, mode, sql, bigquery, financial metrics, sales metrics, business metrics, stakeholder management, data warehousing, data modeling, data pipelines, data governance, data security, data analysis, data mining, data exploration, reporting","bigquery, business intelligence, business metrics, data exploration, data governance, data mining, data security, dataanalytics, datamodeling, datapipeline, datawarehouse, dbt, financial metrics, looker, mode, reporting, sales metrics, sql, stakeholder management, tableau, visualization"
Data Production Engineer,Hudson River Trading,"New York, NY",https://www.linkedin.com/jobs/view/data-production-engineer-at-hudson-river-trading-3736507708,2023-12-17,Middletown,United States,Mid senior,Onsite,"Hudson River Trading (HRT) is looking for a Data Production Engineer to join our Algo team. Data is at the core of everything we do at HRT; we excel at deriving deep insights from all types of data, allowing us to achieve consistent success in a dynamic market.
This role is an opportunity to work directly with live trading teams to support one of the largest automated trading systems in the world. You will write automation, explore data, work closely with our research and trading teams, and interact with a variety of external partners such as data providers, brokers, and exchanges. In addition to being a critical part of the trading process, you will have the opportunity to acquire, analyze, and prepare data for quantitative research.
Responsibilities
Data Engineering: Write tools to classify, onboard, and reconcile data. Onboard datasets, explore data, and automate tasks using a modern Python data stack
Data Analysis: Parse, analyze, and understand data sets. Perform data reconciliations, validations, and quality checks. Identify and develop new processes within the data request process to enrich data. Assist our researchers in cleaning and featurizing data
Data Debugging: Find anomalies in derived datasets and trace the issues back to their source. This can include using a mix of deductive reasoning, technical analysis, and communicating with multiple stakeholders in a data pipeline
Production Support: Provide proactive oversight of our data pipeline, handle inquiries from internal customers, and resolve issues under efficient turnaround times
Profile
Track record of being detail-oriented and thorough
You excel in problem solving and researching large datasets to resolve complex issues
You have a collaborative attitude that lends itself to cross-team customers and projects
You thrive in the fast-paced environment of a daily live trading operation
Qualifications
2+ years of experience in a data engineering/science role OR a degree in data science or a similar discipline
Previous experience working at a hedge fund is required
Experience in Python strongly preferred
Experience managing ETL pipelines is a plus
Experience with financial datasets (e.g. Refinitiv, S&P, Bloomberg) is a big plus
Comfortable with the Linux command line
Experienced in at least one SQL dialect (PostgreSQL, MSSQL, MYSQL) and able to use others as needed
Able to provide technical support in a production trading environment
Annual base salary range of $150,000 to $200,000. Pay (base and bonus) may vary depending on job-related skills and experience. A sign-on and discretionary performance bonus may be provided as part of the total compensation package, in addition to company-paid medical and/or other benefits.
Culture
Hudson River Trading (HRT) brings a scientific approach to trading financial products. We have built one of the world's most sophisticated computing environments for research and development. Our researchers are at the forefront of innovation in the world of algorithmic trading.
At HRT we welcome a variety of expertise: mathematics and computer science, physics and engineering, media and tech. We’re a community of self-starters who are motivated by the excitement of being at the cutting edge of automation in every part of our organization—from trading, to business operations, to recruiting and beyond. We value openness and transparency, and celebrate great ideas from HRT veterans and new hires alike. At HRT we’re friends and colleagues – whether we are sharing a meal, playing the latest board game, or writing elegant code. We embrace a culture of togetherness that extends far beyond the walls of our office.
Feel like you belong at HRT? Our goal is to find the best people and bring them together to do great work in a place where everyone is valued. HRT is proud of our diverse staff; we have offices all over the globe and benefit from our varied and unique perspectives. HRT is an equal opportunity employer; so whoever you are we’d love to get to know you.
Show more
Show less","Data Engineering, Data Analysis, Data Debugging, Production Support, Python, SQL, Linux, ETL, Financial datasets, Refinitiv, S&P, Bloomberg, PostgreSQL, MSSQL, MYSQL","data engineering, data analysis, data debugging, production support, python, sql, linux, etl, financial datasets, refinitiv, sp, bloomberg, postgresql, mssql, mysql","bloomberg, data debugging, data engineering, dataanalytics, etl, financial datasets, linux, mssql, mysql, postgresql, production support, python, refinitiv, sp, sql"
Senior Data Engineer,Verisk,"Jersey City, NJ",https://www.linkedin.com/jobs/view/senior-data-engineer-at-verisk-3684130523,2023-12-17,Middletown,United States,Mid senior,Onsite,"Company Description
We help the world see new possibilities and inspire change for better tomorrows. Our analytic solutions bridge content, data, and analytics to help business, people, and society become stronger, more resilient, and sustainable.
Job Description
We are looking for a savvy Data Engineer to join our growing team of analytics experts. The hire will be responsible for expanding and optimizing our data pipeline architecture. The ideal candidate is an experienced data pipeline builder and data wrangler with strong experience in handling data at scale. The Data Engineer will support our software developers, data analysts and data scientists on various data initiatives.
This role is based in our Jersey City, NJ global headquarters which has a flexible hybrid work model with 2 or 3 days per week in-office and 2 or 3 days per week remote.
Why this role
This is a highly visible role within the enterprise data lake team. Working within our Data group and business analysts, you will be responsible for leading creation of data architecture that produces our data assets to enable our data platform. This role requires working closely with business leaders, architects, engineers, data scientists and wide range of stakeholders throughout the organization to build and execute our strategic data architecture vision.
Job Duties
Extensive understanding of SQL queries. Ability to fine tune queries based on various RDBMS performance parameters such as indexes, partitioning, Explain plans and cost optimizers.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS technologies stack
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Working with data scientists and industry leaders to understand data needs and design appropriate data models.
Participate in the design and development of the AWS-based data platform and data analytics.
Qualifications
Skills Needed
Design and implement data ETL frameworks for secured Data Lake, creating and maintaining an optimal pipeline architecture.
Examine complex data to optimize the efficiency and quality of the data being collected, resolve data quality problems, and collaborate with database developers to improve systems and database designs
Hands-on building data applications using AWS Glue, Lake Formation, Athena, AWS Batch, AWS Lambda, Python, Linux shell & Batch scripting.
Hands on experience with AWS Database services (Redshift, RDS, DynamoDB, Aurora etc.)
Experience in writing advanced SQL scripts involving self joins, windows function, correlated subqueries, CTE’s etc.
An understanding of data management fundamentals, including concepts such as data dictionaries, data models, validation, and reporting.
Education and Training
Minimum of 10 years full-time software engineering experience with at least 4 years in an AWS environment focused on application development.
Bachelor’s degree or foreign equivalent degree in Computer Science, Software Engineering, or related field
Additional Information
In 2022, Verisk received Great Place to Work® Certification for our outstanding workplace culture for the sixth year in a row and second-time certification in the UK, Spain, and India. We’re also one of the 38 companies on the UK’s Best Workplaces™ list and one of 18 companies on Spain’s Best Workplaces™ list.
For over fifty years and through innovation, interpretation, and professional insight, Verisk has replaced uncertainty with precision to unlock opportunities that deliver significant and demonstrable impact. From our historic roots in risk assessment, we’ve grown to provide analytic insights that help transform industries focused on some of the world’s most critical areas. Today, the insurance industry relies on Verisk to be, and to make the world, more productive, resilient, and sustainable.
Verisk works in collaboration with our customers and at the intersection of people, data, and advanced technologies. Through proprietary platformed analytics, advanced modeling, and interpretation, we deliver immediate and sustained value to our customers and through them, to the individuals and societies they serve, with greater speed, precision, and scale.
We’re 9,000 people strong, committed to translating big data into big ideas. We help others see new possibilities and empower certainty into big decisions that impact individuals and societies. And we relentlessly and ethically pursue innovation to help move our customers, and the world, toward better tomorrows.
Everyone at Verisk—from our chief executive officer to our newest employee—is guided by The Verisk Way, to Be Remarkable, Add Value, and Innovate.
Be Remarkable by doing something better each day in service to our customers and each other
Add Value by delivering immediate and sustained results that drive positive outcomes
Innovate by redefining what’s possible, embracing challenges, and pushing boundaries
Verisk Businesses
Underwriting Solutions — provides underwriting and rating solutions for auto and property, general liability, and excess and surplus to assess and price risk with speed and precision
Claims Solutions — supports end-to-end claims handling with analytic and automation tools that streamline workflow, improve claims management, and support better customer experiences
Property Estimating Solutions — offers property estimation software and tools for professionals in estimating all phases of building and repair to make day-to-day workflows the most efficient
Extreme Event Solutions — provides risk modeling solutions to help individuals, businesses, and society become more resilient to extreme events.
Specialty Business Solutions — provides an integrated suite of software for full end-to-end management of insurance and reinsurance business, helping companies manage their businesses through efficiency, flexibility, and data governance
Marketing Solutions — delivers data and insights to improve the reach, timing, relevance, and compliance of every consumer engagement
Life Insurance Solutions – offers end-to-end, data insight-driven core capabilities for carriers, distribution, and direct customers across the entire policy lifecycle of life and annuities for both individual and group.
Verisk Maplecroft — provides intelligence on sustainability, resilience, and ESG, helping people, business, and societies become stronger
At Verisk you can build an exciting career with meaningful work; create positive and lasting impact on business; and find the support, coaching, and training you need to advance your career. We have received the Great Place to Work® Certification for the 7th consecutive year. We’ve been recognized by
Forbes
as a World’s Best Employer and a Best Employer for Women, testaments to our culture of engagement and the value we place on an inclusive and diverse workforce. Verisk’s Statement on Racial Equity and Diversity supports our commitment to these values and affecting positive and lasting change in the communities where we live and work.
Verisk Analytics is an equal opportunity employer.
All members of the Verisk Analytics family of companies are equal opportunity employers. We consider all qualified applicants for employment without regard to race, religion, color, national origin, citizenship, sex, gender identity and/or expression, sexual orientation, veteran's status, age or disability.
http://www.verisk.com/careers.html
Unsolicited resumes sent to Verisk, including unsolicited resumes sent to a Verisk business mailing address, fax machine or email address, or directly to Verisk employees, will be considered Verisk property. Verisk will NOT pay a fee for any placement resulting from the receipt of an unsolicited resume.
HR CCPA Privacy Notice.pdf
Show more
Show less","SQL, AWS, Data Lake, Data engineering, Data acquisition, Data transformation, Data loading, AWS Glue, Lake Formation, Athena, AWS Batch, AWS Lambda, Python, Linux, AWS Database services, Redshift, RDS, DynamoDB, Aurora, ETL, Data dictionaries, Data models, Data validation, Data reporting","sql, aws, data lake, data engineering, data acquisition, data transformation, data loading, aws glue, lake formation, athena, aws batch, aws lambda, python, linux, aws database services, redshift, rds, dynamodb, aurora, etl, data dictionaries, data models, data validation, data reporting","athena, aurora, aws, aws batch, aws database services, aws glue, aws lambda, data acquisition, data dictionaries, data engineering, data lake, data loading, data models, data reporting, data transformation, data validation, dynamodb, etl, lake formation, linux, python, rds, redshift, sql"
Data Management Analyst (BI),Rising Ground,"Brooklyn, NY",https://www.linkedin.com/jobs/view/data-management-analyst-bi-at-rising-ground-3710337452,2023-12-17,Middletown,United States,Mid senior,Onsite,"The Data Management Analyst (BI) is responsible for transforming data into actionable insights. This role will collaborate closely with cross-functional teams to identify trends, create reports, and provide recommendations that enhance operational efficiency.
Rising Ground
Founded as an orphanage in 1831, Rising Ground has been at the forefront of evolving community needs as a leading non-profit human services organization. Driven by the belief that each of us can thrive when life has hope and opportunity, Rising Ground provides caring support and proven paths to positive change, helping children, adults, and families rise above adversity, and each year, Rising Ground is a positive force in the lives of more than 25,000 individuals. We do this through 50 programs in 70 locations throughout New York City and Westchester. Our strong belief and commitment to diversity shows not just through our work with supported persons but is integral to the building of a strong staff that reflects the communities we serve.
Benefits
At Rising Ground, we encourage everyone to live a healthier life. When you enroll in health care benefits — medical, dental, and vision care, you and your family can take advantage of each plan’s preventive services, which will help everyone to stay well. Another way to remain healthy is to eat right and keep active. Ultimately, we all benefit when we make the right choices, whether through diet and exercise or health care services. You can also take advantage of other important benefits, including life and accident, disability benefits, the retirement savings plan, and the employee assistance program.
Annual Salary
$65,000 - $70,000
KEY FUNCTIONS & EXPECTED PERFORMANCES
Deliver data-driven solutions to support the advancement of Rising Ground’s Strategic Plan and meet organization needs related to data collection, data management, analysis and reporting through
Collaboration with project stakeholders to identify needs, goals and methodologies.
Designing systematic and procedural road maps for BI solutions (Tableau).
Engineering innovative and user-friendly databases, applications, and tools for data collection.
Use query languages and scripts to automate ETL processes.
Modernizing, improving and advancing existing processes and systems by identifying process improvement opportunities and facilitating key decisions regarding data standards.
Designing and developing interactive dashboards on organization and program levels
Leading the deployment efforts for the Tableau Online platform.
Monitor data sources and perform routine maintenance by
Executing routine performance-tuning to achieve optimal performance
Analyzing and resolving cross-functional data issues
Troubleshooting reported issues, determining root causes and implementing counter measures
Overseeing and validating integration of source data into the Tableau environment
Tableau system administrator responsibilities include
Testing new functionality that supports business requirements.
Troubleshoot and resolve technical issues.
Perform other related tasks.
Bachelor’s Degree in Business, Computer Science, Information Systems, Social Services or a related technical field is required.
Master’s Degree preferred.
2+ years of experience working with a BI software, preferably Tableau
KNOWLEDGE, TECHNICAL SKILLS & ABILITIES
Ability to leverage advanced Tableau functionality, including but not limited to, Tableau Prep, data blending, joins, parameters, dashboard actions, tooltip modifications, API, LOD calculations, formatting for mobile functionalities.
Advanced knowledge of industry standards and practices in descriptive & predictive analytics and business intelligence reporting concepts.
Advanced knowledge and experience with data modeling, database design, data architecture and data analysis.
Proficiency in MS Office. Advanced skills with SharePoint sites, SharePoint Lists, MS Teams and MS Excel.
Experience working with executive management, employing the ability to communicate technical concepts to a non-technical audience.
Location Brooklyn, New York
Show more
Show less","Data Management, Data Analysis, Data Visualization, Business Intelligence (BI), Tableau, ETL processes, Data architecture, MS Office, SharePoint, MS Teams, Data modeling, Database design, Data standards, LOD calculations, Data blending, Dashboard actions","data management, data analysis, data visualization, business intelligence bi, tableau, etl processes, data architecture, ms office, sharepoint, ms teams, data modeling, database design, data standards, lod calculations, data blending, dashboard actions","business intelligence bi, dashboard actions, data architecture, data blending, data management, data standards, dataanalytics, database design, datamodeling, etl, lod calculations, ms office, ms teams, sharepoint, tableau, visualization"
Senior Big Data Engineer (US),Zortech Solutions,"Newark, NJ",https://www.linkedin.com/jobs/view/senior-big-data-engineer-us-at-zortech-solutions-3782890080,2023-12-17,Middletown,United States,Mid senior,Onsite,"Please submit someone who can go for in person interview
Job Description
We are seeking a highly skilled and experienced Senior Data Engineer to join our dynamic team. As a Senior Data Engineer, you will play a crucial role in designing, implementing, and maintaining data pipelines and infrastructure for our big data projects. Your expertise in Java, Python, Spark cluster management, data science, big data, REST API development, and knowledge of Databricks and Delta Lake will be essential in driving the success of our data initiatives.
Responsibilities
Design, develop, and implement scalable data pipelines and ETL processes using Java, Python, and Spark.
Collaborate with data scientists, analysts, and other stakeholders to understand data requirements and design efficient solutions.
Manage and optimize Spark clusters to ensure high performance and reliability.
Perform data exploration, data cleaning, and data transformation tasks to prepare data for analysis and modeling.
Develop and maintain data models and schemas to support data integration and analysis.
Implement data quality and validation checks to ensure accuracy and consistency of data.
Utilize REST API development skills to create and integrate data services and endpoints for seamless data access and consumption.
Monitor and troubleshoot data pipeline performance, identifying and resolving bottlenecks and issues.
Stay updated with the latest technologies and trends in big data, data engineering, data science, and REST API development, and provide recommendations for process improvements.
Mentor and guide junior team members, providing technical leadership and sharing best practices.
Qualifications
Master's degree in Computer Science, Data Science, or a related field.
Minimum of 3 years of professional experience in data engineering, working with Java, Python, Spark, and big data technologies.
Strong programming skills in Java and Python, with expertise in building scalable and maintainable code.
Proven experience in Spark cluster management, optimization, and performance tuning.
Solid understanding of data science concepts and experience working with data scientists and analysts.
Proficiency in SQL and experience with relational databases (e.g., Snowflake, Delta Tables).
Experience in designing and developing REST APIs using frameworks such as Flask or Spring.
Familiarity with cloud-based data platforms (e.g.Azure)
Experience with data warehousing concepts and tools (e.g., Snowflake, BigQuery) is a plus.
Strong problem-solving and analytical skills, with the ability to tackle complex data engineering challenges.
Excellent communication and collaboration skills, with the ability to work effectively in a team-oriented environment.
If you are a highly motivated and skilled Senior Data Engineer with a passion for big data, data engineering, and REST API development, we would love to hear from you. Join our team and contribute to the success of our data-driven initiatives as we strive to make a significant impact in the industry.
Show more
Show less","Java, Python, Spark, Databricks, Delta lake, REST API development, Data science, Data engineering, Big data, Scala, Hadoop, SQL, Snowflake, BigQuery, Distributed Systems, Cloud Computing, Data Warehousing, Data Quality, Data Modeling, Data Integration, Data Exploration, Data Cleaning, Data Transformation, Data Analysis, Data Visualization, Machine Learning, Artificial Intelligence, ETL, Flask, Spring","java, python, spark, databricks, delta lake, rest api development, data science, data engineering, big data, scala, hadoop, sql, snowflake, bigquery, distributed systems, cloud computing, data warehousing, data quality, data modeling, data integration, data exploration, data cleaning, data transformation, data analysis, data visualization, machine learning, artificial intelligence, etl, flask, spring","artificial intelligence, big data, bigquery, cloud computing, data cleaning, data engineering, data exploration, data integration, data quality, data science, data transformation, dataanalytics, databricks, datamodeling, datawarehouse, delta lake, distributed systems, etl, flask, hadoop, java, machine learning, python, rest api development, scala, snowflake, spark, spring, sql, visualization"
Cloud Data Engineer (Azure),"Liberty Personnel Services, Inc.","New York, NY",https://www.linkedin.com/jobs/view/cloud-data-engineer-azure-at-liberty-personnel-services-inc-3614417701,2023-12-17,Middletown,United States,Mid senior,Onsite,"Job Details:
Liberty is partnering with a desirable company offering Free Healthcare, 4% 401k contribution and a great pension plan to find a Cloud Data Engineer (Azure)
Full Time direct hire, 125k + , Hybrid out of NYC
Qualified candidates will have similar experience to the following:
Play a key role in delivering affordable healthcare through data driven methods
Data warehousing, Data Pipelining, Data integration all in Azure
Azure, Azure AI
Python, R, SQL
XML, JSON
Data Bricks
Must be a USC or Green card holder
No sponsorship offered
If you have the above qualifications and you are looking to work in a stable environment with a company that focuses on technical and professional growth, this position is for you.
---No Third Parties---
To apply, submit resumes to mikes@libertyjobs.com
Michael Sardella
|
Liberty Personnel Services, Inc.
410 Feheley Drive | King of Prussia, PA 19406
610.941.6300 EXT 145| 610.941.2424 Fax
mikes@libertyjobs.com
www.libertyjobs.com
#IT
#midsenior
#womenintechnology
#girlswhocode
#WomenWhoCloud
#Libertyjobs
Show more
Show less","Cloud Data Engineer, Azure, Data warehousing, Data Pipelining, Data integration, Azure AI, Python, R, SQL, XML, JSON, Data Bricks","cloud data engineer, azure, data warehousing, data pipelining, data integration, azure ai, python, r, sql, xml, json, data bricks","azure, azure ai, cloud data engineer, data bricks, data integration, datapipeline, datawarehouse, json, python, r, sql, xml"
Senior Azure Data Engineer (Hybrid),"The Dignify Solutions, LLC","Jersey City, NJ",https://www.linkedin.com/jobs/view/senior-azure-data-engineer-hybrid-at-the-dignify-solutions-llc-3769520894,2023-12-17,Middletown,United States,Mid senior,Onsite,"8+ yrs of Experience
Java, Azure Databricks, Azure Data Factory (ADF), Cosmos DB
Kafka messaging Azure SQL Experience of designing and developing Azure data pipeline for data ingestion and creating APIs for reading the data Experience with DevOps - Maven, Jenkins, Sonarqube Banking experience is a big plus
Show more
Show less","Java, Azure Databricks, Azure Data Factory (ADF), CosmosDB, Kafka messaging, Azure SQL, DevOps, Maven, Jenkins, SonarQube, Banking","java, azure databricks, azure data factory adf, cosmosdb, kafka messaging, azure sql, devops, maven, jenkins, sonarqube, banking","azure data factory adf, azure databricks, azure sql, banking, cosmosdb, devops, java, jenkins, kafka messaging, maven, sonarqube"
Staff Data Engineer,Fanatics,"New York, NY",https://www.linkedin.com/jobs/view/staff-data-engineer-at-fanatics-3741589199,2023-12-17,Middletown,United States,Mid senior,Onsite,"Company Overview
Fanatics is building a leading global digital sports platform. The company ignites the passions of global sports fans and maximizes the presence and reach for hundreds of sports partners globally by offering innovative products and services across Fanatics Commerce, Fanatics Collectibles, and Fanatics Betting & Gaming, allowing sports fans to Buy, Collect and Bet. Through the Fanatics platform, sports fans can buy licensed fan gear, jerseys, lifestyle and streetwear products, headwear, and hardgoods; collect physical and digital trading cards, sports memorabilia, and other digital assets; and bet as the company builds its Sportsbook and iGaming platform. Fanatics has an established database of over 100 million global sports fans, a global partner network with over 900 sports properties, including major national and international professional sports leagues, teams, players associations, athletes, celebrities, colleges, and college conferences, and over 2,000 retail locations, including its Lids retail business stores.
As a market leader with more than 18,000 employees, and hundreds of partners, suppliers, and vendors worldwide, we take responsibility for driving toward more ethical and sustainable practices. We are committed to building an inclusive Fanatics community, reflecting and representing society at every level of the business, including our employees, vendors, partners and fans. Fanatics is also dedicated to making a positive impact in the communities where we all live, work, and play through strategic philanthropic initiatives.
We are a startup building products to transform the trading card industry. We aren’t a startup in the traditional sense, though. While we
are
a small team building new products from the ground up, we also have the backing of Fanatics – the world’s largest sports merchandiser with over 900 sports relationships and more than 81 million reachable fans. We have exclusive licensing deals with the MLB, NFL and NBA and the products we build will be used by millions of trading cards fans from day 1. We are out to reinvent the trading card industry, which is a bold and ambitious mission.
Fanatics Collectible’s Data Engineering, Science, and Analytics Team is hiring experienced data engineers. You will be among the founding members of our team. You will help build a world-class data engineering organization with a strong data and software engineering culture. You will also establish, advocate, and execute data strategy and technical capabilities that support the business and enable data-driven decisions. Ideally, you are passionate about the trading card industry, sports, and importantly, data and technology. Our data engineers are required to partner very closely with other engineering and business teams and understand our business needs. You will own high impact technical decisions and lead hiring efforts to help build out a world class data engineering team.
What does this mean as a member of the engineering team?
We are fans-and-collectors-obsessed and product-focused. We work backward from the focus of fans and collectors to drive our technical work and the development of products (physical and digital) that will reach tens of million.
We are agile and move at warp speed. We work hard and are not afraid to try, experiment, and pivot (as needed). We have a lot to do and are looking for folks who will drive projects to completion with a sense of urgency, but not at the expense of quality, scalability, and performance.
We think about scale. The trading card market is massive, and we will be the de facto entry point into the hobby.
We’re pragmatic. We embrace new technologies if they add business value.
We are passionate about trading cards, sports, and data. We take data seriously and work tirelessly to ensure their highest possible quality. We build data infrastructure to enable the seamless flow data from source to impactful actions.
Qualifications
A computer science or equivalent experience required.
8+ years of experience as a data engineer or software engineer focusing on building data infrastructure, solving complex technical challenges at a large scale, and delivering data products that drive business impact
Experience in architect, develop, deploy, and monitor a new, end-to-end data infrastructure leveraging cloud technologies that ingests data from a wide range of internal and external sources and enables user-friendly consumption of data for data-driven decisions
Experience in designing data architecture for multiple database models such as RDBMS, document, columnar, graph, and key-value data stores
Expertise in both SQL and Python
Proficiency in Java and JavaScript
Proficiency and both SQL and NoSQL database technologies
Well-verse in modern cloud native data tech stacks
Familiar with container tech such as Docker and Kubernetes
Familiar with data and ML orchestration tools
Experience in designing and implementing modern CI/CD pipelines with cloud native tools
Strong computer science fundamentals and data/software engineering hygiene
Excellent communication skills to be an effective bridge among our team, other engineering teams, and business partners/stakeholders
Key Responsibilities Include, But Not Limited To,
Research, architect, develop, deploy, monitor, and maintain an efficient, reliable, and secure cloud data infrastructure to enable data-driven decisions
Collaborate with internal and external tech teams and stakeholders/business partners
Research and integrate 3rd-party vendor data solutions
Help develop, implement, and evolve our data strategy, which will include data governance, data security, and tech roadmap
Establish and participate in on-call technical support
Contribute to data engineering and science team recruitment process
Actively mentor and coach team members on advanced technical methods, advocate best practices, and help others to grow their skill set
Set a high standard for data/software engineering excellence through example
Develop a strong understanding of Fanatics Collectibles and the trading cards industry
$210,000 - $240,000 a year
The salary range for this position is
$210,000-$240,000
, which represents base pay only and does not include short-term or long-term incentive compensation. When determining base pay, as part of a final compensation package, we consider several factors such as location, experience, qualifications, and training.
Ensure your Fanatics job offer is legitimate and don’t fall victim to fraud. Fanatics never seeks payment from job applicants. Feel free to ask your recruiter for a phone call or other type of communication for interview, and ensure your communication is coming from a Fanatics or Fanatics Brand email address. For added security, where possible, apply through our company website at www.fanaticsinc.com/careers
Tryouts are open at Fanatics! Our team is passionate, talented, unified, and charged with creating the fan experience of tomorrow. The ball is in your court now.
Fanatics is committed to responsible planning and purchasing (RPP) practices, working with its business partners across its global and multi-layered supply chain, to ensure that planning, sourcing, and purchasing decisions, along with other supporting processes, do not impede or conflict with the fulfillment of Fanatics’ fair labor practices.
NOTICE TO CALIFORNIA RESIDENTS/APPLICANTS
: In connection with your application, we collect information that identifies, reasonably relates to or describes you (“Personal Information”). The categories of Personal Information that we collect include your name, government issued identification number(s), email address, mailing address, other contact information, emergency contact information, employment history, educational history, criminal record, and demographic information. We collect and use those categories of Personal Information about you for human resources and other business management purposes, including identifying and evaluating you as a candidate for potential or future employment or other types of positions, recordkeeping in relation to recruiting and hiring, conducting criminal background checks as permitted by law, conducting analytics, and ensuring compliance with applicable legal requirements and Company policies. For additional information on how we collect and use personal information in connection with your job application, review our Candidate Privacy Policy-CA
Show more
Show less","Data Engineering, Software Engineering, Data Architecture, Data Infrastructure, Cloud Technologies, SQL, Python, Java, JavaScript, NoSQL, Docker, Kubernetes, CI/CD, Data Governance, Data Security, Data Strategy, Data Analytics, Machine Learning, RDBMS, Document Databases, Columnar Databases, Graph Databases, KeyValue Databases","data engineering, software engineering, data architecture, data infrastructure, cloud technologies, sql, python, java, javascript, nosql, docker, kubernetes, cicd, data governance, data security, data strategy, data analytics, machine learning, rdbms, document databases, columnar databases, graph databases, keyvalue databases","cicd, cloud technologies, columnar databases, data architecture, data engineering, data governance, data infrastructure, data security, data strategy, dataanalytics, docker, document databases, graph databases, java, javascript, keyvalue databases, kubernetes, machine learning, nosql, python, rdbms, software engineering, sql"
IT Data Lead Developer,SoHo Dragon,"New York, NY",https://www.linkedin.com/jobs/view/it-data-lead-developer-at-soho-dragon-3779087822,2023-12-17,Middletown,United States,Mid senior,Onsite,"Job Description
SoHo Dragon represents an investment bank with offices in New York, NY that needs to hire an IT Data Lead Developer, to assist with MS Azure technical support, design and development of ETL pipelines and transformation routines, testing and validation and support for the IT organization.
Major Responsibilities:
Experience designing and developing Enterprise Data Warehouse solutions.
Demonstrated proficiency with Data Analytics, Data Insights
Proficient writing SQL queries and programming including stored procedures and reverse engineering existing process
Leverage SQL, programming language (Python or similar) and/or ETL Tools (Azure Data Factory, Data Bricks, Talend and SnowSQL) to develop data pipeline solutions to ingest and exploit new and existing data sources.
Skills:
5+ years - Enterprise Data Management
5+ years - SQL Server based development of large datasets
1+ years with Data Architecture
3+ years’ experience in Finance / Banking industry – some understanding of Securities and Banking products and their data footprints
3+ years Python coding experience
Proficient with Data Visualization tools
Hands-on experience with Snowflake utilities such as SnowSQL and SnowPipe
Working knowledge of MS Azure configuration items with respect to Snowflake.
Hands-on experience with Tasks, Streams, Time travel, Optimizer, Metadata Manager, data sharing
Experience in Data warehousing - OLTP, OLAP, Dimensions, Facts, and Data modeling
Previous experience contributing to an enterprise-wide Cloud Data Platform migration
Experience with Cloud based data architectures, messaging, analytics
Superior communication skills
Cloud certification(s)
Education
Minimally a BA degree within an engineering and/or computer science discipline
Show more
Show less","MS Azure, ETL, SQL, Python, Azure Data Factory, Data Bricks, Talend, SnowSQL, Snowflake, Data Visualization, Data Warehousing, Data Architecture, Cloud Data Platform, Cloud Architectures, Data Analytics, Data Insights, Enterprise Data Management, Data Modeling","ms azure, etl, sql, python, azure data factory, data bricks, talend, snowsql, snowflake, data visualization, data warehousing, data architecture, cloud data platform, cloud architectures, data analytics, data insights, enterprise data management, data modeling","azure data factory, cloud architectures, cloud data platform, data architecture, data bricks, data insights, dataanalytics, datamodeling, datawarehouse, enterprise data management, etl, ms azure, python, snowflake, snowsql, sql, talend, visualization"
Senior Data Engineer,Talution Group,"Brooklyn, NY",https://www.linkedin.com/jobs/view/senior-data-engineer-at-talution-group-3783510104,2023-12-17,Middletown,United States,Mid senior,Onsite,"Job Description
About The Team:
Our team supports the batch Big Data tools used by engineering teams at
our client
. We work to ensure state-of-the-art tools are readily available and seamlessly integrate with their engineering ecosystem, enabling efficient batch data processing. We consist of 6 dedicated professionals with expertise in Big Data and DevOps.
About The Role
This project-based Data Engineer contractor role focuses on migrating data pipelines from legacy infrastructure and frameworks like Scalding to
our client's
modern infrastructure. You will be responsible for:
Analyzing existing pipelines to understand their architecture, dependencies, and functionalities.
Collaborating with data engineers to develop a detailed migration strategy for converting pipelines to the target framework.
Designing, building, and testing new pipelines in the target framework, ensuring they meet or exceed existing performance and reliability standards.
Debugging and troubleshooting migration issues, working closely with cross-functional teams for swift resolution.
Regularly communicating progress, challenges, and timelines to client stakeholders.
Requirements
Experienced Data Engineer with a strong track record in migrations and Big Data frameworks.
Must-Haves
Expertise in the Scala programming language
Proficient in the Spark framework
Experience working with BigQuery
Familiarity with scheduling jobs in Airflow
Fluency with Google Cloud Platform, especially GCS and Dataproc
Nice-to-Haves
Fluency in the Python programming language
Familiarity with the Scalding framework
Experience with Pyspark and Apache Beam
Show more
Show less","Data Engineering, Scalding, Scala, Spark, BigQuery, Airflow, Google Cloud Platform, Python, Pyspark, Apache Beam","data engineering, scalding, scala, spark, bigquery, airflow, google cloud platform, python, pyspark, apache beam","airflow, apache beam, bigquery, data engineering, google cloud platform, python, scala, scalding, spark"
Data Engineer III (US - Contract),eTeam,"Somerset, NJ",https://www.linkedin.com/jobs/view/data-engineer-iii-us-contract-at-eteam-3571645660,2023-12-17,Middletown,United States,Mid senior,Onsite,"Senior Big Data Engineer
Candidate can sit onsite in Seattle, WA or Austin, Texas. Fully remote candidates will only be considered if there's not an adequate local candidate.
As part of this team, you will be responsible for the delivery of cloud solutions and infrastructure setup for current onPrem platforms to successfully migrate into cloud, and will be playing a key role in this huge technology pivot.
You will not only help train our existing staff making the leap over to our new technologies, but you will also empower the team to approach problems differently.
These solutions drive business value from powering the most critical business decisions to powering how the site behaves through various avenues of personalization.
This entails working with the delivery team and business partners to flush out requirements, complete solution design, development and ultimate delivery to the end consumers.
Functional Activities
Identify necessary improvements in the AWS architecture to align with the AWS Well-Architected Framework and Best Practices
Research, present and accurately articulate benefits and goals of technologies such as Teradata, Big Data, Hadoop, NoSQL, Data Virtualization, and Data Services.
Monitor Performance for effective use of AWS resources
Oversee Provisioning of AWS architecture components
Ensure Architectural standards are followed
Three or more years of experience in architecting, designing, developing, and implementing cloud solutions on AWS platforms
Providing support to maintain, optimize, troubleshoot, and configure the AWS/spark/Hadoop environment as needed
Experience with CI/CD pipelines, unit tests, integration, and regression testing
Good experience with AWS VPC setup, Security Groups, IAM roles and policies. (Mandatory)
Linux System administration is preferred
Minimum Skills
AWS Automation (DevOps knowledge) is must.
Strong scripting skills in bash and python is a must.
Strong understanding and working knowledge of Linux, Github and automation Tools like Ansible and (Chef or Puppet) is must
Strong knowledge of Infrastructure as a code using tools like CloudFormation or Terraform is must.
Experience with Hive, rundeck, Airflow, jenkins and Kafka is must
At least 3 year experience with AWS DevOps tools, technologies and APIs associated with IAM, CloudFormation, AMIs, SNS, SQS, EC2, EBS, S3, RDS, VPC, ELB, Route 53, Security Groups and lambda
Good experience with LMA (Logging, Monitoring, and Alerting) using monitoring tools EX.AWS CloudWatch, Splunk/ELK, Datadog etc
Good experience with Networking Technologies like Load balancer, Firewall and DNS
Provide L2, L3 support for Incident management and provide a focal point for AWS
3+ years’ AWS experience required.
Show more
Show less","AWS, CloudFormation, Terraform, DevOps, Bash, Python, Linux, GitHub, Ansible, Chef, Puppet, Infrastructure as Code, Hive, Rundeck, Airflow, Jenkins, Kafka, IAM, AMIs, SNS, SQS, EC2, EBS, S3, RDS, VPC, ELB, Route 53, Security Groups, Lambda, LMA, Logging, Monitoring, Alerting, CloudWatch, Splunk, ELK, Datadog, Networking Technologies, Load Balancer, Firewall, DNS","aws, cloudformation, terraform, devops, bash, python, linux, github, ansible, chef, puppet, infrastructure as code, hive, rundeck, airflow, jenkins, kafka, iam, amis, sns, sqs, ec2, ebs, s3, rds, vpc, elb, route 53, security groups, lambda, lma, logging, monitoring, alerting, cloudwatch, splunk, elk, datadog, networking technologies, load balancer, firewall, dns","airflow, alerting, amis, ansible, aws, bash, chef, cloudformation, cloudwatch, datadog, devops, dns, ebs, ec2, elb, elk, firewall, github, hive, iam, infrastructure as code, jenkins, kafka, lambda, linux, lma, load balancer, logging, monitoring, networking technologies, puppet, python, rds, route 53, rundeck, s3, security groups, sns, splunk, sqs, terraform, vpc"
Data Engineer [Onsite] - C2C/W2,SmartIPlace,"Newark, NJ",https://www.linkedin.com/jobs/view/data-engineer-onsite-c2c-w2-at-smartiplace-3696015009,2023-12-17,Middletown,United States,Mid senior,Onsite,"Job Title: Data Engineer [Onsite]
Location: Newark NJ 07102
Experience: 10+ years
Visa: USC,GC,EAD
At-a-Glance
Are you ready to build your career by joining a global financial company? If so, our client is hiring a Data Engineer!
What You'll Do
Build new and existing applications in preparation for a launch of new business.
Align with the business teams and rest of the AMFMCT teams in assessing business needs and transforming into scalable applications.
Build and maintain code to manage data received from heterogenous data formats including web-based sources, internal/external databases, flat files, heterogenous data formats (binary, ASCII).
Help build new enterprise Datawarehouse and maintain the existing one.
Design and support effective storage and retrieval of very large internal and external data set and be forward think about convergence strategy with AWS cloud implementation.
Assess the impact of scaling up and scaling out and ensure sustained data management and data delivery performance.
Build interfaces for supporting evolving and new applications and accommodating new data sources and types of data.
What You Bring
Must have 10 years of experience in building out Data pipelines in Java/Scala.
Over 5 years of experience working in AWS Cloud especially services such as S3, EMR, Lambda, AWS Glue and StepFunctions.
Over 5 years of experience with Spark.
Exposed to working in an Agile environment with Scrum Master/Product owner and ability to deliver.
Strong experience with data lake/data marts/data warehouse.
Ability to communicate the status and challenges and align with the team.
Show more
Show less","Data Engineering, Java, Scala, AWS Cloud, S3, EMR, Lambda, AWS Glue, StepFunctions, Spark, Agile, Scrum Master, Product Owner, Data Lake, Data Marts, Data Warehouse","data engineering, java, scala, aws cloud, s3, emr, lambda, aws glue, stepfunctions, spark, agile, scrum master, product owner, data lake, data marts, data warehouse","agile, aws cloud, aws glue, data engineering, data lake, data marts, datawarehouse, emr, java, lambda, product owner, s3, scala, scrum master, spark, stepfunctions"
Oracle Exadata with Data analysis,HRC Global Services,"Jersey City, NJ",https://www.linkedin.com/jobs/view/oracle-exadata-with-data-analysis-at-hrc-global-services-3675252925,2023-12-17,Middletown,United States,Mid senior,Onsite,"The candidate for the Backend/ETL developer position will be responsible for designing and developing batch/ETL interfaces using the proprietary meta data driven approach as part of the LIBRA Interface Team. The candidate will be responsible for maintenance and support of the existing interface processes including investigation of issues and production troubleshooting. Candidate will be responsible for maintaining the data lineage to facilitate the end to end traceability as and when changes to the ETL. In addition candidate will responsible for Liquidity report specific derivations.
Required Skills And Experience
7-10 years' experience in Oracle Exadata, SQL, PL/SQL
Implement ETL solutions for large volume of data by applying various data warehouse modeling techniques like Star Schema, Data partitioning, Data compression, Direct path loads etc. and that maximizes re-usable components and services that incorporate versioning, reconciliation and exception handling
2-3 years' experience in SQL Performance tuning. Perform application tuning using various tools like EXPLAIN PLAN, SQL*TRACE, TKPROF, AUTOTRACE and AWR Reports. Should have a clear understanding of Oracle Server Process Architecture and memory structures PGA (Program Global Areas) & SGA (System Global Areas) to optimize memory usage.
Knowledge about Oracle VPD
Expertise in performance profiling, ability to identify performance improvements and memory optimizations
Strong coding, debugging, and analytical skills
Knowledge about Oracle architecture and process
Knowledge of computer science data structures and algorithms
Experience with Analytic reporting tools like Tableau
Analytical skills to perform technical and functional analysis with strong communication skills.
Desired Skills And Experience
Experience working in regulatory projects is strongly desirable
Experience working in Containers
Knowledge about Python and Apache Airflow
Maintain high standard and follow best practices by
re-factoring existing code to enhance readability, performance and general structure
Experience in CICD, Jenkins, GitExperience working in Agile teams
Show more
Show less","Oracle Exadata, Oracle SQL, Oracle PL/SQL, Data warehousing, Star Schema, Data partitioning, Data compression, Direct path loads, SQL Performance tuning, EXPLAIN PLAN, SQL*TRACE, TKPROF, AUTOTRACE, AWR Reports, Oracle Server Process Architecture, PGA (Program Global Areas), SGA (System Global Areas), Oracle VPD, Performance profiling, Memory optimizations, Strong coding, Debugging, Analytical skills, Oracle architecture, Data structures, Algorithms, Tableau, Python, Apache Airflow, CICD, Jenkins, Git, Agile","oracle exadata, oracle sql, oracle plsql, data warehousing, star schema, data partitioning, data compression, direct path loads, sql performance tuning, explain plan, sqltrace, tkprof, autotrace, awr reports, oracle server process architecture, pga program global areas, sga system global areas, oracle vpd, performance profiling, memory optimizations, strong coding, debugging, analytical skills, oracle architecture, data structures, algorithms, tableau, python, apache airflow, cicd, jenkins, git, agile","agile, algorithms, analytical skills, apache airflow, autotrace, awr reports, cicd, data compression, data partitioning, data structures, datawarehouse, debugging, direct path loads, explain plan, git, jenkins, memory optimizations, oracle architecture, oracle exadata, oracle plsql, oracle server process architecture, oracle sql, oracle vpd, performance profiling, pga program global areas, python, sga system global areas, sql performance tuning, sqltrace, star schema, strong coding, tableau, tkprof"
Senior Data Engineer,Talution Group,"Brooklyn, NY",https://www.linkedin.com/jobs/view/senior-data-engineer-at-talution-group-3779286203,2023-12-17,Middletown,United States,Mid senior,Onsite,"Job Description
Our client's IT Engineering team is seeking a data engineer to join us on the data engineering squad. You will be building a BigQuery-backed data warehouse for employee and organizational analytics, using existing data processing frameworks. You will work closely with the resident Staff Engineers, other developers, data scientists, and analysts to write efficient pipelines aggregating data from various sources. This new position will play a ground-breaking role in driving more informed and objective decisions at the client by modernizing the way that we collect, manage and consume organizational information.
In your day-to-day role you would be working on building ETL pipelines, data warehouses, data marts, and aggregate tables.
Requirements
Must-Haves
General experience building ETL pipelines from start to finish with a focus on data aggregation and rollups in SQL.
Experience with BigQuery.
SQL/analytics background.
Very comfortable with Python.
Nice-to-Haves
Experience with Dataform or dbt.
Experience with Airflow.
This role is to be filled outside the state of Colorado.
Show more
Show less","Data Engineering, BigQuery, SQL, Python, Data Warehousing, ETL Pipelines, Data Analytics, Data Science, Data Aggregation, Data Rollups, Data Marts, Aggregate Tables, Dataform, dbt, Airflow","data engineering, bigquery, sql, python, data warehousing, etl pipelines, data analytics, data science, data aggregation, data rollups, data marts, aggregate tables, dataform, dbt, airflow","aggregate tables, airflow, bigquery, data aggregation, data engineering, data marts, data rollups, data science, dataanalytics, dataform, datawarehouse, dbt, etl pipelines, python, sql"
Cloud Data Engineer,Verisk,"Jersey City, NJ",https://www.linkedin.com/jobs/view/cloud-data-engineer-at-verisk-3640847172,2023-12-17,Middletown,United States,Mid senior,Onsite,"Company Description
We help the world see new possibilities and inspire change for better tomorrows. Our analytic solutions bridge content, data, and analytics to help business, people, and society become stronger, more resilient, and sustainable.
Job Description
We are looking for a Cloud Data Engineer, who will be responsible to plan, design, develop and maintain the data architecture, data models, data pipelines and standards for various Data Integration & Data Lake and Data Warehouse projects in the AWS Cloud. Ensure new features and subject areas are modelled to integrate with existing structures and provide a consistent view. Develop and maintain documentation of the data architecture, data flow and data models of the data warehouse appropriate for various audiences. Provide direction on adoption of Cloud technologies (Redshift, Postgres, Snowflake,) and industry best practices in the field of data lakes and data warehouse architecture and modelling. Providing technical leadership to large enterprise scale projects.
The role is based in our Jersey City, NJ location which has a flexible hybrid work model that has employees in-office 2-3 days per week and 2-3 days per week remote.
This role requires a broad range of skills and the ability to step into different roles depending on the size and scope of the project.
Responsibilities: -
Demonstrated ability to have successfully completed multiple, complex technical data engineering projects and create high-level design and architecture of the solution.
Able to present the solution architecture strategy and define value proposition to key stake holders.
Able to communicate the capabilities and improvements offered by solution architecture with both the development teams and business user teams.
Develop POC's and conduct show and tell sessions.
Design, build & deploy the data pipelines using EMR, EMR studio, Databricks, Spark, Pyspark and Shell Scripting.
Design the Data replication systems and integration systems by evaluating various products and deciding on the better technology that meets the corporate objectives.
Perform Data Analysis, Data Modeling, Data Management, and Data lineage.
Design data security framework considering both internal and external sharing aspects.
Design Data pipes and automate data ingestion and cleansing, transformation activities using the DevOps methodologies.
Qualifications
Must have total 7+ yrs. of experience in IT and working as a Data Engineer
Must have 5+ years of experience in EMR, Databricks, Spark, pyspark programming, Jupyter notebook and shell scripting
3+ years in Data warehouse, ETL Projects.
Must have experience End to End implementation of cloud data warehouse (Redshift, Postgres, or Snowflake).
Hands-on experience with Postgres SQL, RedShift, Redshift spectrum, AWS Glue, Athena, Snowflake utilities, Snow SQL, Snow Pipe, AWS Lambda model techniques using Python.
Experience in Data Migration from RDBMS to Redshift/Snowflake cloud data warehouse.
Expertise in data modelling, ELT using Snowflake SQL, implementing complex stored Procedures and standard DWH and ETL/ELT concepts.
Expertise in advanced concepts like setting up resource monitors, Role-based access controls (RBAC), virtual warehouse sizing, query performance tuning, zero copy clone, time travel and understand how to use these features.
Expertise in deploying features such as data sharing, events, and lake-house patterns.
Deep understanding of relational as well as NoSQL data stores, Methods, and approaches (Star and Snowflake, Dimensional Modelling).
Experience with data security and data access controls and design.
Experience with AWS or Azure data storage and management technologies such as S3 and ADLS.
Build processes supporting data transformation, data structures, metadata, dependency, and workload management.
Proficiency in RDBMS, complex SQL, PL/SQL, Unix Shell Scripting, performance tuning and troubleshoot.
Provide resolution to an extensive range of complicated data pipeline related problems, proactively and as issues surface.
Must have expertise in AWS and Azure Platform as a Service (PAAS).
Should be able to troubleshoot problems across infrastructure, platform, and application domains.
Must have experience of Agile development methodologies.
Strong written communication skills. Is effective and persuasive in both written and oral communication.
#Hybrid
Additional Information
In 2022, Verisk received Great Place to Work® Certification for our outstanding workplace culture for the sixth year in a row and second-time certification in the UK, Spain, and India. We’re also one of the 38 companies on the UK’s Best Workplaces™ list and one of 18 companies on Spain’s Best Workplaces™ list.
For over fifty years and through innovation, interpretation, and professional insight, Verisk has replaced uncertainty with precision to unlock opportunities that deliver significant and demonstrable impact. From our historic roots in risk assessment, we’ve grown to provide analytic insights that help transform industries focused on some of the world’s most critical areas. Today, the insurance industry relies on Verisk to be, and to make the world, more productive, resilient, and sustainable.
Verisk works in collaboration with our customers and at the intersection of people, data, and advanced technologies. Through proprietary platformed analytics, advanced modeling, and interpretation, we deliver immediate and sustained value to our customers and through them, to the individuals and societies they serve, with greater speed, precision, and scale.
We’re 9,000 people strong, committed to translating big data into big ideas. We help others see new possibilities and empower certainty into big decisions that impact individuals and societies. And we relentlessly and ethically pursue innovation to help move our customers, and the world, toward better tomorrows.
Everyone at Verisk—from our chief executive officer to our newest employee—is guided by The Verisk Way, to Be Remarkable, Add Value, and Innovate.
Be Remarkable by doing something better each day in service to our customers and each other
Add Value by delivering immediate and sustained results that drive positive outcomes
Innovate by redefining what’s possible, embracing challenges, and pushing boundaries
Verisk Businesses
Underwriting Solutions — provides underwriting and rating solutions for auto and property, general liability, and excess and surplus to assess and price risk with speed and precision
Claims Solutions — supports end-to-end claims handling with analytic and automation tools that streamline workflow, improve claims management, and support better customer experiences
Property Estimating Solutions — offers property estimation software and tools for professionals in estimating all phases of building and repair to make day-to-day workflows the most efficient
Extreme Event Solutions — provides risk modeling solutions to help individuals, businesses, and society become more resilient to extreme events.
Specialty Business Solutions — provides an integrated suite of software for full end-to-end management of insurance and reinsurance business, helping companies manage their businesses through efficiency, flexibility, and data governance
Marketing Solutions — delivers data and insights to improve the reach, timing, relevance, and compliance of every consumer engagement
Life Insurance Solutions – offers end-to-end, data insight-driven core capabilities for carriers, distribution, and direct customers across the entire policy lifecycle of life and annuities for both individual and group.
Verisk Maplecroft — provides intelligence on sustainability, resilience, and ESG, helping people, business, and societies become stronger
At Verisk you can build an exciting career with meaningful work; create positive and lasting impact on business; and find the support, coaching, and training you need to advance your career. We have received the Great Place to Work® Certification for the 7th consecutive year. We’ve been recognized by
Forbes
as a World’s Best Employer and a Best Employer for Women, testaments to our culture of engagement and the value we place on an inclusive and diverse workforce. Verisk’s Statement on Racial Equity and Diversity supports our commitment to these values and affecting positive and lasting change in the communities where we live and work.
Verisk Analytics is an equal opportunity employer.
All members of the Verisk Analytics family of companies are equal opportunity employers. We consider all qualified applicants for employment without regard to race, religion, color, national origin, citizenship, sex, gender identity and/or expression, sexual orientation, veteran's status, age or disability.
http://www.verisk.com/careers.html
Unsolicited resumes sent to Verisk, including unsolicited resumes sent to a Verisk business mailing address, fax machine or email address, or directly to Verisk employees, will be considered Verisk property. Verisk will NOT pay a fee for any placement resulting from the receipt of an unsolicited resume.
HR CCPA Privacy Notice.pdf
Show more
Show less","Cloud Data Engineering, Data Architecture, Data Modeling, Data Pipelines, AWS Cloud, EMR, EMR Studio, Databricks, Spark, Pyspark, Shell Scripting, Data Replication, Data Integration, Data Analysis, Data Management, Data Lineage, Data Security, DevOps, RDBMS, SQL, PL/SQL, Unix Shell Scripting, Performance Tuning, Data Troubleshooting, AWS, Azure, Agile Development, Data Warehousing, ETL, ELT, Data Sharing, Data Lakes, NoSQL, Data Stores, Star Schema, Snowflake, Snowpipe, AWS Lambda, Python, Redshift, Redshift Spectrum, AWS Glue, Athena, Snowflake Utilities, Snow SQL, Virtual Warehousing, Query Performance Tuning, Zero Copy Clone, Time Travel, Data Transformation, Data Structures, Metadata, Dependency, Workload Management, Hadoop, Hive, Pig, Oozie, Flume, Sqoop, Kafka, ZooKeeper, HBase, Cassandra, MongoDB, CouchDB, Redis, Neo4j, Graph Databases, Machine Learning, Artificial Intelligence, Natural Language Processing, Computer Vision, Speech Recognition, Robotics, Automation, Data Governance, Data Quality, Data Privacy, Information Security, Cloud Computing, Big Data, Internet of Things, Blockchain, 5G, Edge Computing, Quantum Computing","cloud data engineering, data architecture, data modeling, data pipelines, aws cloud, emr, emr studio, databricks, spark, pyspark, shell scripting, data replication, data integration, data analysis, data management, data lineage, data security, devops, rdbms, sql, plsql, unix shell scripting, performance tuning, data troubleshooting, aws, azure, agile development, data warehousing, etl, elt, data sharing, data lakes, nosql, data stores, star schema, snowflake, snowpipe, aws lambda, python, redshift, redshift spectrum, aws glue, athena, snowflake utilities, snow sql, virtual warehousing, query performance tuning, zero copy clone, time travel, data transformation, data structures, metadata, dependency, workload management, hadoop, hive, pig, oozie, flume, sqoop, kafka, zookeeper, hbase, cassandra, mongodb, couchdb, redis, neo4j, graph databases, machine learning, artificial intelligence, natural language processing, computer vision, speech recognition, robotics, automation, data governance, data quality, data privacy, information security, cloud computing, big data, internet of things, blockchain, 5g, edge computing, quantum computing","5g, agile development, artificial intelligence, athena, automation, aws, aws cloud, aws glue, aws lambda, azure, big data, blockchain, cassandra, cloud computing, cloud data engineering, computer vision, couchdb, data architecture, data governance, data integration, data lakes, data lineage, data management, data privacy, data quality, data replication, data security, data sharing, data stores, data structures, data transformation, data troubleshooting, dataanalytics, databricks, datamodeling, datapipeline, datawarehouse, dependency, devops, edge computing, elt, emr, emr studio, etl, flume, graph databases, hadoop, hbase, hive, information security, internet of things, kafka, machine learning, metadata, mongodb, natural language processing, neo4j, nosql, oozie, performance tuning, pig, plsql, python, quantum computing, query performance tuning, rdbms, redis, redshift, redshift spectrum, robotics, shell scripting, snow sql, snowflake, snowflake utilities, snowpipe, spark, speech recognition, sql, sqoop, star schema, time travel, unix shell scripting, virtual warehousing, workload management, zero copy clone, zookeeper"
Data Security Engineer - Slack,Slack,"Jersey City, NJ",https://www.linkedin.com/jobs/view/data-security-engineer-slack-at-slack-3781956212,2023-12-17,Middletown,United States,Mid senior,Remote,"To get the best candidate experience, please consider applying for a maximum of 3 roles within 12 months to ensure you are not duplicating efforts.
Job Category
Software Engineering
Job Details
About Salesforce
We’re Salesforce, the Customer Company, inspiring the future of business with AI+ Data +CRM. Leading with our core values, we help companies across every industry blaze new trails and connect with customers in a whole new way. And, we empower you to be a Trailblazer, too — driving your performance and career growth, charting new paths, and improving the state of the world. If you believe in business as the greatest platform for change and in companies doing well and doing good – you’ve come to the right place.
Our Security team supports the unwritten fourth tenet of Slack’s mission: make people’s working lives more secure. We’re serious about protecting our infrastructure, operations, and most importantly, our customers’ data. We take a systemic approach to security, and strive to ensure we provide low friction high-impact security across everything we do.
As a member of the Slack Security Customer Protection team, you are the first line of detection of bad actors using Slack in unwanted and unexpected ways. As Slack’s data, customers, and features grow, protecting customers’ data from unwanted behaviors becomes an ever more important and challenging problem. The Security Customer Protection team develops and uses tooling to tease out high-quality signal from all the noise, to detect unwanted behaviors, such as abuse of users, workspaces, or tokens. Your work directly impacts the way millions of people, teams and businesses get things done.
Slack's API and web backend is built using PHP/Hack, and our backend services are written in Go. We use Airflow, Presto, Hive and Spark to interact with our data infrastructure.
Slack has a positive, diverse, and supportive culture—we look for people who are curious, inventive, and work to be a little better every single day. In our work together we aim to be smart, humble, hardworking and, above all, collaborative. If this sounds like a good fit for you, why not say hello?
What you will be doing
Creating tools for Slack and our customers to do proactive discovery and prevention of threat actors and unwanted activity in Slack
Work closely with Privacy, Policy, and Product teams to understand customers’ security needs and current priorities
Develop new dashboards to visualize and surface data for analysis and reporting
Develop and execute code to: modify data tables, automate database queries, surface and analyze logs, perform password resets
Manage investigations of abuse and discover new abuse cases, occasionally working with customers and legal entities
Bring to bear security-relevant data and log sources to surface unwanted activity in the customer space, both proactively and reactively
Use data and tools to understand and hunt for threats in the environment
Collaborate with product and features teams on security capabilities
Understand the underpinnings of how Slack works, and where bad actors could take advantage, to develop improved detective tools
Expose measurable data to internal and external partners to improve Slack’s ability to detect future threats
What you should have
4-6 years work experience with data analytics in a security or privacy environment, with a focus on customer-facing environments
Proficiency working with data technologies that power analytics (e.g. Airflow, Hive, Spark, Presto, Kafka, Pinot, MySQL or similar technologies)
Practical experience mining and cleaning large, unstructured datasets, then extracting meaningful and actionable insights, presenting results to an audience of various backgrounds
Experience understanding bad actors, threat intelligence, and abuse; involvement remediating abuse or security-related incidents is a plus
Experience with a high-level programming language such as Python or Go
Experience with Linux, Kibana, and engineering fundamentals at scale such as AWS, Chef, and Terraform
You have a Bachelor's degree in Computer Science, Engineering, Mathematics or a related field, equivalent training, fellowship, or work experience is required
A related technical degree is required
Accommodations
If you require assistance due to a disability applying for open positions please submit a request via this Accommodations Request Form.
Posting Statement
At Salesforce we believe that the business of business is to improve the state of our world. Each of us has a responsibility to drive Equality in our communities and workplaces. We are committed to creating a workforce that reflects society through inclusive programs and initiatives such as equal pay, employee resource groups, inclusive benefits, and more. Learn more about Equality at www.equality.com and explore our company benefits at www.salesforcebenefits.com .
Salesforce is an Equal Employment Opportunity and Affirmative Action Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender perception or identity, national origin, age, marital status, protected veteran status, or disability status. Salesforce does not accept unsolicited headhunter and agency resumes. Salesforce will not pay any third-party agency or company that does not have a signed agreement with Salesforce .
﻿Salesforce welcomes all.
For Colorado-based roles, the base salary hiring range for this position is $133,400 to $183,400.
Compensation offered will be determined by factors such as location, level, job-related knowledge, skills, and experience. Certain roles may be eligible for incentive compensation, equity, benefits. More details about our company benefits can be found at the following link: https://www.salesforcebenefits.com.
Show more
Show less","PHP, Hack, Go, Airflow, Presto, Hive, Spark, Python, Kibana, AWS, Chef, Terraform, Linux, Computer Science, Engineering, Mathematics","php, hack, go, airflow, presto, hive, spark, python, kibana, aws, chef, terraform, linux, computer science, engineering, mathematics","airflow, aws, chef, computer science, engineering, go, hack, hive, kibana, linux, mathematics, php, presto, python, spark, terraform"
Data Engineer - Remote,"The Dignify Solutions, LLC","Edison, NJ",https://www.linkedin.com/jobs/view/data-engineer-remote-at-the-dignify-solutions-llc-3768002998,2023-12-17,Middletown,United States,Mid senior,Remote,"7+ Years of Experience
2+ years of hands-on experience in ETL development for a data warehouse
2+ years of hands-on experience in development, maintenance, and enhancements of Informatica Mappings, Workflows, and processes
Proficient in programming against large data assets with a working knowledge of SQL
Expertise with integration technologies and processes
Hands-on experience or Knowledge about Airflow, or automated job scheduling tools.
Experience with API/REST, JSON
Experience working on an Agile Development team and delivering features incrementally.
Experience with Git repositories
Experience developing in Python
Experience working with Pytest framework, or other testing frameworks
Experience with cloud platforms (AWS, Azure) with strong preference towards AWS.
Experience in Database design practices
Experience building data automation pipelines
Experience working with Azure DevOps, JIRA or similar project tracking software.
Experience with both Windows and Linux.
Show more
Show less","ETL, Data Warehouse, Informatica, SQL, Agile Development, Git, Python, Pytest, AWS, Azure, Database design, Data automation pipelines, Azure DevOps, JIRA","etl, data warehouse, informatica, sql, agile development, git, python, pytest, aws, azure, database design, data automation pipelines, azure devops, jira","agile development, aws, azure, azure devops, data automation pipelines, database design, datawarehouse, etl, git, informatica, jira, pytest, python, sql"
SENIOR BIG DATA ENGINEER – FULLY REMOTE,Vodastra Technologies,"New York, NY",https://www.linkedin.com/jobs/view/senior-big-data-engineer-%E2%80%93-fully-remote-at-vodastra-technologies-3787739131,2023-12-17,Middletown,United States,Mid senior,Remote,"Industry: Healthcare / Health Services
Job Category: Medical / Health - Healthcare IT
JOB SUMMARY:
This job is responsible for designing and engineering data solutions for the enterprise and, working closely with business, analytic and IT stakeholders, assists with the development and maintenance of these solutions. This includes coding data ingestion pipelines, transformations and delivery programs/logic for people and systems to access data for operational and/or analytic needs. Duties include but are not limited to the coding, testing, and implementation of ingestion and extraction pipelines, transformation and cleansing processes, and processes that load and curate data in conformed, fit-for-purpose data structures. The incumbent is expected to partner with others throughout the organization (including other engineers, architects, analysts, data scientists, and non-technical audiences) in their daily work. The incumbent will work with cross-functional teams to deliver and maintain data products and capabilities that support and enable strategies at business unit and enterprise levels. The incumbent is expected to utilize technologies such as, but not limited to: Google Cloud Platform.
ESSENTIAL RESPONSIBILITIES:
In partnership with other business, platform, technology, and analytic teams across the enterprise, design, build and maintain well-engineered data solutions in a variety of environments, including traditional data warehouses, Big Data solutions, and cloud-oriented platforms. Create high performance cloud and big data systems to be used with operational and analytic applications. Work with internal and external platforms and systems to connect and align on data sourcing, flow, structure, and subject matter expertise. Work with business stakeholders and strategic partners to implement and support operational and analytic platforms. This may include products purchased by the organization that must be ingested or modeled/derived data maintained by enterprise platforms and data consumers. Working across multiple, disparate systems and platforms, design, code, test, implement, and maintain scalable and extensible frameworks that support data engineering services. Align with security, data governance and data quality programs by driving assigned components of metadata management, data quality management, and the application of business rules. Develop and maintain associated data engineering processes and participate in required operating procedures as part of the enterprise’s overall information management activities. Includes data cleansing, standardization, technical metadata documentation, and the de-identification and/or tokenization of data. Develop, optimize and/or maintain machine learning and AI engineering processes (MLOps) that are deployed to cloud or big data environments. These may be based on prototypes built by data scientists or capability frameworks implemented to allow data scientists to build efficiently in production environments. Develop tasks across multiple projects with limited need for guidance. This includes providing guidance and education to Intermediate and Junior contributors within team. Manage relationships with customers of the function. Attend meetings with customers on a stand-alone basis or with team as needed. Establish standards and patters for high performance data ingestion, transformation, and delivery of data analytic needs. Keep current with Big Data technologies in order to recommend best tools in order to perform current and future work Other duties as assigned or requested.
EDUCATION:
Required
Bachelor's Degree in Software Engineering, Information Systems, Computer Science, Data Science or related field
EXPERIENCE:
Required
5 years in Data platform development, data engineering, software development, or data science
3 years in Big data or cloud data platform Preferred
3 years in Healthcare Industry
3 years of Data Warehousing
3 years of Database Administration
SKILLS:
SQL Data Warehousing
Problem-Solving
Communication Skills
Analytical Skills
Spark or Python or related tool
Cloud Technologies
Powered by JazzHR
g4JEOLNM0S
Show more
Show less","Google Cloud Platform, Cloud Data Platform, SQL, Data Warehousing, Spark, Python, Data Engineering, Software Development, Data Science, Big data, Machine Learning, AI Engineering, MLOps, Metadata Management, Data Quality Management, Data Cleansing, Standardization, Technical Metadata Documentation, Deidentification, Tokenization","google cloud platform, cloud data platform, sql, data warehousing, spark, python, data engineering, software development, data science, big data, machine learning, ai engineering, mlops, metadata management, data quality management, data cleansing, standardization, technical metadata documentation, deidentification, tokenization","ai engineering, big data, cloud data platform, data engineering, data quality management, data science, datacleaning, datawarehouse, deidentification, google cloud platform, machine learning, metadata management, mlops, python, software development, spark, sql, standardization, technical metadata documentation, tokenization"
Senior Data Engineer,Baton Health,"New York, NY",https://www.linkedin.com/jobs/view/senior-data-engineer-at-baton-health-3783505661,2023-12-17,Middletown,United States,Mid senior,Remote,"About The Company
Baton Health is on a mission to modernize healthcare credentialing by eliminating costly and manual processes.
Healthcare credentialing can be summarized as ""background checks for doctors"" and requires verification of data from hundreds of sources. Baton Health is creating a single Universal Primary Source that federates these disparate data sources into one. In our short time, we've built a rapidly scalable data ingestion system, a schema normalization engine, and a powerful data model for federating and delivering data from upwards of a thousand sources. Our existing stack includes Python, dbt, Snowflake, Prefect, and Postgres, amongst other tools and infrastructure.
About The Role
We are looking for a
Senior Data Engineer
to join and directly shape our early-stage team. You will extend the build out of our ingestion infrastructure and data pipelines from end to end. You will help configure and maintain a data integration service based on the modern data stack, including a cloud data warehouse, a data transformation layer, and workflow orchestration. You will need to think deeply about the details of our various data sources, be creative in how we assess and ensure data quality, and be critical in how we evaluate technologies that serve our use cases. This role will play a crucial part in powering our product and building core business lines.
You will report to the CEO and work remotely along with a Director of Product and a Data Engineer, with opportunities to meet and work in person in NYC.
What You'll Do
Steer the build of data pipelines to external and internal data sources and work with both structured and unstructured data to efficiently extract and load various sources into our data warehouse.
Design and implement data orchestration using tools like Prefect to manage ingestion, processing, and data flow across all the components of the data infrastructure.
Oversee the processing, deduplication, and reconciliation of data from different sources using Snowflake and dbt.
Develop and oversee a state-of-the-art entity resolution system, aimed at integrating and correlating all primary source data associated with individual practitioners into a unified, singular record identifier for each individual person.
Ensure the preservation of original data formatting and create an auditable chain of custody for all data points ingested by Baton.
Work with the Product and Engineering teams to understand how the output data can be used to enrich the workflows of our customers
Develop an understanding of the credentialing world and innovate new ways to fulfill credentialing needs with the data our sources provide.
Integrate our data warehouse with that of other customers using tools like the Snowflake Marketplace, Fivetran, or others in order to support new revenue streams.
What You'll Need
Background:
5+ years of experience as a hands-on Data Engineer specializing in data ingestion, extraction, and modeling using modern data stacks.
Deep experience building dbt models to transform and connect disparate raw data sources.
Experience with ELT tools like Fivetran or Airbyte and rETL tools like Census or Hightouch.
Experience with a cloud data warehouse like Snowflake, Redshift, or Big Query.
Experience with workflow orchestration tools such as Airflow, Prefect, or Argo.
Skills:
Manages Complexity.
You ask the right questions to accurately analyze situations and uncover root causes to difficult issues. Through acquiring data from multiple and diverse sources, you are able to make sense of complex, high-quantity, and sometimes contradictory information to solve problems.
Drive Results.
Has a strong bottom-line orientation. Persists in accomplishing objectives despite obstacles and setbacks. Has a track record of exceeding goals successfully. Pushes self and helps others achieve results. Consistently achieving results, even under tough circumstances.
Communicates & Collaborates.
Is effective in a variety of communication settings: one-on-one, small and large groups, or among diverse styles and position levels. Attentively listens to others. Adjusts to fit the audience and the message. Provides timely and helpful information to others across the organization. Encourages the open expression of diverse ideas and opinions. Developing and delivering multi-mode communications that convey a clear understanding of the unique needs of different audiences.
Additional Information:
Full-time base salary range of $120,000 to $150,000 plus equity
Show more
Show less","Python, dbt, Snowflake, Prefect, Postgres, ETL, ELT, rETL, Cloud data warehouse, Workflow orchestration, Apache Airflow, Argo, Data ingestion, Data extraction, Data modeling, Data transformation, Data integration, Data orchestration, Data deduplication, Data reconciliation, Entity resolution, Data quality, Data governance, Data security, Communication, Collaboration","python, dbt, snowflake, prefect, postgres, etl, elt, retl, cloud data warehouse, workflow orchestration, apache airflow, argo, data ingestion, data extraction, data modeling, data transformation, data integration, data orchestration, data deduplication, data reconciliation, entity resolution, data quality, data governance, data security, communication, collaboration","apache airflow, argo, cloud data warehouse, collaboration, communication, data deduplication, data extraction, data governance, data ingestion, data integration, data orchestration, data quality, data reconciliation, data security, data transformation, datamodeling, dbt, elt, entity resolution, etl, postgres, prefect, python, retl, snowflake, workflow orchestration"
Staff Data Engineer,theSkimm,"New York, NY",https://www.linkedin.com/jobs/view/staff-data-engineer-at-theskimm-3657168160,2023-12-17,Middletown,United States,Mid senior,Remote,"theSkimm'
We're hiring a Staff Data Engineer.
About Our Team And What We'll Build Together
We're looking for an experienced Staff Data Engineer and mentor to join theSkimm's Tech team. Our mission is to enable our partners across theSkimm, from Editorial and Audio to Marketing and Advertising, to achieve their goals with the best systems and processes we can offer. We build tools throughout the stack, share knowledge across departments, and learn quickly so we can take best advantage of what's coming.
As we grow, we're looking for a Staff Data Engineer who will help solidify and expand our pipelines and maintain our data warehouse. Our business is run on detailed analysis of how our products perform with our members, which features they love and which can be improved, and which product and marketing campaigns are bringing in the best quality users. You'll be responsible for working alongside analysts, product, marketing, and other teams to define new data collection components and measurement schemes that support each new product and feature.
How You'll Contribute To Our Mission
Create and maintain data pipelines to provide insights and drive business decisions
Establish theSkimm's data warehousing strategy (ex. Kimball, Data Vault, etc.)
Maintain theSkimm's data infrastructure on our AWS accounts
Collaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization.
Write unit/integration tests, contributes to engineering wiki, and documents work
Implement processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it
You're ready for this! Here's a bit more about what we're looking for
7+ years of industry experience measuring product performance and user behavior
Experience working with a variety of data management technologies, including RedShift, Kinesis/Kafka, Glue, Spark, Postgres, Airflow, dbt, and others.
Experience implementing BI reporting tools such as Looker
Experience interfacing with engineers, product managers and analysts to understand data needs
Knowledge of a variety of measurement beacons, SDKs, APIs including Google Analytics, Amplitude, Braze, Stripe, email service providers
A commitment to building secure, resilient, fault-tolerant architectures, with clear documentation and procedures in place for support
A focus on accuracy and detail as you build, ensuring data pipelines produce clear, predictable results
Ability to thrive in a dynamic, fast-paced, collaborative, and high-growth environment
Facility in presenting and discussing the trade-offs in employing different engineering solutions to a problem, valuing pragmatism over idealism
An empathetic leadership style that encourages open communication and trust
Understanding of the typical metrics a subscription and advertising-supported business needs to measure success
Experience with ML techniques as applied to behavioral segmentation or anomaly identification is a plus
Familiarity using developer tools that increase productivity and facilitate the development of resilient code (eg. Docker, CircleCI, Serverless) is a plus
Familiarity and enthusiasm for theSkimm: a passion for our audience and mission is a plus
The expected annual base salary for this role is $145,000-$180,000. We'll consider a variety of factors when determining the offered base salary including an evaluation of a candidate's skills, abilities, experience, location, market demands, and internal parity.
Why our employees love working here
We are mission driven and values driven:
We are collectively building an impactful brand to empower generations of informed, confident women. Our vision is for every woman to have the info they need to navigate a complex world and the decisions in it. Every role has a purpose and allows us to provide useful information succinctly and contextualized. We tee her up to take action - whichever action is right for her, wherever she is.
We are values driven at every point - how we work, the way we work and our culture come straight from our core values. Each of us supports this incredible brand and takes part in its continued growth, fulfilling the needs of our audience each and every day.
We put you and your personal lives first - we have a generous benefits policy that demonstrates we are listening to what our employees need:
Unlimited vacation policy and generous holiday observances
Encouraged time off on your birthday and Skimm'versary
A hybrid working model with flexibility for remote work
Comprehensive insurance plans and commuter benefits
Auto-enrollment into an Empower 401(k) plan starting on your first day and employer contribution available after one year of employment
Rewards for every work anniversary
One month paid sabbatical after your five year Skimm'versary as a full-time employee
We support our parents at all points on their journey:
18 weeks of paid parental leave (adoption, fostering and surrogacy included)
Bereavement leave for pregnancy loss
Phased return to work schedule availability
Flexibility based on parental schedules
Access to family building and fertility benefits
Flexible and broad-scale child support program
Your well being is our priority:
We honor Sacred Time in our workplace
We have aligned company ""time off"" during the summer to truly allow for us all to break from our work at the same time
We offer fitness membership reimbursement
One Medical Membership is included with our benefits
We have a vibrant, collaborative, and supportive culture (we celebrate and have fun!):
Weekly company updates led by our executive team
Employee Resource Groups
Annual employee award ceremony to celebrate individual accomplishments
Clubs and activities designed to meet other employees like book clubs, new hire buddy programs, off-sites, and wellness focused classes
Numerous culture events that enable our workforce, in the office and remote, to connect and have fun
Your career and development are a priority:
Annual learning and development stipend
LEAD@theSkimm, our leadership development program
DEI initiatives to ensure we are the best partners and colleagues to each other
Career development guidance and planning
And more…
Competitive salary and equity packages
The opportunity to be part of a values-driven, hardworking, and diverse group of people building a media company that makes it easier to live smarter
Our story, Skimm'd
We are a digital media company, dedicated to succinctly giving women the information they need to make confident decisions. We make it easier to live smarter.
At our core, we are writers, editors, producers, designers, marketers, engineers, analysts, sellers, creatives, and strategists all working together to achieve this goal.
Every day we're breaking down the news, trends, policies, and politics that impact women so that they can navigate their daily lives and futures – from managing their paychecks to casting their ballots – with confidence. We provide our dedicated audience of millions with reliable, non-partisan, information, informing and empowering them while fitting into their daily routines.
Since disrupting the media landscape and defining a new category a decade ago, we have become a trusted source for our audience of millions by seamlessly integrating into their existing routines, fundamentally changing the way they consume news and make decisions. Today theSkimm ecosystem includes the Daily Skimm, the Daily Skimm: Weekend, Skimm Money and Skimm Your Life newsletters, B2B marketer's newsletter The SKM Report, ""9 to 5ish with theSkimm"" podcast, theSkimm mobile app. We also house Skimm Studios which creates innovative in-house video and audio content, and our in-house creative agency SKM Lab, which conceptualizes, develops, and produces innovative solutions and content for brands to engage with generations of informed women. Our first book,
How to Skimm Your Life
debuted at #1 on The New York Times Best Seller list. Through Skimm Impact, our purpose-driven platform, we are proud to support get-out-the-vote efforts with Skimm Your Ballot, which has spurred more than two million voting-related actions across the last four election cycles. We have mobilized hundreds of companies to join our #ShowUsYourLeave movement, which has created transparency and pushed for progress for Paid Family Leave in the U.S. And we're empowering women to take agency of their lives and control of their futures through our State of Women initiative, grounded in a study conducted by The Harris Poll.
Come join us!
Show more
Show less","Data Pipelines, Data Warehousing, AWS, Data Modeling, SQL, Redshift, Kinesis, Kafka, Glue, Spark, Postgres, Airflow, dbt, Looker, Measurement Beacons, SDKs, APIs, Google Analytics, Amplitude, Braze, Stripe, Docker, CircleCI, Serverless, Machine Learning, Behavioral Segmentation, Anomaly Identification","data pipelines, data warehousing, aws, data modeling, sql, redshift, kinesis, kafka, glue, spark, postgres, airflow, dbt, looker, measurement beacons, sdks, apis, google analytics, amplitude, braze, stripe, docker, circleci, serverless, machine learning, behavioral segmentation, anomaly identification","airflow, amplitude, anomaly identification, apis, aws, behavioral segmentation, braze, circleci, datamodeling, datapipeline, datawarehouse, dbt, docker, glue, google analytics, kafka, kinesis, looker, machine learning, measurement beacons, postgres, redshift, sdks, serverless, spark, sql, stripe"
Expression of Interest: Data Engineer,Fingerprint for Success (F4S),"New York, NY",https://www.linkedin.com/jobs/view/expression-of-interest-data-engineer-at-fingerprint-for-success-f4s-3787769880,2023-12-17,Middletown,United States,Mid senior,Remote,"We are inviting professionals in high-growth industries who are thinking about their next move or looking for a new opportunity to join our expanding talent pool.
The F4S Talent Pool is a pilot project designed to:
Help job seekers get discovered by our partners based on their anticipated hiring needs.
Provide optional support and resources for job seekers in their career endeavors.
Help individuals understand, and bring out the best in themselves and each other.
The F4S Talent Pool process:
Once you express your interest, you will be asked to complete the F4S work style assessment which measures 48 key attitudes and motivations in the context of work. On completion, you will be automatically added to our growing talent pool and contacted as new opportunities arise.
About Fingerprint For Success (F4S)
Backed by 20+ years of research, F4S’s revolutionary predictive analytics have achieved over 90% reliability in forecasting personal and team motivations, behaviors, and performance. Ultimately, we help people find purpose and fulfillment at work, and help build and scale high performing teams.
Keep in mind, joining our talent pool does not guarantee a job offer. We aim to balance your technical skills with the results of your F4S work style assessment to match the hiring needs of our partners.
Your feedback is a gift! Write to us via:
Powered by JazzHR
oWuPJ9lwq9
Show more
Show less","F4S work style assessment, JazzHR","f4s work style assessment, jazzhr","f4s work style assessment, jazzhr"
Staff Data Engineer,Garner Health,"New York, NY",https://www.linkedin.com/jobs/view/staff-data-engineer-at-garner-health-3779121987,2023-12-17,Middletown,United States,Mid senior,Remote,"Garner's mission is to transform the healthcare economy, delivering high quality and affordable care for all. By helping employers restructure their healthcare benefit to provide clear incentives and data-driven insights, we direct employees to higher quality and lower cost healthcare providers. The result is that patients get better health outcomes while doctors are rewarded for practicing well, not performing more procedures. We are backed by top-tier venture capital firms, are growing rapidly and looking to expand our team.
We are looking for a hands-on Staff Data Engineer to join our data engineering team and drive the implementation of pipelines which deliver the insights core to our business. The ideal candidate should have experience with a modern data stack and implementations of data pipelines using Python in AWS. They should be excited about collaborating with data scientists to bring their research to life.
Main Responsibilities:
Build and maintain data pipelines delivering core insights
Collaborate with data scientists to productionize research results
Protect our users' privacy and security through best practices
Delivering insights via our data warehouse
Support pipelines in production
Assist in task planning, estimation, scheduling, and staffing
Mentor junior and mid-level engineers
Grow engineering teams by interviewing, recruiting, and hiring
Our Tools:
Python, AWS, Terraform, Snowflake, Git
Ideal Qualifications:
7+ years hands-on work building data pipelines and internal applications
Expertise in Python
Comfortable leading research and development projects that produce new designs, products, and processes
Comfortable checking the team's work for technical accuracy
Ability to coordinate work with managers and other staff
Experience working with an orchestration tool such as Airflow, Argo, Prefect, or Dagster
Familiarity with healthcare or insurance
Experience with one or more database warehouses, especially Snowflake
Why You Should Join Our Team:
You are mission-driven and want to work at a company that can change the healthcare system
You want to be on a small, fast-paced team that nimbly moves to meet new challenges
You love ideating on new features and working with data to find new insights
You're excited about researching and working with the latest tools and technologies
The target salary range for this position is $180,000 - $220,000 annually. Individual compensation for this role will depend on a variety of factors including qualifications, skills and applicable laws. In addition to base compensation, this role is eligible to participate in our equity incentive and competitive benefits plans.
Garner Health is proud to be an Equal Employment Opportunity employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, reproductive health decisions, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, genetic information, political views or activity, or other applicable legally protected characteristics.
Garner Health is committed to providing accommodations for qualified individuals with disabilities in our recruiting process. If you need assistance or an accommodation due to a disability, you may contact us at talent@getgarner.com.
Show more
Show less","Python, AWS, Terraform, Snowflake, Git, Airflow, Argo, Prefect, Dagster, Healthcare, Insurance, Data pipelines, Data warehouse, Data engineering","python, aws, terraform, snowflake, git, airflow, argo, prefect, dagster, healthcare, insurance, data pipelines, data warehouse, data engineering","airflow, argo, aws, dagster, data engineering, datapipeline, datawarehouse, git, healthcare, insurance, prefect, python, snowflake, terraform"
Senior Data Engineer & Analyst,Life House,"New York, NY",https://www.linkedin.com/jobs/view/senior-data-engineer-analyst-at-life-house-3673087295,2023-12-17,Middletown,United States,Mid senior,Remote,"Our Company
Life House is a venture-backed, vertically integrated hotel brand, operator and technology company. We develop, design, and operate boutique lifestyle hotels with a mission to make travel more meaningful and reliable for travelers, and to make hotels more seamless and more profitable for owners. We recently raised our Series C and we are looking for talented software developers to join our in-house technology team. We are fast-paced, highly collaborative, and have a friendly, flexible work environment.
Role
At Life House, Sr. Data Engineers & Analysts will work in a product environment to build a game changer dynamic pricing AI- based solution. The Agile team is composed of Data Scientists, Product Managers, Software Engineers, QA's and Revenue Managers. Life House looks to build world-class solutions to the hospitality industry's greatest challenges. With new hotels opening every month, our team is acutely focused on implementing scalable solutions while consistently improving our value proposition to hotel owners - increasing hotel revenue and decreasing hotel operating cost.
Responsibilities
You will share your technical data engineering knowledge, skills and leadership to build a scalable and a top notch product.
Design, build, and productionize complex data pipelines
Develop data ingestion modules that will feed the AI models
Develop baseline AI models with the guidance of Data Scientist(s)
Design and Dev ETL pipeline (Reporting, Machine learning task, Data lake)
Productionize AI models in the cloud and ensure its scalability
Run or participate in running ML/OR models/experiments with the Data Scientists
Design, code, test, and integrate new features and functionality
Apply CI/CD practices to prevent integration problems as well as ensure that the code is releasable at any point in time
Participate in scrum project meetings and update stories using project management tools
Participate in the estimation of the Stories based on defined Acceptance Criteria and Definition of Done
Refactor and test code
Design and implement software architecture that will allow scalability and maintainability
Perform data profiling and analysis
Requirements
Bachelor's degree in related field (Computer/Software Engineering, Computer Science, Artificial Intelligence, Mathematics)
3+ years professional experience in software development or data engineering (post-degree)
3+ years professional experience in designing and developing ETL pipelines
Benefits
An environment that encourages initiative and leadership
Work with highly talented people who are extremely passionate about their craft
Work in Agile, highly collaborative environment
Competitive salary ($90,000 to $150,000 CAD), full benefits, and unlimited PTO
$1200 Travel Stipend
Discount on room reservations at Life House managed properties
Wellness Benefits
Much more…
Show more
Show less","Data engineering, AI, Data pipelines, Data ingestion, Machine learning, ETL pipeline, Cloud computing, CI/CD, Scrum, Software architecture, Data profiling, Data analysis, ETL, Agile, Python, SQL, AWS, Azure, NoSQL, Git, Jira, Tableau","data engineering, ai, data pipelines, data ingestion, machine learning, etl pipeline, cloud computing, cicd, scrum, software architecture, data profiling, data analysis, etl, agile, python, sql, aws, azure, nosql, git, jira, tableau","agile, ai, aws, azure, cicd, cloud computing, data engineering, data ingestion, data profiling, dataanalytics, datapipeline, etl, etl pipeline, git, jira, machine learning, nosql, python, scrum, software architecture, sql, tableau"
Senior Data Engineer / Pyspark / 100% Remote,Motion Recruitment,"New York, NY",https://www.linkedin.com/jobs/view/senior-data-engineer-pyspark-100%25-remote-at-motion-recruitment-3730271793,2023-12-17,Middletown,United States,Mid senior,Remote,"Our client is changing the way we can find our friends and family from around the world. Founded in 2008, they have millions of users per month visiting their platform for various reasons.
They are looking for a Senior Data Engineer to join their Data Ops team to come in and make an impact right away. This Senior Data Engineer should have at least 7 years of professional experience woking with Python, PySpark, Airflow, AWS, and have a computer Science Degree. If this is you APPLY NOW!
Basic Qualifications (Required Skills & Experience)
6-8 years of experience
Python
Pyspark
Spark
AWS
Airflow
EMR
ETL work
SQL
Other Qualifications & Desired Competencies
Fully Remote!!!
Equity and Bonuses involved
You Will Receive The Following Benefits
Medical Insurance
Dental Benefits
Vision Benefits
401(k)
Posted By:
Casey Ryan
Show more
Show less","Python, PySpark, Spark, AWS, Airflow, EMR, ETL, SQL","python, pyspark, spark, aws, airflow, emr, etl, sql","airflow, aws, emr, etl, python, spark, sql"
Senior Data Engineer - healthcare data,Vytalize Health,"Hoboken, NJ",https://www.linkedin.com/jobs/view/senior-data-engineer-healthcare-data-at-vytalize-health-3727309079,2023-12-17,Middletown,United States,Mid senior,Remote,"About Our Company
Vytalize Health is a leading value-based care platform. It helps independent physicians and practices stay ahead in a rapidly changing healthcare system by strengthening relationships with their patients through data-driven, holistic, and personalized care. Vytalize provides an all-in-one solution, including value-based incentives, smart technology, and a virtual clinic that enables independent practices to succeed in value-based care arrangements. Vytalize's care delivery model transforms the healthcare experience for more than 250,000+ Medicare beneficiaries across 36 states by helping them manage their chronic conditions in collaboration with their doctors.
About Our Growth
Vytalize Health has grown its patient base over 100% year-over-year and is now partnered with over 1,000 providers across 36-states. Our all-in-one, vertically integrated solution for value-based care delivery is responsible for $2 billion in medical spending. We are expanding into new markets while increasing the concentration of practices in existing ones.
Visit www.vytalizehealth.com for more information.
Why you will love working here
We are an employee first, mission driven company that cares deeply about solving challenges in the healthcare space. We are open, collaborative and want to enhance how physicians interact with, and treat their patients. Our rapid growth means that we value working together as a team. You will be recognized and appreciated for your curiosity, tenacity and ability to challenge the status quo; approaching problems with an optimistic attitude. We are a diverse team of physicians, technologists, MBAs, nurses, and operators. You will be making a massive impact on people's lives and ultimately feel like you are doing your best work here at Vytalize.
Your opportunity
Data sits at the core of services we deliver to our patients and physicians. The Senior Data Engineer will work as part of the team responsible for ensuring that the data infrastructure is robust, scalable, and secure, enabling the organization to effectively use data to drive insights and make informed decisions. You will be responsible for designing, building, and maintaining scalable data systems that support the organization's business objectives. You will work with cross-functional teams to ensure data is collected, stored, processed, and analyzed in a timely, efficient, and accurate manner. This is a great role for a Senior Data Engineer who loves working with complex data and doing meaningful work that will have a positive impact on other people's lives.
What You Will Do
Design, build, and maintain data pipelines to support business needs
Work with cross-functional teams to understand data requirements and ensure data quality
Implement data storage and processing systems that scale to handle large data sets
Building and maintaining data pipelines to ingest data from various sources and ensure data is properly formatted for analysis.
Develop and maintain data models, dictionaries, and metadata
Collaborate with data scientists and analysts to understand their data needs and ensure that the data systems support their requirements.
Develop and maintain documentation for data systems, processes, and procedures
Implementing data quality control processes to validate and clean data before it is used for analysis.
Designing and implementing security measures to protect sensitive data and ensure data privacy.
Optimizing data systems for performance and scalability to ensure that the data infrastructure can support the growing needs of the organization.
Staying up to date with the latest tools and technologies in the field of data engineering to continue to support the organization's data-related needs.
Qualifications:
Bachelor's degree in Computer Science, Information Systems, or a related field
5+ years of experience in data engineering
10+ years engineering or engineering/analytics experience (note: we will consider exceptional candidates with fewer years of experience for Data Engineer roles)
Strong understanding of data lake, data warehousing, data modeling, and SQL
Strong experience with Python
Experience with healthcare data
Experience with big data technologies such as Hadoop, Spark, and NoSQL databases
Knowledge of data processing and ETL tools such as Airflow or Talend, and understand the tradeoffs of them
Experience with cloud computing platforms such as AWS (or Google Cloud, or Azure)
Strong problem-solving and analytical skills
Excellent written and verbal communication skills
Senior experience in order to have the ability to be self-directed and work autonomously, as well as being part of a collaborative team
Perks/Benefits
Competitive base compensation
Annual bonus potential
Health benefits effective on start date; 100% coverage for base plan, up to 90% coverage on all other plans for individuals and families
Health & Wellness Program; up to $300 per quarter for your overall wellbeing
401K plan effective on the first of the month after your start date; 100% of up to 4% of your annual salary
Company paid STD/LTD
Unlimited (or generous) paid ""Vytal Time"", and 5 paid sick days after your first 90 days
Technology setup
Ability to help build a market leader in value-based healthcare at a rapidly growing organization
We are interested in every qualified candidate who is eligible to work in the United States. However, we are not able to sponsor visas.
Please note at no time during our screening, interview, or selection process do we ask for additional personal information (beyond your resume) or account/financial information. We will also never ask for you to purchase anything; nor will we ever interview you via text message. Any communication received from a Vytalize Health recruiter during your screening, interviewing, or selection process will come from an email ending in @vytalizehealth.com
Show more
Show less","Data Engineering, Data Warehousing, Data Modeling, SQL, Python, Hadoop, Spark, NoSQL, Airflow, Talend, AWS, Cloud Computing, Data Pipelines, Data Analytics, Data Collection, Data Processing, Data Quality Control, Data Security, Data Privacy, Data Lakes","data engineering, data warehousing, data modeling, sql, python, hadoop, spark, nosql, airflow, talend, aws, cloud computing, data pipelines, data analytics, data collection, data processing, data quality control, data security, data privacy, data lakes","airflow, aws, cloud computing, data collection, data engineering, data lakes, data privacy, data processing, data quality control, data security, dataanalytics, datamodeling, datapipeline, datawarehouse, hadoop, nosql, python, spark, sql, talend"
"SR. Scala Engineer, Database Engineering",Experfy,"New York, NY",https://www.linkedin.com/jobs/view/sr-scala-engineer-database-engineering-at-experfy-3590301410,2023-12-17,Middletown,United States,Mid senior,Remote,"As a Sr. Software Engineer for our Data Platform Engineering team you will join skilled Scala engineers and core database developers responsible for developing hosted cloud analytics infrastructure (Apache Spark-based), distributed SQL
processing frameworks, proprietary data science platforms, and core database optimization. This team is responsible for building the automated, intelligent, and highly performant query planner and execution engines, RPC calls between data
warehouse clusters, shared secondary cold storage, etc. This includes building new SQL features and customer-facing functionality, developing novel query optimization techniques for industry-leading performance, and building a database
system that's highly parallel, efficient and fault-tolerant. This is a vital role reporting to exec leadership and senior engineering leadership.
Requirements
Responsibilities:
Writing Scala code with tools like Apache Spark + Apache Arrow to build a hosted, multi-cluster data warehouse for Web3
Developing database optimizers, query planners, query and data routing mechanisms, cluster-to-cluster communication, and workload management techniques
Scaling up from proof of concept to ""cluster scale"" (and eventually hundreds of clusters with hundreds of terabytes each), in terms of both infrastructure/architecture and problem structure
Codifying best practices for future reuse in the form of accessible, reusable patterns, templates, and code bases to facilitate meta data capturing and management
Managing a team of software engineers writing new code to build a bigger, better, faster, more optimized HTAP database (using Apache Spark, Apache Arrow and a wealth of other open source data tools)
Interacting with exec team and senior engineering leadership to define, prioritize, and ensure smooth deployments with other operational components
Highly engaged with industry trends within analytics domain from a data acquisition processing, engineering, management perspective
Understand data and analytics use cases across Web3 / blockchains
Skills & Qualifications
Bachelor's degree in computer science or related technical field. Masters or PhD a plus
6+ years experience engineering software and data platforms / enterprise-scale data warehouses, preferably with knowledge of open source Apache stack (especially Apache Spark, Apache Arrow, and others)
3+ years experience with Scala and Apache Spark
A track record of recruiting and leading technical teams in a demanding talent market
Rock solid engineering fundamentals; query planning, optimizing and distributed data warehouse systems experience is preferred but not required
Nice to have: Knowledge of blockchain indexing, web3 compute paradigms, Proofs and consensus mechanisms... is a strong plus but not required
Experience with rapid development cycles in a web-based environment
Strong scripting and test automation knowledge
Nice to have: Passionate about Web3, blockchain, decentralization, and a base understanding of how data/analytics plays into this
Show more
Show less","Scala, Apache Spark, Apache Arrow, SQL, Query planning, Query optimization, Distributed data warehouse systems, RPC, Data routing, Workload management, Meta data capturing, HTAP database, Blockchain indexing, Web3 compute paradigms, Proofs, Consensus mechanisms, Scripting, Test automation","scala, apache spark, apache arrow, sql, query planning, query optimization, distributed data warehouse systems, rpc, data routing, workload management, meta data capturing, htap database, blockchain indexing, web3 compute paradigms, proofs, consensus mechanisms, scripting, test automation","apache arrow, apache spark, blockchain indexing, consensus mechanisms, data routing, distributed data warehouse systems, htap database, meta data capturing, proofs, query optimization, query planning, rpc, scala, scripting, sql, test automation, web3 compute paradigms, workload management"
Staff Data Engineer,Hinge,"New York, NY",https://www.linkedin.com/jobs/view/staff-data-engineer-at-hinge-3669242937,2023-12-17,Middletown,United States,Mid senior,Remote,"Hinge is the dating app designed to be deleted
In today's digital world, finding genuine relationships is tougher than ever. At Hinge, we’re on a mission to inspire intimate connection to create a less lonely world. We’re obsessed with understanding our users’ behaviors to help them find love, and our success is defined by one simple metric– setting up great dates. With tens of millions of users across the globe, we’ve become the most trusted way to find a relationship, for all.
About The Role
As a Staff Data Engineer at Hinge, you will design, build, and support critical data platforms for our data teams and Hinge at large. The systems you help create, the problems you help solve, and the support of our analytical minds, will be pivotal to the success of Hinge.
You will be making a real impact on the data platform team and will be key to the success of Hinge. Your work will enable the organization to make data-driven decisions and drive improvements, which will affect the love lives of tons of people.
The Staff Data Engineer will identify areas where data engineering could be impactful, architect initiatives, and shepherd those initiatives to a successful delivery. You will be a leader, representing data engineering within the data platform team. This role is vital to our team and has the potential to make a lasting impact.
Responsibilities:
Work with our data teams to ensure data is flowing accurately through data creation to our presentation layers.
Become an advocate for the Data Engineering team by developing and championing Data Engineering practices with the team and with the company at large.
Improve our data infrastructure through containerization, data modeling, developing our ETL pipelines, and more. Work with stakeholders and translate their needs and expectations into action items and deliverables.
Assess opportunities for ways data infrastructure could be utilized to support the data organization.
Lead infrastructure initiatives; from design to implementation to delivery.
Participate in our on-call rotation.
What We’re Looking For:
Expertise in Python, SQL, DevOps, and databases.
Experience utilizing multiple programming paradigms, specifically functional and object-oriented.
Expertise with data modeling, data pipelines, and data governance concepts, and agile methodologies.
7+ years of data engineering experience.
Proven ability to drive initiatives and articulate their value to Engineering and other stakeholders.
Experience delivering data products from conception to delivery.
Great communication skills (written/verbal).
Passionate about designing elegant data infrastructure and pipelines.
Familiarity with our stack: Kubernetes, Docker, Terraform, Kafka, Airflow, dbt, Looker, AWS technologies (S3, Redshift), GCP technologies (Dataflow, BigQuery). and CI/CD technologies (CircleCI).
$213,000 - $255,500 a year
The salary range for this position is $213,500 - $255,500. Factors such as scope and responsibilities of the position, candidate's work experience, education/training, job-related skills, internal peer equity, as well as market and business considerations may influence base pay offered. This salary range is reflective of a position based in New York City. This salary will be subject to a geographic adjustment (according to a specific city and state), if an authorization is granted to work outside of the location listed in this posting.
As a member of our team, you’ll enjoy:
401(k) Matching:
We match 100% of the first 10% of pre-tax 401(k) contributions you make, up to a maximum of $10,000 per year.
Professional Growth:
Get a $3,000 annual Learning & Development stipend once you’ve been with us for three months. You also get free access to Udemy, an online learning and teaching marketplace with over 6000 courses, starting your first day.
Parental Leave & Planning:
When you become a new parent, you’re eligible for 100% paid parental leave (20 paid weeks for both birth and non-birth parents.)
Fertility Support:
You’ll get easy access to fertility care through Carrot, from basic treatments to fertility preservation. We also provide $10,000 toward fertility preservation. You and your spouse/domestic partner are both eligible.
Date Stipend:
All Hinge employees receive a $100 monthly stipend for epic dates– Romantic or otherwise. Hinge Premium is also free for employees and their loved ones.
ERGs:
We have eight Employee Resource Groups (ERGs)—Asian, Unapologetic, Disability, LGBTQIA+, Vibras, Women/Nonbinary, Parents, and Remote—that hold regular meetings, host events, and provide dedicated support to the organization & its community.
At Hinge, our core values are…
Authenticity:
We share, never hide, our words, actions and intentions.
Courage:
We embrace lofty goals and tough challenges.
Empathy:
We deeply consider the perspective of others.
Diversity inspires innovation
Hinge is an equal-opportunity employer. We value diversity at our company and do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. We believe success is created by a diverse workforce of individuals with different ideas, strengths, interests, and cultural backgrounds.
Show more
Show less","Python, SQL, DevOps, Databases, Data Modeling, Data Pipelines, Data Governance, Agile Methodologies, Kubernetes, Docker, Terraform, Kafka, Airflow, Dbt, Looker, AWS technologies, GCP technologies, CI/CD technologies, Functional programming, Objectoriented programming","python, sql, devops, databases, data modeling, data pipelines, data governance, agile methodologies, kubernetes, docker, terraform, kafka, airflow, dbt, looker, aws technologies, gcp technologies, cicd technologies, functional programming, objectoriented programming","agile methodologies, airflow, aws technologies, cicd technologies, data governance, databases, datamodeling, datapipeline, dbt, devops, docker, functional programming, gcp technologies, kafka, kubernetes, looker, objectoriented programming, python, sql, terraform"
"Senior Data Engineer - $180k-$220k (Snowflake, Python)",CyberCoders,"New York, NY",https://www.linkedin.com/jobs/view/senior-data-engineer-%24180k-%24220k-snowflake-python-at-cybercoders-3773343001,2023-12-17,Middletown,United States,Mid senior,Remote,"If you are a Senior Data Engineer with Snowflake, Python & Data Pipeline experience, please read on...
We are founded and owned by one of the largest media companies. Our mission is to be bring simplicity & scale to our industry. As a rapidly growing company (founded in 2017 & up 140% year over year) weve recently elevated our team in preparation for our next stage of growth and are building out our technology team.
What You Need for this Position
10+ yrs industry experience
Snowflake
Python
Experience building pipelines
Proven History working in smaller data teams
BSCS or related degree
Benefits
Vacation/PTO
Medical
Dental
Vision
Bonus
So, if you are a Senior Data Engineer - $180k-$220k (Snowflake, Python) with experience, please apply today!
Colorado employees will receive paid sick leave. For additional information about available benefits, please contact Nitu Gulati-Pauly
Email Your Resume In Word To
Looking forward to receiving your resume through our website and going over the position with you. Clicking apply is the best way to apply, but you may also:
Nitu.Gulati-Pauly@cybercoders.com
Please do NOT change the email subject line in any way. You must keep the JobID: linkedin : NNG-1776063 -- in the email subject line for your application to be considered.***
Nitu Gulati-Pauly - VP of Recruiting - CyberCoders
Applicants must be authorized to work in the U.S.
CyberCoders is proud to be an Equal Opportunity Employer
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, protected veteran status, or any other characteristic protected by law.
Your Right to Work
– In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire.
CyberCoders will consider for Employment in the City of Los Angeles qualified Applicants with Criminal Histories in a manner consistent with the requirements of the Los Angeles Fair Chance Initiative for Hiring (Ban the Box) Ordinance.
Show more
Show less","Data Engineering, Snowflake, Python, Data Pipelines, SQL, Statistics, Machine Learning, Cloud Computing, Software Development, Problem Solving, Communication, Teamwork","data engineering, snowflake, python, data pipelines, sql, statistics, machine learning, cloud computing, software development, problem solving, communication, teamwork","cloud computing, communication, data engineering, datapipeline, machine learning, problem solving, python, snowflake, software development, sql, statistics, teamwork"
Data Engineer,C2R Ventures,"New York, NY",https://www.linkedin.com/jobs/view/data-engineer-at-c2r-ventures-3780545357,2023-12-17,Middletown,United States,Mid senior,Hybrid,"Top Financial firm is looking for a Data Engineer to work with large and complex data sets.
Qualified candidates must have:
BS and/or MS degree in Computer Science or a related STEM field
3+ years of Python programming experience
Strong SQL experience (experience designing schemas, data mapping, query optimization)
AWS, EKS/Kubernetes, Kafka, and NoSQL experience a plus
Financial industry experience
Show more
Show less","Data Engineering, Python, SQL, AWS, EKS/Kubernetes, Kafka, NoSQL, Financial industry","data engineering, python, sql, aws, ekskubernetes, kafka, nosql, financial industry","aws, data engineering, ekskubernetes, financial industry, kafka, nosql, python, sql"
Data Engineer,C2R Ventures,"New York, NY",https://www.linkedin.com/jobs/view/data-engineer-at-c2r-ventures-3780545701,2023-12-17,Middletown,United States,Mid senior,Hybrid,"Top Financial firm is looking for a Data Engineer to work with large and complex data sets.
Qualified candidates must have:
BS and/or MS degree in Computer Science or a related STEM field
3+ years of Python programming experience
Strong SQL experience (experience designing schemas, data mapping, query optimization)
AWS, EKS/Kubernetes, Kafka, and NoSQL experience a plus
Financial industry experience
Show more
Show less","Computer Science, Python, SQL, AWS, EKS, Kubernetes, Kafka, NoSQL","computer science, python, sql, aws, eks, kubernetes, kafka, nosql","aws, computer science, eks, kafka, kubernetes, nosql, python, sql"
Data Engineer,Emergent365 Inc,"Newark, NJ",https://www.linkedin.com/jobs/view/data-engineer-at-emergent365-inc-3776445045,2023-12-17,Middletown,United States,Mid senior,Hybrid,"Job Summary:
Financial Firm in Newark, NJ is seeking a Data Engineer to join a team responsible for Fixed Income technology. We are looking for an experienced Data Engineer for building out data pipelines, data warehousing design and dimensional modeling. Must have Fixed Income product knowledge, Python, SQL, Snowflake, AWS, and Spark. Will be hybrid onsite 2 days a week in Newark, NJ.
Required Skills:
5+ years of experience in building out data pipelines using Python
3 + years of experience working in AWS Cloud especially services like S3, EMR, Lambda, Glue, Athena, Event Bridge and Step Functions.
3+ years of experience with Spark
Experience with workflow management tools like Airflow and messaging technologies like Kafka.
Cloud database experience using Snowflake and RedShift
Database and data warehousing design and implementation as well as dimensional modeling experience is required.
Has good working knowledge of conceptual, logical, and physical data modeling concepts.
Ability to communicate the status and challenges and align with the team
Demonstrating the ability to learn new skills and work as a team
Your Desired Skills:
Experience working in Hadoop or other big data platforms
Exposure to deploying code through pipeline
Good exposure to Containers like ECS or Docker
Direct experience supporting multiple business units for foundational data work and sound understanding of capital markets within Fixed Income
Knowledge of Jira, Confluence, SAFe development methodology & DevOps
Excellent analytical and problem-solving skills with the ability to think quickly and offer alternatives both independently and within teams.
Proven ability to work quickly in a dynamic environment.
Bachelor's degree Computer Science or a related field.
Show more
Show less","Python, SQL, Snowflake, AWS, Spark, Airflow, Kafka, Hadoop, ECS, Docker, Jira, Confluence, SAFe, DevOps","python, sql, snowflake, aws, spark, airflow, kafka, hadoop, ecs, docker, jira, confluence, safe, devops","airflow, aws, confluence, devops, docker, ecs, hadoop, jira, kafka, python, safe, snowflake, spark, sql"
Senior Data Engineer,Oakmont Consulting,"New York, NY",https://www.linkedin.com/jobs/view/senior-data-engineer-at-oakmont-consulting-3769273797,2023-12-17,Middletown,United States,Mid senior,Hybrid,"Senior Data Engineer
Location: New York (Hybrid)
Salary: $150k-$200K DOE
The Company
Our clients mission is to improve the quality & safety of engineered products - (airplanes, electric vehicles, and medical devices) by creating a best in class proactive management platform, powered by the latest advances in AI.
They are revolutionizing the way next-gen product are made by partnering with forward-looking engineering leaders to create and deploy AI models. They utilise bleeding-edge tech & AI stack - including Generative AI and NLP/LLMs - to solve real-world problems.
The team includes experts in Enterprise AI from Palantir, McKinsey & QuantumBlack, and other top tech companies. They have deployed across some of the largest Automotive and Aerospace companies in the world.
Key Responsibilities
- Work on both data exploration and building data pipeline
- Help the engineering leaders we work with understand their data and problems which can be solved using the platform
- Play an active role in team meetings and workshops with clients
- Rotate between client project work and internal product development in alignment with your personal development plan
Requirements
- 4+ years of industry experience building
data pipelines
- Writing clean, well-documented code in
Python/Scala (Spark/Pandas)
- Comfortable in deploying data/code to the cloud
(GCP/AWS)
- Has build data pipelines ensuring required data accuracy using unit/integration tests
- Is proficient in distributed computing frameworks like Apache Beam, Spark
- Worked with
orchestration
&
scheduling
tools (eg. airflow, kubeflow, luigi, dagster)
- Is able to use SQL to derive quick insights from databases
- Comfortable working in an unstructured environment
Desired/Bonus
- Working in Startups on SaaS products
- Skills: Data analysis tools, jupyter notebooks, Pandas, GraphQL, MongoDB
Show more
Show less","Data exploration, Data pipeline, Python, Scala, Spark, Pandas, Cloud deployment, GCP, AWS, Unit testing, Integration testing, Distributed computing, Apache Beam, Airflow, Kubeflow, Luigi, Dagster, SQL, Data analysis tools, Jupyter notebooks, GraphQL, MongoDB","data exploration, data pipeline, python, scala, spark, pandas, cloud deployment, gcp, aws, unit testing, integration testing, distributed computing, apache beam, airflow, kubeflow, luigi, dagster, sql, data analysis tools, jupyter notebooks, graphql, mongodb","airflow, apache beam, aws, cloud deployment, dagster, data analysis tools, data exploration, data pipeline, distributed computing, gcp, graphql, integration testing, jupyter notebooks, kubeflow, luigi, mongodb, pandas, python, scala, spark, sql, unit testing"
Senior Data Engineer,Alldus,"New York, NY",https://www.linkedin.com/jobs/view/senior-data-engineer-at-alldus-3757281354,2023-12-17,Middletown,United States,Mid senior,Hybrid,"Data Engineer
We are currently recruiting a Senior Data Engineer for an exciting seed level startup in New York. Our client are using data and AI to work with customers and predict consumer trends.
You as the ideal candidate will be responsible for:
Building data pipelines
Working across various teams including Data Science and Machine Learning to translate requirements into data pipelines
Writing code in Python
Integrating data from various sources
You as the ideal candidate will require:
Data Engineering experience
Experience building out data pipelines
Python
GCP Experience
ML/MLOps exposure – beneficial
*Please not this role would be hybrid*
This is a great opportunity for an ambitious Data Engineer to join an exciting startup with great growth opportunities.
If you are interested, please apply or send a resume to kieran@alldus.com
Show more
Show less","Data Engineering, Python, GCP, Data pipelines, Data Science, Machine Learning, ML/MLOps","data engineering, python, gcp, data pipelines, data science, machine learning, mlmlops","data engineering, data science, datapipeline, gcp, machine learning, mlmlops, python"
Data Engineer,TalentOla,"Jersey City, NJ",https://www.linkedin.com/jobs/view/data-engineer-at-talentola-3773566045,2023-12-17,Middletown,United States,Mid senior,Hybrid,"Hello,
Greetings of the day!!
I just got a few excellent opportunities for Role Data Engineer.
It is an onsite role in Jersey City, NJ. A very quick fill role with immediate interviews.
Job Title/Role – Data Engineer
Job Location - Jersey City, NJ
Experience required - 9+ Years
Job Description:
System integration, Data analysis, Schema management, Testing of features, JIRA story writeups.
Technical knowledge/background of databases, Kafka, UI/UX mockups etc.
--
Thanks & Regards
Sharad Solanki
US Recruitment Team
E: Sharad@TalentOla.com
W: https://www.talentola.com/
Add - 8 The Green STE A Dover DE – 19901
TalentOla Inc.
Show more
Show less","Data Engineering, System Integration, Data Analysis, Schema Management, Feature Testing, JIRA, Databases, Kafka, UI/UX Mockups","data engineering, system integration, data analysis, schema management, feature testing, jira, databases, kafka, uiux mockups","data engineering, dataanalytics, databases, feature testing, jira, kafka, schema management, system integration, uiux mockups"
Sr. Data Engineer,Theorem,"New York, NY",https://www.linkedin.com/jobs/view/sr-data-engineer-at-theorem-3779062008,2023-12-17,Middletown,United States,Mid senior,Hybrid,"About Us
Pursuit of truth in credit.
By using machine learning to anticipate and manage risk in credit, we’re empowering our partners and lenders to unlock opportunity and access for more borrowers, everywhere.
We strive to be the preferred partner to lending platforms, providing not only access to capital but also underwriting technology capabilities to allow innovative lending platforms to grow their business.
Our firm is made up of 60+ professionals working in San Mateo (HQ) and New York, working in-office on Tuesdays and Thursdays. We are passionate, hard-working, relentlessly-resourceful, impact-focused individuals. We deeply value intellectual curiosity, independence of thought, creative idea generation, empathy, and close collaboration.
The Role
As a Data Engineer, you will work alongside quantitative researchers, finance & operations, investor relations & sales, and capital markets & partnerships.
Your role is to develop the systems and data pipelines that enable the shared data assets informing every decision of the firm.
What You'll Do
Partner with stakeholders and senior leaders across Theorem to understand needs and set priorities for data-driven workflows
Scope new infrastructure capabilities and partner with engineers to get those delivered
Develop robust software to support new and existing data initiatives across the firm
Contribute to the creation of an industry-leading loan modeling platform that will generate data around which the business will coalesce
Build complex data pipelines with high demands for correctness, speed, robustness, and availability
Grow as an engineer by working with and receiving mentorship from senior engineers at the firm
You Will Own
Availability and freshness of data through clear ETL ownership and automated alerting strategies
Integration of system operational data sources into the Theorem data warehouse
Quantification and tracking of data quality across all pipelines and build technology
End-user discovery and accessibility of data used across the firm by normalizing, standardizing, and cataloging all assets
What We're Looking For
A minimum of 2 years of Data Engineering experience at a technology company
Track record of being able to effectively communicate technical concepts and scope solutions
Experience building automated reports, dashboards, and visualizations of curated data
Experience assessing, implementing, and monitoring data validation and quality (correctness, completeness, availability, etc.)
Fluency in SQL and Python
Deep expertise in relational data modeling, schema design, and normalization
Bonus
Previous experience in the financial industry
Characteristics To Thrive
Hardworking and gritty
Ethical, intellectually honest, and transparent
Detail-oriented
Proactive with communication
Collaborative and team success-oriented
Excited to learn and grow from feedback and experience
At home on small, high-impact teams
Thorough in your end-to-end ownership of outcomes
Biased towards action to solve problems
Additional Information
Expected full-time salary range between $150,000 to $220,000 + bonus + equity + benefits
Advertised and actual salary ranges may differ by geographic area, work experience, education, and/or skill level
Our Commitment
We foster an environment that welcomes professionals with a diversity of backgrounds and ideas. We value professionals who are thoughtful, innovative, tenacious, and mission-driven. Every member of the team has a major impact on the company's success with visible contributions to the business. We encourage and reward growth, learning, and a solutions-seeking mindset. We offer a competitive salary and opportunity for equity ownership, generous benefits, and an inclusive and collaborative work environment. If you’re excited by the opportunities to create outsized impact as part of a world-class team, we strongly encourage you to apply.
We provide reasonable accommodation for qualified individuals with disabilities and disabled veterans in job application procedures. If you have any difficulty using our online system and you need an accommodation due to a disability, you may use the following alternative email address to contact us about your interest in employment: careers@theoremlp.com. Alternatively, you can contact us at 415-489-0457.
Theorem does not accept unsolicited agency resumes and is not responsible for any fees related to unsolicited resumes.
Show more
Show less","Python, SQL, Data Engineering, ETL, Data Warehousing, Relational Data Modeling, Schema Design, Data Normalization, Data Quality Assessment, Data Validation, Data Visualization, Dashboards, Reporting, Business Intelligence, Financial Industry","python, sql, data engineering, etl, data warehousing, relational data modeling, schema design, data normalization, data quality assessment, data validation, data visualization, dashboards, reporting, business intelligence, financial industry","business intelligence, dashboard, data engineering, data normalization, data quality assessment, data validation, datawarehouse, etl, financial industry, python, relational data modeling, reporting, schema design, sql, visualization"
Sr. Data Engineer,Axion Ray,"New York, NY",https://www.linkedin.com/jobs/view/sr-data-engineer-at-axion-ray-3698442168,2023-12-17,Middletown,United States,Mid senior,Hybrid,"Axion Ray’s mission is to improve the quality and safety of engineered products - airplanes, electric vehicles, and medical devices, by creating the world’s best proactive management platform, powered by the latest advances in artificial intelligence. We're revolutionizing the way next-gen vehicles are made and are partnering with forward-looking engineering leaders to create and deploy AI models that will accelerate our speed to an electric and supersonic future. Axion leverages bleeding-edge tech & AI stack - including Generative AI and NLP/LLMs - to solve real-world problems.
Our team includes experts in Enterprise AI from Palantir, McKinsey & QuantumBlack, and other top tech companies.
Since our founding at the onset of 2021, we’ve deployed across some of the largest Automotive and Aerospace companies in the world. If you want the chance to help build the future of engineering, join us!
This role is hybrid, 3+ days a week in our NYC office. This is not an internship or new grad position as we are seeking experienced individuals.
What You'll Do
Data Exploration and Pipeline Development:Design, develop, and optimize data pipelines to ensure efficient data flow from various sources to our systems. Your expertise will be instrumental in exploring new data sources and methods to enhance our data-driven approach.
Proof of Concept Implementation: Ideate and develop novel features as proof-of-concept projects, and spearhead their integration into our production systems.
Hands-on Development and Planning: Be actively involved in both the technical and strategic aspects of our projects. Spend around 70% of your time in hands-on coding and development, while the remaining 30% is dedicated to strategic planning and scoping with the broader team.
Cross-Functional Collaboration: Work collaboratively with a diverse team of professionals, including data scientists, machine learning engineers, front-end engineers, and the rest of the business. Your expertise will be pivotal in aligning engineering efforts across teams to achieve our product objectives.
Who You Are
Thrive in an unstructured environment, with the opportunity to find gaps and fill them with scalable processes.
Prioritize clean, well-documented Python code that future engineers can build upon with ease.
Possess proficiency e in deploying data and code to cloud platforms (GCP/AWS/Azure).
Demonstrate at least 3+ years of building and deploying data pipelines into production environments.
Expertise in orchestration tools like Airflow, Dagster, Prefect, or Luigi, with a preference for Dagster.
Possess at least 2 years of experience working with pandas.
Demonstrate hands-on experience working with MongoDB.
Nice to have
Prior experience in a rapidly evolving startup environment.
Background in the aerospace or automotive industries.
Proficiency with GraphQL.
Familiarity with Terraform and networking.
The base salary range for New York is $140-$200K with meaningful equity DOE
Axion Ray is an Equal Opportunity / Affirmative Action employer committed to diversity in the workplace. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, age, national origin, disability, protected veteran status, gender identity, or any other factor protected by applicable federal, state, or local laws.
Show more
Show less","Generative AI, NLP/LLMs, Enterprise AI, Data Pipelines, Data Exploration, Proof of Concept Implementation, Handson Development, Strategic Planning, CrossFunctional Collaboration, Python, GCP/AWS/Azure, Airflow, Dagster, Prefect, Luigi, pandas, MongoDB, GraphQL, Terraform","generative ai, nlpllms, enterprise ai, data pipelines, data exploration, proof of concept implementation, handson development, strategic planning, crossfunctional collaboration, python, gcpawsazure, airflow, dagster, prefect, luigi, pandas, mongodb, graphql, terraform","airflow, crossfunctional collaboration, dagster, data exploration, datapipeline, enterprise ai, gcpawsazure, generative ai, graphql, handson development, luigi, mongodb, nlpllms, pandas, prefect, proof of concept implementation, python, strategic planning, terraform"
Data Engineer,Motion Recruitment,"New York, NY",https://www.linkedin.com/jobs/view/data-engineer-at-motion-recruitment-3742725823,2023-12-17,Middletown,United States,Mid senior,Hybrid,"My client is looking for a Data Engineer to join their growing team, this is a large and established media company here in NYC. They are looking for a Data Engineer who will be responsible for expanding and optimizing their data and data pipeline architecture, as well as optimizing data flow and collection.
The team functions in a hybrid working capacity, with an office in Manhattan. They are offering a competitive base salary, with a generous equity package.
Qualifications
Professional Data Engineering Experience, 3+ yrs
Strong experience with their current stack. Python, SQL, AWS.
Experience working with Snowflake is a plus.
Offer/Benefits
Competitive base salary and generous equity
Strong healthcare coverage for you and your dependents
Hybrid in their Manhattan office
401(k) Program
Posted By:
Max Singer
Show more
Show less","Data Engineering, Python, SQL, AWS, Snowflake","data engineering, python, sql, aws, snowflake","aws, data engineering, python, snowflake, sql"
Sr. Data Engineer,MHK TECH INC,"Jersey City, NJ",https://www.linkedin.com/jobs/view/sr-data-engineer-at-mhk-tech-inc-3767590420,2023-12-17,Middletown,United States,Mid senior,Hybrid,"Responsibilities
Designing and implementing large-scale, distributed data processing systems using technologies such as Apache Hadoop, Apache Spark, or Apache Flink.
Developing and optimizing data pipelines and workflows for ingesting, storing, processing, and analyzing large volumes of structured and unstructured data.
Collaborating with data scientists, data analysts, and other stakeholders to understand data requirements and translate them into technical solutions.
Building and maintaining data infrastructure, including data lakes, data warehouses, and real-time streaming platforms.
Designing and implementing data models and schemas for efficient data storage and retrieval.
Ensuring the scalability, availability, and fault-tolerance of big data systems through proper configuration, monitoring, and performance tuning.
Identifying and evaluating new technologies, tools, and frameworks to improve the efficiency and effectiveness of big data processing.
Implementing data security and privacy measures to protect sensitive information throughout the data lifecycle.
Collaborating with cross-functional teams to integrate data from various sources, including structured databases, unstructured files, APIs, and streaming data.
Developing and maintaining documentation, including data flow diagrams, system architecture, and technical specifications.
Requirements
Bachelor's or higher degree in Computer Science, Engineering, or a related field.
Proven experience as a big data engineer or a similar role, with a deep understanding of big data technologies, frameworks, and best practices.
Strong programming skills in languages such as Java, Scala, or Python for developing big data solutions.
Experience with big data processing frameworks like Apache Hadoop, Apache Spark, Apache Flink, or similar.
Proficiency in SQL and NoSQL databases, as well as data modeling and database design principles.
Familiarity with cloud platforms and services, such as Amazon Web Services (AWS), Microsoft Azure, or Google Cloud Platform (GCP).
Knowledge of distributed computing principles and technologies, such as HDFS, YARN, and containerization (e.g., Docker, Kubernetes).
Understanding of real-time streaming technologies and frameworks, such as Apache Kafka or Apache Pulsar.
Strong problem-solving skills and ability to optimize and tune big data processing systems for performance and scalability.
Excellent communication and teamwork skills to collaborate with cross-functional teams and stakeholders.
Show more
Show less","Apache Hadoop, Apache Spark, Apache Flink, Data pipelines, SQL, NoSQL, Data modeling, Data warehouses, Cloud platforms, Distributed computing, HDFS, YARN, Docker, Kubernetes, Apache Kafka, Apache Pulsar, Java, Scala, Python","apache hadoop, apache spark, apache flink, data pipelines, sql, nosql, data modeling, data warehouses, cloud platforms, distributed computing, hdfs, yarn, docker, kubernetes, apache kafka, apache pulsar, java, scala, python","apache flink, apache hadoop, apache kafka, apache pulsar, apache spark, cloud platforms, data warehouses, datamodeling, datapipeline, distributed computing, docker, hdfs, java, kubernetes, nosql, python, scala, sql, yarn"
Onsite work - Need Sr. Data Engineer Jersey City NJ,Steneral Consulting,"Jersey City, NJ",https://www.linkedin.com/jobs/view/onsite-work-need-sr-data-engineer%C2%A0jersey-city-nj-at-steneral-consulting-3681091324,2023-12-17,Middletown,United States,Mid senior,Hybrid,"Sr. Data Engineer
Jersey City, NJ - Onsite - Local candidates only under 60 Min commute
A minimum of ten (10) years of IT experience in data engineering or data management field
8 - 10 years of experience in application development using SQL, PLSQL, Python, and shell scripting
Strong working knowledge and experience in Oracle, SQL Server, Snowflake, or any SQL database
Recent experience as a Senior Data Engineer in any public cloud, preferably on Azure as well as on a cloud warehouse like Snowflake or Azure Synapse is required
5+ years of experience in data warehousing and building data pipelines using various ETL and ELT tools
Hands-on experience with Snowflake, DBT, and Airflow is highly preferred
Experience in building cloud-native solutions, preferably on Azure including the use of DevOps CI / CD tools and containers.
Familiarity with
Familiarity with data modeling tools/techniques is a plus
Azure or Snowflake training/certification is a plus
Strong analytical, quantitative, problem-solving, communication, and organizational skills
Self-driven, self-directed, passionate analytical, and focused on delivering the right results
Show more
Show less","SQL, PLSQL, Python, Shell scripting, Oracle, SQL Server, Snowflake, DBT, Airflow, DevOps CI / CD tools, Containers, Data modeling tools/techniques, Azure, Azure Synapse, ETL, ELT","sql, plsql, python, shell scripting, oracle, sql server, snowflake, dbt, airflow, devops ci cd tools, containers, data modeling toolstechniques, azure, azure synapse, etl, elt","airflow, azure, azure synapse, containers, data modeling toolstechniques, dbt, devops ci cd tools, elt, etl, oracle, plsql, python, shell scripting, snowflake, sql, sql server"
Senior Data Engineer,adonis,"New York, NY",https://www.linkedin.com/jobs/view/senior-data-engineer-at-adonis-3745551821,2023-12-17,Middletown,United States,Mid senior,Hybrid,"At Adonis, we are building a medical billing engine that automates billing workflows and improves reimbursement rates for medical providers. Billing and insurance related activities represent nearly $1 in every $7 collected by medical providers - and the high costs of collecting payments, as well as and other costs that stem from the administrative challenges within the space, unfortunately get passed down to the consumer. This is why healthcare costs continue to rise, with no end to that in sight. We’re set on helping to mitigate these challenges.
Responsibilities
Design, build, maintain, and enhance Adonis’s data platform, data lakehouse, and internal data-focused APIs
Build and maintain ETL data pipelines involving homogenous and heterogenous data from external healthcare data sources and our internal databases in a medallion architecture
Design data solutions using various big data technologies and low latency architectures
Collaborate with data scientists, product managers, and frontend engineers to develop, implement, and validate deployed data solutions
Understand and implement data engineering best practices
Improve, manage, and teach standards for code maintainability and performance in code submitted and reviewed
Mentor and provide guidance to other engineers on the job
Qualifications
Expert at writing and optimizing SQL queries, especially when using PostgreSQL databases
Expert in writing and optimizing Python 3.x focused on data processing and providing REST APIs
Familiarity with event-driven architectures to stitch workflows together in a more reactive fashion over purely CRON-based triggers
Familiarity with streaming workflows to process datasets at low latencies
Familiarity with data engineering and data science development tools and libraries (i.e. Pandas, NumPy, Jupyter)
Familiarity with data and API engineering offerings of Amazon Web Services (i.e. Athena, Fargate, Lambda, RDS, Redshift, SageMaker, Step Functions)
Familiarity with data warehousing concepts
Familiarity with basic principles of distributed computing (able to optimize partitioning, distribution and MPP of high-level data structures)
Authoritative in ETL optimization, designing, coding, and tuning big data processes
Experience in Prefect, Airflow, or other workflow orchestrators
Experience in working with large databases, efficiently moving billions of rows, and complex data modeling
Experience in managing data lineage, quality, and discovery
Experience with big data & concurrency technologies like PySpark, Ray, or others
At Adonis, we adopt an in-person culture. It is expected you will be in the office 5 days per week.
Salary Range: $150,000 - $180,000
Show more
Show less","SQL, PostgreSQL, Python 3.x, Pandas, NumPy, Jupyter, AWS (Athena Fargate Lambda RDS Redshift SageMaker Step Functions), Data warehousing, Distributed computing, ETL optimization, Prefect, Airflow, PySpark, Ray","sql, postgresql, python 3x, pandas, numpy, jupyter, aws athena fargate lambda rds redshift sagemaker step functions, data warehousing, distributed computing, etl optimization, prefect, airflow, pyspark, ray","airflow, aws athena fargate lambda rds redshift sagemaker step functions, datawarehouse, distributed computing, etl optimization, jupyter, numpy, pandas, postgresql, prefect, python 3x, ray, spark, sql"
"Senior Data Engineer, Productivity Engineer",SiriusXM,"New York, NY",https://www.linkedin.com/jobs/view/senior-data-engineer-productivity-engineer-at-siriusxm-3770197659,2023-12-17,Middletown,United States,Mid senior,Hybrid,"Who We Are
SiriusXM and its brands (Pandora, SXM Media, AdsWizz, Simplecast, and SiriusXM Connected Vehicle Services) are leading a new era of audio entertainment and services by delivering the most compelling subscription and ad-supported audio entertainment experience for listeners -- in the car, at home, and anywhere on the go with connected devices. Our vision is to shape the future of audio, where everyone can be effortlessly connected to the voices, stories and music they love wherever they are.
This is the place where a diverse group of emerging talent and legends alike come to share authentic and purposeful songs, stories, sounds and insights through some of the best programming and technology in the world. Our critically-acclaimed, industry-leading audio entertainment encompasses music, sports, comedy, news, talk, live events, and podcasting. No matter their individual role, each of our employees plays a vital part in bringing SiriusXM’s vision to life every day.
SiriusXM is the leading audio entertainment company in North America, and the premier programmer and platform for subscription and digital advertising-supported audio products. SiriusXM’s platforms collectively reach approximately 150 million listeners, the largest digital audio audience across paid and free tiers in North America, and deliver music, sports, talk, news, comedy, entertainment and podcasts. Pandora, a subsidiary of SiriusXM, is the largest ad-supported audio entertainment streaming service in the U.S. SiriusXM's subsidiaries Simplecast and AdsWizz make it a leader in podcast hosting, production, distribution, analytics and monetization. The Company’s advertising sales organization, which operates as SXM Media, leverages its scale, cross-platform sales organization and ad tech capabilities to deliver results for audio creators and advertisers. SiriusXM, through Sirius XM Canada Holdings, Inc., also offers satellite radio and audio entertainment in Canada. In addition to its audio entertainment businesses, SiriusXM offers connected vehicle services to automakers.
How You’ll Make An Impact
We are seeking a highly skilled and motivated Senior Data Engineer, Productivity Engineer to join our dynamic team at SiriusXM. As a Data Productivity Engineer, you will play a key role in designing, building, and maintaining the tools and services used by our data professionals in order to effectively drive the business in a data-driven manner. The ideal candidate will have a strong background in cloud technologies, data engineering, infrastructure as code, API design, and experience applying software engineering and DevOps best practices.
What You’ll Do
Design, develop, and maintain tools and services that empower data professionals to streamline their workflows and enhance productivity.
Collaborate with cross-functional teams to understand data analysis, engineering, and modeling needs and translate them into effective and user-friendly solutions.
Implement best practices for optimizing data processing workflows, ensuring efficient utilization of resources, and minimizing latency in data-related tasks.
Identify and address bottlenecks in existing tools and services to improve overall system performance.
Integrate data productivity tools with existing data infrastructure and platforms, fostering seamless collaboration among teams.
Develop and implement automation solutions to streamline repetitive tasks and enhance the efficiency of data processes.
Create comprehensive documentation for tools and services, ensuring that users have access to clear and concise instructions.
Provide training and support to data professionals, enabling them to effectively leverage the tools and services developed.
Work closely with data scientists, engineers, and analysts to understand their requirements and challenges, fostering a collaborative and innovative environment.
Communicate project status, issues, and solutions effectively to stakeholders and team members.
What You’ll Need
Bachelor's degree in a relevant technical field (Computer Science, Information Technology, etc.).
5+ years’ experience in software development, with a focus on tools and services for data professionals.
Proficiency in programming languages such as Python, Scala, or Java.
Experience with infrastructure as code tools such as CDK or Terraform.
Experience with big data technologies (e.g., Apache Spark, Hadoop) and data processing frameworks.
Knowledge of data storage solutions, database systems, and data warehousing.
Familiarity with machine learning frameworks and model development is a plus.
Excellent problem-solving skills and a proactive approach to addressing challenges.
Strong communication and collaboration skills.
Must have legal right to work in the U.S.
At SiriusXM, we carefully consider a wide range of factors when determining compensation, including your background and experience. These considerations can cause your compensation to vary. We expect the base salary for this position to be in the range of $126,000 to $145,800 and will depend on your skills, qualifications, and experience. Additionally, this role might be eligible for discretionary short-term and long-term incentives. We encourage all interested candidates to apply.
Our goal at SiriusXM is to provide and maintain a work environment that fosters mutual respect, professionalism and cooperation. SiriusXM is an equal opportunity employer that does not discriminate on the basis of actual or perceived race, creed, color, religion, national origin, ancestry, alienage or citizenship status, age, disability or handicap, sex, gender identity, marital status, familial status, veteran status, sexual orientation or any other characteristic protected by applicable federal, state or local laws.
The requirements and duties described above may be modified or waived by the Company in its sole discretion without notice.
R-2023-11-101
Show more
Show less","Python, Scala, Java, CDK, Terraform, Apache Spark, Hadoop, Machine learning frameworks, Data storage solutions, Database systems, Data warehousing","python, scala, java, cdk, terraform, apache spark, hadoop, machine learning frameworks, data storage solutions, database systems, data warehousing","apache spark, cdk, data storage solutions, database systems, datawarehouse, hadoop, java, machine learning frameworks, python, scala, terraform"
Sr. Data Engineer,Ekodus INC.,"Jersey City, NJ",https://www.linkedin.com/jobs/view/sr-data-engineer-at-ekodus-inc-3682464088,2023-12-17,Middletown,United States,Mid senior,Hybrid,"Only GC or US Citizen needed for this
Sr. Data Engineer (Contract)
Onsite - New Jersy City
A minimum of ten (10) years of IT experience in data engineering or data management field
8 - 10 years of experience in application development using SQL, PLSQL, Python, and shell scripting
Strong working knowledge and experience in Oracle, SQL Server, Snowflake, or any SQL database
Recent experience as a Senior Data Engineer in any public cloud, preferably on Azure as well as on a cloud warehouse like Snowflake or Azure Synapse is required
5+ years of experience in data warehousing and building data pipelines using various ETL and ELT tools
Hands-on experience with Snowflake, DBT (Data build tool), and Airflow is highly preferred
Experience in building cloud-native solutions, preferably on Azure including the use of DevOps CI / CD tools and containers.
Familiarity with
Familiarity with data modeling tools/techniques is a plus
Azure or Snowflake training/certification is a plus
Strong analytical, quantitative, problem-solving, communication, and organizational skills
Self-driven, self-directed, passionate analytical, and focused on delivering the right results
Please share resume at msharma@ekodusinc.com or career@ekodusinc.com.
Show more
Show less","Data Engineering, Data Management, Application Development, SQL, PLSQL, Python, Shell Scripting, Oracle, SQL Server, Snowflake, Azure, Azure Synapse, Data Warehousing, Data Pipelines, ETL, ELT, Snowflake, DBT, Airflow, Cloudnative Solutions, DevOps, CI/CD, Azure or Snowflake Training/Certification, Analytical Skills, Quantitative Skills, ProblemSolving Skills, Communication Skills, Organizational Skills, SelfDriven, SelfDirected, Analytical, Focused on Delivering Results","data engineering, data management, application development, sql, plsql, python, shell scripting, oracle, sql server, snowflake, azure, azure synapse, data warehousing, data pipelines, etl, elt, snowflake, dbt, airflow, cloudnative solutions, devops, cicd, azure or snowflake trainingcertification, analytical skills, quantitative skills, problemsolving skills, communication skills, organizational skills, selfdriven, selfdirected, analytical, focused on delivering results","airflow, analytical, analytical skills, application development, azure, azure or snowflake trainingcertification, azure synapse, cicd, cloudnative solutions, communication skills, data engineering, data management, datapipeline, datawarehouse, dbt, devops, elt, etl, focused on delivering results, oracle, organizational skills, plsql, problemsolving skills, python, quantitative skills, selfdirected, selfdriven, shell scripting, snowflake, sql, sql server"
Onsite work - Need Sr. Data Engineer in Jersey City NJ,Steneral Consulting,"Jersey City, NJ",https://www.linkedin.com/jobs/view/onsite-work-need-sr-data-engineer-in%C2%A0jersey-city-nj-at-steneral-consulting-3682440536,2023-12-17,Middletown,United States,Mid senior,Hybrid,"Sr. Data Engineer
Jersey City, NJ - Onsite - Local candidates only under 60 Min commute
A minimum of ten (10) years of IT experience in data engineering or data management field
8 - 10 years of experience in application development using SQL, PLSQL, Python, and shell scripting
Strong working knowledge and experience in Oracle, SQL Server, Snowflake, or any SQL database
Recent experience as a Senior Data Engineer in any public cloud, preferably on Azure as well as on a cloud warehouse like Snowflake or Azure Synapse is required
5+ years of experience in data warehousing and building data pipelines using various ETL and ELT tools
Hands-on experience with Snowflake, DBT, and Airflow is highly preferred
Experience in building cloud-native solutions, preferably on Azure including the use of DevOps CI / CD tools and containers.
Familiarity with
Familiarity with data modeling tools/techniques is a plus
Azure or Snowflake training/certification is a plus
Strong analytical, quantitative, problem-solving, communication, and organizational skills
Self-driven, self-directed, passionate analytical, and focused on delivering the right results
Show more
Show less","SQL, PLSQL, Python, Shell Scripting, Oracle, SQL Server, Snowflake, Azure, Azure Synapse, ETL, ELT, DBT, Airflow, DevOps, CI/CD, Containers, Data Modeling","sql, plsql, python, shell scripting, oracle, sql server, snowflake, azure, azure synapse, etl, elt, dbt, airflow, devops, cicd, containers, data modeling","airflow, azure, azure synapse, cicd, containers, datamodeling, dbt, devops, elt, etl, oracle, plsql, python, shell scripting, snowflake, sql, sql server"
Senior Data Engineer,Zenith,"New York, NY",https://www.linkedin.com/jobs/view/senior-data-engineer-at-zenith-3781738810,2023-12-17,Middletown,United States,Mid senior,Hybrid,"About Zenith
Zenith is the ROI Agency. Our more than 6,000 specialists across 95 markets offer unparalleled capabilities in Media, Data, Technology, Commerce and Content. We put effectiveness at the heart of our work to solve complex challenges, drive successful business outcomes, and grow our clients’ businesses. Zenith is part of Publicis Media, the media arm of Publicis Groupe.
Responsibilities
Manage the complete API development process from conception to deployment.
Write clean, efficient, and reusable code.
Maintain and upgrade the APIs following deployment.
Write application and database code based on business requirements or user stories, architectural requirements, and existing code.
Modify and improve data engineering processes to handle ever larger, more complex, and more types of data sources and pipelines.
Estimate and plan development work, track and report on task progress, and deliver work on schedule.
Write ad-hoc queries based on schema knowledge for various application requirements
Contribute to operational support documentation.
Participate in weekly on-call rotation.
Qualifications
Bachelor's degree in information technology, computer science, or related field.
At least two years' experience as a Python or Node.js developer.
Advanced SQL coding 4+ years, tuning and query optimization, especially within cloud-based data warehouses MS Azure and Amazon Redshift.
In-depth knowledge of libraries, frameworks, and tech stacks
Architecting code for efficiencies and technical limitations
Ability to use Asynchronous Programming to improve productivity
Attention to detail, especially in identifying and fixing errors
Knowledge of design patterns
Good understanding of Agile development methodologies
Professional or academic experience working with large data sets for conducting quantitative analysis.
Self-motivated and a self-starter with strong ability to multitask projects/tasks effectively.
Digital media experience, particularly experience working in ad tech industry with a data-centric role plus.
Additional Information
All your information will be kept confidential according to EEO guidelines.
Veterans Encouraged to Apply
Compensation Range: $81,500 - $100,000 annually. This is the pay range the Company believes it will pay for this position at the time of this posting. Consistent with applicable law, compensation will be determined based on the skills, qualifications, and experience of the applicant along with the requirements of the position, and the Company reserves the right to modify this pay range at any time. For this role, the Company will offer medical coverage, dental, vision, disability, 401k, and paid time off.
Show more
Show less","Python, Node.js, SQL, Cloudbased data warehouses, MS Azure, Amazon Redshift, Asynchronous Programming, Design patterns, Agile development methodologies, Quantitative analysis, Datacentric role","python, nodejs, sql, cloudbased data warehouses, ms azure, amazon redshift, asynchronous programming, design patterns, agile development methodologies, quantitative analysis, datacentric role","agile development methodologies, amazon redshift, asynchronous programming, cloudbased data warehouses, datacentric role, design patterns, ms azure, nodejs, python, quantitative analysis, sql"
"Data Research Analyst, gt.school (Remote) - $60,000/year USD",Crossover,"Detroit, MI",https://www.linkedin.com/jobs/view/data-research-analyst-gt-school-remote-%2460-000-year-usd-at-crossover-3783188217,2023-12-17,Oak Park,United States,Associate,Remote,"Crossover is the world's #1 source of full-time remote jobs. Our clients offer top-tier pay for top-tier talent. We're recruiting this role for our client, gt.school. Have you got what it takes?
Are you ready to revolutionize education with the power of cutting-edge technology? We're seeking strategic problem solvers who thrive on developing and implementing innovative solutions and are eager to apply the power of generative AI to supercharge their work. Your consistent high performance and dedication will drive our mission to revolutionize education.
At gt.school, we're on a mission to accelerate the pace of learning by 5-10X, harnessing the incredible potential of technology, algorithms, and generative AI. We are creating a platform where students can discover, explore, and learn at an unprecedented rate. You'll play a pivotal role in bringing this vision to life. You'll have the opportunity to build new tools and operational processes, as well as streamline existing workflows for maximum efficiency.
But that's not all! At gt.school, we believe in your growth and development. You'll receive regular coaching and training from a team of global tech experts and operations professionals, ensuring your continuous improvement. We value collaboration and provide a supportive environment where you can meet with team members to work together on exciting projects.
If you're eager for a role that combines research, strategy, product development, and data science in the ever-evolving realm of education and technology, seize the moment and apply below. Let's embark on this exhilarating journey together!
What You Will Be Doing
Engineering generative AI prompts in Python/JavaScript/JSON
Analyze AI outputs and iterate on prompts to reach desired outcomes
Leverage generative AI and technology to develop new educational tools, iterate on current products, and finetune AI training models
Research topics to support the product engineering team
What You Won’t Be Doing
Collecting data or performing administrative tasks - we have a junior operations analyst team to do this type of work.
Working on unimportant projects - you'll be tasked with projects that are mission-critical
Data Research Analyst Key Responsibilities
Create technical functionalities that revolutionize the way education is delivered.
Basic Requirements
Excellent written and verbal English communication skills
Proficient in Python, R, JavaScript, or another OOP language
2+ years of work experience working in a technical field (science, technology, programming, financial services, data science, etc.)
Consistently able to work 40 hours per week, Monday through Friday, between 6AM and 6PM CST
Must be a proactive communicator who can effectively manage multiple priorities and stakeholders simultaneously
About Gt.school
GT School is an EdTech Startup that leverages technology, AI, and subject matter experts to cultivate a new way of learning. Our unique approach leverages 50+ years of learning science, cutting-edge data analytics and high-performance coaching. In doing so, we can help students learn more, learn faster, and learn better - and have fun while doing it. We are a remote-first company that hires globally via Crossover.
There is so much to cover for this exciting role, and space here is limited. Hit the Apply button if you found this interesting and want to learn more. We look forward to meeting you!
Working with Crossover
This is a full-time (40 hours per week), long-term position. The position is immediately available and requires entering into an independent contractor agreement with Crossover. The compensation level for this role is $30 USD/hour, which equates to $60,000 USD/year assuming 40 hours per week and 50 weeks per year. The payment period is weekly. Consult www.crossover.com/help-and-faqs for more details on this topic.
What to expect next:
You will receive an email with a link to start your self-paced, online job application.
Our hiring platform will guide you through a series of online “screening” assessments to check for basic job fit, job-related skills, and finally a few real-world job-specific assignments.
Important!
If you do not receive an email from us:
First, emails may take up to 15 minutes to send, refresh and check again.
Second, check your spam and junk folders for an email from Crossover.com, mark as “Not Spam” since you will receive other emails as well.
Third, we will send to whatever email account you indicated on the Apply form - by default, that is the email address you use as your LinkedIn username and it might be different than the one you have already checked.
If all else fails, just reset your password by visiting https://www.crossover.com/auth/password-recovery if you already applied using LinkedIn EasyApply.
Crossover Job Code: LJ-4880-US-Detroit-DataResearchAn.006
Show more
Show less","Python, JavaScript, JSON, R, OOP, Generative AI, AI prompts, Data science, Data analytics, Educational tools, Workflow optimization, Agile development, Continuous improvement, Remote work, English communication, Stakeholder management, Data Research, Technology, Programming, Financial services","python, javascript, json, r, oop, generative ai, ai prompts, data science, data analytics, educational tools, workflow optimization, agile development, continuous improvement, remote work, english communication, stakeholder management, data research, technology, programming, financial services","agile development, ai prompts, continuous improvement, data research, data science, dataanalytics, educational tools, english communication, financial services, generative ai, javascript, json, oop, programming, python, r, remote work, stakeholder management, technology, workflow optimization"
Business Data Analyst I,DHL Supply Chain,"Livonia, MI",https://www.linkedin.com/jobs/view/business-data-analyst-i-at-dhl-supply-chain-3733089180,2023-12-17,Oak Park,United States,Mid senior,Onsite,"Are you a passionate leader looking for autonomy and exciting career possibilities?Do you take an energetic and resourceful approach to problem-solving while bringing innovative ideas and analytics to life on behalf of your team and your customers?Do you enjoy effectively translating requirements into an efficient process and/or system solution? If so, DHL Supply Chain has the opportunity for you.
Job Description
To apply knowledge and analytics to develop and communicate timely, accurate, and actionable insight to the business through the use of modeling, visualization, and optimization. Responsible for the reporting, analyzing, and predicting of operational processes, performance, and Key Performance Indicators. Communication with site leadership, operations, and finance on efficiency, customer requirements, account specific issues, and insight into to the business, operations, and customer.
Applies hindsight, insight, and foresight techniques to communicate complex findings and recommendations to influence others to take action
Uses knowledge of business and data structure to discover and/or anticipate problems where data can be used to solve the problem
Uses spreadsheets, databases, and relevant software to provide ongoing analysis of operational activities
Applies data visualization for discovery and timely insights to decrease Cycle Time to Action (CTA)
Assists site operations in identifying areas for improving service levels, reducing operational costs, and providing other operational enhancements
Supports account start-up analysis and/or report implementation as needed
Develop standardized and ad hoc site and/or customer reporting
Streamlines and/or automates internal and external reporting
May investigate and recommend new technologies and information systems
May conduct feasibility analyses on various processes and equipment to increase efficiency of operations
Partners with Finance to develop financial models to analyze productivity and payroll; calculates cost benefits and business impact and proposes solutions
Develops predictive models to help drive decision making
Designs, develops, and implements data gathering and reporting methods and procedures for Operations
Responsible for tracking, planning, analysis, and forecasting of storage capacities, inventory levels, equipment and/or labor requirements
Coordinates with Operations Systems group to ensure technical issues and problems are being identified, addressed, and resolved in a timely manner
May coordinate with ILD group on issues related to modeling customer solutions, including providing data and relevant insight for customer pursuits
Responsible for assisting finance and senior leadership in modeling yearly labor budget based on operational and profile changes
Required Education And Experience
Undergraduate degree in business, logistics, mathematics, statistics, related field, or equivalent experience, required
1+ years of analytics experience, required
Our Organization has a business casual environment and focuses on teamwork, associate development, training, and continuous improvement. We offer competitive wages, excellent affordable insurance benefits (including health, dental, vision and life), 401K plan, paid vacation and holidays.
Our Organization is an equal opportunity employer.
Power BI building and visualization skills
SQL
MS Excel
MS Azure Data Factory is a plus
Show more
Show less","Power BI, SQL, MS Excel, MS Azure Data Factory, Warehousing, Supply Chain Management, Data Analytics, Data Visualization, Forecasting, Modeling, Optimization, Data Discovery, Data Mining, Data Integration, Reporting","power bi, sql, ms excel, ms azure data factory, warehousing, supply chain management, data analytics, data visualization, forecasting, modeling, optimization, data discovery, data mining, data integration, reporting","data discovery, data integration, data mining, dataanalytics, datawarehouse, forecasting, modeling, ms azure data factory, ms excel, optimization, powerbi, reporting, sql, supply chain management, visualization"
"Senior Data Analyst, Revenue Strategy",Royal Caribbean Group,"Miami, FL",https://www.linkedin.com/jobs/view/senior-data-analyst-revenue-strategy-at-royal-caribbean-group-3781724938,2023-12-17,Miami,United States,Mid senior,Hybrid,"Position Summary
As Senior Data Analyst, Revenue Strategy, this individual is responsible for supporting our revenue strategy and automation initiatives by measuring revenue initiative effectiveness and execution of revenue opportunities in excess of $100M annually. This includes pricing and inventory strategies through the use of automation techniques to impact over 15 million price points for Revenue Management. This person will work across numerous departments to build consensus across pricing strategies, inventory utilization strategies, automation, and other revenue initiatives. They will have exposure to and work closely with Revenue Management, Data Science & Analytics, IT and Commercial Strategy, from Senior Leadership to Analyst level.
Essential Duties And Responsibilities
Responsible for continuous analysis on revenue performance of ongoing RM Automation Projects.
Identify new revenue generating opportunities through internal research and analyses; build business cases that support long-term financial impact of growth initiatives.
Report on status updates and findings through written and verbal means to Senior Leadership.
Spearhead growth initiatives through change management with all necessary stakeholders including, but not limited to Revenue Management Product Team, Data Science & Analytics, IT and Commercial Strategy.
Drive continuous improvement and refinement of all initiatives after implementation.
Performance of these tasks requires an entrepreneurial mindset with very limited supervision.
Employees will be required to perform any other job-related duties assigned by their supervisor or management.
Qualifications And Education
To perform this position successfully, the individual should be a self-starter and be able to complete a project with little direction. Furthermore, the candidate must be able to perform each essential duty in a timely and accurate manner. The requirements listed below are representative of the knowledge, skills, and or ability required to satisfy this position:
Master of Business Administration (MBA) or master’s degree (MA) preferred but not required.
Bachelor’s degree (BA) from four-year college or university required with focus in quantitative discipline preferred.
One to Three years of related experience in financial / business analysis or related field preferred. Familiarization with revenue management concepts and tools preferred.
Must be able to demonstrate understanding of Revenue Management concepts and tools or basic economics.
Previous experience with Sales, Marketing, Analytics preferred but not required.
Previous experience in implementing system enhancements and decision-making tools preferred but not required.
Demonstrated ability to develop and deliver high quality, clear and concise presentations.
Demonstrated experience in project management.
Proficiency in Excel, Brio Intelligence/Hyperion Studio, Toad Oracle SQL, PowerBI, Word, and PowerPoint required and ability to manipulate data essential.
Knowledge of reservations system and revenue management data preferred but not required.
Attention to detail required.
Willingness to accept immediate product and project responsibility.
Ability to operate in a team environment.
Excellent interpersonal skills and demonstrated maturity. Demonstrated aptitude for problem solving and problem identification.
Ability to collect, analyze and interpret revenue performance data.
Ability to logically structure quantitative analyses of complex issues.
Strong written and verbal communication skills and ability to work in global environment.
Ability to manage multiple projects and responsibilities at once
Financial/QUantitative Responsabilities
Determines strategies and executes decision making to drive in excess of $100M by 2025 in annual incremental net ticket revenues.
Responsible for supporting initiatives to optimally manage 100% of price points on each sailing & category across the Royal Caribbean International brand. Understand implications of automated changes and alter revenue initiative strategies to execute initiatives in a revenue optimal manner.
Physical Requirements
The physical demands described here are representative of those requirements employees must meet to perform the essential functions of this job with or without reasonable accommodations. While performing job functions the employee is regularly required to sit, stand, write, review and type reports, compile data, operate a pc, communicate, listen, and assess information. The employee may move about the office complex, may travel to other office locations, and may lift, push, pull or move 10 - 15 pounds. Visual requirements include distant, close and color vision, and ability to adjust focus.
Working Conditions
The work environment characteristics described here are representative of those an employee encounters while performing the essential functions of the job. The environment includes work inside/outside the office.
Show more
Show less","Revenue Management, Data Analytics, Strategy, Automation, Pricing, Inventory, Data Science, Business Analysis, Financial Analysis, Project Management, Excel, Brio Intelligence, Hyperion Studio, Toad Oracle SQL, PowerBI, Word, PowerPoint, Reservations System, Communication, Problem Solving, Data Interpretation, Quantitative Analyses","revenue management, data analytics, strategy, automation, pricing, inventory, data science, business analysis, financial analysis, project management, excel, brio intelligence, hyperion studio, toad oracle sql, powerbi, word, powerpoint, reservations system, communication, problem solving, data interpretation, quantitative analyses","automation, brio intelligence, business analysis, communication, data interpretation, data science, dataanalytics, excel, financial analysis, hyperion studio, inventory, powerbi, powerpoint, pricing, problem solving, project management, quantitative analyses, reservations system, revenue management, strategy, toad oracle sql, word"
Privacy Data Analyst,Venable LLP,"New York, NY",https://www.linkedin.com/jobs/view/privacy-data-analyst-at-venable-llp-3725938907,2023-12-17,North Hempstead,United States,Associate,Hybrid,"Venable LLP’s Technology & Innovation Group seeks a Privacy Data Analyst to join the Venable Blue team in the Washington, D.C., New York, Los Angeles, or San Francisco office. The Privacy Data Analyst, works on and supports all aspects of client-based projects related to privacy program management, privacy operations, and regulatory response. This is a non-attorney position.
This role will help bring efficiency to how clients respond to regulatory compliance requests, including data gathering, automation of regulatory responses, and measuring operational effectiveness of privacy safeguards. This will include gap analysis and actionable recommendations for improvements in quality and accuracy.
The Privacy Data Analyst will work on and support the delivery of various workstreams such as Safeguard Maintenance, Safeguard Development, Program Management, and Audit Support.
Key responsibilities include working with clients to:
Formulate responses to privacy compliance requests within SLA, perform data quality reviews, and conduct operational effectiveness testing against privacy safeguards.
Develop and communicate best practices for regulatory response.
Develop automation for compliance responses to increase efficiency and to provide consistency and quality in responses.
Articulate data and technology gaps and their compliance impact to a variety of technical and non-technical stakeholders, including product and engineering teams, risk and compliance partners, assessors and regulators.
Recommend process improvements and strategic initiatives as related to privacy compliance response.
Coordinate and drive client privacy response activities for both inbound and outbound relationships.
Support business relationships with internal and external auditors and regulators.
Qualifications:
B.A. or B.S. degree; Master’s degree (or equivalent) preferred
3 years of minimum work experience in high profile settings, such as presenting to leadership and driving cross-functional teams
3 years of minimum experience in a quantitative role related to data, reporting, and analytical problem solving
3 years of minimum experience writing complex SQL queries to drive analysis and insights
Experience coding in Python
Preferred Qualifications:
Experience coding in R, PHP and/or similar programming language
Experience with developing security/privacy reporting and recommendations that are meaningful, defensible and actionable for a variety of audiences
Experience developing and submitting audit and compliance reports to governing bodies, legal entities, and/or external authorities
Experienced in processes for assessing and designing internal controls for large scale organizations
Experience performing risk assessments or safeguard/control operational effectiveness testing
Venable offers full-service solutions to everything from routine to novel privacy and cybersecurity challenges. Our team brings to bear significant experience and industry knowledge to help clients satisfy data privacy and security laws and maximize their business potential. Fully immersed in all aspects of data privacy, cybersecurity, and information governance, Venable is unique among privacy and cybersecurity practices.
We participate in legislative advocacy, rulemakings, and development of new legal standards. Our team advises organizations with regard to industry best practices and drafting codes of conduct and standards, helping them stay compliant with federal, state, international, and self-regulatory requirements.
We strengthen the integrity of our clients’ data, ecommerce security, and customer or user records; develop internal data collection and use practices; and ensure the creation of sound privacy policies and procedures.
Venable Blue helps organizations and individuals manage and mitigate risk in the online space. Whether it’s an issue of data access, account takeover, cyber harassment, child safety, or a government or regulatory investigation, we build, operationalize, and deploy integrated programs and systems designed with people and products in mind.
For additional information about the Technology & Innovation Group, Venable Blue team see: www.venableblue.com
Venable LLP is an
American Lawyer
Global 100 law firm headquartered in Washington, D.C., with offices in California, Delaware, Florida, Illinois, Maryland, New York, and Virginia. Our lawyers and legislative advisors serve domestic and international clients in all areas of corporate and business law, complex litigation, intellectual property, regulatory matters, and government affairs. Additional information can be found at Venable.com.
The salary range for Privacy Data Analyst positions is $85,000 - $130,000 per year. This is the minimum and maximum salary that Venable in good faith believes at the time of this posting that it is willing to pay for the advertised position. Exact compensation will be determined based on individual candidate qualifications and location.
We comply with the Los Angeles Fair Chance Initiative for Hiring and the San Francisco Fair Chance Ordinance. Conviction of a crime will not necessarily be a bar to employment at the Firm. Factors such as age at the time of the offense, type of the offense, seriousness of the offense, remoteness of the offense in time, position applied for, rehabilitation, overall record, and other relevant factors will be taken into account in determining effect on suitability for employment.
Venable LLP is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, age, protected Veteran status and any other protected basis under applicable law.
Show more
Show less","Privacy Program Management, Privacy Operations, Regulatory Response, Gap Analysis, Process Improvement, SQL, Python, R, PHP, Security/Privacy Reporting, Audit/Compliance Reporting, Risk Assessment, Data Governance, Data Privacy, Cybersecurity, Legislative Advocacy, Intellectual Property, Litigation, Regulatory Matters","privacy program management, privacy operations, regulatory response, gap analysis, process improvement, sql, python, r, php, securityprivacy reporting, auditcompliance reporting, risk assessment, data governance, data privacy, cybersecurity, legislative advocacy, intellectual property, litigation, regulatory matters","auditcompliance reporting, cybersecurity, data governance, data privacy, gap analysis, intellectual property, legislative advocacy, litigation, php, privacy operations, privacy program management, process improvement, python, r, regulatory matters, regulatory response, risk assessment, securityprivacy reporting, sql"
"Director, Data Scientist - Biopharma",Pfizer,"New York, NY",https://www.linkedin.com/jobs/view/director-data-scientist-biopharma-at-pfizer-3729745373,2023-12-17,North Hempstead,United States,Associate,Hybrid,"ROLE SUMMARY:
The Commercial Analytics team at Pfizer is looking for a Director, Data Science who is passionate about crafting and implementing predictive modeling and statistical analysis to build end-to-end solutions and insights that have a direct impact on patient's lives and the future of Pfizer as a data-driven organization. You will be a thought partner to the business, understand strategic goals, and then use your skills and subject matter expertise to surface impactful insights that drive business decisions and patient benefits. With colleagues across the globe, our rigorous analytical expertise is relied upon as the compass and decision support for the enterprise. Our dynamic, exciting team of subject-matter experts comes from diverse backgrounds and experiences, including market research, data science, digital analytics, finance, and consulting. Our culture is about getting things done iteratively and rapidly, with open feedback and debate along the way. We believe Data Science is a team sport, but we strive for independent decision-making and taking smart risks.
ROLE RESPONSIBILITIES:
This role is accountable for delivering data science driven support for an assigned brand and will partner with US Commercial, business, and digital teams, to develop and implement models, insights, and data products that drive brands’ strategic priorities. This is an individual contributor role.
Take deep dives in large-scale data to identify key insights that will shape future product/brand strategy for a specific therapeutic area.
Collaborate with cross-functional teams to identify new growth opportunities, develop data requirements, establish critical metrics, and evangelize data products.
Design, deploy, and evaluate experiments that help define opportunities for higher adoption, improved business performance, and better patient experience.
Conduct hypothesis-driven exploratory analyses, select appropriate ML algorithms, and build complex optimization engines to deliver impactful data solutions!
Research new technologies and methods across data science and data engineering to improve the technical capabilities of the team.
Communicate insights to senior management by distilling complex analysis and concepts into concise business-focused takeaways.
QUALIFICATIONS:
Bachelor’s degree with 10+ years of experience OR Masters Degree with 9+ years of experience OR PhD with 7+ years of experience
Degree preferably in engineering, economics, statistics, computer science, or related quantitative field.
Preferred experience in Applied Econometrics, Statistics, Data Mining, Machine Learning, Analytics, Mathematics, Operations Research, Industrial Engineering, or related field preferred.
Working knowledge of relational databases, including SQL, and large-scale distributed systems such as Hadoop and or working in Snowflake/Databricks
Ability to implement data science pipelines and applications in a general programming language such as Python, Scala, Java, or R.
Practical experience with and theoretical understanding of ML algorithms for classification, regression, clustering, and anomaly detection
Well versed with and have experience applying various statistical methodologies including Bayesian and non-parametric techniques, hypothesis testing, ANOVA, Regression, fixed and random effects etc. to measure the impact of experiments
Hands on experience working in big data environments such as Hadoop, Spark, and using Python / SQL or comparable languages for manipulating and analyzing complex clickstream and or unstructured data
Ability to extract significant business insights from data and identify the roots behind the patterns
Experience working with a data visualization tool/package, including Dash, Tableau, and Angular etc.
Communication skills for communicating complex quantitative analyses to senior business executives
Candidate demonstrates a breadth of diverse leadership experiences and capabilities including: the ability to influence and collaborate with peers, develop and coach others, oversee and guide the work of other colleagues to achieve meaningful outcomes and create business impact.
Other Job Details:
Last Date to Apply for Job: October 31, 2023
The annual base salary for this position ranges from $144,900.00 to $241,500.00.* In addition, this position is eligible for participation in Pfizer’s Global Performance Plan with a bonus target of 20.0% of the base salary and eligibility to participate in our share based long term incentive program. We offer comprehensive and generous benefits and programs to help our colleagues lead healthy lives and to support each of life’s moments. Benefits offered include a 401(k) plan with Pfizer Matching Contributions and an additional Pfizer Retirement Savings Contribution, paid vacation, holiday and personal days, paid caregiver/parental and medical leave, and health benefits to include medical, prescription drug, dental and vision coverage. Learn more at Pfizer Candidate Site – U.S. Benefits | (uscandidates.mypfizerbenefits.com). Pfizer compensation structures and benefit packages are aligned based on the location of hire. The United States salary range provided does not apply to Tampa, FL or any location outside of the United States.
The annual base salary for this position in Tampa, FL ranges from $130,400.00 to $217,300.00.
Relocation assistance may be available based on business needs and/or eligibility.
Sunshine Act
Pfizer reports payments and other transfers of value to health care providers as required by federal and state transparency laws and implementing regulations. These laws and regulations require Pfizer to provide government agencies with information such as a health care provider’s name, address and the type of payments or other value received, generally for public disclosure. Subject to further legal review and statutory or regulatory clarification, which Pfizer intends to pursue, reimbursement of recruiting expenses for licensed physicians may constitute a reportable transfer of value under the federal transparency law commonly known as the Sunshine Act. Therefore, if you are a licensed physician who incurs recruiting expenses as a result of interviewing with Pfizer that we pay or reimburse, your name, address and the amount of payments made currently will be reported to the government. If you have questions regarding this matter, please do not hesitate to contact your Talent Acquisition representative.
EEO & Employment Eligibility
Pfizer is committed to equal opportunity in the terms and conditions of employment for all employees and job applicants without regard to race, color, religion, sex, sexual orientation, age, gender identity or gender expression, national origin, disability or veteran status. Pfizer also complies with all applicable national, state and local laws governing nondiscrimination in employment as well as work authorization and employment eligibility verification requirements of the Immigration and Nationality Act and IRCA. Pfizer is an E-Verify employer.
Marketing and Market Research
Show more
Show less","Data Science, Predictive Modeling, Statistical Analysis, DataDriven Organization, Machine Learning, Analytics, Mathematics, Optimization, Business Intelligence, Data Visualization, Data Mining, Econometrics, Statistics, Applied Econometrics, Bayesian and NonParametric Techniques, Hypothesis Testing, ANOVA, Regression, Fixed and Random Effects, SQL, Hadoop, Snowflake/Databricks, Python, Scala, Java, R, Spark, Dash, Tableau, Angular","data science, predictive modeling, statistical analysis, datadriven organization, machine learning, analytics, mathematics, optimization, business intelligence, data visualization, data mining, econometrics, statistics, applied econometrics, bayesian and nonparametric techniques, hypothesis testing, anova, regression, fixed and random effects, sql, hadoop, snowflakedatabricks, python, scala, java, r, spark, dash, tableau, angular","analytics, angular, anova, applied econometrics, bayesian and nonparametric techniques, business intelligence, dash, data mining, data science, datadriven organization, econometrics, fixed and random effects, hadoop, hypothesis testing, java, machine learning, mathematics, optimization, predictive modeling, python, r, regression, scala, snowflakedatabricks, spark, sql, statistical analysis, statistics, tableau, visualization"
Database Engineer,Iceberg Cyber Security,New York City Metropolitan Area,https://www.linkedin.com/jobs/view/database-engineer-at-iceberg-cyber-security-3784383019,2023-12-17,North Hempstead,United States,Mid senior,Hybrid,"Location: New York City (3 days in the office & 2 WFH)
Comp: $200k-$250k + guaranteed first year bonus + relocation bonus (if relocating)
My client, an industry-leading quantitative hedge fund at the forefront of technology, is seeking a skilled Database Engineer to join their dynamic cross-platform engineering team. Headquartered in New York City, this firm specializes in leveraging cutting-edge technology for investment strategies. In this role, you will be responsible for the complete lifecycle of database platforms, including design, implementation, and integration on both Windows and Linux systems. It’s also essential that you can handle SQL server operational escalations.
Collaborating closely with the wider infrastructure engineering team, you will play a pivotal role in delivering self-service and automation-driven solutions. As a Database Engineer, you will also collaborate with software developers and trading teams to optimize and scale applications and software that involve database access.
Proficiency in multiple programming languages, including Python, Java, and C#, is a key requirement for this role. Additionally, you should possess extensive expertise in at least one database platform, complemented by a strong understanding of other platforms such as SQL Server, Postgres, Redis, MongoDB, or Cassandra.
Show more
Show less","Python, Java, C#, SQL Server, PostgreSQL, Redis, MongoDB, Cassandra, Windows, Linux","python, java, c, sql server, postgresql, redis, mongodb, cassandra, windows, linux","c, cassandra, java, linux, mongodb, postgresql, python, redis, sql server, windows"
Accountant - Data Analyst,Apex Logistics International,"East Rancho Dominguez, CA",https://www.linkedin.com/jobs/view/accountant-data-analyst-at-apex-logistics-international-3785591883,2023-12-17,Huntington Beach,United States,Associate,Onsite,"Delivering Passion with Every Shipment!
Established in 2001, Apex Logistics International has become a top dependable solution for those seeking expertise in freight forwarding, contract logistics, customs brokerage, transportation management, warehousing, and distribution. With a presence in 70 countries across six continents, 42 offices, 2,500+ dedicated employees, and consistent, dependable services, Apex continues to grow rapidly and deliver passion worldwide. In 2021, Apex joined the Kuehne+Nagel Group and together have become the Number 1 Global Air Freight Forwarder.
***This is a hybrid position that requires 2 days in office at Rancho Dominguez, CA.***
Position Overview:
We are seeking a highly skilled and detail-oriented Accountant – data analyst to join our dynamic team. The successful candidate will play a crucial role in ensuring accurate and efficient financial operations through monthly intercompany data analysis and settlement, Accounts Payable data analysis and settlement, and forecasting key customer's monthly cash inflow.
Responsibilities:
Monthly Intercompany Data Analysis and Settlement:
· Conduct thorough analysis of global Intercompany financial data to identify discrepancies and ensure accurate reconciliation.
· Collaborate with various departments to obtain necessary data and resolve any outstanding issues.
· Prepare detailed reports summarizing intercompany transactions and settlements.
Accounts Payable Data Analysis and Settlement:
· Analyze Accounts Payable data to ensure accuracy and compliance with financial policies.
· Work closely with vendors and internal teams to resolve discrepancies and streamline the payment process.
· Generate reports on Accounts Payable status and contribute to process improvement initiatives.
Forecast and collection of Key Customer's Monthly Cash Inflow:
· Utilize historical data and market trends to forecast key customer cash inflow accurately.
· Provide insights and recommendations to optimize cash flow management.
Qualifications:
· Must have a Bachelor's degree in Finance, Accounting, Business or other related field
· 1-3 years Accounting experience
· Must have advanced Excel skills
· Background in Data Analysis and Computer Science is highly preferred.
· Proficiency in data analysis tools such as Excel, SQL, Python, and data visualization tools.
· Strong analytical and problem-solving skills with attention to detail.
· Excellent communication and interpersonal skills to collaborate with cross-functional teams.
· Ability to work independently and meet tight deadlines.
Show more
Show less","Accounting, Financial Analysis, Data Analysis, Intercompany Reconciliation, Accounts Payable, Cash Flow Forecasting, Advanced Excel, SQL, Python, Data Visualization, CrossFunctional Collaboration","accounting, financial analysis, data analysis, intercompany reconciliation, accounts payable, cash flow forecasting, advanced excel, sql, python, data visualization, crossfunctional collaboration","accounting, accounts payable, advanced excel, cash flow forecasting, crossfunctional collaboration, dataanalytics, financial analysis, intercompany reconciliation, python, sql, visualization"
Sr. Data Analyst,Ledgent Technology,"Placentia, CA",https://www.linkedin.com/jobs/view/sr-data-analyst-at-ledgent-technology-3779392491,2023-12-17,Costa Mesa,United States,Mid senior,Onsite,"No C2C, No 3rd parties
100% onsite nearby Placentia, CA
Contract-to-hire
Partnered with a client who simplifies shipping logistics for merchants and warehouses. There is plenty of room for growth - play a pivotal role in the growth!
You will work closely with cross-functional teams to analyze complex data sets, identify trends, and help drive business growth.
Most of their data is in excel, they will be moving towards establishing a larger database (SQL) and visualization (PowerBI and Tableau). Someone who has helped spearhead data analytics efforts will be helpful.
The ideal candidate will have a minimum of 5 years of experience in data analysis, advanced MS Excel skills as well as past experience in establishing a larger database. Experience in data visualization skills to drive key data and promote insight to the business side.
Qualifications:
Experience working with a large database and working with SQL
Problem Solvers/Analytical
A level of experience with visualization tools like PowerBI or Tableau
Excel VBA coding ability, Macros for large data sets is huge plus
Must have experience in data analysis of freight, shipping carriers, or similar
Desired Skills and Experience
No C2C, No 3rd parties
100% onsite nearby Placentia, CA
Contract-to-hire
Partnered with a client who simplifies shipping logistics for merchants and warehouses. There is plenty of room for growth - play a pivotal role in the growth!
You will work closely with cross-functional teams to analyze complex data sets, identify trends, and help drive business growth.
Most of their data is in excel, they will be moving towards establishing a larger database (SQL) and visualization (PowerBI and Tableau). Someone who has helped spearhead data analytics efforts will be helpful.
The ideal candidate will have a minimum of 5 years of experience in data analysis, advanced MS Excel skills as well as past experience in establishing a larger database. Experience in data visualization skills to drive key data and promote insight to the business side.
Qualifications:
Experience working with a large database and working with SQL
Problem Solvers/Analytical
A level of experience with visualization tools like PowerBI or Tableau
Excel VBA coding ability, Macros for large data sets is huge plus
Must have experience in data analysis of freight, shipping carriers, or similar
All qualified applicants will receive consideration for employment without regard to
race, color, national origin, age, ancestry, religion, sex, sexual orientation, gender identity, gender expression, marital status, disability, medical condition, genetic information, pregnancy, or military or veteran status.
We consider all qualified applicants, including those with criminal histories, in a manner consistent with state and local laws, including the City of Los Angeles' Fair Chance Initiative for Hiring Ordinance.
Show more
Show less","Data Analysis, MS Excel, SQL, PowerBI, Tableau, Excel VBA, Data Visualization, Freight, Shipping Carriers","data analysis, ms excel, sql, powerbi, tableau, excel vba, data visualization, freight, shipping carriers","dataanalytics, excel vba, freight, ms excel, powerbi, shipping carriers, sql, tableau, visualization"
Staff Software Engineer - Data/ML (Hybrid),Panasonic Avionics Corporation,"Irvine, CA",https://www.linkedin.com/jobs/view/staff-software-engineer-data-ml-hybrid-at-panasonic-avionics-corporation-3745309187,2023-12-17,Costa Mesa,United States,Mid senior,Onsite,"Our new global headquarters is conveniently located in Irvine, CA near John Wayne Airport in the Park Place development. For our onsite and hybrid employees you will be able to enjoy amenities such as access to many restaurants and shops, running trails, a fitness deck, outdoor seating, dry cleaning, car wash, free garage parking, car charging stations, shuttle service for train commuters, outdoor games like bocce, horseshoes, gaming tables, pickle ball, and basketball. For more information on Park Place visit www.parkplaceirvine.com
Who We Are:
Ever wonder who brings the entertainment to your flights? Panasonic Avionics Corporation is #1 in the industry for delivering inflight products such as movies, games, WiFi, and now Bluetooth headphone connectivity!
How exciting would it be to be a part of the innovation that goes into creating technology that delights millions of people in an industry that’s here to stay! With our company’s history spanning over 40 years, you will have stability, career growth opportunities, and will work with the brightest minds in the industry. And we are committed to a diverse and inclusive culture that will help our organization thrive! We seek diversity in many areas such as background, culture, gender, ways of thinking, skills and more.
If you want to learn more about us visit us at www.panasonic.aero
And for a full listing of open job opportunities go to www.panasonic.aero/join-us/
Job Summary:
Analyzes complex business problems and issues using data from internal and external sources to provide insight to decision-makers. Identifies and interprets trends and patterns in datasets to locate influences. Have strong predictive, statistical modeling skills. Constructs forecasts, recommendations and strategic/tactical plans based on business data and market knowledge. Creates specifications for reports and analysis based on business needs and required or available data elements.
Provide consultation to users and lead cross-functional teams to address business issues.
Directly produce datasets and reports for analysis using system reporting tools.
What you will be doing:
Software Development
Create visual stories and business cases from data that helps leadership make quick and effective decisions.
Work closely with teams inside the Global Connectivity’s groups to develop metrics and visualizations that will highlight inefficiencies, transaction volumes, and track process improvement initiatives as well as uncover revenue opportunities.
Identify undeveloped data and extract insights that will help guide and influence effective business decisions.
Partner with business process experts and owners to build BI and leading indicators of operational benchmarks.
Modeling and Applied Statistics
Build out predictive modeling using R and Python that harnesses customer and competitive analysis data to spot emerging trends and areas of opportunity.
Work with various corporate stakeholders and the business intelligence community to understand data integrity and differences across enterprise-wide systems.
Programming
Deliver ad hoc business data analyses using Python, R, and SQL, creating dashboards/reports/insights in multiple visualization tools like Tableau and Power BI
The salary range of $74,000 - $124,000 is just one component of Panasonic’s total package. The final offer amount may vary based on factors including but not limited to individual’s knowledge, skills, experience, and location. In addition, this role may be eligible for discretionary bonuses and incentives.
Base pay offered may vary depending on skills, experience, job-related knowledge and location.
What we are looking for:
Required:
Bachelor’s degree in Finance, Engineering, Analytics, or Information Management is required. (Masters in Analytics preferred).
3+ years equivalent experience in modeling, forecasting and/or data analytics.
Data Science Certifications Preferred.
Knowledge of AWS Cloud environments and tools for Data Science, AI and ML.
Ability to take ambiguous sets of data and decipher meaning to tell a story or business case that drives business decisions is required. Should be strong in AWS suite.
Strong ability to communicate both written and verbally to internal and external clients.
Ability to work independently with business users and cross functional leaders.
Strong ability to create and deliver effective presentations to multiple audiences.
Knowledge of data analytic software such as R, Python, SQL is essential.
Knowledge of Microsoft productivity tools: Excel, Word, and Visio and other software is necessary.
Ability to work in a collaborative environment.
Knowledge of Extract Transform Load (ETL), Data Architecture preferred.
Our Principles:
Contribution to Society | Fairness & Honesty | Cooperation & Team Spirit | Untiring Effort for Improvement | Courtesy & Humility | Adaptability | Gratitude
What we offer:
At Panasonic Avionics Corporation we realize the most important aspects in leading our industry are the bright minds behind everything we do. We are proud to offer our employees a highly competitive, comprehensive and flexible benefits program.
Paid time off: Exempt Salaried employees receive unlimited PTO. This means that there is no fixed number, range, or limit to the amount of Personal and Vacation Days that may be taken for exempt employees. Non-exempt hourly employees accrue 14 vacation days per year + 7 sick days + 3 personal days. Accrual rate increases with tenure. All employees receive 11 company paid holidays per year plus a paid company-wide shut down in the U.S. between Christmas and New Year.
Insurance: Medical insurance offerings from Aetna and Kaiser (CA &HI). Options for Employee Only, Employee + Spouse/Domestic Partner, Employee + Children, or Family. Dental PPO and DMO options & Vision insurance through EyeMed or VSP.
401K with 50% match on up to 8% contribution, full vested from day 1
Other offerings include: Wellness Program, Counseling services, FSA & HSA, Life Insurance for employee, spouse and child, AD&D Insurance, Long-term and Short-term disability, Critical Illness Insurance, Accident Insurance, Legal Assistance, Pet Insurance, Identity Theft Protection, Dependent Care FLSA, Education Assistance, Commuter Program, Employee Purchase Program, Service Award Program.
All applicants are subject to Company policies, third party customer and worksite requirements, and government requirements, regarding vaccination and/or testing for COVID-19. Where permitted by applicable law, applicants may be required to be fully vaccinated with an authorized COVID-19 vaccine as a condition of employment, unless they are eligible for and obtain an exemption based on a reasonable accommodation because of a disability or a sincerely held religious belief, practice, or observance. While the Company strongly encourages COVID-19 vaccinations, it may require vaccination and/or testing for positions in which third party customer, worksite, or government requirements apply, in accordance with applicable law. At those locations where requirements apply, exemptions will be considered based on applicable law.
Panasonic is proud to be an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, national origin, disability status, protected veteran status, and any other characteristic protected by law or company policy. All qualified individuals are required to perform the essential functions of the job with or without reasonable accommodation. Pre-employment drug testing is required for safety sensitive positions or as may otherwise be required by contract or law. Due to the high volume of responses, we will only be able to respond to candidates of interest. All candidates must have valid authorization to work in the U.S. Thank you for your interest in Panasonic Avionics Corporation.
Show more
Show less","Data analytics, Modeling, Forecasting, Statistical modeling, R, Python, SQL, Tableau, Power BI, AWS, Data Science, AI, ML, ETL, Data Architecture, Microsoft Office Suite, Excel, Word, Visio","data analytics, modeling, forecasting, statistical modeling, r, python, sql, tableau, power bi, aws, data science, ai, ml, etl, data architecture, microsoft office suite, excel, word, visio","ai, aws, data architecture, data science, dataanalytics, etl, excel, forecasting, microsoft office suite, ml, modeling, powerbi, python, r, sql, statistical modeling, tableau, visio, word"
Sr. Staff Software Engineer - Data/ML (Hybrid),Panasonic Avionics Corporation,"Irvine, CA",https://www.linkedin.com/jobs/view/sr-staff-software-engineer-data-ml-hybrid-at-panasonic-avionics-corporation-3763146844,2023-12-17,Costa Mesa,United States,Mid senior,Onsite,"Our new global headquarters is conveniently located in Irvine, CA near John Wayne Airport in the Park Place development. For our onsite and hybrid employees you will be able to enjoy amenities such as access to many restaurants and shops, running trails, a fitness deck, outdoor seating, dry cleaning, car wash, free garage parking, car charging stations, shuttle service for train commuters, outdoor games like bocce, horseshoes, gaming tables, pickle ball, and basketball. For more information on Park Place visit www.parkplaceirvine.com
Who We Are:
Ever wonder who brings the entertainment to your flights? Panasonic Avionics Corporation is #1 in the industry for delivering inflight products such as movies, games, WiFi, and now Bluetooth headphone connectivity!
How exciting would it be to be a part of the innovation that goes into creating technology that delights millions of people in an industry that’s here to stay! With our company’s history spanning over 40 years, you will have stability, career growth opportunities, and will work with the brightest minds in the industry. And we are committed to a diverse and inclusive culture that will help our organization thrive! We seek diversity in many areas such as background, culture, gender, ways of thinking, skills and more.
If you want to learn more about us visit us at www.panasonic.aero
And for a full listing of open job opportunities go to www.panasonic.aero/join-us/
Job Summary:
As a Senior Software Engineer, you will collaborate closely with the data engineering team and work on developing and maintaining data pipelines, data processing applications, and data integration solutions. Your expertise in software development, data engineering, machine learning, and inferencing will be instrumental in enhancing our data infrastructure.
What you will be doing:
Software Development: Design, develop, and maintain data processing applications and data pipelines, with a focus on efficiency, scalability, and reliability.
Collaboration: Work closely with the data engineering team and cross-functional teams to integrate data from various sources, ensuring data quality, consistency, and accuracy.
Real-Time Data Processing: Develop real-time data processing components to support our data analytics and reporting needs.
Machine Learning Integration: Incorporate machine learning models into data pipelines and applications for predictive analytics and decision support.
Inferencing: Implement inferencing engines and optimize the execution of machine learning models for real-time and batch processing.
Technology Stack: Utilize a variety of data engineering technologies, frameworks, and cloud-based platforms to build robust data and machine learning solutions.
Code Quality: Maintain high coding standards, implement best practices, and participate in code reviews to ensure data engineering and machine learning quality.
Documentation: Create and maintain comprehensive documentation for data processes, applications, machine learning models, and inferencing.
Problem-Solving: Identify and troubleshoot data-related and machine learning issues, proposing effective solutions and optimizations.
Self-Service Data Analytics: Collaborate with the data engineering team to enable self-service data analytics for business users.
Data Governance: Contribute to the implementation of data governance and data management best practices.
The salary range of $128,000 - $215,000 is just one component of Panasonic’s total package. The final offer amount may vary based on factors including but not limited to individual’s knowledge, skills, experience, and location. In addition, this role may be eligible for discretionary bonuses and incentives.
Base pay offered may vary depending on skills, experience, job-related knowledge and location.
What we are looking for:
Required:
Minimum 12 years of professional experience as a data software engineer
Bachelor's or master’s degree in computer science or a related field.
Proven experience as a software engineer with a strong focus on data engineering and machine learning.
Strong programming skills in languages such as Python, Java, or Scala.
Experience in data engineering technologies, ETL processes, and data integration.
Familiarity with real-time data processing, data streaming frameworks, and machine learning libraries.
Proficiency in cloud-based data platforms (e.g., AWS, Azure, GCP).
Excellent problem-solving skills and attention to detail.
Ability to collaborate effectively with cross-functional teams.
Strong commitment to code quality, documentation, machine learning best practices, and data governance.
Our Principles:
Contribution to Society | Fairness & Honesty | Cooperation & Team Spirit | Untiring Effort for Improvement | Courtesy & Humility | Adaptability | Gratitude
What we offer:
At Panasonic Avionics Corporation we realize the most important aspects in leading our industry are the bright minds behind everything we do. We are proud to offer our employees a highly competitive, comprehensive and flexible benefits program.
Paid time off: Exempt Salaried employees receive unlimited PTO. This means that there is no fixed number, range, or limit to the amount of Personal and Vacation Days that may be taken for exempt employees. Non-exempt hourly employees accrue 14 vacation days per year + 7 sick days + 3 personal days. Accrual rate increases with tenure. All employees receive 11 company paid holidays per year plus a paid company-wide shut down in the U.S. between Christmas and New Year.
Insurance: Medical insurance offerings from Aetna and Kaiser (CA &HI). Options for Employee Only, Employee + Spouse/Domestic Partner, Employee + Children, or Family. Dental PPO and DMO options & Vision insurance through EyeMed or VSP.
401K with 50% match on up to 8% contribution, full vested from day 1
Other offerings include: Wellness Program, Counseling services, FSA & HSA, Life Insurance for employee, spouse and child, AD&D Insurance, Long-term and Short-term disability, Critical Illness Insurance, Accident Insurance, Legal Assistance, Pet Insurance, Identity Theft Protection, Dependent Care FLSA, Education Assistance, Commuter Program, Employee Purchase Program, Service Award Program.
All applicants are subject to Company policies, third party customer and worksite requirements, and government requirements, regarding vaccination and/or testing for COVID-19. Where permitted by applicable law, applicants may be required to be fully vaccinated with an authorized COVID-19 vaccine as a condition of employment, unless they are eligible for and obtain an exemption based on a reasonable accommodation because of a disability or a sincerely held religious belief, practice, or observance. While the Company strongly encourages COVID-19 vaccinations, it may require vaccination and/or testing for positions in which third party customer, worksite, or government requirements apply, in accordance with applicable law. At those locations where requirements apply, exemptions will be considered based on applicable law.
Panasonic is proud to be an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, national origin, disability status, protected veteran status, and any other characteristic protected by law or company policy. All qualified individuals are required to perform the essential functions of the job with or without reasonable accommodation. Pre-employment drug testing is required for safety sensitive positions or as may otherwise be required by contract or law. Due to the high volume of responses, we will only be able to respond to candidates of interest. All candidates must have valid authorization to work in the U.S. Thank you for your interest in Panasonic Avionics Corporation.
Show more
Show less","Python, Java, Scala, AWS, Azure, GCP, Software engineering, Data engineering, Machine learning, Inferencing, Data processing, Data integration, Realtime data processing, Data streaming, Data pipelines, ETL, Cloudbased platforms, Coding standards, Code reviews, Data governance, Problemsolving, Collaboration, Documentation","python, java, scala, aws, azure, gcp, software engineering, data engineering, machine learning, inferencing, data processing, data integration, realtime data processing, data streaming, data pipelines, etl, cloudbased platforms, coding standards, code reviews, data governance, problemsolving, collaboration, documentation","aws, azure, cloudbased platforms, code reviews, coding standards, collaboration, data engineering, data governance, data integration, data processing, data streaming, datapipeline, documentation, etl, gcp, inferencing, java, machine learning, problemsolving, python, realtime data processing, scala, software engineering"
Data Engineer III - Network,Crown Castle,"Irvine, CA",https://www.linkedin.com/jobs/view/data-engineer-iii-network-at-crown-castle-3772653817,2023-12-17,Costa Mesa,United States,Mid senior,Remote,"Position Title:
Data Engineer III, Network (P3)
Company Summary
Crown Castle is the nation’s largest provider of shared communications infrastructure: towers, small cells and fiber. Whenever you make a call, track a workout or stream music and videos, we’re the ones providing the communications infrastructure that makes it possible to transform the way we live and work. From 5G and the internet of things to drones, autonomous vehicles and AR/VR, we enable the technologies that help people stay safe, connected and ready for the future. Crown Castle is publicly traded on the S&P 500, and one of the largest Real Estate Investment Trusts in the US.
Role
The Network Automation and Platforms (NAP) team provides innovative solutions using open-source and commercial technologies on-premises and in the cloud, to deliver critical data, geo-spatial, and automation capabilities to our customers. You will work in a highly collaborative team consisting of software architects, developers, database admins, network planners, and network engineers. As a Data Engineer III, you will interpret Network Engineering data requirements, procure datasets, and build reporting & analytical capabilities that will enable data-driven decisions leading to increased revenue and greater efficiencies for the network business. You will also assist in setting the data engineering strategy and implementation roadmap necessary to achieve our vision of a next generation, end-to-end autonomous network.
Position Summary
The Data Engineer III is responsible for automating the ingestion, transformation, and integration of data between applications and managing, advancing, and delivering business intelligence solutions based on that data to meet the business objectives of the organization's Network Engineering teams with minimal supervision.
Essential Job Functions
Meet with subject matter experts in network engineering areas of the business, be able to quickly learn concepts and language relevant to their departments and develop an understanding of how their data supports their business processes.
Develop and maintain efficient enterprise-grade dashboards, reports and datasets adhering to industry best practices.
Review, interpret, troubleshoot, and optimize complex SQL queries. Develop maintainable code via proper structures, comments, design using best practices.
Provide database development and data integration support, developing database application code and data integration layers using database programming languages, ETL tools, APIs, and scripting languages.
Provide feedback as needed to internal and external resources that provide source data.
Continuously demonstrate proficiency in the use of business intelligence technologies.
Perform source data analysis – data profiling, validation, conceptual and logical data modeling, etc. – to determine the suitability of the source data for meeting the reporting requirements.
Research and understand business objectives and provide guidance, options, and proof of concepts (if appropriate) to solve business needs.
Work with third-party software vendors to manage both support and feature requests to improve the end-user experience.
Participate in weekly standups and provide code reviews for team projects.
Well organized and capable of executing on multiple high-level projects at the same time.
Adheres to project methodology, change management, and departmental procedures.
Develop documentation including but not limited to flowcharts and entity-relationship diagrams, as well as requirements and solutions.
Exceptional ability to interact in a team environment with peers and members of other teams that might be located remotely.
Education/Certifications
BS Degree in Engineering, Computer Science, or related technical discipline.
Code-camp with a portfolio and additional years of experience may be considered in lieu of a degree (put your GitHub, etc. link in your resume)
Experience/Minimum Requirements
5+ years of experience with SQL database technologies such as PostgresSQL, Oracle, SQL Server, and MySQL including developing database-specific SQL queries, data modeling, migrations, and integration strategies.
3+ years of design experience with BI reporting technologies in the creation of data-rich dashboards, such as Grafana and/or Power BI.
Proficiency with operational database and data warehouse design, ETL development, scalable data pipeline design, API integration and automation.
Proficiency with scripting languages such as Python (preferred), Ruby, and Go.
Experience with software development best practices, including coding standards, code reviews, source control management, automated build processes, testing, and operations.
Experience with modern ETL pipelines and schedulers a plus (e.g., Airflow, etc.)
Reports to:
Principal Architect
We offer a total benefits package and professional growth development for teammates in any stage of their career. Along with caring for our teammates, we’re an active member in the communities where we live, work and do business. We have a responsibility to give back, which we do through our Connected by Good program. Giving back allows us to improve public spaces where people connect, promote public safety and advance access to education and technology
For New York City, Colorado, California, and Washington state residents:
The hiring range offered for this position is $101,800 - $146,400 annually. In addition to salary, employees are eligible for an annual bonus of up to 15% of annual salary and restricted stock. Employees (and their families) are eligible for medical, dental, vision, and basic life insurance. Employees are able to enroll in our company’s 401k plan. Employees will also receive 18 days of paid time off each year and 12 paid holidays throughout the calendar year.
Show more
Show less","SQL, PostgresSQL, Oracle, SQL Server, MySQL, Grafana, Power BI, Python, Ruby, Go, Airflow, ETL, Data Warehouse Design, Data Pipeline Design, API Integration, Automation, Coding Standards, Code Reviews, Source Control Management, Automated Builds, Testing, Operations, Network Engineering, Data Integration, Data Modeling, Data Analysis, Data Profiling, Data Validation, Conceptual Data Modeling, Logical Data Modeling, Business Intelligence, Dashboards, Reports, Datasets, Flowcharts, EntityRelationship Diagrams, Requirements, Solutions, Documentation","sql, postgressql, oracle, sql server, mysql, grafana, power bi, python, ruby, go, airflow, etl, data warehouse design, data pipeline design, api integration, automation, coding standards, code reviews, source control management, automated builds, testing, operations, network engineering, data integration, data modeling, data analysis, data profiling, data validation, conceptual data modeling, logical data modeling, business intelligence, dashboards, reports, datasets, flowcharts, entityrelationship diagrams, requirements, solutions, documentation","airflow, api integration, automated builds, automation, business intelligence, code reviews, coding standards, conceptual data modeling, dashboard, data integration, data pipeline design, data profiling, data validation, data warehouse design, dataanalytics, datamodeling, datasets, documentation, entityrelationship diagrams, etl, flowcharts, go, grafana, logical data modeling, mysql, network engineering, operations, oracle, postgressql, powerbi, python, reports, requirements, ruby, solutions, source control management, sql, sql server, testing"
Data Engineer,CINQCARE,"New York, NY",https://www.linkedin.com/jobs/view/data-engineer-at-cinqcare-3787790461,2023-12-17,Victoria, Canada,Mid senior,Onsite,"Overview
The Data Engineer is a critical member of our growing data science team. In this role, you will have the chance to define and develop a core data asset which provides a representative and culturally aware view of the individuals and the communities that CINQCARE serves. You will work to evolve this asset over time using a product roadmap that includes identification and closure of gaps in existing data, introduction of new data sources, generation of proprietary data while quantifying and eliminating areas of structural biases. CINQCARE seeks to fix gaps that have persisted for generations in the delivery of care to Black and Brown populations and to so, we must also seek to fix gaps in data that ignore and marginalizes the Black and Brown communities that we serve.
An ideal candidate for this role will embody CINQCARE’s core values, including,
Trusted
, Empathetic, Committed, Humble, Creative and Community-Minded
. At CINQCARE, we don’t have patients or customers –
we have Family Members.
Job Responsibilities
The Data Engineer will have the following responsibilities:
The Data Engineer will be responsible for the design, development and delivery of data pipelines and value-added data assets, leveraging a variety of data warehousing methodologies and disciplines to ingest the data from heterogeneous sources into cloud-based Data-Lake Environment in AWS.
Manage initiatives & projects of significant complexity and risk. Excellent business and communication skills to be able to work with business owners providing input to prioritized roadmaps, develop work estimates, and ensure successful delivery to support strategic planning and initiatives, improve organizational performance, and advance progress towards CINQCARE’s goals.
Assist in the overall architecture of the ETL Design and proactively provide inputs in designing, implementing, and automating the data pipelines.
Investigate and mine data to identify potential issues within the data pipelines, notify end-users and propose adequate solutions.
Ensure data quality and integrity within the data lake in the AWS environment with a focus on compliance to HIPAA and state level compliance requirements.
Oversee user permissions and configurations for adherence to documented access management standards and policies.
Independently (with minimal oversight) develop and maintain trusted advisor relationships with business, clinical, and operations leaders at the senior leadership level and with external partners, that include guidance for optimizing use of analytic capabilities and deliverables, and prioritization based on strategic vision.
Use coding/scripting pipelines and APIs to uncover and turn data into assets that are analysis-friendly using AWS services like Athena Data Catalogue, Quick Sight or any other big data tool on AWS.
Create a high-quality catalog of all pertinent data with the primary goal of establishing a single source of truth and significantly increasing productivity by reducing the time required for data search and discovery.
Crossing team boundaries, educate/advise on data projects, on how to combine and aggregate client data across platforms or technologies, and how to make the greatest use of data.
Lead consistent adherence to the Software Development Life Cycle framework and governance processes including, but not limited to leading planning sessions, collecting, and documenting requirements, identifying design patterns, create and define custom transformations, aggregations, and other data manipulations, developing data pipelines, creating documentation, developing test plans, performing unit testing, conducting peer review sessions.
Use knowledge of healthcare industry, market environment, and clinical and business workflows and activities, to inform solution design and development to execute high-quality or differentiated solutions in an established problem space.
Perform other job-related duties as assigned.
General Duties
The Data Engineer should have the following duties:
Leadership: The Data Engineer will lead the continued build out of the data asset to create business value, including collaborating with their team to design, develop, and execute those strategies and solutions to deliver desired outcomes.
Strategy: The Data Engineer will contribute to the business strategy and roadmap: (1) improve outcomes for CINQCARE Family Members; (2) enhance the efficacy of other CINQCARE. business divisions; and (3) develop and deliver external market opportunities for CINQCARE products and services. In establishing the business strategy, the Data Engineer will define and innovate sustainable revenue models to drive profitability of the Company.
Collaboration: The Data Engineer will ensure that AI capabilities form a cohesive offering, including by working closely with other business divisions to learn their needs, internalize their knowledge, and define solutions to achieve the business objectives of CINQCARE.
Knowledge: The Data Engineer will provide subject matter expertise in the AI solutions, including determining and recommended approaches for designing and building elegant data structures in support of existing reporting tools and custom visualization platforms.
Culture: The Data Engineer is accountable for creating a productive, collaborative, safe and inclusive work environment for their team and as part of the larger Company.
Qualifications
The Data Engineer should have the following qualifications:
Education: Bachelor’s degree in Computer Science, Engineering, Software Engineering, or related field; Master’s degree preferred.
Experience: The ideal candidate should have at least 3+ years of experience in healthcare data engineering. Experience with a variety of data projects and environments, whether on-prem or in-cloud (5+ years in SQL Server, ETL Tools, Business Intelligence & Analysis, Architecture). Familiarity with the Microsoft Stack; experience with other platforms is a plus. Mastery of Python and SQL. Strong foundational knowledge of data lakes and AWS products such as AWS Glue. Experience with healthcare eligibility and claims, implementing APIs, HL7/FHIR standards, ETL scheduling solutions, SQL, and healthcare data security.
Entrepreneurial: CINQCARE seeks to fix gaps that have persisted for generations in the delivery of care to Black and Brown populations. This position is accountable for ensuring CINQCARE is positioned to innovatively deliver on its promise.
Communication: Strong analytical and collaboration skills is required. Excellent verbal, written communication and presentation skills; ability to clearly articulate and present concepts and models in an accessible manner to CINQCARE’s team, investors, partners, and other stakeholders.
Relationships: Ability to build and effectively manage relationships with business leaders and external constituents.
Culture. Good judgement, impeccable ethics, and a strong team player; desire to succeed and grow in a fast-paced, demanding, and entrepreneurial Company.
Location
: New York, NY
Compensation
: $100,000-$120,000
Powered by JazzHR
c0Bz9fI6vE
Show more
Show less","Data Warehousing, ETL, Cloud Computing, AWS, AWS Glue, Data Pipelines, Data Quality, Data Security, Python, SQL, Data lakes, Big Data, Data Catalog, Data Discovery, Data Manipulation, Software Development Life Cycle, Healthcare Data Engineering, Eligibility and Claims, HL7/FHIR, APIs, Data Integration, Business Intelligence, Data Analytics, Data Visualization, Presentation Skills, Communication Skills, Collaboration Skills, Leadership Skills, Problem Solving","data warehousing, etl, cloud computing, aws, aws glue, data pipelines, data quality, data security, python, sql, data lakes, big data, data catalog, data discovery, data manipulation, software development life cycle, healthcare data engineering, eligibility and claims, hl7fhir, apis, data integration, business intelligence, data analytics, data visualization, presentation skills, communication skills, collaboration skills, leadership skills, problem solving","apis, aws, aws glue, big data, business intelligence, cloud computing, collaboration skills, communication skills, data catalog, data discovery, data integration, data lakes, data manipulation, data quality, data security, dataanalytics, datapipeline, datawarehouse, eligibility and claims, etl, healthcare data engineering, hl7fhir, leadership skills, presentation skills, problem solving, python, software development life cycle, sql, visualization"
Big Data Engineer,"Conch Technologies, Inc","Phoenix, AZ",https://www.linkedin.com/jobs/view/big-data-engineer-at-conch-technologies-inc-3747469526,2023-12-17,Victoria, Canada,Mid senior,Onsite,"Hi,
Hope you're doing great,
Greetings from Conch Technologies Inc
Job Description
Position: BigData Engineer ( 7 position open )
Location: Phoenix, AZ ( Onsite Daya 1 )
Duration: 12+ Months Contract
Big Data engineers
Minimum Qualifications:
Bachelor's degree in Engineering or Computer Science or equivalent OR Master's in Computer Applications or equivalent.
10+ years of software development experience and leading teams of engineers and scrum teams 5+ years of hands-on experience of working with Map-Reduce, Hive, Spark (core, SQL, and PySpark) Solid Data warehousing concepts
Knowledge of the Financial reporting ecosystem will be a plus .5+ years of experience within Data Engineering/ Data Warehousing using Big Data technologies will be an add-on Expert on Distributed ecosystems Hands-on experience with programming using Core Java or Python/Scala.
Expert on Hadoop and Spark Architecture and its working principle Hands-on experience on writing and understanding complex SQL(Hive/Py Spark-dataframes), optimizing joins while processing huge amounts of data.
Experience In UNIX Shell Scripting.
Ability to design and develop optimized Data pipelines for batch and real time data processing Should have experience in analysis, design, development, testing, and implementation of system applications Demonstrated ability to develop and document technical and functional specifications and analyze software and system processing flows.
Preferred Qualifications: Knowledge of cloud platforms like
GCP/AWS, building Microservices and scalable solutions, will be an advantage 1 + years of experience in designing and building solutions using Kafka streams or queues Experience with GitHub/Bitbucket and leveraging CI/CD pipelines .Experience with NoSQL i.e., HBase, Couchbase, MongoDB is good to have Excellent technical and analytical aptitude Good communication skills .Excellent Project management skills. Results driven
--
With Regards,
Nagesh G
Mobile:
408-381-5645
Desk:
901-313-3066
Email: nagesh@conchtech.com
Web:
www.conchtech.com
Show more
Show less","BigData, Java, Hadoop, Spark, SQL, Python, Scala, Hive, Pig, MapReduce, UNIX, NoSQL, HBase, Couchbase, MongoDB, AWS, GCP, Kafka, CI/CD, Microservices, Data lakes, Data warehousing, Agile, Scrum","bigdata, java, hadoop, spark, sql, python, scala, hive, pig, mapreduce, unix, nosql, hbase, couchbase, mongodb, aws, gcp, kafka, cicd, microservices, data lakes, data warehousing, agile, scrum","agile, aws, bigdata, cicd, couchbase, data lakes, datawarehouse, gcp, hadoop, hbase, hive, java, kafka, mapreduce, microservices, mongodb, nosql, pig, python, scala, scrum, spark, sql, unix"
Sr. Data Engineer,Experfy,"New York, NY",https://www.linkedin.com/jobs/view/sr-data-engineer-at-experfy-3590330243,2023-12-17,Victoria, Canada,Mid senior,Onsite,"A Sr. Data Engineer is proficient in the development of all aspects of data processing including data warehouse architecture/modeling and ETL processing. The position focuses research on development and delivery of analytical solutions using various tools including Confluent Kafka, Kinesis, Glue, Lambda, Snowflake and SQL Server. A Sr. Data Engineer must be able to work autonomously with little guidance or instruction to deliver business value.
Responsibilities
Position Responsibilities
Partner with business stakeholders to gather requirements and translate them into technical specifications and process documentation for IT counterparts (on-prem and offshore)
Highly proficient in the architecture and development of an event driven data warehouse; streaming, batch, data modeling, and storage
Advanced database knowledge; creating/optimizing SQL queries, stored procedures, functions, partitioning data, indexing, and reading execution plans
Skilled experience in writing and troubleshooting Python/PySpark scripts to generate extracts, cleanse, conform and deliver data for consumption
Expert level of understanding and implementing ETL architecture; data profiling, process flow, metric logging and error handling
Support continuous improvement by investigating and presenting alternatives to processes and technologies to an architectural review board
Develop and ensure adherence to published system architectural decisions and development standards
Multi-task across several ongoing projects and daily duties of varying priorities as required
Interact with global technical teams to communicate business requirements and collaboratively build data solutions
Requirements
Requirements
8+ years of development experience
Expert level in data warehouse design/architecture, dimensional data modeling and ETL process development
Advanced level development in SQL/NoSQL scripting and complex stored procedures (Snowflake, SQL Server, DynomoDB, NEO4J a plus)
Extremely proficient in Python, PySpark, and Java
AWS Expertise - Kinesis, Glue (Spark), EMR, S3, Lambda, and Athena
Streaming Services - Confluent Kafka and Kinesis (or equivalent)
Hands on experience in designing and developing applications using Java Spring Framework (Spring Boot, Spring Cloud, Spring Data etc)
Apply for this job
Show more
Show less","Data warehouse architecture/modeling, ETL processing, Confluent Kafka, Kinesis, Glue, Lambda, Snowflake, SQL Server, SQL, Stored procedures, Functions, Partitioning data, Indexing, Python, PySpark, Data profiling, Process flow, Metric logging, Error handling, System architectural decisions, Development standards, AWS Expertise, EMR, S3, Athena, Streaming Services, Java Spring Framework, Spring Boot, Spring Cloud, Spring Data","data warehouse architecturemodeling, etl processing, confluent kafka, kinesis, glue, lambda, snowflake, sql server, sql, stored procedures, functions, partitioning data, indexing, python, pyspark, data profiling, process flow, metric logging, error handling, system architectural decisions, development standards, aws expertise, emr, s3, athena, streaming services, java spring framework, spring boot, spring cloud, spring data","athena, aws expertise, confluent kafka, data profiling, data warehouse architecturemodeling, development standards, emr, error handling, etl processing, functions, glue, indexing, java spring framework, kinesis, lambda, metric logging, partitioning data, process flow, python, s3, snowflake, spark, spring boot, spring cloud, spring data, sql, sql server, stored procedures, streaming services, system architectural decisions"
Digital Data Engineer,Experfy,"Bridgewater Township, NJ",https://www.linkedin.com/jobs/view/digital-data-engineer-at-experfy-3590305288,2023-12-17,Victoria, Canada,Mid senior,Onsite,"Our client is looking for:
Work with business teams to understand requirements, and translate them into technical needs
Gather/organize large & complex data assets, and perform relevant analysis
Ensure the quality of the data in coordination with Data Analysts and Data Scientists (peer validation)
Propose and implement relevant data models for each business case
Create data models and optimize queries performance
Communicate results and findings in a structured way
Partner with Product Owner and Data Analysts to prioritize the pipeline implementation plan
Partner with Data Analysts and Data scientists to design pipelines relevant for business requirements
Leverage existing or create new ""standard data pipelines"" within the company to bring value through business use cases
Ensure best practices in data manipulation are enforced end-to-end
Actively contribute to Data governance community
Remains up to date on company's standards, industry practices and emerging technologies
Requirements
Key Functional Requirements & Qualifications:
Experience working with a variety of cross-functional teams
Good understanding of agile/scrum development processes and concepts
Able to work in a fast-paced, constantly evolving environment and manage multiple priorities
Pragmatic and capable of solving complex issues
Service-oriented, flexible team player
Attention to detail & technical intuition
Excellent written, verbal, and interpersonal skills for executive level communication and collaboration
Fluent in English (Other languages a plus)
Key Technical Requirements & Qualifications:
Bachelor's Degree or equivalent in in Computer Science, Engineering, or relevant field
Experience with AWS cloud services (Azure & GCP a plus)
Good knowledge of SQL and relational databases technologies/concepts
Experience working with data models and query tuning
Experience in Data warehousing solutions (Snowflake a plus)
Experience in Integration Services (IICS, Tibco a plus)
Working knowledge of scripting languages (Python, R a plus)
Familiarity with Source Code Management Tools (GitHub a plus)
Familiarity with Visualization Tools (PowerBI, Tableau a plus)
Familiarity with Project Management Tools (JIRA, Confluence a plus)
Familiarity with Service Management Tools (Service Now a plus)
Experience working in life sciences/pharmaceutical industry is a plus
Relevant cloud certifications (AWS, Azure, Snowflake, IICS) are a plus
Experience on working within compliance (e.g.: quality, regulatory - data privacy, GxP, SOX) and cybersecurity requirements is a plus
Mentoring and/or technology evangelism/advocacy experience
Additions For Data Engineer - Database
Strong experience in automation tools and methodologies specifically using Gitlab, Github action, Terraform, Ansible
Experience with programming languages such as Python, JSON, YAML, Shell Scripting
Experience with backup system like Netbackup & CommVault
Good knowledge of ServiceNow and monitoring tool such as Splunk, BPPM
Additions for Data Engineer - RWE
Experience with Real World Data (e,g, EHR, Claims) and standard data models (e,g, OMOP, FHIR)
Experience using frameworks to create pipelines (e.g. Apache Airflow, Kedro)
Show more
Show less","Data Analysis, Data Modeling, Data Warehousing, SQL, Python, R, AWS, Azure, GCP, Snowflake, IICS, Tibco, GitHub, PowerBI, Tableau, JIRA, Confluence, Service Now, Gitlab, Github action, Terraform, Ansible, JSON, YAML, Shell Scripting, Netbackup, CommVault, Splunk, BPPM, Real World Data, Apache Airflow, Kedro, OMOP, FHIR","data analysis, data modeling, data warehousing, sql, python, r, aws, azure, gcp, snowflake, iics, tibco, github, powerbi, tableau, jira, confluence, service now, gitlab, github action, terraform, ansible, json, yaml, shell scripting, netbackup, commvault, splunk, bppm, real world data, apache airflow, kedro, omop, fhir","ansible, apache airflow, aws, azure, bppm, commvault, confluence, dataanalytics, datamodeling, datawarehouse, fhir, gcp, github, github action, gitlab, iics, jira, json, kedro, netbackup, omop, powerbi, python, r, real world data, service now, shell scripting, snowflake, splunk, sql, tableau, terraform, tibco, yaml"
Sr. Data Engineer,Experfy,"Boston, MA",https://www.linkedin.com/jobs/view/sr-data-engineer-at-experfy-3590325743,2023-12-17,Victoria, Canada,Mid senior,Onsite,"A Sr. Data Engineer is proficient in the development of all aspects of data processing including data warehouse architecture/modeling and ETL processing. The position focuses research on development and delivery of analytical solutions using various tools including Confluent Kafka, Kinesis, Glue, Lambda, Snowflake and SQL Server. A Sr. Data Engineer must be able to work autonomously with little guidance or instruction to deliver business value.
Responsibilities
Position Responsibilities
Partner with business stakeholders to gather requirements and translate them into technical specifications and process documentation for IT counterparts (on-prem and offshore)
Highly proficient in the architecture and development of an event driven data warehouse; streaming, batch, data modeling, and storage
Advanced database knowledge; creating/optimizing SQL queries, stored procedures, functions, partitioning data, indexing, and reading execution plans
Skilled experience in writing and troubleshooting Python/PySpark scripts to generate extracts, cleanse, conform and deliver data for consumption
Expert level of understanding and implementing ETL architecture; data profiling, process flow, metric logging and error handling
Support continuous improvement by investigating and presenting alternatives to processes and technologies to an architectural review board
Develop and ensure adherence to published system architectural decisions and development standards
Multi-task across several ongoing projects and daily duties of varying priorities as required
Interact with global technical teams to communicate business requirements and collaboratively build data solutions
Requirements
8+ years of development experience
Expert level in data warehouse design/architecture, dimensional data modeling and ETL process development
Advanced level development in SQL/NoSQL scripting and complex stored procedures (Snowflake, SQL Server, DynomoDB, NEO4J a plus)
Extremely proficient in Python, PySpark, and Java
AWS Expertise - Kinesis, Glue (Spark), EMR, S3, Lambda, and Athena
Streaming Services - Confluent Kafka and Kinesis (or equivalent)
Hands on experience in designing and developing applications using Java Spring Framework (Spring Boot, Spring Cloud, Spring Data etc)
Apply for this job
Show more
Show less","Data Processing, Data Warehouse Architecture, ETL Processing, Confluent Kafka, Kinesis, Glue, Lambda, Snowflake, SQL Server, Python, PySpark, SQL, Data Modeling, Stored Procedures, Functions, Partitioning, Indexing, Data Profiling, Process Flow, Metric Logging, Error Handling, ETL Architecture, Dimensional Data Modeling, SQL/NoSQL Scripting, Java, AWS, EMR, S3, Athena, Streaming Services, Java Spring Framework, Spring Boot, Spring Cloud, Spring Data, Spring Batch","data processing, data warehouse architecture, etl processing, confluent kafka, kinesis, glue, lambda, snowflake, sql server, python, pyspark, sql, data modeling, stored procedures, functions, partitioning, indexing, data profiling, process flow, metric logging, error handling, etl architecture, dimensional data modeling, sqlnosql scripting, java, aws, emr, s3, athena, streaming services, java spring framework, spring boot, spring cloud, spring data, spring batch","athena, aws, confluent kafka, data processing, data profiling, data warehouse architecture, datamodeling, dimensional data modeling, emr, error handling, etl architecture, etl processing, functions, glue, indexing, java, java spring framework, kinesis, lambda, metric logging, partitioning, process flow, python, s3, snowflake, spark, spring batch, spring boot, spring cloud, spring data, sql, sql server, sqlnosql scripting, stored procedures, streaming services"
Database Engineer - Bellevue,Resulticks,"Bellevue, WA",https://www.linkedin.com/jobs/view/database-engineer-bellevue-at-resulticks-3787754039,2023-12-17,Victoria, Canada,Mid senior,Onsite,"JOB DESCRIPTION - Database Engineer
Title:
Database Engineer
Job Function
:
Customer Success
Role Type:
Full time
Location:
Bellevue, WA
Reporting to:
Database Architect
Stakeholders:
Database Architect / Customer Success Teams
About Resulticks
Resulticks is a fully integrated, real-time marketing automation platform designed to help brands worldwide reach, acquire, and retain satisﬁed customers. Built from the ground up by experts in marketing, technology, and business management, Resulticks enables brands to make a transformational leap to conversion-driven, growth-focused personalized engagement. With an advanced CDP at its core. Resulticks offers AI-powered omnichannel orchestration, complete analytics, next-best engagement, attribution at the segment-of-one level, and the world’s first marketing block chain.
Resulticks has been named to the Gartner’s Magic Quadrant 2022 for Multichannel Marketing Hubs for six years in a row and has been awarded with the Microsoft “Elevating Customer Experience with AI” award.
Headquartered in Singapore and New York City, Resulticks is one of the top global martech solutions servicing both B2B/B2B2C and B2C segments. Resulticks’ global presence includes the United States, India, Singapore, Indonesia, Malaysia, Vietnam, Thailand, and the Philippines.
Watch video on RESULTICKS -
Candidate Profile
Resulticks seeks optimistic industry experts to join our team as a Database Engineer. The ideal candidate must be experienced in MySQL, having worked with complex programs. You should be able to work in a team while developing programs. You should also be able to answer question from various teams regarding the program and provide assistance when needed.
The ideal candidate should be detail-oriented to notice any minor error in the program and should be a problem-solver to fix such an error. You must adhere to industry standard best practices in an efficient and effective manner.
Key Responsibilities
Assist in design and development of database systems
Optimize database systems for performance and reliability
Perform database maintenance and troubleshooting activities
Test database systems and perform bug fixes
Provide database solutions based on technical documents and business requirements
Develop database functions, scripts, stored procedures, and triggers to support application development
Provide technical assistance to resolve all database issues related to performance, capacity and access
Ensure data integrity and quality in database systems
Maintain standard policies for database development activities and help in identifying, rectifying database errors in a timely manner
Create physical and logical database models as per the business requirements
Manage and monitor performance, capacity, and security of database syst
Prepare documentation regarding database design, configuration and change management tasks
Perform data back-up and archival on regular basis
Pre-Requisites
4-8 years of relevant experience with in-depth knowledge of MySQL.
Phenomenal written and verbal communication.
Demonstrated ability to onboard and integrate with an organization long-term
Demonstrated capacity to clearly and concisely communicate about complex technical, architectural, and or organizational problems and propose thorough iterative solutions
Desirable Skills
Comfort working in a highly agile, intensively iterative software development process
Positive and solution-oriented mindset
Expert or intermediate Microsoft Power Point and Excel Skills.
Self-motivated and self-managing, with strong organizational skills
Knowledge of best practices in the industry.
Experience And Academic Qualifications
At minimum, a B.S. in information systems, computer science or a relevant field.
Advanced degree, or MBA is a plus
Must have authorization to work in the United States
Resulticks is an equal opportunity employer that is committed to diversity and inclusion in the workplace. We prohibit discrimination and harassment of any kind based on race, color, gender, religion, sexual orientation, national origin, disability, genetic information, pregnancy, or any other protected characteristic as outlined by federal, state, or local laws.
Powered by JazzHR
C2CNChzpMi
Show more
Show less","MySQL, Database Systems, SQL, Performance Optimization, Troubleshooting, Bug Fixes, Data Integrity, Data Quality, Data Backup, Data Archival, Database Development, Database Management, Database Monitoring, Database Security, Data Modeling, Microsoft PowerPoint, Microsoft Excel, Agile Development, Iterative Development, Organizational Skills","mysql, database systems, sql, performance optimization, troubleshooting, bug fixes, data integrity, data quality, data backup, data archival, database development, database management, database monitoring, database security, data modeling, microsoft powerpoint, microsoft excel, agile development, iterative development, organizational skills","agile development, bug fixes, data archival, data backup, data integrity, data quality, database development, database management, database monitoring, database security, database systems, datamodeling, iterative development, microsoft excel, microsoft powerpoint, mysql, organizational skills, performance optimization, sql, troubleshooting"
Data Engineer,ProMark Concepts,"United, LA",https://www.linkedin.com/jobs/view/data-engineer-at-promark-concepts-3787900358,2023-12-17,Victoria, Canada,Mid senior,Onsite,"HR Professionals is currently seeking candidates to fill various positions for Business Intelligence Developer candidates that are interested in working throughout the State of Louisiana or are interested in Remote opportunities. These positions will require the experiences outlined below.
Job Summary
The Data Engineer will be responsible for operationalizing data and analytics initiatives for the company. They will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection. The Data Engineer is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, data architects, and data analysts on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems, and products.
Minimum Requirements
A bachelor's or master's degree in computer science, statistics, applied mathematics, data management, information systems, information science or a related quantitative field or equivalent work experience
At least five years or more of work experience in data management disciplines including data integration, modeling, optimization and data quality, and/or other areas directly relevant to data engineering responsibilities and tasks
At least three years of experience working in cross-functional teams and collaborating with business stakeholders in support of a departmental and/or multi-departmental data management and analytics initiative
Knowledge and/or familiarity of the midstream services industry and data generated in support of business activities related to the gathering, compressing, treating, processing, and selling natural gas, NGLs and NGL products, and crude oil will be strongly preferred
Strong experience with advanced analytics tools for Object-oriented/object function scripting using languages such as R, Python, Java, C++, Scala, and others
Strong ability to design, build and manage data pipelines for data structures encompassing data transformation, data models, schemas, metadata, and workload management
The ability to work with both IT and business in integrating analytics and data science output into business processes and workflows
Strong experience with database programming languages including SQL, PL/SQL, and others for relational databases, and knowledge and/or certifications on upcoming NoSQL/Hadoop-oriented databases like MongoDB, Cassandra, and others for nonrelational databases
Strong experience in working with large, heterogeneous datasets in building and optimizing data pipelines, pipeline architectures and integrated datasets using traditional data integration technologies.
Knowledge and/or experience in working with SQL on Hadoop tools and technologies including HIVE, Impala, Presto, others from an open source perspective and Hortonworks Data Flow (HDF), Dremio, Informatica, Talend, others from a commercial vendor perspective
Experience in working with both open-source and commercial message queuing technologies such as Kafka, JMS, Azure Service Bus, Amazon Simple Queuing Service, others, stream data integration technologies such as Apache Nifi, Apache Beam, Apache Kafka Streams, Amazon Kinesis, and others
Basic experience working with popular data discovery, analytics, and BI software tools like Tableau, Qlik, PowerBI and others for semantic-layer-based data discovery
Strong experience in working with data science teams in refining and optimizing data science and machine learning models and algorithms
Basic experience in working with data governance/data quality and data security teams and specifically data stewards and security resources in moving data pipelines into production with appropriate data quality, governance and security standards and certification
Demonstrated ability to work across multiple deployment environments including cloud, on-premises and hybrid, multiple operating systems and through containerization techniques such as Docker, Kubernetes, AWS Elastic Container Service and others
Familiarity with agile methodologies and capable of applying DevOps and increasingly DataOps principles to data pipelines to improve the communication, integration, reuse and automation of data flows between data managers and consumers across an organization
Strong written and verbal communication skills with an aptitude for problem-solving
Must be able to independently resolve issues and efficiently self-direct work activities based on the ability to capture, organize, and analyze information
Experience troubleshooting complicated issues across multiple systems and driving solutions
Experience providing technical solutions to non-technical individuals
Demonstrated team-building skills
Ability to deal with internal employees and external business contacts while conveying a positive, service-oriented attitude
Summary Of Essential Functions
Develop, construct, test, and maintain data architectures or data pipelines
Ensure data architecture will support the requirements of the business
Discover opportunities for data acquisition
Develop data set processes for data modeling, mining, and production
Employ a variety of languages and tools to marry systems together
Recommend ways to improve data reliability, efficiency, and quality
Leverage large volumes of data from internal and external sources to answer business demands
Employ sophisticated analytics programs, machine learning, and statistical methods to prepare data for use in predictive and prescriptive modeling while exploring and examining data to find hidden patterns
Drive automation through effective metadata management using innovative and modern tools, techniques, and architectures to partially or completely automate the most common, repeatable, and tedious data preparation and integration tasks to minimize manual and error-prone processes and improve productivity
Propose appropriate (and innovative) data ingestion, preparation, integration, and operationalization techniques in optimally addressing data requirements
Ensure that the data users and consumers use the data provisioned to them responsibly through data governance and compliance initiatives
Promote the available data and analytics capabilities and expertise to business unit leaders and educate them in leveraging these capabilities in achieving their business goals
Powered by JazzHR
V0JPAmSoWn
Show more
Show less","Data Integration, Data Modeling, Data Optimization, Data Quality, Data Analytics, Data Science, Data Security, Data Governance, Agile Methodologies, DevOps, DataOps, Cloud Computing, OnPremises Computing, Hybrid Computing, Containerization, Docker, Kubernetes, AWS Elastic Container Service, Tableau, Qlik, PowerBI, SQL, PL/SQL, MongoDB, Cassandra, HIVE, Impala, Presto, Hortonworks Data Flow (HDF), Dremio, Informatica, Talend, Kafka, JMS, Azure Service Bus, Amazon Simple Queuing Service, Apache Nifi, Apache Beam, Apache Kafka Streams, Amazon Kinesis, Machine Learning, Statistical Methods, Predictive Modeling, Prescriptive Modeling, Data Mining, Data Production, R, Python, Java, C++, Scala","data integration, data modeling, data optimization, data quality, data analytics, data science, data security, data governance, agile methodologies, devops, dataops, cloud computing, onpremises computing, hybrid computing, containerization, docker, kubernetes, aws elastic container service, tableau, qlik, powerbi, sql, plsql, mongodb, cassandra, hive, impala, presto, hortonworks data flow hdf, dremio, informatica, talend, kafka, jms, azure service bus, amazon simple queuing service, apache nifi, apache beam, apache kafka streams, amazon kinesis, machine learning, statistical methods, predictive modeling, prescriptive modeling, data mining, data production, r, python, java, c, scala","agile methodologies, amazon kinesis, amazon simple queuing service, apache beam, apache kafka streams, apache nifi, aws elastic container service, azure service bus, c, cassandra, cloud computing, containerization, data governance, data integration, data mining, data optimization, data production, data quality, data science, data security, dataanalytics, datamodeling, dataops, devops, docker, dremio, hive, hortonworks data flow hdf, hybrid computing, impala, informatica, java, jms, kafka, kubernetes, machine learning, mongodb, onpremises computing, plsql, powerbi, predictive modeling, prescriptive modeling, presto, python, qlik, r, scala, sql, statistical methods, tableau, talend"
"Data Engineer || Juno Beach, FL",Steneral Consulting,"Juno Beach, FL",https://www.linkedin.com/jobs/view/data-engineer-juno-beach-fl-at-steneral-consulting-3704928308,2023-12-17,Victoria, Canada,Mid senior,Onsite,"Title- Data Engineer
Location-
Onsite in Juno Beach, FL (must live in Juno Beach area relocation not accepted at this time)
JD-
Must Haves
Top Skills our client is ideally looking for:
4+ years of Data Engineer experience (or similar)
Java
Python
Spark
Cloud experience (AWS or Azure)
Data Modeling
IoT Data (Internet of Things) (EX: Smart Home devices, Wearable Devices like fitness trackers etc, GPS, patient monitoring devices, smart meters for utility management, etc..)
Overview
Responsible for the development and integration of new or existing applications into the technical infrastructure and existing business processes. Provides technical or functional guidance to project or work teams as needed within a specific discipline. Collaborates on an on-going basis with the Business Systems Analyst. Analyzes, designs, develops, tests, debugs, implements, maintains and/or enhances existing or new systems that are reliable and efficient.
Analyzes, designs, develops, tests, debugs, implements, maintains, integrates, customizes, and enhances existing or new systems
4+ years experience as a data engineer using Java, python, spark
Hands-on experience with data modeling, with a strong understanding of IoT data.
Experience building systems with cloud providers such as AWS or Azure
Applies appropriate Agile or development methodologies, system development lifecycles, tools, and technology to manage development activities
Automates the handoff of code releases from development to operations (DevOps)
Ensures user satisfaction by providing preventative maintenance, troubleshooting, and timely resolution of more complex problems
Designs, develops, and tests experimental application work to test new ideas with bounded cost and time frames
Show more
Show less","Data Engineering, Java, Python, Spark, Cloud Computing, AWS, Azure, Data Modeling, IoT Data, Agile Development, DevOps, Experimental Application Development","data engineering, java, python, spark, cloud computing, aws, azure, data modeling, iot data, agile development, devops, experimental application development","agile development, aws, azure, cloud computing, data engineering, datamodeling, devops, experimental application development, iot data, java, python, spark"
Data Engineer,Cargomatic,"San Francisco, CA",https://www.linkedin.com/jobs/view/data-engineer-at-cargomatic-3787741852,2023-12-17,Victoria, Canada,Mid senior,Onsite,"Join a rapidly growing company revolutionizing the trucking industry! Cargomatic is the leading technology platform and digital marketplace for powering world-class, local trucking. Local trucking is the lifeblood of every regional economy, and yet this industry still relies heavily on phone calls and fax machines. Cargomatic is transforming the way goods move around metropolitan areas by connecting shippers and commercial truck drivers with mobile technology. We are solving complex, real-world problems every day and giving full transparency to the shipping process.
Cargomatic was named to the list of Built In Best Places to Work for 2023, which recognizes the benefits we offer, our people first culture and commitment to supporting our employees’ success, growth and well-being.
Cargomatic is looking for a talented and highly motivated Data Engineer to join our team. In this role, you will be a crucial part of our data strategy, working closely with both the Engineering and Product teams to build a scalable data model, develop data pipelines for near real-time data feeds, optimize Redshift for top performance, and create Tableau dashboards for actionable insights. If you are passionate about data and possess the skills to drive data-driven decisions, we want to hear from you.
Responsibilities:
Data Model Development: Collaborate with the Engineering and Product teams to design and construct a scalable data model that aligns with our business objectives and accommodates evolving data needs.
Data Pipeline Construction: Create and maintain data pipelines to process near real-time data feeds, ensuring data is efficiently ingested, transformed, and available for analysis.
Expert in efficient and optimal management of Data Warehouse, specifically Redshift. Good knowledge on other Cloud Data stores (Databrick,Snowflake).
Tableau Dashboard Creation: Develop Tableau dashboards that provide actionable insights, enabling cross-functional teams to make informed decisions based on data.
Data Quality Assurance: Ensure data accuracy, consistency, and integrity throughout all processes, and implement data governance best practices.
Collaboration: Work closely with data analysts, data scientists, and other teams to understand their data requirements and provide solutions that meet their needs.
Performance Monitoring: Continuously monitor and optimize database and query performance to ensure efficient data processing and retrieval.
Stay Updated: Keep abreast of industry best practices, emerging technologies, and tools in the data engineering field.
Qualifications:
Bachelor's degree in Computer Science, Data Engineering, or a related field (Master's degree is a plus).
A minimum of 5 years of experience in data engineering, ETL processes, and database management.
Proficiency in SQL, data modeling, and database design.
Hands-on experience with Amazon Redshift or similar data warehousing solutions.
Strong programming skills, especially in languages like Python, Java, or Scala.
Experience with big data technologies and frameworks (e.g., Hadoop, Spark, Kafka).
Knowledge of data visualization tools, with expertise in Tableau or similar platforms.
Strong problem-solving skills and the ability to work collaboratively in a team-oriented environment.
Excellent communication skills to convey technical concepts to non-technical stakeholders.
Sound Knowledge in NoSql db(Mongodb)
This is a hybrid position based out of the SanFrancisco office.
The expected salary range for this role is $145,000 to $155,000. The actual base pay offered will be determined on factors such as experience, skills, training, location, certifications, education, and other factors permitted by law. Decisions will be made on a case-by-case basis. In addition to the base salary, this position may be eligible for performance-based incentives.
To learn more about how we use your data, .
To Learn More About How We Use Your Data,
Powered by JazzHR
Uanud4Q7aE
Show more
Show less","Data Engineering, SQL, Data Modeling, Database Design, Amazon Redshift, Hadoop, Spark, Kafka, Tableau, Python, Java, Scala, ETL, MongoDB","data engineering, sql, data modeling, database design, amazon redshift, hadoop, spark, kafka, tableau, python, java, scala, etl, mongodb","amazon redshift, data engineering, database design, datamodeling, etl, hadoop, java, kafka, mongodb, python, scala, spark, sql, tableau"
Sr. ETL Data Engineer- REMOTE,PSRTEK,"Portland, OR",https://www.linkedin.com/jobs/view/sr-etl-data-engineer-remote-at-psrtek-3766318683,2023-12-17,Victoria, Canada,Mid senior,Onsite,"Sr. ETL Data Engineer
REMOTE
Pacific or Mountain time zone
Data profiling to identify primary keys and issues with the data.
ETL to bring data onto the Data Platform, de-duplicate data, create or update dimensional data structures, and produce use case-specific output.
Unit testing, functional testing, and performance testing and tuning.
Interacting with the Product team to understand and refine requirements.
Interacting with QA to address reported findings.
Working individually and as a team to achieve our goals.
PSRTEK is a reputed technology recruitment and IT staffing brand with a global footprint and an admired client base. As an ideas and innovation powerhouse with a culture of excellence, we bring remarkable expertise and deliver powerfully transformative results.
Show more
Show less","ETL, Data profiling, Data cleansing, Data deduplication, Dimensional modeling, Unit testing, Functional testing, Performance testing, Performance tuning, Product requirements gathering, Quality assurance, Teamwork","etl, data profiling, data cleansing, data deduplication, dimensional modeling, unit testing, functional testing, performance testing, performance tuning, product requirements gathering, quality assurance, teamwork","data deduplication, data profiling, datacleaning, dimensional modeling, etl, functional testing, performance testing, performance tuning, product requirements gathering, quality assurance, teamwork, unit testing"
Data Engineer,"The Dignify Solutions, LLC","Bellevue, WA",https://www.linkedin.com/jobs/view/data-engineer-at-the-dignify-solutions-llc-3768014160,2023-12-17,Victoria, Canada,Mid senior,Onsite,"Data Engineering experience primarily on Spark. Someone who has worked on Azure cloud with knowledge on Azure DEVOPS (CI/CD & Infrastructure as a code), ADF, ADW & Power BI. Apache Nifi will be good to have.Power BI: Understand business requirements to set functional specifications for reporting applications Build automated reports and dashboards with the help of Power BI reporting tool Be experienced in tools and systems on MS SQL Server BI Stack, including SSRS and TSQL, Power Query, MDX, PowerBI, and DAX Be able to quickly shape data into reporting and analytics solutions Have knowledge of database fundamentals such as multidimensional database design, relational database design, and more Create functional reporting Study, analyze and understand business requirements in context to business intelligence. Design and map data models to shift raw data into meaningful insights. Utilize Power BI to build interactive and visually appealing dashboards and reports. Spot key performance indicators with apt objectives Run DAX queries and functions in Power BI Should have an edge over making DAX queries in Power BI desktop. Developing visual reports, KPI scorecards, and dashboards using Power BI desktop. Connecting data sources, importing data, and transforming data for Business intelligence.
Show more
Show less","Spark, Azure DEVOPS, CI/CD, Infrastructure as a code, ADF, ADW, Power BI, Apache Nifi, MS SQL Server BI Stack, SSRS, TSQL, Power Query, MDX, DAX, PowerBI, Multidimensional database design, Relational database design, Key performance indicators","spark, azure devops, cicd, infrastructure as a code, adf, adw, power bi, apache nifi, ms sql server bi stack, ssrs, tsql, power query, mdx, dax, powerbi, multidimensional database design, relational database design, key performance indicators","adf, adw, apache nifi, azure devops, cicd, dax, infrastructure as a code, key performance indicators, mdx, ms sql server bi stack, multidimensional database design, power query, powerbi, relational database design, spark, ssrs, tsql"
Data Engineer,Intelletec,"Arlington, VA",https://www.linkedin.com/jobs/view/data-engineer-at-intelletec-3646977344,2023-12-17,Victoria, Canada,Mid senior,Onsite,"Intelletec is actively hiring for IT engineering experts looking for a challenging but rewarding career path in the Intelligence Community (IC). Intelletec seeks a Data Engineer to work with the team to provide engineering support to the data science and software engineering team members.
Key Responsibilities:
Support ad-hoc data analysis requirements defined by the client's Leadership.
Reporting solutions will encompass multiple technology platforms used by the client and will drive business process re-design and raining/change management initiatives.
Knowledge of business intelligence reporting tools and data visualization software including Tableau.
Will work on Data cleaning and transformation efforts in delivery of CDRLs.
Design and optimize Data Pipelines using Spark, Hudi, EMR cloud services, and Kubernetes containers
Make sure pedigree and provenance of the data is maintained such that the access to data is protected
Clean and preprocess data to enable analytic access
Collaborate with enterprise working groups to advance the state of data standards
Collaborate with the engineering team, data stewards, and mission partners to aid in getting actionable value out of the data holdings architects complex, repeatable ETL processes
Provide Advanced Database Administration support in Oracle, MySQL, MariaDB, MongoDB, Elastic and others
Supports Experience with Targeting using Sponsor Tools, Reverse Engineering
Develop API connectors to enable ingest of new data catalog entries from databases and files
ensure that data mappings will provide the best performance for expected user experience
Requirements: (TS + FSP REQUIRED AT START)
MUST HAVE: All candidates will be required to be US Citizens, willing to undergo the Government issued background investigation process
Experienced in extracting and aggregating structured and unstructured data
Experienced in data programming languages and tools such as Python and R
Experience with SQL or similar database language
Experience designing and implementing data models to enable, sustain and enhance the value of information they contain
Strong analytical and critical thinking skills
Ability to work collaboratively and effectively in a team environment
Show more
Show less","Data Engineering, Data Visualization, Tableau, Data Cleaning, Data Transformation, Spark, Hudi, EMR, Kubernetes, Data Pipelines, Data Standards, ETL, Oracle, MySQL, MariaDB, MongoDB, Elastic, API Connectors, Python, R, SQL, Data Models, Analytical Skills, Critical Thinking, Teamwork","data engineering, data visualization, tableau, data cleaning, data transformation, spark, hudi, emr, kubernetes, data pipelines, data standards, etl, oracle, mysql, mariadb, mongodb, elastic, api connectors, python, r, sql, data models, analytical skills, critical thinking, teamwork","analytical skills, api connectors, critical thinking, data cleaning, data engineering, data models, data standards, data transformation, datapipeline, elastic, emr, etl, hudi, kubernetes, mariadb, mongodb, mysql, oracle, python, r, spark, sql, tableau, teamwork, visualization"
Cloud Data Engineer,Talener,"Melville, NY",https://www.linkedin.com/jobs/view/cloud-data-engineer-at-talener-3748836564,2023-12-17,Victoria, Canada,Mid senior,Onsite,"Our client is a health-tec firm that provides a platform for claims management and an online pharmacy. They are looking to hire a Cloud Data Enigneer/Architect that is strong with Azure. You will play a critical role in designing, implementing, and maintaining data solutions on Microsoft Azure. You will be responsible for developing robust, scalable, and high-performance data pipelines, databases, and analytics.
Title:
Cloud Data Engineer
Location:
Melville, NY
Required Skills And Responsibilities
5+ years of experience as a Cloud Data Engineer or Architect
Experience in designing, implementing, and managing data solutions using Azure SQL.
Proficiency in SQL, T-SQL, and database design principles.
Experience with ETL tools and data integration techniques
Data Architecture: Collaborate with cross-functional teams to design and implement Azure SQL-based data architectures that meet business and technical requirements.
Data Ingestion: Develop and maintain data pipelines for ingesting data from various sources into Azure SQL databases.
Data Transformation: Implement data transformation processes, ensuring data quality, consistency, and accuracy.
Database Management: Manage, optimize, and maintain Azure SQL databases to ensure high availability, performance, and security.
Performance Tuning: Identify and resolve performance bottlenecks and optimize database queries for efficient data retrieval.
Monitoring and Troubleshooting: Monitor data systems, perform diagnostics, and address issues in a timely manner. Set up alerts and proactive monitoring.
Nice To Have Skills
Azure certifications (e.g., Azure Data Engineer, Azure SQL Database) are a plus.
Familiarity with data modeling, data governance, and data security.
Compensation
$120,000-150,000
Annual bonus
For additional information, please reach out to Jed Pillion at jpillion@talener.com
Show more
Show less","Cloud Data Engineering, Azure, Data Architecture, Data Ingestion, Data Transformation, Database Management, Performance Tuning, Monitoring and Troubleshooting, SQL, TSQL, ETL tools, Data Integration techniques, Azure SQL","cloud data engineering, azure, data architecture, data ingestion, data transformation, database management, performance tuning, monitoring and troubleshooting, sql, tsql, etl tools, data integration techniques, azure sql","azure, azure sql, cloud data engineering, data architecture, data ingestion, data integration techniques, data transformation, database management, etl tools, monitoring and troubleshooting, performance tuning, sql, tsql"
Senior Data Engineer (On-Site),PrismHR,"Hopkinton, MA",https://www.linkedin.com/jobs/view/senior-data-engineer-on-site-at-prismhr-3768113935,2023-12-17,Victoria, Canada,Mid senior,Onsite,"Do you have a passion for building data architectures that enable smooth and seamless product experiences? Are you an all-around data enthusiast with a knack for ETL? We're hiring Data Engineers to help build and optimize the foundational architecture of our product's data.
We’ve built a strong data engineering team to date, but have a lot of work ahead of us, including:
Migrating from relational databases to a streaming and big data architecture, including a complete overhaul of our data feeds
Defining streaming event data feeds required for real-time analytics and reporting
Leveling up our platform, including enhancing our automation, test coverage, observability, alerting, and performance
As a Senior Data Engineer, you will work with the development team to construct a data streaming platform and data warehouse that serves as the data foundations for our product.
Help us scale our business to meet the needs of our growing customer base and develop new products on our platform. You'll be a critical part of our growing company, working on a cross-functional team to implement best practices in technology, architecture, and process. You’ll have the chance to work in an open and collaborative environment, receive hands-on mentorship and have ample opportunities to grow and accelerate your career!
Responsibilities
Build our next generation data warehouse
Build our event stream platform
Translate user requirements for reporting and analysis into actionable deliverables
Enhance automation, operation, and expansion of real-time and batch data environment
Manage numerous projects in an ever-changing work environment
Extract, transform, and load complex data into the data warehouse using cutting-edge technologies
Build processes for topnotch security, performance, reliability, and accuracy
Provide mentorship and collaborate with fellow team members
Requirements
Bachelor’s or Master’s degree in Computer Science, Information Systems, Operations Research, or related field required
5+ years of experience building data pipelines
5+ years of experience building data frameworks for unit testing, data lineage tracking, and automation
Fluency in Scala is required
Working knowledge of Apache Spark
Familiarity with streaming technologies (e.g., Kafka, Kinesis, Flink)
Nice To Have
Experience with Machine Learning
Familiarity with Looker a plus
Knowledge of additional server-side programming languages (e.g. Golang, C#, Ruby)
PrismHR is a fast-paced SaaS company which provides customers with a cloud-based payroll process software application. PrismHR also provides professional services including system implementation consulting, custom configurations, and training. Lastly, via the Company’s Marketplace platform customers and end users access other human resources and employee benefits applications from PrismHR’s Marketplace Partners.
Diversity, Equity And Inclusion Program/Affirmative Action Plan
We have transformed our company into an inclusive environment where individuals are valued for their talents and empowered to reach their fullest potential. At PrismHR, we strive to continually lead with our values and beliefs that enable our employees to develop their potential, bring their full self to work, and engage in a world of inclusion.
Ensuring an inclusive environment for our employees is an integral part of the PrismHR culture. We aren't just checking a box, we are truly committed to creating a workplace that celebrates the diversity of our employees and fosters a sense of belonging for everyone. This is essential to our success. We are dedicated to building a diverse, inclusive, and authentic workplace, so if you’re excited about our roles but your past experience doesn’t align perfectly with every qualification in the job description, we encourage you to apply anyway. You may be just the right candidate for these open roles or other open roles. We particularly encourage applicants from traditionally under-represented groups as we seek to increase the diversity of our workforce and provide fair opportunities for all.
As a proud Equal Opportunity and Affirmative Action Employer, PrismHR encourages talent from all backgrounds to join our team. Employment decisions are based on an individual’s qualifications as they relate to the job under consideration. The Company’s policy prohibits unlawful discrimination based on sex (which includes pregnancy, childbirth, breastfeeding, or related medical conditions, the actual sex of the individual, or the gender identity or gender expression), race, color, religion, including religious dress practices and religious grooming practices, sexual orientation, national origin, ancestry, citizenship, marital status, familial status, age, physical disability, mental disability, medical condition, genetic information, protected veteran or military status, or any other consideration made unlawful by federal, state or local laws, ordinances, or regulations.
The Company is committed to complying with all applicable laws providing equal employment opportunities. This commitment applies to all persons involved in the operations of the Company and prohibits unlawful discrimination by any employee of the Company, including supervisors and co-workers.
Privacy Policy: For information about how we collect and use your personal information, please see our privacy statement available at https://www.prismhr.com/about/privacy-policy.
PrismHR provides reasonable accommodation for qualified individuals with disabilities and disabled veterans in job application procedures. If you have any difficulty using our online system and you need a reasonable accommodation due to a disability, you may use the following alternative email address to contact us about your interest in employment at PrismHR: taglobal@prismhr.com. Please indicate in the subject line of your email that you are requesting accommodation. Only candidates being considered for a position who require an accommodation will receive a follow-up response.
Show more
Show less","ETL, Data Architecture, Data Warehousing, Data Pipelines, Data Frameworks, Unit Testing, Data Lineage Tracking, Automation, Scala, Apache Spark, Streaming Technologies, Kafka, Kinesis, Flink, Machine Learning, Looker, Golang, C#, Ruby, PrismHR, Relational Databases, Big Data Architecture, Data Feeds, Realtime Analytics, Reporting, Observability, Alerting, Performance","etl, data architecture, data warehousing, data pipelines, data frameworks, unit testing, data lineage tracking, automation, scala, apache spark, streaming technologies, kafka, kinesis, flink, machine learning, looker, golang, c, ruby, prismhr, relational databases, big data architecture, data feeds, realtime analytics, reporting, observability, alerting, performance","alerting, apache spark, automation, big data architecture, c, data architecture, data feeds, data frameworks, data lineage tracking, datapipeline, datawarehouse, etl, flink, golang, kafka, kinesis, looker, machine learning, observability, performance, prismhr, realtime analytics, relational databases, reporting, ruby, scala, streaming technologies, unit testing"
Sr. Data Engineer,Experfy,"San Diego, CA",https://www.linkedin.com/jobs/view/sr-data-engineer-at-experfy-3590325748,2023-12-17,Victoria, Canada,Mid senior,Onsite,"A Sr. Data Engineer is proficient in the development of all aspects of data processing including data warehouse architecture/modeling and ETL processing. The position focuses research on development and delivery of analytical solutions using various tools including Confluent Kafka, Kinesis, Glue, Lambda, Snowflake and SQL Server. A Sr. Data Engineer must be able to work autonomously with little guidance or instruction to deliver business value.
Responsibilities
Position Responsibilities
Partner with business stakeholders to gather requirements and translate them into technical specifications and process documentation for IT counterparts (on-prem and offshore)
Highly proficient in the architecture and development of an event driven data warehouse; streaming, batch, data modeling, and storage
Advanced database knowledge; creating/optimizing SQL queries, stored procedures, functions, partitioning data, indexing, and reading execution plans
Skilled experience in writing and troubleshooting Python/PySpark scripts to generate extracts, cleanse, conform and deliver data for consumption
Expert level of understanding and implementing ETL architecture; data profiling, process flow, metric logging and error handling
Support continuous improvement by investigating and presenting alternatives to processes and technologies to an architectural review board
Develop and ensure adherence to published system architectural decisions and development standards
Multi-task across several ongoing projects and daily duties of varying priorities as required
Interact with global technical teams to communicate business requirements and collaboratively build data solutions
Requirements
8+ years of development experience
Expert level in data warehouse design/architecture, dimensional data modeling and ETL process development
Advanced level development in SQL/NoSQL scripting and complex stored procedures (Snowflake, SQL Server, DynomoDB, NEO4J a plus)
Extremely proficient in Python, PySpark, and Java
AWS Expertise - Kinesis, Glue (Spark), EMR, S3, Lambda, and Athena
Streaming Services - Confluent Kafka and Kinesis (or equivalent)
Hands on experience in designing and developing applications using Java Spring Framework (Spring Boot, Spring Cloud, Spring Data etc)
Show more
Show less","Data warehouse architecture/modeling, ETL processing, Confluent Kafka, Apache Kinesis, AWS Glue, AWS Lambda, Snowflake, SQL Server, SQL, Python, PySpark, Data profiling, Process flow, Metric logging, Error handling, ETL architecture, System architectural decisions, Development standards, Data warehouse design, Dimensional data modeling, NoSQL scripting, Stored procedures, AWS Expertise, Java Spring Framework, Spring Boot, Spring Cloud, Spring Data","data warehouse architecturemodeling, etl processing, confluent kafka, apache kinesis, aws glue, aws lambda, snowflake, sql server, sql, python, pyspark, data profiling, process flow, metric logging, error handling, etl architecture, system architectural decisions, development standards, data warehouse design, dimensional data modeling, nosql scripting, stored procedures, aws expertise, java spring framework, spring boot, spring cloud, spring data","apache kinesis, aws expertise, aws glue, aws lambda, confluent kafka, data profiling, data warehouse architecturemodeling, data warehouse design, development standards, dimensional data modeling, error handling, etl architecture, etl processing, java spring framework, metric logging, nosql scripting, process flow, python, snowflake, spark, spring boot, spring cloud, spring data, sql, sql server, stored procedures, system architectural decisions"
Sr. Data Engineer,Experfy,"Los Angeles, CA",https://www.linkedin.com/jobs/view/sr-data-engineer-at-experfy-3646108866,2023-12-17,Victoria, Canada,Mid senior,Onsite,"A Sr. Data Engineer is proficient in the development of all aspects of data processing including data warehouse architecture/modeling and ETL processing. The position focuses research on development and delivery of analytical solutions using various tools including Confluent Kafka, Kinesis, Glue, Lambda, Snowflake and SQL Server. A Sr. Data Engineer must be able to work autonomously with little guidance or instruction to deliver business value.
Responsibilities
Position Responsibilities
Partner with business stakeholders to gather requirements and translate them into technical specifications and process documentation for IT counterparts (on-prem and offshore)
Highly proficient in the architecture and development of an event driven data warehouse; streaming, batch, data modeling, and storage
Advanced database knowledge; creating/optimizing SQL queries, stored procedures, functions, partitioning data, indexing, and reading execution plans
Skilled experience in writing and troubleshooting Python/PySpark scripts to generate extracts, cleanse, conform and deliver data for consumption
Expert level of understanding and implementing ETL architecture; data profiling, process flow, metric logging and error handling
Support continuous improvement by investigating and presenting alternatives to processes and technologies to an architectural review board
Develop and ensure adherence to published system architectural decisions and development standards
Multi-task across several ongoing projects and daily duties of varying priorities as required
Interact with global technical teams to communicate business requirements and collaboratively build data solutions
Requirements
Requirements
8+ years of development experience
Expert level in data warehouse design/architecture, dimensional data modeling and ETL process development
Advanced level development in SQL/NoSQL scripting and complex stored procedures (Snowflake, SQL Server, DynomoDB, NEO4J a plus)
Extremely proficient in Python, PySpark, and Java
AWS Expertise - Kinesis, Glue (Spark), EMR, S3, Lambda, and Athena
Streaming Services - Confluent Kafka and Kinesis (or equivalent)
Hands on experience in designing and developing applications using Java Spring Framework (Spring Boot, Spring Cloud, Spring Data etc)
Apply for this job
Show more
Show less","Data warehouse architecture, ETL processing, Confluent Kafka, Kinesis, Glue, Lambda, Snowflake, SQL Server, Python, PySpark, SQL, Data profiling, Metric logging, Error handling, System architectural decisions, Development standards, Global technical teams, Data warehouse design, Dimensional data modeling, SQL/NoSQL scripting, Complex stored procedures, Java, AWS Expertise, EMR, S3, Athena, Java Spring Framework, Spring Boot, Spring Cloud, Spring Data","data warehouse architecture, etl processing, confluent kafka, kinesis, glue, lambda, snowflake, sql server, python, pyspark, sql, data profiling, metric logging, error handling, system architectural decisions, development standards, global technical teams, data warehouse design, dimensional data modeling, sqlnosql scripting, complex stored procedures, java, aws expertise, emr, s3, athena, java spring framework, spring boot, spring cloud, spring data","athena, aws expertise, complex stored procedures, confluent kafka, data profiling, data warehouse architecture, data warehouse design, development standards, dimensional data modeling, emr, error handling, etl processing, global technical teams, glue, java, java spring framework, kinesis, lambda, metric logging, python, s3, snowflake, spark, spring boot, spring cloud, spring data, sql, sql server, sqlnosql scripting, system architectural decisions"
Azure Data Engineer,Saransh Inc,"Warren, NJ",https://www.linkedin.com/jobs/view/azure-data-engineer-at-saransh-inc-3723273797,2023-12-17,Victoria, Canada,Mid senior,Onsite,"Job Title: Azure Data Engineer
Location: Warren, NJ OR
Stamford Connecticut -(Onsite)
Duration: Long term
Required Skills.
Azure Data Lake, Data Bricks, PY spark, SQL
Show more
Show less","Azure Data Lake, Data Bricks, PySpark, SQL","azure data lake, data bricks, pyspark, sql","azure data lake, data bricks, spark, sql"
"Looking for Sr. Data Engineer- NYC, NY - Fulltime",Extend Information Systems Inc.,"New York, NY",https://www.linkedin.com/jobs/view/looking-for-sr-data-engineer-nyc-ny-fulltime-at-extend-information-systems-inc-3750869838,2023-12-17,Victoria, Canada,Mid senior,Onsite,"Hi,
I hope you are doing well!
We have an opportunity for
Sr. Data Engineer
with one of our clients for
NYC, NY,
Please see the job details below and let me know if you would be interested in this role.
If interested, please send me a copy of your resume, contact details, availability, and a good time to connect with you.
Title:
Sr. Data Engineer
Location: NYC, NY,
Terms:
Fulltime
Job Description
Skills : Scala, Spark, SQL
8+ years of data engineering experience
Experience building streaming pipelines using Scala, Spark, and SQL
Must have good exposure in AWS technology
Excellent communication and ownership skills
Thanks & Regards
Priyanka tiwari
Extend Information System Inc
Phone: (703) 956-1120
Email: priyanka1@extendinfosys.com
44258 Mercure Circle, UNIT 102 A, Sterling VA, USA – 20166
Show more
Show less","Scala, Apache Spark, SQL, AWS, Communication skills, Ownership skills","scala, apache spark, sql, aws, communication skills, ownership skills","apache spark, aws, communication skills, ownership skills, scala, sql"
Data Engineer,Netvision Resources Inc,"Woodlawn, MD",https://www.linkedin.com/jobs/view/data-engineer-at-netvision-resources-inc-3766094313,2023-12-17,Victoria, Canada,Mid senior,Onsite,"Job Description
Collaborate with Solution & Enterprise Architects, Business analysts, DBAs, and others to understand business requirements.
Provide suggestions and guidance to different MI/BI projects, based on best practices, optimal solutions, and project standards will be expected.
Lead complex discussions and engagements that may involve multiple project teams from client.
Basic Qualifications
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI analytics.
Strong working experience with SQL and other databases (i.e., DB2 and Oracle)
Strong working experience with BI Development (i.e., Tableau and other BI tools)
Strong working experience with ETL development and methodologies.
Required Skills
Strong oral and written communication skills and ability to communicate with all levels within the organization.
Strong data analysis and problem-solving skills
Strong interpersonal skills with the ability to collaborate with others effectively and efficiently.
Ability to negotiate and compromise to convince internal and external parties to accept concepts, practices, and approaches of the area.
Desired Skills
Database development skills
Familiarity with data modeling (Erwin data modeler)
Enterprise Data Catalog experience (Informatica)
Education And Other Qualifications
Master's degree and 4+ years of experience in an enterprise business architect field or bachelor's degree and 8+ years of experience in an enterprise business architect-related field
NetVision Resources, Inc. (NVR) is a fast growing, IS9001, IS20000, CMMI DEV/2, MBE certified technology consulting and software development firm, based out of the DC Metro area. To know more, please visit: www.netvisionresources.com
Show more
Show less","Information Architecture, Data Architecture, Data Modeling, Data Governance, ETL Design, Data Quality, BI Analytics, SQL, Databases, BI Development, Tableau, BI Tools, ETL Development, Oral Communication, Written Communication, Data Analysis, ProblemSolving, Interpersonal Skills, Collaboration, Negotiation, Compromise, Database Development, Data Modeling, Enterprise Data Catalog, Informatica, Master's Degree, Bachelor's Degree","information architecture, data architecture, data modeling, data governance, etl design, data quality, bi analytics, sql, databases, bi development, tableau, bi tools, etl development, oral communication, written communication, data analysis, problemsolving, interpersonal skills, collaboration, negotiation, compromise, database development, data modeling, enterprise data catalog, informatica, masters degree, bachelors degree","bachelors degree, bi analytics, bi development, bi tools, collaboration, compromise, data architecture, data governance, data quality, dataanalytics, database development, databases, datamodeling, enterprise data catalog, etl design, etl development, informatica, information architecture, interpersonal skills, masters degree, negotiation, oral communication, problemsolving, sql, tableau, written communication"
Senior Data Engineer,HCSS,"Sugar Land, TX",https://www.linkedin.com/jobs/view/senior-data-engineer-at-hcss-3787927608,2023-12-17,Victoria, Canada,Mid senior,Onsite,"We’re HCSS.
We’re a software company based in Sugar Land, TX and we
provide innovative solutions for the construction industry that help streamline
their operations. Our mission at HCSS is to help customers dramatically
improve their business through our innovative, high-quality software and
exceptionally helpful service while providing a great life for our employees.
With this mission at the forefront of everything we do, we’re recognized as a
pioneer and leader in our market and nominated the “Best Companies to
Work for in Texas” 15 years in a row.
WHO WE NEED:
We’re currently looking for a Senior Data Engineer with proven experience in
database administration to join our high-performing Technology team and
provide our customers with quality service. If you're a fit, the following things
should sound like you:
Advanced knowledge of SQL database administration with strong
replication and high availability knowledge
Demonstrated mastery of industry best practices, including CI/CD
principles and secure coding practices
Experience managing and providing oversight of Azure cloud
databases, Azure Elastic Pools, and Azure SQL Servers.
Relational Database Administrator and Engineering experience (SQL
Server, Azure SQL), index optimization, merge processing
optimization, database maintenance (index maintenance, statistics
maintenance, DBCC CHECKDB), deadlock resolution, and database
user and role security
Knowledge of modern and emerging cloud technologies is a plus
Excellent communication skills with the ability to ensure continuous
value delivery through iterative and incremental improvements.
Experience running mission critical systems at scale in the cloud
ROLE RESPONSIBILITIES:
Lead the architecture and design for the database layer for our suite
of cloud applications to ensure high availability, to plan disaster
recovery strategies, and to implement backup and maintenance
processes
Provide leadership, technical direction, and oversight to multiple
software engineering, business intelligence and enterprise software
services teams
Contribute to the technology strategy, architectural vision, integration,
and problem solving across the customer facing and internal facing
products
Have a strong desire to understand the root cause and details of
systems, get hands-on with code, data, and analysis to evaluate how
the teams and the products and services are growing.
Be the organization's sought after individual for modern data
architecture and providing support for addressing defense and
compliance.
WHY WORK WITH OUR TECHNOLOGY TEAM?
A sought after expert. You are in the front seat, leading our data tier
architecture and design process for infrastructure that serves some of
the world’s largest construction companies.
Engaging, driven, and a collaborative team. We are a fun and
friendly team that immensely values teamwork. We deeply care and
support each other so we can collectively impact the company, the
customer, and the industry.
Impact an industry. Construction is going through a technology
evolution and we have the fantastic opportunity to be in the front seat
to aid in that transition.
Build products of real value. We come to work dedicated to building
proven and dependable products for the people of the construction
industry. We strive to delight them through outcome-driven,
customer-centric, and evidence-based solutions.
BENEFITS & PERKS:
Part of our mission statement is to provide a great life for our employees. We
believe that happy employees make for a better company, so we take care of
them. Here are a few of the perks we offer:
Flexibility for you to work in-office, hybrid or remote
Company paid medical and dental premiums
On-site gym and fitness classes
Protection plan for your pets
401k match
Educational tuition reimbursement
And more!
Powered by JazzHR
04302oxarZ
Show more
Show less","SQL, Azure, CI/CD, Cloud computing, Database administration, Database security, Software engineering, Business intelligence, Data architecture, Data tier architecture, Disaster recovery, Backup, Maintenance, Index optimization, Database maintenance, Deadlock resolution, User and role security, Communication, Data analysis, Data availability, Handson experience, Infrastructure, Construction","sql, azure, cicd, cloud computing, database administration, database security, software engineering, business intelligence, data architecture, data tier architecture, disaster recovery, backup, maintenance, index optimization, database maintenance, deadlock resolution, user and role security, communication, data analysis, data availability, handson experience, infrastructure, construction","azure, backup, business intelligence, cicd, cloud computing, communication, construction, data architecture, data availability, data tier architecture, dataanalytics, database administration, database maintenance, database security, deadlock resolution, disaster recovery, handson experience, index optimization, infrastructure, maintenance, software engineering, sql, user and role security"
Data Engineer - 16479,Surge Technology Solutions Inc,"Morton, IL",https://www.linkedin.com/jobs/view/data-engineer-16479-at-surge-technology-solutions-inc-3662658613,2023-12-17,Victoria, Canada,Mid senior,Onsite,"Emp Type
: W2 or 1099........
(No C2C)
Visa
: H1B, H4EAD, GCEAD, L2, Green Card, US Citizens (
Only USA Applicants
)
Workplace Type
: Onsite / Hybrid ( 2 /3 Days a Week)
Experience
6+ Yrs.
Job Description
Work is typically directed by a direct supervisor, project or team lead. Decisions on routine, medium risk issues that may affect the project team, suppliers or internal customers may be made by this position. Challenges include meeting expectations in delivering results, learning to refine solutions to better fit complex situations, making timely decisions, and communicating effectively with all project stakeholders.
Education/Experience
Associate's degree in computer programming or a relevant field required. Bachelor's degree preferred.
Specific Responsibilities
4 years experience required.
Extract large, complex data sets that meet business requirements. Work to build the on-prem /cloud infrastructure for optimal extraction, transformation, and loading of a wide variety of complex business data from on-prem/cloud databases.
Identify ways to improve data reliability, efficiency, and quality.
Work with internal and external stakeholders to assist with data-related technical issues and support data needs.
Own the design and development of ongoing business metrics/KPI, reports and dashboards to drive key business decisions.
Prepare data for predictive and prescriptive modeling
.
Critical Technical Skills
Familiarity with database such as Snowflake, DB2, SQL Server, Oracle (2-3 of these are required)
Preferred Required
Programming languages SQL (required), Python(required) and SAS (preferred)
Experience working with large data sets, preferably in several GB or millions of transactions.
Visualization Power BI (required), Tableau(preferred)
Experience working with platform integration tool like Snap logic is preferred
Experience Working With AWS (required)
Top 3 technical skills:
Python, Snowflake, PowerBI
Soft Skills Required
Communication, Team-work, Problem Solving, Customer Focus
Please forward your resume and contact details to vahini_b@surgetechinc.com / sahithi_s@surgetechinc.com or can call on 647-294-2946.
Show more
Show less","Snowflake, DB2, SQL Server, Oracle, SQL, Python, SAS, Power BI, Tableau, Snap Logic, AWS","snowflake, db2, sql server, oracle, sql, python, sas, power bi, tableau, snap logic, aws","aws, db2, oracle, powerbi, python, sas, snap logic, snowflake, sql, sql server, tableau"
Senior Data Engineer (On-Site),PrismHR,"Rolling Meadows, IL",https://www.linkedin.com/jobs/view/senior-data-engineer-on-site-at-prismhr-3768113916,2023-12-17,Victoria, Canada,Mid senior,Onsite,"Do you have a passion for building data architectures that enable smooth and seamless product experiences? Are you an all-around data enthusiast with a knack for ETL? We're hiring Data Engineers to help build and optimize the foundational architecture of our product's data.
We’ve built a strong data engineering team to date, but have a lot of work ahead of us, including:
Migrating from relational databases to a streaming and big data architecture, including a complete overhaul of our data feeds
Defining streaming event data feeds required for real-time analytics and reporting
Leveling up our platform, including enhancing our automation, test coverage, observability, alerting, and performance
As a Senior Data Engineer, you will work with the development team to construct a data streaming platform and data warehouse that serves as the data foundations for our product.
Help us scale our business to meet the needs of our growing customer base and develop new products on our platform. You'll be a critical part of our growing company, working on a cross-functional team to implement best practices in technology, architecture, and process. You’ll have the chance to work in an open and collaborative environment, receive hands-on mentorship and have ample opportunities to grow and accelerate your career!
Responsibilities
Build our next generation data warehouse
Build our event stream platform
Translate user requirements for reporting and analysis into actionable deliverables
Enhance automation, operation, and expansion of real-time and batch data environment
Manage numerous projects in an ever-changing work environment
Extract, transform, and load complex data into the data warehouse using cutting-edge technologies
Build processes for topnotch security, performance, reliability, and accuracy
Provide mentorship and collaborate with fellow team members
Requirements
Bachelor’s or Master’s degree in Computer Science, Information Systems, Operations Research, or related field required
5+ years of experience building data pipelines
5+ years of experience building data frameworks for unit testing, data lineage tracking, and automation
Fluency in Scala is required
Working knowledge of Apache Spark
Familiarity with streaming technologies (e.g., Kafka, Kinesis, Flink)
Nice To Have
Experience with Machine Learning
Familiarity with Looker a plus
Knowledge of additional server-side programming languages (e.g. Golang, C#, Ruby)
PrismHR is a fast-paced SaaS company which provides customers with a cloud-based payroll process software application. PrismHR also provides professional services including system implementation consulting, custom configurations, and training. Lastly, via the Company’s Marketplace platform customers and end users access other human resources and employee benefits applications from PrismHR’s Marketplace Partners.
Diversity, Equity And Inclusion Program/Affirmative Action Plan
We have transformed our company into an inclusive environment where individuals are valued for their talents and empowered to reach their fullest potential. At PrismHR, we strive to continually lead with our values and beliefs that enable our employees to develop their potential, bring their full self to work, and engage in a world of inclusion.
Ensuring an inclusive environment for our employees is an integral part of the PrismHR culture. We aren't just checking a box, we are truly committed to creating a workplace that celebrates the diversity of our employees and fosters a sense of belonging for everyone. This is essential to our success. We are dedicated to building a diverse, inclusive, and authentic workplace, so if you’re excited about our roles but your past experience doesn’t align perfectly with every qualification in the job description, we encourage you to apply anyway. You may be just the right candidate for these open roles or other open roles. We particularly encourage applicants from traditionally under-represented groups as we seek to increase the diversity of our workforce and provide fair opportunities for all.
As a proud Equal Opportunity and Affirmative Action Employer, PrismHR encourages talent from all backgrounds to join our team. Employment decisions are based on an individual’s qualifications as they relate to the job under consideration. The Company’s policy prohibits unlawful discrimination based on sex (which includes pregnancy, childbirth, breastfeeding, or related medical conditions, the actual sex of the individual, or the gender identity or gender expression), race, color, religion, including religious dress practices and religious grooming practices, sexual orientation, national origin, ancestry, citizenship, marital status, familial status, age, physical disability, mental disability, medical condition, genetic information, protected veteran or military status, or any other consideration made unlawful by federal, state or local laws, ordinances, or regulations.
The Company is committed to complying with all applicable laws providing equal employment opportunities. This commitment applies to all persons involved in the operations of the Company and prohibits unlawful discrimination by any employee of the Company, including supervisors and co-workers.
Privacy Policy: For information about how we collect and use your personal information, please see our privacy statement available at https://www.prismhr.com/about/privacy-policy.
PrismHR provides reasonable accommodation for qualified individuals with disabilities and disabled veterans in job application procedures. If you have any difficulty using our online system and you need a reasonable accommodation due to a disability, you may use the following alternative email address to contact us about your interest in employment at PrismHR: taglobal@prismhr.com. Please indicate in the subject line of your email that you are requesting accommodation. Only candidates being considered for a position who require an accommodation will receive a follow-up response.
Show more
Show less","Data engineering, ETL, Data streaming, Big data architecture, Data warehouses, Realtime analytics, Observability, Automation, Test coverage, Apache Spark, Kafka, Kinesis, Flink, Scala, Machine learning, Looker, Golang, C#, Ruby, Serverside programming languages","data engineering, etl, data streaming, big data architecture, data warehouses, realtime analytics, observability, automation, test coverage, apache spark, kafka, kinesis, flink, scala, machine learning, looker, golang, c, ruby, serverside programming languages","apache spark, automation, big data architecture, c, data engineering, data streaming, data warehouses, etl, flink, golang, kafka, kinesis, looker, machine learning, observability, realtime analytics, ruby, scala, serverside programming languages, test coverage"
Senior Data Engineer (On-Site),PrismHR,"Chandler, AZ",https://www.linkedin.com/jobs/view/senior-data-engineer-on-site-at-prismhr-3768113934,2023-12-17,Victoria, Canada,Mid senior,Onsite,"Do you have a passion for building data architectures that enable smooth and seamless product experiences? Are you an all-around data enthusiast with a knack for ETL? We're hiring Data Engineers to help build and optimize the foundational architecture of our product's data.
We’ve built a strong data engineering team to date, but have a lot of work ahead of us, including:
Migrating from relational databases to a streaming and big data architecture, including a complete overhaul of our data feeds
Defining streaming event data feeds required for real-time analytics and reporting
Leveling up our platform, including enhancing our automation, test coverage, observability, alerting, and performance
As a Senior Data Engineer, you will work with the development team to construct a data streaming platform and data warehouse that serves as the data foundations for our product.
Help us scale our business to meet the needs of our growing customer base and develop new products on our platform. You'll be a critical part of our growing company, working on a cross-functional team to implement best practices in technology, architecture, and process. You’ll have the chance to work in an open and collaborative environment, receive hands-on mentorship and have ample opportunities to grow and accelerate your career!
Responsibilities
Build our next generation data warehouse
Build our event stream platform
Translate user requirements for reporting and analysis into actionable deliverables
Enhance automation, operation, and expansion of real-time and batch data environment
Manage numerous projects in an ever-changing work environment
Extract, transform, and load complex data into the data warehouse using cutting-edge technologies
Build processes for topnotch security, performance, reliability, and accuracy
Provide mentorship and collaborate with fellow team members
Requirements
Bachelor’s or Master’s degree in Computer Science, Information Systems, Operations Research, or related field required
5+ years of experience building data pipelines
 across engineering and non-engineering roles, including unlimited access to O’Reilly
Giving back – the ability to get involved nationally and regionally with partnerships to get people from diverse backgrounds into tech
You will become part of a squad with people from different areas within the business who will help you grow at BJSS
We have a busy social calendar that you can chose to join– quarterly town halls/squad nights out/weekends away with families included/office get togethers
GymFlex gym membership programme
Show more
Show less","Software engineering, DataOps, Scalable data solutions, Python, Objectoriented programming, CI/CD, Cloud data services, Parallel computing, Data storage, Data processing, Relational databases, Nonrelational databases, AWS, Azure, GCP, Databricks, Data Factory, Synapse, Kafka, Redshift, Glue, Athena, BigQuery, S3, Cloud Data Fusion","software engineering, dataops, scalable data solutions, python, objectoriented programming, cicd, cloud data services, parallel computing, data storage, data processing, relational databases, nonrelational databases, aws, azure, gcp, databricks, data factory, synapse, kafka, redshift, glue, athena, bigquery, s3, cloud data fusion","athena, aws, azure, bigquery, cicd, cloud data fusion, cloud data services, data factory, data processing, data storage, databricks, dataops, gcp, glue, kafka, nonrelational databases, objectoriented programming, parallel computing, python, redshift, relational databases, s3, scalable data solutions, software engineering, synapse"
Senior Data Integration Engineer,Commonwealth Financial Network,"Waltham, MA",https://www.linkedin.com/jobs/view/senior-data-integration-engineer-at-commonwealth-financial-network-3785829702,2023-12-17,Lynn,United States,Mid senior,Hybrid,"Whether you’re looking for a high-energy, inclusive atmosphere and a company that understands the importance of work/life balance, Commonwealth is your match! From generous bonus and 401(k) programs to tuition reimbursement and flexible work schedules, Commonwealth is focused on helping its employees thrive in an environment suited to their needs. On top of all that, the Technology department offers a hybrid work schedule, so you’ll be able to work from home for part of the week!
We’re looking for a senior data integration engineer to join our ranks. This role is responsible for data integration, data collection, and establishing data integration architecture standards. The ideal candidate will also support lake house architecture and many data integration applications that support our Commonwealth business solutions.
Key Responsibilities
Develop and support large-scale data sets and pipelines to ingest, move, transform, and integrate data
Generate insights and advanced analytics reports across Commonwealth business groups
Conduct data exploration, enrichment, and support feature engineering, in data science, data visualization, and business process improvement initiatives
Interface with business-aligned IT teams to design, build, and launch efficient and reliable data pipelines to move and transform data
Meet with application teams to understand requirements and recommend high-level direction and approach to upcoming projects
Ensure the availability, performance, scalability, and security of production environments
Troubleshoot systems and solve problems
Core Strengths And Skills
7+ years’ experience in ELT and real time integration with APIs using various data integration tools
Experience with data modeling, data warehousing, data integration, and data governance tools
Experience creating and managing data pipelines and data sets using Databricks
Experience with data lakehouse architecture using (i.e. Spark, Kafka, Spark & Kafka Streaming, Python, FiveTran, etc.)
Experience with Azure cloud, Azure Data factory, Azure Event Hubs, and serverless solutions
Knowledge of integrating with MDM (Reltio or similar)
Proficiency in SQL and experience with relational databases, NoSQL and graph databases
Knowledge of BI Tools like PowerBIand Tableau
Strong communication skills with both technical and non-technical clients
Have we piqued your curiosity? Can you see yourself thriving in this opportunity? Let’s introduce ourselves.
Picture Yourself Here
At Commonwealth, we believe in a better world. We hold ourselves and each other to higher standards. We take care of one another. That’s why we invest in you—we encourage employee growth both in your career and education; we are building out a robust diversity, equity, and inclusion program; we offer incredible health care benefits; and we find plenty of occasions to celebrate. What’s not to love?
We are always striving to be better, and we are looking for employees who share that same mindset. Better people, better coworkers, better leaders, better creators. Bring your best work and your full self to the table, and we will do the same. Together, we can build a better future for our advisors, their clients, our company, and you.
About Commonwealth
Commonwealth Financial Network, Member FINRA/SIPC, a Registered Investment Adviser, provides a suite of business solutions that empowers more than 2,000 independent financial advisors nationwide. Privately held since 1979, the firm has headquarters in Waltham, Massachusetts, and San Diego, California.
Turning our advisors into raving fans starts by doing the same for our employees. We foster an environment of excellence, growth, rewards, and fun in equal measure, which has earned us 43 Best Place to Work awards.
The Fine Print
We care about your online safety as a prospective employee and encourage you to exercise caution when responding to job postings online. Commonwealth will never ask potential hiring candidates to pay or transfer funds as a precondition of interviews or employment, nor will we authorize recruiters or agents to do so on our behalf.
Commonwealth is an equal opportunity employer, making intentional efforts to source talent from all backgrounds.
Min
USD $125,000.00/Yr.
Max
USD $150,000.00/Yr.
Show more
Show less","Data Integration, Data Collection, Data Integration Architecture, Lake House Architecture, Data Integration Applications, Data Sets, Pipelines, Data Exploration, Data Enrichment, Feature Engineering, Data Visualization, Business Process Improvement, Data Pipelines, Azure Cloud, Azure Data Factory, Azure Event Hubs, Serverless Solutions, MDM, PowerBI, Tableau, SQL, Relational Databases, NoSQL, Graph Databases, BI Tools, Python, FiveTran","data integration, data collection, data integration architecture, lake house architecture, data integration applications, data sets, pipelines, data exploration, data enrichment, feature engineering, data visualization, business process improvement, data pipelines, azure cloud, azure data factory, azure event hubs, serverless solutions, mdm, powerbi, tableau, sql, relational databases, nosql, graph databases, bi tools, python, fivetran","azure cloud, azure data factory, azure event hubs, bi tools, business process improvement, data collection, data enrichment, data exploration, data integration, data integration applications, data integration architecture, data sets, datapipeline, feature engineering, fivetran, graph databases, lake house architecture, mdm, nosql, pipelines, powerbi, python, relational databases, serverless solutions, sql, tableau, visualization"
"Data Scientist / Engineer, Team Lead",Shift Technology,"Boston, MA",https://www.linkedin.com/jobs/view/data-scientist-engineer-team-lead-at-shift-technology-3765357871,2023-12-17,Lynn,United States,Mid senior,Hybrid,"Did you know that about 10% of all insurance payouts are flowing directly into the pockets of fraudsters? The future of insurance starts with Decisions Made Better.
Shift Technology harnesses the power of AI to enable the world’s leading insurance organizations to make better decisions. Our products automate and optimize decisions from underwriting to claims, resulting in increased operational efficiency, reduced costs, and superior customer experiences for millions of people around the globe.
Our culture is built on innovation, trust, and a drive to transform the insurance industry by imagining and innovating solutions that impact insurers and their customers - like you! We come from more than 50 different countries and cultures and together we are creating the future of insurance.
The Data Science team works on a broad range of subjects. We actively participate in the implementation and development of our suite of products on fraud detection, anti-money laundering, and claim automation. We have a breadth of technical and professional experience in data science, data engineering, programming, business understanding, and client interaction. Additionally, we work on various data types such as structured data, free text, documents, and images.
As a Data Science Team Lead at Shift Technology, you will play a pivotal role in leading a team of 3-5 skilled data scientists and actively contribute to the development of data-driven solutions that have a significant impact on the insurance industry. This position is ideal for a Data Scientist with 2-4 years in a technical team lead or mentor role who excels in both technical expertise and leadership capabilities.
Our company is small enough that each person’s achievements have an impact on overall performance, yet big enough to be a world leader in our domain.
What You Will Do
You will lead and mentor a team of data scientists, providing technical guidance, support, and career development opportunities
You will actively contribute to the development and improvements of our fraud detection models, and write and maintain code in C# and SQL.
You will clean, preprocess, and analyze large datasets to find creative methods to detect large scale fraud rings, involving medical providers, identity thefts, fake claims, often worth several million dollars of fraudulent payments.
You will manage the team's priorities and lead project planning, execution, and delivery, ensuring that projects are completed within deadlines and meet high-quality standards.
You will oversee the implementation and deployment of our models into production, ensuring scalability, reliability, and performance.
You will work closely with our clients to understand their business and organizational context, present results and discuss the performances of our models and ensure our products add value within their organizational priorities and constraints
You will collaborate with cross-functional teams (Product Management, Project Management, Software Engineering, Customer Success Management, etc.), providing technical expertise and leadership to drive innovative solutions that meet our clients' needs.
You will support our Sales and Value Engineering teams, and present demos of our products to prospects.
This is a Hybrid role based in Boston, MA and the selected candidate will need to be able to work onsite at least once a week.
What You Will Bring
Bachelor’s or advanced Degree in Data Science, Computer Science or a related field
3+ years as an individual contributor in Data Science, Data Engineering, Data Analytics or Software Engineering field with a track record of delivering impactful data-driven solutions in a business context.
Strong proficiency in programming, and, experience in coding with an object-oriented language (Java, C#, C++, etc.)
Prior experience in a leadership or supervisory role, demonstrating the ability to lead, mentor, and inspire a team
Excellent communication and presentation skills, with the ability to convey complex ideas to both technical and non-technical stakeholders
Excellent problem-solving skills and the ability to translate complex findings into actionable insights
Ability to manage multiple priorities and adapt to a rapidly changing environment.
Ability to structure a complex problem in an ambiguous context with unknown factors.
Nice to have:
Prior experience with C#
Prior experience in a SaaS company
Prior experience in the Insurance/ InsurTech industry
Prior experience as a Data Science Team Lead or Manager
Interview Process:
HR Interview
Interview 1: Interview with a Data Science Manager
Interview 2: Technical Interview with a Tech Lead
Interview 3: Head of Data Science - US P&C + Office Meeting
Reference call
The range listed is for base compensation. Your actual base salary will vary based on factors including location and individual qualifications objectively assessed during the interview process.
In addition to base salary, your total rewards package will include additional components such as incentive pay, equity, and benefits. If you're interviewing for this role, speak with your Talent Acquisition Partner to learn more about the specific details for this position.
US Base Salary Pay Range
$125,000—$140,000 USD
To support our employees at every stage of their careers and lives, we provide a competitive total rewards and benefits package. Here are the global benefits we’d like to highlight:
Flexible remote and hybrid working options
Competitive Salary and a variable component tied to personal and company performance
Company equity
Focus Fridays, a half-day each month to focus on learning and personal growth
Generous PTO and paid holidays
Mental health benefits
2 MAD Days per year (Make A Difference Days for paid volunteering)
Additional benefits may be offered by country - ask your recruiter for more information.
At Shift we strive to be a diverse and inclusive workforce. We hire and trust people without regard to race, color, religion, marital status, age, national or ethnic origin, physical or mental disability, medical condition, pregnancy, genetic information, gender identity or expression, sexual orientation, or other non-merit criteria.
Shift Technology is committed to providing reasonable accommodations for qualified individuals with disabilities in our application and employment process. Should you require accommodation, please email accommodation@shift-technology.com and we will work with you to meet your accessibility needs.
Shift Technology does not accept unsolicited CVs from recruiters or employment agencies in response to the Shift Technology Careers page or a Shift Technology social media post. Any unsolicited CVs, including those submitted directly to hiring managers, are deemed to be the property of Shift Technology.
#LI-HYBRID
#LI-MG1
Show more
Show less","Data Science, Machine Learning, AI, Fraud Detection, AntiMoney Laundering, Claim Automation, Data Engineering, Programming, C#, SQL, Data Analytics, Software Engineering, Java, C++, ObjectOriented Programming, Linux, SaaS, Team Lead, Mentoring, Communication, Presentation, ProblemSolving, Adaptability, Complex Problem Solving","data science, machine learning, ai, fraud detection, antimoney laundering, claim automation, data engineering, programming, c, sql, data analytics, software engineering, java, c, objectoriented programming, linux, saas, team lead, mentoring, communication, presentation, problemsolving, adaptability, complex problem solving","adaptability, ai, antimoney laundering, c, claim automation, communication, complex problem solving, data engineering, data science, dataanalytics, fraud detection, java, linux, machine learning, mentoring, objectoriented programming, presentation, problemsolving, programming, saas, software engineering, sql, team lead"
Data Engineer,Syrinx Consulting,"Boston, MA",https://www.linkedin.com/jobs/view/data-engineer-at-syrinx-consulting-3684753340,2023-12-17,Lynn,United States,Mid senior,Hybrid,"Summary
Our client is seeking a Data Engineer to join their internal Data Team. The team is responsible for leading efforts to ingest, structure, and model our data to support critical decision making. Our enterprise data warehouse is based on AWS Redshift, supporting Looker analytical dashboards, and is backed by an AWS data lake. In addition to analytics, the data warehouse also supports an active and growing data science practice.
The ideal candidate will have experience building and supporting data platforms that help end users make sound business decisions. This includes data pipeline creation, data quality management, system reliability, job orchestration and recovery aka “Data Ops”. In additional to technical competency, they should be truly motivated to empower end users and help our client meet its goals.
Responsibilities
Implement data pipelines using agile, iterative processes to deliver value quickly
Treat data quality, reliability, and documentation as fundamental acceptance criteria for data pipelines
Collaborate with the team to make the best technology choices
Deliver solutions that ensure data accuracy, high availability, robust security, and that are built for rapid scaling
Create and maintain data models, data catalogues, and data security
Partner with data analysts to implement features that serve client’s demand for reliable information
Qualifications
5+ years experience with data engineering or data warehousing, or an equivalent data-oriented software engineering background
3+ years experience with ETL technologies (e.g. Talend, Informatica, Matillion)
3+ years experience with key AWS technologies (Redshift, S3, IAM; also Kinesis, Lambda, EMR, Spark, Hive)
5+ years of experience with SQL
Experience in optimizing queries and tuning database performance
Familiarity with data analysis and data science tools (Looker, Python, Jupyter, R) is a plus
Python or Java experience is a plus
Behaviors/ Traits
Understands business value, context and priorities of the team's work
Collaborates in scoping, solution design and evaluation
Adheres to team priorities
Assists other team members in developing, testing and deploying software changes
Engages problems directly and creatively
Respectfully and constructively engages team members, stakeholders, and the business community
Drives for results with focus and energy
As a member of the client’s Team you get a great benefits package including health and dental insurance, 401k, vacation time, paid holidays, personal and sick leave, along with a full complement of other insurance and support programs.
Show more
Show less","Data Engineering, Data Warehousing, Agile, Data Pipelines, Data Quality Management, Data Modeling, Data Catalogues, Data Security, Looker, Talend, Informatica, Matillion, Redshift, S3, IAM, Kinesis, Lambda, EMR, Spark, Hive, SQL, Python, Java, Jupyter, R","data engineering, data warehousing, agile, data pipelines, data quality management, data modeling, data catalogues, data security, looker, talend, informatica, matillion, redshift, s3, iam, kinesis, lambda, emr, spark, hive, sql, python, java, jupyter, r","agile, data catalogues, data engineering, data quality management, data security, datamodeling, datapipeline, datawarehouse, emr, hive, iam, informatica, java, jupyter, kinesis, lambda, looker, matillion, python, r, redshift, s3, spark, sql, talend"
Data Engineer,Syrinx Consulting,"Boston, MA",https://www.linkedin.com/jobs/view/data-engineer-at-syrinx-consulting-3648826930,2023-12-17,Lynn,United States,Mid senior,Hybrid,"Summary
Our client is seeking a Data Engineer to join their internal Data Team. The team is responsible for leading efforts to ingest, structure, and model our data to support critical decision making. Our enterprise data warehouse is based on AWS Redshift, supporting Looker analytical dashboards, and is backed by an AWS data lake. In addition to analytics, the data warehouse also supports an active and growing data science practice.
The ideal candidate will have experience building and supporting data platforms that help end users make sound business decisions. This includes data pipeline creation, data quality management, system reliability, job orchestration and recovery aka “Data Ops”. In additional to technical competency, they should be truly motivated to empower end users and help our client meet its goals.
Responsibilities
Implement data pipelines using agile, iterative processes to deliver value quickly
Treat data quality, reliability, and documentation as fundamental acceptance criteria for data pipelines
Collaborate with the team to make the best technology choices
Deliver solutions that ensure data accuracy, high availability, robust security, and that are built for rapid scaling
Create and maintain data models, data catalogues, and data security
Partner with data analysts to implement features that serve client’s demand for reliable information
Qualifications
5+ years experience with data engineering or data warehousing, or an equivalent data-oriented software engineering background
3+ years experience with ETL technologies (e.g. Talend, Informatica, Matillion)
3+ years experience with key AWS technologies (Redshift, S3, IAM; also Kinesis, Lambda, EMR, Spark, Hive)
5+ years of experience with SQL
Experience in optimizing queries and tuning database performance
Familiarity with data analysis and data science tools (Looker, Python, Jupyter, R) is a plus
Python or Java experience is a plus
Behaviors/ Traits
Understands business value, context and priorities of the team's work
Collaborates in scoping, solution design and evaluation
Adheres to team priorities
Assists other team members in developing, testing and deploying software changes
Engages problems directly and creatively
Respectfully and constructively engages team members, stakeholders, and the business community
Drives for results with focus and energy
As a member of the client’s Team you get a great benefits package including health and dental insurance, 401k, vacation time, paid holidays, personal and sick leave, along with a full complement of other insurance and support programs.
Show more
Show less","Data Engineering, Data Warehousing, Talend, Informatica, Matillion, AWS, Redshift, S3, IAM, Kinesis, Lambda, EMR, Spark, Hive, SQL, Looker, Python, Jupyter, R, Agile, Iterative Processes, Data Quality, Data Reliability, Data Documentation, Data Pipelines, Data Models, Data Catalogues, Data Security, Data Analysis, Data Science","data engineering, data warehousing, talend, informatica, matillion, aws, redshift, s3, iam, kinesis, lambda, emr, spark, hive, sql, looker, python, jupyter, r, agile, iterative processes, data quality, data reliability, data documentation, data pipelines, data models, data catalogues, data security, data analysis, data science","agile, aws, data catalogues, data documentation, data engineering, data models, data quality, data reliability, data science, data security, dataanalytics, datapipeline, datawarehouse, emr, hive, iam, informatica, iterative processes, jupyter, kinesis, lambda, looker, matillion, python, r, redshift, s3, spark, sql, talend"
Data Analyst 2 - 51574,New York State Department of Health,"Albany, NY",https://www.linkedin.com/jobs/view/data-analyst-2-51574-at-new-york-state-department-of-health-3778602190,2023-12-17,Albany,United States,Mid senior,Onsite,"Minimum Qualifications
Non-competitive: Bachelor’s degree in data or computer science, informatics, economics, statistics, mathematics, or a related field, and three years of experience* in data visualization, and statistical models and data mining tools.
Substitution: A master’s degree in the above fields may substitute for one year of the specialized experience. A Ph.D. in the above fields may substitute for two years of the specialized experience.
Preferred Qualifications: Experience with IT systems and/or large, population-based health related data sets (e.g. SPARCS, ECLRS, CDESS), disease registries (e.g. NYSIIS), working with confidential data, public health, and analytics with relational databases. Additional experiences include use of and supervising programming tools such as SAS and SQL (or R, Python) and data analysis using relational databases.
Duties Description
The Data Analyst 2 is needed for the coordination of the Bureau's syndromic surveillance efforts, focused on cannabis-related conditions, healthcare utilization, and health outcomes for both pediatric and adult populations. The Data Analyst 2 will validate and refine syndrome definitions and chief complaint filters (including cannabis abuse, dependence, and use, cannabis poisoning, cannabinoid hyperemesis syndrome, and other cannabis-related injuries and conditions), and monitoring incoming HL7 data feeds for completeness and timeliness. The Data Analyst 2 position will monitor reports and alerts for detecting cannabis-related condition spikes and outbreaks. The Data Analyst 2 will maintain the Electronic Syndromic Surveillance System (ESSS) report module, including user acceptance testing, updating the user guide, and training users related to cannabis-use related syndromes.
Additional Comments
We offer a work-life balance and a generous benefits package, worth 65% of salary, including:
Holiday & Paid Time Off
Public Service Loan Forgiveness (PSLF)
Pension from New York State Employees’ Retirement System
Shift & Geographic pay differentials
Affordable Health Care options
Family dental and vision benefits at no additional cost
NYS Deferred Compensation plan
Access to NY 529 and NY ABLE College Savings Programs, and U.S. Savings Bonds
And many more...
For new State employees appointed to graded positions, the annual salary is the hiring rate (beginning of the Salary Range) of the position. Promotion salaries are calculated by the NYS Office of the State Comptroller in accordance with NYS Civil Service Law, OSC Payroll rules and regulations and negotiated union contracts.
The NYS Department of Health is committed to making New York a safer, healthier, and more equitable place to live. Understanding health equity, social determinants of health and health disparities is critical to accomplish our goal of eliminating health disparities. For more information on the NYS Department of Health’s Mission, Vision, Values and Strategic Plan, please visit: https://health.ny.gov/commissioner/index.htm
Show more
Show less","Data visualization, Statistical models, Data mining, IT systems, Populationbased health related data sets (SPARCS ECLRS CDESS), Disease registries (NYSIIS), Public health, Analytics, Relational databases, SAS, SQL, R, Python, HL7, Electronic Syndromic Surveillance System (ESSS), User acceptance testing, User guide maintenance, Training","data visualization, statistical models, data mining, it systems, populationbased health related data sets sparcs eclrs cdess, disease registries nysiis, public health, analytics, relational databases, sas, sql, r, python, hl7, electronic syndromic surveillance system esss, user acceptance testing, user guide maintenance, training","analytics, data mining, disease registries nysiis, electronic syndromic surveillance system esss, hl7, it systems, populationbased health related data sets sparcs eclrs cdess, public health, python, r, relational databases, sas, sql, statistical models, training, user acceptance testing, user guide maintenance, visualization"
Data Analyst 3 - 07275,New York State Department of Health,"Albany, NY",https://www.linkedin.com/jobs/view/data-analyst-3-07275-at-new-york-state-department-of-health-3782536939,2023-12-17,Albany,United States,Mid senior,Onsite,"Minimum Qualifications
Non-competitive: Bachelor’s degree in data or computer science, informatics, economics, statistics, mathematics, or a related field, and five years of experience* in data visualization, and statistical models and data mining tools.
Substitution: A master’s degree in the above fields may substitute for one year of the specialized experience. A Ph.D. in the above fields may substitute for two years of the specialized experience.
Preferred Qualifications: Experience with IT systems and/or large, population-based health related data sets (e.g. SPARCS, ECLRS, CDESS), disease registries (e.g. NYSIIS), working with confidential data, public health, and analytics with relational databases. Additional experiences include use of and supervising programming tools such as SAS and SQL (or R, Python) and data analysis using relational databases.
Duties Description
The incumbent will oversee business intelligence solutions using the Bureau of Surveillance and Data Systems (BSDS) mission-critical surveillance systems. They will design and develop metrics to meet complex agency surveillance and reporting needs, including for agency monitoring, external reporting, and grant tracking activities. They will also develop programs to match with other applicable databases (e.g., SPARCS) to enable transforming raw data into intelligence, analytics, and reports. The incumbent will direct and conduct complex ad hoc data analyses and data-related guidance requested by internal and external partners using SAS/SQL or other related software. They will oversee quality assurance (QA) checks to ensure accuracy in developed metrics, reports, and ad-hoc analyses, as well as the QA associated with data in BSDS systems. The incumbent will oversee and contribute to reporting and analyses of process and outcome metrics as needed to generate evaluation results and findings. They will develop presentations, abstracts, reports on analytics and findings, and technical/methodological write-ups/manuscripts. The incumbent will ensure the use of analytics and visualization of data to detect emerging threats and to inform and evaluate interventions. They will supervise data analyst team staff in the Bureau, ensuring deadlines and grant deliverables are met.
Additional Comments
We offer a work-life balance and a generous benefits package, worth 65% of salary, including:
Holiday & Paid Time Off
Public Service Loan Forgiveness (PSLF)
Pension from New York State Employees’ Retirement System
Shift & Geographic pay differentials
Affordable Health Care options
Family dental and vision benefits at no additional cost
NYS Deferred Compensation plan
Access to NY 529 and NY ABLE College Savings Programs, and U.S. Savings Bonds
And many more...
For new State employees appointed to graded positions, the annual salary is the hiring rate (beginning of the Salary Range) of the position. Promotion salaries are calculated by the NYS Office of the State Comptroller in accordance with NYS Civil Service Law, OSC Payroll rules and regulations and negotiated union contracts.
The NYS Department of Health is committed to making New York a safer, healthier, and more equitable place to live. Understanding health equity, social determinants of health and health disparities is critical to accomplish our goal of eliminating health disparities. For more information on the NYS Department of Health’s Mission, Vision, Values and Strategic Plan, please visit: https://health.ny.gov/commissioner/index.htm
Show more
Show less","Data visualization, Statistical models, Data mining, IT systems, SAS, SQL, R, Python, Relational databases, SAS/SQL, Quality assurance, Data analysis, Process metrics, Outcome metrics, Analytics, Data mining tools, Public health, Relational databases, SPARCS, ECLRS, CDESS, NYSIIS, Hadoop, Tableau, Power BI","data visualization, statistical models, data mining, it systems, sas, sql, r, python, relational databases, sassql, quality assurance, data analysis, process metrics, outcome metrics, analytics, data mining tools, public health, relational databases, sparcs, eclrs, cdess, nysiis, hadoop, tableau, power bi","analytics, cdess, data mining, data mining tools, dataanalytics, eclrs, hadoop, it systems, nysiis, outcome metrics, powerbi, process metrics, public health, python, quality assurance, r, relational databases, sas, sassql, sparcs, sql, statistical models, tableau, visualization"
Data Engineer Terraform ETL,Vedsoft,"Durham, NC",https://www.linkedin.com/jobs/view/data-engineer-terraform-etl-3768099943,2023-12-17,Albany,United States,Mid senior,Hybrid,"No corp to corp
need candidates on our direct W2 payroll
$75-85/hr W2
long term contract year+
Hybrid- 1 week/ month on site rest remote
Data Engineer Smithfield RI or Durham NC.
MUST HAVE: heavy Python experience building ETL pipelines (Do not need an Informatica developer), AWS, SQL, automation knowledge or experience Terraform/CFT.
We are currently sourcing for a Data Engineer (Python / ETL / AWS / SQL) work location in Smithfield, RI, or Durham, NC!
Show more
Show less","Python, ETL, AWS, SQL, Terraform, CloudFormation","python, etl, aws, sql, terraform, cloudformation","aws, cloudformation, etl, python, sql, terraform"
"Data Engineer, Data Platform",Grammarly,"Washington, United States",https://www.linkedin.com/jobs/view/data-engineer-data-platform-at-grammarly-3689960959,2023-12-17,Albany,United States,Mid senior,Hybrid,"Grammarly is excited to offer a
remote-first hybrid working model
. Team members work primarily remotely in the United States, Canada, Ukraine, Germany, or Poland. Certain roles have specific location requirements to facilitate collaboration at a particular Grammarly hub.
All roles have an in-person component: Conditions permitting, teams meet 2–4 weeks every quarter at one of Grammarly’s hubs in San Francisco, Kyiv, New York, Vancouver, and Berlin, or in a workspace in Kraków.
This flexible approach gives team members the best of both worlds: plenty of focus time along with in-person collaboration that fosters trust and unlocks creativity.
Grammarly team members in this role must be based in the United States or Canada, and they must be able to collaborate in person 2 weeks per quarter, traveling if necessary to the hub(s) where the team is based.
The opportunity
Grammarly is the world’s leading AI writing assistance company trusted by over 30 million people and 70,000 professional teams every day. From instantly creating a first draft to perfecting every message, Grammarly’s product offerings help people at 96% of the Fortune 500 get their point across—and get results. Grammarly has been profitable for over a decade because we’ve stayed true to our values and built an enterprise-grade product that’s secure, reliable, and helps people do their best work—without selling their data. We’re proud to be one of Inc.’s best workplaces, a Glassdoor Best Place to Work, one of TIME’s 100 Most Influential Companies, and one of Fast Company’s Most Innovative Companies in AI.
To achieve our ambitious goals, we’re looking for a Data Engineer to join our Data Engineering Platform team. This person will build highly automated, low latency core datasets that will help data engineers and end users across Grammarly to work with analytical data at scale.
Grammarly’s engineers and researchers have the freedom to innovate and uncover breakthroughs—and, in turn, influence our product roadmap. The complexity of our technical challenges is growing rapidly as we scale our interfaces, algorithms, and infrastructure. Read more about our stack or hear from our team on our technical blog.
Your impact
As a Data Engineer on our Data Engineering Platform team, you will:
Drive improvements to make our analytics effortless by creating and adjusting core data models and storage structures, all while understanding the needs of our users.
Make analytical data and metrics usable within a few minutes of real world events occuring, and build streaming processes for the output derived events and aggregate data.
Model structure, storage, and access of data at very high volumes for our data lakehouse.
Improve developer productivity and self-serve solutions by contributing components to our stream data processing framework(s).
Own data engineering's infrastructure-as-code for provisioning services that allow our engineers to deploy mature software installations within a few hours.
Build a world-class process that will allow our systems to scale.
Mentor other back-end engineers on the team and help them grow.
Build and contribute to AWS high-scale distributed systems on the back-end.
We’re Looking For Someone Who
Embodies our EAGER values—is ethical, adaptable, gritty, empathetic, and remarkable.
Is inspired by our MOVE principles, which are the blueprint for how things get done at Grammarly: move fast and learn faster, obsess about creating customer value, value impact over activity, and embrace healthy disagreement rooted in trust.
Is able to collaborate in person 2 weeks per quarter, traveling if necessary to the hub where the team is based.
Has experience with Python, Scala, or Java.
Has experience with designing database objects and writing relational queries
Has experience designing and standing up APIs and services.
Has experience with system design and building internal tools.
Has experience handling applications that work with data from data lakes.
Has at least some experience building internal Admin sites.
Has good knowledge of and at least some experience with AWS (or, alternatively, has deep expertise in Azure or GCE and is willing to learn AWS in a short time frame).
Can knowledgeably choose an open source or third-party service to accomplish what they need or, alternatively, can devise a quick and simple solution on their own.
Support for you, professionally and personally
Professional growth: We believe that autonomy and trust are key to empowering our team members to do their best, most innovative work in a way that aligns with their interests, talents, and well-being. We support professional development and advancement with training, coaching, and regular feedback.
A connected team: Grammarly builds a product that helps people connect, and we apply this mindset to our own team. Our remote-first hybrid model enables a highly collaborative culture supported by our EAGER (ethical, adaptable, gritty, empathetic, and remarkable) values. We work to foster belonging among team members in a variety of ways. This includes our employee resource groups, Grammarly Circles, which promote connection among those with shared identities, such as BIPOC and LGBTQIA+ team members, women, and parents. We also celebrate our colleagues and accomplishments with global, local, and team-specific programs.
Compensation And Benefits
Grammarly offers all team members competitive pay along with a benefits package encompassing the following and more:
Excellent health care (including a wide range of medical, dental, vision, mental health, and fertility benefits)
Disability and life insurance options
401(k) and RRSP matching
Paid parental leave
Twenty days of paid time off per year, eleven days of paid holidays per year, and unlimited sick days
Home office stipends
Caregiver and pet care stipends
Wellness stipends
Admission discounts
Learning and development opportunities
Grammarly takes a market-based approach to compensation, which means base pay may vary depending on your location. Our US and Canada locations are categorized into compensation zones based on each geographic region’s cost of labor index. For more information about our compensation zones and locations where we currently support employment, please refer to this page. If a location of interest is not listed, please speak with a recruiter for additional information.
Base pay may vary considerably depending on job-related knowledge, skills, and experience. The expected salary ranges for this position are outlined below by compensation zone and may be modified in the future.
United States
Zone 1: $167,000 - $242,000/year (USD)
Zone 2: $150,000 – $218,000/year (USD)
Zone 3: $142,000 – $206,000/year (USD)
Zone 4: $134,000 – $194,000/year (USD)
We encourage you to apply
At Grammarly, we value our differences, and we encourage all—especially those whose identities are traditionally underrepresented in tech organizations—to apply. We do not discriminate on the basis of race, religion, color, gender expression or identity, sexual orientation, ancestry, national origin, citizenship, age, marital status, veteran status, disability status, political belief, or any other characteristic protected by law. Grammarly is an equal opportunity employer and a participant in the US federal E-Verify program (US). We also abide by the Employment Equity Act (Canada).
Please note that EEOC is optional and specific to US-based candidates.
#NA
All team members meeting in person for official Grammarly business or working from a hub location are strongly encouraged to be vaccinated against COVID-19.
Show more
Show less","Python, Scala, Java, SQL, AWS, Cloud computing, Data engineering, Data modeling, Data processing, Data analysis, ETL, APIs, Microservices, System design, Internal tools, Open source software, Linux, Unix, Agile, DevOps, Continuous integration, Continuous delivery, Testdriven development, Version control, Collaboration, Communication, Problemsolving, Critical thinking, Analytical skills, Attention to detail","python, scala, java, sql, aws, cloud computing, data engineering, data modeling, data processing, data analysis, etl, apis, microservices, system design, internal tools, open source software, linux, unix, agile, devops, continuous integration, continuous delivery, testdriven development, version control, collaboration, communication, problemsolving, critical thinking, analytical skills, attention to detail","agile, analytical skills, apis, attention to detail, aws, cloud computing, collaboration, communication, continuous delivery, continuous integration, critical thinking, data engineering, data processing, dataanalytics, datamodeling, devops, etl, internal tools, java, linux, microservices, open source software, problemsolving, python, scala, sql, system design, testdriven development, unix, version control"
Data Engineer,Advanced Software Talent,"Oceanside, CA",https://www.linkedin.com/jobs/view/data-engineer-at-advanced-software-talent-3762632671,2023-12-17,Albany,United States,Mid senior,Hybrid,"Only local candidates! Hybrid contract - 2 to 3 days a week onsite
Direct W2 employees only! No 3rd party agencies.
Start date: 01/02/2024
Data Engineer with responsibility to configure non-GMP computer systems used by Manufacturing Science & Technology (MSAT) department staff to organize manufacturing process information and to assist with biop rocess monitoring and analysis.
This employee should be passionate about the intersection of data and life sciences. They should actively apply both data engineering and software development skills to solve problems.
Job Responsibilities
Work closely with MSAT staff to understand workflows and questions that originate in manufacturing and laboratory environments.
Work as part of a larger data engineering team to develop analysis computer systems and tools with the overall goal to improve E2E operational performance (e.g., capacity to be freed up, quality, yield, productivity, lead times; exact KPIs) at various manufacturing sites across the globe.
Standard data engineering tasks: obtain data extracts and define secure data exchange approaches, perform data quality checks and contribute to the data pipeline.
Acquire, ingest, and process data from multiple sources and systems into Big Data platforms.
Collaborate with data scientists to map data fields to hypotheses and curate, wrangle, and prepare data for use in their advanced analytical models.
Skills required:
4 years plus experience with Python in a Data Analyst role
Experience with the following technologies: Distributed Processing (Spark, Hadoop, EMR), traditional RDBMS (MS SQL Server, Oracle, MySQL, PostgreSQL), MPP (AWS Redshift, Teradata), NoSQL (MongoDB, DynamoDB, Cassandra, Neo4J, Titan). Preference is Spark
Experience and interest in Cloud platforms
Experience in traditional data warehousing and deploying ETL processes with Python.
Skills desired:
1 year of work experience in current good manufacturing practice bio pharmaceutical production setting (process development and/or manufacturing technical support).
Some knowledge of statistical process control and data analysis techniques
The ideal candidate will have a general knowledge of the underlying scientific principles applied to the development and manufacture of bio pharmaceuticals. They will have a keen interest in learning bio process operations.
Proficiency in data analysis techniques and a willingness to learn new techniques is desired.
Ability to work across structured, semi-structured, and unstructured data, extracting information and identifying linkages across disparate data sets
BS or higher degree in Chemical Engineering, Biochemical Engineering, Biochemistry, Computer Science, related field or equivalent work experience
Show more
Show less","Python, Spark, Hadoop, EMR, MS SQL Server, Oracle, MySQL, PostgreSQL, AWS Redshift, Teradata, MongoDB, DynamoDB, Cassandra, Neo4J, Titan, Cloud platforms, ETL, Statistical process control, Data analysis, Chemical Engineering, Biochemical Engineering, Biochemistry, Computer Science","python, spark, hadoop, emr, ms sql server, oracle, mysql, postgresql, aws redshift, teradata, mongodb, dynamodb, cassandra, neo4j, titan, cloud platforms, etl, statistical process control, data analysis, chemical engineering, biochemical engineering, biochemistry, computer science","aws redshift, biochemical engineering, biochemistry, cassandra, chemical engineering, cloud platforms, computer science, dataanalytics, dynamodb, emr, etl, hadoop, mongodb, ms sql server, mysql, neo4j, oracle, postgresql, python, spark, statistical process control, teradata, titan"
Sr. Data Engineer,ConsultNet,Salt Lake City Metropolitan Area,https://www.linkedin.com/jobs/view/sr-data-engineer-at-consultnet-3777089400,2023-12-17,Albany,United States,Mid senior,Hybrid,"Senior Data Engineer/Snowflake
Salt Lake City, UT - hybrid (2 days/week onsite)
Direct Hire: $120-$130k annual salary + bonus
Our client is in search of a Senior Data Engineer to join their growing team. You will play a key role in the design and development of our client's data infrastructure. You will ensure that the organization has access to reliable and high-quality data. Strong experience with Snowflake required.
Required Skills
5+ years of data engineering experience
2+ years of recent hands-on experience with Snowflake
Strong SQL skills
Experience with Business Intelligence
DVT (Design Validation Test) experience
dbt (data build tool) experience
Bonus Skills:
Familiar with the Kimball Method
Python
Show more
Show less","Data Engineering, Snowflake, SQL, Business Intelligence, DVT (Design Validation Test), dbt (data build tool), Kimball Method, Python","data engineering, snowflake, sql, business intelligence, dvt design validation test, dbt data build tool, kimball method, python","business intelligence, data engineering, dbt data build tool, dvt design validation test, kimball method, python, snowflake, sql"
GCP Data Architect- Hadoop,Keylent Inc,"Dearborn, MI",https://www.linkedin.com/jobs/view/gcp-data-architect-hadoop-at-keylent-inc-3768059954,2023-12-17,Wyandotte,United States,Mid senior,Onsite,"Position :- GCP Data Architect- Hadoop
Location :- Dearborn MI (Day 1 onsite)
Duration: 12 months
Skills: GCP Professional cloud architect
Good to have GCP Certification (Either GCP Data Engineer or GCP Cloud Architect)
15+ years of experience in Architecting Data projects and knowledge of multiple Hadoop/Hive/Spark/Client implementation
5+ experience in Data modeling and Data warehouse and Data lake implementation
Working experience in implementing Hadoop to GCS and HIVE to Bigquery migration project
Ability to identify and gather requirements to define a solution to be built and operated on GCP, perform high-level and low-level design for the GCP platform
Capabilities to implement and provide GCP operations and deployment guidance and best practices throughout the lifecycle of a project.
GCP technology areas of Data store, Big Query, Cloud storage, Persistent disk IAM, Roles, Projects, Organization.
Databases including Big table, Cloud SQL, Cloud Spanner, Memory store, Data Analytics Data Flow, DataProc, Cloud Pub/Sub, Kubernetes, Docker, managing containers, container auto scaling and container security
GCP technology areas of Data store, Big Query, Cloud storage, Persistent disk IAM, Roles, Projects, Organization.
Databases including Big table, Cloud SQL, Cloud Spanner, Memory store, Data Analytics Data Flow, DataProc, Cloud Pub/Sub, Kubernetes, Docker, managing containers, container auto scaling and container security
Experience in Design, Deployment, configuration and Integration of application infrastructure resources including GKE clusters, Anthos, APIGEE and DevOps Platform
Application development concepts and technologies (e.g. CI/CD, Java, Python)
Capabilities to implement and provide GCP operations and deployment guidance and best practices throughout the lifecycle of a project.
Show more
Show less","GCP, Cloud Architect, Hadoop, Hive, Spark, GCP Data Engineer, Data Modeling, Data Warehouse, Data Lake, GCS, HIVE, Bigquery, Data store, Big Query, Cloud storage, Persistent disk, IAM, Roles, Projects, Organization, Big table, Cloud SQL, Cloud Spanner, Memory store, Data Analytics, Data Flow, DataProc, Cloud Pub/Sub, Kubernetes, Docker, GKE clusters, Anthos, APIGEE, DevOps Platform, CI/CD, Java, Python","gcp, cloud architect, hadoop, hive, spark, gcp data engineer, data modeling, data warehouse, data lake, gcs, hive, bigquery, data store, big query, cloud storage, persistent disk, iam, roles, projects, organization, big table, cloud sql, cloud spanner, memory store, data analytics, data flow, dataproc, cloud pubsub, kubernetes, docker, gke clusters, anthos, apigee, devops platform, cicd, java, python","anthos, apigee, big query, big table, bigquery, cicd, cloud architect, cloud pubsub, cloud spanner, cloud sql, cloud storage, data flow, data lake, data store, dataanalytics, datamodeling, dataproc, datawarehouse, devops platform, docker, gcp, gcp data engineer, gcs, gke clusters, hadoop, hive, iam, java, kubernetes, memory store, organization, persistent disk, projects, python, roles, spark"
GCP Data Architect- Hadoop,Keylent Inc,"Dearborn, MI",https://www.linkedin.com/jobs/view/gcp-data-architect-hadoop-at-keylent-inc-3768061865,2023-12-17,Wyandotte,United States,Mid senior,Onsite,"Position :- GCP Data Architect- Hadoop
Location :- Dearborn MI (Day 1 onsite)
Duration: 12 Months
Good to have GCP Certification (Either GCP Data Engineer or GCP Cloud Architect)
15+ years of experience in Architecting Data projects and knowledge of multiple Hadoop/Hive/Spark/Client implementation
5+ experience in Data modeling and Data warehouse and Data lake implementation
Working experience in implementing Hadoop to GCS and HIVE to Bigquery migration project
Ability to identify and gather requirements to define a solution to be built and operated on GCP, perform high-level and low-level design for the GCP platform
Capabilities to implement and provide GCP operations and deployment guidance and best practices throughout the lifecycle of a project.
GCP technology areas of Data store, Big Query, Cloud storage, Persistent disk IAM, Roles, Projects, Organization.
Databases including Big table, Cloud SQL, Cloud Spanner, Memory store, Data Analytics Data Flow, DataProc, Cloud Pub/Sub, Kubernetes, Docker, managing containers, container auto scaling and container security
GCP technology areas of Data store, Big Query, Cloud storage, Persistent disk IAM, Roles, Projects, Organization.
Databases including Big table, Cloud SQL, Cloud Spanner, Memory store, Data Analytics Data Flow, DataProc, Cloud Pub/Sub, Kubernetes, Docker, managing containers, container auto scaling and container security
Experience in Design, Deployment, configuration and Integration of application infrastructure resources including GKE clusters, Anthos, APIGEE and DevOps Platform
Application development concepts and technologies (e.g. CI/CD, Java, Python)
Capabilities to implement and provide GCP operations and deployment guidance and best practices throughout the lifecycle of a project.
Show more
Show less","Data Architect, GCP Data Engineer, GCP Cloud Architect, Hadoop, Hive, Spark, GCS, HIVE, Bigquery, Data modeling, Data warehouse, Data lake, Data store, Big Query, Cloud storage, Persistent disk, IAM, Roles, Projects, Organization, Big table, Cloud SQL, Cloud Spanner, Memory store, Data Analytics Data Flow, DataProc, Cloud Pub/Sub, Kubernetes, Docker, Container auto scaling, Container security, Anthos, APIGEE, CI/CD, Java, Python","data architect, gcp data engineer, gcp cloud architect, hadoop, hive, spark, gcs, hive, bigquery, data modeling, data warehouse, data lake, data store, big query, cloud storage, persistent disk, iam, roles, projects, organization, big table, cloud sql, cloud spanner, memory store, data analytics data flow, dataproc, cloud pubsub, kubernetes, docker, container auto scaling, container security, anthos, apigee, cicd, java, python","anthos, apigee, big query, big table, bigquery, cicd, cloud pubsub, cloud spanner, cloud sql, cloud storage, container auto scaling, container security, data analytics data flow, data architect, data lake, data store, datamodeling, dataproc, datawarehouse, docker, gcp cloud architect, gcp data engineer, gcs, hadoop, hive, iam, java, kubernetes, memory store, organization, persistent disk, projects, python, roles, spark"
Lead Data Engineer,Aveanna Healthcare,"Atlanta, GA",https://www.linkedin.com/jobs/view/lead-data-engineer-at-aveanna-healthcare-3785884663,2023-12-17,East Point,United States,Mid senior,Onsite,"Overview
Are you passionate about making a difference in people's lives through superior home healthcare? Look no further! Aveanna Healthcare is seeking dedicated individuals to join our compassionate and purpose-driven team.
About Aveanna:
At Aveanna, we are more than just a healthcare company; we are a community of compassionate individuals committed to providing exceptional care to those we serve. Our mission is to bring new possibilities and hope to patients and families in the comfort of their own homes.
Why Work with Us?
Compassion in Action: Aveanna is built on the foundation of empathy and genuine care. We believe that providing care in the comfort of home creates a more personal and meaningful connection with our patients.
Experienced Leadership: Our leadership team brings over 40 years of expertise in building successful home healthcare companies, creating a dynamic environment for growth and development.
Recognized for Career Growth: Aveanna has been honored by Comparably as one of the Best Companies for Career Growth in 2023. We champion the countless team members who have advanced their careers here, with the support of our leaders, mentors, and our tuition reimbursement program.
Celebrating Diversity: Aveanna has been recognized by Newsweek as one of America's Greatest Workplaces for Diversity in 2023. We embrace and celebrate the unique perspectives and backgrounds of our team members, fostering an inclusive and welcoming work environment.
Recognized for Excellence: We are honored to be recognized by Newsweek as one of America's Greatest Workplaces in 2023. At Aveanna, we are proud to have a strong team of clinicians, caregivers, and support staff that works hard every day to make a difference in the lives of the individuals we serve.
Current Opportunities:
The Lead Data Engineer will design, develop and support the data pipelines needed to populate the Aveanna Data Warehouse based on business requirements as part of our enterprise wide business intelligence initiative.
Responsibilities
Responsibilities & Qualifications
Lead the effort to design and implement our data pipeline process
Collaborate with the Data Architect to on the population of needed tables
Participate in on-call rotation to support nightly processes
Coach other team members in data engineering best practices
Participate in testing efforts to maintain quality
Work with the rest of the team to analyze new requirements and data sources
Performance tune slow running processes
Continually research methods to improve pipelines
Required Qualifications
Must be authorized to work in the United States
2-4 years experience as a Data Engineer
3-5 years experience in designing and developing efficient ETL workflows using tools such as Informatica, Ab Initio, Talend, SSIS, etc.
2 years in a Lead or Senior role supervising or leading a small team
Strong Python experience
1-2 years AWS Glue experience
1-2 years Snowflake experience
Strong SQL skills
Extensive experience with XML
REST API experience
Excellent debugging and analysis skills
Well versed in dimensional data modelling
Works well with a team
Strong verbal and written communication
Self-motivated and detail-oriented
Comfortable sharing and presenting ideas at a whiteboard
Strong analytic and problem-solving skills
Preferred Qualifications
Experience with healthcare data
Experience with Informatica
Education
Requires BA/BS degree
We Can't Wait to Welcome You to the Aveanna Family!
Come be a part of a team that believes in the power of caring and compassionate home healthcare. At Aveanna, we value each member of our team and the unique contributions they bring. Join us, and let's make a positive impact together!
Show more
Show less","Data Engineering, ETL Workflows, Python, AWS Glue, Snowflake, SQL, XML, REST API, Dimensional Data Modelling, Healthcare Data, Informatica","data engineering, etl workflows, python, aws glue, snowflake, sql, xml, rest api, dimensional data modelling, healthcare data, informatica","aws glue, data engineering, dimensional data modelling, etl workflows, healthcare data, informatica, python, rest api, snowflake, sql, xml"
Lead Data Engineer,Zelis,"Atlanta, GA",https://www.linkedin.com/jobs/view/lead-data-engineer-at-zelis-3767528282,2023-12-17,East Point,United States,Mid senior,Onsite,"Summary
Build High level technical design both for Streaming and batch processing systems
Design and build reusable components, frameworks and libraries at scale to support analytics data products
Perform POCs on new technology, architecture patterns
Design and implement product features in collaboration with business and Technology stakeholders
Anticipate, identify, and solve issues concerning data management to improve data quality
Clean, prepare and optimize data at scale for ingestion and consumption
Drive the implementation of new data management projects and re-structure of the current data architecture
Implement complex automated workflows and routines using workflow scheduling tools
Build continuous integration, test-driven development and production deployment frameworks
Drive collaborative reviews of design, code, test plans and dataset implementation performed by other data engineers in support of maintaining data engineering standards
Analyze and profile data for the purpose of designing scalable solutions
Troubleshoot complex data issues and perform root cause analysis to proactively resolve product and operational issues
Lead, Mentor and develop offshore Data Engineers in adopting best practices and deliver data products.
Partner closely with product management to understand business requirements, breakdown Epics,
Partner with Engineering Managers to define technology roadmaps, align on design, architecture, and enterprise strategy
Requirements
Minimum of 8+ years experience with the following:
Snowflake (Columnar MPP Cloud data warehouse)
DBT (ETL tool)
Python
Experience designing and implementing Data Warehouse
Preferred Skills
Azure/AWS cloud technology
SQL objects (procedures, triggers, views, functions) in SQL Server. SQL query optimizations
Understanding of T-SQL, indexes, stored procedures, triggers, functions, views, etc.
Design and development of Azure/AWS Data Factory Pipelines preferred.
Design and development of data marts in Snowflake preferred
Working knowledge of Azure/AWS Architecture, Data Lake, Data Factory
Business analysis experience to analyze data to write code and drive solutions
Knowledge of: Git, Azure DevOps, Agile, Jira and Confluence
Independence/ Accountability
Requires minimal daily supervision
Receives detailed instruction on new assignments and determines next steps with guidance
Regularly reviews goals and objectives with supervisor
Demonstrates competence in relevant job responsibilities which allows for increasing level of independence
Ability to manage and prioritize multiple tasks
Ability to work under pressure and meet deadlines
Problem Solving
Makes logical suggestions of likely causes of problems and independently suggests solutions
Excellent organizational skills are required to prioritize responsibilities, thus completing work in a timely fashion
Outstanding ability to multiplex tasks as required
Excellent project management and/or business analysis skills.
Attention to detail and concern for impact is essential
As a leading payments company in healthcare, we guide, price, explain, and pay for care on behalf of insurers and their members. We’re Zelis in our pursuit to align the interests of payers, providers, and consumers to deliver a better financial experience and more affordable, transparent care for all. We partner with more than 700 payers, including the top-5 national health plans, BCBS insurers, regional health plans, TPAs and self-insured employers, over 4 million providers, and 100 million members, enabling the healthcare industry to pay for care, with care. Zelis brings adaptive technology, a deeply ingrained service culture, and a comprehensive navigation through adjudication and payment platform to manage the complete payment process.
Commitment to Diversity, Equity, Inclusion, and Belonging
At Zelis, we champion diversity, equity, inclusion, and belonging in all aspects of our operations. We embrace the power of diversity and create an environment where people can bring their authentic and best selves to work. We know that a sense of belonging is key not only to your success at Zelis, but also to your ability to bring your best each day.
Equal Employment Opportunity
Zelis is proud to be an equal opportunity employer. All applicants will receive consideration for employment without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.
We encourage members of traditionally underrepresented communities to apply, even if you do not believe you 100% fit the qualifications of the position, including women, LGBTQIA people, people of color, and people with disabilities.
Accessibility Support
We are dedicated to ensuring our application process is accessible to all candidates. If you are a qualified individual with a disability or a disabled veteran and require a reasonable accommodation with any part of the application and/or interview process, please email TalentAcquisition@zelis.com
SCAM ALERT: There is an active nationwide employment scam which is now using Zelis to garner personal information or financial scams. This site is secure, and any applications made here are with our legitimate partner. If you’re contacted by a Zelis Recruiter, please ensure whomever is contacting you truly represents Zelis Healthcare. We will never asked for the exchange of any money or credit card details during the recruitment process. Please be aware of any suspicious email activity from people who could be pretending to be recruiters or senior professionals at Zelis.
Show more
Show less","Snowflake, DBT, Python, SQL, Azure, AWS, Data Warehouse, Data Factory Pipelines, Data Marts, Data Lake, Git, Azure DevOps, Agile, Jira, Confluence","snowflake, dbt, python, sql, azure, aws, data warehouse, data factory pipelines, data marts, data lake, git, azure devops, agile, jira, confluence","agile, aws, azure, azure devops, confluence, data factory pipelines, data lake, data marts, datawarehouse, dbt, git, jira, python, snowflake, sql"
Lead Big Data Engineer - Full Time,Vimerse InfoTech Inc,"Atlanta, GA",https://www.linkedin.com/jobs/view/lead-big-data-engineer-full-time-at-vimerse-infotech-inc-3780689224,2023-12-17,East Point,United States,Mid senior,Onsite,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, Vimerse Infotech Inc, is seeking the following. Apply via Dice today!
This is a hybrid, full-time position.
Candidates will have a client interview.
12 + years of relevant experience.
Automotive industry experience is not required.
Highlights of the JD as per our conversation with the DM: Heavy focus on Python, AWS, and Snowflake experience. Having a consulting mindset and being proactive in identifying opportunities for automation and enhancements.
3-4 rounds of interviews
Job Description
Must have Skills: Cloud architecture (Strong), Cloud development (Strong), Python (Strong), Snowflake, AWS Lambda.
We are looking for a strong Data Engineer cum Architect, to create new modern data ingestion pipelines using latest technologies like AWS Athena, Lambdas, Python, Spark.
You'll be working on data pipelines and tools to provide the underlying data ingestion framework.""
Technical Skills
Frontrunner
Be inclined towards process automations & improvements, Identifying & automating repetitive things.
Must be able to handle Data engineering operations / enhancement project with a technical consultant bend.
SQL, Python, PySpark , S3, Lambda, EMR, Glue, Athena, EC2, IAM, Redshift, DMS, Airflow, Jenkins, Snowflake.
End-to-end data solutions (ingest, storage, integration, processing, access) on AWS.
Migrate data from traditional relational database systems to AWS relational databases such as Amazon RDS, Aurora, and Redshift.
12+ years IT experience. Background and experience in data engineering/analytics.
Should have a very good hands-on experience in Cloud DB platforms (Snowflake is preferable), Building data pipelines & SQL, Python for Data Engineering.
Got experience to Perform, Support and Lead all aspects of Data Engineering strategy.
Excellent root cause analysis skills.
Ensure effective data pipeline engineering, deployment, ongoing operations, and continuous improvement.
Manage and perform data operations and data engineering requirements including automation and optimization.
Highly motivated, a self-starter, ability to work in a fast faced environment while managing competing priorities.
Creative problem solver and highly collaborative teammate who is comfortable working as a key contributor.
Certification in Data Engineering and/or Cloud Platforms are a plus.
Good written and verbal communication skills, and comfortable presenting findings to Sr. Management
Vimerse is a fast growing IT Company having a team of well trained professionals and with a huge spectrum of services to offer. It s having
Innovation and Reliability
as its driving force. We are proud to say that with our fleeting yet diverse experiences we are able to become a partner of choice with an astonishing accomplishment and customer satisfaction for our clients.
Our main motive is to provide professional support system to our clients aligned to their core business activity with an aim to provide cost effectiveness and optimum utilization of available resource while keeping pace with dynamic market conditions.
We are an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status
Lead Big Data Engineer - Full Time
Show more
Show less","Cloud architecture, Cloud development, Python, Snowflake, AWS Lambda, AWS Athena, Spark, SQL, PySpark, S3, EMR, Glue, EC2, IAM, Redshift, DMS, Airflow, Jenkins, RDS, Aurora, Data engineering, Data analytics, Data pipelines, Automation, Optimization, Root cause analysis","cloud architecture, cloud development, python, snowflake, aws lambda, aws athena, spark, sql, pyspark, s3, emr, glue, ec2, iam, redshift, dms, airflow, jenkins, rds, aurora, data engineering, data analytics, data pipelines, automation, optimization, root cause analysis","airflow, aurora, automation, aws athena, aws lambda, cloud architecture, cloud development, data engineering, dataanalytics, datapipeline, dms, ec2, emr, glue, iam, jenkins, optimization, python, rds, redshift, root cause analysis, s3, snowflake, spark, sql"
Data Analyst,Ivey Business School at Western University,"London, Ontario, Canada",https://ca.linkedin.com/jobs/view/data-analyst-at-ivey-business-school-at-western-university-3784732426,2023-12-17,London, Canada,Associate,Hybrid,"Position:
Data Analyst
Location:
London, Ontario
Position Type:
Full-time, 3-year renewable contract.
Hiring Range:
$55,900-$69,900.
Application Deadline:
Thursday, January 4th at 11:59PM.
-
About Ivey Business School:
Ivey Business School acknowledges and respects the traditional lands of the Anishinaabek, Haudenosaunee, Lūnaapéewak, and Attawandaron peoples, where Western University and Ivey Business School are located. This land continues to be home to diverse Indigenous peoples, whom we recognize as contemporary stewards of the land and vital contributors of the community.
Ivey Business School (www.ivey.ca) at Western University (www.uwo.ca) is Canada’s leading provider of real-world, case-based business education. Drawing on extensive research and business experience, Ivey’s faculty provides the best classroom experience, equipping students with the knowledge, skills, and capabilities they need to confidently take on today’s leadership challenges and opportunities through Case-Method Learning. Ivey offers undergraduate and graduate degree programs, as well as Executive Education at campuses in London, Toronto, and Hong Kong.
Position Overview:
In this critical role, you'll use your data-centric mindset to generate actionable insights and drive strategic decisions within Career Management. With a focus on data aggregation, analysis, and presentation, your work will enhance Career Management processes, and assist in forming a data-driven narrative. Adept at collaborating, you're an exceptional communicator, able to articulate complex findings with clarity, and a proactive thinker who thrives in fast-paced environments.
Responsibilities:
Data Analysis and Reporting
Develop and maintain departmental processes/practices/templates for conducting and sharing business intelligence with relevant stakeholders, including development of regular reports (e.g., student employment reports, trends report, end-of-year employment reports), and dashboards (e.g., student employment tracking) relevant to different stakeholder groups.
Analyze and transform raw data for departmental metrics (e.g. survey results, student employment data) into digestible, report and presentation-ready formats that include both quantitative information, as well as narrative (analytical insights), and that reveal actionable insights and inform strategic decisions
Collaborate with internal stakeholders for report amendments and data presentation.
Remain current with international reporting standards, advising CM team on changes, setting and managing reporting timelines accordingly, and then distributing employment information in a transparent, consistent and comparable way.
Conduct analysis of all Career Management surveys using appropriate statistical tools and communicate findings in clear and concise manner.
Data Collection and Management
Develop and maintain awareness of available internal data relevant to Career Management operations and advise on how it could be utilized to inform key decisions related to programming, operations, corporate partner engagement, etc.
Leverage survey design skills to advise and collaborate with CM team members on creating/updating student and corporate partner surveys to yield impactful data and inform decision-making.
Oversee data collection of student employment outcomes in accordance with international reporting standards and in a statistically reliable manner.
Remain current with international data collection standards, ensuring compliance, and maintaining data integrity to be audit-ready, if required.
Liaise with Manager, Analysis & Reporting to support school-wide initiatives that require CM data collection and/or analysis including global rankings and accreditations.
In consultation with the Associate Director, Operations and applicable internal stakeholders, develop and implement processes to ensure data is accurate and secure in accordance with university policies.
Respond to all requests for data collection from departmental and school-wide stakeholders, working collaboratively to identify and understand deliverables, methodology, timelines, and adherence to data governance policies.
Analysis & Reporting Support
Foster a positive, creative and collaborative environment that encourages innovative ideas and continuous improvement amongst the CM team and school-wide collaborators.
Support the development of the teams’ confidence with data collection and interpretation for decision-making.
Understand inter-departmental data connections and provide recommendations on partnership opportunities.
Build and maintain a network of data analysis professionals at national and international business schools to remain current on best practices in data collection and reporting.
Technology
Maintain comprehensive knowledge of technologies such as 12Twenty, Salesforce, LEARN, etc. for data reporting and program management.
Qualifications:
Education and Experience:
Bachelor’s degree, with Masters degree in business analytics, data analysis or related discipline preferred
Minimum 2 years of experience demonstrating expertise in data collection, analysis, visualization, and presentation.
Skills and Abilities:
Demonstrated advanced use of analytics platforms and data visualization experience.
Exceptional organizational and problem-solving skills with ability to prioritize work to meet timelines and achieve objectives.
Exceptional attention to detail and commitment to data integrity
Proven experience in synthesizing and analyzing data from qualitative and quantitative data sets and designing and generating impactful reports.
Proven track record of defining new data collection and analysis processes to increase efficiencies or effectiveness of reporting.
Demonstrated critical thinking skills, with an ability to identify patterns, trend, and abnormalities in data sets.
Strong communication skills with ability to use active listening skills to understand needs, comfort with asking questions to seek clarity, guide and manage expectations, and produce written documents that are clear, persuasive and audience appropriate.
Demonstrated ability to collaborate with multiple stakeholders within department and across the organization.
Familiarity with the University and business school environment, programs offered, market challenges, and employment industries is an asset.
Self-directed individual, with a high tolerance for ambiguity and comfort with using professional judgment to make recommendations on the data provided.
Accustomed to working with confidential data and adherence to all privacy regulations.
Advanced computer skills in MS Office Suite
Comfortability with learning new software tools in an efficient manner.
What We Offer:
A bright, inclusive, and modern eco-friendly working environment
Flexible or hybrid work plans for many roles
A collaborative and engaging work culture
Opportunity to join a community of people-focused professionals
Market competitive compensation with annual increases available
Generous benefits package with employer-paid premiums, including a comprehensive Health and Dental Plan, Health Care Spending Account, as well as short- and long-term disability programs
Enhanced mental health benefits coverage and resources
A generous group retirement savings plan with 7.5% employer contributions
Life and AD&D insurance for you and your family
Out-of-country travel insurance
Vacation starting at 15 days per year plus 2 paid Ivey Personal Days
Paid time off during the School’s December holiday closure
Numerous campus-wide arts, culture, and sport events
Supports for professional development and career progression opportunities
Employee Assistance Program with family coverage
Equity, Diversity, and Inclusion at Ivey
Ivey Business School is committed to Equity, Diversity, and Inclusion. Please explore Ivey’s EDI homepage for more information on Ivey's commitment to EDI, to read about Ivey’s progress in the EDI Update, and to meet Ivey’s EDI Advisory Council members.
Ivey Business School invites applications from all qualified individuals. Ivey is committed to employment equity and diversity in the workplace, and welcomes applications from women, members of racialized groups/visible minorities, Indigenous persons, persons with disabilities, persons of any sexual orientation, and persons of any gender identity or gender expression.
Accommodation
Prior to the next step in the recruiting process, we welcome you to inform us confidentially if you may require any accommodations in order to participate fully in our recruitment experience. Please contact us at
hr@ivey.ca to notify us of any needs related to completing the job application, and/or throughout the recruitment process.
Application:
If you're excited about joining Canada's leading business school and meet the qualifications, please submit your resume and cover letter via the Ivey Careers page by the application deadline.
Please note:
Ivey Business School will be closed over the holidays from December 22nd, 2023 to January 4th, 2024. Applications will be reviewed the week of January 8th, 2024.
Show more
Show less","Data analysis, Data visualization, Data collection, Data management, Data reporting, Data integrity, Analytics platforms, Data governance, Data mining, Statistical tools, Programming, 12Twenty, Salesforce, LEARN, MS Office Suite, Quantitative data, Qualitative data, Business analytics, Data integrity, Critical thinking, Problemsolving, Communication, Collaboration, Leadership, Teamwork, Adaptability, Flexibility, Time management, Project management, Budget management, Risk management, Change management, Innovation, Continuous improvement, Research, Consulting","data analysis, data visualization, data collection, data management, data reporting, data integrity, analytics platforms, data governance, data mining, statistical tools, programming, 12twenty, salesforce, learn, ms office suite, quantitative data, qualitative data, business analytics, data integrity, critical thinking, problemsolving, communication, collaboration, leadership, teamwork, adaptability, flexibility, time management, project management, budget management, risk management, change management, innovation, continuous improvement, research, consulting","12twenty, adaptability, analytics platforms, budget management, business analytics, change management, collaboration, communication, consulting, continuous improvement, critical thinking, data collection, data governance, data integrity, data management, data mining, data reporting, dataanalytics, flexibility, innovation, leadership, learn, ms office suite, problemsolving, programming, project management, qualitative data, quantitative data, research, risk management, salesforce, statistical tools, teamwork, time management, visualization"
Senior Data Insights Analyst,CARFAX Canada,"London, Ontario, Canada",https://ca.linkedin.com/jobs/view/senior-data-insights-analyst-at-carfax-canada-3776655514,2023-12-17,London, Canada,Mid senior,Onsite,"Working Here
At CARFAX Canada, we’re more than just obsessed with cars. We’re obsessed with data and using it to help millions of Canadians buy, sell and maintain cars - but you can learn that on our website. Let’s talk about the sweet perks you’ll get when working here (i.e. what you actually want to know): Some days you’ll be taking in-house leadership training courses, other days you’ll be eating a catered lunch with your team mates. Grab a seat in the state-of-the-art office at 100 Kellogg Lane and collaborate the day away. What’s that, it’s the summer? Well, the 4-day work weeks have kicked in; enjoy those extra paid days off! Why not use one of your paid volunteer days to give back to your community? What else can we list? Competitive wages, amazing benefits like a wellness spending fund, a company-matched pension program, monthly “work from anywhere” days, yearly performance-based bonuses, health and wellness programs, a literal award-winning culture, parental leave top-ups and all kinds of social events. To top all of this off, every day you get to choose how you get to do meaningful work with incredible people. So, looks like we’re obsessed with a few things here – data and our people!
Job Details
CARFAX Canada is excited to announce the position of Senior Data Insights Analyst. In this role, you will work on-site from our London, ON office and report to the Director Data Insights and Governance.
The Senior Data Insights Analyst will work closely with our Product, Data Acquisition, and Business Development teams to analyze the unique nature and value in new prospective data sources, validate business logic to increase our return on investment from the data we have and bring new fact-based perspectives to prove out new ideas.
The Senior Data Insights Analyst will possess strong Microsoft Azure experience including Azure Synapse, SQL, PowerBI, and emerging Microsoft tools to bring artificial intelligence and machine learning to the table in our analyses and to leverage productivity and effectiveness as we explore big data.
Position Responsibilities
Act as a subject matter expert with a thorough understanding of the data and its lineage.
Contribute to the team’s data governance practice.
Contribute to strategic initiatives and data-first business opportunities.
Represent Data Insights on enterprise roadmap projects, providing business-focused data validation, analysis, and modeling as required.
Respond to industry and business needs with creative approaches to build, test, and validate new logic as we combine multiple data sources to draw accurate conclusions.
Proactively seek new and undiscovered opportunities within our data libraries in the form of new products, insights.
Provide leadership and advocacy across the business, promoting the importance of enterprise data capabilities and how they factor into corporate growth targets.
Identify, research and resolve problems to maintain standards of data quality and accuracy.
Education and Experience Required
Post-secondary education, college diploma or university degree in Computer Science or Engineering.
5 + years of experience working with large data sets in a professional setting.
Provable ability to deliver tailored complex solutions to suit nontechnical audiences
Strong experience with BI Solutions (PowerBI, Cognos, Tableau, etc.).
Strong experience with SQL Query, Azure Data Studio, statistical analysis software such as SPSS, SAS, R, Python
Ability to work with stakeholders to identify and define business requirements and business rules
Experience in business data analytics an asset (i.e. Financial valuations, quality assessments).
Knowledge of data governance practices and approaches include data policies, data lineage.
Excellent communication and presentation skills to share recommendations and findings in a clear, concise manner to a technical and non-technical audience.
Strong problem-solving skills and an ability to use data to drive strategic business decisions.
Equal Opportunity Employer
CARFAX Canada is an equal opportunity employer, and all qualified candidates will receive consideration for employment without regard to race/ethnicity, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, marital status, military veteran status, unemployment status, or any other status protected by law.
We’re committed to providing accommodations by request for candidates taking part in all aspects of the recruitment and selection process. For a confidential inquiry or to request an accommodation, please contact your recruiter or email hr@carfax.ca.
If you are interested in applying for this position, please visit our website
https://www.carfax.ca/careers
.
Applications will be accepted until a suitable candidate is hired.
We thank all applicants for their interest; however only those selected for an interview will be contacted.
About Us
CARFAX’s mission is to help millions of people shop, buy, service and sell used cars with more confidence. As a leader in vehicle history and valuation, CARFAX provides impartial and comprehensive information for consumers and the automotive industry. CARFAX‘s Canadian headquarters in London, Ontario supports Canadian and U.S. markets, drawing on billions of data records from thousands of sources, enabling used vehicle buyers and sellers to make informed decisions. CARFAX is consistently recognized as a top employer and business. CARFAX is a part of S&P Global (NYSE: SPGI). Find out more at www.carfax.ca and connect with CARFAX on Instagram, Facebook and LinkedIn.
Show more
Show less","Microsoft Azure, Azure Synapse, SQL, PowerBI, Artificial intelligence, Machine learning, Data governance, Data analytics, Business intelligence (BI), Cognos, Tableau, SQL Query, Azure Data Studio, SPSS, SAS, R, Python, Data mining, Data modeling, Data visualization, Problemsolving, Communication, Presentation skills","microsoft azure, azure synapse, sql, powerbi, artificial intelligence, machine learning, data governance, data analytics, business intelligence bi, cognos, tableau, sql query, azure data studio, spss, sas, r, python, data mining, data modeling, data visualization, problemsolving, communication, presentation skills","artificial intelligence, azure data studio, azure synapse, business intelligence bi, cognos, communication, data governance, data mining, dataanalytics, datamodeling, machine learning, microsoft azure, powerbi, presentation skills, problemsolving, python, r, sas, spss, sql, sql query, tableau, visualization"
Customer Service Representative/Data Analyst/Data Entry Clerk Full Time,Bluebeacontruckwash,"London, Ontario, Canada",https://ca.linkedin.com/jobs/view/customer-service-representative-data-analyst-data-entry-clerk-full-time-at-bluebeacontruckwash-3751472184,2023-12-17,London, Canada,Mid senior,Onsite,"Summary:
The Data Analyst will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : hiring@jobsai.live
Summary:
The Data Analyst will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : hiring@jobsai.live
Powered by Webbtree
Show more
Show less","Data Analysis, Statistical Modeling, SQL, R, Python, Hypothesis Testing, A/B Testing, Tableau, Power BI, Data Visualization, ETL, Data Cleaning, Data Management","data analysis, statistical modeling, sql, r, python, hypothesis testing, ab testing, tableau, power bi, data visualization, etl, data cleaning, data management","ab testing, data cleaning, data management, dataanalytics, etl, hypothesis testing, powerbi, python, r, sql, statistical modeling, tableau, visualization"
Customer Service Representative/Data Analyst/Data Entry Clerk Full Time,Bluebeacontruckwash,"Aylmer, Ontario, Canada",https://ca.linkedin.com/jobs/view/customer-service-representative-data-analyst-data-entry-clerk-full-time-at-bluebeacontruckwash-3751472188,2023-12-17,London, Canada,Mid senior,Onsite,"Summary:
The Data Analyst will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : hiring@jobsai.live
Powered by Webbtree
Show more
Show less","Data Analysis, Statistical techniques, Data interpretation, Datadriven decisionmaking, Data visualization, SQL, R, Python, Tableau, Power BI, Statistical modeling, Hypothesis testing, A/B testing, Data management, ETL processes","data analysis, statistical techniques, data interpretation, datadriven decisionmaking, data visualization, sql, r, python, tableau, power bi, statistical modeling, hypothesis testing, ab testing, data management, etl processes","ab testing, data interpretation, data management, dataanalytics, datadriven decisionmaking, etl, hypothesis testing, powerbi, python, r, sql, statistical modeling, statistical techniques, tableau, visualization"
Customer Service Representative/Data Analyst/Data Entry Clerk Part Time,Ropesgray,"Aylmer, Ontario, Canada",https://ca.linkedin.com/jobs/view/customer-service-representative-data-analyst-data-entry-clerk-part-time-at-ropesgray-3752010858,2023-12-17,London, Canada,Mid senior,Onsite,"Summary:
The Data Analyst will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : hiring@jobsai.live
Powered by Webbtree
Show more
Show less","Data Analysis, Statistical Techniques, R, Python, SQL, Tableau, Power BI, Data Visualization, Statistical Modeling, Hypothesis Testing, A/B Testing, ETL, Data Management","data analysis, statistical techniques, r, python, sql, tableau, power bi, data visualization, statistical modeling, hypothesis testing, ab testing, etl, data management","ab testing, data management, dataanalytics, etl, hypothesis testing, powerbi, python, r, sql, statistical modeling, statistical techniques, tableau, visualization"
"Biostatistician II, Biostatistics and Data Science",Atrium Health Wake Forest Baptist,"Winston-Salem, NC",https://www.linkedin.com/jobs/view/biostatistician-ii-biostatistics-and-data-science-at-atrium-health-wake-forest-baptist-3715194366,2023-12-17,High Point,United States,Mid senior,Onsite,"Job Description
Department of Biostatistics and Data Science
Biostatistician (MS and BS) Positions
Winston-Salem, North Carolina
https://school.wakehealth.edu/Departments/Public-Health-Sciences
X (Twitter): @WakeBDS, @WakeForestMed
The Department of Biostatistics and Data Science in the Division of Public Health Sciences at Wake Forest School of Medicine is recruiting to fill multiple biostatistician positions. A Master’s degree in biostatistics, epidemiology, bioinformatics, or a related field is preferred, but candidates with a Bachelor’s degree in statistics/biostatistics or mathematics and strong analytic skills will be considered. Excellent verbal and written communication skills are required. Responsibilities include collaborating with medical investigators on the design, conduct, analysis, and presentation of single- and multi-center research projects and clinical trials. Experience with consulting and statistical programming, especially SAS, Python and/or R, are preferred. Also, experience with diverse and complex data such as claims data, EHR data, CMS data, IQVIA data, and other public data sources and dataset is strongly desired. The position also includes opportunities for professional development such as conference attendance, presentations, and publications as part of the grant collaborations through the NIH with consortiums such as Michigan’s SBE-CCC, NIH HEAL Data2Action Network, and NIH MIRHIQL Network.
The Department of Biostatistics and Data Science is a vibrant academic unit comprised of 35 faculty and more than 130 staff including biostatisticians, programmers, project managers, and administrative staff. This group has demonstrated long-standing growth in extramural funding, aiding the development of a rich research environment and facilitating advancements in a variety of public health disciplines including geriatrics, cardiovascular disease, diabetes, women’s health, population genetics, and cancer control. The Department resides within the Division of Public Health Sciences, which also includes the Department of Epidemiology and Prevention, the Department of Social Sciences and Health Policy, and the newly formed Department of Implementation Science. Together, Public Health Sciences is the largest research group at Wake Forest, including more than 60 faculty and 180 staff.
Our department engages in data science work with collaborative teams spanning academic, governmental, and industry organizations. Our main research areas are aging, Alzheimer’s disease and related dementias, diabetes, cardiovascular and public health. We often deal with large datasets generated by clinical trials, available to the public or in the medical records system including imaging, omics, wearable devices, etc. Data Science is constantly evolving and we are also continually developing and expanding our work, including in the areas of clinical trials, biostatistics, machine learning, artificial intelligence, new software platforms and high-performance computing.
Located in the Innovation Quarter (https://www.innovationquarter.com/) adjacent to Bailey Park ( http://baileyparkws.com/ ) in downtown Winston-Salem, NC, Wake Forest University School of Medicine is the cornerstone of one of the fastest-growing urban-based districts for innovation in the United States. A city of approximately 220,000 people, Winston-Salem is home to Wake Forest Baptist Medical Center and Wake Forest University. The city is nestled in the northwestern region of North Carolina, within easy driving distance to the beautiful Blue Ridge Mountains and the pristine beaches of North Carolina.
We celebrate diversity and are committed to creating an inclusive environment for all employees. We promote diversity of thought, culture and background, which connects the entire Atrium family. Applicants may apply by sending a cover letter, graduate school transcript, and curriculum vitae to dbsrecruit@wakehealth.edu . Employer will assist with relocation costs.
Wake Forest Baptist Medical Center is an Affirmative Action and Equal Opportunity Employer with a strong commitment to achieving diversity among its faculty and staff. EOE/AA: Minorities/Females/Disabled/Vets. Atrium Wake Forest Baptist is an equal opportunity employer.
About Us
Wake Forest University School of Medicine (WFUSM) is a U.S. News and World Report top 50 ranked medical school, integrated with a world-class health system, Atrium Health. WFUSM, the academic core of Atrium Health Enterprise, is a recognized leader in experiential medical education and groundbreaking research that includes Wake Forest Innovations, a commercialization enterprise focused on advancing health care through new medical technologies and biomedical discovery. WFUSM, has over $300M in annual, extramural funding that drives a cutting-edge Academic Learning Health System by integrating innovative research with excellent patient care across our enterprise.
Atrium Health is based in Winston-Salem, North Carolina and is part of Advocate Health, which is headquartered in Charlotte, North Carolina, and is the fifth-largest nonprofit health system in the United States, created from the combination of Atrium Health and Advocate Aurora Health. AHWFB is an 885-bed tertiary-care hospital in Winston-Salem – that includes Brenner Children’s Hospital, five community hospitals, more than 300 primary and specialty care locations and more than 2,700 physicians.
Our highly integrated academic and clinical environment is deeply committed to improving health, elevating hope, and advancing healing – for all.
It should be noted that while you are applying on the Wake Forest University School of Medicine Career Site, you will receive communications from the Atrium Health Recruitment Team. Please know that this is an expected process. Thanks in advance for your flexibility.
Organization
Mission, Vision and Culture Commitments
Mission
To improve health, elevate hope and advance healing – for all.
Vision
To be the first and best choice for care.
Culture Commitments
We create a space where all Belong
We Work as One to make great things happen
We earn Trust in all we do
We Innovate to better the now and create the future
We drive for Excellence – always
Show more
Show less","SAS, Python, R, Biostatistics, Epidemiology, Bioinformatics, Statistics, Mathematics, Data Science, Clinical Trials, Consulting, Statistical Programming, Machine Learning, Geriatrics, Cardiovascular Disease, Diabetes, Women’s Health, Population Genetics, Cancer Control, Public Health, Artificial Intelligence, Software Platforms, HighPerformance Computing","sas, python, r, biostatistics, epidemiology, bioinformatics, statistics, mathematics, data science, clinical trials, consulting, statistical programming, machine learning, geriatrics, cardiovascular disease, diabetes, womens health, population genetics, cancer control, public health, artificial intelligence, software platforms, highperformance computing","artificial intelligence, bioinformatics, biostatistics, cancer control, cardiovascular disease, clinical trials, consulting, data science, diabetes, epidemiology, geriatrics, highperformance computing, machine learning, mathematics, population genetics, public health, python, r, sas, software platforms, statistical programming, statistics, womens health"
Senior Data Engineer,bp,Greater Houston,https://www.linkedin.com/jobs/view/senior-data-engineer-at-bp-3781954562,2023-12-17,Texas,United States,Associate,Onsite,"Entity:
Innovation & Engineering
Job Family Group:
IT&S Group
Job Summary:
As a bp Data Engineer you will be developing and maintaining data platforms and infrastructure, along with writing, deploying and maintaining software to build, integrate, manage, maintain, and quality-assure data. You will be part of bp’s Data & Analytics Platform organisation, the group responsible for the platforms and services that underpin bp’s data supply chain.
For this role specifically within Knowledge and Data Stores, you will be involved in designing, implementing and delivering the necessary platforms to enable master and reference data management, data modelling, searching and indexing, and data transformations and movement for data in bp’s enterprise systems. This role is key in ensuring availability and reliability of quality data that bp’s enterprise organisation can use to detect, anticipate and prevent issues, provide insights to improve our operations, and have timely access to data that will allow our platform engineers to build the necessary automation for operational efficiency, self-healing, etc.
Job Description:
Key accountabilities
Design, select, and implement data and knowledge management solutions within bp. Implement, and guide direction of platforms for master and reference data management, knowledge management, searching and indexing, and oil and gas data frameworks. Work with business developers and users to provide experienced project guidance and assure alignment to processes. Work with third party vendors and open source collaborations to assure bp needs are met within selected systems.
Maintain toolsets in areas such as master data management, search and indexing, and data modelling, and facilitate effective use by business users, developers and architects across the corporation. Provide architectural and business process guidance to platform engineers and business users. Support on these platforms the creation of data pipelines for Azure and AWS data platforms and services, provisioning of data nodes and assuring telemetry for performance and utilisation analytics.
Qualifications
Deep and hands-on experience designing, planning, implementing, maintaining, and documenting reliable and scalable data infrastructure and data products in complex environments.
Experience with data management patterns and standard practices and Agile development.
Development experience in one or more object-oriented programming languages (e.g. Python, Go, Java, C++)
BS degree in computer science or related field
Desirable Criteria
Data Manipulation: debug and maintain the end-to-end data engineering lifecycle of the data products; design and implementation of the end-to-end data stack, including designing complex data systems, e.g. interoperability across cloud platforms; experience on various types of data (streaming, structured and un-structured) is a plus.
Software Engineering: hands-on experience with SQL and NoSQL database fundamentals, query structures and design standard practices, including scalability, readability, and reliability; you are proficient in at least one object-oriented programming language, e.g. Python [specifically data manipulation packages - Pandas, seaborn, matplotlib], Apache Spark or Scala
Scalability, Reliability, Maintenance: proven experience in building scalable and re-usable systems; knowledge and experience in automating operations and building for long-term productivity over short-term speed/gains, and fulfill those opportunities to improve products or services.
Cloud Engineering– Recent experience applying data analytics offerings and services from Azure and AWS
Why join us
At bp, we support our people to learn and grow in a diverse and exciting environment. We believe that our team is strengthened by diversity. We are committed to fostering an inclusive environment in which everyone is respected and treated fairly.
There are many aspects of our employees’ lives that are important, so we offer benefits to enable your work to fit with your life. These benefits can include flexible working options, a generous paid parental leave policy, and excellent retirement benefits, among others!
We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.
Travel Requirement
Negligible travel should be expected with this role
Relocation Assistance:
This role is not eligible for relocation
Remote Type:
This position is a hybrid of office/remote working
Skills:
Legal Disclaimer:
We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, sex, gender, gender expression, sexual orientation, age, marital status, socioeconomic status, neurodiversity/neurocognitive functioning, veteran status or disability status. Individuals with disabilities may request a reasonable accommodation related to bp’s recruiting process (e.g., accessing the job application, completing required assessments, participating in telephone screenings or interviews, etc.). If you would like to request an accommodation related to the recruitment process, please contact us to request accommodations.
If you are selected for a position and depending upon your role, your employment may be contingent upon adherence to local policy. This may include pre-placement drug screening, medical review of physical fitness for the role, and background checks.
Show more
Show less","Data Engineering, Data Platforms, Data Infrastructure, Data Quality Assurance, Data Management, Data Modelling, Data Transformations, Data Movement, Master Data Management, Reference Data Management, Knowledge Management, Searching, Indexing, Oil and Gas Data Frameworks, Azure, AWS, Python, Go, Java, C++, SQL, NoSQL, Pandas, Seaborn, Matplotlib, Apache Spark, Scala","data engineering, data platforms, data infrastructure, data quality assurance, data management, data modelling, data transformations, data movement, master data management, reference data management, knowledge management, searching, indexing, oil and gas data frameworks, azure, aws, python, go, java, c, sql, nosql, pandas, seaborn, matplotlib, apache spark, scala","apache spark, aws, azure, c, data engineering, data infrastructure, data management, data modelling, data movement, data platforms, data quality assurance, data transformations, go, indexing, java, knowledge management, master data management, matplotlib, nosql, oil and gas data frameworks, pandas, python, reference data management, scala, seaborn, searching, sql"
Senior data engineer,bp,Greater Houston,https://www.linkedin.com/jobs/view/senior-data-engineer-at-bp-3746768894,2023-12-17,Texas,United States,Associate,Onsite,"Entity:
Innovation & Engineering
Job Family Group:
IT&S Group
Job Summary:
As part of bp “reinvent”, we have created a major new business line called “Innovation & Engineering” (I&E). One key remit of this group is to drive the transformation of the company through its use of digital and data. A major digital sub-team within I&E is Digital Production & Business Services (DP&BS). DP&BS are responsible for all digital and data initiatives and operations across the following areas of the bp business:
Production & Projects including Health, Safety, Environment & Carbon
Refining & Operations
Wells & Subsurface
Business Services including Finance, Procurement, People & Culture, Performance Management
Strategy & Sustainability
“dataWorx” is the name for the data team that is responsible for all data within these areas, and we are developing deep data capability to transform the access, supply, control, and quality to our vast and ever-growing reserves of data. The dataWorx team covers many data sub-disciplines, including data science, analytics, engineering, and management as well as specialist areas such as geospatial, remote sensing, knowledge management and digital twin.
The dataWorx team is looking for outstanding data engineers to power this transformation and unlock the value of our digital assets to power our journey to NetZero emissions and build a new, sustainable bp.
Responsibilities
Effectively works with cross-disciplinary team, working closely with architects, other data engineers, software engineers, data scientists, data managers and business partners.
Implements and maintains reliable and scalable data infrastructure.
Leads, grows, and develops a squad of data engineers that writes, deploys, and maintains software to build, integrate, manage, maintain, and quality-assure data at bp.
Advocates for and ensures their teams adheres to software engineering best practices (e.g. technical design, technical design review, unit testing, monitoring & alerting, checking in code, code review, documentation),
Responsible for deploying secure and well-tested software that meets privacy and compliance requirements; develops, maintains and improves CI / CD pipeline,
Responsible for service reliability and following site-reliability engineering best practices: on-call rotations for services they maintain, responsible for defining and maintaining SLAs. Help design, build, deploy and maintain infrastructure as code. Containerizes server deployments.
Actively contributes to improve developer velocity.
Job Description:
Effectively works with cross-disciplinary team, working closely with architects, other data engineers, software engineers, data scientists, data managers and business partners.
Implements and maintains reliable and scalable data infrastructure.
Leads, grows, and develops a squad of data engineers that writes, deploys, and maintains software to build, integrate, manage, maintain, and quality-assure data at bp.
Advocates for and ensures their teams adheres to software engineering best practices (e.g. technical design, technical design review, unit testing, monitoring & alerting, checking in code, code review, documentation),
Responsible for deploying secure and well-tested software that meets privacy and compliance requirements; develops, maintains and improves CI / CD pipeline,
Responsible for service reliability and following site-reliability engineering best practices: on-call rotations for services they maintain, responsible for defining and maintaining SLAs. Help design, build, deploy and maintain infrastructure as code. Containerizes server deployments.
Actively contributes to improve developer velocity.
Qualifications
Deep and hands-on experience (typically 3 - 5 years) designing, planning, productionizing, maintaining, and documenting reliable and scalable data infrastructure and data products in complex environments
Depp and hands-on experience on Azure Big Data Stack and Databrick
Development experience in one or more object-oriented programming languages (e.g. Python, Go, Java, C++)
Experience with SQL and noSQL database fundamentals, query structures and design best practices, including scalability, readability, and reliability
Experience implementing large-scale distributed systems in collaboration with more senior team members
Knowledge and hands-on experience in technologies across all data lifecycle stages
Strong stakeholder management
Continuous learning and improvement mindset
BS degree in computer science or related field
Travel Requirement
Up to 10% travel should be expected with this role
Relocation Assistance:
This role is eligible for relocation within country
Remote Type:
This position is a hybrid of office/remote working
Skills:
Business Acumen, Commercial acumen, Communication, Data Analysis, Data cleansing and transformation, Data domain knowledge, Data Integration, Data Management, Data Manipulation, Data Sourcing, Data strategy and governance, Data Structures and Algorithms, Data visualization and interpretation, Digital Security, Extract, transform and load, Problem Solving
Legal Disclaimer:
We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, sex, gender, gender expression, sexual orientation, age, marital status, socioeconomic status, neurodiversity/neurocognitive functioning, veteran status or disability status. Individuals with disabilities may request a reasonable accommodation related to bp’s recruiting process (e.g., accessing the job application, completing required assessments, participating in telephone screenings or interviews, etc.). If you would like to request an accommodation related to the recruitment process, please contact us to request accommodations.
If you are selected for a position and depending upon your role, your employment may be contingent upon adherence to local policy. This may include pre-placement drug screening, medical review of physical fitness for the role, and background checks.
Show more
Show less","Azure Big Data Stack, Databrick, Python, Go, Java, C++, SQL, NoSQL, Stakeholder management, Continuous learning, Computer science, Business acumen, Commercial acumen, Communication, Data analysis, Data cleansing, Data transformation, Data domain knowledge, Data integration, Data management, Data manipulation, Data sourcing, Data strategy, Data governance, Data structures, Algorithms, Data visualization, Interpretation, Digital security, Extract transform load, Problem solving","azure big data stack, databrick, python, go, java, c, sql, nosql, stakeholder management, continuous learning, computer science, business acumen, commercial acumen, communication, data analysis, data cleansing, data transformation, data domain knowledge, data integration, data management, data manipulation, data sourcing, data strategy, data governance, data structures, algorithms, data visualization, interpretation, digital security, extract transform load, problem solving","algorithms, azure big data stack, business acumen, c, commercial acumen, communication, computer science, continuous learning, data domain knowledge, data governance, data integration, data management, data manipulation, data sourcing, data strategy, data structures, data transformation, dataanalytics, databrick, datacleaning, digital security, extract transform load, go, interpretation, java, nosql, problem solving, python, sql, stakeholder management, visualization"
Senior data engineer,bp,Greater Houston,https://www.linkedin.com/jobs/view/senior-data-engineer-at-bp-3733536941,2023-12-17,Texas,United States,Associate,Onsite,"Entity:
Innovation & Engineering
Job Family Group:
IT&S Group
Job Summary:
Part of a cross-disciplinary team, working closely with other data engineers, software engineers, data scientists, data managers and business partners.
Architects, designs, implements and maintains reliable and scalable data infrastructure to move, process and serve data.
Writes, deploys and maintains software to build, integrate, manage, maintain, and quality-assure data at bp.
Adheres to and advocates for software engineering best practices (e.g. technical design, technical design review, unit testing, monitoring & alerting, checking in code, code review, documentation),
Responsible for deploying secure and well-tested software that meets privacy and compliance requirements; develops, maintains and improves CI / CD pipeline,
Responsible for service reliability and following site-reliability engineering best practices: on-call rotations for services they maintain, responsible for defining and maintaining SLAs. Design, build, deploy and maintain infrastructure as code. Containerizes server deployments.
Actively contributes to improve developer velocity.
Mentors others.
Job Description:
BS degree in computer science or related field
Deep and hands-on experience (typically 5+ years) designing, planning, building, productionizing, maintaining and documenting reliable and scalable data infrastructure and data products in complex environments
Development experience in one or more object-oriented programming languages (e.g. Python, Scala, Java, C#)
Advanced database and SQL knowledge
Experience designing and implementing large-scale distributed data systems
Deep knowledge and hands-on experience in technologies across all data lifecycle stages
Strong stakeholder management and ability to lead initiatives through technical influence
Continuous learning and improvement mindset
Desired
No Prior Experience In The Energy Industry Required
Why join us
At bp, we support our people to learn and grow in a diverse and exciting environment. We believe that our team is strengthened by diversity. We are committed to fostering an inclusive environment in which everyone is respected and treated fairly.
There are many aspects of our employees’ lives that are important, so we offer benefits to enable your work to fit with your life. These benefits can include flexible working options, a generous paid parental leave policy, and excellent retirement benefits, among others!
We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.
Travel Requirement
Up to 10% travel should be expected with this role
Relocation Assistance:
This role is eligible for relocation within country
Remote Type:
This position is a hybrid of office/remote working
Skills:
Legal Disclaimer:
We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, sex, gender, gender expression, sexual orientation, age, marital status, socioeconomic status, neurodiversity/neurocognitive functioning, veteran status or disability status. Individuals with disabilities may request a reasonable accommodation related to bp’s recruiting process (e.g., accessing the job application, completing required assessments, participating in telephone screenings or interviews, etc.). If you would like to request an accommodation related to the recruitment process, please contact us to request accommodations.
If you are selected for a position and depending upon your role, your employment may be contingent upon adherence to local policy. This may include pre-placement drug screening, medical review of physical fitness for the role, and background checks.
Show more
Show less","Python, Scala, Java, C#, SQL, Objectoriented programming, Data infrastructure, Data products, Distributed data systems, Data lifecycle stages, Stakeholder management, CI/CD pipeline, Software engineering best practices, Technical design, Technical design review, Unit testing, Monitoring, Alerting, Code review, Documentation, Secure software, Privacy and compliance, Sitereliability engineering, Service reliability, SLAs, Infrastructure as code, Containerization, Developer velocity, Mentorship","python, scala, java, c, sql, objectoriented programming, data infrastructure, data products, distributed data systems, data lifecycle stages, stakeholder management, cicd pipeline, software engineering best practices, technical design, technical design review, unit testing, monitoring, alerting, code review, documentation, secure software, privacy and compliance, sitereliability engineering, service reliability, slas, infrastructure as code, containerization, developer velocity, mentorship","alerting, c, cicd pipeline, code review, containerization, data infrastructure, data lifecycle stages, data products, developer velocity, distributed data systems, documentation, infrastructure as code, java, mentorship, monitoring, objectoriented programming, privacy and compliance, python, scala, secure software, service reliability, sitereliability engineering, slas, software engineering best practices, sql, stakeholder management, technical design, technical design review, unit testing"
Data Center Engineer,World Wide Technology,"Georgetown, TX",https://www.linkedin.com/jobs/view/data-center-engineer-at-world-wide-technology-3782264840,2023-12-17,Texas,United States,Associate,Onsite,"Data Center Engineer
Company Overview
World Wide Technology (WWT), a global technology solutions provider with $17 billion in annual revenue, combines the power of strategy, execution and partnership to accelerate transformational outcomes for large public and private organizations around the world. Through its Advanced Technology Center, a collaborative ecosystem of the world's most advanced hardware and software solutions, WWT helps customers and partners conceptualize, test and validate innovative technology solutions for the best business outcomes and then deploys them at scale through its 4 million square feet of global warehousing, distribution and integration space. With over 10,000 employees and more than 55 locations around the world, WWT's culture, built on a set of core values and established leadership philosophies, has been recognized 11 years in a row by Fortune and Great Place to Work® for its unique blend of determination, innovation and leadership for diversity and inclusion. With this culture at its foundation, WWT bridges the gap between business and technology to make a new world happen for its customers, partners and communities.
World Wide Technology Holding Co, LLC. (WWT) has an opportunity available for a
Data Center Engineer
to support our client in an ongoing Data Center refresh project.
Location:
Georgetown, TX
Available Shifts:
(2 Openings) 7 PM – 7 AM CST Thursday, Friday, Saturday, and Alternating Wednesdays.
MUST BE OKAY WITH 12 HOUR NIGHT SHIFTS - Expectation is to have 80 hours of work in a 2 week period.
Duration:
12 Months (Expected to renew for up to 3 years, on an ongoing 12-month renewal)
Contract Designation:
Full Time Contingent – Contractors will be eligible for WWT’s Full Time Employee Benefits Package including Medical, Vision, Dental, PTO, Paid Holidays, and more.
Responsibilities:
Installing/de-installing/relocating all distributed systems and network hardware (CSUs, DSUs, routers, switches, encryptors, firewalls, etc.) in the Americas Data Centers within the internal service level mandates
Installing/de-installing /extending/relocating/testing all carrier circuits to the network hardware
Installing/de-installing/relocating all patch cabling for systems and network hardware
Installing/de-installing/relocating all Data Center hardware
Assist with the coordination of cabinet power, circuit, and patch infrastructure installations w/various facilities, electrical and communications vendors
Assist with the coordination of network component configurations
Coordinate and Install SAN cabling infrastructure
Managing network ports and assist with the management of all consumable items (cables, labels, tie wraps, rail kits, etc.)
Maintaining the integrity of the data center facilities, systems and communications environments through general housekeeping and best operations practices
Qualifications:
Required skills include 3+ years of experience in the implementation, maintenance and analysis of data center facilities, hardware, communications infrastructure, strategies, tools and effective troubleshooting techniques.
Basic background on enterprise data center facilities and infrastructure environments such as PDUs, RPPs, network and SAN infrastructures. In depth knowledge on complex, Enterprise class inter-networked environments involving a combination of switched/routed/shared Ethernet, TwinAx (100GigE, 25GigE,10GigE, GigE, 100M, and 10M), token ring, SAN, and wide area connectivity.
Strong knowledge of WAN technologies (OC-x, DS-x), subnetting and TCP/IP protocol a must.
Excellent communication and writing skills a must.
Knowledge of trouble ticketing systems, change control, Project processes and associated tools.
Logical problem- solving techniques and associated experience in system, data center facilities, and telecommunications.
Must be Able to Lift up to 50lbs.
Equal Opportunity Employer Minorities/Women/Veterans/Disabled
Show more
Show less","Data center engineering, Installing/deinstalling/relocating hardware, Cabling infrastructure, Cisco hardware (CSUs DSUs routers switches etc.), Carrier circuits, Patch cabling, SAN cabling infrastructure, Network component configurations, Managing network ports, Consumable items management, Troubleshooting techniques, Enterprise data center facilities, PDUs, RPPs, Network infrastructures, SAN infrastructures, Complex Enterprise class internetworked environments, Switched/routed/shared Ethernet, TwinAx (100GigE 25GigE10GigE GigE 100M and 10M), Token ring, WAN technologies (OCx DSx), Subnetting, TCP/IP protocol, Communication skills, Writing skills, Trouble ticketing systems, Change control, Project processes, Logical problem solving techniques","data center engineering, installingdeinstallingrelocating hardware, cabling infrastructure, cisco hardware csus dsus routers switches etc, carrier circuits, patch cabling, san cabling infrastructure, network component configurations, managing network ports, consumable items management, troubleshooting techniques, enterprise data center facilities, pdus, rpps, network infrastructures, san infrastructures, complex enterprise class internetworked environments, switchedroutedshared ethernet, twinax 100gige 25gige10gige gige 100m and 10m, token ring, wan technologies ocx dsx, subnetting, tcpip protocol, communication skills, writing skills, trouble ticketing systems, change control, project processes, logical problem solving techniques","cabling infrastructure, carrier circuits, change control, cisco hardware csus dsus routers switches etc, communication skills, complex enterprise class internetworked environments, consumable items management, data center engineering, enterprise data center facilities, installingdeinstallingrelocating hardware, logical problem solving techniques, managing network ports, network component configurations, network infrastructures, patch cabling, pdus, project processes, rpps, san cabling infrastructure, san infrastructures, subnetting, switchedroutedshared ethernet, tcpip protocol, token ring, trouble ticketing systems, troubleshooting techniques, twinax 100gige 25gige10gige gige 100m and 10m, wan technologies ocx dsx, writing skills"
Data Center Engineer,K&L Gates,"Austin, TX",https://www.linkedin.com/jobs/view/data-center-engineer-at-k-l-gates-3728651279,2023-12-17,Texas,United States,Associate,Onsite,"Job Summary
Job Description
At K&L Gates, we are looking for smart, imaginative and hard-working people with diverse backgrounds, experiences and ideas to join us. Perhaps our search for talented visionaries and your search for important and impactful work lead to the same place.
We are seeking a Data Center Engineer to join any U.S. office. The candidate will be responsible for daily operations of data center services for K&L Gates. Responsible for hardware and systems support and engineering relating to data centers and local office server rooms. Candidates must be self-starters who possess the ability to work independently and as part of a team. Excellent communication skills and a commitment to providing the highest quality client service are strongly preferred.
Essential duties
Develops UPS strategies and solutions for new infrastructure configurations within the local office environments, for either existing or new office build-outs, taking into consideration load and run-time requirements. Knowledge of smart PDUs a plus.
Provides operational data center reports for various departmental subscribers which contributes to the firm’s capacity planning efforts. Reports are crafted using tools such as Visio, Excel and Powerpoint.
Configures and monitors alerts from data center systems in order to ensure system reliability and uptime. Monitoring systems such as Schneider’s Struxureware or standalone power systems are used for this monitoring.
Coordinates maintenance scheduling with vendors and data center staff, tracks open maintenance tickets and works with Change Management on system change details.
Provides system analysis and debugging strategies for resolving communication links between power systems and monitoring systems.
Configures security layers for local and data center monitoring systems. Manages the UPS device passwords in secret server (or equivalent password store).
Operates UPS test lab for systems testing and automation upgrades.
Analyzes power consumption of systems to provide estimates for local office power installations and makes recommendations for implementing efficiencies or reducing consumption when applicable.
Works directly (or as a team member) with technical staff in other departments to resolve incidents or complete project tasks related to data center systems.
Provides technical expertise related to UPS and data center systems to management or other engineers for incident or planning purposes.
Complies with physical and logical security policies covering data center systems and sites.
Complies with departmental documentation standards such as creating and updating standard operating procedures, following through on installation qualification/operational qualification steps, or adhering to departmental web sites document requirements.
Utilizes vendor-based portals and interfaces to stay abreast of open incidents. Stays on top of internal data center requests originating from peer engineers in order to ensure jobs are not slipping from desired implementation dates.
Integrates data from monitoring systems into reports or databases which are used for operations reports and capacity planning decisions by management.
Tracks equipment shipments, alerts team lead and management of equipment provisioning delays.
Gathers and/or transcribes data related to data center systems and local office hardware into various repositories. Data is used to produce rack elevation diagrams, hardware identifiers, or data extracts for other databases/reports.
Sets up alerts for power consumption monitoring, gathers data regarding power quality and consumption at the data center and local offices
Works with asset management personnel regarding equipment attrition and destroy orders and receipts.
Provides guidance on media destruction policy and procedures to ensure that the firm’s sensitive information on media is secure.
Experience And Skills Required
Demonstrates a solid discipline for consistent operational housekeeping and ability to stay on tasks and meet deadlines.
Sufficient knowledge of information technology systems including data center specific systems, power and UPS systems. Networking and DHCP concepts must be understood along with layer 1 troubleshooting (physical layer).
Understanding of power operation for both single phase and 3 phase power. Understanding of power draw and ability to size UPS devices based on power and load requirements.
Knowledge of server room hardware, cabinets, PDUs, server room layout types and power design is recommended.
Strong communication and interpersonal skills required for illustrating technical/complex engineering concepts.
Ability to adapt to fast pace work environment and rapidly changing priorities. Projects can be delayed or changed at any time to adapt to system failures/incidents.
Bachelors Degree, or equivalent plus 2+ years experience in information technology field. Experience with data centers or server rooms is highly recommended.
Additional Abilities Required
Collaborative demeanor with other team members and/or departments on but not limited to hardware installs and decommissions.
Maintain a congenial yet assertive position with vendors and data center staff.
Responsibilities occasionally may require an adjusted work schedule during rollouts or DR events.
Some physical work is involved when installing/removing/adjusting equipment such as servers, switches, cabling and heavy UPS devices.
On-call duties are shared among team members. On call duties require ability to resolve issues 24 hours per day while on call.
Able to effectively work remotely.
Ability to travel domestically and internationally.
Compensation Salary $80,400 - $120,700/ year
The compensation salary for this position will be determined during the interview process and will vary based on multiple factors, including but not limited to prior experience, relevant expertise, current business needs, and market factors.
As required by the Los Angeles Fair Chance Initiative Ordinance and San Francisco Fair Chance Ordinance, qualified applicants for our Los Angeles and San Francisco offices with arrest and conviction records will be considered.
About The Firm
K&L Gates is a fully integrated global law firm with lawyers located across five continents in more than 40 offices. We have experienced dramatic growth in the past decade and now rank among the largest U.S. based law firms in the world. We take pride in constantly striving for innovation, imagination and an entrepreneurial spirit. We come up with big ideas and then roll up our sleeves to get the job done, guiding our clients through their most complex issues in a variety of industry sectors and across multiple regions of the world.
The industry recognition the firm has garnered emanates from the foundation of a global community aligned on behalf of our clients. The people at K&L Gates are committed to working together to create a legacy for each other, the firm, our clients, and the communities in which we serve. We thrive in an inclusive and socially conscious environment that embraces diversity and takes a holistic approach to the career evolution of all our professionals.
For more information or to view other job opportunities, please click here to go back to our careers page.
Notice: We participate in E-Verify in certain Firm locations for purposes of verifying employment eligibility.
Benefits
K&L Gates offers our personnel a comprehensive suite of benefits to help meet your needs now and in the future. Depending on your eligibility, options for full-time personnel include:
Medical/Prescription Drug Coverage (including a Health Savings Account feature)
Back-up Child/Elder Care and access to a caregiving concierge
Dental Insurance
Wellness Program
Vision Insurance
Pre-tax Commuting Benefits
401(k) Retirement Plan and Profit Sharing
Business Travel Accident Insurance
Short- and Long-term Disability Protection
Pet Insurance
Life Insurance (including Basic, Supplemental, Spouse, Child, and Accidental Death and Dismemberment)
Health Advocacy Services
Paid Time Off (25-30 days per year)
Identity Protection/Restoration and Fraud Insurance
Parental Leave (18 weeks of which 6 are paid; short-term disability may provide additional paid time off)
Student loan refinancing options and access to a student loan concierge service
Paid Holidays (12)
Addiction Resources
Family Building Benefits
Breast Milk Delivery and Lactation Support Services
Flexible Spending Accounts
Employees also may be eligible to receive bonuses and certain expense reimbursements
Employee Assistance Program
Professional Development and CLE Credit Opportunities
529 Deductions
Relocation
Accident Insurance
Employee Referral Program
Critical Illness Insurance
Hybrid/Remote Work Opportunities
Hospital Indemnity Insurance
Perks including: Technology, Entertainment, and Travel Discount Programs
Bereavement Leave
All other benefits (such as leaves of absence) required by law
EQUAL EMPLOYMENT OPPORTUNITY
The Firm is an equal opportunity employer. All employment decisions will be based on merit, qualifications, competence, and business need. Employment practices will not be influenced or affected by a person’s race, color, religion, sex (including pregnancy, childbirth, or related conditions), national origin, age, sexual orientation, gender identity or expression, marital status, disability, genetic information, military or veteran status, or any other characteristic protected by applicable law. This policy governs all aspects of employment including, without limitation, recruiting, hiring, compensation, benefits, promotion, assignment, and dismissal. In addition, it is the Firm’s policy to provide an environment that is free of all unlawful harassment including, without limitation, that based on sex, race, age, disability, or national origin. The Firm complies with federal and state disability laws and makes reasonable accommodations for applicants and employees with disabilities. If you require reasonable accommodation in completing this application, interviewing, or otherwise participating in the employee selection process, please contact askHR@klgates.com .
Show more
Show less","Data Center Operations, Power and UPS Systems, Networking, DHCP Concepts, Layer 1 Troubleshooting, Power Operation, Power Draw Analysis, UPS Sizing, Server Room Hardware, Cabinets, PDUs, Server Room Layout Types, Power Design, Communication Skills, Interpersonal Skills, Adaptability, FastPaced Work Environment, Bachelors Degree, Information Technology Field Experience, Collaborative Demeanor, Vendor and Data Center Staff Management, OnCall Duties, Remote Work, Domestic and International Travel, Microsoft Office Suite, Visio, Excel, PowerPoint, Schneider's Struxureware, UPS Test Lab Operations, Power Consumption Analysis, Rack Elevation Diagrams, Hardware Identifiers, Data Extraction, Asset Management, Media Destruction Policy and Procedures, Project Management, Problem Solving, Time Management, Physical Work, Heavy Lifting","data center operations, power and ups systems, networking, dhcp concepts, layer 1 troubleshooting, power operation, power draw analysis, ups sizing, server room hardware, cabinets, pdus, server room layout types, power design, communication skills, interpersonal skills, adaptability, fastpaced work environment, bachelors degree, information technology field experience, collaborative demeanor, vendor and data center staff management, oncall duties, remote work, domestic and international travel, microsoft office suite, visio, excel, powerpoint, schneiders struxureware, ups test lab operations, power consumption analysis, rack elevation diagrams, hardware identifiers, data extraction, asset management, media destruction policy and procedures, project management, problem solving, time management, physical work, heavy lifting","adaptability, asset management, bachelors degree, cabinets, collaborative demeanor, communication skills, data center operations, data extraction, dhcp concepts, domestic and international travel, excel, fastpaced work environment, hardware identifiers, heavy lifting, information technology field experience, interpersonal skills, layer 1 troubleshooting, media destruction policy and procedures, microsoft office suite, networking, oncall duties, pdus, physical work, power and ups systems, power consumption analysis, power design, power draw analysis, power operation, powerpoint, problem solving, project management, rack elevation diagrams, remote work, schneiders struxureware, server room hardware, server room layout types, time management, ups sizing, ups test lab operations, vendor and data center staff management, visio"
Data & Progress Analyst (Ref ID: 230),NextDecade,"Houston, TX",https://www.linkedin.com/jobs/view/data-progress-analyst-ref-id-230-at-nextdecade-3718470080,2023-12-17,Texas,United States,Associate,Onsite,"ABOUT NEXTDECADE
NextDecade Corporation
is an energy company accelerating the path to a net-zero future and leading innovation in more sustainable LNG and carbon capture solutions, NextDecade is committed to providing the world access to cleaner energy. Through our subsidiaries Rio Grande LNG and NEXT Carbon Solutions, we are developing a 27 MTPA LNG export facility in South Texas along with one of the largest carbon capture and storage projects in North America. We are also working with third-party customers around the world to deploy our proprietary processes to lower the cost of carbon capture and storage and reduce CO2 emissions at their industrial-scale facilities. By combining emissions reduction associated with our carbon capture and storage project, responsibly sourced gas, and our pledge to use net-zero electricity, Rio Grande LNG is expected to produce
a lower carbon intensive LNG
for the world.
NextDecade’s common stock is listed on the Nasdaq Stock Market under the symbol “NEXT.” NextDecade is headquartered in Houston, Texas.
SUMMARY OF THE ROLE:
This is a Data and Progress Analyst position within the Project Controls department to support execution of over 12 billion Dollar LNG Project. This role is responsible for designing, developing, and deploying project analytic dashboards using Microsoft BI technologies such as SQL, Power BI, etc.
RESPONSIBILITIES:
• Using Power BI, create dashboards and interactive visual reports.
• Recognize project requirements in the context of BI and create data models to transform raw data into relevant insights.
• Define key performance indicators (KPIs) with specific objectives and track them regularly.
• Analyze data and display it in reports to aid decision-making.
• Create, test, and deploy Power BI scripts, as well as execute efficient deep analysis.
• Use Power BI to run DAX queries and functions.
• Create charts and data documentation with explanations of algorithms, parameters, models, and relationships.
• Construct a data warehouse.
• Use SQL queries to get the best results.
• Make technological adjustments to current BI systems to improve their performance.
• For a better understanding of the data, use filters and visualizations.
• Analyze current ETL procedures to define and create new systems.
REQUIREMENTS:
Minimum/preferred experience required for the position:
•
3+ years
working with Power BI and DAX.
•
4+ years
of experience working in an analyst role or related education.
•
4+ years
of experience in tools and systems on MS SQL Server BI Stack, including SSRS and TSQL, Power Query, MDX.
• Strong experience in excel (pivots, formulas, and charts) to dissect preexisting stakeholder workflows.
•
2+ years
of experience in EPC (Engineering, Procurement and Construction) Project environment is preferred.
Minimum/preferred knowledge, skills and abilities required of the position:
• Background with BI tools and systems such as Power BI.
• Prior experience in data-related tasks.
• Understanding of the Microsoft BI Stack.
• Be familiar with MS SQL Server BI Stack tools and technologies, such as SSRS and TSQL, Power Query, MDX, Power BI, and DAX.
• Analytical thinking for converting data into relevant reports and graphics.
• Capable of enabling row-level data security.
• Knowledge of Power BI application security layer models.
• Ability to run DAX queries on Power BI desktop.
• Proficient in doing advanced-level computations on the data set.
• Good communication skills are required to communicate with project team.
• Judgement, trust, and carefulness in handling sensitive and confidential information.
• Strong organizational and time management skills to prioritize workload during peak periods.
• Ability to identify and implement actions with minimum directions.
Required/preferred education:
• Bachelor’s in computer science, or information system, or engineering, or construction management, or business.
TRAVEL REQUIREMENTS:
Assignment in Houston may need to visit to project site during construction phase.
ADDITIONAL INFORMATION:
Communicates effectively at all levels of the organization to ensure clarity in expectations requirements, and deliverables. Excellent analytical skills.
Team Player, good communication and reporting skills
Welcomes working in a fast paced challenging and diverse entrepreneurial environment
Hands-on type of personality
This Position Description is not an exhaustive list of the duties and responsibilities, and the employee is expected to perform other duties as necessary and assigned. The duties and responsibilities of this position may be modified at any time to meet changing business needs.
*In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire.
NEXTDECADE VALUES
Safety
– We make safety a priority. Everything we do relies on the safety of our people and the communities around us.
Integrity
– We do the right thing, and are open, ethical, and fair. We hold ourselves to the highest standards in all that we do.
Honesty
– We value truth and honesty in ourselves and others. We honor our commitments and take responsibility for our actions.
Respect
– We listen, and respect people, the environment, and the communities in which we live and work.
Transparency
– Transparency builds trust. We promote open communication with our people, our customers, and all our stakeholders.
Diversity
– We value diversity of people and thought. It takes people with different strengths, ideas, and cultural backgrounds to make our company succeed.
Show more
Show less","Microsoft BI, SQL, Power BI, DAX, Data visualization, Dashboards, Reporting, Data analysis, KPI, ETL, Power Query, MDX, SSRS, TSQL, Data security, Data modeling, Data warehouse, Power BI desktop, Data computation, Communication, Analytical thinking, Problem solving, Teamwork, Project management, Time management, Prioritization, Attention to detail, Bachelor's degree in computer science information system engineering construction management or business","microsoft bi, sql, power bi, dax, data visualization, dashboards, reporting, data analysis, kpi, etl, power query, mdx, ssrs, tsql, data security, data modeling, data warehouse, power bi desktop, data computation, communication, analytical thinking, problem solving, teamwork, project management, time management, prioritization, attention to detail, bachelors degree in computer science information system engineering construction management or business","analytical thinking, attention to detail, bachelors degree in computer science information system engineering construction management or business, communication, dashboard, data computation, data security, dataanalytics, datamodeling, datawarehouse, dax, etl, kpi, mdx, microsoft bi, power bi desktop, power query, powerbi, prioritization, problem solving, project management, reporting, sql, ssrs, teamwork, time management, tsql, visualization"
MAOP Data Analyst,Acadian Ambulance,"Houston, TX",https://www.linkedin.com/jobs/view/maop-data-analyst-at-acadian-ambulance-3778734216,2023-12-17,Texas,United States,Associate,Hybrid,"Job Description
The contract MAOP Data Analyst will work with a team of four to five other analysts under the supervision of a team lead. Duties and tasks would include:
Compilation
Sorting and indexing of MAOP records for historic pipeline projects
Reviewing and conducting comparative analysis of MAOP records
Correlating technical documents against baseline GIS data to identify any gap between the two Following documented workflow and procedures.
Required Skills And Experience
Must be familiar with pipe construction and MAOP validation work
MAOP/MOP records verification for pipelines, components, and facilities
Experience with PHMSA Audits for pipelines
Analysis of MAOP/MOP for high pressure pipeline segments
Procurement process for pipeline materials and components
Ability to read and interpret pipeline as built drawings
Working knowledge of pipeline data tools such PODS, GIS, etc.
Working knowledge of pipeline materials and yield strength calculation
Working knowledge of PHMSA 49 CFR Part 192/195 regulations and fundamental knowledge of construction and operation of high-pressure pipelines
Strong verbal and written communication and customer service skills to interact with other groups
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.
Liberty Energy Services, a division of Safety Management Systems, has extensive professional experience and expertise within the oil and gas industry. Our clients have come to realize that they receive phenomenal services from an elite group of highly skilled consultants that deliver every time.
Show more
Show less","Data Analysis, Data Compilation, Data Sorting, Data Indexing, Data Review, Comparative Analysis, GIS Data, PHMSA Audits, Pipeline Materials, Pipeline Components, Pipeline Construction, Pipeline Drawings, Pipeline Regulations, Construction Operations, Verbal Communication, Written Communication, Customer Service","data analysis, data compilation, data sorting, data indexing, data review, comparative analysis, gis data, phmsa audits, pipeline materials, pipeline components, pipeline construction, pipeline drawings, pipeline regulations, construction operations, verbal communication, written communication, customer service","comparative analysis, construction operations, customer service, data compilation, data indexing, data review, data sorting, dataanalytics, gis data, phmsa audits, pipeline components, pipeline construction, pipeline drawings, pipeline materials, pipeline regulations, verbal communication, written communication"
Azure Data Engineer,Extend Information Systems Inc.,"Houston, TX",https://www.linkedin.com/jobs/view/azure-data-engineer-at-extend-information-systems-inc-3716008950,2023-12-17,Texas,United States,Mid senior,Onsite,"Job Title: Azure Data Engineer
Location: Houston, TX (Day 1 Onsite)
Duration: Full Time
Experience level : 7+ yrs
Technical Skills
You have a minimum of 4+ years' experience working with Azure Data tools (ADF,Data Catalog, event hub,IOT hub) etc
You have proficiency in PySpark ,Python and SQL
Experience working with Databricks ,SQL End Point, Synapse
Knowledge on Power BI and willingness to enhance PBI skill set.
You have already written code capable of efficiently handling large volumes of data across large numbers of tables, and brought this code into a controlled productive environment.
Knowledge and experience in DevOps environments is a plus
Roles & Responsibilities
You are strong on handling large and complex amounts of structured data, you have a track record of having delivered end-to-end data solutions in corporate environments.
As an all-rounder, you enjoy taking time to understand business requirement, articulate a data product solution and then roll up your sleeves and develop it into production grade.
You enjoy navigating ambiguity and working as part of a distributed team to solve business problems using data.
You are comfortable with agile development processes, with both scheduled and ad-hoc release cycles.
Good English communication skills are a must, both spoken and written
You are comfortable working with some level of independence in a multinational environment
You are happiest solving concrete problems
You are able to lead a discussion with Business and understand requirements
Thanks & Regards
Anoop Tiwari
Extend Information Systems
Cell: - 571 - 386 - 2431
Email:
Anoop@extendinfosys.com
Show more
Show less","Azure Data tools, ADF, Data Catalog, Event hub, IOT hub, PySpark, Python, SQL, Databricks, SQL End Point, Synapse, Power BI, DevOps, Data solution, Data product solution, Agile development, English communication skills","azure data tools, adf, data catalog, event hub, iot hub, pyspark, python, sql, databricks, sql end point, synapse, power bi, devops, data solution, data product solution, agile development, english communication skills","adf, agile development, azure data tools, data catalog, data product solution, data solution, databricks, devops, english communication skills, event hub, iot hub, powerbi, python, spark, sql, sql end point, synapse"
Senior Data Engineer (Remote First),European Wax Center,"Texas, United States",https://www.linkedin.com/jobs/view/senior-data-engineer-remote-first-at-european-wax-center-3757566437,2023-12-17,Texas,United States,Mid senior,Remote,"Perks & Benefits
Remote-First Workplace
Flexible Fridays
Diversity, Equity & Inclusion Council
Monthly Remote Stipend
Professional Development Stipend (up to $500 annually)
1 Wellness/Mental Health Paid Day Off
1 Volunteer Paid Day Off
Health Benefits (Medical, Dental, Vision)
HDHP with HSA plan (annual employer contribution to HSA)
Employer-Paid Basic Life Insurance and AD&D
Employer-Paid Short- and Long-term Disability
Employer-Paid Wellness Reward Program
Employer-Paid Mental Health Benefit
Employer-Paid Employee Assistance Program
Employer-Paid Out of State Medical Travel Benefit
401(k) Safe-Harbor Matching
Ancillary Benefits (pet insurance, legal coverage, identity theft protection, accident, hospital, and critical illness coverages)
Paid Time Off (increases with tenure)
Paid Parental, Adoption, and Foster Leave
Out of State Medical Travel Benefit
About The Role
EWC is looking for a motivated Senior Data Engineer to join our growing team of data experts. In this role, you will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross-functional teams. The ideal candidate is an experienced data pipeline builder and engineer who enjoys optimizing data systems and building them from the ground up. The Sr. Data Engineer will support our data analysts/scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.
A Day In The Life
Collecting, organizing, managing, and converting raw data into a format that can be easily analyzed by Business Intelligence analysts and data scientists.
Building and maintaining data pipelines that collect and transport data from various sources to EWC’s data storage systems.
Using algorithms and programming languages such as SQL and Python to prepare data for analysis.
Working closely with the management and end-users to understand and address business requirements related to data storage, management, and analysis.
Creating data analysis tools and developing new data validation methods to ensure data accuracy and completeness.
Identifying ways to make data more reliable, efficient, and accessible to relevant stakeholders.
Creating and maintaining the organization’s software and hardware architecture to support efficient and secure data storage and management.
Conducting research and troubleshooting to address potential problems that may arise in the data storage and management systems.
Play a key role in designing and crafting a modern Data and Information Delivery and Analytics platform in the cloud to support retail service and product distribution for the US market.
Create and maintain optimal data pipeline architecture.
Assemble large, complex data sets that meet functional/non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Operations, FP&A, and Supply Chain teams to assist with data-related technical issues and support their data infrastructure needs.
Keep our data separated and secure during transmission and at rest.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Work with data and analytics experts to strive for greater functionality in our data systems.
What Sets You Apart
Adopts EWC values in personal work behaviors, decision making, contributions and interpersonal interactions.
Helps shape a positive work environment by demonstrating and influencing others to reward performance and value ""can do"" people, accountability, diversity and inclusion, flexibility, continuous improvement, collaboration, creativity, and fun.
Experience with commercial data engineering/science solution initiatives.
Ability to manage a broad range of deliverables with ambiguous task symptomatology while consistently achieving collaborative success with others to accomplish goals.
Works well in a team environment and takes pride in participating in projects that employ the skills of all team members.
Ability to learn quickly in a dynamic environment and to troubleshoot issues.
Business savvy communications skills and concise written communication skills.
Ability to be self-sufficient and self-driven in a small team.
Understanding of the current threat and vulnerability landscape.
Excellent organization and presentation skills.
Education And Experience
BS in Computer Science, Data Science, or equivalent.
7+ years of professional software development or data engineering experience.
5+ years of experience using and strategizing the use of DBT and Airflow.
Strong working knowledge of SQL, of datastores and their tradeoffs (including relational, columnar, and document stores), data modeling, data structures, data manipulation.
Strong knowledge of Extract, Transform, Load (ETL) pipeline design, tooling, and support.
Experience designing, building and optimizing ‘big data’ data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Proven ability to architect, implement, and optimize high throughput data pipelines.
Experience deploying production systems in the cloud (i.e., AWS, Azure).
Strong communication skills in writing and conversation.
Experience with tools we use every day:
Storage: Snowflake, AWS Storage Services (e.g., S3, RDS, Glacier)
ETL/BI: Astronomer, DBT, Domo, Tableau, PowerBI
Proven passion and talent for teaching fellow engineers and non-engineers.
Experience with encryption at rest, including multiple approaches and tradeoffs.
Experience in Retail operations.
European Wax Center is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, age, disability status, protected veteran status, or any other characteristic protected by law.
This job description is a general description of essential job functions. It is not intended to describe all duties someone in this position may perform. All employees of EWC and operating subsidiaries are expected to perform tasks as assigned by supervisory/management personnel, regardless of job.
Show more
Show less","SQL, Python, Big Data, Data Engineering, Data Pipelines, AWS, DBT, Airflow, Data Modeling, Data Structures, Data Manipulation, Extract Transform Load (ETL), Snowflake, AWS Storage Services, Astronomer, Domo, Tableau, PowerBI, Encryption, Data Analysis, Data Science","sql, python, big data, data engineering, data pipelines, aws, dbt, airflow, data modeling, data structures, data manipulation, extract transform load etl, snowflake, aws storage services, astronomer, domo, tableau, powerbi, encryption, data analysis, data science","airflow, astronomer, aws, aws storage services, big data, data engineering, data manipulation, data science, data structures, dataanalytics, datamodeling, datapipeline, dbt, domo, encryption, extract transform load etl, powerbi, python, snowflake, sql, tableau"
Big Data Engineer,Optomi,"Plano, TX",https://www.linkedin.com/jobs/view/big-data-engineer-at-optomi-3766938892,2023-12-17,Texas,United States,Mid senior,Remote,"Big-Data Engineer:
Optomi, in partnership with a leader in Automotive Technology, is looking for a creative problem solver who enjoys collaborating with other software engineers, ML Engineers, and Data Scientists. This role is remote but candidates need to be in the DFW area. The ideal candidate will have Data Engineering experience with heavy software engineering skills and experience using Spark, Scala, Terraform, and AWS cloud tools.
Summary:
We are inviting applications for software/big-data engineering professionals with proven experience building big-data solutions leveraging cutting edge technologies such as AWS (EMR, Redshift, Glue, Athena, Lamda, CloudFormation), Terraform, GraphQL, Spark, Databricks, and Kubernetes.
Responsibilities:
Lead the design of scalable big data platforms/solutions used to ingest and process large volumes of financial/capital markets data
Develop and automate large scale, high-performance, scalable platform (batch and/or streaming) to drive faster analytics.
Build and maintain custom frameworks to support engineering/analytics needs.
Partner with analytic consumers and data scientists to build and improve new/existing constructs and solve data engineering problems at scale.
Deploy inclusive data quality checks to ensure high quality of data.
Evangelize high quality software engineering practices towards building data infrastructure and pipelines at scale.
Qualifications:
4+ years of experience as a Data Engineer or in a similar role working with large petabyte size data sets.
Experience writing clean, concise, tested & maintainable code in Spark Scala.
Experience with SQL, data modeling, data warehousing, and building ETL pipelines.
Experience building and deploying applications in AWS.
Proven track record of independently delivering big data solutions.
Knowledge of continuous integration, testing methodologies, TDD and agile development methodologies.
Exposure to structured or unstructured storage and distributed caching.
Experience in open-source technologies is plus.
Structured thinking with ability to easily break down ambiguous problems and propose impactful solutions.
You can manage ambiguity and are comfortable being set loose without a lot of direction
Benefits:
Remote work environment
Advanced team using advanced tech
Cool industry and competitive pay
Show more
Show less","Data Engineering, Spark, Scala, Terraform, AWS, EMR, Redshift, Glue, Athena, Lamda, CloudFormation, GraphQL, Databricks, Kubernetes, SQL, Structured Query Language (SQL), Data Warehousing, ETL Pipelines, Continuous Integration, Testing Methodologies, TDD, Agile Development, Structured Storage, Unstructured Storage, Distributed Caching, OpenSource Technologies","data engineering, spark, scala, terraform, aws, emr, redshift, glue, athena, lamda, cloudformation, graphql, databricks, kubernetes, sql, structured query language sql, data warehousing, etl pipelines, continuous integration, testing methodologies, tdd, agile development, structured storage, unstructured storage, distributed caching, opensource technologies","agile development, athena, aws, cloudformation, continuous integration, data engineering, databricks, datawarehouse, distributed caching, emr, etl pipelines, glue, graphql, kubernetes, lamda, opensource technologies, redshift, scala, spark, sql, structured query language sql, structured storage, tdd, terraform, testing methodologies, unstructured storage"
Data Engineer—Data Modeling & Analytics,Curate Partners,"Texas, United States",https://www.linkedin.com/jobs/view/data-engineer%E2%80%94data-modeling-analytics-at-curate-partners-3780185747,2023-12-17,Texas,United States,Mid senior,Remote,"We are seeking a Data Engineer—Data Modeling & Analytics Solution Development to join the Network Operations team. The Network Operations team monitors and
maintains assets, supporting both frontline representation in the field and strategic
programs.
As a Data Engineer—Data Modeling & Analytics Solution Development, you
will partner cross-functionally to serve as the team’s data modeling and analytics subject matter expert
and provide user interface (“UI”) experiences and workflows that lead to net cost reductions for the
Company.
Description % of Time Spent
•Provide technical expertise to and leadership for the Unmanned Aircraft System
(“UAS”) program by analyzing, designing, developing, and implementing UAS
data-driven UI and deliverables.
•Deliver cost effective, accurate, photo-realistic, interactive models of the
Company’s U.S. assets.
•Lead UAS Team projects to develop analysis automation and UIs for
employees/customers. Work with IT, Shared Services and cross functional
teams to identify, develop and project manage system integrations to ensure
UAS program delivers an integrated solution.
•Partner with business leaders and customers to identify opportunities for UAS data;
determine scopes, evaluate requirements, manage implementations, and monitor quality during and post-production.
•Develop customer-facing data that can be generated using standard data collection
and processing techniques.
•Collaborate with the Manager, UAS Operations to ensure collected data meet
processing and analysis requirements; build quality and production metrics and
ensure requirements are scalable.
•Approximately 10% of the time the role will be performed outside of a conventional
office environment.
•Other duties as assigned.
What You Need to Succeed
Education Education Level Description
Required 4 Year / Bachelors
Degree
· Bachelor’s degree in Computer Science,
Engineering, Remote Sensing, Physics, or a
related field required.
Preferred Graduate Degree · Master’s degree in a related field preferred.
Experience Description
Required · A minimum of 3 years of relevant UAS, GIS, modeling, application design, or
other quantitative industry experience required.
· A minimum of 5 years of professional work experience required.
· Experience transferring data from 3D modeling software or comparable
relevant projects required.
· Experience with point cloud manipulation, CAD, third-party GIS, or
photogrammetry software required.
Preferred · Experience with remote sensing, oblique imagery, LIDAR, or 3D point clouds
preferred.
· UAS knowledge and capture from motion experience preferred.
· Photogrammetry and professional surveyor certificates preferred.
Show more
Show less","Data Engineering, Data Modeling, Analytics Solution Development, Unmanned Aircraft System (UAS), User Interface (UI) Development, 3D Modeling, Data Visualization, Cost Reduction, Data Collection, Data Processing, Data Validation, Data Quality Management, Requirements Gathering, Project Management, System Integration, Business Intelligence, Customer Relationship Management, GIS, Photogrammetry, Remote Sensing, CAD, Point Cloud Manipulation","data engineering, data modeling, analytics solution development, unmanned aircraft system uas, user interface ui development, 3d modeling, data visualization, cost reduction, data collection, data processing, data validation, data quality management, requirements gathering, project management, system integration, business intelligence, customer relationship management, gis, photogrammetry, remote sensing, cad, point cloud manipulation","3d modeling, analytics solution development, business intelligence, cad, cost reduction, customer relationship management, data collection, data engineering, data processing, data quality management, data validation, datamodeling, gis, photogrammetry, point cloud manipulation, project management, remote sensing, requirements gathering, system integration, unmanned aircraft system uas, user interface ui development, visualization"
Lead Data Engineer,Brookwood Search & Selection,"Texas, United States",https://www.linkedin.com/jobs/view/lead-data-engineer-at-brookwood-search-selection-3782726725,2023-12-17,Texas,United States,Mid senior,Remote,"Lead Data Engineer - Up to $230k + bonus (depending on experience)
Location: Texas (Remote)
Join a robust and growing consultancy that offers end-to-end solutions for their clients, digitally transforming their business, solving IT challenges, and improving ROI on data and technology.
You will be part of a cross functional team, leading engineering practices in implementing new technologies and delivering end-to-end solutions. As a leader on projects, you will engage with internal and external stakeholders, understanding requirements and long-term strategy to increase value gained from data. Be part of commercial element of the business ensuring project success through risk analysis and mitigation strategies.
Requirements:
5+ years' experience in a data engineering position
Masters in data, maths, physics, computer science or related degree
Strong track record of working with various data platforms (SQL, SSIS, Postgres, Oracle, etc)
Experience as a project lead to completion
3+ years' experience with commercial experience with Azure, GCP, or AWS
Experienced with data visualisation tools
Apply now!
Show more
Show less","Data Engineering, SQL, SSIS, Postgres, Oracle, Azure, GCP, AWS, Data Visualization","data engineering, sql, ssis, postgres, oracle, azure, gcp, aws, data visualization","aws, azure, data engineering, gcp, oracle, postgres, sql, ssis, visualization"
Senior Software Engineer (Data Engineering),Harbor Health,"Texas, United States",https://www.linkedin.com/jobs/view/senior-software-engineer-data-engineering-at-harbor-health-3787915497,2023-12-17,Texas,United States,Mid senior,Remote,"Harbor Health looking for a Senior Software Engineer to become a member of our team! Harbor Health is an entirely new multi-specialty clinic group in Austin, TX utilizing a modern approach to co-create health with those who get, give, and pay for it, allowing everyone to fully flourish. Join us as we build a fully integrated system that connects care to a better payment model that truly puts the human being at the center.
As a Senior Software Engineer with a focus on data engineering at Harbor Health, you will play a pivotal role in our data transformation initiatives. You will work with a team of dedicated professionals using AWS, Snowflake, and DBT Labs to create efficient data pipelines. Your expertise in Python, SQL, and SQL performance optimization, along with your ability to take a proactive and collaborative approach to your work, will be essential in shaping the future of healthcare data analytics.
Our Sr. Software Engineer will be responsible for:
Collaborate with cross-functional teams to design, develop, and maintain data engineering solutions.
Develop and optimize ETL processes using DBT Labs and other technologies.
Design and implement data models and data transformations for our Snowflake data warehouse.
Create robust, scalable, and maintainable code using Python.
Write efficient SQL queries and be an expert at SQL performance tuning.
Participate in daily standup meetings with the team to ensure alignment and effective collaboration.
Independently manage and execute projects while being adaptable and productive, whether working alone or in a team.
Successful Sr. Software Engineer's will have:
Bachelor's degree in Computer Science, Software Engineering, or a related field (or equivalent work experience).
5+ years in a Data Engineering role with proven experience as a software engineer
Demonstrated strong proficiency in AWS, Snowflake, and DBT Labs.
Advanced skills in Python for data engineering and transformation.
Expertise in SQL and SQL performance optimization.
Curious and action-oriented mindset, self-motivated, and able to work effectively both independently and in a team.
Excellent problem-solving skills and the ability to drive projects to completion.
Additional Skills & Experiences Preferred include:
Kimball data modeling experience
Inmon data modeling experience
Experience working with healthcare data and healthcare quality metrics such as HEDIS measures.
Ability to travel as needed, less than 10% of time.
If you are passionate about health care and you want to create something new together, we want you to be apart of our team!
Powered by JazzHR
8Lt6GnsYsF
Show more
Show less","Python, SQL, AWS, Snowflake, DBT Labs, Data Engineering, ETL, Data Models, Data Transformations, Data Warehousing, Software Engineering, Kimball Data Modeling, Inmon Data Modeling, Healthcare Data, Healthcare Quality Metrics, HEDIS Measures","python, sql, aws, snowflake, dbt labs, data engineering, etl, data models, data transformations, data warehousing, software engineering, kimball data modeling, inmon data modeling, healthcare data, healthcare quality metrics, hedis measures","aws, data engineering, data models, data transformations, datawarehouse, dbt labs, etl, healthcare data, healthcare quality metrics, hedis measures, inmon data modeling, kimball data modeling, python, snowflake, software engineering, sql"
Senior Data Engineer,Advarra,"Austin, TX",https://www.linkedin.com/jobs/view/senior-data-engineer-at-advarra-3782082706,2023-12-17,Texas,United States,Mid senior,Remote,"Advarra provides integrated solutions that safeguard trial participants, empower clinical sites, ensure compliance, and optimize research performance. Connecting the clinical research ecosystem, Advarra delivers solutions through a site-centric approach that unifies and accelerates the drug development lifecycle, making clinical trials safer, smarter, and faster.
General Summary
This role supports the Data Science team as a Sr Data Engineer for enterprise Data Warehouse (DW) Platform. Will require deep expertise in Snowflake and writing complex data transformations to support Business and Product Analytics. Understanding of security measures provided by Snowflake to secure the data on the platform.
Principal Duties & Responsibilities
Responsible for security and administration of the Snowflake Data Platform.
Build data pipeline using Fivetran to ingest large volume of data from both internal applications and external data source.
Write data transformation for Enablement of BI/ Analytics, Data Sciences, and external customers.
Configure Snowflake data security features to secure the data on the platform
Supports MDM implementation plan and direction toward a single version of truth.
Create, follow, and ensure compliance with database security.
Provide technical recommendations for reporting infrastructure project costing.
Design, maintain and deploy data sources for operational and strategic dashboards.
Education
Bachelor’s degree or equivalent combination of education and related work experience.
Snowflake certification preferred.
Experience
5 years of experience on Snowflake platform.
Experience in writing data transformation with dbt platform.
Experience in building connector with Fivetran platform.
Experience with Sigma Computing platform is a plu.s
Experience in writing test automation scripts.
Data Architecture & Data modeling experience in Clinical trials domain / Life Sciences.
Knowledge of data management tools and process; Data governance tools is a plus.
Working experience with version control platforms, e.g. Github, and agile methodologies and supporting tools JIRA.
Knowledge, Skills, Abilities
Understanding of Snowflake Warehouses, reader accounts, SSO Setup, and data masking policies.
Good understanding of Change Data Capture and Change Data Tracking.
Understanding challenges of ingesting large volume data.
Ability to writ complex data transformation logic using Advanced SQL query skills such as complex joins, sub-queries, grouping, aggregation, as well as stored procedures.
Programming languages, especially Python and Java.
Knowledge of data modeling and transformation tools like dbt, Tableau data prep etc.
Expert in data warehousing concepts, methodologies, and best practices.
Position requires a high level of responsibility regarding confidential information; must maintain confidentiality at all times.
Must be comfortable independently evaluating a situation, exercising good judgment and discretion, and independently making a decision matter of significance.
Excellent oral and written communication skills including the ability to speak in front of large groups.
Comfort working in a geographically distributed team-based environment.
Ability to handle stress and interact with others in a professional manner.
Physical and Mental Requirements:
Sit or stand for extended periods of time at stationary workstation
Occasionally, carry, raise, and lower objects of up to 10lbs
Learn and comprehend basic instructions
Focus and attention to tasks and responsibilities
Verbal communication; listening and understanding, responding and speaking
#mogul
EEO Statement
Advarra provides equal employment opportunity to all individuals regardless of their race, color, religion, creed, sex, sexual orientation, gender identity, national origin, age, disability, veteran, marital, or domestic partner status, citizenship, genetic information or any other status or characteristic covered by federal, state or local law. Further, the company takes affirmative action to ensure that applicants are employed, and employees are treated during employment without regard to any of these characteristics. Discrimination of any type will not be tolerated.
EEO/M/F/Disabled/Vets
Show more
Show less","Snowflake, Data Warehouse, dbt, Python, Java, Tableau data prep, Advanced SQL, Fivetran, Sigma Computing","snowflake, data warehouse, dbt, python, java, tableau data prep, advanced sql, fivetran, sigma computing","advanced sql, datawarehouse, dbt, fivetran, java, python, sigma computing, snowflake, tableau data prep"
Sr. Data & ETL Platform Engineer,Corebridge Financial,"Houston, TX",https://www.linkedin.com/jobs/view/sr-data-etl-platform-engineer-at-corebridge-financial-3706383620,2023-12-17,Texas,United States,Mid senior,Remote,"Who We Are
Corebridge Financial helps people make some of the most meaningful decisions they’re ever going to make. We help them plan and take action to protect the future they envision, and respond to some of life’s most difficult moments through the solutions and services we provide. We do this through our broad portfolio of life insurance, retirement and institutional products, offered through an extensive, multichannel distribution network. We provide solutions for a brighter future through our client centered service, breadth of product expertise, deep distribution relationships, and outstanding team of hardworking and passionate employees.
About The Role
As a Sr. Data & ETL Platform Engineer you will be responsible for leading, influencing AIG’s Datahub platform to address needs of all enterprise datasets at AIG. You will engineer for reliability, scalability, performance, observability and supportability of enterprise Datahub environment.
In this role, you will be responsible for administering, supporting, monitoring the entire technology stacks of Datahub environment. This involves the management and administration of enterprise Datahubs, including IT Data Lake and Worker Datahub. You will be responsible for maturing implementation of Datahub services, and to maintain a highly secure and reliable operational environment. You will be current in Datahub best practices and apply them to the environment to continuously improve performance, scalability, and proactively manage capacity of the environment.
This role provides an opportunity to make a significant impact across L&R and to define Datahub technology roadmaps. This senior leadership role requires extensive experience and is hands-on along with requiring the candidate to lead and influence a small team. The candidate will act as Datahub thought leader and interface with Architecture and Engineering teams, IT Security, and Production Operations to design and implement transformational improvements. The technologies you will manage include but are not limited to the following: Data Stage, Talend, AWS EMR, Snowflake, Oracle, SQL Server and AWS database services.
Please note:
The job can only be performed in the State locations listed: Houston, TX and Remote-TX.
The Senior Datahub Engineer
is expected to perform the following duties:
Leading the engineering, design, development, and launch of high available, low latency, flexible and scalable Datahub services.
Significantly influence overall strategy by helping define features, drive the system architecture, and spearhead the best practices that enable quality product and services.
Design and implement service data models, ETL process, caching models and APIs.
Promoting infrastructure as code and helping teams to shift to an automated deployment process
Responsible for improving the performance, reliability, and observability of the Datahub platform
Design, build, and maintain highly scalable and reliable data integration layer
Contribute to the design and documentation of the Datahub system architecture
Troubleshoot and resolve production issues across services and multiple layers of the stack
Maintain a high level of code quality and testing
Extending service monitoring capabilities and publicly available data such as uptime and performance metrics
Expanding the Datahub application and services for availability and redundancy
Maintaining back end services and data stores to power the Datahub application
Develop and oversee monitoring systems to measure usage and ensure operational stability.
Monitor the process during the entire lifecycle for adherence and updating or creating new process for improvement and cost effectiveness.
Excellent communication, conceptual, critical thinking, analytical, problem-solving abilities, and organizational skills.
Complex Problem Solving – Identifying complex problems and reviewing related information to develop and evaluate options and implement solutions.
Primary Technical Skills
ETL Tools: Talend, IBM DataStage, AWS EMR
Cloud: AWS
Secondary Technical Skills
Data: SQL (Snowflake, Oracle)
Position Requirements
10+ years
Hands-on Administration experience with ETL tools like Talend, DataStage and AWS EMR or Hadoop
Technical experience in troubleshooting and ITIL process and practices
7+ years
Administration expertise in Talend, IBM DataStage and AWS EMR or Hadoop
Experience supporting infrastructure and applications hosted in AWS or Azure
Experience establishing and maturing ETL and High availability best practices
Strong proficiency in AWS services, particularly EMR and related big data technologies (Hadoop, Hive, Spark, etc.)
Sound knowledge of database management, SQL querying, data modeling, data warehousing, business intelligence, and OLAP (Online Analytical Processing)
Experience managing/working with Windows / Linux infrastructure teams
Experience in creating disaster recovery plans for both on-premise and cloud infrastructures
Excellent communication, conceptual, critical thinking, analytical, problem-solving abilities, and organizational skills
Complex Problem Solving – Identifying complex problems and reviewing related information to develop and evaluate options and implement solutions.
Comfortable leading discussions with leadership and have experience tailoring the level of technical details to suit the audience
Experience with leading a team
Preferred/Plus Experience
Batchelor’s degree in Computer Science, Information Technology, or related
5 years Proven experience as a Platform Engineer or similar role with a focus on ETL tools and big data technologies.
Familiarity with Administering reporting tools is a plus.
Strong background with Public Cloud Infrastructure management
Excellent problem-solving skills and the ability to work in a dynamic, collaborative environment
Experience with Cloud Cost Management, Demand Forecasting, Budget forecasting, Capacity management, Chargeback mechanism
What our employees like most about working for Corebridge Financial
We care about your professional development. Our career progression program will provide you with the opportunity to develop your skills, strengthen your productivity and be eligible to progressively advance to positions with an increased responsibility and increased compensation.
Our “Giving Back” policy is at the core of our daily operations and guides our future progress. Don’t believe us? We put our money where our mouth is! Corebridge Financial will give you up to 16 hours a year paid time off to volunteer in the community.
Our people are our most important asset therefore we provide a generous benefits plan and competitive pay. Benefit package includes:
Paid Time Off (Corebridge Financial recognizes the importance of work life balance). We offer 24 PTO days to start. YES, 24! 17 paid holidays per calendar year.
A 401(k) Retirement Plan which will be HARD TO BEAT. Our 401K - $1 for $1 match up to 6% with immediate vesting, plus Corebridge Financial automatically contributes an additional 3% into your 401K regardless of if you enroll or not.
We are an Equal Opportunity Employer
Corebridge Financial, Inc., its subsidiaries and affiliates are committed to be an Equal Opportunity Employer and its policies and procedures reflect this commitment. We provide equal opportunity to all qualified individuals regardless of race, color, religion, age, gender, gender expression, national origin, veteran status, disability or any other legally protected categories such as sexual orientation. At Corebridge Financial, we believe that diversity and inclusion are critical to our future and our mission – creating a foundation for a creative workplace that leads to innovation, growth, and profitability. Through a wide variety of programs and initiatives, we invest in each employee, seeking to ensure that our people are not only respected as individuals, but also truly valued for their unique perspectives.
To learn more please visit: www.corebridgefinancial.com
Corebridge Financial is committed to working with and providing reasonable accommodations to job applicants and employees with physical or mental disabilities. If you believe you need a reasonable accommodation in order to search for a job opening or to complete any part of the application or hiring process, please send an email to TalentandInclusion@corebridgefinancial.com. Reasonable accommodations will be determined on a case-by-case basis.
Functional Area
IT - Information Technology
Estimated Travel Percentage (%): No Travel
Relocation Provided: No
American General Life Insurance Company
Show more
Show less","ETL, Talend, DataStage, AWS EMR, SQL, Snowflake, Oracle, AWS, Hadoop, Hive, Spark, OLAP, Linux, Windows, Cloud Cost Management, Demand Forecasting, Budget forecasting, Capacity management, Chargeback mechanism","etl, talend, datastage, aws emr, sql, snowflake, oracle, aws, hadoop, hive, spark, olap, linux, windows, cloud cost management, demand forecasting, budget forecasting, capacity management, chargeback mechanism","aws, aws emr, budget forecasting, capacity management, chargeback mechanism, cloud cost management, datastage, demand forecasting, etl, hadoop, hive, linux, olap, oracle, snowflake, spark, sql, talend, windows"
"Senior Data Engineer - $180k-$220k (Snowflake, Coding)",CyberCoders,"San Antonio, TX",https://www.linkedin.com/jobs/view/senior-data-engineer-%24180k-%24220k-snowflake-coding-at-cybercoders-3766365260,2023-12-17,Texas,United States,Mid senior,Remote,"Permanently Remote in US
Job Title:
Senior Data Engineer - $180k-$220k + Bonus (Snowflake, Coding)
Salary:
$180k-$220k Base + Bonus, No Stock, 401k, Benefits
Requirements:
Expert w/ Snowflake & Coding Ability
Based in beautiful New York City, we are a cutting edge ad-tech org for television-based ads.
We are founded and owned by T.V.s largest publishers.
Our mission is to be bring simplicity & scale to audience based campaigns in television.
We're working with over 100 advertisers and anticipating another year of significant growth!
As a rapidly growing company
(founded in 2017 & up 140% year over year)
we've recently elevated our C-Suite Team in preparation for our next stage of growth and are building our Technology, Product, and Operations Executive Teams.
We have been in business for 7 years and have around 40 employees.
Due to growth, we are actively hiring a Senior Data Engineer with
Snowflake experience (ideally certified)
Ability to write production level code (ideally JavaScript or Python)
Experience building data pipelines from scratch and/or working with APIs
Experience with the following is a big plus!
Fivetran and/or DBT
You will be working with 12 other engineers on the data side + several other technical folks.
This role will consist of tasks such as building out data pipelines, data architecture via snowflake, and data modeling.
If you have this experience, please apply immediately. We are actively interviewing this week and next for this high profile position.
Top Reasons to Work with Us
Compensation: $180k-$220k Base + Bonus, No Stock, 401k, Benefits
Raid Growth: Founded in 2017 & up 140% year over year
Culture: Fast paced, mission driven culture
Technology: Cutting edge technology
What You Will Be Doing
Building data pipelines from scratch
Data architecture via Snowflake
Data modeling
Technical review of everything this group builds.
Mange development velocity, team capacity, and backlogs
Partner closely with the product team
Take on key assignments and delegate as needed
Act as the main technical point of contact for engineering
Translate technical requirements to the rest of the engineering team
What You Need for this Position
Must Have Experience
Snowflake experience
Ability to write production level code (ideally JavaScript or Python)
Experience building data pipelines from scratch and/or working with APIs
Some Experience With:
-
Fivetran and/or DBT
What's In It for You
$180k-$220k Base + Bonus, No Stock, 401k, Benefits
401k
Vacation/PTO
Medical
Dental
Vision
Bonus
401k
Benefits
Vacation/PTO
Medical
Dental
Vision
Bonus
So, if you are a Senior Data Engineer - $180k-$220k + Bonus (Snowflake, Coding) with experience, please apply today!
Colorado employees will receive paid sick leave. For additional information about available benefits, please contact Nitu Gulati-Pauly
Email Your Resume In Word To
Looking forward to receiving your resume through our website and going over the position with you. Clicking apply is the best way to apply, but you may also:
Nitu.Gulati-Pauly@cybercoders.com
Please do NOT change the email subject line in any way. You must keep the JobID: linkedin : MA5-1745335L570 -- in the email subject line for your application to be considered.***
Nitu Gulati-Pauly - VP of Recruiting - CyberCoders
Applicants must be authorized to work in the U.S.
CyberCoders is proud to be an Equal Opportunity Employer
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, protected veteran status, or any other characteristic protected by law.
Your Right to Work
– In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire.
CyberCoders will consider for Employment in the City of Los Angeles qualified Applicants with Criminal Histories in a manner consistent with the requirements of the Los Angeles Fair Chance Initiative for Hiring (Ban the Box) Ordinance.
Show more
Show less","Snowflake, JavaScript, Python, Data Pipelines, APIs, Fivetran, DBT, Data Architecture, Data Modeling, Team Management, Product Collaboration, Key Assignments, Technical Point of Contact, Translating Technical Requirements","snowflake, javascript, python, data pipelines, apis, fivetran, dbt, data architecture, data modeling, team management, product collaboration, key assignments, technical point of contact, translating technical requirements","apis, data architecture, datamodeling, datapipeline, dbt, fivetran, javascript, key assignments, product collaboration, python, snowflake, team management, technical point of contact, translating technical requirements"
Data Engineer Manager - Remote,NRG Energy,"Houston, TX",https://www.linkedin.com/jobs/view/data-engineer-manager-remote-at-nrg-energy-3771060654,2023-12-17,Texas,United States,Mid senior,Remote,"At NRG, we’re bringing the power of energy to people and organizations by putting customers at the center of everything we do. We generate electricity and provide energy solutions and natural gas to millions of customers through our diverse portfolio of retail brands. A Fortune 500 company, operating in the United States and Canada, NRG delivers innovative solutions while advocating for competitive energy markets and customer choice, working towards a sustainable energy future. More information is available at www.nrg.com. Connect with NRG on Facebook, LinkedIn and follow us on Twitter @nrgenergy.
We are looking for a highly skilled Data Engineering Manager who has a passion for data and analytics. Data Engineering Manager will be responsible for managing the delivery of projects, encouraging the adoption and integration of data, working closely with partners and leading a team of Data engineers with various levels of experience and capability.
Requirements
8+ years of experience managing data
5+ years of experience leading project or technical teams which includes experience providing technical direction, thought leadership, coaching and mentoring.
5+ years of strong experience in SQL and Data Analysis
3+ years in development of data pipelines/ETL for data ingestion, data preparation, data integration and data aggregation areas.
3+ years of solid experience in Python and Spark
3+ years of hands-on experience with Amazon Web Services (AWS)
Deep understanding of data warehouse, data cloud architecture, building data pipelines, and orchestration
Experience managing vendor/supplier relationships
Essential Duties/Responsibilities
Manage the design and development of data models and pipelines that capture and transform data from internal and external systems.
Collaborate with team members for the purpose of collecting data and executing the company’s data mission
Communicate effectively with technical and non-technical stakeholders
You will partner with cross functional teams to identify opportunities and continuously develop and improve processes for efficiency
Identify and document standard methodologies, standards, and architecture guidelines
Ensure solutions are highly usable, scalable, and maintainable
Why NRG Is a Great Place To Work
Great company culture!! Voted as a BEST employer by Forbes
A competitive total compensation package, including annual incentive and/or commission
Stock Purchase Plan
Benefits on the first day of employment - Medical, Dental, Vision, Life Insurance, and Short Term Disability, Wellness program, etc.
Company-paid life insurance and disability insurance
401 (k) plan to help save for retirement
Generous PTO plan, plus 8 company holidays, and 3 floating holidays
Numerous discounts, including electricity discounts on NRG brands
If you reside in or intend to work remotely from California, Colorado, New York or Washington State, you may contact Careers@nrg.com for compensation information related to this position and other information as required by applicable law. Please include the job title in your request.
NRG Energy is committed to a drug and alcohol free workplace. To the extent permitted by law and any applicable collective bargaining agreement, employees are subject to periodic random drug testing, and post-accident and reasonable suspicion drug and alcohol testing. EOE AA M/F/Protected Veteran Status/Disability
EEO is the Law Poster (The poster can be found at http://www.eeoc.gov/employers/upload/poster_screen_reader_optimized.pdf)
Level, Title and/or Salary may be adjusted based on the applicant's experience or skills.
Official description on file with Talent.
Show more
Show less","Leadership, Data engineering, Data warehousing, Analysis, Data modeling, Pipelines, Oracle SQL, Python, Spark, AWS, Data aggregation, Data integration, ETL, Vendor management, Data governance, Cloud architecture, DataBricks, Informatica","leadership, data engineering, data warehousing, analysis, data modeling, pipelines, oracle sql, python, spark, aws, data aggregation, data integration, etl, vendor management, data governance, cloud architecture, databricks, informatica","analysis, aws, cloud architecture, data aggregation, data engineering, data governance, data integration, databricks, datamodeling, datawarehouse, etl, informatica, leadership, oracle sql, pipelines, python, spark, vendor management"
Expression of Interest: Data Engineer,Fingerprint for Success (F4S),"Houston, TX",https://www.linkedin.com/jobs/view/expression-of-interest-data-engineer-at-fingerprint-for-success-f4s-3787780492,2023-12-17,Texas,United States,Mid senior,Remote,"We are inviting professionals in high-growth industries who are thinking about their next move or looking for a new opportunity to join our expanding talent pool.
The F4S Talent Pool is a pilot project designed to:
Help job seekers get discovered by our partners based on their anticipated hiring needs.
Provide optional support and resources for job seekers in their career endeavors.
Help individuals understand, and bring out the best in themselves and each other.
The F4S Talent Pool process:
Once you express your interest, you will be asked to complete the F4S work style assessment which measures 48 key attitudes and motivations in the context of work. On completion, you will be automatically added to our growing talent pool and contacted as new opportunities arise.
About Fingerprint For Success (F4S)
Backed by 20+ years of research, F4S’s revolutionary predictive analytics have achieved over 90% reliability in forecasting personal and team motivations, behaviors, and performance. Ultimately, we help people find purpose and fulfillment at work, and help build and scale high performing teams.
Keep in mind, joining our talent pool does not guarantee a job offer. We aim to balance your technical skills with the results of your F4S work style assessment to match the hiring needs of our partners.
Your feedback is a gift! Write to us via:
Powered by JazzHR
uxXiH9aBn4
Show more
Show less","work style assessment, predictive analytics, team motivations, team performance, technical skills","work style assessment, predictive analytics, team motivations, team performance, technical skills","predictive analytics, team motivations, team performance, technical skills, work style assessment"
Senior Data Engineer,Raft,"San Antonio, TX",https://www.linkedin.com/jobs/view/senior-data-engineer-at-raft-3690645958,2023-12-17,Texas,United States,Mid senior,Remote,"This is a U.S. based position. All of the programs we support require
U.S. citizenship to be eligible for employment. All work must be conducted within the continental U.S.
Who we are:
Raft ( https://TeamRaft.com ) is a customer-obsessed non-traditional small business with purposeful focus on Distributed Data Systems, Platforms at Scale, and Complex Application Development, with headquarters in Reston, VA. Our range of clients include innovative federal and public agencies leveraging design thinking, cutting edge tech stack, and cloud native ecosystem. We build digital solutions that impact the lives of millions of Americans.
We’re looking for an experienced
Senior Data Engineer
to support our customer and join our passionate team of high-impact problem solvers.
About the role:
As a Senior Data Engineer, you will be responsible for building and maintaining extract, transform, load (ETL) pipelines to enable comprehensive data operations, from ingest to query. You will use your in-depth understanding of data architecture, database management, and data processing workflows, ensuring efficient and secure handling of our data resources.
What we are looking for:
Minimum of 7 years of hands-on experience in data engineering or related field
Extensive experience in building and managing ETL pipelines
Experienced in building ETL pipelines for comprehensive data operations, meeting and exceeding business value requirements, potentially demonstrated through significant experiences or one exceptionally impactful event
Experience with Cloud-based systems and/or AI/Machine Learning Development
Bachelor's degree in Statistics, Computer Science, Data Science, or a related field
Highly preferred:
Understanding of data security practices and policies, with the ability to design and implement security measures
Exceptional problem-solving abilities and strong analytical skills
Excellent interpersonal and communication skills, able to effectively collaborate with team members and stakeholders
Ability to adapt to changing business requirements and learn new technologies quickly
Advanced certification related to data management or data engineering would be a plus
Clearance Requirements:
Ability to obtain/maintain a Top Secret Security clearance
Work Type:
San Antonio, TX (Local remote)
May require up to 10% travel
What we will offer you:
Highly competitive salary
Fully covered healthcare, dental, and vision coverage
401(k) and company match
Unlimited PTO + 11 paid holidays
Education & training benefits
Annual budget for your tech/gadgets needs
Monthly box of yummy snacks to eat while doing meaningful work
Remote, hybrid, and flexible work options
Team off-site in fun places!
Generous Referral Bonuses
And More!
Our Vision Statement:
We bridge the gap between humans and data through radical transparency and our obsession with the mission.
Our Customer Obsession:
We will approach every deliverable like it's a product. We will adopt a customer-obsessed mentality. As we grow, and our footprint becomes larger, teams and employees will treat each other not only as teammates but customers. We must live the customer-obsessed mindset, always. This will help us scale and it will translate to the interactions that our Rafters have with their clients and other product teams that they integrate with. Our culture will enable our success and set us apart from other companies.
How do we get there?
Public-sector modernization is critical for us to live in a better world. We, at Raft, want to innovate and solve complex problems. And, if we are successful, our generation and the ones that follow us will live in a delightful, efficient, and accessible world where out-of-box thinking, and collaboration is a norm.
Raft’s core philosophy is
Ubuntu: I Am, Because We are
. We support our “nadi” by elevating the other Rafters. We work as a hyper collaborative team where each team member brings a unique perspective, adding value that did not exist before. People make Raft special. We celebrate each other and our cognitive and cultural diversity. We are devoted to our practice of innovation and collaboration.
We’re an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.
Show more
Show less","Data Engineering, ETL Pipelines, Cloudbased Systems, AI/Machine Learning, Data Security, Data Management, Data Processing, Data Architecture, Database Management, Statistics, Computer Science, Data Science","data engineering, etl pipelines, cloudbased systems, aimachine learning, data security, data management, data processing, data architecture, database management, statistics, computer science, data science","aimachine learning, cloudbased systems, computer science, data architecture, data engineering, data management, data processing, data science, data security, database management, etl pipelines, statistics"
Senior Data Engineer,Raft,"San Antonio, TX",https://www.linkedin.com/jobs/view/senior-data-engineer-at-raft-3731727301,2023-12-17,Texas,United States,Mid senior,Remote,"This is a U.S. based position. All of the programs we support require
U.S. citizenship to be eligible for employment. All work must be conducted within the continental U.S.
Who we are:
Raft ( https://TeamRaft.com ) is a customer-obsessed non-traditional small business with purposeful focus on Distributed Data Systems, Platforms at Scale, and Complex Application Development, with headquarters in Reston, VA. Our range of clients include innovative federal and public agencies leveraging design thinking, cutting edge tech stack, and cloud native ecosystem. We build digital solutions that impact the lives of millions of Americans.
We’re looking for an experienced
Senior Data Engineer
to support our customer and join our passionate team of high-impact problem solvers.
About the role:
As a Senior Data Engineer, you will be responsible for building and maintaining extract, transform, load (ETL) pipelines to enable comprehensive data operations, from ingest to query. You will use your in-depth understanding of data architecture, database management, and data processing workflows, ensuring efficient and secure handling of our data resources.
What we are looking for:
Minimum of 7 years of hands-on experience in data engineering or related field
Extensive experience in building and managing ETL pipelines
Experienced in building ETL pipelines for comprehensive data operations, meeting and exceeding business value requirements, potentially demonstrated through significant experiences or one exceptionally impactful event
Experience with Cloud-based systems and/or AI/Machine Learning Development
Bachelor's degree in Statistics, Computer Science, Data Science, or a related field
Highly preferred:
Understanding of data security practices and policies, with the ability to design and implement security measures
Exceptional problem-solving abilities and strong analytical skills
Excellent interpersonal and communication skills, able to effectively collaborate with team members and stakeholders
Ability to adapt to changing business requirements and learn new technologies quickly
Advanced certification related to data management or data engineering would be a plus
Clearance Requirements:
Active Top Secret/SCI Security clearance
Work Type:
San Antonio, TX (Local remote)
May require up to 10% travel
What we will offer you:
Highly competitive salary
Fully covered healthcare, dental, and vision coverage
401(k) and company match
Unlimited PTO + 11 paid holidays
Education & training benefits
Annual budget for your tech/gadgets needs
Monthly box of yummy snacks to eat while doing meaningful work
Remote, hybrid, and flexible work options
Team off-site in fun places!
Generous Referral Bonuses
And More!
Our Vision Statement:
We bridge the gap between humans and data through radical transparency and our obsession with the mission.
Our Customer Obsession:
We will approach every deliverable like it's a product. We will adopt a customer-obsessed mentality. As we grow, and our footprint becomes larger, teams and employees will treat each other not only as teammates but customers. We must live the customer-obsessed mindset, always. This will help us scale and it will translate to the interactions that our Rafters have with their clients and other product teams that they integrate with. Our culture will enable our success and set us apart from other companies.
How do we get there?
Public-sector modernization is critical for us to live in a better world. We, at Raft, want to innovate and solve complex problems. And, if we are successful, our generation and the ones that follow us will live in a delightful, efficient, and accessible world where out-of-box thinking, and collaboration is a norm.
Raft’s core philosophy is
Ubuntu: I Am, Because We are
. We support our “nadi” by elevating the other Rafters. We work as a hyper collaborative team where each team member brings a unique perspective, adding value that did not exist before. People make Raft special. We celebrate each other and our cognitive and cultural diversity. We are devoted to our practice of innovation and collaboration.
We’re an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.
Show more
Show less","Data Engineering, ETL Pipelines, Data Architecture, Database Management, Data Security, Cloudbased Systems, AI/Machine Learning, Statistics, Computer Science, Data Science, Data Security Practices, Data Management, Data Engineering Certification, Top Secret/SCI Security Clearance","data engineering, etl pipelines, data architecture, database management, data security, cloudbased systems, aimachine learning, statistics, computer science, data science, data security practices, data management, data engineering certification, top secretsci security clearance","aimachine learning, cloudbased systems, computer science, data architecture, data engineering, data engineering certification, data management, data science, data security, data security practices, database management, etl pipelines, statistics, top secretsci security clearance"
Principal Data Engineer (Remote),Collins Aerospace,"Texas, United States",https://www.linkedin.com/jobs/view/principal-data-engineer-remote-at-collins-aerospace-3770133137,2023-12-17,Texas,United States,Mid senior,Remote,"Date Posted:
2023-11-20
Country:
United States of America
Location:
HTX99: Field Office - TX Remote Location, Remote City, TX, 73301 USA
Position Role Type:
Remote
Do you want to be part of the team that builds the Data Platform at the center of Transforming the Aviation Industry?
As a
Principal
Data Engineer
, you will be on a mission to ensure that our Data Platform can leverage data from various sources to help transform the passenger journey as well as help our customers leverage data to improve their operations. This role is
responsible for the design, development and maintenance of data processes and pipelines
supporting critical Strategic Business Unit (SBU) Data initiatives in support of the Digital Transformation. The scope of the role encompasses many facets of the aviation industry. You will
work with Data Scientists and Application Developers
to
build data-based products
that benefit commercial airlines, airports and passengers.
Your work will influence the next generation of connected aviation products.
You will work with a team where you will be able to
share your ideas and vulnerabilities and will be treated with care and empathy
. You will work with a team that shows courage in doing the right thing, not because it is easy. This is a great opportunity with room to
grow and learn about new and interesting technology solutions.
Our business unit, Connected Aviation Solutions (CAS), is leading the Connected Ecosystem strategic pillar for Collins Aerospace. The Data Management & Data Science (DM&DS) team has end-to-end responsibility to
ensure that CAS data assets are managed with integrity and quality prior to consumption by our critical customer facing applications
- whether via API’s, analytics and/or data visualizations.
What YOU will do:
YOU will contribute to establishing Data Engineering best practices, building the DataOps model and enabling the ML and AI roadmap of the future.
YOU will develop automation and monitoring processes that support the data pipelines.
YOU will work closely with the architecture team to implement modern data repositories that support the CAS use cases (Pipelines, API’s, Data Science, Applications and Visualizations).
YOU will work with internal business customers and software development teams to gather and document requirements for data publishing and data consumption.
YOU will work with the CAS and DT Enterprise Data Architects to automate cloud deployments, as well as build CI/CD pipeline to support cloud-based workloads. This includes developing views, materialized views, and SQL scripts.
YOU may travel domestically and internationally up to 15%.
YOU will work on a distributed and diverse team that collaborates and communicates well.
What YOU will learn:
YOU will learn all about the datasets that are produced in the aerospace ecosystem; such as how the various components in an aircraft interact with each other.
YOU will learn how to enable Data Scientists to perform ML and AI experiments.
YOU will gain exposure to large scale data processing leveraging modern technology stacks including Databricks and AWS native services.
YOU can take flight to becoming a subject matter expert and leader in Data Engineering with exposure to the variety of business and products in an ever-evolving aerospace industry. CAS is growing and so can you.
Education & Experience:
Typically requires a degree in Science, Technology, Engineering or Mathematics (STEM) unless prohibited by local laws/regulations and minimum 8 years prior relevant experience or an Advanced Degree in a related field and minimum 5 years of experience or in absence of a degree, 12 years of relevant experience.
Qualifications You Must Have:
Must be authorized to work in the U.S. without sponsorship now or in the future. RTX will not offer sponsorship for this position.
Demonstrated engineering experience in system integration and design, data pipeline development, or software/service development and deployment.
Experience building data pipelines leveraging tools like Spark, Python, PySpark & SQL as well as working with AWS/Azure Cloud platforms and related services and Terraform, Gitlab and similar CI/CD tools.
Experience with Databricks Platform and leveraging that platform to build out a Data Lake.
Skills We Value:
Professional background developing complex SQL queries and programming in Python with ability to transform raw data into valuable insights.
Experience designing cloud-based data platforms that ensure cost optimization, scalability, performance and ease of use for end users of the platform.
Experience with Micro Services Architectures.
AWS/Azure certifications.
Collins Aerospace, an RTX company, is a leader in technologically advanced and intelligent solutions for the global aerospace and defense industry. Collins Aerospace has the capabilities, comprehensive portfolio, and expertise to solve customers’ toughest challenges and to meet the demands of a rapidly evolving global market.
#reempowerprogram
This role is also eligible for the Re-Empower Program. The Re-Empower Program helps support talented and committed professionals as they rebuild their capabilities, enhance leadership skills, and continue their professional journey. Over the course of the 14-week program, experienced professionals will gain paid, on-the-job experience, have an opportunity to participate in sessions with leadership, develop personalized plans for success and receive coaching to guide their return-to-work experience. Upon completion of the program, based on performance and contributions participants will be eligible for a career at RTX.
Minimum Program Qualifications:
Be on a career break of one or more year at time of application
Have prior experience in functional area of interest
Have interest in returning in either a full-time or part-time position
Connected Aviation Solutions:
Our Connected Aviation Solutions team provides advanced information management systems, products and services that enable the connected ecosystem by bringing together Collins’ unique breadth of aviation products with our smart digital solutions to help us enhance every aspect of the end-to-end travel experience. We help airlines, airports and business aircraft turn data into value to streamline operations, increase efficiency and reduce cost, enhance the passenger experience and contribute to sustainable flight. By combining the best networks, connectivity and data/analytics solutions, we’re solving big problems for our customers and the world, while enhancing the security and connectivity of systems both on and off the aircraft, to help operators and passengers stay more connected and informed and create a more sustainable, efficient, reliable and enjoyable travel experience. Aviation connects the world. Our Connected Aviation Solutions team connects aviation. Sustainably. Seamlessly. Securely.
Diversity drives innovation; inclusion drives success
. We believe a multitude of approaches and ideas enable us to deliver the best results for our workforce, workplace, and customers. We are committed to fostering a culture where all employees can share their passions and ideas so we can tackle the toughest challenges in our industry and pave new paths to limitless possibility.
WE ARE REDEFINING AEROSPACE.
Please ensure the role type (defined below) is appropriate for your needs before applying to this role.
Remote:
Employees who are working in Remote roles will work primarily offsite (from home). An employee may be expected to travel to the site location as needed.
*Position is remote; however, if you live within a reasonable commute of a Collins site with other colleagues you interact with, your manager will discuss whether there is a degree of onsite presence associated with this role.
Some of our competitive benefits package includes:
Medical, dental, and vision insurance
Three weeks of vacation for newly hired employees
Generous 401(k) plan that includes employer matching funds and separate employer retirement contribution, including a Lifetime Income Strategy option
Tuition reimbursement program
Student Loan Repayment Program
Life insurance and disability coverage
Optional coverages you can buy: pet insurance, home and auto insurance, additional life and accident insurance, critical illness insurance, group legal, ID theft protection
Birth, adoption, parental leave benefits
Ovia Health, fertility, and family planning
Adoption Assistance
Autism Benefit
Employee Assistance Plan, including up to 10 free counseling sessions
Healthy You Incentives, wellness rewards program
Doctor on Demand, virtual doctor visits
Bright Horizons, child and elder care services
Teladoc Medical Experts, second opinion program
And more!
At Collins, the paths we pave together lead to limitless possibility. And the bonds we form – with our customers and with each other -- propel us all higher, again and again.
Apply now and be part of the team that’s redefining aerospace, every day.
The salary range for this role is 94,000 USD - 196,000 USD; however, RTX considers several factors when extending an offer, including but not limited to, the role and associated responsibilities, a candidate’s work experience, location, education/training, and key skills. Hired applicants may be eligible for benefits, including but not limited to, medical, dental, vision, life insurance, short-term disability, long-term disability, 401(k) match, flexible spending accounts, flexible work schedules, employee assistance program, Employee Scholar Program, parental leave, paid time off, and holidays. Specific benefits are dependent upon the specific business unit as well as whether or not the position is covered by a collective-bargaining agreement. Hired applicants may be eligible for annual short-term and/or long-term incentive compensation programs depending on the level of the position and whether or not it is covered by a collective-bargaining agreement. Payments under these annual programs are not guaranteed and are dependent upon a variety of factors including, but not limited to, individual performance, business unit performance, and/or the company’s performance.
RTX is An Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or veteran status, age or any other federally protected class.
Privacy Policy and Terms:
Click on this link to read the Policy and Terms
01663890
Show more
Show less","Data Engineering, DataOps, Machine Learning, Cloud Computing, Python, Data Pipelines, Spark, PySpark, SQL, AWS, Azure, Terraform, Gitlab, Databricks, Data Lake, Cloudbased Data Platforms, Cost Optimization, Scalability, Performance, Micro Services Architectures, AWS/Azure Certifications","data engineering, dataops, machine learning, cloud computing, python, data pipelines, spark, pyspark, sql, aws, azure, terraform, gitlab, databricks, data lake, cloudbased data platforms, cost optimization, scalability, performance, micro services architectures, awsazure certifications","aws, awsazure certifications, azure, cloud computing, cloudbased data platforms, cost optimization, data engineering, data lake, databricks, dataops, datapipeline, gitlab, machine learning, micro services architectures, performance, python, scalability, spark, sql, terraform"
Senior Data Engineer,Raft,"San Antonio, TX",https://www.linkedin.com/jobs/view/senior-data-engineer-at-raft-3731774405,2023-12-17,Texas,United States,Mid senior,Remote,"This is a U.S. based position. All of the programs we support require
U.S. citizenship to be eligible for employment. All work must be conducted within the continental U.S.
Who we are:
Raft (https://goraft.tech) is a customer-obsessed non-traditional small business with purposeful focus on Distributed Data Systems, Platforms at Scale, and Complex Application Development, with headquarters in Reston, VA. Our range of clients include innovative federal and public agencies leveraging design thinking, cutting edge tech stack, and cloud native ecosystem. We build digital solutions that impact the lives of millions of Americans.
Our team is rapidly growing and looking for an experienced
Senior Data Engineer
to support our customer and join our passionate team of high-impact problem solvers. We enjoy the challenges of human-centered design, security, and scale to create better outcomes for our federal agency partners. We are a remote-first and work completely in the
open source
.
About the role:
Senior Data Engineers
on our
Distributed Systems
team are focused on building data platforms that make it easy for different types of user personas to access data from a central control plane. This includes building backend services, connecting OSS projects in a repeatable and performant way, and extending feature sets.
Required Qualifications:
Experience building data infrastructure and platforms using streaming frameworks
Experience/Interest in OSS projects like: Apache Flink, Apache Pulsar, Apache Kafka, Apache Beam, Apache Storm, Apache Airflow
Hands-on experience with Golang
Hands-on experience with Spark
Ability to build cloud-native, scalable services
Higher education degree or drop-out in Mathematics, CS, Statistics
Obtain Security+ within the first 90 days of employment with Raft
Highly preferred:
Work with the Platform team to run distributed OSS systems at scale on Kubernetes
Think of ways to implement data security at the row and column levels
Contribute features back to the OSS projects in dedicated time
Build prototypes, gather/implement feedback, delight users.
Design and develop data best practices using Kafka, ElasticSearch, Presto/Trinio
Clearance Requirements:
Ability to obtain/maintain a Top Secret Security clearance
Position Type and Location:
Hybrid - San Antonio, TX
May require up to 10% travel
What we will offer you:
Highly competitive salary
Fully covered healthcare, dental, and vision coverage
401(k) and company match
Unlimited PTO + 11 paid holidays
Education & training benefits
Annual budget for your tech/gadgets needs
Monthly box of yummy snacks to eat while doing meaningful work
Remote, hybrid, and flexible work options
Team off-site in fun places!
Generous Referral Bonuses
And More!
Our Vision Statement:
We bridge the gap between humans and data through radical transparency and our obsession with the mission.
Our Customer Obsession:
We will approach every deliverable like it's a product. We will adopt a customer-obsessed mentality. As we grow, and our footprint becomes larger, teams and employees will treat each other not only as teammates but customers. We must live the customer-obsessed mindset, always. This will help us scale and it will translate to the interactions that our Rafters have with their clients and other product teams that they integrate with. Our culture will enable our success and set us apart from other companies.
How do we get there?
Public-sector modernization is critical for us to live in a better world. We, at Raft, want to innovate and solve complex problems. And, if we are successful, our generation and the ones that follow us will live in a delightful, efficient, and accessible world where out-of-box thinking, and collaboration is a norm.
Raft’s core philosophy is
Ubuntu: I Am, Because We are
. We support our “nadi” by elevating the other Rafters. We work as a hyper collaborative team where each team member brings a unique perspective, adding value that did not exist before. People make Raft special. We celebrate each other and our cognitive and cultural diversity. We are devoted to our practice of innovation and collaboration.
We’re an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.
Show more
Show less","Apache Flink, Apache Kafka, Apache Beam, Apache Storm, Apache Airflow, Golang, Spark, Kubernetes, Data Security, Kafka, ElasticSearch, Presto/Trinio","apache flink, apache kafka, apache beam, apache storm, apache airflow, golang, spark, kubernetes, data security, kafka, elasticsearch, prestotrinio","apache airflow, apache beam, apache flink, apache kafka, apache storm, data security, elasticsearch, golang, kafka, kubernetes, prestotrinio, spark"
Expression of Interest: Data Engineer,Fingerprint for Success (F4S),"San Antonio, TX",https://www.linkedin.com/jobs/view/expression-of-interest-data-engineer-at-fingerprint-for-success-f4s-3787776554,2023-12-17,Texas,United States,Mid senior,Remote,"We are inviting professionals in high-growth industries who are thinking about their next move or looking for a new opportunity to join our expanding talent pool.
The F4S Talent Pool is a pilot project designed to:
Help job seekers get discovered by our partners based on their anticipated hiring needs.
Provide optional support and resources for job seekers in their career endeavors.
Help individuals understand, and bring out the best in themselves and each other.
The F4S Talent Pool process:
Once you express your interest, you will be asked to complete the F4S work style assessment which measures 48 key attitudes and motivations in the context of work. On completion, you will be automatically added to our growing talent pool and contacted as new opportunities arise.
About Fingerprint For Success (F4S)
Backed by 20+ years of research, F4S’s revolutionary predictive analytics have achieved over 90% reliability in forecasting personal and team motivations, behaviors, and performance. Ultimately, we help people find purpose and fulfillment at work, and help build and scale high performing teams.
Keep in mind, joining our talent pool does not guarantee a job offer. We aim to balance your technical skills with the results of your F4S work style assessment to match the hiring needs of our partners.
Your feedback is a gift! Write to us via:
Powered by JazzHR
o33Cn9hdYx
Show more
Show less","F4S work style assessment, Predictive analytics","f4s work style assessment, predictive analytics","f4s work style assessment, predictive analytics"
Data Engineer III,Frost,"Texas, United States",https://www.linkedin.com/jobs/view/data-engineer-iii-at-frost-3767134629,2023-12-17,Texas,United States,Mid senior,Hybrid,"The candidate for this exciting opportunity can work from their Texas home near a Frost Texas location or at a Frost Texas office location (the candidate must live in Texas). The candidate would have to travel to San Antonio, Texas a couple times per month. Frost will help reimburse relocation costs to move to Texas if one is not currently living in Texas.
Job Description
It’s about putting our best to the test.
Are you described as someone with an inquisitive mind and an innovative personality? Are you never satisfied with good enough? Does solving complex problems and ensuring top-quality systems excite you? If so, being a Data Engineer III with Frost could be for you.
At Frost, it’s about more than a job. It’s about having a flourishing career where you can thrive, both in and out of work. At Frost, we’re committed to fostering an environment that reflects our values and encourages team members to be the best they can be. In joining our adaptable, integrity-driven team, you’ll become part of Frost’s over 150-year legacy of providing unparalleled banking services.
Who You Are
As a
Data Engineer III
,
you
will lead the development and implementation of ETL processes and data pipelines. You’ll play an important role in designing and building scalable and reliable data infrastructures. You’ll use your strong problem-solving skills to ensure that the systems are performing optimally and meet our high standards. You believe in effective communication and will have the opportunity to address potential problems and solutions to complex issues.
What You’ll Do
Develop and maintain data pipelines to automate data ingestion and processing
Collaborate with stakeholders to identify data requirements and ensure data infrastructure meets business needs
Develop and enforce data governance policies and standards
Monitor data infrastructure and performance to identify and resolve issues
Drive best practices via code and design reviews
Coach, mentor, and provide technical assessments
Provide guidance to other Data Engineers as needed
Stay up to date with industry trends and new technologies in data engineering
Always take action using integrity, caring, and excellence to achieve all-win outcomes
What You’ll Need
Bachelor's degree in Computer Science, Information Technology, or related field
4+ Years of experience as a data engineer
3+ Years of experience developing in either Python, Java , Scala or Spark
Advanced understanding of database technologies such as SQL and NoSQL
Expertise in ETL tools and techniques
Knowledge of data modeling and schema design
Strong problem-solving and analytical skills
Familiarity with Big Data technologies such as Hadoop, Spark, Hive, and Cloud native data engineering technologies
Experience leveraging cloud technologies to develop data pipelines
Mastery of data modeling concepts
Strong understanding of ETL concepts and Data Warehousing concepts
Experience with CI/CD pipelines
Experience with version control software
Strong understanding of Agile Principles (Scrum)
Excellent written and verbal communication skills
Additional Preferred Skills
Know how of Informatica Intelligent Cloud Services (IICS) will be a plus.
Our Benefits
At Frost, we care about your health, your family, and your future and strive to have our benefits reflect that. This includes:
Medical, dental, vision, long-term disability, and life insurance
401(k) matching
Generous holiday and paid time off schedule
Tuition reimbursement
Extensive health and wellness programs, including our Employee Assistance Program
Referral bonus program + more!
Since 1868, Frost has dedicated their expertise to provide exceptional banking, investment, and insurance services to businesses and individuals throughout Texas. Frost is one of the 50 largest U.S. banks by asset size and is a leader in banking customer satisfaction. At Frost, it’s about being part of something bigger. If this sounds like you, we encourage you to apply and see what’s possible at Frost.
Show more
Show less","Python, Java, Scala, Spark, SQL, NoSQL, Hadoop, Hive, Informatica Intelligent Cloud Services (IICS), CI/CD pipelines, Agile Principles, Scrum","python, java, scala, spark, sql, nosql, hadoop, hive, informatica intelligent cloud services iics, cicd pipelines, agile principles, scrum","agile principles, cicd pipelines, hadoop, hive, informatica intelligent cloud services iics, java, nosql, python, scala, scrum, spark, sql"
Engineer Database 2 with Security Clearance,ClearanceJobs,"Azusa, CA",https://www.linkedin.com/jobs/view/engineer-database-2-with-security-clearance-at-clearancejobs-3781966655,2023-12-17,Rancho Cucamonga,United States,Mid senior,Onsite,"HERE'S THE JOB DESCRIPTION:Title: Engineer Database 2 #8082-1 (1 Opening)Pay Rate: DOELocation: Azusa, CA 91702 (Hybrid 50/50)Length: 1 year contract + extension and conversionClearance Requirement:
Active DoD Secret Clearance Required Job Description:
The Database Engineer in Azusa, CA will have systems engineering duties with a specialty in database architecture and validation against spacecraft systems.
The candidate is required to have a skillset domain in systems engineering such as experience in validation and test, requirements development, and experience with command and data handling of spacecraft/aircraft systems.
Experience with Department of Defense (DOD) /NASA systems are preferred. Responsibilities:
The role designs, models, documents, and guides the logical and conceptual relationship of data and database changes for complex applications.
Analyzes needs and requirements of existing and proposed systems, and develops technical, structural, and organizational specifications.
May create standards and/or do modeling to monitor and enhance capacity and performance. Basic Qualifications:
Bachelor's Degree in a STEM field (Science, Technology, Engineering, Mathematics) & 5 years of related engineering experience, OR a Master's Degree in a STEM field & 3 years of related engineering experience
Knowledge of systems engineering domain (requirements development, test and validation, Command and Data Handling architectures)
Knowledge of relational databases (i.e. Oracle)
Experience with implementing and maintaining Oracle databases
Proficient with writing Structure Query Language (SQL) scripts
Ability to obtain & maintain a DoD security clearance Preferred Qualifications:
Experience architecting databases on aerospace systems
Experience with writing Python/Matlab scripts to automate data insertion/extraction
Experience with configuration management and version control tools such as JIRA, Git/Bitbucket
Excellent communication and organizational skills; strong interpersonal and analytic skills
Current & active DoD security clearance #Cj
Show more
Show less","Systems engineering, Validation, Test, Requirements development, Command and data handling, Oracle databases, SQL, DoD security clearance, Aerospace systems, Python, Matlab, JIRA, Git, Bitbucket","systems engineering, validation, test, requirements development, command and data handling, oracle databases, sql, dod security clearance, aerospace systems, python, matlab, jira, git, bitbucket","aerospace systems, bitbucket, command and data handling, dod security clearance, git, jira, matlab, oracle databases, python, requirements development, sql, systems engineering, test, validation"
Data Analytics Developer,CANARIE,"Ottawa, Ontario, Canada",https://ca.linkedin.com/jobs/view/data-analytics-developer-at-canarie-3759227905,2023-12-17,Gatineau, Canada,Mid senior,Hybrid,"About Us
CANARIE connects Canada to the world. Our programs equip researchers, students, and startups in Canada to excel on the global stage.
Together with our 13 provincial and territorial partners, we form Canada’s National Research and Education Network (NREN). This ultra-high-speed network connects Canada’s researchers, educators, and innovators to each other and to global data, technology, and colleagues.
To strengthen the security of Canada’s research and education sector, we collaborate with our partners in the NREN, government, academia, and the private sector to fund, implement, and support cybersecurity initiatives. We also provide identity management services to the academic community and boost Canada’s startups with cloud resources and expertise in emerging technologies.
Established in 1993, CANARIE is a non-profit corporation, with most of our funding provided by the Government of Canada.
Who We Are
We’re a small team of dedicated thinkers, innovators, and problem-solvers. Our team is positive, passionate, and collaborative.
We work with Canada’s higher-ed community, private sector, and government partners to ensure that Canada’s bright researchers and entrepreneurs benefit from the technologies and tools that are critical to their success. Together with our partners across the country, we work to ensure that Canada leads in research and innovation.
The Position: Data Analytics Developer
The Data Analytics Developer will be accountable for developing and implementing data-driven analytics strategies that help improve program or business performance. The Data Analytics Developer will be driving the collection, interpretation, analysis, and presentation of data to make informed decisions that support the CANARIE's objectives.
This is a full-time, permanent position that reports to CANARIE’s Senior Director Applications.
Key Responsibilities
Developing data-driven strategies that improve business performance
Maintaining a deep understanding of the CANARIE's data landscape, including data sources, data models, and data governance policies
Developing and implementing data analytics processes that ensure data accuracy, integrity, and consistency across all sources
Partnering with stakeholders to define data requirements, build data models, and develop reports that enable data-driven decision-making
Developing and implementing predictive models, machine learning algorithms, and other statistical tools to forecast business outcomes
Developing and implementing data visualization tools that analyze complex data sets to identify trends, patterns, and insights that inform business decisions
Working with cross-functional teams to develop and implement business intelligence tools that provide timely and accurate insights into programs and services
Staying up to date with emerging trends in data analytics, machine learning, and artificial intelligence
Travel may occasionally be required
Related duties and responsibilities, as required
The Ideal Candidate
To excel at CANARIE, you have a demonstrated capacity to work collaboratively on teams and are also an energetic self-starter with the self-motivation to work independently. You have excellent oral and written communication skills, and can count time management, problem solving, and managing multiple priorities as your strengths. You also share a dedication to CANARIE’s core values: service, teamwork, inclusion, innovation, quality, and integrity.
Relevant experience in publicly funded not-for-profit organizations is desirable. Bilingualism is preferred.
What You Bring
Education and Experience
College diploma or university degree in Computer Science, Mathematics, Statistics or a suitable combination of education, experience, or other relevant training
Minimum of 7 years relevant progressive experience
Experience building and maintaining data-driven solutions
Proven experience as a developer and managing data analytics systems
Skills
Ability to present complex data analysis in a clear and concise manner
Ability to learn new platforms, programming languages, and technologies
Proven ability to communicate effectively with all levels in the organization
Technical Proficiencies
Strong technical skills in data modeling, statistical analysis, and business intelligence tools such as Power BI or Tableau
Proficiency in programming languages such as Python, R, JavaScript, Power-Query M language, Data Analysis Expressions (DAX) in Power BI Desktop, SQL, or SAS
Experience with machine learning algorithms, predictive modeling, and data mining techniques
Working knowledge of Microsoft Office Suite
Come Work with Us
We are committed to employment equity and encourage anyone who can contribute to the diversification of ideas and perspectives to apply. For applicants with disabilities, accommodation is available upon request throughout the recruitment and assessment process.
If this seems to be the right fit for you, please send a note and your resume to careers@canarie.ca, and include the position title in your subject. We thank all applicants, however only those applicants selected for an interview will be contacted.
Show more
Show less","Data Analytics, Data Visualization, Machine Learning, Artificial Intelligence, Data Governance, Data Modeling, Business Intelligence, Power BI, Tableau, PowerQuery M language, Data Analysis Expressions (DAX), SQL, SAS, Python, R, JavaScript, Microsoft Office Suite","data analytics, data visualization, machine learning, artificial intelligence, data governance, data modeling, business intelligence, power bi, tableau, powerquery m language, data analysis expressions dax, sql, sas, python, r, javascript, microsoft office suite","artificial intelligence, business intelligence, data analysis expressions dax, data governance, dataanalytics, datamodeling, javascript, machine learning, microsoft office suite, powerbi, powerquery m language, python, r, sas, sql, tableau, visualization"
Data Analyst / Research and Evaluation Officer Inventory (EC-03/EC-04) / Répertoire d’analyste de données / Agent en recherche et évaluation (EC-03/EC-04),"Immigration, Refugees and Citizenship Canada / Immigration, Réfugiés et Citoyenneté Canada","Ottawa, Ontario, Canada",https://ca.linkedin.com/jobs/view/data-analyst-research-and-evaluation-officer-inventory-ec-03-ec-04-r%C3%A9pertoire-d%E2%80%99analyste-de-donn%C3%A9es-agent-en-recherche-et-%C3%A9valuation-ec-03-ec-04-at-immigration-refugees-and-citizenship-canada-immigration-r%C3%A9fugi%C3%A9s-et-citoyennet%C3%A9-canada-3773578237,2023-12-17,Gatineau, Canada,Mid senior,Hybrid,"Le français suit
Global Reach, Human Impact.
Join the Team!
WHY WORK WITH US?
We are a department focused on people. Every day our work affects lives across the world. We collaborate with Canadian citizens, sponsors, employers, and educators. We protect refugees, promote citizenship, facilitate the arrival of immigrants and pursue passport innovation.
Our individual efforts have a global impact when, together, we:
welcome hundreds of thousands of newcomers each year,
help people settle into Canada,
reunite families,
attract top talent, and,
find new ways to decide who comes to visit, work or stay
Our programs help meet our country’s needs. Our work helps Canada become more economically, socially, and culturally prosperous.
We’re people helping people in Canada and abroad. We bring people together.
DID YOU KNOW?
As a part of Canada 150, 1,400 citizenship ceremonies were held across Canada. Of these, 30% were enhanced ceremonies which promote the value and awareness of Canadian citizenship!
JOIN our professional and dynamic team AND YOU WILL BE PART OF...
• one of the TOP 100 Canada's Best Employer in 2018, 2020 and 2021 (Forbes).
• a culture of experimentation and innovation that regularly tests new ways of doing business to improve programs and services.
• an organization that selects and welcomes foreign nationals whose skills contribute to Canadian prosperity as permanent and temporary residents.
• an organization that encourages career aspirations and promotes the professional development of its employees!
Leave your mark! Don't miss this opportunity to join us!
For more information and to apply: https://emploisfp-psjobs.cfp-psc.gc.ca/psrs-srfp/applicant/page1800?poster=1976365&toggleLanguage=en
**
Une portée mondiale, à dimension humaine.
Venez travailler avec nous!
POURQUOI TRAVAILLER AVEC NOUS?
Notre ministère est centré sur les gens. Chaque jour, notre travail change des vies partout dans le monde. Nous collaborons avec les citoyens canadiens, les répondants, les employeurs et les éducateurs. Nous protégeons les réfugiés, nous promouvons la citoyenneté, facilitons l’arrivée des immigrants et poursuivons l’innovation dans le passeport.
Les efforts déployés par chacun de nous ont une portée mondiale quand, ensemble, nous :
· accueillons des centaines de milliers de nouveaux arrivants chaque année;
· aidons les gens à s’établir au Canada;
· réunissons les familles;
· attirons les meilleurs talents; et
· trouvons de nouvelles façons de décider qui vient visiter, travailler ou rester.
Nos programmes contribuent à répondre aux besoins de notre pays. Notre travail aide le Canada à devenir plus prospère sur les plans économique, social et culturel.
Nous aidons les gens au Canada et à l’étranger. Nous rassemblons les gens.
LE SAVIEZ-VOUS?
À l’occasion du 150e anniversaire du Canada, 1 400 cérémonies de citoyenneté ont eu lieu partout au pays. 30 % d’entre elles étaient des cérémonies spéciales qui visent à promouvoir la valeur de la citoyenneté canadienne et à accroître la sensibilisation à son égard!
JOINS TOI à notre équipe professionnelle et dynamique ET TU FERAS PARTIE...
• de l'un parmi le TOP 100 des meilleurs employeurs du Canada en 2018, 2020 et 2021) (Forbes).
• d'une culture d’expérimentation et d'innovation qui met régulièrement à l’essai de nouvelles façons de faire afin d’améliorer les programmes et les services.
• d'une organisation qui sélectionne et accueille des étrangers, à titre de résidents permanents et temporaires, dont les compétences contribuent à la prospérité du Canada .
• d'une organisation qui encourage les aspirations professionnelles et qui promeut l'avancement de carrière de ses employés!
Laisse ta trace! Ne rate pas cette occasion de te joindre à nous !
Pour plus d'information et pour postuler: https://emploisfp-psjobs.cfp-psc.gc.ca/psrs-srfp/applicant/page1800?poster=1976365&toggleLanguage=fr
Show more
Show less","Immigration, Citizenship, Refugees, Passport, French, English","immigration, citizenship, refugees, passport, french, english","citizenship, english, french, immigration, passport, refugees"
Data Analyst / Research and Evaluation Officer Inventory (EC-03/EC-04) / Répertoire d’analyste de données / Agent en recherche et évaluation (EC-03/EC-04),"Immigration, Refugees and Citizenship Canada / Immigration, Réfugiés et Citoyenneté Canada","Gatineau, Quebec, Canada",https://ca.linkedin.com/jobs/view/data-analyst-research-and-evaluation-officer-inventory-ec-03-ec-04-r%C3%A9pertoire-d%E2%80%99analyste-de-donn%C3%A9es-agent-en-recherche-et-%C3%A9valuation-ec-03-ec-04-at-immigration-refugees-and-citizenship-canada-immigration-r%C3%A9fugi%C3%A9s-et-citoyennet%C3%A9-canada-3784576754,2023-12-17,Gatineau, Canada,Mid senior,Hybrid,"Le français suit
Global Reach, Human Impact.
Join the Team!
WHY WORK WITH US?
We are a department focused on people. Every day our work affects lives across the world. We collaborate with Canadian citizens, sponsors, employers, and educators. We protect refugees, promote citizenship, facilitate the arrival of immigrants and pursue passport innovation.
Our individual efforts have a global impact when, together, we:
welcome hundreds of thousands of newcomers each year,
help people settle into Canada,
reunite families,
attract top talent, and,
find new ways to decide who comes to visit, work or stay
Our programs help meet our country’s needs. Our work helps Canada become more economically, socially, and culturally prosperous.
We’re people helping people in Canada and abroad. We bring people together.
DID YOU KNOW?
As a part of Canada 150, 1,400 citizenship ceremonies were held across Canada. Of these, 30% were enhanced ceremonies which promote the value and awareness of Canadian citizenship!
JOIN our professional and dynamic team AND YOU WILL BE PART OF...
• one of the TOP 100 Canada's Best Employer in 2018, 2020 and 2021 (Forbes).
• a culture of experimentation and innovation that regularly tests new ways of doing business to improve programs and services.
• an organization that selects and welcomes foreign nationals whose skills contribute to Canadian prosperity as permanent and temporary residents.
• an organization that encourages career aspirations and promotes the professional development of its employees!
Leave your mark! Don't miss this opportunity to join us!
For more information and to apply: https://emploisfp-psjobs.cfp-psc.gc.ca/psrs-srfp/applicant/page1800?poster=1976365&toggleLanguage=en
**
Une portée mondiale, à dimension humaine.
Venez travailler avec nous!
POURQUOI TRAVAILLER AVEC NOUS?
Notre ministère est centré sur les gens. Chaque jour, notre travail change des vies partout dans le monde. Nous collaborons avec les citoyens canadiens, les répondants, les employeurs et les éducateurs. Nous protégeons les réfugiés, nous promouvons la citoyenneté, facilitons l’arrivée des immigrants et poursuivons l’innovation dans le passeport.
Les efforts déployés par chacun de nous ont une portée mondiale quand, ensemble, nous :
· accueillons des centaines de milliers de nouveaux arrivants chaque année;
· aidons les gens à s’établir au Canada;
· réunissons les familles;
· attirons les meilleurs talents; et
· trouvons de nouvelles façons de décider qui vient visiter, travailler ou rester.
Nos programmes contribuent à répondre aux besoins de notre pays. Notre travail aide le Canada à devenir plus prospère sur les plans économique, social et culturel.
Nous aidons les gens au Canada et à l’étranger. Nous rassemblons les gens.
LE SAVIEZ-VOUS?
À l’occasion du 150e anniversaire du Canada, 1 400 cérémonies de citoyenneté ont eu lieu partout au pays. 30 % d’entre elles étaient des cérémonies spéciales qui visent à promouvoir la valeur de la citoyenneté canadienne et à accroître la sensibilisation à son égard!
JOINS TOI à notre équipe professionnelle et dynamique ET TU FERAS PARTIE...
• de l'un parmi le TOP 100 des meilleurs employeurs du Canada en 2018, 2020 et 2021) (Forbes).
• d'une culture d’expérimentation et d'innovation qui met régulièrement à l’essai de nouvelles façons de faire afin d’améliorer les programmes et les services.
• d'une organisation qui sélectionne et accueille des étrangers, à titre de résidents permanents et temporaires, dont les compétences contribuent à la prospérité du Canada .
• d'une organisation qui encourage les aspirations professionnelles et qui promeut l'avancement de carrière de ses employés!
Laisse ta trace! Ne rate pas cette occasion de te joindre à nous !
Pour plus d'information et pour postuler: https://emploisfp-psjobs.cfp-psc.gc.ca/psrs-srfp/applicant/page1800?poster=1976365&toggleLanguage=fr
Show more
Show less","Citizenship, Immigration, Passport innovation, Humanitarian work, Customer service, Collaboration, Communication, Problem solving, Teamwork, Professional development, French language","citizenship, immigration, passport innovation, humanitarian work, customer service, collaboration, communication, problem solving, teamwork, professional development, french language","citizenship, collaboration, communication, customer service, french language, humanitarian work, immigration, passport innovation, problem solving, professional development, teamwork"
"Senior Analyst, Data Strategy and Operations",Canada Mortgage and Housing Corporation (CMHC) Société canadienne d'hypothèques et de logement(SCHL),"Ottawa, Ontario, Canada",https://ca.linkedin.com/jobs/view/senior-analyst-data-strategy-and-operations-at-canada-mortgage-and-housing-corporation-cmhc-soci%C3%A9t%C3%A9-canadienne-d-hypoth%C3%A8ques-et-de-logement-schl-3766855675,2023-12-17,Gatineau, Canada,Mid senior,Hybrid,"Job Requisition ID:
9775
Position Status:
Temporary Full Time
Position Type:
Hybrid
Office Location:
Ottawa (ON); Montreal (QC); Toronto (ON)
Travel Requirement:
Travel not required
Language Designation:
English Essential
Language Skill Levels (Read/Write/Speak):
ZZZ
Salary:
Our salaries generally range from $ 68974.7 to $ 86218.38 and are based on qualifications and experience.
About CMHC
At CMHC, the work you do and the work we do together matters. We come to work every day with a common purpose: to realize a future where everyone in Canada has a home that they can afford and meets their needs.
Our people are second to none. We lean in with courage, band together as a community and try new things to make a lasting impact on housing from coast to coast to coast.
Join us and be part of a team that's committed to making a real difference and be part of something meaningful.
What’s in it for you
We’ve Got The Purpose, The People And The Perks You Need For a Fulfilling Career. Here’s What You Get When You’re a Contract Employee
3 weeks of accrued vacation.
Annual individual performance bonus.
Support in your personal and professional growth with training, mentorship and more – because when you thrive, we thrive.
An inclusive workplace culture and environment with Employee Resource Groups and more.
A hybrid work model that lets you balance working from home and nurturing in-person connections by coming into your region’s office at a minimum of 4 times a month.
About The Role
Join the Enterprise Data Office in the Senior Analyst, Data Strategy and Operations position, where we facilitate the planning and execution of data strategy and initiatives. You will assist CMHC in realizing the true value of its data assets and promote a data-centric culture within the organization. Your skills and passion for data, data strategy and project management will drive efficiencies and value to business areas and to clients.
This is a temporary position of a duration of 12 months.
What You’ll Do
Support workstreams for CMHC’s corporate data strategy, for example strategic planning, program operations, or project execution.
Collect, analyze, synthesize, and present information from diverse sources to provide management with timely assessments and recommendations of potential implications/opportunities for housing data or data strategy in the form of summaries, presentation decks, briefing notes, etc.
Develop and provide relevant research materials, summaries and policy analyses to senior management, committees, and task forces/working groups.
Develop and enhance optimal data collection processes and ensure utilization of data to report on data initiatives.
Support, represent and/or organize meetings, committees, and working groups to communicate priorities and to obtain input into CMHC data strategy or projects.
Support day-to-day operations of data strategy, including responding to internal queries, raising IT support requests, problem-solving with diverse CMHC teams and maintenance of operational support documentation (minutes, procedures, etc).
What You Should Have
A degree in a related field such as information studies, business, finance or commerce.
A minimum of three years of prior related experience working in information technology, data, analytics, or project-focused environment.
Strong strategic and analytical thinking, research and problem-solving skills, with an ability to synthesize and conceptualize.
Strong initiative: must be able to track and follow up on diverse deliverables with competing deadlines.
Strong oral and written communication skills, including the ability to write and present complex ideas and solutions to a variety of audiences, including senior management.
It would be great if you also had:
Experience developing or executing data strategies.
A sound knowledge of mortgage financing, real estate, housing and mortgage markets and systems.
Posting closing date:
Note, the competition will remain active until filled.
Our commitment to diversity, equity, and inclusion
We’re committed to employment equity and encourage women, Indigenous Peoples, persons with disabilities, veterans and persons of all races, ethnicities, religions, abilities, sexual orientations, and gender identities and expressions to apply. We also welcome applications from non-Canadians who are eligible to work in Canada.
CMHC is an inclusive workplace where diversity of thought – and of people – are recognized, valued, and considered essential to achieving our mission.
About
Learn more about our commitment to diversity and inclusion
What happens after you apply
We know that applying for a new job can be both exciting and daunting, and we appreciate your effort.
Learn more about our hiring process
. If you are selected for an interview or testing, please advise us if you require an accommodation.
If you applied before and you were not successful don’t worry – we're always posting new positions, so don’t hesitate to give it another shot. We’re excited to see what you bring to the table this time around!
Show more
Show less","Data Strategy, Project Management, Business Analysis, Data Analytics, Research, Communication, Problem Solving, Strategic Thinking, Information Technology, Data, Mortgage Financing, Real Estate, Housing Markets, Mortgage Systems","data strategy, project management, business analysis, data analytics, research, communication, problem solving, strategic thinking, information technology, data, mortgage financing, real estate, housing markets, mortgage systems","business analysis, communication, data, data strategy, dataanalytics, housing markets, information technology, mortgage financing, mortgage systems, problem solving, project management, real estate, research, strategic thinking"
Data Analyst,Meridian Technologies,Charlotte Metro,https://www.linkedin.com/jobs/view/data-analyst-at-meridian-technologies-3787360633,2023-12-17,Harrisburg,United States,Associate,Onsite,"12 month contract to start out
Local to Charlotte, NC
Need Strong PowerBI experience and SQL.
Several databases. Excel, SQL, Oracle. Need to be able to collect & join data from a variety of data sources.
Will work to automate current manual processes. Make reporting more efficient.
Individuals have primary responsibility for the design, development, and maintenance of reporting, dashboard, and data warehouse solutions (Power BI, SSSAS models, SSRS, SQL, etc.) that support decision making within the Customer Delivery (Distribution) organization.
Duties and Responsibilities:
Work with internal customers on the development of reporting requirements
Uses technical understanding of existing reporting warehouses, or may develop local warehouses when necessary to support the development of solutions
Aggregate data from various data sources to automate a manual business process
Development of medium to complex solutions using Power BI or SQL Server Reporting Services (SSRS)
Support creation and maintenance of tabular models and cubes in PowerPivot or SQL Server Analysis Services (SSAS)
Develop medium to complex SQL against either Oracle or SQL Server databases
Document warehouses to support cross training of co-workers
Interface with internal organizations such as Finance, Project Management, Lighting, or Resource Management to facilitate solution delivery
Coordinate with Information Technology on requests
Develop solutions to complex business problems in a systematic manner
Using Project Management fundamentals to help drive projects and initiatives to successful conclusions
Understanding of Power Automate and/or Power Apps
Show more
Show less","PowerBI, SQL, Excel, Oracle, Databases, Reporting, Data Warehouse, Data Analysis, Project Management, Power Automate, Power Apps, Tabular Models, Cubes, SSAS, SSRS","powerbi, sql, excel, oracle, databases, reporting, data warehouse, data analysis, project management, power automate, power apps, tabular models, cubes, ssas, ssrs","cubes, dataanalytics, databases, datawarehouse, excel, oracle, power apps, power automate, powerbi, project management, reporting, sql, ssas, ssrs, tabular models"
Senior Data Engineer (Python / AWS),Jobot,"Matthews, NC",https://www.linkedin.com/jobs/view/senior-data-engineer-python-aws-at-jobot-3785306983,2023-12-17,Harrisburg,United States,Mid senior,Onsite,"Want to learn more about this role and Jobot? Click our Jobot logo and follow our LinkedIn page!
Job details
Fast growing AI powered Fintech startup / Python + AWS & Building Data Pipelines!!
This Jobot Job is hosted by Craig Rosecrans
Are you a fit? Easy Apply now by clicking the ""Easy Apply"" button and sending us your resume.
Salary $120,000 - $180,000 per year
A Bit About Us
We are on the hunt for a highly skilled and passionate Senior Data Engineer to join our dynamic technology team. This is a permanent, full-time position where you will have the opportunity to shape the future of Fintech by developing, maintaining and testing our cutting-edge data systems. You will be working with a team of talented engineers and data scientists to design scalable and efficient data pipelines, create data architectures, and ensure all system designs and implementations adhere to the latest data management principles and security standards.
Why join us?
Competitive Base Salary
Equity in high-growth start-up (not in lieu of a salary)
Flexible Hours
Very generous PTO
Dental and Vision, FSA, HSA
Small team, autonomy
Many more great perks!
Job Details
Responsibilities
Design, construct, install, test and maintain highly scalable data management systems.
Collaborate with data scientists and architects on several projects.
Develop and maintain scripts and queries to import, clean, transform, and augment data.
Develop data set processes for data modeling, mining and production.
Employ an array of technological languages and tools to connect systems together.
Recommend ways to improve data reliability, efficiency, and quality.
Collaborate with data architects to visualize data in a manner that is accessible to data scientists and analysts.
Use AWS cloud services to build services and data stores.
Ensure all solutions are aligned with the cloud design principles and strategy.
Work with the team to evaluate new and emerging technologies and developmental methodologies to improve performance and feasibility of future projects.
Qualifications
Bachelor’s degree in Computer Science, Engineering, or a related technical discipline.
Minimum of 5+ years of experience in a Data Engineer role, preferably in a Fintech or startup environment.
Proven experience with the following or equivalent skillset Python, AWS, AWS Aurora, Dynamo DB, Athena.
Solid experience in data architecture, data modeling, master data management, data staging, ETL processes, and business intelligence.
Proficient understanding of distributed computing principles.
Experience with building stream-processing systems and big data technologies.
Strong problem-solving skills with an emphasis on product development.
Excellent written and verbal communication skills.
Ability to work in a fast-paced environment and manage multiple tasks simultaneously.
Strong team player with the ability to work independently when required.
Join us and be a part of a team that values innovation, growth, and collaboration. If you are passionate about data and eager to make a significant impact in the Fintech industry, we would love to hear from you.
Interested in hearing more? Easy Apply now by clicking the ""Easy Apply"" button.
Want to learn more about this role and Jobot?
Click our Jobot logo and follow our LinkedIn page!
Show more
Show less","Python, AWS, AWS Aurora, Dynamo DB, Athena, Data architecture, Data modeling, Master data management, Data staging, ETL processes, Business intelligence, Distributed computing, Streamprocessing systems, Big data technologies, SQL queries, NoSQL databases, Cloud services, Data visualization, Data mining, Data production, Data reliability, Data efficiency, Data quality, Problemsolving, Product development, Communication skills, Teamwork, Independence","python, aws, aws aurora, dynamo db, athena, data architecture, data modeling, master data management, data staging, etl processes, business intelligence, distributed computing, streamprocessing systems, big data technologies, sql queries, nosql databases, cloud services, data visualization, data mining, data production, data reliability, data efficiency, data quality, problemsolving, product development, communication skills, teamwork, independence","athena, aws, aws aurora, big data technologies, business intelligence, cloud services, communication skills, data architecture, data efficiency, data mining, data production, data quality, data reliability, data staging, datamodeling, distributed computing, dynamo db, etl, independence, master data management, nosql databases, problemsolving, product development, python, sql queries, streamprocessing systems, teamwork, visualization"
Sr. AWS Data Engineer,Cognizant,"Charlotte, NC",https://www.linkedin.com/jobs/view/sr-aws-data-engineer-at-cognizant-3786447263,2023-12-17,Harrisburg,United States,Mid senior,Onsite,"We are Cognizant Artificial Intelligence
Digital technologies, including analytics and AI, give companies a once-in-a-generation opportunity to perform orders of magnitude better than ever before. But clients need new business models built from analyzing customers and business operations at every angle to really understand them.
With the power to apply artificial intelligence and data science to business decisions via enterprise data management solutions, we help leading companies prototype, refine, validate and scale the most desirable products and delivery models to enterprise scale within weeks
*
You must be legally authorized to work in United States without the need of employer sponsorship, now or at any time in the future *
This is an Onsite position open to any qualified applicant in the United States
Job Title:
PySpark Tech Lead
Roles/Responsibilities
Experience in end-to-end implementation of projects in Python and Spark from Analysis Design Model to Coding & testing and promote to production especially Python server-side backend programming.
Good understanding of OOPS concepts
Strong knowledge in Data Structures Algorithms Collections Multi-threading and memory management concurrency (GIL)
Strong knowledge and hands-on experience in SQL Unix shell scriptingSound Knowledge of Software engineering design patterns and practices
Strong understanding of Functional programming and RESTful APIs
Experience in Big data ecosystem using Hadoop Spark Scala Python packages and libraries for large scale data is preferred.
Experience with design and implementation of ETL/ELT framework for complex warehouses/marts Knowledge of large data sets and experience with performance tuning and troubleshooting
Strong SQL skills and knowledge of databases (snowflake/Oracle etc) and familiarity with other data stores such as Hadoop data stores
Ability to manage team of pyspark developers
Experience with performance tuning data transformations across large data sets
Exceptional problem solving skills & Excellent communication skills
Understanding of the Agile methodology
Qualifications
Extensive software engineering experience in an agile environment.
General insurance domain knowledge is preferred
Python
PySpark
Salary and Other Compensation
:
The annual salary for this position is between $120,000.00 – $145,000.00 depending on experience and other qualifications of the successful candidate.
This position is also eligible for Cognizant’s discretionary annual incentive program, based on performance and subject to the terms of Cognizant’s applicable plans.
Benefits
: Cognizant offers the following benefits for this position, subject to applicable eligibility requirements:
Medical/Dental/Vision/Life Insurance
Paid holidays plus Paid Time Off
401(k) plan and contributions
Long-term/Short-term Disability
Paid Parental Leave
Employee Stock Purchase Plan
Disclaimer:
The salary, other compensation, and benefits information is accurate as of the date of this posting. Cognizant reserves the right to modify this information at any time, subject to applicable law.
Show more
Show less","Python, PySpark, Spark, Hadoop, Scala, SQL, Unix, RESTful APIs, ETL/ELT, Agile, Data Structures, Algorithms, Collections, Multithreading, Memory management, Software engineering, Design patterns, Functional programming, Data science","python, pyspark, spark, hadoop, scala, sql, unix, restful apis, etlelt, agile, data structures, algorithms, collections, multithreading, memory management, software engineering, design patterns, functional programming, data science","agile, algorithms, collections, data science, data structures, design patterns, etlelt, functional programming, hadoop, memory management, multithreading, python, restful apis, scala, software engineering, spark, sql, unix"
"Manager, Data Engineering",Circle K,"Charlotte, NC",https://www.linkedin.com/jobs/view/manager-data-engineering-at-circle-k-3759840805,2023-12-17,Harrisburg,United States,Mid senior,Onsite,"Manager – Data Engineering (Global Fuel)
We are looking for a Manager to be responsible for leading the Global Tech Data Engineering team to support the Global Fuels function. You are responsible for business and tech planning, collaborate with architects for data architecture design, cross-team collaboration, and engineer management. You will have the opportunity to partner closely with globalized engineering and product teams in a high-impact and fast-paced environment.
Responsibilities
Build and Lead the Fuels Data Engineering Global team sitting across North American, India GCC and European offices.
Oversight and execution of the data engineering roadmaps.
Set up Dev Ops and CICD processes for data engineering.
Managed advanced analytics in support of the Global Fuels function.
Business and tech planning for various company and project initiatives
Collaborate with architects for data architecture design.
Effectively scale/manage the engineering team by attracting and mentoring extraordinary people at various experience levels to grow a high performing and diverse team.
Act as a coach, mentor, and leader for the Data Engineering team. Ensuring development, engagement, and workflow management. This will include engagement initiatives, succession planning, performance management etc.
Manage project priorities and product deliveries. Drive technical initiatives from execution toward delivery.
Collaboration with key business and technical stakeholders.
Manage various budgets/costs associated with projects and team initiatives.
Could be hands-on in the architecture design and building team engineering best practices.
Must Haves
6+ years of data engineering experience in a high scale distributed environment.
3+ years of engineering leadership experience, including people management.
Strong data engineering background on one or more cloud technologies – GCP. AWS, Azure.
Strong ETL design, PL/SQL development, Python and performance tuning using ETL tools in a multi-dimensional Data Warehousing environment.
Proficient in Data management, data operations and data security.
Experience in Enterprise Data solution architecture.
Demonstrated experience in defining and enabling data quality standards for auditing, and monitoring.
Strong Leadership capabilities, aptitude, and interest
An ability and willingness to lead through our core values.
Experience driving collaboration across cross functional teams on delivering shared goals.
Strong communication and teamwork skills.
Experience with SAFe Agile framework, JIRA for work & resource management.
Retail background strongly preferred, comparable skillsets and experience will be considered.
Nice to Have:
Experience with Databricks and Spark
Understanding of Machine Learning and experience working with Data Science teams.
Experience with Fuel retail business
Circle K is an Equal Opportunity Employer.
The Company complies with the Americans with Disabilities Act (the ADA) and all state and local disability laws. Applicants with disabilities may be entitled to a reasonable accommodation under the terms of the ADA and certain state or local laws as long as it does not impose an undue hardship on the Company. Please inform the Company’s Human Resources Representative if you need assistance completing any forms or to otherwise participate in the application process.
Click below to review information about our company's use of the federal E-Verify program to check work eligibility:
In English
In Spanish
R379359
Show more
Show less","Data Engineering, GCP, AWS, Azure, PL/SQL, Python, ETL, Data Warehousing, Data Management, Data Operations, Data Security, Data Quality, SAFe Agile, JIRA, Spark, Databricks, Machine Learning","data engineering, gcp, aws, azure, plsql, python, etl, data warehousing, data management, data operations, data security, data quality, safe agile, jira, spark, databricks, machine learning","aws, azure, data engineering, data management, data operations, data quality, data security, databricks, datawarehouse, etl, gcp, jira, machine learning, plsql, python, safe agile, spark"
"Data Engineer (AWS, Azure, GCP)",CapTech,"Charlotte, NC",https://www.linkedin.com/jobs/view/data-engineer-aws-azure-gcp-at-captech-3774199173,2023-12-17,Harrisburg,United States,Mid senior,Remote,"Company Description
CapTech is an award-winning consulting firm that collaborates with clients to achieve what’s possible through the power of technology. At CapTech, we’re passionate about the work we do and the results we achieve for our clients. From the outset, our founders shared a collective passion to create a consultancy centered on strong relationships that would stand the test of time. Today we work alongside clients that include Fortune 100 companies, mid-sized enterprises, and government agencies, a list that spans across the country.
Job Description
CapTech Data Engineering consultants enable clients to build and maintain advanced data systems that bring together data from disparate sources in order to enable decision-makers. We build pipelines and prepare data for use by data scientists, data analysts, and other data systems. We love solving problems and providing creative solutions for our clients. Cloud Data Engineers leverage the client’s cloud infrastructure to deliver this value today and to scale for the future. We enjoy a collaborative environment and have many opportunities to learn from and share knowledge with other developers, architects, and our clients.
Specific responsibilities for the Data Engineer – Cloud position include:
Developing data pipelines and other data products using Amazon Web Services (AWS), Microsoft Azure, or Google Cloud Platform (GCP)
Advising clients on specific technologies and methodologies for utilizing cloud resources to efficiently ingest and process data quickly
Utilizing your skills in engineering best practices to solve complex data problems
Collaborating with end users, development staff, and business analysts to ensure that prospective data architecture plans maximize the value of client data across the organization.
Articulating architectural differences between solution methods and the advantages/disadvantages of each
Qualifications
Typical experience for successful candidates includes:
Experience delivering solutions on a major cloud platform
Ability to think strategically and relate architectural decisions/recommendations to business needs and client culture
Experience in the design and implementation of data architecture solutions
A wide range of production database experience, usually including substantial SQL expertise, database administration, and scripting data pipelines
Ability to assess and utilize traditional and modern architectural components required based on business needs.
A demonstrable ability to deliver production data pipelines and other data products. This could be hands on experience, degree, certification, bootcamp, or other learning.
Skills
Successful candidates usually have demonstrable experience with technologies in some of these categories:
Languages: SQL, Python, Java, R, C# / C++ / C
Database: SQL Server, PostgreSQL, Snowflake, Redshift, Aurora, Presto, BigQuery, Oracle
DevOps: git, docker, subversion, Kubernetes, Jenkins
Additional Technologies: Spark, Databricks, Kafka, Kinesis, Hadoop, Lambda, EMR
Popular Certifications: AWS Cloud Practitioner, Microsoft Azure Data Fundamentals, Google Associate Cloud Engineer
Additional Information
We want everyone at CapTech to be able to envision a lasting and rewarding career here, which is why we offer a variety of career paths based on your skills and passions. You decide where and how you want to develop, and we help get you there with customizable career progression and a comprehensive benefits package to support you along the way. Alongside our suite of traditional benefits encompassing generous PTO, health coverage, disability insurance, paid family leave and more, we’ve launched extended benefits to help meet our employees’ needs.
CapFlex – Employee-first mentality that supports a remote and hybrid workforce and empowers daily flexibility while servicing our clients
Learning & Development – Programs offering certification and tuition support, digital on-demand learning courses, mentorship, and skill development paths
Modern Health –A mental health and well-being platform that provides 1:1 care, group support sessions, and self-serve resources to support employees and their families through life’s ups and downs
Carrot Fertility –Inclusive fertility and family-forming coverage for all paths to parenthood – including adoption, surrogacy, fertility treatments, pregnancy, and more – and opportunities for employer-sponsored funds to help pay for care
Fringe –A company paid stipend program for personalized lifestyle benefits, allowing employees to choose benefits that matter most to them – ranging from vendors like Netflix, Spotify, and GrubHub to services like student loan repayment, travel, fitness, and more
Employee Resource Groups – Employee-led committees that embrace and incorporate diversity and inclusion into our day-to-day operations
Philanthropic Partnerships – Opportunities to engage in partnerships and pro-bono projects that support our communities.
401(k) Matching – Generous matching and no vesting period to help you continue to build financial wellness
CapTech is an equal opportunity employer committed to fostering a culture of equality, inclusion and fairness — each foundational to our core values. We strive to create a diverse environment where each employee is encouraged to bring their unique ideas, backgrounds and experiences to the workplace. For more information about our Diversity, Inclusion and Belonging efforts, click HERE. As part of this commitment, CapTech will ensure that persons with disabilities are provided reasonable accommodations. If reasonable accommodation is needed to participate in the job application or interview process, to perform essential job functions, and/or to receive other benefits and privileges of employment, please contact Laura Massa directly via email lmassa@captechconsulting.com.
At this time, CapTech cannot transfer nor sponsor a work visa for this position. Applicants must be authorized to work directly for any employer in the United States without visa sponsorship.
Show more
Show less","AWS, Azure, GCP, SQL, Python, Java, R, C#, C++, C, SQL Server, PostgreSQL, Snowflake, Redshift, Aurora, Presto, BigQuery, Oracle, Git, Docker, Subversion, Kubernetes, Jenkins, Spark, Databricks, Kafka, Kinesis, Hadoop, Lambda, EMR, AWS Cloud Practitioner, Microsoft Azure Data Fundamentals, Google Associate Cloud Engineer","aws, azure, gcp, sql, python, java, r, c, c, c, sql server, postgresql, snowflake, redshift, aurora, presto, bigquery, oracle, git, docker, subversion, kubernetes, jenkins, spark, databricks, kafka, kinesis, hadoop, lambda, emr, aws cloud practitioner, microsoft azure data fundamentals, google associate cloud engineer","aurora, aws, aws cloud practitioner, azure, bigquery, c, databricks, docker, emr, gcp, git, google associate cloud engineer, hadoop, java, jenkins, kafka, kinesis, kubernetes, lambda, microsoft azure data fundamentals, oracle, postgresql, presto, python, r, redshift, snowflake, spark, sql, sql server, subversion"
Senior Data Engineer (Databricks),Movement Mortgage,"Charlotte, NC",https://www.linkedin.com/jobs/view/senior-data-engineer-databricks-at-movement-mortgage-3745168284,2023-12-17,Harrisburg,United States,Mid senior,Remote,"Get, Do, Earn and Give More at Movement!
We’re definitely not your average mortgage company. Movement Mortgage is made up of passionate, talented and hardworking individuals who strive for excellence while demonstrating servant leadership in everything we do.
Here are the basics: we work hard, we have fun, we invest in our people and we make a difference. Sound like a plan? Check us out on our website www.movement.com.
We are seeking a highly skilled
Senior Data Engineer
to join our team. In this role, you will be responsible for designing, building, and maintaining complex data infrastructure that supports our company's data processing needs, with the Databricks platform at the core. You will work closely with technologists and stakeholders to understand their data requirements and develop efficient and scalable data solutions. The ideal candidate will have extensive experience with designing and building complex data pipelines, data warehousing, and a deep understanding of big data technologies.
Roles & Responsibilities
Design, implement and test complex data integrations and transformation processes with nascent data sources while having a strong focus on accuracy, completeness, reliability and relevance.
Engineer, orchestrate and maintain complex data pipelines, catalogs, reports and dashboards using data platform tools
Building and optimizing data storage and processing systems, including data warehouses, data lakes, and NoSQL databases.
Continuously improving the performance, scalability, and reliability of data infrastructure.
Create and maintain technical documentation and metadata, especially for data assets.
Proactively triage operational alarms, exceptions, and failures while driving to the root cause, communicating with the right parties, contributing to the resolution and making improvements to prevent the problem from recurring.
Identify areas for improvements and recommend solutions
Collaborate with cross-functional teams, stakeholders and customers to discover needs and define business requirements and goals
Develop and maintain relationships with key stakeholders across the organization, communicating insights, findings and recommendations
Develop and implement best practices, policies and procedures for data governance, data security, and data privacy
Mentor junior technologists
Required Skills & Qualifications
Bachelor's degree in a relevant field such as computer science, statistics, or mathematics; Master's degree preferred
5+ years of experience in data engineering, software development, or related field
Expertise in multiple programming languages such as Scala and Python
Expertise with big data pipelines, architectures and data sets
Experience with big data technologies such as Databricks, Spark, Kakfa andHadoop
Proficiency with cloud technologies, such as Azure and AWS
Proficiency with distributed systems, parallel processing, microservices, serverless and containerization technologies
Expert understanding of software engineering principles, data structures, and algorithms.
Proficiency with modern source control, test automation and CI/CD tools and practices
Proficiency in managing complex data infrastructure projects, preferably using Terraform
Proficiency with SQL, data modeling, ELT, schema design, data warehousing and data visualization
Proficiency with statistical analysis and problem-solving techniques
Excellent communication and interpersonal skills
Ability to work independently as well as part of a team
Be someone who stays up to date with emerging technologies and best practices and encourages your team to do the same
Experience mentoring technologists
So, Why Movement?
Competitive pay
Medical, dental and life insurance
Company matched 401K (Up to 3.5%)
Excellent career growth opportunity
Fun, team-focused working environment
Employee driven community outreach program
At Movement, we exist to love and value people. We are disrupting the mortgage industry by lending with integrity, building an outstanding corporate culture and investing in our communities. On any given day, you might find a processor volunteering for a local charity or an underwriter winning the corn-hole tournament at Family Fun Day. You’ll also find that we’re the seventh largest mortgage lender in the country. Sound too good to be true? Check us out on our website: www.movement.com
Disclaimer:
The above statements are intended to describe the general nature and level of work being performed by the persons assigned to this position. They are not intended to be an exhaustive list of all associated responsibilities, skills, efforts or working conditions. The Company reserves the right to change, amend, add, delete and otherwise assign any and all duties, responsibilities and position titles as it deems necessary to meet the needs of the business.
Show more
Show less","Scala, Python, Azure, AWS, Cloud technologies, Databricks, Spark, Kakfa, Hadoop, Data engineering, Software development, Scala, Python, Distributed systems, Parallel processing, Microservices, Software engineering principles, Data structures, Algorithms, Terraform, SQL, Data modeling, ELT, Schema design, Data warehousing, Data visualization, Statistical analysis, Problemsolving techniques","scala, python, azure, aws, cloud technologies, databricks, spark, kakfa, hadoop, data engineering, software development, scala, python, distributed systems, parallel processing, microservices, software engineering principles, data structures, algorithms, terraform, sql, data modeling, elt, schema design, data warehousing, data visualization, statistical analysis, problemsolving techniques","algorithms, aws, azure, cloud technologies, data engineering, data structures, databricks, datamodeling, datawarehouse, distributed systems, elt, hadoop, kakfa, microservices, parallel processing, problemsolving techniques, python, scala, schema design, software development, software engineering principles, spark, sql, statistical analysis, terraform, visualization"
Marketing Data Analysts,STS Technical Services,"Melbourne, FL",https://www.linkedin.com/jobs/view/marketing-data-analysts-at-sts-technical-services-3783191159,2023-12-17,Melbourne,United States,Mid senior,Onsite,"STS Technical Services is hiring
Marketing Data Analysts
in
Melbourne, Florida
.
Responsibilities
Ensure basic administrative functions like user account maintenance, reports, dashboards, workflows and other routine tasks.
Analyze and import leads, contacts, and other data into Salesforce
Compile, verify the accuracy, and sort information according to priorities to prepare source data for computer entry
Review data for deficiencies or errors, correct any incompatibilities if possible, and check the output
Research and obtain further information for incomplete documents
Apply data program techniques and procedures
Generate reports, store completed work in designated locations, and perform backup operations
Identify risks and areas for improvement.
Troubleshoot and resolve product issues and concerns
Create and document processes and procedures
Qualifications:
Positive and professional demeanor
Strong analytical, interpersonal, problem-solving, and critical thinking skills
Well-organized, capable of handling a variety of issues and assignments
Self-motivated with the ability to multi-task, prioritize commitments, meet deadlines, and manage changing priorities
Excellent written and verbal communication skills
Attention to details and follow-through
Eager to learn CRM tools and technology
Previous experience with Salesforce.com and in IT, customer service, sales, marketing or other related fields preferred
About STS Technical Services:
STS Technical Services
is a
Top 100 Staffing Firm
that’s partnered with some of the largest names in the aerospace, manufacturing, defense and industrial industries.
Our professional recruitment teams put talented individuals to work at client locations all over the world, and we have hundreds of exciting career opportunities for you to explore!
If you want to speak to a Recruiting Professional directly, call
1-800-359-4787.
STS Technical Services is an equal opportunity employer.
#IndeedSTS
Show more
Show less","User account maintenance, Reports, Dashboards, Workflows, Salesforce, Data analysis, Data entry, Data verification, Data processing, Data reporting, Problemsolving, Critical thinking, Communication, Attention to detail, Salesforce.com, Customer service, Sales, Marketing","user account maintenance, reports, dashboards, workflows, salesforce, data analysis, data entry, data verification, data processing, data reporting, problemsolving, critical thinking, communication, attention to detail, salesforcecom, customer service, sales, marketing","attention to detail, communication, critical thinking, customer service, dashboard, data entry, data processing, data reporting, data verification, dataanalytics, marketing, problemsolving, reports, sales, salesforce, salesforcecom, user account maintenance, workflows"
Senior Principal Consultant – Data and Analytics,Genesys,"Michigan, United States",https://www.linkedin.com/jobs/view/senior-principal-consultant-%E2%80%93-data-and-analytics-at-genesys-3781017372,2023-12-17,Traverse City,United States,Mid senior,Remote,"Build something new with a world-class team.
At Genesys, we allow our employees to make their mark by entrusting them to make decisions and do what they’ve been hired to do: their very best. Your potential is waiting; why are you?
About Genesys:
GENESYS® powers more than 25 billion of the world’s best customer experiences each year.
In GENESYS, Innovations team builds solutions that enable our customers to get more value from their Genesys platforms. The Innovations team’s primary job is to build new, innovative applications using AI and analytics, with the goal of helping our customers effectively and efficiently deliver experience as a service (XaaS).
Summary: We are seeking a highly skilled and experienced Contact Center Data and Analytics Consultant to join our team. The ideal candidate will have a strong background in data analysis, data visualization, data engineering, and business intelligence. This is a customer facing role and requires the ability to present to both technical and business executive audiences.
Responsibilities:
Present Genesys data and analytics technology and vision to executives and technical contributors at prospects and customers
Work hands-on with prospects and customers to demonstrate and communicate the value of Genesys data and analytics apps throughout the sales cycle, from demo to proof of concept to design and implementation
Immerse yourself in the ever-evolving industry of contact center and experience management SaaS, maintaining a deep understanding of competitive and complementary technologies and vendors and how to position our analytics apps in relation to them
Collaborate with Product Management, Engineering, and Saloes to continuously improve Genesys data and analytics apps and marketing
Requirements:
8+ years direct experience with Genesys Engage Infomart or GCXI or NICE CX Analytics; strong understanding of Contact Center and Customer Experience data and metrics
Strong proficiency in SQL, data modeling, and data engineering
Familiarity with scripting languages for data engineering
Outstanding presenting skills to both technical and executive audiences, whether impromptu on a whiteboard or using presentations and demos.
Hands-on expertise with SQL and SQL analytics
Ability to connect a customer’s specific business problems to Genesys analytics solutions
Experience with data visualization tools such as Tableau or Power BI
Preferred Qualifications:
Experience with Snowflake
Experience with Elastic (ELK stack)
Experience with data governance and data management best practices.
Experience working with remote engineering teams
Familiar with software development and project management frameworks
Experience in consulting or developing products and solutions with contact center technologies like Genesys Engage, Genesys Cloud, NICE, Cisco, Avaya, etc.
Compensation:
This role has a market-competitive salary with an anticipated base compensation range listed below. Actual salaries will vary depending on a candidate’s experience, qualifications, skills, and location. This role might also be eligible for a commission or performance-based bonus opportunities.
$121,500.00 - $238,400.00
Benefits:
Medical, Dental, and Vision Insurance.
Telehealth coverage
Flexible work schedules and work from home opportunities
Development and career growth opportunities
Open Time Off in addition to 10 paid holidays
401(k) matching program
Adoption Assistance
Fertility treatments
More details about our company benefits can be found at the following link: https://mygenesysbenefits.com
If a Genesys employee referred you, please use the link they sent you to apply.
About Genesys:
Every year, Genesys orchestrates billions of remarkable customer experiences for organizations in more than 100 countries. Through the power of our cloud, digital and AI technologies, organizations can realize Experience as a Service™ our vision for empathetic customer experiences at scale. With Genesys, organizations have the power to deliver proactive, predictive, and hyper personalized experiences to deepen their customer connection across every marketing, sales, and service moment on any channel, while also improving employee productivity and engagement. By transforming back-office technology to a modern revenue velocity engine Genesys enables true intimacy at scale to foster customer trust and loyalty. Visit www.genesys.com.
Reasonable Accommodations:
If you require a reasonable accommodation to complete any part of the application process or are limited in the ability or unable to access or use this online application process and need an alternative method for applying, you or someone you know may complete the Reasonable Accommodations Form for assistance. Please use the Candidate field in the dropdown menu to ensure a timely response.
This form is designed to assist job seekers who seek reasonable accommodation for the application process. Submissions entered for non-accommodation-related issues, such as following up on an application or submitting a resume, may not receive a response.
Genesys is an equal opportunity employer committed to equity in the workplace. We evaluate qualified applicants without regard to race, color, age, religion, sex, sexual orientation, gender identity or expression, marital status, domestic partner status, national origin, genetics, disability, military and veteran status, and other protected characteristics.
Please note that recruiters will never ask for sensitive personal or financial information during the application phase.
Show more
Show less","Data analysis, Data visualization, Data engineering, Business intelligence, AI, Analytics, Genesys data and analytics technology, SQL, SQL analytics, SQL modeling, Scripting languages, Data governance, Data management, Snowflake, Elastic (ELK stack), Software development frameworks, Project management frameworks, Genesys Engage, Genesys Cloud, NICE, Cisco, Avaya, Tableau, Power BI","data analysis, data visualization, data engineering, business intelligence, ai, analytics, genesys data and analytics technology, sql, sql analytics, sql modeling, scripting languages, data governance, data management, snowflake, elastic elk stack, software development frameworks, project management frameworks, genesys engage, genesys cloud, nice, cisco, avaya, tableau, power bi","ai, analytics, avaya, business intelligence, cisco, data engineering, data governance, data management, dataanalytics, elastic elk stack, genesys cloud, genesys data and analytics technology, genesys engage, nice, powerbi, project management frameworks, scripting languages, snowflake, software development frameworks, sql, sql analytics, sql modeling, tableau, visualization"
Data Engineer,Digital Waffle,"Nottingham, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-digital-waffle-3734619092,2023-12-17,Nottingham, United Kingdom,Associate,Onsite,"👩🏽‍💻 Data Engineer
📍Nottingham
🚎 (Hybrid 1x every 2 weeks)
💰 £65,000
🤖 ETL, SQL, Data Migration, Azure data house
JOB PURPOSE:
Act as a key member of the Data and Analytics Team, working directly with various stakeholders from across the business, designing stable and reliable data repositories, proactively monitoring the company's existing reporting solutions and participating in the design and implementation of the associated data hub, storage and integration environments.
The role holder will be responsible for:
Developing the data platform, testing, improving, and maintaining new and existing data technologies and data feeds.
Your duties will include:
Drive best practices for our Azure Data Warehouse - payload design, logical data modeling, implementation, metadata, and testing standards
Proactive cost optimisation of our Data Warehouse, spotting opportunities to refactor to reduce the cost of our batch jobs
Support the building of robust data models downstream of backend services that support internal business intelligence as well as financial/regulatory reporting
Supporting the operational functionality, availability, efficiency, backup/recovery and security of the company's data technology
Managing solutions across distributed data centres and data platforms
Maintaining all relevant documentation and knowledge bases
Escalating application problems, documenting and following up to resolution
Streamline integration toolset
✨ If this sounds like something up your street, please apply here! ✨
Requirements
Essential
You have experience and a passion for data warehousing, big data and ETL projects
SQL is second nature to you
You’ve worked on a wide range of Data Warehouse problems -regulatory reporting, Business Intelligence, financial reporting projects and supporting Machine Learning
You have experience building robust and reliable data sets requiring a high level of control
You strive for improvement in your work and that of others, proactively identifying issues and opportunities
Microsoft SQL Server or MySQL Database technologies
Azure Data Bricks and Azure Data Factory
In-depth understanding of data management (e.g. permissions, recovery, security and monitoring)
Desirable
Experience/Knowledge of working in an agile environment and experience with agile methodologies such as TDD, Scrum, Kanban
Used to AGILE ways of working (Kanban, Scrum) tracked in JIRA
Tableau or Power BI as a visualization tool
Familiarity with C#.NET, PHP, JSON and the data access options of the language
Able to communicate in another language (Spanish, Italian, French, German)
Show more
Show less","Data Warehousing, Big Data, ETL, SQL, Azure Data Warehouse, Azure Data Bricks, Azure Data Factory, Data Management, Microsoft SQL Server, MySQL, Agile, TDD, Scrum, Kanban, JIRA, Tableau, Power BI, C#.NET, PHP, JSON","data warehousing, big data, etl, sql, azure data warehouse, azure data bricks, azure data factory, data management, microsoft sql server, mysql, agile, tdd, scrum, kanban, jira, tableau, power bi, cnet, php, json","agile, azure data bricks, azure data factory, azure data warehouse, big data, cnet, data management, datawarehouse, etl, jira, json, kanban, microsoft sql server, mysql, php, powerbi, scrum, sql, tableau, tdd"
E-Commerce Data Analyst,MinsterFB | Full Service Amazon Agency | B Corp,"Southwell, England, United Kingdom",https://uk.linkedin.com/jobs/view/e-commerce-data-analyst-at-minsterfb-full-service-amazon-agency-b-corp-3784335911,2023-12-17,Nottingham, United Kingdom,Associate,Hybrid,"MinsterFB is looking for an
E-Commerce Data Analyst
. To support our strong growth we are seeking a highly numerate, hard-working, online savvy individual to provide business insight to grow and improve our businesses.
You will be analysing data in both in Excel and within our proprietary tech stack.
Key Roles & Responsibilities:
You will:
As a Data Analyst you will be working alongside colleagues from other functions to analyse Data, provide reports and recommendations to support the companies' objective of profitable growth for our clients
Be part of a team that supports decision making through combining data from multiple sources. You will specialise in the commercial or operational sphere of the business
Key Skills:
Ability to work independently and to prioritise workload
Confident use of Excel and some knowledge of SQL
Strong numeracy and analytical skills
Good written and verbal communication skills
Desired Qualifications:
Educated to Degree Level with evidence of ability in Maths and an affinity with online/ IT being advantageous
Salary & Other Benefits:
£23,000 p.a.
Annual holiday entitlement is 33 days (inclusive of bank/public holidays) in the complete holiday year
Access to 24/7 employee assistance programme: including GP telephone consultation at a time to suit you, counselling service, legal and debt advice
Quarterly team and charity days
Plus, a host of other employee benefits
Key Personal Qualities:
MinsterFB are looking for somebody who is:
Enthusiastic about all things data and digital
Happy working carefully through complex and detailed tasks
Willing to learn with a 'can do' attitude
Works well as part of a team
Self motivated
Adaptable
Analytical - able to pick out and focus on the important measures
Location:
Minimum of two days a week from Southwell Office (mandatory)
When working from home you will be staying in regular contact through Zoom and MS Teams
We have a tight supportive culture that is strengthened through face to face interactions if you are unable to work two days a week from Southwell please don't apply
About MinsterFB
MinsterFB is a Certified B Corporation. As such we're part of a global community of businesses that meet high standards of social and environmental impact.
MinsterFB works with some of the UK's favourite brands such as Grenade, Yorkshire Tea, McVities and Chupa Chups to build their business on Amazon. They provide full account management, sales strategy, catalogue management, issue resolution and training. Key to their success is their deployment of all of the growth tools available to Amazon Sellers and Vendors.
Job Details:
Work hours are, Monday to Thursday 9am until 5.30pm, Friday 9am until 3pm
Flexibility to work remotely anywhere in the world for up to 4 consecutive weeks a year
This job unapologetically online / on screen. However with the flexibility that fantastic global internet coverage provides you will be able to work remotely anywhere in the world for up to 4 consecutive weeks a year. In addition employees are entitled to a 3mth unpaid sabbatical after 4 years of continuous employment
Entitlement for a 3month unpaid sabbatical after 4 years of continuous employment
How to Apply
If this role is of interest and you would like to learn more please attach your CV to the link provided and we will be in direct contact.
MinsterFB values a diverse workforce. Women, people of colour, people with disabilities, and members of the LGBTQ community are encouraged to apply. They believe an equitable and inclusive work environment and a diverse empowered team are key to achieving their mission. They're looking for candidates who can expand their business culture, are curious, plain-dealing, action orientated, bring their whole selves to work and meet the requirements of the role. All else is secondary. They strive to provide all candidates with an equitable and accessible recruitment process. If they can offer accommodations for you in the recruitment process or you have feedback on how to make their recruitment more accessible, please let them know.
Show more
Show less","Excel, SQL, Data Analysis, Communication, Team player, Adaptability, MS Teams, Zoom","excel, sql, data analysis, communication, team player, adaptability, ms teams, zoom","adaptability, communication, dataanalytics, excel, ms teams, sql, team player, zoom"
Senior Data & Process Analyst,Pendragon PLC,"Nottinghamshire, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-process-analyst-at-pendragon-plc-3779470334,2023-12-17,Nottingham, United Kingdom,Associate,Hybrid,"Senior Data & Process Analyst
Pendragon PLC
Hybrid working - requirements to work in the office once every 2 weeks with additional adhoc travel as required
Salary of up to £40,000, pension, critical illness, life assurance, 23 days holiday, plus bank holidays
As a Senior Data & Process Analyst, your role entails the design, development, and maintenance of reports, dashboards, and data visualizations. Additionally, you will collaborate closely with business units, providing data products and toolkits that support informed decision-making and data-driven insights. Reporting directly to the BI Manager, your expertise in data analysis, reporting, and data quality assurance will play a crucial role in enhancing our business operations and strategy.
If you’re looking for a role that’s a blend of being fast-paced and challenging, with a culture that enables you to be yourself and enjoy the ride, you’ll feel right at home:
Your Journey as a Senior BI & Process Analyst:
Create, develop, and maintain reports, dashboards, and data visualisations to deliver actionable insights that steer our strategic direction
Collaborate closely with business teams to ensure reporting solutions are tailored to meet their specific information requirements and to unearth data-driven insights and opportunities, driving our organisational strategy forward
Partner with business stakeholders, providing data products and toolkits that elevate operational efficiency and decision-making capabilities
Guarantee the quality, consistency, and accessibility of our data assets, safeguarding the integrity of our information
Implement robust data quality checks, validation processes, and error handling mechanisms to ensure sustained data accuracy
Are You Ready to Embrace the Challenge? We’re looking for someone who has:
Strong Microsoft proficiency, encompassing Power Automate and Power Apps, along with advanced Excell skills including VBA and Microsoft scripts
Extensive experience in data analysis and reporting, particularly with Power BI, demonstrating a proven track record in BI development
Proficiency in data visualisation tools and techniques, complemented by a good understanding of SQL and data querying
Sound knowledge of data modelling, database design and ETL processes
Excellent communication skills with the ability to work collaboratively and is open to new ideas whilst appreciating the difference each person makes
Exceptional project management skills adept at handling complex data projects
At Pendragon, Together, We’re Unstoppable:
Here at Pendragon, together, we’re unstoppable. Collaboration is at the heart of our culture, and it’s the power of our group coming together that unlocks our potential to transform automotive retail. We thrive in a fast-paced environment where the doors are open, if you have the attitude and drive to walk through them. We’ll empower you with the opportunities to bring out the best in yourself, and in return you’ll support and bring out the best in others.
Embrace the Gift of Movement:
We are dedicated to igniting the spark of movement and exploration. Our teams empower millions to experience the thrill of discovering new horizons, connecting with others, and embracing life’s fullest potential. Join us and seize the chance to make a real impact whilst embracing change and adapting to a world of endless possibilities.
Diversity is Our Strength:
Pendragon recognises the value that diversity brings to the workforce. This is why we positively welcome applications from all walks of life, backgrounds, and communities. If you have the motivation, skills and talent potential that we are looking for then get in touch. We are an equal opportunities employer.
Show more
Show less","Microsoft Power Automate, Microsoft Power Apps, Microsoft Excel, VBA, Microsoft scripts, Power BI, SQL, Data visualization, Data modelling, Database design, ETL processes, Data quality assurance, Project management","microsoft power automate, microsoft power apps, microsoft excel, vba, microsoft scripts, power bi, sql, data visualization, data modelling, database design, etl processes, data quality assurance, project management","data modelling, data quality assurance, database design, etl, microsoft excel, microsoft power apps, microsoft power automate, microsoft scripts, powerbi, project management, sql, vba, visualization"
Junior Data Engineer,Pendragon PLC,"Derbyshire, England, United Kingdom",https://uk.linkedin.com/jobs/view/junior-data-engineer-at-pendragon-plc-3785808482,2023-12-17,Nottingham, United Kingdom,Mid senior,Onsite,"Pendragon PLC, Hybrid – requirement to work from the office once every 2 weeks, with occasional ad hoc travel as required
Salary of up to £33,500 with pension, critical illness, life assurance, 23 days holiday (including bank holidays), exclusive company discounts on used car purchases, leasing deals and aftersales services
Are you ready to drive beyond the possible; to work for an industry game-changer with inspirational people and the opportunity to thrive in your career?
Here at Pendragon, we’re going through an exciting period of transformation and we’re looking for an enthusiastic individual to embrace this new challenge as a Junior Data Engineer.
As a Junior Data Engineer, you’ll be a crucial member of our BI Team. You will be integral in supporting data analysts, data scientists and the wider team to ensure high data accuracy and the smooth flow of data for analysis and reporting.
Your Journey As a Junior Data Engineer
If you’re looking for a role that’s a blend of being fast-paced and challenging, with a culture that enables you to be yourself and enjoy the ride, you’ll feel right at home:
Guarantee data accuracy, integrity and reliability by actively assisting in the collection of raw data
Work collaboratively in the extraction and transformation and loading of data into suitable formats for analysis and reporting
Positively contribute to the development and maintenance of data pipelines to facilitate the smooth flow of data utilising Azure technologies
Support in managing databases and storage solutions, ensuring data retrieval and storage efficiency
Meticulously record data processes and procedures for seamless knowledge transfer
Proactively identify and address data-related issues and provide support in troubleshooting and interrogation of data and data environments
Foster a culture of unity and collaboration, propelling the data team to exceed goals
Are You Ready to Embrace the Challenge? We’re looking for someone who has:
Demonstrated understanding of data integration and ETL processes, as well as data warehousing and data modelling principles
Excellent problem-solving skills with the ability to effectively overcome troubleshooting issues
Sound knowledge of cloud platforms, data engineering concepts and data validation
Passion for learning and personal development, with eagerness to pursue advanced data engineering skills and certifications
Ability to work collaboratively and is open to new ideas whilst appreciating the difference each person makes
At Pendragon, Together, We’re Unstoppable
Here at Pendragon, together, we’re unstoppable. Collaboration is at the heart of our culture, and it’s the power of our group coming together that unlocks our potential to transform automotive retail. We thrive in a fast-paced environment where the doors are open, if you have the attitude and drive to walk through them. We’ll empower you with the opportunities to bring out the best in yourself, and in return you’ll support and bring out the best in others.
Embrace The Gift Of Movement
We are dedicated to igniting the spark of movement and exploration. Our teams empower millions to experience the thrill of discovering new horizons, connecting with others, and embracing life’s fullest potential. Join us and seize the chance to make a real impact whilst embracing change and adapting to a world of endless possibilities.
Diversity Is Our Strength
Pendragon recognises the value that diversity brings to the workforce. This is why we positively welcome applications from all walks of life, backgrounds, and communities. If you have the motivation, skills and talent potential that we are looking for then get in touch. We are an equal opportunities employer.
Show more
Show less","Data Warehousing, Data Modelling, Data Integration, ETL, Cloud Platforms, Azure, Data Engineering Concepts, Data Validation, ProblemSolving, Troubleshooting, Collaboration, Data Pipelines","data warehousing, data modelling, data integration, etl, cloud platforms, azure, data engineering concepts, data validation, problemsolving, troubleshooting, collaboration, data pipelines","azure, cloud platforms, collaboration, data engineering concepts, data integration, data modelling, data validation, datapipeline, datawarehouse, etl, problemsolving, troubleshooting"
