job_title,company,job_location,job_link,first_seen,search_city,search_country,job level,job_type,job_summary,job_skills,job_skills_raw,job_skills_cleaned
Data Cabling Engineer (Contract),Digital Waffle,"Nottingham, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-cabling-engineer-contract-at-digital-waffle-3779524084,2023-12-17,Nottingham, United Kingdom,Mid senior,Onsite,"Position:
Data Cabling Engineer (Temp/Contract/Freelance)
Location:
Nottingham
Duration:
6 Months+
Job Summary: We are seeking a skilled and dedicated Data Cabling Engineer. The successful candidate will be responsible for the installation, testing, and termination of Cat6 and Cat6a branded copper cables within metal trunking and containment to ensure a reliable and efficient network infrastructure. The role involves working closely with the IT team to maintain high-quality data connectivity, support network expansion, and ensure optimal performance.
Responsibilities:
Cabling Installation:
Plan, install, and route Cat6 and Cat6a copper cables according to industry standards and best practices
Mount, secure, and organize cable pathways, including cable trays, conduits, and wall penetrations
Install cable connectors, outlets, and termination panels in compliance with design specifications
Termination and Labeling:
Terminate copper cables onto jacks, connectors, and patch panels with precise attention to detail and adherence to industry standards
Ensure proper labelling of cables, connectors, and outlets for easy identification and maintenance
Maintain accurate documentation of cable layouts, terminations, and labelling
Testing and Troubleshooting:
Conduct comprehensive cable testing using appropriate tools to validate signal quality, continuity, and performance
Troubleshoot and resolve connectivity issues, signal degradation, and other cable-related problems
Utilize testing equipment such as cable testers and certification tools to ensure compliance with required specifications
Quality Assurance:
Ensure all cabling work is performed to high-quality standards and follows company policies and guidelines
Perform quality checks on cable installations to confirm compliance with industry standards and design specifications
Network Expansion:
Collaborate with IT team members to support network expansion projects and ensure seamless integration of new cabling
Participate in designing and implementing cable layouts for new network infrastructure components
Health and Safety:
Adhere to safety protocols and guidelines to prevent accidents and maintain a safe working environment
Operate tools and equipment safely and responsibly, following industry best practices.
Requirements:
High school diploma or equivalent; additional technical certifications or coursework in cabling and network infrastructure is a plus
Proven experience as a Data Cabling Engineer or similar role, with expertise in Cat6 and Cat6a copper cable installation, termination, and testing
Familiarity with relevant cabling standards, such as TIA/EIA and ISO/IEC
Proficiency in using cable testing and certification tools
Strong understanding of network topologies, protocols, and network equipment
Ability to interpret technical drawings, schematics, and cabling diagrams
Excellent problem-solving skills with a keen attention to detail
Effective communication skills to collaborate with IT team members and project stakeholders
Ability to work independently or as part of a team in various environments, including office spaces, data centres, and remote locations
Strong commitment to safety and adherence to industry regulations
If you are a motivated Data Cabling Engineer with a passion for maintaining robust network infrastructure and ensuring seamless connectivity, we encourage you to apply. Contact Joe on joe@digitalwaffle.co.uk
Show more
Show less","Cat6, Cat6a, Copper cabling, Network infrastructure, Cable installation, Cable trays, Conduits, Cable connectors, Termination panels, Cable testing, Troubleshooting, Cable testing tools, Quality assurance, Network expansion, Network topologies, Network protocols, Technical drawings, Schematics, Cabling diagrams, Problemsolving, Effective communication, TIA/EIA, ISO/IEC","cat6, cat6a, copper cabling, network infrastructure, cable installation, cable trays, conduits, cable connectors, termination panels, cable testing, troubleshooting, cable testing tools, quality assurance, network expansion, network topologies, network protocols, technical drawings, schematics, cabling diagrams, problemsolving, effective communication, tiaeia, isoiec","cable connectors, cable installation, cable testing, cable testing tools, cable trays, cabling diagrams, cat6, cat6a, conduits, copper cabling, effective communication, isoiec, network expansion, network infrastructure, network protocols, network topologies, problemsolving, quality assurance, schematics, technical drawings, termination panels, tiaeia, troubleshooting"
Senior Database Security Engineer,Back on Track! Solutions,"Nottingham, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-database-security-engineer-at-back-on-track%21-solutions-3775789508,2023-12-17,Nottingham, United Kingdom,Mid senior,Onsite,"Company Description
Discover the Unexpected
Experian is the worlds leading global information services company. Were passionate about unlocking the power of data in order to transform lives and create opportunities for consumers, businesses and society. For more than 125 years, weve helped economies and communities flourish and were not done.
Our 21k amazing employees in 40+ countries believe the possibilities for you, and the world, are growing. Were investing in the future, through new technologies, talented people and innovation so we can help create a better tomorrow. To do this we employ the brightest minds that share our purpose and want to make a difference.
Job Description
This position sits within Experian Global Security Office (ESGO)
The mission of the EGSO?Engineering and Innovation Database Activity Monitoring (DAM) team is to monitor Experian sensitive data, safeguarding against negative impacting cyber events?that compromise the?Confidentiality, Integrity, and Availability of that data.
The EGSO?Engineering and Innovation DAM Team performs the following key functions:
Identify Sensitive Data and Protection of that sensitive data through suitable logging and monitoring methods.
Perform Control Engineering of security tools to detect, monitor and protect sensitive data against potential malicious activity. Currently focused on logging and monitoring Database activity using tools such as IBM Guardium.
Ensuring Security Compliance against regulations enforced by respective governments.
Identify restricted data (PII, PCI and PHI) in structured and unstructured data both On-Prem and cloud platforms.
The focus for the role will be on Data identification and Monitoring both in cloud and on-premises data repositories.
Senior Database Security Engineer
As a Database Security Engineer, you will be responsible for developing roadmaps and setting the technical direction for data protection service areas such as database activity monitoring On-Prem and in Cloud (AWS, AZURE ,GCP and OCI). This engineer will establish the tactical roadmap (aligned to the tower strategy) that is focused on the implementation of future state security controls following these guiding principles: Agile, Available, Simple to implement, and Automated.
The Data Security Engineer is responsible for the build-out?of the data security controls and for planning to ensure health and data protection is maintained at the highest levels.?This role is the escalation point for Data Protection technical gaps and provides change governance.
Role Functions
Provide SME expertise on security tool capabilities and configuration adjustments,?when needed, to implement controls such as: Identify restricted Data, Implement database monitoring, Perform Active Threat Analytics, Database misconfiguration and Database Entitlement reporting as well as during security incidents or block future security attacks
Collaborate?with the Manager to identify capability gaps and operational inconsistencies within the Data Protection controls environment and develop a plan to address through product enhancement, reconfiguration, upgrades and/or automation of processes
Build, manage, and maintain the automated reporting dashboards system
Provide on-call support
Collaborate with Customers and other technology teams to ensure operational requests and incidents are resolved within defined SLAs
Leverage tooling to monitor and optimize the performance of the wider data protection operations team
Analyze, troubleshoot, and resolve complex technical Data Protection control issues
Build, manage, and maintain the intake process for requests on service now or a suitable tool.
Examine concepts, work on Proof of Concepts, develop Minimum Viable Product and scale and operationalize the product including documentation and process development
Key Responsibilities:-
Establish a baseline plan and corresponding execution roadmaps.
Refresh and re-publish tactical Plan for advanced data protection controls in June of every year
Report progress against roadmap every month
Refresh Data Protection roadmap/s quarterly
Define plan and roadmap for dashboard automation and service now intake process and report weekly progress against plan
Ensure Technical and Process documentation is 100% current all the time (all changes thoroughly documented)
Ensure new builds/integrations/agent implementation follow operational readiness processes, are fully documented, health/performance KPIs are defined and in place, and monitoring and alerting is in place before promoting to production
Provide change governance
Demonstrate continual progress toward obtaining a security-specific (or specific security product certification)- or maintain a current certification
Complete?two career-related training courses per year
Perform monthly training sessions for business teams
Attend lunch and learn sessions to?share knowledge with Engineering group
Mentors team members to promote continual growth
Independently/informally leading teams on projects through completion
Publish build?documentation for the Relevant Data Protection controls on the EGSO WiKi
Advocate Data Protection Controls across Experian
Maintain objective progress documentation.
Documentation documentation of business cases, POCs, Procedures, results of POCs, Test cases, integration guides and relevant runbooks to operationalize database activity monitoring and data discovery for Experian.
Qualifications
Demonstrable experience in a similar position.
Strong Database Administration skills.
Strong leadership skills
In-depth knowledge of various operation systems On-Premise: Windows, Linux/Unix , Mainframe and shell scripts Cloud:AWS, Azure, OCI, Google
Experience working with cloud platforms like AWS, Oracle, Azure , Google
Experience with Data activity monitoring tools like IBM Guardium, Imperva
Knowledge of Network technologies
Knowledge of SIEM tools
Preferable
Certified in DAM tools
Experience with Splunk and Exabeam tools
Knowledge and Experience with Data Governance tools like IBM Watson Knowledge catalog, Data360, ASG
Additional Information
Our uniqueness is that we truly celebrate yours.
Experian's culture and people are key differentiators. We take our people agenda very seriously. We focus on what truly matters; DEI, work/life balance, development, authenticity, engagement, collaboration, wellness, reward & recognition, volunteering... the list goes on. Were an award winning organisation due to our strong people first approach.
Innovation is a critical part of Experian's DNA and practices, and our diverse workforce drives our success. Everyone can succeed at Experian and bring their whole self to work, irrespective of their gender, ethnicity, colour, sexuality, physical ability or age. If you have a disability or special need that requires accommodation, please let us know at the earliest opportunity.
Experian Careers - Creating a better tomorrow together
Find out what its like to work for Experian by clicking here
Show more
Show less","Database Security, Data Protection, Data Activity Monitoring, IBM Guardium, Data Governance, Identity and Access Management, Data Analytics, Data Discovery, AWS, Azure, OCI, Google Cloud Platform, Unix, Windows, Mainframe, Splunk, SIEM, Data360, ASG","database security, data protection, data activity monitoring, ibm guardium, data governance, identity and access management, data analytics, data discovery, aws, azure, oci, google cloud platform, unix, windows, mainframe, splunk, siem, data360, asg","asg, aws, azure, data activity monitoring, data discovery, data governance, data protection, data360, dataanalytics, database security, google cloud platform, ibm guardium, identity and access management, mainframe, oci, siem, splunk, unix, windows"
Senior Data Engineer,Nigel Frank International,"Derby, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-at-nigel-frank-international-3731258810,2023-12-17,Nottingham, United Kingdom,Mid senior,Onsite,"I am working with a market leading manufacturer of specialist medical equipment who are looking for a Senior Data Engineer at the start of a number of exciting data driven projects. You would be joining at the start of a greenfield project where you will be responsible for the data migration to the brand new Azure Data Platform!
The overall goal of the business and ultimately this role is to deliver data to healthcare professionals to aid with rehabilitation. You will join a close-knit team, with talented professionals who specialise in a number of areas including software development and data science. You will become the expert for this team on all things data engineering and will take the lead on the creation, development and maintenance of the Azure data platform.
As part of this role, you will be responsible for some of the following areas.
Design, maintain and optimise on premise database solutions.
Develop and maintain robust ETL pipelines.
Work with other specialists within the team to make technical decisions that will benefit the overall business.
Take the lead of migration of data to the new Azure data platform.
This is a salaried role paying up to £65,000 per annum depending on experience and a company benefits package. This role would be largely a remote opportunity with occasional visits to the office in Stafford. These visits would be as and when needed in line with project requirements.
To be successful in the role you will have.
Strong knowledge of Databricks for data ingestion and transformation.
Experience working with the Azure tech stack including Azure Data Factory, Synapse and Azure Data Lake.
Experience designing and implementing ETL solutions.
Strong coding experience with languages such as Python or C#.
This is just a brief overview of the role. For the full information, simply apply to the role with your CV, and I will call you to discuss further. My client is looking to begin the interview process ASAP, so don't miss out, APPLY now! To do so please email me at a.pinkerton@nigelfrank.com or call me on 0191 3387487.
Nigel Frank International are the go-to recruiter for Power BI and Azure Data Platform roles in the UK offering more opportunities across the country than any other recruitment agency. We're the proud sponsor and supporter of SQLBits, Power Platform World Tour, the London Power BI User Group, Newcastle Power BI User Group and Newcastle Data Platform and Cloud User Group. We are the global leaders in Microsoft recruitment.
Show more
Show less","SQL, Azure Data Platform, Azure Data Factory, Synapse, Azure Data Lake, Python, C#, Databricks, ETL, Data Migration, Software Development, Data Science","sql, azure data platform, azure data factory, synapse, azure data lake, python, c, databricks, etl, data migration, software development, data science","azure data factory, azure data lake, azure data platform, c, data migration, data science, databricks, etl, python, software development, sql, synapse"
Data Analyst - SQL & Power BI - UK-wide,Nigel Frank International,"Leicester, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-analyst-sql-power-bi-uk-wide-at-nigel-frank-international-3780312515,2023-12-17,Nottingham, United Kingdom,Mid senior,Onsite,"A healthcare provider are looking for an Information Analyst with skills in SQL and either SSRS or Power BI to join their established Management Information team, where you'll be responsible for delivering timely and accurate data and reporting solutions for users across the organisation.
This organisation are headquartered in Hertfordshire, although this is a home-based role, so you can be based anywhere in the UK. It's likely you will be asked to travel to their Head Office around twice per year for full IT team meetings, and this will be fully expensed.
We are looking for an individual with strong SQL and T-SQL skills in order to effectively work with the data in their SQL and Snowflake database, as well as experience with either SSRS or Power BI to produce insightful reports and visualisations - currently they're on a journey to migrate most of their SSRS reports over to Power BI, with any new reports being created in Power BI.
This is a really exciting opportunity for an ambitious data professional to progress their career with an organisation who will be committed to your ongoing professional development, with the chance to work towards your Microsoft certifications - you'll have the opportunity to complete your Power BI and Snowflake certifications, and get your ITIL qualifications too.
Requirements:
Strong SQL and T-SQL skills - including complex queries, stored procedures, tables, views etc.
SSRS or Power BI Report builder experience (including DAX)
Excellent communication, problem solving and stakeholder management skills
Benefits:
Salary of £25-32,000, depending upon experience
25 days' annual leave plus bank holidays plus day off for your birthday (increasing by 1 day for every 2 years' service up to 28 days)
Pension with 5% employer contribution and 3% employee contribution
Enhanced maternity package
Death in service (2X salary)
Learning and development opportunities
Please Note: This is a permanent role for UK residents only. This role does not offer Sponsorship. You must have the right to work in the UK with no restrictions. Some of our roles may be subject to successful background checks including a DBS and Credit Check.
Nigel Frank are the go-to recruiter for Power BI and Azure Data Platform roles in the UK, offering more opportunities across the country than any other. We're the proud sponsor and supporter of SQLBits, Power Platform World Tour, the London Power BI User Group, Newcastle Power BI User Group and Newcastle Data Platform and Cloud User Group. To find out more and speak confidentially about your job search or hiring needs, please contact me directly at v.simpson@nigelfrank.com
Show more
Show less","SQL, TSQL, SSRS, Power BI, DAX, Snowflake","sql, tsql, ssrs, power bi, dax, snowflake","dax, powerbi, snowflake, sql, ssrs, tsql"
Business Data Analyst,Yunex Traffic,"Leicester, England, United Kingdom",https://uk.linkedin.com/jobs/view/business-data-analyst-at-yunex-traffic-3769000653,2023-12-17,Nottingham, United Kingdom,Mid senior,Onsite,"Do you want to help shape the future of the city?
We connect the dots of a new mobility revolution that will transform cities all over the world.We do this by empowering mobility, bringing it to the next level with intelligent road traffic technology. Complex problems, focused solutions, and direct impact on the satisfaction of millions of motorists, pedestrians, and cyclists all over the world. We are the magic behind the scenes, the team responsible for installing and maintaining traffic controllers, signals, and other traffic products. Our technology keeps our roads moving safely. We are the UK’s #1 Intelligent Traffic Solutions (ITS) provider and maintain over 40% of the UK’s ITS assets. The Traffic Enforcement market is booming! We’re at the forefront of developing smart answers to these challenges to actively shape the mobility world for the next generation.
Do you want to join a business that puts its people at the forefront of all we do, an agile, innovative business that is leading the way in traffic solutions? Come and join us at Yunex Traffic.
We are seeking an experienced and results-oriented Business Analyst, based out of our office in Leicester, with a strong focus on driving business improvements within our service organisation. You will be responsible for analysing and optimizing existing processes, identifying areas of inefficiency, and recommending solutions to enhance operational service performance and increase profitability. You will collaborate closely with cross-functional teams, stakeholders, and senior management to implement strategic changes and achieve measurable business outcomes.
What will you do?
Process Optimisation: Conduct thorough analysis of current business processes and workflows to identify bottlenecks, inefficiencies, and areas for improvement across the full spectrum of our support and maintenance contracts.
Continuous Improvement Initiatives: Proactively lead and participate in continuous improvement initiatives across the Enforcement organization, leveraging Lean, Six Sigma, or other relevant methodologies.
Cost Reduction Strategies: Identify cost-saving opportunities and recommend strategies to optimize resource allocation, reduce operational expenses, and increase overall profitability.
Data-Driven Insights: Utilize data analytics and modelling techniques to extract insights, identify trends, and support evidence-based decision-making for business improvement initiatives.
Performance Metrics: Utilise and present contractual and internal key performance indicators to measure the success and impact of business improvement projects and contract performance.
Innovation Initiatives: Identify opportunities for innovation and propose technology or digital solutions that streamline operations and enhance customer experiences.
Who are you?
An operational or commercial background coupled with a formal business qualification.
You have excellent communication skills, with customers, external partners, and cross functionally.
Experience in driving positive change and enhancing business performance.
Confident to initiate, negotiate, and conclude difficult contractual discussions where necessary to ensure that change is managed and communicated effectively.
The ability to lead change through small projects, control tasks, resource, and cost to ensure and measure the success of sponsored initiatives.
Good knowledge of SQL and Microsoft Office tools; particularly Excel, PowerPoint, Word, Microsoft Project, Power Automate and other relevant modelling and Management Information Products to analyse and present large data sets.
What do we offer?
Base salary and annual bonus
26 days holiday, increasing up to 29 days with length of service
Excellent pension, matching contributions up to 10% of pensionable salary
Flexible benefits to suit your personal needs (plenty of choice)
Investment in personal development and support to membership of professional institutions
About Us
We’re Yunex Traffic. We have offices across the UK and worldwide, full of hardworking individual’s helping us to challenge the today and work towards a brighter tomorrow. We’re excited to hear that you would like to join us here at Yunex Traffic. Our people love it here and we want you to be a part of helping us make real, what matters.
Our Commitment: Yunex Traffic is committed to promoting equality, diversity, and inclusivity. We recognise that building a diverse workforce is essential to the success of our business. We strongly encourage applications from a diverse talent pool and welcome the opportunity to discuss any requirements you may have, including workplace adjustments.
How do I apply?
We are looking forward to receiving your online application. Please ensure you complete all areas, of the application form, to the best of your ability as we will use this data to review your suitability for the role.
Show more
Show less","Data analytics, Data modelling, Lean Six Sigma, SQL, Microsoft Office Suite, Microsoft Project, Power Automate, Contract management","data analytics, data modelling, lean six sigma, sql, microsoft office suite, microsoft project, power automate, contract management","contract management, data modelling, dataanalytics, lean six sigma, microsoft office suite, microsoft project, power automate, sql"
Portfolio Analyst (Data Analyst),Capital One,"Nottingham, England, United Kingdom",https://uk.linkedin.com/jobs/view/portfolio-analyst-data-analyst-at-capital-one-3779605992,2023-12-17,Nottingham, United Kingdom,Mid senior,Onsite,"Nottingham Trent House (95002), United Kingdom, Nottingham, NottinghamshirePortfolio Analyst (Data Analyst)
About This Role
This role sits within the Strategy, Insight and Governance team (S.I.G) in our Technology Department that holds accountability for providing internal consultancy to help transform UK Tech.
What You’ll Do
Ability to pay attention to detail, communicate well and be a strong business partner are essential skills for this role. We need you to not only understand the data, but be able to provide insight and analysis through clear visual, written and verbal communication.
As an analyst within the team, you'll need
Strong analytical, critical thinking, and creative problem-solving skills
Proficient at manipulating and analysing large datasets using SQL (preferably using Snowflake)
Able to design, develop, and implement business intelligence solutions (preferably using Tableau)
Prepare and present reports and visualisations in order to communicate findings to stakeholders
Business partner within Tech Delivery in order to produce high quality, accurate and timely reporting that effectively informs and drives the investment portfolio of the business.
Identify areas to increase efficiency and automation of processes
Maintain automated data processes
Develop and support reporting processes
Gather, understand and document detailed customer requirements using appropriate tools and techniques
Manipulate, analyse and interpret complex data sets relating to the Tech Delivery
Prepare reports for internal audiences using business analytics reporting tools
Create data dashboards, graphs and visualisations
What We’re Looking For
Knowledge of Database and Data Warehousing concepts
Experience delivering self-service tools, analysis and data solutions
Good facilitation skills – ability to collaborate/present to a variety of individuals
Excellent numerical and analytical skills
Knowledge of data analysis tools
The ability to carry out data quality control, validation and linkage
Strong programming languages - SQL
Advanced Excel/Google Sheets
Strong problem-solving abilities and attention to detail
Ability to work independently and as part of a team
What you’ll get to learn (any previous experience would be advantageous)
Business focussed with a strong appreciation for/background in Tech
Experience in business intelligence and analytics platforms - Tableau
Experience with using the delivery tool Jira
Where And How You'll Work
This is a permanent position based in our Nottingham office.
We have a hybrid working model which gives you flexibility to work from our offices and from home.
We’re big on collaboration and connection, so you’ll be based in our Nottingham office 3 days a week on Tuesdays, Wednesdays and Thursdays.
Many of our associates have flexible working arrangements, and we're open to talking about an arrangement that works for you.
What’s In It For You
Bring us all this - and you’ll be well rewarded with a role contributing to the roadmap of an organisation committed to transformation
We offer high performers strong and diverse career progression, investing heavily in developing great people through our Capital One University training programmes (and appropriate external providers)
Immediate access to our core benefits including pension scheme, bonus, generous holiday entitlement and private medical insurance – with flexible benefits available including season-ticket loans, cycle to work scheme and enhanced parental leave
Open-plan workspaces and accessible facilities designed to inspire and support you. Our Nottingham head-office has a fully-serviced gym, subsidised restaurant, mindfulness and music rooms. In London, you can heighten your mood with a run on our rooftop running track or an espresso at the Workshop Coffee café
What You Should Know About How We Recruit
We pride ourselves on hiring the best people, not the same people. Building diverse and inclusive teams is the right thing to do and the smart thing to do. We want to work with top talent: whoever you are, whatever you look like, wherever you come from. We know it’s about what you do, not just what you say. That’s why we make our recruitment process fair and accessible. And we offer benefits that attract people at all ages and stages.
We also partner with organisations including the Women in Finance and Race At Work Charters, Stonewall and upReach to find people from every walk of life and help them thrive with us. We have a whole host of internal networks and support groups you could be involved in, to name a few:
REACH – Race Equality and Culture Heritage group focuses on representation, retention and engagement for associates from minority ethnic groups and allies
OutFront – to provide LGBTQ+ support for all associates
Mind Your Mind – signposting support and promoting positive mental wellbeing for all
Women in Tech – promoting an inclusive environment in tech
EmpowHER - network of female associates and allies focusing on developing future leaders, particularly for female talent in our industry
Capital One is committed to diversity in the workplace.
If you require a reasonable adjustment, please contact ukrecruitment@capitalone.com All information will be kept confidential and will only be used for the purpose of applying a reasonable adjustment.
For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com
Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site.
Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).
Show more
Show less","SQL, Snowflake, Tableau, Business intelligence, Data analysis, Data quality control, Data validation, Data linkage, Programming languages, Advanced Excel, Google Sheets, Problemsolving, Jira, Database, Data warehousing, Selfservice tools, Facilitation skills, Numerical skills, Analytical skills","sql, snowflake, tableau, business intelligence, data analysis, data quality control, data validation, data linkage, programming languages, advanced excel, google sheets, problemsolving, jira, database, data warehousing, selfservice tools, facilitation skills, numerical skills, analytical skills","advanced excel, analytical skills, business intelligence, data linkage, data quality control, data validation, dataanalytics, database, datawarehouse, facilitation skills, google sheets, jira, numerical skills, problemsolving, programming languages, selfservice tools, snowflake, sql, tableau"
Senior Data Analyst,Manila North Harbour Port,"Derby, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-analyst-at-manila-north-harbour-port-3767660849,2023-12-17,Nottingham, United Kingdom,Mid senior,Onsite,"Our Incora Story is really taking off wed love you to join us for the journey
We are currently seeking an exceptional professional to join us as a
Senior Data Analyst (Derby/Poland).
As a
Senior Data Analyst
you will support our Supply Chain function by providing first-class, reporting and analysis skills to our stakeholders.
You will be a key part of our organisation, ensuring analysis of information is translated into meaningful data that can be used to improve the profitability of Incora.
About You
You will be a resilient strong communicator, exceptional at working with large numbers, with a background in Supply Chain. Some experience of managing people is preferable.
Advanced excel skills are a must for this role, along with a knowledge of MRP/ERP systems and an understanding of planning. This role requires a good level of reporting and an ability to translate that data to enable meaningful conversations with both internal and external stakeholders. Also, improving existing reporting methodologies utilizing latest technologies such as Qlikview and Power BI for efficient data processing.
We are interested in people with a mathematics background (a degree in this area is preferable but not essential) with around two years experience of working in an analytics environment where excellence is at the heart of what you do.
Most importantly you will be passionate about our Incora Values and exceeding customer expectations.
Courage
Take ownership for issues and problems.
Collaboration Share Knowledge And Experience With Others
Commitment
Focus on ensuring internal and external customer expectations are met in a timely way.
In return we will offer the things that matter
Starting salary between £35,000 to £40,000 dependent on experience
We want you to share in the success of Incora, so every employee is eligible for one of our incentive programs.
We want you to enjoy life as well as work so we offer great holiday benefits such as 25 days holiday plus 8 bank holidays with a holiday buying scheme with the ability to buy up to 5 additional days
We want you to have peace of mind regarding your future with our generous Company pension scheme including 4 times salary life cover
We want to help you look after you and your family by providing Health cash plans saving you money and helping you to stay well
We want to support you and the people that matter to you with our flexible working and family friendly policies and practices
Incora cares about our environmental impact and thats why we reward people who use green transport to travel to work with our Green Travel Allowance
We believe every little helps by offering a generous discount portal only available to Incora employees
Very importantly we want to be with you to help your career take off and become part of the Incora Story
So, if you feel you have all of this, and want to be part of developing the Incora Story apply now we cant wait to hear from you
We are passionate about Diversity & Inclusion and we welcome applications from all persons regardless of their age, race, gender, disability, religion/belief, or sexual orientation.
Incora
is a leading, global provider of innovative supply chain solutions. Our promise to our customers is to reduce complexity and increase reliability so they can deliver mission-critical solutions that improve our world. We exist to enable the mission critical.
With a strong foundation in the aerospace and defence markets, we also utilize our expertise to serve industrial manufacturing, marine, pharmaceuticals, and beyond. We manage all aspects of supply chain from procurement to warehouse management, from logistics to on-site customer services.
We Pride Ourselves On Offering
Industry-leading capabilities
Innovative solutions
Greater customer value
Trusted supplier relationships
Business growth and operational efficiency
Incorarequests that recruitment agencies do not speculatively submit CVs to any employee or associate of our company unless explicitly asked to do so by a member of our Talent Acquisition Team or Hiring Manager for a particular role. Any CVs sent to an Incora employee on a speculative basis will not berecognisedand we will not accept liability for any fee or commission, should we subsequently employ a candidate who applied to us directly or was introduced by a member of our Talent Acquisition Team.
Show more
Show less","Data Analysis, Reporting, Advanced Excel, MRP/ERP systems, Planning, Qlikview, Power BI, Mathematics, Supply Chain","data analysis, reporting, advanced excel, mrperp systems, planning, qlikview, power bi, mathematics, supply chain","advanced excel, dataanalytics, mathematics, mrperp systems, planning, powerbi, qlikview, reporting, supply chain"
Capability Engineer -Data Exchange Controller - Submarines,Rolls-Royce,"Derby, England, United Kingdom",https://uk.linkedin.com/jobs/view/capability-engineer-data-exchange-controller-submarines-at-rolls-royce-3730217166,2023-12-17,Nottingham, United Kingdom,Mid senior,Onsite,"Job Description
Capability Engineer – Data Exchange Controller - Submarines
Full Time
Derby
Why Rolls-Royce?
An exciting opportunity has arisen for a
Capability Engineer – Data Exchange Controller
to join Rolls-Royce Submarines Business in Derby.
We’re a leader in propulsion system design and development for the Royal Navy’s nuclear submarine fleet and the sole provider and technical authority.
Join us and we’ll provide an environment where you can be yourself. An inclusive, digital-first culture that invests in you, gives you a platform for continuous learning, and access to an incredible breadth and depth of opportunities to grow your career.
As a
Capability Engineer – Data Exchange Controller y
ou’ll be responsible for ensuring the movement and control of data and information between IT networks in support of Novel Nuclear.
This is an exciting time to join the team with key programmes and developments in the pipeline and plenty of opportunity to contribute to a broad technology development portfolio.
We offer excellent development, a competitive salary and exceptional benefits. These include bonus, employee support assistance and employee discounts.
Your needs are as unique as you are. Hybrid working is a way in which our people can balance their time between the office, home or another remote location. It’s a locally managed and flexed informal discretionary arrangement. As a minimum we’re all expected to attend the workplace for collaboration and other specific reasons, on average three days per week.
What You Will Be Doing
You will ensure that the Novel Nuclear Team are equipped to exchange data and information between IT networks and external organisations in a controlled and auditable manner. With this exciting opportunity you will be also carrying out following key tasks:
Responsible for the logging of data and information received from external partners to Novel Nuclear, ensuring compliance to Export Control and Security classification requirements.
Responsible for the definition of processes to transfer information between Rolls-Royce networks.
Responsible for carrying out secondary checks on Export Control classifications prior to data transfer outside of the organisation.
Responsible, with support from the wider Capability team, for the development of Integrated Working Agreements between organisations and to develop appropriate collaborative exchange mechanisms/processes.
Who We Are Looking For
At Rolls-Royce we embrace agility, are bold, pursue collaboration and seek simplicity in everything we do. These principles form our values and behaviours and are an essential component of our assessment process and are fundamental qualities that we seek for all roles.
To be successful in this role you will need to have:
Good organisational skills.
Great communication and interpersonal skills.
A good basic working IT system knowledge.
A keen attention to detail.
A good understanding of export control within the Submarines Business.
We are an equal opportunities employer. We’re committed to developing a diverse workforce and an inclusive working environment. We believe that people from different backgrounds and cultures give us different perspectives. And the more perspectives we have, the more successful we’ll be. By building a culture of respect and appreciation, we give everyone who works here the opportunity to realise their full potential.
You can learn more about our global Inclusion strategy at Our people | Rolls-Royce
To work for Rolls-Royce Submarines you need to hold a Security Check (SC) clearance without any caveats to that clearance. Rolls-Royce will support the application for Security Clearance if you don’t currently already have this in place. Due to the nature of work the business conducts and the protection of certain assets, you must hold a UK nationality. Any dual nationals will require additional scrutiny and background checks prior to commencing work with RRSL.
Closing Date: 21st December 2023
Job Category
Electrical and Electronics
Posting Date
06 Dec 2023; 00:12
Show more
Show less","Data Exchange, IT Networks, Export Control, Security Classifications, Integrated Working Agreements, Organizational Skills, Communication, Interpersonal Skills, IT Systems Knowledge, Attention to Detail","data exchange, it networks, export control, security classifications, integrated working agreements, organizational skills, communication, interpersonal skills, it systems knowledge, attention to detail","attention to detail, communication, data exchange, export control, integrated working agreements, interpersonal skills, it networks, it systems knowledge, organizational skills, security classifications"
Data Analyst (18 month contract),Capital One,"Nottingham, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-analyst-18-month-contract-at-capital-one-3785725457,2023-12-17,Nottingham, United Kingdom,Mid senior,Onsite,"Nottingham Trent House (95002), United Kingdom, Nottingham, NottinghamshireData Analyst (18 month contract)
About The Role
If you like data and want to learn more then this could be the role for you.
Data Analysis sits at the heart of the data function within our UK business. The work we do drives significant value, directly influencing information based strategies, and helping steer the company agenda.
You will have the opportunity to learn advanced data analytics. Leveraging analytic and technical skills to innovate, build, and maintain well-managed data solutions, to make our business data more consumable.
On any given day you will be tasked with multiple types of work, across Problem solving, Solution generation, Innovation, and Data Management.
What you’ll do;
Extract & Manipulate data to fulfill on a range of requests.
Partner with diverse teams and key stakeholders to design and develop usable data solutions, tools, and capabilities to enable self-service frameworks for data consumers.
Scan and use available data sources to help provide insight to business areas
Demonstrate ability to explore and quickly grasp new technologies to progress varied initiatives.
Hone in on specific skills sets and workstreams, with the potential of developing into a business wide subject matter expert.
Advanced use of campaign analysis tools combined with expert knowledge of customer targeting, personalisation and segmentation methods, to develop automated campaign solutions.
Other tasks include but not limited to; consultancy and resolution support for customer impacting issues, fulfillment of customer account updates and producing large data sets and sample files to support customer surveys.
What We’re Looking For
Structured thinking and a rational and inquisitive approach to solving problems using data.
A natural affinity with numbers and enthusiasm to use numerical ability to spot trends.
The ability to take your own ideas, articulate them clearly and then develop solutions.
Can demonstrate competencies in core Data Analyst skills, including: Coding, data knowledge and system understanding. Experience in an Analytical role desired.
Some knowledge of at least one relevant language is essential (i.e. SQL, Python, or R).
Good stakeholder management, able to think through and communicate data to non technical stakeholders.
Able to self prioritise workloads, displaying consistent delivery of work, drive successful outcomes, and understand when to escalate to their manager.
Where And How You'll Work
This is a temporary position initially lasting 18 months, however this may lead to future permanent opportunities within the business. The position will be based in our Nottinghamoffices.
Our hybrid working model offers you the flexibility to work from our offices and from home.
We're big on collaboration and connection, and so generally encourage our associates to use our offices on Tuesdays, Wednesdays and Thursdays.
The number of days you spend in the office will usually be led by the type of work you’re doing, and the hybrid working patterns of the people you partner most closely with.
Many of our associates have flexible working arrangements, and we're open to talking about an arrangement that works for you.
What’s In It For You
Bring us all this - and you’ll be well rewarded with a role contributing to the roadmap of an organisation committed to transformation
Should the position progress to a permanent role, we offer high performers strong and diverse career progression, investing heavily in developing great people through our Capital One University training programmes (and appropriate external providers)
Immediate access to our core benefits including pension scheme, bonus, generous holiday entitlement and private medical insurance – with flexible benefits available including season-ticket loans, cycle to work scheme and enhanced parental leave
Open-plan workspaces and accessible facilities designed to inspire and support you. Our Nottingham head-office has a fully-serviced gym, subsidised restaurant, mindfulness and games rooms. In London, you can heighten your mood with a run on our rooftop running track or an espresso at the Workshop Coffee café
What You Should Know About How We Recruit
We pride ourselves on hiring the best people, not the same people. Building diverse and inclusive teams is the right thing to do and the smart thing to do. We want to work with top talent: whoever you are, whatever you look like, wherever you come from. We know it’s about what you do, not just what you say. That’s why we make our recruitment process fair and accessible. And we offer benefits that attract people at all ages and stages.
We also partner with organisations including the Women in Finance and Race At Work Charters, Stonewall and upReach to find people from every walk of life and help them thrive with us. We have a whole host of internal networks and support groups you could be involved in, to name a few:
REACH – Race Equality and Culture Heritage group focuses on representation, retention and engagement for associates from minority ethnic groups and allies
OutFront – to provide LGBTQ+ support for all associates
Mind Your Mind – signposting support and promoting positive mental wellbeing for all
Women in Tech – promoting an inclusive environment in tech
EmpowHER - network of female associates and allies focusing on developing future leaders, particularly for female talent in our industry
Capital One is committed to diversity in the workplace.
If you require a reasonable adjustment, please contact ukrecruitment@capitalone.com All information will be kept confidential and will only be used for the purpose of applying a reasonable adjustment.
For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com
Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site.
Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).
Show more
Show less","Data Analysis, Solution Generation, Innovation, Data Management, Data Extraction, Data Manipulation, SQL, Python, R, Campaign Analysis, Customer Targeting, Personalization, Segmentation, Structured Thinking, Rational Approach, Numerical Ability, Coding, Data Knowledge, Stakeholder Management, Communication, Prioritization, Delivery, Escalation, Collaboration, Flexibility, Hybrid Working, Openplan Workspaces, Accessible Facilities, Gym, Restaurant, Mindfulness, Games Rooms, Running Track, Coffee Café, Diversity, Inclusion, Reasonable Adjustment, Technical Support","data analysis, solution generation, innovation, data management, data extraction, data manipulation, sql, python, r, campaign analysis, customer targeting, personalization, segmentation, structured thinking, rational approach, numerical ability, coding, data knowledge, stakeholder management, communication, prioritization, delivery, escalation, collaboration, flexibility, hybrid working, openplan workspaces, accessible facilities, gym, restaurant, mindfulness, games rooms, running track, coffee caf, diversity, inclusion, reasonable adjustment, technical support","accessible facilities, campaign analysis, coding, coffee caf, collaboration, communication, customer targeting, data extraction, data knowledge, data management, data manipulation, dataanalytics, delivery, diversity, escalation, flexibility, games rooms, gym, hybrid working, inclusion, innovation, mindfulness, numerical ability, openplan workspaces, personalization, prioritization, python, r, rational approach, reasonable adjustment, restaurant, running track, segmentation, solution generation, sql, stakeholder management, structured thinking, technical support"
Senior Database Security Engineer,Experian,"Ruddington, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-database-security-engineer-at-experian-3776237473,2023-12-17,Nottingham, United Kingdom,Mid senior,Remote,"Company Description
Discover the Unexpected
Experian is the world’s leading global information services company. We’re passionate about unlocking the power of data in order to transform lives and create opportunities for consumers, businesses and society. For more than 125 years, we’ve helped economies and communities flourish – and we’re not done.
Our 21k amazing employees in 40+ countries believe the possibilities for you, and the world, are growing. We’re investing in the future, through new technologies, talented people and innovation so we can help create a better tomorrow. To do this we employ the brightest minds that share our purpose and want to make a difference.
Job Description
This position sits within Experian Global Security Office (ESGO)
The mission of the EGSO Engineering and Innovation Database Activity Monitoring (DAM) team is to monitor Experian sensitive data, safeguarding against negative impacting cyber events that compromise the Confidentiality, Integrity, and Availability of that data.
The EGSO Engineering and Innovation DAM Team performs the following key functions:
Identify Sensitive Data and Protection of that sensitive data through suitable logging and monitoring methods.
Perform Control Engineering of security tools to detect, monitor and protect sensitive data against potential malicious activity. Currently focused on logging and monitoring Database activity using tools such as IBM Guardium.
Ensuring Security Compliance against regulations enforced by respective governments.
Identify restricted data (PII, PCI and PHI) in structured and unstructured data both On-Prem and cloud platforms.
The focus for the role will be on Data identification and Monitoring both in cloud and on-premises data repositories.
Senior Database Security Engineer
As a Database Security Engineer, you will be responsible for developing roadmaps and setting the technical direction for data protection service areas such as database activity monitoring On-Prem and in Cloud (AWS, AZURE ,GCP and OCI).  This engineer will establish the tactical roadmap (aligned to the tower strategy) that is focused on the implementation of future state security controls following these guiding principles: Agile, Available, Simple to implement, and Automated.
The Data Security Engineer is responsible for the build-out of the data security controls and for planning  to ensure health and data protection is maintained at the highest levels. This role is the escalation point for Data Protection technical gaps and provides change governance.
Role Functions
Provide SME expertise on security tool capabilities and configuration adjustments, when needed, to implement controls such as: Identify restricted Data, Implement database monitoring, Perform Active Threat Analytics, Database misconfiguration and Database Entitlement reporting as well as during security incidents or block future security attacks
Collaborate with the Manager to identify capability gaps and operational inconsistencies within the Data Protection controls environment and develop a plan to address through product enhancement, reconfiguration, upgrades and/or automation of processes
Build, manage, and maintain the automated reporting dashboards system
Provide on-call support
Collaborate with Customers and other technology teams to ensure operational requests and incidents are resolved within defined SLAs
Leverage tooling to monitor and optimize the performance of the wider data protection operations team
Analyze, troubleshoot, and resolve complex technical Data Protection control issues
Build, manage, and maintain the intake process for requests on service now or a suitable tool.
Examine concepts, work on Proof of Concepts, develop Minimum Viable Product and scale and operationalize the product including documentation and process development
Key Responsibilities:-
Establish a baseline plan and corresponding execution roadmaps.
Refresh and re-publish tactical Plan for advanced data protection controls in June of every year
Report progress against roadmap every month
Refresh Data Protection roadmap/s quarterly
Define plan and roadmap for dashboard automation and service now intake process and report weekly progress against plan
Ensure Technical and Process documentation is 100% current all the time (all changes thoroughly documented)
Ensure new builds/integrations/agent implementation follow operational readiness processes, are fully documented, health/performance KPIs are defined and in place, and monitoring and alerting is in place before promoting to production
Provide change governance
Demonstrate continual progress toward obtaining a security-specific (or specific security product certification)- or maintain a current certification
Complete two career-related training courses per year
Perform monthly training sessions for business teams
Attend lunch and learn sessions to share knowledge with Engineering group
Mentors team members to promote continual growth
Independently/informally leading teams on projects through completion
Publish build documentation for the Relevant Data Protection controls on the EGSO WiKi
Advocate Data Protection Controls across Experian
Maintain objective progress documentation.
Documentation – documentation of business cases, POCs, Procedures, results of POCs, Test cases, integration guides and relevant runbooks to operationalize database activity monitoring and data discovery for Experian.
Qualifications
Demonstrable experience in a similar position.
Strong Database Administration skills.
Strong leadership skills
In-depth knowledge of various operation systems On-Premise: Windows, Linux/Unix , Mainframe and shell scripts Cloud:AWS, Azure, OCI, Google
Experience working with cloud platforms like AWS, Oracle, Azure , Google
Experience with Data activity monitoring tools like IBM Guardium, Imperva
Knowledge of Network technologies
Knowledge of SIEM tools
Preferable
Certified in DAM tools
Experience with Splunk and Exabeam tools
Knowledge and Experience with Data Governance tools like IBM Watson Knowledge catalog, Data360, ASG
Additional Information
Our uniqueness is that we truly celebrate yours.
Experian's culture and people are key differentiators. We take our people agenda very seriously. We focus on what truly matters; DEI, work/life balance, development, authenticity, engagement, collaboration, wellness, reward & recognition, volunteering... the list goes on. We’re an award winning organisation due to our strong people first approach.
Innovation is a critical part of Experian's DNA and practices, and our diverse workforce drives our success. Everyone can succeed at Experian and bring their whole self to work, irrespective of their gender, ethnicity, colour, sexuality, physical ability or age. If you have a disability or special need that requires accommodation, please let us know at the earliest opportunity.
Experian Careers - Creating a better tomorrow together
Find out what its like to work for Experian by clicking here
Show more
Show less","IBM Guardium, AWS, Azure, GCP, Oracle, Exabeam, Splunk, Data Governance tools, SIEM tools, Windows, Linux/Unix, Mainframe, Network Technologies, SQL, Data Management, Security Controls, Data Discovery, Cloud Platforms, Data Protection","ibm guardium, aws, azure, gcp, oracle, exabeam, splunk, data governance tools, siem tools, windows, linuxunix, mainframe, network technologies, sql, data management, security controls, data discovery, cloud platforms, data protection","aws, azure, cloud platforms, data discovery, data governance tools, data management, data protection, exabeam, gcp, ibm guardium, linuxunix, mainframe, network technologies, oracle, security controls, siem tools, splunk, sql, windows"
Data Engineer,HM Revenue & Customs,"Nottingham, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-hm-revenue-customs-3778846352,2023-12-17,Nottingham, United Kingdom,Mid senior,Hybrid,"Job summary
CREST (Compliance and Risk Engineering Solutions Team) sits within Risk and Intelligence Service, and we play a responsible role in projects and programmes which transform our approach to data processing, and its use in risk services.
We will provide the opportunity for you to hit the ground running with data engineering, data processing, data models, data profiling, deep investigations and unlock new data assets.
The Data Engineer will work closely with wider technical delivery partners to deliver Investigation, Detection & Risking System (IDRS) as a live service. You will support the delivery of the service into live and to continue to expand and develop capability post go live.
The role will initially focus on the continuation of designing, building, testing, and promoting to environments of new data products, to ensure elements for which RIS are responsible, are delivered to time, cost, accuracy, and quality.
CREST is responsible for the configuration of the vital components of IDRS, and we will be responsible for maintaining and enhancing these components, actioning service level requests from our end-users, maintaining and resolving incidents as they occur.
See what it’s like to work at HMRC:
find out more about us or ask our colleagues a question. Questions relating to an individual application must be emailed as detailed later in this advert.
Job description
Key Responsibilities
• Design, build, test, promote and maintain complex data management systems, making sure they meet business requirements and user needs.
• Working under agile methodology, work closely with wider build teams, product owners and delivery lead to ensure priorities are met and build products are aligned.
• Deliver services that are automated, reliable, and secure.
• Recommend ways to improve data efficiency and reliability.
• You will report to a dedicated People Manager and Work Manager in RIS Analysis CREST and work closely with external suppliers and stakeholders to ensure optimum service outcomes are determined and delivered.
Person specification
Skills
• Uses agreed standards and tools to design, code, test, correct and document moderate to complex programs and scripts from agreed specifications.
• Understand the concepts and principles of data modelling and use these to design and implement relevant data models across multiple subject areas. Can reverse engineer data models from a live system. Understands industry recognised data modelling patterns and standards and when to apply them. Compares and aligns different data models.
• Design appropriate metadata repositories and present changes to existing metadata repositories. You understand a range of tools for storing and working with metadata and provide advice to team members.
• You can respond to problems in databases, data processes, data products and services as they occur. You can initiate actions, monitor services, and identify trends to solve problems. You can determine the appropriate remedy and assist with implementation of it as well as preventative measures.
• Able to effectively translate and accurately communicate across technical and non-technical stakeholders as well as facilitate discussions within a multidisciplinary team, with potentially difficult dynamics. Able to advocate for the team externally and can manage differing perspectives.
• When required gather business requirements and analyse data to produce actionable insights to range of stakeholders.
• Use appropriate environments for development and testing. Promote code between environments and recognise the significance of environmental alignment.
Essential Criteria:
• Data Development - Experience working in large scale projects, by leveraging tools to design, build, integrate, test, and deploy complex data products.
• Problem Resolution - Effectively respond to performance issues and problems in databases, data processes, data modelling, investigations, analysis, or data quality assurance.
• Communication - Effectively translate and accurately communicate across technical and non-technical stakeholders.
Desirable Criteria:
• Experience of using the SAS tools to deliver data products and solutions is highly desirable but training will be provided.
• Experience of Oracle, SQL developer, Agile understanding, and DevOps CI/CD principles are desirable.
• Understands the impact of emerging trends in data tools, analysis techniques and data usage.
• Utilise strategies to build and maintain ETL (extract, transform and load) processes, including continuous development, module integration, impact analysis, solution configuration and deployment.
Behaviours
We'll assess you against these behaviours during the selection process:
Changing and Improving
Making Effective Decisions
Managing a Quality Service
Technical skills
We'll assess you against these technical skills during the selection process:
Presentation
Show more
Show less","Data engineering, Data processing, Data models, Data profiling, Deep investigations, Data management systems, Agile methodology, Automation, Data efficiency, Data reliability, Metadata repositories, Problem solving, Communication, Business requirements, Data analysis, Development environments, Testing environments, Data products, Performance issues, Data quality assurance, SAS tools, Oracle, SQL developer, Agile, DevOps CI/CD, ETL processes, Continuous development, Module integration, Impact analysis, Solution configuration, Deployment","data engineering, data processing, data models, data profiling, deep investigations, data management systems, agile methodology, automation, data efficiency, data reliability, metadata repositories, problem solving, communication, business requirements, data analysis, development environments, testing environments, data products, performance issues, data quality assurance, sas tools, oracle, sql developer, agile, devops cicd, etl processes, continuous development, module integration, impact analysis, solution configuration, deployment","agile, agile methodology, automation, business requirements, communication, continuous development, data efficiency, data engineering, data management systems, data models, data processing, data products, data profiling, data quality assurance, data reliability, dataanalytics, deep investigations, deployment, development environments, devops cicd, etl, impact analysis, metadata repositories, module integration, oracle, performance issues, problem solving, sas tools, solution configuration, sql developer, testing environments"
Senior Data Engineer,Mindera,"Leicester, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-at-mindera-3767611396,2023-12-17,Nottingham, United Kingdom,Mid senior,Hybrid,"Here at Mindera, we are continuously developing a fantastic team, and would love it for you to join us.
As a Senior Data Engineer, you will be a key member of our data team responsible for designing, building, and maintaining the data infrastructure and pipelines that drive our data-driven decision-making processes. You will collaborate with cross-functional teams to ensure the availability, reliability, and accessibility of our data assets, enabling our organization to extract actionable insights and deliver high-impact solutions.
This is a hybrid role and requires you to be present in our office in Leicester 4 times a month. Additionally, travel to other sites might be required.
Requirements
Object-oriented Programming languages
Strong grasp of and experience with cloud technology, cloud architecture, scalable design patterns, CI/CD, DevSecOps practices and microservices
Proven experience as a data engineer, with a focus on data pipeline development, data warehouse architecture and Microsoft Azure
Experience with big data technologies and distributed computing platforms (e.g., Apache Spark, Hadoop)
Familiarity with cloud-based data platforms (e.g., AWS, Azure, GCP) and containerization (e.g., Docker, Kubernetes)
Advanced working SQL knowledge and experience working with relational databases, as well as familiarity with one or more cloud-based data warehouses such as Snowflake, Redshift, BigQuery
Experience with DBT (data build tool), Azure Data Flow, Azure Active Directory, Azure Key Vaults, Azure Synapse, Airflow, Azure Data Factory
Experience with IaC tools such as Terraform, Cloudformation
Knowledge of data modeling, schema design, and ETL best practices
Excellent problem-solving and analytical skills with the ability to design innovative solutions
Strong communication and collaboration skills to work effectively in a team environment
You Rock At...
Knowledge of Scrum, Kanban, or other agile frameworks is beneficial
Azure certifications (e.g., Azure Solutions Architect, Azure Data Engineer) are a plus
Benefits
Growth opportunities including access to Udemy
Flexible and autonomous working
Competitive salary
Annual bonus
25 days holiday + bank holidays. After 2 years you are entitled to 1 additional day/year up to 30 days
Private Medical Healthcare with Aviva
Enhanced pension scheme (up to 8% matched)
Enhanced Maternity, Paternity, Shared Parental and Adoption leave and pay
Conference Budget
Annual company team-building trip and Christmas party
Cycle2Work
Retail Discounts Scheme
At our office in Leicester we have a wide range of snacks and drinks to keep you fed and healthy
About Mindera
At Mindera we use technology to build products we are proud of, with people we love. Software Engineering Applications, including Web and Mobile, are at the core of what we do at Mindera.
We partner with our clients, to understand their products and deliver high-performance, resilient, and scalable software systems that create an impact on their users and businesses across the world.
You get to work with a bunch of great people, and the whole team owns the project together.
Our culture reflects our lean and self-management attitude. We encourage our colleagues to take risks, make decisions, work in a collaborative way, and talk to everyone to enhance communication.
We are proud of our work and we love to learn all and everything while navigating through an Agile, Lean, and collaborative environment.
Check out our Blog and our Handbook!
Our offices are located: Porto, Portugal | Aveiro, Portugal | Coimbra, Portugal | Leicester, UK | San Diego, USA | San Francisco, USA | Chennai, India | Bengaluru, India | Cluj-Napoca, Romania | Blumenau, Brazil
Show more
Show less","Data engineering, Cloud technology, Cloud architecture, Scalable design patterns, CI/CD, DevSecOps, Microservices, Data pipeline development, Data warehouse architecture, Microsoft Azure, Big data technologies, Distributed computing platforms, Apache Spark, Hadoop, Cloudbased data platforms, AWS, Azure, GCP, Containerization, Docker, Kubernetes, SQL, Relational databases, Snowflake, Redshift, BigQuery, DBT (data build tool), Azure Data Flow, Azure Active Directory, Azure Key Vaults, Azure Synapse, Airflow, Azure Data Factory, IaC tools, Terraform, Cloudformation, Data modeling, Schema design, ETL, Scrum, Kanban, Agile frameworks","data engineering, cloud technology, cloud architecture, scalable design patterns, cicd, devsecops, microservices, data pipeline development, data warehouse architecture, microsoft azure, big data technologies, distributed computing platforms, apache spark, hadoop, cloudbased data platforms, aws, azure, gcp, containerization, docker, kubernetes, sql, relational databases, snowflake, redshift, bigquery, dbt data build tool, azure data flow, azure active directory, azure key vaults, azure synapse, airflow, azure data factory, iac tools, terraform, cloudformation, data modeling, schema design, etl, scrum, kanban, agile frameworks","agile frameworks, airflow, apache spark, aws, azure, azure active directory, azure data factory, azure data flow, azure key vaults, azure synapse, big data technologies, bigquery, cicd, cloud architecture, cloud technology, cloudbased data platforms, cloudformation, containerization, data engineering, data pipeline development, data warehouse architecture, datamodeling, dbt data build tool, devsecops, distributed computing platforms, docker, etl, gcp, hadoop, iac tools, kanban, kubernetes, microservices, microsoft azure, redshift, relational databases, scalable design patterns, schema design, scrum, snowflake, sql, terraform"
Data Engineer,BJSS,"Nottingham, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-bjss-2742688287,2023-12-17,Nottingham, United Kingdom,Mid senior,Hybrid,"About Us
We’re an award-winning innovative tech consultancy - a team of creative problem solvers. Since 1993 we’ve been finding better, more sustainable ways to solve complex technology problems for some of the world’s leading organisations and delivered solutions that millions of people use every day.
In the last 30 years we won several awards, including a prestigious Queen’s Award for Enterprise in the Innovation category for our Enterprise Agile delivery approach.
Operating from 26 locations across the world, we bring together teams of creative experts with diverse backgrounds and experiences, who enjoy working and learning in our collaborative and open culture and are committed to world-class delivery.
We want to continue to grow our team with people just like you!
We are DataOps advocates and use software engineering best practices to build scalable and re-usable data solutions to help clients use their data to gain insights, drive decisions and deliver business value. Clients don’t engage BJSS to do the straightforward things, they ask us to help on their biggest challenges which means we get to work with a wide range of tools and technologies and there are always new things to learn.
About the Role
BJSS data engineers are specialist software engineers that build, optimise and maintain data applications, systems and services. This role combines the discipline of software engineering with the knowledge and experience of building data solutions in order to deliver business value.
As a BJSS data engineer you’ll help our clients deploy data pipelines and processes in a production-safe manner, using the latest technologies and with a DataOps culture.
You’ll work in a fast moving, agile environment, within multi-disciplinary teams of highly skilled consultants, delivering modern data platforms into large organisations.
You can expect to get involved in variety of projects in the cloud (AWS, Azure, GCP), learning about and using data services such as Databricks, Data Factory, Synapse, Kafka, Redshift, Glue, Athena, BigQuery, S3, Cloud Data Fusion etc.
About You
You're an engineer at heart and enjoy the challenge of building reliable, efficient data applications systems, services and platforms.
You have a good understanding of coding best practices and design patterns and experience with code and data versioning, dependency management, code quality and optimisation, error handling, logging, monitoring, validation and alerting.
You have experience in writing well tested object-oriented Python.
You have experience with using CI/CD tooling to analyse, build, test and deploy your code.
You have a good understanding of design choices for data storage and data processing, with a particular focus on cloud data services.
You have experience in using parallel computing to process large datasets and to optimise computationally intensive tasks.
You have experience in programmatically deploying, scheduling and monitoring components in a workflow.
You have experience in writing complex queries against relational and non-relational data stores.
Some of the Perks
Flexible benefits allowance – you choose how to spend your allowance (additional pension contributions, healthcare, dental and more)
Industry leading health and wellbeing plan - we partner with several wellbeing support functions to cater to each individual's need, including 24/7 GP services, mental health support, and other
Life Assurance (4 x annual salary)
25 days annual leave plus bank holidays
Hybrid working - Our roles are not fully remote as we take pride in the tight knit communities we have created at our local offices. But we offer plenty of flexibility and you can split your time between the office, client site and WFH
Discounts – we have preferred rates from dozens of retail, lifestyle, and utility brands
An industry-leading referral scheme with no limits on the number of referrals
Flexible holiday buy/sell option
Electric vehicle scheme
Training opportunities and incentives – we support professional certifications across engineering and non-engineering roles, including unlimited access to O’Reilly
Giving back – the ability to get involved nationally and regionally with partnerships to get people from diverse backgrounds into tech
You will become part of a squad with people from different areas within the business who will help you grow at BJSS
We have a busy social calendar that you can chose to join– quarterly town halls/squad nights out/weekends away with families included/office get togethers
GymFlex gym membership programme
Show more
Show less","Software engineering, Data engineering, DataOps, Python, Objectoriented programming, CI/CD, Cloud data services, Parallel computing, SQL, NoSQL, AWS, Azure, GCP, Data Factory, Synapse, Kafka, Redshift, Glue, Athena, BigQuery, S3, Cloud Data Fusion, Databricks","software engineering, data engineering, dataops, python, objectoriented programming, cicd, cloud data services, parallel computing, sql, nosql, aws, azure, gcp, data factory, synapse, kafka, redshift, glue, athena, bigquery, s3, cloud data fusion, databricks","athena, aws, azure, bigquery, cicd, cloud data fusion, cloud data services, data engineering, data factory, databricks, dataops, gcp, glue, kafka, nosql, objectoriented programming, parallel computing, python, redshift, s3, software engineering, sql, synapse"
Senior Data Engineer - up to GBP75k,Nigel Frank International,"Nottingham, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-up-to-gbp75k-at-nigel-frank-international-3728584221,2023-12-17,Nottingham, United Kingdom,Mid senior,Hybrid,"Are you a Data Engineer with a passion for Databricks?
My client is a leader in the legal industry and they are looking to grow their data team. They offer an amazing opportunity for a Senior Data Engineer, with heavy focus on Databricks. Within this role, you will join an established, small team of data engineers and you will be working alongside the reporting team too.
You will be developing pipelines with Databricks on premises and you will be using Azure Data Factory for orchestration too. Also, you will be supporting internal business apps as they are moving into new bespoke applications which relies on Databricks.
There is great room for progression within this role, and support and training will be provided as well.
Experience required:
Very strong experience with Databricks and SQL
Strong experience with Python / PySpark
Azure Data Factory
Excellent communication skills
This is a permanent position and pays between £60,000 and £70,000, depending on experience and the strength of your skillset.
Benefits
Well-being fund
Hybrid working (1-2x/month in London)
Great pension package
Private medical health insurance
Support and training provided
If this sounds like you, then apply NOW as my client is hiring ASAP and you don't want to miss this opportunity!
To discuss in more detail, please send your CV s.fokas@nigelfrank.com or alternatively, call Spiros Fokas on 191 243 5445
NOTE: This role is for UK Residents only and does not offer sponsorship.
Nigel Frank International is the leading Microsoft Business Intelligence recruitment firm in the UK, advertising more Business Intelligence jobs than any other agency. We deal with both Microsoft Partners & End Users throughout the UK and Europe and we have never had more live requirements jobs for Microsoft Business Intelligence professionals. By specialising solely in placing candidates in the market I have built relationships with key employers in the UK and have an unrivalled understanding of where the best opportunities & Business Intelligence jobs are.
Show more
Show less","Databricks, SQL, Python, PySpark, Azure Data Factory","databricks, sql, python, pyspark, azure data factory","azure data factory, databricks, python, spark, sql"
Senior Data Engineer - up to GBP75k,Nigel Frank International,"Leicester, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-up-to-gbp75k-at-nigel-frank-international-3728581671,2023-12-17,Nottingham, United Kingdom,Mid senior,Hybrid,"Are you a Data Engineer with a passion for Databricks?
My client is a leader in the legal industry and they are looking to grow their data team. They offer an amazing opportunity for a Senior Data Engineer, with heavy focus on Databricks. Within this role, you will join an established, small team of data engineers and you will be working alongside the reporting team too.
You will be developing pipelines with Databricks on premises and you will be using Azure Data Factory for orchestration too. Also, you will be supporting internal business apps as they are moving into new bespoke applications which relies on Databricks.
There is great room for progression within this role, and support and training will be provided as well.
Experience required:
Very strong experience with Databricks and SQL
Strong experience with Python / PySpark
Azure Data Factory
Excellent communication skills
This is a permanent position and pays between £60,000 and £70,000, depending on experience and the strength of your skillset.
Benefits
Well-being fund
Hybrid working (1-2x/month in London)
Great pension package
Private medical health insurance
Support and training provided
If this sounds like you, then apply NOW as my client is hiring ASAP and you don't want to miss this opportunity!
To discuss in more detail, please send your CV s.fokas@nigelfrank.com or alternatively, call Spiros Fokas on 191 243 5445
NOTE: This role is for UK Residents only and does not offer sponsorship.
Nigel Frank International is the leading Microsoft Business Intelligence recruitment firm in the UK, advertising more Business Intelligence jobs than any other agency. We deal with both Microsoft Partners & End Users throughout the UK and Europe and we have never had more live requirements jobs for Microsoft Business Intelligence professionals. By specialising solely in placing candidates in the market I have built relationships with key employers in the UK and have an unrivalled understanding of where the best opportunities & Business Intelligence jobs are.
Show more
Show less","Databricks, SQL, Python, PySpark, Azure Data Factory","databricks, sql, python, pyspark, azure data factory","azure data factory, databricks, python, spark, sql"
Business & Data Analyst,Gleeson Recruitment Group,"Leicestershire, England, United Kingdom",https://uk.linkedin.com/jobs/view/business-data-analyst-at-gleeson-recruitment-group-3776702832,2023-12-17,Nottingham, United Kingdom,Mid senior,Hybrid,"Do you have a passion for analysing data? Are you looking for a role where you can expand your commercial understanding? Have you got experience within a data analyst or business analyst role and looking to take the next step in your career? If so, please read on!
GRG are supporting an extraordinary organisation that has a clear vision for positive impact within the financial services sector. You will join a rapidly expanding organisation as it is now poised for further expansion.
As a Business & Data Analyst, you will play a pivotal role in supporting my client's Group Chairman by extracting and interpreting client data from potential acquisition targets. This is a high-profile, stand-alone role offering a unique opportunity to contribute to the growth and success of this rapidly expanding organisation. In this full-time, permanent role, you will be required to work regularly from their Leicestershire offices, on a flexible working basis between the hours of 9am-5pm.
We are seeking a talented and experienced Data and Finance Analyst to join our client's expanding dynamic team in Leicestershire. The successful candidate will play a crucial role in analysing financial data, providing valuable insights, and contributing to strategic decision-making. If you have a proven track record in financial services, wealth management, asset & fund management, and possess strong analytical skills, this could be an incredible opportunity to progress your career within an accelerating organisation.
Key Responsibilities:
Conduct in-depth analysis of financial data to identify trends, patterns, and key insights.
Utilise Excel and Zoho (CRM) to manage and analyse data from potential acquisition targets.
Work closely with the Board, Group MD, Finance Director, and Project Manager to assess progress on current acquisitions.
Take ownership of maintaining dashboards and ensuring accurate reporting.
Interact with auditors and demonstrate a strong business acumen.
Provide proactive insights and contribute to business improvements and acquisitions, supporting forecasting and decision-making processes.
Generate regular and ad-hoc financial reports for internal stakeholders.
Develop visually compelling dashboards to communicate financial information effectively.
Assist in the development of budgets and financial forecasts.
Collaborate with cross-functional teams to ensure alignment with organisational goals.
Identify and assess financial risks, proposing mitigation strategies as needed.
Monitor and report on key risk indicators, ensuring compliance with industry standards.
Apply expertise in financial services, wealth management, asset management, or fund management to analyse industry trends and market dynamics.
Experience Required:
Degree in Finance, Accounting, Economics, or a related field.
Proven experience working in financial services, wealth management, asset management, or fund management.
Strong analytical and quantitative skills, with proficiency in data analysis tools (e.g., Excel, CRM Systems).
Experience in building and maintaining financial models.
Strong attention to detail and ability to work independently.
Excellent communication skills, with the ability to convey complex financial information to diverse audiences, both written and verbally.
Knowledge of financial regulations and reporting requirements.
Benefits:
Pension scheme (5% contribution).
Private medical scheme.
25 days of annual leave plus additional options to buy or sell days.
Plus much more!
If you are a motivated and results-driven individual with a passion for data analysis and finance, we would love to hear from you!
By applying you will be registered as a candidate with Gleeson Recruitment Limited. Our Privacy Policy is available on our website and explains how we will use your data.
Show more
Show less","Data analysis, Financial analysis, Excel, Zoho (CRM), Dashboards, Reporting, Financial reporting, Financial modeling, Risk assessment, Risk management, Financial regulations, Financial services, Wealth management, Asset management, Fund management, Quantitative analysis, Analytical skills, Communication skills, Attention to detail, Independence, Problemsolving, Decisionmaking, Forecasting, Budgeting","data analysis, financial analysis, excel, zoho crm, dashboards, reporting, financial reporting, financial modeling, risk assessment, risk management, financial regulations, financial services, wealth management, asset management, fund management, quantitative analysis, analytical skills, communication skills, attention to detail, independence, problemsolving, decisionmaking, forecasting, budgeting","analytical skills, asset management, attention to detail, budgeting, communication skills, dashboard, dataanalytics, decisionmaking, excel, financial analysis, financial modeling, financial regulations, financial reporting, financial services, forecasting, fund management, independence, problemsolving, quantitative analysis, reporting, risk assessment, risk management, wealth management, zoho crm"
Data Analyst Part Time,Toyandsons,"Sherbrooke, Quebec, Canada",https://ca.linkedin.com/jobs/view/data-analyst-part-time-at-toyandsons-3757201990,2023-12-17,Magog, Canada,Mid senior,Onsite,"Summary:
Data Analyst Part Time will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : hiring@jobsai.live
Powered by Webbtree
Show more
Show less","Data Analysis, Data Interpretation, Machine Learning, Statistical Techniques, SQL, R, Python, Data Visualization, Tableau, Power BI, Statistical Modeling, Hypothesis Testing, A/B Testing, Data Management, ETL Processes","data analysis, data interpretation, machine learning, statistical techniques, sql, r, python, data visualization, tableau, power bi, statistical modeling, hypothesis testing, ab testing, data management, etl processes","ab testing, data interpretation, data management, dataanalytics, etl, hypothesis testing, machine learning, powerbi, python, r, sql, statistical modeling, statistical techniques, tableau, visualization"
Customer Service Representative/Data Analyst/Data Entry Clerk/Full Time,Rodtookjing,"Sherbrooke, Quebec, Canada",https://ca.linkedin.com/jobs/view/customer-service-representative-data-analyst-data-entry-clerk-full-time-at-rodtookjing-3741435685,2023-12-17,Magog, Canada,Mid senior,Onsite,"Summary:
Customer Service Representative/Data Analyst/Data Entry Clerk/Full Time will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : hiring@jobsolutionsai.online
Powered by Webbtree
Show more
Show less","Data Analysis, Data Interpretation, Advanced Statistical Techniques, DataDriven DecisionMaking, Data Visualization, Data Integrity, Data Collection, Data Cleansing, Data Manipulation, SQL, R, Python, Tableau, Power BI, Statistical Modeling, Hypothesis Testing, A/B Testing, ETL Processes","data analysis, data interpretation, advanced statistical techniques, datadriven decisionmaking, data visualization, data integrity, data collection, data cleansing, data manipulation, sql, r, python, tableau, power bi, statistical modeling, hypothesis testing, ab testing, etl processes","ab testing, advanced statistical techniques, data collection, data integrity, data interpretation, data manipulation, dataanalytics, datacleaning, datadriven decisionmaking, etl, hypothesis testing, powerbi, python, r, sql, statistical modeling, tableau, visualization"
Customer Service Representative/Data Analyst/Data Entry Clerk Part Time,Bluebeacontruckwash,"Magog, Quebec, Canada",https://ca.linkedin.com/jobs/view/customer-service-representative-data-analyst-data-entry-clerk-part-time-at-bluebeacontruckwash-3756472407,2023-12-17,Magog, Canada,Mid senior,Onsite,"Summary:
The Data Analyst will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : hiring@jobsai.live
Powered by Webbtree
Show more
Show less","Data Analysis, Statistics, SQL, R, Python, Data Visualization, Tableau, Power BI, Hypothesis Testing, A/B Testing, Data Quality Control, Data Management, ETL","data analysis, statistics, sql, r, python, data visualization, tableau, power bi, hypothesis testing, ab testing, data quality control, data management, etl","ab testing, data management, data quality control, dataanalytics, etl, hypothesis testing, powerbi, python, r, sql, statistics, tableau, visualization"
Customer Service Representative/Data Analyst/Data Entry Clerk/Part Time,Toyandsons,"Sherbrooke, Quebec, Canada",https://ca.linkedin.com/jobs/view/customer-service-representative-data-analyst-data-entry-clerk-part-time-at-toyandsons-3741439547,2023-12-17,Magog, Canada,Mid senior,Onsite,"Summary:
Customer Service Representative/Data Analyst/Data Entry Clerk/Part Time will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : hiring@jobsolutionsai.online
Powered by Webbtree
Show more
Show less","Data analysis, Data mining, Statistical modeling, Machine learning, Data visualization, A/B testing, SQL, Python, R, Tableau, Power BI","data analysis, data mining, statistical modeling, machine learning, data visualization, ab testing, sql, python, r, tableau, power bi","ab testing, data mining, dataanalytics, machine learning, powerbi, python, r, sql, statistical modeling, tableau, visualization"
Data Business Analyst,BRP,"Sherbrooke, Quebec, Canada",https://ca.linkedin.com/jobs/view/data-business-analyst-at-brp-3761308959,2023-12-17,Magog, Canada,Mid senior,Hybrid,"As a Business Data Analyst for the Fulfillment data domain at BRP, you will support the transformation of the company by delivering reporting and insights visibility to the Custom, Procurement and Logistics teams and to the company leadership.
In this high-visibility role, you will work with business stakeholders, business leaders and data consumers to identify reporting and analytical requirements, independently developing business and technical requirement documentation while also leading in developing reports, dashboards and analytics to meet these requirements.
If you love wrangling data to deliver insights, digging into processes and becoming a lead on the data domain you support, all while exercising creative freedom to shape a growing next gen people data platform in a fast-paced environment, you will have a blast with us.
You'll Have The Opportunity To
Collaborate with a cross-functional team (data architects, data engineers, BI developers, etc.) that is 100% focused on the Fulfillment priorities.
Lead and participate in business-facing meetings to gather requirements and make recommendations around reporting/analytics solutions.
Play a key role in the daily support of end-users’ ability to access data and/or effectively utilize the data platform.
Prioritize work with the team in an agile methodology, ensuring there is a forward-looking roadmap for features to be delivered for the data products of the Customer data domain.
Participate or lead activities that ensure redundancies for the function (documentation, automation, etc.).
Develop a deep understanding of the various Custom, planning, procurement and logistic (Transport Management and Warehouse Management processes, data sets and data deliverables.
YOU’LL THRIVE IN THIS ROLE IF YOU HAVE THE FOLLOWING SKILLS AND QUALITIES:
Bachelor’s degree or equivalent experience in Computer Science, Business Management or Business Analysis.
3 to 5 years of experience in a similar role.
Experience in custom, planning, procurement and logistic fields and related processes.
Working knowledge of BI and data warehouse concepts & terminology (Snowflake environment is an asset).
Working knowledge of SAP and his ecosystem, such as IBP is an asset.
Experience building or using reports and dashboards in PowerBI (experience in Tableau, Qlik, BOBJ or similar platforms is an asset).
Proven experience with business requirements documentation, analysis, validation, and methodology development.
Self-motivated and able to work independently.
Experience in agile methodology and backlog management is an asset.
Capable of prioritizing multiple projects while still achieving deadlines.
Excellent business acumen, with an ability to connect the dots and tell stories using data.
Detail-oriented person who is passionate about quality and enthusiastic about new technologies.
ACKNOWLEDGING THE POWER OF DIVERSITY
BRP is dedicated to nurturing a culture that invites, connects, and propels the ambitions of people of all backgrounds, profiles, beliefs and experiences. Ultimately, the diversity and uniqueness of our people fuel our ingenuity and set the course for the path ahead!
For this reason, we value diversity and we strive to always push each other forward to build an inclusive workplace where every employee feels like they belong, where they can grow and find meaning.
AT BRP, WHEN WE TALK ABOUT BENEFITS, WE GO ALL IN.
Let’s start with a strong foundation — You want it, we have it:
Annual bonus: we work hard, and we know it. This yearly thank you is our way to fuel your dreams.
Generous paid time away: winding down is as important as creating the future of recreational products.
Pension plan: we want to make sure you’ll enjoy retirement; we’ve got you covered.
Collective saving opportunities: being part of BRP allows you to tap into the strength of our saving programs.
Industry leading healthcare: fully paid by BRP, this program is here for you, whatever happens, so you can focus on what matters.
What About Some Feel Good Extras
Flexible work schedule: balance is key to innovation, and we want to make sure you are at the top of your game with a schedule that will work for you.
Summer schedule: time flies at BRP, so we hit pause to enjoy it.
Holiday season shutdown: between Christmas and New Year’s, we close our facilities and refill on good times.
Educational resources: you love what you do, and we get you resources to stay cutting-edge, so we all win together.
Discount on BRP products: what we build is amazing, freeing, revolutionary, and you're first in line for the experience.
WELCOME TO BRP
We’re a world leader in recreational vehicles and boats, creating innovative ways to move on snow, water, asphalt, dirt and even in the air. Headquartered in the Canadian town of Valcourt, Quebec, our company is rooted in a spirit of ingenuity and intense customer focus. Today, we operate manufacturing facilities in Canada, the United States, Mexico, Finland, Australia and Austria, with a workforce made up of close to 23,000 spirited people, all driven by the deeply held belief that at work, as with life itself, it’s not about the destination; It’s about the journey.
Show more
Show less","Business Data Analyst, Data Architect, Data Engineer, BI Developer, Business Requirements Documentation, Analysis, Validation, Methodology Development, Agile Methodology, Backlog Management, Business Intelligence (BI), Data Warehouse, Snowflake, SAP, IBP, Power BI, Tableau, Qlik, BOBJ, Data Visualization, Dashboarding, Reporting, Data Wrangling, Data Platform, Data Products, Processes, Data Sets, Data Deliverables, Procurement, Logistics, Custom Planning, SAP Ecosystem, Computer Science, Business Management, Business Analysis","business data analyst, data architect, data engineer, bi developer, business requirements documentation, analysis, validation, methodology development, agile methodology, backlog management, business intelligence bi, data warehouse, snowflake, sap, ibp, power bi, tableau, qlik, bobj, data visualization, dashboarding, reporting, data wrangling, data platform, data products, processes, data sets, data deliverables, procurement, logistics, custom planning, sap ecosystem, computer science, business management, business analysis","agile methodology, analysis, backlog management, bi developer, bobj, business analysis, business data analyst, business intelligence bi, business management, business requirements documentation, computer science, custom planning, dashboard, data architect, data deliverables, data platform, data products, data sets, data wrangling, dataengineering, datawarehouse, ibp, logistics, methodology development, powerbi, processes, procurement, qlik, reporting, sap, sap ecosystem, snowflake, tableau, validation, visualization"
Data Engineer - Scala(U.S. remote),Railroad19,"Cape Coral, FL",https://www.linkedin.com/jobs/view/data-engineer-scala-u-s-remote-at-railroad19-3783322682,2023-12-17,Cape May,United States,Mid senior,Remote,"Railroad19, Inc.
is hiring
remote
Senior Data Engineers
to be a solid technical resource on a dynamic and growing team of accomplished engineers.
Our ideal candidate is passionate about creating well-architected solutions containing thoroughly tested code. The ability to communicate effectively and develop relationships by empathizing with client goals is a highly valued skill within our company culture.
Core Responsibilities:
Develop new and enhance existing application services
Writing tests to maintain code quality
Understand and adapt to our client's evolving business requirements within the television advertising domain.
Participate in detailed technical design sessions to understand client needs and provide productive feedback
Identify new opportunities, tools, and services to enhance the software platform
Support and troubleshoot issues, identify the root cause, and proactively recommend corrective actions
Skills & Experience:
Scala 2.12 + development experience
Passionate about developing clean and maintainable code with little or no side-effects
Experience building Restful APIs in Scala using Spark 2.4
Strong hands-on experience with AWS running spark jobs on ephemeral EMR clusters on AWS and S3.
Experience with relational and non-relational databases
Willingness to learn new technologies and takes pride in keeping up with the latest technologies and practices within the Scala and Spark development community
Excellent oral and written communication skills
Strong analytical and problem-solving skills
Self-directed and can effectively deliver solutions with little oversight
A bachelor's or master's degree in computer science, computer engineering, or other technical disciplines or equivalent work experience is preferred but not required.
$120,000 - $160,000 a year
Salary is commensurate with experience.
We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender, gender identity or expression, or veteran status. We are proud to be an equal-opportunity workplace.
#Hiringnow
We are actively hiring (Data Engineers)
#remote #Scala #DataEngineering #ApacheSpark
Show more
Show less","Scala 2.12, Spark 2.4, AWS, EMR, S3, Restful APIs, Relational databases, Nonrelational databases, Computer science, Computer engineering, Data engineering, Apache Spark","scala 212, spark 24, aws, emr, s3, restful apis, relational databases, nonrelational databases, computer science, computer engineering, data engineering, apache spark","apache spark, aws, computer engineering, computer science, data engineering, emr, nonrelational databases, relational databases, restful apis, s3, scala 212, spark 24"
"Sr. Engineer, Database Infrastructure - Slack",Slack,Greater Wilmington Area,https://www.linkedin.com/jobs/view/sr-engineer-database-infrastructure-slack-at-slack-3760630808,2023-12-17,Wilmington,United States,Mid senior,Remote,"To get the best candidate experience, please consider applying for a maximum of 3 roles within 12 months to ensure you are not duplicating efforts.
Job Category
Software Engineering
Job Details
About Salesforce
We’re Salesforce, the Customer Company, inspiring the future of business with AI+ Data +CRM. Leading with our core values, we help companies across every industry blaze new trails and connect with customers in a whole new way. And, we empower you to be a Trailblazer, too — driving your performance and career growth, charting new paths, and improving the state of the world. If you believe in business as the greatest platform for change and in companies doing well and doing good – you’ve come to the right place.
Slack's DataStores team builds and operates the database platform powering Slack. We write software to manage thousands of stateful hosts, providing several petabytes of online database capacity. We are building one of the fastest-growing database platforms in the world. Our MySQL databases run in Vitess. You can read more about our migration to Vitess at: Scaling Datastores at Slack with Vitess
Background
Slack enables people all over the world to communicate and collaborate together. Teams of all scales — from the world’s largest public companies to the smallest of startups — use Slack to get work done, so we take performance and reliability very seriously. A taste of our scale:
The average user spends over 10 hours connected and 2.5 hours active in Slack every single day
1.5 billion+ messages are sent per month, half of those outside the United States
Every day we see over 10M+ daily active users, over a billion web requests, and 10s of billions of database queries.
For millions of people, Slack is the primary communication tool they use at work all day long. They expect it to be exceptionally reliable and fast, all the time.
Infrastructure at Slack
We operate at tremendous scale with systems that process millions of events per second. Teams in our group maintain and build the lower levels of our stack, including:
Edge services
Data Stores and Caches
Real-time messaging
Asynchronous background job processing
We know we’ve done our job correctly when
none of our users think about us.
We don’t typically ship new user-facing features, but rather ensure our systems are incredibly performant, highly available, reliable, and scalable. In other words, Slack just works seamlessly.
Slack's API and web backend is built on PHP/Hack, our backend services are written in Java and Go, and we use Vitess as our storage engine. Our architecture is constantly evolving to handle millions more users. You can read about how we scaled our datastores with Vitess, how we respond to incidents, and much more on our blog.
What you will be doing:
Operate and enhance our large, highly-available database infrastructure, utilizing technologies such as MySQL and Vitess.
Develop tools to enable self-service and self-managing capabilities of our database infrastructure so that other teams can operate full-stack while rapidly building new features for our customers.
Collaborate with engineering teams on their database storage needs, and advise them throughout the development lifecycle.
Write code to capture database performance, and create tools and dashboards to provide actionable insight into that data.
Participate in our on-call rotation and collaborate with our operations team to triage and resolve production issues.
Support FedRAMP and DoD SRG activities.
What you should have:
Have been working in data storage, core infrastructure, or distributed system-owning teams with increasing responsibilities for 5+ years.
Have professional experience using Go, PHP/Hacklang, Python, Ruby, or Java.
Write code that can be easily understood by others with an eye towards clarity and maintainability. Collaborate with other teams to integrate new features of your platform or adopt self-service features.
Operated at least one distributed system, at scale and in a team environment. Some examples include: a relational database like MySQL/Postgres, or systems like Kafka, Cassandra, or ElasticSearch.
Deployed server software on Linux, and then operated it at scale. You’ve debugged its problems, and analyzed and optimized its performance.
Have experience operating cloud infrastructure, especially AWS.
Are familiar with deployment automation/configuration management tools like Chef, Ansible, Puppet, or Terraform.
Are a very strong communicator. You’re excited to explain complex technical concepts and share your knowledge with different audiences.
Are curious how things work; when they break you are eager and able to help fix them.
Infrastructure is a diverse and inclusive team that treats their colleagues exceptionally well. We are happy to help you learn what you need to know; we encourage and support each other’s growth and thus it’s not expected that you would have expertise across all of these areas.
Come join us!
Accommodations
If you require assistance due to a disability applying for open positions please submit a request via this Accommodations Request Form.
Posting Statement
At Salesforce we believe that the business of business is to improve the state of our world. Each of us has a responsibility to drive Equality in our communities and workplaces. We are committed to creating a workforce that reflects society through inclusive programs and initiatives such as equal pay, employee resource groups, inclusive benefits, and more. Learn more about Equality at www.equality.com and explore our company benefits at www.salesforcebenefits.com .
Salesforce is an Equal Employment Opportunity and Affirmative Action Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender perception or identity, national origin, age, marital status, protected veteran status, or disability status. Salesforce does not accept unsolicited headhunter and agency resumes. Salesforce will not pay any third-party agency or company that does not have a signed agreement with Salesforce .
﻿Salesforce welcomes all.
For Colorado-based roles, the base salary hiring range for this position is $156,800 to $215,600.
Compensation offered will be determined by factors such as location, level, job-related knowledge, skills, and experience. Certain roles may be eligible for incentive compensation, equity, benefits. More details about our company benefits can be found at the following link: https://www.salesforcebenefits.com.
Show more
Show less","MySQL, Vitess, Go, PHP, Hack, Python, Ruby, Java, Linux, AWS, Chef, Ansible, Puppet, Terraform","mysql, vitess, go, php, hack, python, ruby, java, linux, aws, chef, ansible, puppet, terraform","ansible, aws, chef, go, hack, java, linux, mysql, php, puppet, python, ruby, terraform, vitess"
Customer Service Representative/Data Analyst/Data Entry Clerk Full Time,Bluebeacontruckwash,"Summerside, Prince Edward Island, Canada",https://ca.linkedin.com/jobs/view/customer-service-representative-data-analyst-data-entry-clerk-full-time-at-bluebeacontruckwash-3751468570,2023-12-17,Prince Edward Island, Canada,Mid senior,Onsite,"Summary:
The Data Analyst will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : hiring@jobsai.live
Powered by Webbtree
Show more
Show less","SQL, R, Python, Tableau, Power BI, A/B testing, Data cleaning, Data visualization, Data management, ETL, Statistical modeling, Hypothesis testing","sql, r, python, tableau, power bi, ab testing, data cleaning, data visualization, data management, etl, statistical modeling, hypothesis testing","ab testing, data cleaning, data management, etl, hypothesis testing, powerbi, python, r, sql, statistical modeling, tableau, visualization"
Data Engineer - Scala(U.S. remote),Railroad19,"Lakeland, FL",https://www.linkedin.com/jobs/view/data-engineer-scala-u-s-remote-at-railroad19-3783325344,2023-12-17,Fairmont,United States,Mid senior,Remote,"Railroad19, Inc.
is hiring
remote
Senior Data Engineers
to be a solid technical resource on a dynamic and growing team of accomplished engineers.
Our ideal candidate is passionate about creating well-architected solutions containing thoroughly tested code. The ability to communicate effectively and develop relationships by empathizing with client goals is a highly valued skill within our company culture.
Core Responsibilities:
Develop new and enhance existing application services
Writing tests to maintain code quality
Understand and adapt to our client's evolving business requirements within the television advertising domain.
Participate in detailed technical design sessions to understand client needs and provide productive feedback
Identify new opportunities, tools, and services to enhance the software platform
Support and troubleshoot issues, identify the root cause, and proactively recommend corrective actions
Skills & Experience:
Scala 2.12 + development experience
Passionate about developing clean and maintainable code with little or no side-effects
Experience building Restful APIs in Scala using Spark 2.4
Strong hands-on experience with AWS running spark jobs on ephemeral EMR clusters on AWS and S3.
Experience with relational and non-relational databases
Willingness to learn new technologies and takes pride in keeping up with the latest technologies and practices within the Scala and Spark development community
Excellent oral and written communication skills
Strong analytical and problem-solving skills
Self-directed and can effectively deliver solutions with little oversight
A bachelor's or master's degree in computer science, computer engineering, or other technical disciplines or equivalent work experience is preferred but not required.
$120,000 - $160,000 a year
Salary is commensurate with experience.
We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender, gender identity or expression, or veteran status. We are proud to be an equal-opportunity workplace.
#Hiringnow
We are actively hiring (Data Engineers)
#remote #Scala #DataEngineering #ApacheSpark
Show more
Show less","Scala, Spark, AWS, EMR, Relational databases, Nonrelational databases, Restful APIs","scala, spark, aws, emr, relational databases, nonrelational databases, restful apis","aws, emr, nonrelational databases, relational databases, restful apis, scala, spark"
Cancer Data Analyst,Steneral Consulting,"Indianapolis, IN",https://www.linkedin.com/jobs/view/cancer-data-analyst-at-steneral-consulting-3783924055,2023-12-17,Carmel,United States,Associate,Onsite,"Position Summary
The Cancer Section Data Analyst works under the guidance of the Cancer Epidemiologist within the Indiana Department of Health Chronic Disease Division. This position is charged with identifying, analyzing, and compiling data and resources regarding cancer. Surveillance activities will include monitoring surveillance data for population trends in incidence, mortality, screening, survivorship, and health care information. All work is to be completed in compliance with state and federal guidelines, budget limitations, and agency policies.
The position is a full-time, contract position of a defined duration.
Duties and Responsibilities
Using epidemiologic strategies, identify primary and secondary data sources; interpret and analyze data using statistical techniques; and provide ongoing oral and written reports.
Utilize epidemiologic expertise to identify disparities, data sources, communities and priority populations, measure performance, and provide guidance for prioritizing Program activities.
Transforms, maps, cleans, and prepares data to support program needs.
Work collaboratively with others across programs and agencies to expand Program reach, reduce the duplication of efforts, and to identify opportunities for leveraging data to drive program strategies and development.
Identify opportunities for developing and implementing data tools including data collection and tracking systems and provide technical assistance to internal and external stakeholders. Respond to data requests in an appropriate and timely manner, utilizing agency protocols.
Recognize and define new process improvement opportunities for maximizing programmatic effectiveness and efficiency.
Meaningfully engage in programmatic decision-making and planning, including in the development and/or administration of grants at the national, state, and local level.
Develop business cases and provide leadership in conducting economic evaluations.
Prepare and present communication tools to a variety of stakeholders in written and oral formats.
Supports documentation and organization of the Cancer Inquiries committee. Provide support for other cancer and chronic disease committees which relate to the reduction of cancer mortality.
Participate in cancer team related activities and grants making activities through data analysis and report development.
Other duties as assigned.
Qualifications And Skills Needed For Success
Bachelor’s degree in public health with a focus on data analysis or epidemiology is required.
Strong working knowledge of epidemiological methods, study design, and analysis needed with an understanding of qualitative and quantitative data collection methods and analysis.
Experience with extensive data sets such as registries, hospitalization, vital records, and Behavioral Risk Factor Surveillance System (BRFSS) preferred.
Proficient knowledge of computer software (SAS, Excel, Tableau, ArcGIS etc.) to manipulate and draw insights from large data sets is preferred.
Skilled in data collection and analysis and must be able to assimilate and interpret data and clearly communicate information to a variety of stakeholders.
Familiarity with organizational structure, administrative processes, and quality improvement initiatives preferred.
Requires excellent written and verbal communication skills, and the ability to work effectively with other individuals and diverse groups and teams.
Strong problem-solving skills, and the ability to effectively prioritize a variety of projects and functions.
Effective time management and program management skills with the ability to work independently, anticipate problems, and initiate corrective actions.
Ability to research and document findings.
Actively engage as a member of a team.
Show more
Show less","Data analysis, Epidemiology, Statistical techniques, Data transformation, Data cleaning, SAS, Excel, Tableau, ArcGIS, Data collection, Data visualization, Communication, Problem solving, Prioritization, Time management, Program management","data analysis, epidemiology, statistical techniques, data transformation, data cleaning, sas, excel, tableau, arcgis, data collection, data visualization, communication, problem solving, prioritization, time management, program management","arcgis, communication, data cleaning, data collection, data transformation, dataanalytics, epidemiology, excel, prioritization, problem solving, program management, sas, statistical techniques, tableau, time management, visualization"
Data Scientist,Brooksource,"Indianapolis, IN",https://www.linkedin.com/jobs/view/data-scientist-at-brooksource-3744929684,2023-12-17,Carmel,United States,Associate,Onsite,"Associate Data Scientist
Indianapolis, IN
12 mth contract
Pay: $36.43/hr.
*not able to provide sponsorship
As the Associate Data Scientist, you will work with partners and outside stakeholders to gather and analyze data regarding a variety of issues and topics to develop data driven solutions to difficult business challenges.
Responsibilities:
Mines and analyzes data from organization’s databases to drive optimization and improvement of program development, marketing techniques and business strategies
Assesses the effectiveness and accuracy of new data sources and data gathering techniques
Standardize operational procedures, reviewing code, analyzing, and interpreting datasets, and working with Git for version control
R Shiny Dashboard (Maintenance / Debugging / Enhancements)
Data analysis (Assist in analysis for evaluation, EDA)
Performs exploratory data analysis to understand the organization’s data
Interprets results from multiple sources using a variety of techniques, ranging from simple data aggregation via statistical analysis to complex data mining independently
Develops analytic and predictive models for business needs
Coordinates with different functional teams to implement models and monitor outcomes
Create meaningful visualizations that communicate information and answers
Ensure deployments meet security standards
Work with data engineers/architects to create data pipelines
Work with data scientists to deploy solutions
Maintain / debug Azure cloud infrastructure
Creates process change through the integration of new processes with existing ones and communication of changes to impacted Business Systems teams
Develops compelling data visualizations to communicate findings to target audience(s)
Establishes contracts for the procurement of technical business solutions
Proficient in: SQL, R, Python
Qualifications:
Bachelor’s degree in Data Science, Statistics, Math, Computer science, Informatics, or related fields
1 – 1.5 years of educational, internship, or professional experience in statistical techniques and concepts (regression, properties of distributions, statistical tests, etc.)
Confidence in working with Python, and/or, R
Confidence in R Shiny
Working knowledge using SQL to manipulate and draw insights from large data sets
Machine learning, visualization and storytelling understanding
Ability to research and document findings
Brooksource (Eight Eleven Group) provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, national origin, age, sex, citizenship, disability, genetic information, gender, sexual orientation, gender identity, marital status, amnesty or status as a covered veteran in accordance with applicable federal, state, and local laws.
Show more
Show less","Data Analysis, Data Mining, Data Visualization, Machine Learning, Python, SQL, R Shiny, Git, Azure, Data pipelines","data analysis, data mining, data visualization, machine learning, python, sql, r shiny, git, azure, data pipelines","azure, data mining, dataanalytics, datapipeline, git, machine learning, python, r shiny, sql, visualization"
Hybrid Work - Need Data Analyst in Indianapolis IN,Steneral Consulting,"Indianapolis, IN",https://www.linkedin.com/jobs/view/hybrid-work-need-data-analyst%C2%A0in-indianapolis-in-at-steneral-consulting-3764172728,2023-12-17,Carmel,United States,Associate,Hybrid,"Data Analyst
Indianapolis,IN -
hybrid - 2/3 days Must be local to Indianapolis
Must have skills: Tableau, Tableau Server, Tableau Prep SQL experience,Snowflake Excel, financial background or experience with Procurement
Need someone to provide reporting and analytics as well as undertake large changes due to systems integrations .
Job Description
Guide internal customers through the entire project lifecycle - Negotiate detailed requirement and solution design trade-offs - Contribute to cross-functional and cross-technical dependency management - Provide guidance to the portfolio roadmap for other teams/functions - Anticipate and communicate blockers, conflicts, and obstacles - Connect-the-dots between business strategies and project deliverables - Leads the writing of Acceptance Criteria.
Has broad expertise or unique knowledge, uses skills to contribute to development of company objectives and principles and to achieve goals in creative and effective ways. May be looked to as a lead displaying advanced knowledge in specialized area(s).Works on significant and unique issues where analysis of situations or data requires an evaluation of intangibles. Exercises independent judgment in methods, techniques and evaluation criteria for obtaining results. Contacts pertain to significant matters often involving coordination among groups.
Create and Maintain quantitative models, reporting and analytics, leveraging Snowflake, Microsoft Excel, Google Sheets, Salesforce, Tableau CRM, Tableau Desktop, Tableau Prep, and Tableau Server.
Clean and preprocess data to ensure accuracy, consistency, and readiness for analysis. This may involve handling missing values, outliers, and data formatting.
Ensure data accuracy and reliability through routine data quality checks and maintenance.
Maintain existing Salesforce & Tableau Dashboards for the Supplier Diversity organization and the Global Procurement and Shared Services (GPS) team.
Create global diversity spend dashboards for: Canada, UK, Australia, South Africa , China
Collaborate with our 3rd party supplier tracking and reporting partner.
Manage the monthly, quarterly, and annual Supplier Diversity/Sustainability customer reporting process.
Partner with Department Heads to provide ad-hoc and operational reporting, and to enable sound data-driven business decisions to shape impactful outcomes
Develop dashboards that provide reliable insights and visualization relative to KPIs, projections, and historical performance.
Manage tier 2 Supplier Diversity spend, in collaboration with our service provider partners. ex: revising and updating Tier 2 dashboard metrics
Analyze operational financial results and key performance indicators/metrics on a periodic basis, and understand drivers of business growth, risk, and opportunities
Engage cross-geographical and functional teams to drive scalable deep dive analysis, insights, and tools.
Create executive ready presentations leveraging analytics and working with business partners for content
Show more
Show less","Tableau, Tableau Server, Tableau Prep, SQL, Snowflake, Excel, Salesforce, Tableau CRM, Tableau Desktop, Microsoft Excel, Google Sheets, Salesforce & Tableau Dashboards, Supplier Dashboard, KPI Dashboards, Data Quality Checks, Data Preprocessing, Data Cleaning, Data Formatting, Data Analysis, Data Visualization, Reporting, Analytics, DataDriven Decision Making, Business Intelligence, Project Management, Requirement Gathering, Solution Design, Dependency Management, Roadmap Planning, Communication, Problem Solving, CrossFunctional Collaboration, Leadership, Advanced Knowledge Sharing, Independent Judgment, Collaboration","tableau, tableau server, tableau prep, sql, snowflake, excel, salesforce, tableau crm, tableau desktop, microsoft excel, google sheets, salesforce tableau dashboards, supplier dashboard, kpi dashboards, data quality checks, data preprocessing, data cleaning, data formatting, data analysis, data visualization, reporting, analytics, datadriven decision making, business intelligence, project management, requirement gathering, solution design, dependency management, roadmap planning, communication, problem solving, crossfunctional collaboration, leadership, advanced knowledge sharing, independent judgment, collaboration","advanced knowledge sharing, analytics, business intelligence, collaboration, communication, crossfunctional collaboration, data cleaning, data formatting, data preprocessing, data quality checks, dataanalytics, datadriven decision making, dependency management, excel, google sheets, independent judgment, kpi dashboards, leadership, microsoft excel, problem solving, project management, reporting, requirement gathering, roadmap planning, salesforce, salesforce tableau dashboards, snowflake, solution design, sql, supplier dashboard, tableau, tableau crm, tableau desktop, tableau prep, tableau server, visualization"
Senior Data Engineer I,Allegion,"Carmel, IN",https://www.linkedin.com/jobs/view/senior-data-engineer-i-at-allegion-3620147404,2023-12-17,Carmel,United States,Mid senior,Onsite,"Creating Peace of Mind by Pioneering Safety and Security
At Allegion, we help keep the people you know and love safe and secure where they live, work and visit. With more than 30 brands, 11,000+ employees globally and products sold in 130 countries, we specialize in security around the doorway and beyond.
Summary
This position functions within Allegion’s Global Data & Analytics Team and will support Data Warehouse initiatives of the Data Engineering Team. This role is strategically designed to support existing Analytical Solutions as well as implement ETL solutions within Azure Data Factory. This role will be responsible for collaborating with the Global Data Team, designing and rolling out technical solutions within Allegion’s Global Data Architecture and Best Practice.
Major Job Duties And Responsibilities
Collaborate with Global Data Team Members to review requirements & translate to ETL design and implementation.
Understand Extract Transform and Load technical approaches and designs
Understand the agile approach, leveraging best practices and implementing/documenting repeatable processes
Coordinate with all source-system, and IT teams to ensure integration points are managed and involved
Perform technical validation of extracted data
Follow Allegion code management processes and utilize specified tools.
Support & Maintain Allegion’s Global Data Assets
Skills And Competencies
Ability to execute and deliver to milestones
5+ Years’ experience in data warehousing.
Be self-motivated and highly organized with good time management skills.
Execute and deliver with agile / iterative approach.
Work well in teams that may be globally co-located.
Flexibility and adaptability in responsibilities as required
Experience working with Azure Data Factory or Extract-Transform-Load Tool equivalent
Experience working with SQL Server Databases
Bachelor’s degree (preferred in an IT-related discipline)
Preferred Education/ Experience.
Basic Python Data Transformation Skills
Basic PowerBI or Analysis Services Knowledge for Data Analysis
Working Knowledge of Event Hub, Azure Function Apps, & IoT Hub
We Celebrate Who We Are!
Allegion is committed to building and maintaining a diverse and inclusive workplace. Together, we embrace all differences and similarities among colleagues, as well as the differences and similarities within the relationships that we foster with customers, suppliers and the communities where we live and work. Whatever your background, experience, race, color, national origin, religion, age, gender, gender identity, disability status, sexual orientation, protected veteran status, or any other characteristic protected by law, we will make sure that you have every opportunity to impress us in your application and the opportunity to give your best at work, not because we’re required to, but because it’s the right thing to do. We are also committed to providing accommodations for persons with disabilities. If for any reason you cannot apply through our career site and require an accommodation or assistance, please contact our Talent Acquisition Team.
© Allegion plc, 2020 | Block D, Iveagh Court, Harcourt Road, Dublin 2, Co. Dublin, Ireland
REGISTERED IN IRELAND WITH LIMITED LIABILITY REGISTERED NUMBER 527370
Allegion is an equal opportunity and affirmative action employer
Privacy Policy
Show more
Show less","Data Warehousing, Extract Transform Load, SQL Server, Python, PowerBI, Analysis Services, Event Hub, Azure Function Apps, IoT Hub, Agile, Azure Data Factory","data warehousing, extract transform load, sql server, python, powerbi, analysis services, event hub, azure function apps, iot hub, agile, azure data factory","agile, analysis services, azure data factory, azure function apps, datawarehouse, event hub, extract transform load, iot hub, powerbi, python, sql server"
Data Scientist (NEED LOCAL CANDIDATES),Alrek Business Solutions Inc,"Indianapolis, IN",https://www.linkedin.com/jobs/view/data-scientist-need-local-candidates-at-alrek-business-solutions-inc-3748446949,2023-12-17,Carmel,United States,Mid senior,Onsite,"Works with agency partners and outside stakeholders to gather and analyze data regarding a variety of issues and topics in order to develop data driven solutions to difficult business challenges.
Only local candidates will be considered for this position.
The Data Scientist position will work with agency partners and outside stakeholders to gather and analyze data regarding a variety of issues and topics in order to develop data driven solutions to difficult business challenges.
Required/Desired Skills
Skill Required /Desired Amount of Experience Experience with the Microsoft Power BI Desktop/Service Required 2 Years Proficiency in designing data models that underpin reports and dashboards. Required 2 Years Experience with DAX, a formula language used in Power BI for calculations and data manipulation. Required 2 Years Skills in transforming raw data into a format suitable for analysis. Required 2 Years Experience in creating visually compelling and informative reports and dashboards. Required 2 Years Knowledge of connecting to various data sources, such as databases, cloud services, and APIs. Desired 0 Familiarity with SQL for querying databases. Desired 0 An ability to work with data, including data cleansing and transformation. Desired 0 The capacity to extract insights and trends from data. Desired 0 Translating data insights into understandable narratives for non-technical stakeholders. Desired 0 Experience with Microsoft Power BI external related tools Nice to have 0
Show more
Show less","Microsoft Power BI Desktop/Service, Data modeling, DAX, Data transformation, Data visualization, Data cleansing, Data analysis, SQL, Data storytelling, Microsoft Power BI external related tools","microsoft power bi desktopservice, data modeling, dax, data transformation, data visualization, data cleansing, data analysis, sql, data storytelling, microsoft power bi external related tools","data storytelling, data transformation, dataanalytics, datacleaning, datamodeling, dax, microsoft power bi desktopservice, microsoft power bi external related tools, sql, visualization"
"Manager, Data Engineering & Architecture - Indianapolis Health",Milliman,"Indianapolis, IN",https://www.linkedin.com/jobs/view/manager-data-engineering-architecture-indianapolis-health-at-milliman-3641127401,2023-12-17,Carmel,United States,Mid senior,Onsite,"The
Manager, Data Engineering & Architecture
is responsible for managing a team of data engineers and data analysts who are accountable for data pipelines, database architecture, and data analytics to support consulting teams in providing services to our clients. The data pipelines handle healthcare claims, enrollment, and related data, primarily for state Medicaid agency clients. The data engineering and architecture manager will also collaborate with consultants to develop healthcare data analytics and tools to support our clients.
Responsibilities
You will:
Be accountable for data engineering lifecycle including research, proof of concepts, design, development, testing, deployment, and maintenance.
Plan, design, build, implement, and maintain accurate, robust, scalable, and efficient data processing pipelines for the extraction, transformation, and loading (ETL/ELT) of structured and unstructured healthcare data from various client sources in Databricks.
Layer in proactive monitoring in the development process so that data pipelines are monitored, and issues are identified with minimal impact to users. Evaluate and recommend tools and technologies for data infrastructure and processing.
Ensure adherence to industry best practices to reduce current and minimize future technical debt.
Collaborate with team members and consultants to translate business requirements to technical specifications and coded data pipelines.
Oversee the creation and delivery of high-quality, innovative analysis, and work to grow analytic offerings.
Partner with consultants to develop tools and analytical services such as dashboards, decision aids, and visualizations to support our clients.
Present analysis, insights, and recommendations to consultants.
Manage projects and resources.
Maintain high quality documentation of data definitions, transformations, and processes to ensure data governance and security.
Ensure quality assurance and peer review are completed on all work deliverables.
Incorporate regulatory requirements, e.g., HIPAA, into database design and security.
Provide technical guidance to and coach/mentor team members.
Routinely communicate and collaborate with the consulting teams to prioritize data pipeline and project deliverables and ensure on-time completion.
Qualifications
Preferred:
Extensive knowledge of healthcare claims and enrollment data, standards, and regulatory requirements.
Advanced knowledge of SQL and Relational Database Management Systems.
Prior experience as a data architect or data engineer building data pipelines.
Experience with cloud infrastructure and services (Azure/AWS).
Experience supervising multiple team members.
Proficient in working with cross functional teams.
Strong oral and written communication skills.
Ability to effectively manage multiple ongoing projects.
Passion for high-quality work, continuous improvement, and routinely exceeding consultant expectations.
Required
7+ years of professional experience with data engineering and building data pipelines using healthcare claims and enrollment data
3+ years of experience in Microsoft Azure and/or AWS services
Experience in Databricks
Effective written and verbal communication skills
Ability to articulate issues and analytic results in both technical and layperson terminology
Demonstrated project and staff management experience
Ability to work independently and collaborate with consulting teams
Bachelor of science or advanced degree in computer science, software engineering, information technology, applied math, data science, or a related field
Location
Candidates hired into this role will be required to work in-person in the Milliman office in Indianapolis, IN on a weekly basis, but flexible work arrangements will be considered.
Benefits
At Milliman, we focus on creating an environment that recognizes – and meets – the personal and professional needs of the individual and their family. We offer competitive benefits which include the following based on plan eligibility:
Medical, dental and vision coverage for employees and their dependents, including domestic partners
A 401(k) plan with matching program, and profit sharing contribution
Employee Assistance Program (EAP)
A discretionary bonus program
Paid Time Off (PTO) starts accruing on the first day of work and can be used for any reason; full-time employees will accrue 15 days of PTO per year, and employees working less than a full-time schedule will accrue PTO at a prorated amount based on hours worked
Family building benefits, including adoption and fertility assistance and paid parental leave up to 12 weeks for employees who have worked for Milliman for at least 12 months and have worked at least 1,250 hours in the preceding 12-month period
A minimum of 8 paid holidays
Milliman covers 100% of the premiums for life insurance, AD&D, and both short-term and long-term disability coverage
Flexible spending accounts allow employees to set aside pre-tax dollars to pay for dependent care, transportation, and applicable medical needs
Who We Are
Independent for 75 years, Milliman delivers market-leading services and solutions to clients worldwide. Today, we are helping companies take on some of the world’s most critical and complex issues, including retirement funding and healthcare financing, risk management and regulatory compliance, data analytics and business transformation.
Through a team of professionals ranging from actuaries to clinicians, technology specialists to plan administrators, we offer unparalleled expertise in employee benefits, investment consulting, healthcare, life insurance and financial services, and property and casualty insurance.
All qualified applicants will receive consideration for employment, without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.
Show more
Show less","Data engineering, Data architecture, Data analytics, Data pipelines, Healthcare claims, Enrollment data, Database architecture, Databricks, ETL/ELT, Cloud infrastructure, Azure, AWS, SQL, Relational Database Management Systems, HIPAA, Computer science, Software engineering, Information technology, Applied math, Data science","data engineering, data architecture, data analytics, data pipelines, healthcare claims, enrollment data, database architecture, databricks, etlelt, cloud infrastructure, azure, aws, sql, relational database management systems, hipaa, computer science, software engineering, information technology, applied math, data science","applied math, aws, azure, cloud infrastructure, computer science, data architecture, data engineering, data science, dataanalytics, database architecture, databricks, datapipeline, enrollment data, etlelt, healthcare claims, hipaa, information technology, relational database management systems, software engineering, sql"
Analyst-Information Services Data Senior,Indiana University Health,"Indianapolis, IN",https://www.linkedin.com/jobs/view/analyst-information-services-data-senior-at-indiana-university-health-3752332377,2023-12-17,Carmel,United States,Mid senior,Onsite,"Overview
ANALYST-INFORMATION SERVICES DATA SENIOR
RESPONSIBILITIES:
Provides analysis, reporting and data discovery services for end-users under limited supervision.
Incorporates end-user requests and requirements to develop enterprise reporting/dashboarding solutions.
Collaborates with end-users to generate and prepare solutions to support business and clinical objectives.
Administers access and maintains data warehouse security to ensure reliability and security of data.
Trains end-users on reporting tools.
Solves complex analytical problems.
Leads projects or project steps within a broader project or has accountability for ongoing activities or objectives.
Provides breakthrough thinking on analytical needs and solutions.
Acts as a resource/mentor for colleagues with less experience.
PREFERRED:
3-5 years of Healthcare Analytics experience
Experience using SQL (T-SQL preferred on an Azure environment)
Extensive use of Data Visualization Software (Power BI highly preferred)
Knowledge of programming languages for statistical programming (R preferred, Python is welcome)
Ability to use version control systems for code (git)
Experience working with frontline clinician teams & disseminating information/data to them.
REQUIRED:
Bachelor's degree preferred or equivalent experience.
5-7 years of experience required.
Requires knowledge of SQL programming.
We are an equal opportunity employer and value diversity and inclusion at IU Health. IU Health does not discriminate on the basis of race, color, religion, sex, sexual orientation, age, disability, genetic information, veteran status, national origin, gender identity and/or expression, marital status or any other characteristic protected by federal, state or local law. We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.
Show more
Show less","Data Analysis, Data Discovery, Reporting, Dashboarding, SQL, TSQL, Azure, Power BI, R, Python, Git, Version Control, Healthcare Analytics, Data Visualization","data analysis, data discovery, reporting, dashboarding, sql, tsql, azure, power bi, r, python, git, version control, healthcare analytics, data visualization","azure, dashboard, data discovery, dataanalytics, git, healthcare analytics, powerbi, python, r, reporting, sql, tsql, version control, visualization"
Senior Data Engineer,Lever Middleware Test Company 2,"Indianapolis, IN",https://www.linkedin.com/jobs/view/senior-data-engineer-at-lever-middleware-test-company-2-3787339119,2023-12-17,Carmel,United States,Mid senior,Remote,"a Bit About Us
Lever, an Employ brand, has a vision of connecting human potential to meaningful work. Hiring is the most strategic challenge that every company faces, and Lever is one of the most recognized brands in talent acquisition software. As the innovation leader in our space, we are searching for great people to join us and push further.
With an overall gender ratio of 50:50 women and men, Lever is also fiercely committed to building a team culture that celebrates diversity and inclusion. We have been certified as a “Great Place to Work”, along with “Best Company Outlook 2022,” and “Forbes Best Startup Employers 2022,” and the list keeps growing! Our people are Lever’s biggest competitive advantage. For more information on our team culture, visit https://inside.lever.co/.
THE CHALLENGE
Lever is on a mission to provide the best analytics feature set in the world of Talent software. We’ve made good progress in the last 2 years, but we still have room to improve.
This is where you come in. You will be the technical lead for the engineering team that is evolving Lever’s analytics features. You will help Lever’s customers recruit more efficiently and more predictably by providing them with a reliable and accurate data set to analyze.
This position will focus on maintaining and improving our data pipelines while consulting with partners in product management and product design on building user facing analytics features. As part of that collaboration, you will be encouraged to use your expertise and conduct technical research to help inform the direction we take with our analytics products.
THE OPPORTUNITY
One of Lever’s top priorities is to make our analytics features a reason that people buy and stay with Lever. As the lead engineer on the Analytics team you will have the opportunity to have a big impact on the success of Lever’s product and Lever’s business. You’ll work collaboratively with partners on the analytics team to drive the implementation of features, and be a champion of best practices for writing well-tested, well-organized code.
In addition to working on a high impact product area, as part of your daily work you will have the opportunity to build expertise in multiple Business Intelligence tools. You will be able to exercise and strengthen your automation and optimization skills. You will have the opportunity to learn about state of the art data infrastructure systems like Kafka and Snowflake, and you will be able to leverage observability tools like Datadog and Kibana to observe our systems.
You will also have the opportunity to lead the development of user facing analytics features, depending on your preferences and prior experience.
THE TEAM
Lever’s reporting engineering team combines ETL engineers, product engineers, data analysts, designers, and product managers to build, define, and design analytics products for customers. You will join the team as the lead software engineer building and maintaining advanced analytics products for Lever’s customers.
You will collaborate closely with our brilliant data analysts who create dashboards in the product, and scrappy Talent Analytics Consultants provide custom analysis services for customers. You will use your data and engineering knowledge to help guide the product and design team that strives to build analytics features to connect human potential to meaningful work.
THE SKILL SET
6+ years of software engineering experience
Experience with data pipelines and both the common pitfalls and best practices for maintaining them is a plus
Professional experience with relational databases that speak SQL (Postgres, MySql, MSSql, Oracle, Snowflake, etc)
An understanding of Business Intelligence tools, like Tableau or Looker, and the problems they solve
Knowledge of Looker is a plus, a desire to learn Looker is a must
A desire to teach others with the communication skills needed to effectively share knowledge
A healthy appetite for being a technical mentor for other engineers
This role includes a lot of technical freedom, so the ability to research technical solutions beyond those that already exist in our code base is a plus
Empathy for users of the Lever product and internal users that work at Lever
WITHIN 1 MONTH, YOU'LL
Complete Ramp Camp, our onboarding program designed to get you up to speed on our business, vision, and team.
Learn about the technology behind Lever’s analytics features (Lever Talent Intelligence, Data Warehouse Sync, and Visual Insights)
Participate in the reporting team’s ceremonies and get to know your teammates
Meet internal users of Business Intelligence tools
Deploy your first code change
WITHIN 3 MONTHS, YOU'LL
Learn about Lever’s product and how the analytics feature fit into the full picture
Build familiarity with Lever’s application data model in Mongo, how it aligns with data in SQL, and how the our data pipelines perform the translation
Understand the existing observability tools and monitoring we have in place for our data pipelines
Participate in Lever’s analytics product development process and ship your first feature to customers
Improve tools that reporting engineers use to maintain data systems
WITHIN 6 MONTHS, YOU'LL
Find and fill gaps in observability and automated testing for reporting features
Engage in your first on-call rotation for the product to help diagnose and resolve production problems and customer issues
Collaborate with other product engineering teams to incorporate data generated by new features into our analytics products
Identify and fix large areas of improvement in our data pipelines and schema management tool set
Contribute new ideas to the long-term roadmap of the reporting team
WITHIN 12 MONTHS, YOU'LL
Help the data analysts improve performance of pre-built Looker dashboards with optimizations and pre-calculated aggregations
Improve the experience of data analysts by providing them with better data environments to use for development
Automate continuous data quality and end-to-end service testing of data pipelines
Incorporate custom access control rules into the data available to end users of Lever
Is this role not an exact fit?
Sign up to stay in touch, we'll let you know when we have new positions on the team.
The Lever Story
Lever, an Employ brand, is a leading Talent Acquisition Suite that makes it easy for talent teams to reach their hiring goals and to connect companies with top talent. Lever is the only platform that provides all talent acquisition leaders with complete ATS and robust CRM capabilities in one product, LeverTRM. The LeverTRM features allow leaders to scale and grow their people pipeline, build authentic and long-lasting relationships, and source the right people to hire. Lever Analytics provides customized reports with data visualization, see offers completed and interview feedback, and more, to inform strategic decisions between hiring managers and executives alike.
Our platform also enables companies to hire with inclusivity in mind, helping eliminate any hiring bias. Lever supports the hiring needs of over 5,000 companies around the globe including the teams at Netflix, Spotify, Atlassian, KPMG, and Nielsen. For more information, visit https://www.lever.co.
California residents applying for positions at Lever can see our privacy policy here.
#BI-REMOTE
Show more
Show less","Software Engineering, Data Pipelines, SQL, Business Intelligence, Tableau, Looker, Observability Tools, Mongo, Data Model, Testing, Accessibility Control, Data Quality, Data Visualization","software engineering, data pipelines, sql, business intelligence, tableau, looker, observability tools, mongo, data model, testing, accessibility control, data quality, data visualization","accessibility control, business intelligence, data model, data quality, datapipeline, looker, mongo, observability tools, software engineering, sql, tableau, testing, visualization"
Data Visualization/D3/JavaScript Engineer,IVY TECH SOLUTIONS INC,"Indianapolis, IN",https://www.linkedin.com/jobs/view/data-visualization-d3-javascript-engineer-at-ivy-tech-solutions-inc-3787774072,2023-12-17,Carmel,United States,Mid senior,Remote,"Data Visualization/D3/JavaScript Engineer | Quick Summary
You are passionate about telling stories with data and can drive the development process from the initial prototype through the implementation of the final product. You will design and develop data-driven, dynamic user-facing interactive dashboard and visualization tools with high-performing responsiveness, usability, and visual appeal. You will implement solutions using JavaScript-based data visualization libraries and tools, primarily D3js (Experience with additional JS libraries such as React.js, Vue.js, Angular.js, Node.js, Bone.js, CartoDb.js would be helpful!) Having JMP, Tableau or Spotfire is a HUGE BONUS!
JavaScript Engineer | Desired Skills & Experience
5+ years of JavaScript development
In-depth knowledge of JavaScript with extensive experience with D3.js required!
Experience with Vue.js and/or React.js are preferred
Experience implementing web services (SOAP and RESTful)
Experience connecting front-end interfaces with SQL or NoSQL
Fluent in HTML5, CSS, Bootstrap, jQuery
Fluency in other complementary JavaScript-based tools and frameworks or scripting language
Familiarity with UX design and image editing tools
Proficient understanding of code versioning tools
Strong knowledge of computer science fundamentals and coding best practices
An understanding of Agile/Scrum development and a collaborative, proactive attitude
Enterprise experience, healthcare, pharma, biotechnology experience is a bonus
Tableau or other Data Visualization tool experience is a bonus!
Powered by JazzHR
4kDYknqLos
Show more
Show less","JavaScript, D3.js, Vue.js, React.js, Angular.js, Node.js, Bone.js, CartoDb.js, JMP, Tableau, Spotfire, SOAP, RESTful, SQL, NoSQL, HTML5, CSS, Bootstrap, jQuery, Agile, Scrum, Git","javascript, d3js, vuejs, reactjs, angularjs, nodejs, bonejs, cartodbjs, jmp, tableau, spotfire, soap, restful, sql, nosql, html5, css, bootstrap, jquery, agile, scrum, git","agile, angularjs, bonejs, bootstrap, cartodbjs, css, d3js, git, html5, javascript, jmp, jquery, nodejs, nosql, reactjs, restful, scrum, soap, spotfire, sql, tableau, vuejs"
SQL Data Analyst I,PRECISIONxtract,"Indianapolis, IN",https://www.linkedin.com/jobs/view/sql-data-analyst-i-at-precisionxtract-3748933418,2023-12-17,Carmel,United States,Mid senior,Remote,"The SQL Data Analyst role is fully remote (or work from our luxury offices in Chicago, Indianapolis, Boston, NYC, Gladstone NJ if preferred).
Unfortunately, we are not able to provide sponsorship assistance at this time or in the future.***
Are you a problem-solver who thrives in a data rich environment in the Life Sciences space? Do you love mentoring and encouraging others? Join our
Customer Engagement Solutions Team
within
PRECISIONxtract
where we excel at understanding the ever-changing and complex healthcare market and helping pharmaceutical and biotech clients drive the greatest performance for their products through innovative analytics, strategies, and tools.
Highly analytical, dedicated professionals who also love data, thrive in the details and want to flourish in a growing company an outstanding fit for us. We are excited to bring a forward-thinking, motivated Business Intelligence Analyst onto our growing team.
The primary responsibility of the
SQL Data Analyst I
is to support our clients through consistent delivery of development projects, thorough quality control, and associated documentation. A successful SQL Developer I is expected to understand, implement, and transform business requirements into well-documented data functionality.
Essential functions of the job include but are not limited to:
Owning the transformation of clear business rules into functional data requirements
Executing code and self-validating results
Proficient in understanding stored procedures, views, and functions within T-SQL
Understanding and maintaining proper branch management within development projects
Writing and modifying procedural code that is well-documented, logical, easy to follow
Communicating to others in regard to data output and limitations
Follow established procedures and standards as outlined by leadership
Maintain associated procedural documentation
Other duties as assigned
Qualifications
Minimum Required:
Undergraduate degree
Experience with relational databases
Preferred
2 to 5 years industry experience
Evident proficiency and hands-on experience using Microsoft SQL Server
Undergraduate degree with a quantitative focus
Skills
Owning the outcome of your work; be accountable
Good time management
Clear and effective communication
Attention to detail
Ability to work remotely and independently
Capable of dealing gracefully with failure
Demonstrated proficiency in Microsoft Office product suite including PowerPoint, Word, and Excel
Demonstrated proficiency in Microsoft SQL Server and T-SQL
Experience in SSIS, AzureDevOps, or Visual Studio is a plus but not required
Experience in code optimization is a plus but not required
Precision is required by law in some states or cities to include a reasonable estimate of the compensation range for this role. This compensation range takes into account the wide range of factors that are considered in making compensation decisions including but not limited to: skill sets, experience and training, licensure and certifications, and other business and organizational needs. The disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled. At Precision, it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. This role is also eligible for a discretionary annual bonus, health insurance, retirement savings benefits, life insurance and disability benefits, parental leave, and paid time off for sick leave and vacation, among other benefits.
Reasonable estimate of the current range
$62,500 — $87,500 USD
Any data provided as a part of this application will be stored in accordance with our Privacy Policy. For CA applicants, please also refer to our CA Privacy Notice .
Precision Medicine Group is an Equal Opportunity Employer. Employment decisions are made without regard to race, color, age, religion, sex, sexual orientation, gender identity, national origin, disability, veteran status or other characteristics protected by law. © 2020 Precision Medicine Group, LLC
If you are an individual with a disability and require a reasonable accommodation to complete any part of the application process or are limited in the ability or unable to access or use this online application process and need an alternative method for applying, you may contact Precision Medicine Group at QuestionForHR@precisionmedicinegrp.com .
Show more
Show less","SQL, TSQL, Microsoft SQL Server, SSIS, AzureDevOps, Visual Studio, Microsoft Office, PowerPoint, Word, Excel, Relational databases, Code optimization, Business intelligence, Data analytics","sql, tsql, microsoft sql server, ssis, azuredevops, visual studio, microsoft office, powerpoint, word, excel, relational databases, code optimization, business intelligence, data analytics","azuredevops, business intelligence, code optimization, dataanalytics, excel, microsoft office, microsoft sql server, powerpoint, relational databases, sql, ssis, tsql, visual studio, word"
"Data Conversion Developer, Senior Associate",PwC,"Indianapolis, IN",https://www.linkedin.com/jobs/view/data-conversion-developer-senior-associate-at-pwc-3749934869,2023-12-17,Carmel,United States,Mid senior,Remote,"Specialty/Competency:
Functional & Industry Technologies
Industry/Sector:
Not Applicable
Time Type:
Full time
Travel Requirements:
Up to 80%
A career within Functional and Industry Technologies services will provide you with the opportunity to build secure and new digital experiences for customers, employees, and suppliers. We focus on improving apps or developing new apps for traditional and mobile devices as well as conducting usability testing to find ways to improve our clients’ user experience. Our team helps clients transform their business through enabling technologies across marketing, finance and operations in the functional areas such as Maximo and PowerPlant.
To really stand out and make us fit for the future in a constantly changing world, each and every one of us at PwC needs to be a purpose-led and values-driven leader at every level. To help us achieve this we have the PwC Professional; our global leadership development framework. It gives us a single set of expectations across our lines, geographies and career paths, and provides transparency on the skills we need as individuals to be successful and progress in our careers, now and in the future.
Responsibilities
As a Senior Associate, you'll work as part of a team of problem solvers, helping to solve complex business issues from strategy to execution. PwC Professional skills and responsibilities for this management level include but are not limited to:
Use feedback and reflection to develop self awareness, personal strengths and address development areas.
Delegate to others to provide stretch opportunities, coaching them to deliver results.
Demonstrate critical thinking and the ability to bring order to unstructured problems.
Use a broad range of tools and techniques to extract insights from current industry or sector trends.
Review your work and that of others for quality, accuracy and relevance.
Know how and when to use tools available for a given situation and can explain the reasons for this choice.
Seek and embrace opportunities which give exposure to different situations, environments and perspectives.
Use straightforward communication, in a structured way, when influencing and connecting with others.
Able to read situations and modify behavior to build quality relationships.
Uphold the firm's code of ethics and business conduct.
Basic Qualifications
Minimum Degree Required:
Bachelor Degree
Minimum Years Of Experience
4 years
Preferred Qualifications
Degree Preferred:
Master Degree
Certification(s) Preferred
Azure Data Engineer Associate
Databricks Certified Data Engineer Associate
Preferred Fields Of Study
Computer and Information Science, Computer Engineering, Computer Management, Management Information Systems, Information Technology
Preferred Knowledge/Skills
Demonstrates a thorough level of abilities with, and/or a proven record of success as both an individual contributor and team member, identifying and addressing client needs:
Supports in data analysis techniques to assess source data structures, identify mapping requirements, and define transformation rules for data conversion into Maximo;
Leads Maximo's modules and functionalities related to Asset Management and Work Order Management and IBM Maximo, including its data structures, configuration settings, and integration capabilities;
Identifies relational databases, preferably experience in working with databases commonly used in Maximo, such as IBM DB2, Oracle, Microsoft SQL Server, along with familiarity with Maximo's Integration Framework (MIF) and its capabilities for data integration and conversion;
Supports in designing and implementing data extraction, transformation, and loading processes for Maximo data conversion;
Showcases understanding in SQL and database querying languages to extract and manipulate data from source systems along with understanding ETL tools and methodologies commonly used in Maximo data conversion;
Identifies technologies commonly used with Maximo, including web services (SOAP, RESTful APIs), XML, JSON, and other relevant data exchange formats;
Supports in pipeline architecture and development using one of the tools such as Azure ADF, AWS Glue, SSIS, DataBricks (multiple preferred);
Utilizes data cleansing techniques and methodologies to ensure the integrity and accuracy of converted data in Maximo;
Developes data cleansing functional business rules as per Maximo Business Object (MBO) definitions for source to Maximo conversion requirements;and comprehensive testing plans and executing validation processes to verify the accuracy and integrity of converted data in Maximo;
Customizes Maximo options, such as Automation Scripts, Java Customizations, Database Configuration, or Application Designer, to support data conversion requirements;
Identifies integrations within Maximo with other enterprise systems, such as ERP systems, GIS systems, or asset management systems. Knowledge of integration patterns, data synchronization, and data exchange protocols; and,
Showcases work experience as a Data Engineer, Data Architect or similar role, along with experience in programming languages including Python, PySpark, Scala, SQL.
Learn more about how we work: https://pwc.to/how-we-work
PwC does not intend to hire experienced or entry level job seekers who will need, now or in the future, PwC sponsorship through the H-1B lottery, except as set forth within the following policy: https://pwc.to/H-1B-Lottery-Policy.
All qualified applicants will receive consideration for employment at PwC without regard to race; creed; color; religion; national origin; sex; age; disability; sexual orientation; gender identity or expression; genetic predisposition or carrier status; veteran, marital, or citizenship status; or any other status protected by law. PwC is proud to be an affirmative action and equal opportunity employer.
For positions based in San Francisco, consideration of qualified candidates with arrest and conviction records will be in a manner consistent with the San Francisco Fair Chance Ordinance.
Applications will be accepted until the position is filled or the posting is removed, unless otherwise set forth on the following webpage. Please visit this link for information about anticipated application deadlines: https://pwc.to/us-application-deadlines
For positions in California, Colorado, Hawaii, Nevada, New York State, or Washington State, or for opportunities that will report to a supervisor, office or other work site in New York State, please visit the following link for pay range information: https://pwc.to/payrange-v1-advisoryseniorassociate
Show more
Show less","Azure Data Engineer Associate, Databricks Certified Data Engineer Associate, Python, PySpark, Scala, SQL, XML, JSON, SOAP, RESTful APIs, Web services, ETL, Data cleansing, Data conversion, Maximo, IBM DB2, Oracle, Microsoft SQL Server, Maximo Integration Framework (MIF), Azure ADF, AWS Glue, SSIS, DataBricks, Automation Scripts, Java Customizations, Database Configuration, Application Designer, Data synchronization, Data exchange protocols, ERP systems, GIS systems, Asset management systems","azure data engineer associate, databricks certified data engineer associate, python, pyspark, scala, sql, xml, json, soap, restful apis, web services, etl, data cleansing, data conversion, maximo, ibm db2, oracle, microsoft sql server, maximo integration framework mif, azure adf, aws glue, ssis, databricks, automation scripts, java customizations, database configuration, application designer, data synchronization, data exchange protocols, erp systems, gis systems, asset management systems","application designer, asset management systems, automation scripts, aws glue, azure adf, azure data engineer associate, data conversion, data exchange protocols, data synchronization, database configuration, databricks, databricks certified data engineer associate, datacleaning, erp systems, etl, gis systems, ibm db2, java customizations, json, maximo, maximo integration framework mif, microsoft sql server, oracle, python, restful apis, scala, soap, spark, sql, ssis, web services, xml"
Sr Data Analyst Exp with MSFT,IVY TECH SOLUTIONS INC,"Indianapolis, IN",https://www.linkedin.com/jobs/view/sr-data-analyst-exp-with-msft-at-ivy-tech-solutions-inc-3787776614,2023-12-17,Carmel,United States,Mid senior,Remote,"HI,
Kindly let me know if you have a suitable fit for the following position
Thanks
Sr Data Analyst
Location: Puget Sound, WA (open to non-local candidates)
Duration: 3-6 Months, potential for extension
Please send the resume to
or 847- 350-1008
Job Description:
Executes project work and has basic experience. Responsible for data de-duplication (matching and merging data), data cleansing (matching, linking, updating records, such as organization records, in batch and real time), hierarchy management (identifying, creating, correcting, update, and maintaining complex relationships among parent and child records), and data conversion (changing / transforming the data based on business rules, usually done before the data is integrated into a new system). Also raises escalations and communicates issues to DOC Project Lead.
Excellent excel skills, and previous experience in one of the Microsoft Sales organizations
Ideal Profile
Experience in a Microsoft sales organization and understands the Segment (Enterprise, SMC, SMB, etc) and A14 structure (United States, Germany, etc)
Excellent Excel and Power BI skills
Analytical thinker with a can-do attitude and flexibility to accommodate to evolving business needs.
High level of organization, planning, prioritization and attention to detail
Day to day tasks:
Working within excel and power BI dashboard - As part of the Fiscal Year Rollover, need help converting FY21 data into the FY22 structure. Will need to take multiple excel files and consolidate, match, etc . Link the excel files to back end datamarts, ie. HR Data Lake, MS People, ISP, ICIMS, etc. Create a Power BI dashboard
Top 3 Must-Haves:
1 Proficient Excel experience
2 Power BI
3 Work within Segment/Sales experience (Enterprise, SMC, SMB, etc) / A14 structur
Charan Kumar
| IVY Tech Sols Inc.
3403 N Kennecott Avenue, Suite B&C Arlington Heights, IL 60004
PH.
( Direct:
(847) 350-1008
|Gtalk : charan.ivytech|
Powered by JazzHR
VkmnOKtspM
Show more
Show less","Data DeDuplication, Data Cleansing, Hierarchy Management, Data Conversion, Excel, Power BI, Data Lakes, HR Data Lake, MS People, ISP, ICIMS","data deduplication, data cleansing, hierarchy management, data conversion, excel, power bi, data lakes, hr data lake, ms people, isp, icims","data conversion, data deduplication, data lakes, datacleaning, excel, hierarchy management, hr data lake, icims, isp, ms people, powerbi"
Senior Cloud Data Engineer,BDO USA,"Indianapolis, IN",https://www.linkedin.com/jobs/view/senior-cloud-data-engineer-at-bdo-usa-3765467840,2023-12-17,Carmel,United States,Mid senior,Hybrid,"Job Description
Job Summary:
This position will work with cutting edge technology, deliver high quality solutions across various industries, and oversee team(s) on engagements that range in size and scope. This position will receive continuous career development opportunities, given the size and potential of client engagements. This role will perform hands-on delivery of data analytics projects, contributing to the development and unit testing of solutions.
Job Duties
Designs and implements best in class data ingestion strategies, data warehouse and data mart structures, semantic layers and models, visualizations, streaming processes, API integrations, and automation (RPA) solutions for end-to-end data analytics solutions on primarily, but not limited to, cloud analytics platforms such as Azure and AWS
Listens to client needs to align solution with business requirements and delivery schedule
Creates written functional and technical designs
Participates in project status and stand meetings, and assists with providing aggregated project status for project and program managers
Assists with SLA compliance of solutions, and performs performance tuning and optimization efforts of end-to-end solutions
Writes code using multiple languages and correctly applies frameworks, architectural patterns, and software development principles
Delivers high-performance, scalable, repeatable, and secure deliverables with broad impact (high throughput and low latency)
Assists with implementation of data governance programs and best practices
Performs the cleaning and transforming of data from source systems into analytics models
Implements models to support data visualizations and integrations
Assists with implementing DevOps, DataOps and MLOps methodologies on projects
Writes custom integration logic in applicable programming languages
Assists project managers with work breakdown structure creation, project estimation, resource staffing, workload planning and adjustments throughout the project lifecycle
Assists clients with licensing, security, and cost estimation of solutions
Performs code reviews to ensure adherence to standards
Works directly with clients and team members to establish secure data analytics platforms and infrastructure
Contributes to successful deployments of developed solutions and integration of DevOps tools
Maintains a broad and current understanding of data analytics and business intelligence strategies, cloud platforms, methodologies, and tools
Builds client relationships during project execution, effectively becoming a trusted advisor of the client
Participates in support activities for existing software solutions
Other duties as assigned
Supervisory Responsibilities
Supervises the day-to-day workload of Associates on assigned engagements to ensure that timelines and deliverables are met, and reviews work product
Education
Qualifications, Knowledge, Skills and Abilities:
High School Diploma or GED equivalent, required
Bachelor’s degree, preferred; focus in Information Systems, Data Science or Computer Science, preferred
Experience
Five (5) or more years of experience within Data Analytics, Business Intelligence, Artificial Intelligence, or Application Development, required
One (1) or more years of experience technically leading development projects, preferred
One (1) or more years of consulting experience or implementation of cloud-based data analytics solutions, preferred
Software
Strong SQL skills including Data Definition Language (DDL), Data Manipulation Language (DML), views, functions, stored procedures, or performance tuning, required
Experience with Data Warehousing, Data Modeling, Semantic Model Definition or Star Schema Construction, required
Hands on delivery experience of end-to-end cloud data analytics solutions within Azure or AWS, preferred
Experience with one (1) or more of the following computer languages, preferred:
C#
Python
Java
Scala
Experience with tabular modeling within Microsoft Fabric, Power BI, or Azure Analysis Services, preferred
Experience with Git and DevOps deployment technologies, preferred
Experience with Linux, preferred
Experience with one (1) or more of the following, preferred:
Data Lake Medallion Architecture
Batch and/or streaming data ingestion into a data lake
AI Algorithms/Machine Learning
Automation tools such as UiPath, Alteryx, etc.
Computer Vision based AI technologies
Other Knowledge, Skills & Abilities
Ability to work with a high degree of professionalism and autonomy
Excellent verbal and written communication skills
Solid organizational skills, especially the ability to meet project deadlines with a focus on details
Ability to successfully multi-task while working independently or within a group environment
Ability to work in a deadline-driven environment, and handle multiple projects simultaneously
Ability to interact effectively with people at all organizational levels of the Firm
Ability to effectively interact with a team of professionals and delegating work assignments, as needed
Ability to build and maintain strong relationships with internal and client personnel
Ability to encourage a team environment on engagements, and contribute to the professional development of assigned personnel
Keywords:
Data Analytics, Business Intelligence, BI, Synapse, IoT, Machine Learning, Data Lake, Stream, Cube, Microsoft, SQL Server, Tableau, .Net, C#, Qlik, Power BI, Machine Learning, Azure Data Factory, RedShift, UiPath, Cloud, RPA, AWS, Redshift, Kinesis, QuickSight, SageMaker, S3, Databricks, AWS Lake Formation, Snowflake, Python, Qlik, Athena, Data Pipeline, Glue, Star Schema, Data Modeling, SQL, SSIS, SSAS, SSRS, PySpark, Microsoft Fabric, dbt, Linux, Terraform, Bicep, Data Ops, Purview, Git, Delta, Pandas, Spark SQL
Individual salaries that are offered to a candidate are determined after consideration of numerous factors including but not limited to the candidate’s qualifications, experience, skills, and geography.
California Range: $111,000 - $152,000
Colorado Range: $111,000 - $152,000
New York City/ Valhalla Range: $111,000 - $152,000
Washington Range: $111,000 - $152,000
About Us
BDO delivers assurance, tax, digital technology solutions and financial advisory services to clients throughout the country and around the globe. We offer numerous industry-specific practices, world-class resources, and an unparalleled commitment to meeting our clients’ needs. We currently serve more than 400 publicly traded domestic and international clients.
Unparalleled partner-involvement
Deep industry knowledge and participation
Geographic coverage across the U.S.
Cohesive global network
Focused capabilities across disciplines
BDO brings world-class resources and exceptional service to each and every one of our clients. BDO USA is a member of BDO International, the world’s fifth largest accounting network.
BDO offers a competitive Total Rewards package that encompass so much more than – “traditional benefits”. Our wide range of rewards and our employees’ ability to customize rewards to their individual needs are two of the reasons why BDO has been honored with so many workplace awards, including 100 Best Companies for Working Parents, Working Mother 100 Best Companies, Top Entry Level Employer, 2022 National Best & Brightest Companies to Work For and more.
Some Examples Of Our Total Rewards Offerings Include
Competitive pay and eligibility for an annual performance bonus.
A 401k plan plus an employer match
Comprehensive, medical, dental, vision, FSA, and prescription insurance from day one
Competitive Paid Time Off with daily accrual from day one of employment, plus paid holidays
Paid Parental Leave
Adoption Assistance
Firm paid life insurance
Wellness programs
Additional offerings include BDO Flex, Group Legal insurance, Pet insurance and Long-Term Care Insurance
Above offerings may be subject to eligibility requirements.
Click here to find out more!
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability or protected veteran status.
""BDO USA, P.A. is an EO employer M/F/Veteran/Disability""
Show more
Show less","Data Analytics, Business Intelligence, Artificial Intelligence, Application Development, SQL, Data Warehousing, Data Modeling, Semantic Model Definition, Star Schema Construction, Cloud Data Analytics, C#, Python, Java, Scala, Tabular Modeling, Microsoft Fabric, Power BI, Azure Analysis Services, Git, DevOps, Linux, Data Lake Medallion Architecture, Batch Data Ingestion, Streaming Data Ingestion, AI Algorithms, Machine Learning, Automation Tools, UiPath, Alteryx, Computer Vision, DataOps, Purview, Delta, Pandas, Spark SQL, SSIS, SSAS, SSRS, PySpark, dbt, Terraform, Bicep, Glue, Star Schema, Data Pipeline, Athena","data analytics, business intelligence, artificial intelligence, application development, sql, data warehousing, data modeling, semantic model definition, star schema construction, cloud data analytics, c, python, java, scala, tabular modeling, microsoft fabric, power bi, azure analysis services, git, devops, linux, data lake medallion architecture, batch data ingestion, streaming data ingestion, ai algorithms, machine learning, automation tools, uipath, alteryx, computer vision, dataops, purview, delta, pandas, spark sql, ssis, ssas, ssrs, pyspark, dbt, terraform, bicep, glue, star schema, data pipeline, athena","ai algorithms, alteryx, application development, artificial intelligence, athena, automation tools, azure analysis services, batch data ingestion, bicep, business intelligence, c, cloud data analytics, computer vision, data lake medallion architecture, data pipeline, dataanalytics, datamodeling, dataops, datawarehouse, dbt, delta, devops, git, glue, java, linux, machine learning, microsoft fabric, pandas, powerbi, purview, python, scala, semantic model definition, spark, spark sql, sql, ssas, ssis, ssrs, star schema, star schema construction, streaming data ingestion, tabular modeling, terraform, uipath"
Data Engineer IV - Max Digital (Data Operations),ACV Auctions,"Indiana, United States",https://www.linkedin.com/jobs/view/data-engineer-iv-max-digital-data-operations-at-acv-auctions-3762877420,2023-12-17,Carmel,United States,Mid senior,Hybrid,"If you are looking for a career at a dynamic company with a people-first mindset and a deep culture of growth and autonomy, ACV is the right place for you! Competitive compensation packages and learning and development opportunities, ACV has what you need to advance to the next level in your career. We will continue to raise the bar every day by investing in our people and technology to help our customers succeed. We hire people who share our passion, bring innovative ideas to the table, and enjoy a collaborative atmosphere.
Who we are:
ACV is a technology company that has revolutionized how dealers buy and sell cars online. We are transforming the automotive industry. ACV Auctions Inc. (ACV), has applied innovation and user-designed, data driven applications and solutions. We are building the most trusted and efficient digital marketplace with data solutions for sourcing, selling and managing used vehicles with transparency and comprehensive insights that were once unimaginable. We are disruptors of the industry and we want you to join us on our journey. ACV’s network of brands includes ACV Auctions, ACV Transportation, ClearCar, MAX Digital and ACV Capital within its Marketplace Products, as well as, True360 and Data Services.
At ACV we focus on the Health, Physical, Financial, Social and Emotional Wellness of our Teammates and to support this we offer:
Multiple medical plans including a high deductible health plan that costs $0 out of your paycheck
Company-sponsored (paid) Short-Term Disability, Long-Term Disability, and Life Insurance
Comprehensive optional benefits such as Dental, Vision, Supplemental Life/AD&D, Legal/ID Protection, and Accident and Critical Illness Insurance
Generous paid time off options, including vacation time, sick days, Company holidays, floating holidays, parental leave, bereavement leave, jury duty leave, voting leave, and other forms of paid leave as required by applicable law or regulation
Employee Stock Purchase Program with additional opportunities to earn stock in the Company
Retirement planning through the Company’s 401(k)
Who we are looking for:
We are seeking a highly skilled Engineer IV in Data Engineering with a strong foundation in computer science and excellent problem-solving skills. You will be responsible for maintaining and extending our database operations, optimizing SQL queries, and designing scalable data services.
What you will do:
Actively and consistently support all efforts to simplify and enhance the customer experience.
Maintain and extend (as required) existing database operations solution for backups, index defragmentation, data retention, etc.
Troubleshoot any SQL Server or ETL stack (SSIS, C#, Web APIs) outages during our operational support window.
Leverage monitoring tools to ensure high performance and availability; work with operations and engineering to improve as required.
Leverage DMVs and monitoring tools to ensure system performance; work with data operations and engineering to improve as required.
Ensure existing HADR (availability groups) solution is functional and meets requirements.
Support development, integration, and stage SQL Server environments for application development and data science teams.
Ensure that new database development meets company standards for readability, reliability, and performance. Work with internal teams on transactional and analytical schema design.
Collaborate with software and DevOps engineers to design scalable services, plan feature roll-out, and ensure high reliability and performance of our products.
Conduct code reviews, develop high-quality documentation, and build robust test suites.
Own the overall performance of products and services within a defined area of focus.
Respond-to and troubleshoot highly complex problems quickly, efficiently, and effectively. This may include being part of the emergency after-hours on-call rotation.
Mentor junior data engineers.
Perform additional duties as assigned.
What you will need:
Bachelor's degree in Computer Science, Information Technology, Computer Information Systems, Management Information Systems, or similar
5 years' building & supporting the database-tier of SaaS web applications.
Ability to read, write, speak, and understand English.
Expert in SQL Query optimization
ETL workflow implementation (SSIS, Airflow, C#, Python)
Experience working with Cloud Services (AWS RDS, S3, SQS, SNS)
Experience working with NoSQL data stores (e.g., MongoDB)
Experience developing Windows services in C#
Experience writing unit and integration testing
Expert SQL and data-layer development experience; OLTP schema design.
Experience using and integrating with cloud services, specifically: AWS RDS, S3, SQS, SNS.
Nice to Have
Experience with Airflow
Experience with DBT
Our Values
Trust & Transparency | People First | Positive Experiences | Calm Persistence | Never Settling
At ACV, we are committed to an inclusive culture in which every individual is welcomed and empowered to celebrate their true selves. We achieve this by fostering a work environment of acceptance and understanding that is free from discrimination. ACV is committed to being an equal opportunity employer regardless of sex, race, creed, color, religion, marital status, national origin, age, pregnancy, sexual orientation, gender, gender identity, gender expression, genetic information, disability, military status, status as a veteran, or any other protected characteristic. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you have a disability or special need that requires reasonable accommodation, please let us know.
For information on our collection and use of your personal information, please see our Privacy Notice.
Apply Now
Show more
Show less","SQL, SQL Server, SSIS, C#, Python, Airflow, DBT, MongoDB, AWS RDS, S3, SQS, SNS, ETL, OLTP, Data Engineering","sql, sql server, ssis, c, python, airflow, dbt, mongodb, aws rds, s3, sqs, sns, etl, oltp, data engineering","airflow, aws rds, c, data engineering, dbt, etl, mongodb, oltp, python, s3, sns, sql, sql server, sqs, ssis"
Data Warehouse Engineer Lead/Senior,"Anveta, Inc","Indianapolis, IN",https://www.linkedin.com/jobs/view/data-warehouse-engineer-lead-senior-at-anveta-inc-3716361836,2023-12-17,Carmel,United States,Mid senior,Hybrid,"Role: IN - DWD Data Warehouse Engineer Lead/Senior (719654)
Location: Indianapolis, IN 46204--- Webcam only---Hybrid
Duration: Long term
Client: State of Indiana
DWD is just getting started with our data warehouse and dashboarding strategies. We have a strong understanding of the data but are looking for expertise in getting that data into our Azure Synapse subscription. Through another project the data warehouse has been created.
The Essential Functions Of This Role Are As Follows
Work with various IT teams (data, database and infrastructure) in analyzing the agency's use of the data warehouse.
Provide guidance and education on warehouse environments using industry standards and best practices.
Research leading edge capabilities, within the Indiana Office of Technology purview and guidelines, to determine the best solution for the overall environment.
Set technology architectural direction, delivering assessment recommendations.
Develop a project/implementation plan for setting up the warehouse including ideal data pipelines and other data and visual interfaces.
Follow through on setting up the agency's warehouse environment as per the agency's initiative.
Configure and setup Azure Synapse with correct structure.
Participate with Data Analyst(s) as needed to define minimal viable data assets in support of the visual needs.
Review and provide architectural guidance for all analytic solutions.
Advise management as needed of any shortcomings with current tools and/or approaches related to the business initiatives
The job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee. Other duties, responsibilities and activities may change or be assigned at any time.
Thanks & Regards
Charan
Anveta, Inc.
1333 Corporate Drive, Suite #108
Irving, TX 75038
charan@anveta.com
https://www.linkedin.com/in/charan-reddy-ba6450236/
Show more
Show less","Azure Synapse, Data Warehousing, Data Visualization, Data Analysis, Cloud Computing, Data Pipelines, Data Structures, Data Architecture, Data Analytics, Data Management, Business Intelligence, SQL","azure synapse, data warehousing, data visualization, data analysis, cloud computing, data pipelines, data structures, data architecture, data analytics, data management, business intelligence, sql","azure synapse, business intelligence, cloud computing, data architecture, data management, data structures, dataanalytics, datapipeline, datawarehouse, sql, visualization"
"Data Scientist - Indianapolis, IN (NEED ONLY LOCALS)",Spanco Solutions,"Indianapolis, IN",https://www.linkedin.com/jobs/view/data-scientist-indianapolis-in-need-only-locals-at-spanco-solutions-3748462503,2023-12-17,Carmel,United States,Mid senior,Hybrid,"Data Scientist
Indianapolis, IN (NEED ONLY LOCALS)
Essential Duties/Responsibilities
Represent IOT in meetings and other events with technical and non-technical agency stakeholders to understand, analyze, and make recommendations regarding the customer's data transparency needs.
Gather data, experiment, and develop automation processes to improve the agency customer experience.
Use customer-based feedback and best practices to design, develop, and implement dashboards and other visual materials to support customer billing and data transparency.
Present IOT recommendations and products to stakeholders and customers at all levels.
Create, maintain, and revise metric reports and key performance indicators associated with the business area to inform IOT chief executives.
Be technically and quantitatively proficient, while also capable of delivering qualitative strategic advice and recommendation to senior leaders.
Create, review, update, and enhance IOT's Data Driven Decision Portal (D3P) and will provide updates on this resource to agency and customer stakeholders.
Become a subject matter expert in practices and processes associated with current IOT data collection, retention, and dissemination.
Job Requirements
Knowledge using statistical computer languages (R, Python, SLQ, etc.) to manipulate and draw insights from large data sets
Strong problem-solving skills with an emphasis on data driven solutions
Excellent written and verbal communication skills for coordinating across teams
Knowledge of project management principles
Ability to analyze policies and procedures and be able to recommend improvements
Ability to work independently, anticipate problems and initiate corrective actions
Ability to effectively prioritize a variety of projects and functions
Ability to research and document findings
Ability to establish and maintain effective working relationships
DIFFICULTY OF WORK
Work is broad in scope involving complex program elements, which require critical and analytical abilities. Employee receives a general outline of work and is expected to develop own methods. Work is diversified and involves multiple unrelated steps. Analytical thought is required to deal with complex data and situations. Employee exercises considerable independent judgment in formulating solutions, and in interpreting large data sets.
RESPONSIBILITY
Employee is responsible for collecting, analyzing, and interpreting large data sets to be used in identifying issues and developing corresponding solutions. Employee is also responsible for the development of data modeling solutions provided to agency partners and stakeholders as well as continued monitoring of the effectiveness of the modeling solutions and assurance that they meet customer requirements.
PERSONAL WORK RELATIONSHIPS
Internal, statewide contacts such as agency heads, directors and managers are for the purpose of planning, directing, managing, gathering information, problem solving, team building, influencing, motivating, presentation of recommendations, negotiating and implementation of programs.
Show more
Show less","Data Analysis, Data Visualization, DataDriven Decision Making, Statistical Programming, R, Python, SQL, Project Management, Policy Analysis, Problem Solving, Communication, Independent Work, Prioritization, Research, Relationship Building","data analysis, data visualization, datadriven decision making, statistical programming, r, python, sql, project management, policy analysis, problem solving, communication, independent work, prioritization, research, relationship building","communication, dataanalytics, datadriven decision making, independent work, policy analysis, prioritization, problem solving, project management, python, r, relationship building, research, sql, statistical programming, visualization"
Data Analyst //Pay rate: $43.35/hr,Stellar Professionals,"Indianapolis, IN",https://www.linkedin.com/jobs/view/data-analyst-pay-rate-%2443-35-hr-at-stellar-professionals-3683236716,2023-12-17,Carmel,United States,Mid senior,Hybrid,"Applicant must have 5 years of relevant experience with the following:
Bachelor's degree in computer science or related field, or equivalent experience.
Experience using Excel Data analysis Functions, scripts, reports, and charts
Experience with Data Preparation for Reports and Dashboards
Business Intelligence experience, Data Warehouse and Reporting
SQL and PL/SQL
Experience establishing Connections with different Data Sources
Tableau Platform (Tableau Desktop, Tableau Prep Builder, Tableau Server)
Experience with ETL frameworks and online analytical processing (OLAP)
Python and R Languages
Experience with Mainframe and DB2 Z/OS
Experience with Salesforce
Experience with MuleSoft
Show more
Show less","Excel, Data analysis, Data preparation, Business intelligence, Data warehouse, Reporting, SQL, PL/SQL, Data sources, Tableau Platform, ETL frameworks, OLAP, Python, R, Mainframe, DB2 Z/OS, Salesforce, MuleSoft","excel, data analysis, data preparation, business intelligence, data warehouse, reporting, sql, plsql, data sources, tableau platform, etl frameworks, olap, python, r, mainframe, db2 zos, salesforce, mulesoft","business intelligence, data preparation, data sources, dataanalytics, datawarehouse, db2 zos, etl frameworks, excel, mainframe, mulesoft, olap, plsql, python, r, reporting, salesforce, sql, tableau platform"
Data analyst// Pay rate: $43.20/hr,Stellar Professionals,"Indianapolis, IN",https://www.linkedin.com/jobs/view/data-analyst-pay-rate-%2443-20-hr-at-stellar-professionals-3662338046,2023-12-17,Carmel,United States,Mid senior,Hybrid,"Applicant must have 4 years of relevant experience with the following:
Tableau dashboard development
Tableau server knowledge
Recent experience working with one or more BI platforms (ex: Power BI or Tableau)
Data cleansing experience
Basic understanding of SQL
Experience with data validation and data integrity
Experience working in Salesforce reporting
Bachelor's Degree
Independent thinking and an aptitude for problem solving
Ability to tactfully deal with wide variety of stakeholders to resolve problems
Exceptional written/oral communication and presentation/interpersonal skills
Exceptional team player
Visual and artistic creative skills
Able to perform well in a fast-paced environment with high levels of weekly task variation
Ability to interact with all levels of state government
Knowledge of Child Welfare
Show more
Show less","Tableau, Tableau Server, Power BI, Data Cleansing, SQL, Data Validation, Data Integrity, Salesforce Reporting, Child Welfare","tableau, tableau server, power bi, data cleansing, sql, data validation, data integrity, salesforce reporting, child welfare","child welfare, data integrity, data validation, datacleaning, powerbi, salesforce reporting, sql, tableau, tableau server"
Statistical Data Reporting Analyst,Group 1001,"Zionsville, IN",https://www.linkedin.com/jobs/view/statistical-data-reporting-analyst-at-group-1001-3659429385,2023-12-17,Carmel,United States,Mid senior,Hybrid,"Group 1001
is a consumer-centric, technology-driven family of insurance companies on a mission to deliver outstanding value and operational performance by combining financial strength and stability with deep insurance expertise and a can-do culture. Group1001’s culture emphasizes the importance of collaboration, communication, core business focus, risk management, and striving for outcomes. This goal extends to how we hire and onboard our most valuable assets – our employees.
Company Overview:
Clear Spring Property and Casualty is a leading provider of property and casualty insurance. Our future-forward strategy is rooted in deep industry knowledge and a mastery of emerging technologies. Clear Spring Property and Casualty Company is a subsidiary of Delaware Life Insurance Company and its parent company, Group1001. Our relationship with these organizations has yielded an innovative technology platform that provides smarter risk management solutions for property and casualty insurance brokers. Clear Spring is dedicated to Group1001's mission: making insurance more useful, intuitive and accessible for everyone. We’re confident in our forward-thinking products, and that you’ll love working with us.
Group 1001, and its affiliated companies, is strongly committed to providing a supportive work environment where employee differences are valued. Diversity is an essential ingredient in making Group 1001 a welcoming place to work and is fundamental in building a high-performance team. Diversity embodies all the differences that make us unique individuals. All employees share the responsibility for maintaining a workplace culture of dignity, respect, understanding and appreciation of individual and group differences.
Job Summary
The Statistical Data Reporting Analyst will be primarily responsible for supporting the P&C Division and actuarial department with a wide variety of statistical reporting processes to ensure compliance.
Main Accountabilities
Researches, formulates, and communicates simple-to-moderate data for statistical and regulatory reporting solutions.
Assists with data call requirements/filing forms for regulatory agencies and/or state insurance departments. Ensures required data is reported to reporting bureau accurately and timely and recommends corrective action when required.
Stays current with statistical manuals, bureau filing requirements, statistical plans, data requirements, and regulatory reporting items (i.e. advanced knowledge IAIABC).
Maintain, collect and compile the information required to be reported to regulators.
Research, analyze and understand complex regulatory bulletins, circulars and data requests to complete data submissions.
Ensure statistical information is complete and accurate.
Develops, writes, and participates in testing scenarios based on analysis to business process or product changes. Participates in investigations and resolution of fines or inquiries from regulatory reporting bureaus.
Assists in the design and development of new reporting processes and trains staff to maintain processing once implemented.
Qualifications
Associate degree in business or related field. Equivalent industry experience may be considered.
3+ years of Property and Casualty insurance industry experience.
Knowledge of commercial P&C insurance products and insurance department requirements and processes.
Strong communication, analytical, and problem-solving skills.
Prior experience with statistical plans, edit definitions, and submission processes highly desirable.
Prior experience with ISO, NCCI, and independent state bureaus and analysis of circulars, regulations, and statutes highly desirable.
Compensation:
Our compensation reflects the cost of labor across several U.S. geographic markets. The base pay for this position ranges from $ $70,200/year in our lowest geographic market up to $140,200/year in our highest geographic market. Pay is based on a number of factors including market location and may vary depending on job-related knowledge, skills, and experience.
Benefits:
Employees (and their families) are eligible to participate in the Company’s comprehensive health, dental, and vision insurance plan options. Employees are also eligible for Basic and Supplemental Life Insurance, Short and Long-Term Disability, and to enroll in the Company’s Employee Assistance Program and other wellness initiatives. Employees may also participate in the Company’s 401K plan, with matching contributions by the Company.
Show more
Show less","Statistical data analysis, Data reporting, Compliance, Regulatory reporting, Statistical manuals, Data requirements, Regulatory reporting items, Statistical information, ISO, NCCI, Independent state bureaus, Circulars, Regulations, Statutes, Communication, Analytical skills, Problemsolving, Statistical plans, Edit definitions, Submission processes, Commercial P&C insurance products, Insurance department requirements, Processes","statistical data analysis, data reporting, compliance, regulatory reporting, statistical manuals, data requirements, regulatory reporting items, statistical information, iso, ncci, independent state bureaus, circulars, regulations, statutes, communication, analytical skills, problemsolving, statistical plans, edit definitions, submission processes, commercial pc insurance products, insurance department requirements, processes","analytical skills, circulars, commercial pc insurance products, communication, compliance, data reporting, data requirements, edit definitions, independent state bureaus, insurance department requirements, iso, ncci, problemsolving, processes, regulations, regulatory reporting, regulatory reporting items, statistical data analysis, statistical information, statistical manuals, statistical plans, statutes, submission processes"
Data Analyst(Candidate must currently reside in the state of IN),"Veridian Tech Solutions, Inc.","Indianapolis, IN",https://www.linkedin.com/jobs/view/data-analyst-candidate-must-currently-reside-in-the-state-of-in-at-veridian-tech-solutions-inc-3683265389,2023-12-17,Carmel,United States,Mid senior,Hybrid,"ID-717178
Title-Data Analyst
Loction-Hybrid work in Indianapolis, IN 46204
Duration-6 Months contract with the possibility of extension.
Resource will be responsible for the activities of Data Analyst for the State of Indiana's child support system.
Candidate must currently reside in the state of IN.
Job Duties
Design, develop, and format polished data visualizations (reports and dashboards) to support business requirements. Focus on Visualization Creation using data to perform reporting and direct analysis. Perform analysis that may be descriptive, diagnostic, predictive, or prescriptive. Responsible for maintaining dashboards, generating reports, preparing data visualizations, and using data to forecast or guide business activity. Present data in a fashion that is easy to understand with proper documentation and user testing for successful adoption. Publish workbooks to appropriate QA, production, and public servers. Meet project deadlines and requirements. Participate in and contribute to the CSB-IT Data Services Team. Performs other related duties as assigned.
Job Requirements
Experience with Analyzing data, identify trends, interpret results, and prepare reports and data visualizations for bureau leadership, county partners, and other stakeholders. Experience with creating Analytical, Time Series, Metrics, Rankings, Statistical, Christmas Tree, Tracking, and Expenditure reports. Experience with using excel data analysis functions, scripts, reports, and charts. Experience with writing SQL queries to get data from different databases like DB2 Z/OS; AWS Aurora and AWS RDS and Data Warehouse. Experience in working with Cross Technical, Functional and Business Teams. Capable to understand business requirements and develop reports based on requirements. Capable to work individually and work in a team environment.
Skills
Bachelor's degree in computer science or related field, or equivalent experience.-Required
Experience using Excel Data analysis Functions, scripts, reports, and charts-Required
Experience With Data Preparation For Reports And Dashboards-Required
Business Intelligence experience, Data Warehouse and Reporting-Required
SQL and PL/SQL-Required
Experience establishing Connections with different Data Sources
Tableau Platform (Tableau Desktop, Tableau Prep Builder, Tableau Server)-Desired
Experience with ETL frameworks and online analytical processing (OLAP)-Desired
Python and R Languages -desired
Experience with Mainframe and DB2 Z/OS-Desired
Experience with Salesforce-Desired
Experience with MuleSoft-Desired
Show more
Show less","Data Analysis, Data Visualization, Reporting, Dashboarding, Data Forecasting, Data Mining, Data Interpretation, SQL, PL/SQL, Tableau, ETL frameworks, OLAP, Python, R, Mainframe, DB2 Z/OS, Salesforce, MuleSoft","data analysis, data visualization, reporting, dashboarding, data forecasting, data mining, data interpretation, sql, plsql, tableau, etl frameworks, olap, python, r, mainframe, db2 zos, salesforce, mulesoft","dashboard, data forecasting, data interpretation, data mining, dataanalytics, db2 zos, etl frameworks, mainframe, mulesoft, olap, plsql, python, r, reporting, salesforce, sql, tableau, visualization"
Data Scientist,Parin Technologies,"Indianapolis, IN",https://www.linkedin.com/jobs/view/data-scientist-at-parin-technologies-3748446919,2023-12-17,Carmel,United States,Mid senior,Hybrid,"Role
: Data Scientist
Location
: Indianapolis, IN - Hybrid
Only candidates local to Indiana will be considered for this position.
Job Details
Works with agency partners and outside stakeholders to gather and analyze data regarding a variety of issues and topics in order to develop data driven solutions to difficult business challenges.
Create, maintain, and revise metric reports used for transparency. Monitor and troubleshoot data sets as needed.
The Data Scientist position will work with agency partners and outside stakeholders to gather and analyze data regarding a variety of issues and topics in order to develop data driven solutions to difficult business challenges.
Essential Duties/Responsibilities
Represent in meetings and other events with technical and non-technical agency stakeholders to understand, analyze, and make recommendations regarding the customer's data transparency needs.
Gather data, experiment, and develop automation processes to improve the agency customer experience.
Use customer-based feedback and best practices to design, develop, and implement dashboards and other visual materials to support customer billing and data transparency.
Present recommendations and products to stakeholders and customers at all levels.
Create, maintain, and revise metric reports and key performance indicators associated with the business area to inform chief executives.
Be technically and quantitatively proficient, while also capable of delivering qualitative strategic advice and recommendation to senior leaders.
Create, review, update, and enhance Data Driven Decision Portal (D3P) and will provide updates on this resource to agency and customer stakeholders.
Become a subject matter expert in practices and processes associated with current data collection, retention, and dissemination.
Job Requirements
Knowledge using statistical computer languages (R, Python, SLQ, etc.) to manipulate and draw insights from large data sets
Strong problem-solving skills with an emphasis on data driven solutions
Excellent written and verbal communication skills for coordinating across teams
Knowledge of project management principles
Ability to analyze policies and procedures and be able to recommend improvements
Ability to work independently, anticipate problems and initiate corrective actions
Ability to effectively prioritize a variety of projects and functions
Ability to research and document findings
Ability to establish and maintain effective working relationships
DIFFICULTY OF WORK
Work is broad in scope involving complex program elements, which require critical and analytical abilities. Employee receives a general outline of work and is expected to develop own methods. Work is diversified and involves multiple unrelated steps. Analytical thought is required to deal with complex data and situations. Employee exercises considerable independent judgment in formulating solutions, and in interpreting large data sets.
RESPONSIBILITY
Employee is responsible for collecting, analyzing, and interpreting large data sets to be used in identifying issues and developing corresponding solutions. Employee is also responsible for the development of data modeling solutions provided to agency partners and stakeholders as well as continued monitoring of the effectiveness of the modeling solutions and assurance that they meet customer requirements.
PERSONAL WORK RELATIONSHIPS
Internal, statewide contacts such as agency heads, directors and managers are for the purpose of planning, directing, managing, gathering information, problem solving, team building, influencing, motivating, presentation of recommendations, negotiating and implementation of programs.
Required/Desired Skills
Skill Required /Desired Amount of Experience Experience with the Microsoft Power BI Desktop/Service Required 2 Years Proficiency in designing data models that underpin reports and dashboards. Required 2 Years Experience with DAX, a formula language used in Power BI for calculations and data manipulation. Required 2 Years Skills in transforming raw data into a format suitable for analysis. Required 2 Years Experience in creating visually compelling and informative reports and dashboards. Required 2 Years Knowledge of connecting to various data sources, such as databases, cloud services, and APIs. Desired 0 Familiarity with SQL for querying databases. Desired 0 An ability to work with data, including data cleansing and transformation. Desired 0 The capacity to extract insights and trends from data. Desired 0 Translating data insights into understandable narratives for non-technical stakeholders. Desired 0 Experience with Microsoft Power BI external related tools Nice to have 0
Show more
Show less","Data Science, Statistical Computing, R, Python, SQL, Power BI, DAX, Data Analysis, Data Manipulation, Data Visualization, Data Modeling, Data Interpretation, Tableau, SAS, Hadoop, Hive, Spark, Machine Learning, Artificial Intelligence, Cloud Computing, Big Data, Data Mining, Data Warehousing","data science, statistical computing, r, python, sql, power bi, dax, data analysis, data manipulation, data visualization, data modeling, data interpretation, tableau, sas, hadoop, hive, spark, machine learning, artificial intelligence, cloud computing, big data, data mining, data warehousing","artificial intelligence, big data, cloud computing, data interpretation, data manipulation, data mining, data science, dataanalytics, datamodeling, datawarehouse, dax, hadoop, hive, machine learning, powerbi, python, r, sas, spark, sql, statistical computing, tableau, visualization"
Healthcare Data Analyst III (Remote capable),Geisinger,"Danville, PA",https://www.linkedin.com/jobs/view/healthcare-data-analyst-iii-remote-capable-at-geisinger-3783923542,2023-12-17,Bloomsburg,United States,Mid senior,Remote,"Job Summary
Uses data expertise, programing abilities, and critical thinking skills to provide analytic expertise, deliver innovative solutions, and act as a strategic thought partner. Responsible for supporting investigator driven research from inception to closeout by providing feedback on study design, data collection strategies, effort estimation, analytic strategies, and stewardship. Develops code to deliver high quality data products on time to customers. Mentors junior staff by offering project feedback and code review. Works with research staff to determine goals, priorities, and decision making. Participates on high visibility projects for both internal and external clients. Leads efforts to assess, recommend, implement, troubleshoot, validate, and interpret results. Experience in developing programs using large relational database models written in SQL and/or SAS required. Experience using data from an electronic health record or health care claims in the area of health research or healthcare analytics (preference for Epic’s Clarity models) required.
Job Duties
Performs complex data extraction, manipulation, and summarization of large databases to create analytical datasets and provides a range of solutions to support Research activities.
Takes lead role in the design, testing, validation, analysis and merging of complicated data structures from a wide variety of source systems.
Highly effective in written and oral communication of complex, analytical, technical information, results/recommendations to research project team.
Able to create summarized findings and recommendations that are clearly presented and adapted for audiences that have a varying range of technical and clinical experience.
Actively engages and takes lead role in the development of complex phenotype algorithms and increasingly significant tasks.
Acts as a strategic partner for investigators to identify key areas and priorities based on analytic insights and data analysis.
Utilizes extensive knowledge of research methodology and research data collection to drive discussions with clients and the team.
Leads discussions that proactively educate the requester of any complexities associated with the data request.
Oversees, leads and consults on all phases of SAS/SQL/analytical programming, data management, quality control, and reporting.
Expert ability to identify and resolve problems using knowledge, background and troubleshooting skills.
Leads efforts to ensure accuracy, data integrity and validity of data and analysis in all work.
Work is typically performed in an office environment. Accountable for satisfying all job specific obligations and complying with all organization policies and procedures. The specific statements in this profile are not intended to be all-inclusive. They represent typical elements considered necessary to successfully perform the job.
Relevant experience may be a combination of related work experience and/or Master's degree obtained (Master's Degree = 2 years relevant experience).
Position Details
Education
Bachelor's Degree-Computer Science, Information Systems, Information Science (Required)
Experience
Minimum of 5 years-Relevant experience* (Required)
Certification(s) and License(s)
OUR PURPOSE & VALUES: Everything we do is about caring for our patients, our members, our students, our Geisinger family and our communities. KINDNESS: We strive to treat everyone as we would hope to be treated ourselves. EXCELLENCE: We treasure colleagues who humbly strive for excellence. LEARNING: We share our knowledge with the best and brightest to better prepare the caregivers for tomorrow. INNOVATION: We constantly seek new and better ways to care for our patients, our members, our community, and the nation. SAFETY: We provide a safe environment for our patients and members and the Geisinger family We offer healthcare benefits for full time and part time positions from day one, including vision, dental and domestic partners. Perhaps just as important, from senior management on down, we encourage an atmosphere of collaboration, cooperation and collegiality. We know that a diverse workforce with unique experiences and backgrounds makes our team stronger. Our patients, members and community come from a wide variety of backgrounds, and it takes a diverse workforce to make better health easier for all. We are proud to be an affirmative action, equal opportunity employer and all qualified applicants will receive consideration for employment regardless to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or status as a protected veteran.
Show more
Show less","Data expertise, Programming skills, Critical thinking, SQL, SAS, Data extraction, Data manipulation, Data summarization, Data structures, Data analysis, Data merging, Written communication, Oral communication, Technical writing, Presentation skills, Phenotype algorithms, Research methodology, Research data collection, Data management, Quality control, Data reporting, Problemsolving, Troubleshooting, Data integrity, Data validity, Bachelor's DegreeComputer Science, Information Systems, Information Science, 5 yearsRelevant experience*","data expertise, programming skills, critical thinking, sql, sas, data extraction, data manipulation, data summarization, data structures, data analysis, data merging, written communication, oral communication, technical writing, presentation skills, phenotype algorithms, research methodology, research data collection, data management, quality control, data reporting, problemsolving, troubleshooting, data integrity, data validity, bachelors degreecomputer science, information systems, information science, 5 yearsrelevant experience","5 yearsrelevant experience, bachelors degreecomputer science, critical thinking, data expertise, data extraction, data integrity, data management, data manipulation, data merging, data reporting, data structures, data summarization, data validity, dataanalytics, information science, information systems, oral communication, phenotype algorithms, presentation skills, problemsolving, programming skills, quality control, research data collection, research methodology, sas, sql, technical writing, troubleshooting, written communication"
"Summer Intern - Operations Services, Data Analytics",The Port Authority of New York & New Jersey,"Jersey City, NJ",https://www.linkedin.com/jobs/view/summer-intern-operations-services-data-analytics-at-the-port-authority-of-new-york-new-jersey-3786580695,2023-12-17,Morristown,United States,Associate,Onsite,"Description
The Port Authority of New York & New Jersey offers paid internships, providing students with the opportunity to work with experienced professionals on assignments critical to building, operating, and maintaining transportation and infrastructure assets in New York and New Jersey. Assignments include hands-on projects where interns quickly become part of the team and help the Agency with its mission of keeping the region moving. The diversity of our businesses creates a variety of opportunities for students to gain exposure to the work of a first-class public agency.
The Operations Services Department is looking for a Data Analytics intern to join the Data and Operational Performance team in the Director's Office!
About the Operations Services Department & Data and Operational Performance Division:
The Operations Services Department (OSD) is the Port Authority’s centralized operations and maintenance service provider for all lines of business. OSD provides a diverse portfolio of services to clients throughout the agency, including: procurement, maintenance, and management of the Agency’s automotive fleet; operation of Agency cafeteria facilities; management of print production and internal mail services; in-house construction, maintenance, and communication systems support; office space management; and development and administration of operations and maintenance training programs for Port Authority employees.
The Data and Operational Performance (DOP) team leverages data and technology in support of these services. Specifically, the DOP team is responsible for modernizing and maintaining OSD’s technology systems, enhancing the efficacy and efficiency of departmental business processes via technology, and collecting, managing, and analyzing data to support data-driven decision-making within the department. DOP works closely with stakeholders throughout OSD (and the Port Authority) to develop tools and systems that make it easier to manage and access services provided by OSD.
Responsibilities:
Provide support for ongoing initiatives to modernize OSD technologies and processes
Support continued build-out of Azure data warehouse for various departmental data streams
Develop analyses, reports, and dashboards to monitor performance on departmental priorities
Engage with OSD stakeholders to understand and document business needs and identify how technology can meet these needs
Assess existing OSD technologies, tools and processes and identify opportunities for improvement
Extensively leverage Microsoft Power Platform (and other tools) in support of all the above
Qualifications:
Enrollment in an undergraduate or graduate program through the Fall 2024 semester (August 2024 or beyond)
Experience with Microsoft apps such as Word, Excel, PowerPoint, Outlook, Teams, SharePoint, OneDrive, etc.
Excellent written and oral communication skills
Interest in contributing to the success of the New York and New Jersey region
Interest in public service and/or transportation
Willingness to learn
Desired:
Academic or professional experience in data science, data engineering, user experience, and/or database management
Experience with (or willingness to learn) Microsoft Power Platform and other Microsoft tools like Power Automate, Azure, PowerBI, PowerApps, and Forms
Strong consideration for user experience and ability to creatively identify how different technologies may be able to meet these needs
Willingness to document any tools or processes developed during the internship
Excellent attention to detail
Excellent problem-solving and troubleshooting skills
Knowledge of SQL, Python, or other programming languages
Internship Details:
The start date will be Thursday, May 30 with an orientation that will take place at 4 World Trade Center in Lower Manhattan
The internship will last 12 weeks, ending on Friday, August 23
This is a full-time internship (generally 5 days per week at 7.25 hours of work per day plus a 45-minute break)
The format is expected to be hybrid, partially virtual and partially in-person at the Port Authority Technical Center in Jersey City, NJ
The hourly salary range is $17.20 to $28.85 (the hourly salary is determined by internship type and school year)
These details are subject to change
How To Apply:
Click “Apply Now”
Follow the prompts
Please submit the following with your application:
A resume that clearly indicates your expected graduation date
A cover letter detailing why you want to intern at The Port Authority of New York & New Jersey
How To Apply:
Interested candidates should apply to this job by clicking on the ""Apply Now"" button and submitting a combined cover letter and resume. The Port Authority of NY & NJ welcomes veteran and military spouse applications.
Only applicants under consideration will be contacted.
About The Port Authority:
Founded in 1921, the Port Authority of New York and New Jersey builds, operates, and maintains many of the most important transportation and trade infrastructure assets in the country. The agency's network of aviation, ground, rail, and seaport facilities is among the busiest in the country, supports more than 550,000 regional jobs, and generates more than twenty three billion in annual wages and eighty billion in annual economic activity. The Port Authority also owns and manages the 16-acre World Trade Center site, where the 1,776-foot-tall One World Trade Center is now the tallest skyscraper in the Western Hemisphere.
Equal Opportunity Employer
The Port Authority of New York & New Jersey/Port Authority Trans-Hudson (PATH) is an Equal Opportunity Employer.
Show more
Show less","Microsoft Azure, Power Automate, Azure Synapse, Power BI, PowerApps, SharePoint, MS Teams, MS Office 365, SQL, Python, Data Analysis, Data Science, User Experience, Technical Writing, Problem Solving, Troubleshooting","microsoft azure, power automate, azure synapse, power bi, powerapps, sharepoint, ms teams, ms office 365, sql, python, data analysis, data science, user experience, technical writing, problem solving, troubleshooting","azure synapse, data science, dataanalytics, microsoft azure, ms office 365, ms teams, power automate, powerapps, powerbi, problem solving, python, sharepoint, sql, technical writing, troubleshooting, user experience"
"Data Analyst, Print Contract Management Specialist","KYOCERA Document Solutions America, Inc.","Fairfield, NJ",https://www.linkedin.com/jobs/view/data-analyst-print-contract-management-specialist-at-kyocera-document-solutions-america-inc-3778726611,2023-12-17,Morristown,United States,Associate,Hybrid,"When you join Kyocera Document Solutions America, Inc. you are joining an organization that is deeply rooted in the Philosophy of “doing what is right as a human being.” Through this Philosophy, our employees are passionate about providing best in class customer service through efficiency, accountability, and a sense of urgency all while putting knowledge to work to drive change.
In addition, we’re a Great Place to Work… and we really mean it! Kyocera Document Solutions America, Inc. has been officially certified a Great Place to Work since 2021. An organization earns this distinction when its employees have expressed their trust in the people they work for, have pride in what they do and enjoy the people they work with.
As a Data Analyst – Print Contract Management Specialist, you will be responsible for developing and implementing data analyses on the profitability of the Direct Sales Service Contracts. Additionally, you will be responsible for identifying issues within each operation and developing solutions to ensure contract profitability. The position is also responsible for overseeing the Maintenance Contract Process within the Direct Channel to include Service Contract creation, termination, and invoicing as well as the meter collection process to ensure invoicing is completed accurately and in a timely manner.
The chance to do something meaningful, to challenge yourself, to be a part of change in an industry, to influence change doesn’t come around every day.
Responsibilities:
+ Identify underperforming machines and service contracts across the Direct Subsidiaries’ fleet of machines.
+ Develop reporting that proactively notifies account managers and Subsidiary Leadership of low performing machines.
+ Develop corrective action plans to improve profitability of low performing machines and track progress.
+ Ensure contracts bases, escalations, and other critical terms are in place and enforced.
+ Provide comprehensive visibility of the fleet including progress reporting and customer responses.
+ Define new process improvement opportunities related to contract management and enforcement.
+ Field questions and inquiries from Direct Channel staff regarding service contacts.
+ Interface with the Direct Channel Contract Administrators to ensure corrective actions are implemented.
Qualifications
Required:
+ Strong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information.
+ Strong knowledge of and experience with eAutomate.
+ Strong knowledge of and experience with CEOJuice.
+ Proven strong problem solving skills.
+ Proven working experience as a Data Analyst or Business Data Analyst.
+ Ability to create and present impactful presentations that effectively deliver strategies, results, issues, progress updates.
+ Advanced knowledge of Excel including the ability to use Pivot Tables, V Look Ups, and Calculations, to efficiently process large quantities of data.
+ Excellent computer skills, strong interpersonal and administrative skills.
+ Excellent verbal and written communication.
+ Proficiency with the English language is required.
+ Ability to interact with all levels of management.
+ Physical requirements: sitting and working with computer, bending, filing, and light lifting.
Preferred:
+ Bachelor’s degree.
+ 5 Years Experience within the office product industry.
Note
This is a general description of the duties and responsibilities most frequently required of this position. The company may from time-to-time request that the incumbent perform other related tasks and assume reasonable responsibilities that have not been specifically included in this description.
Kyocera Document Solutions America, Inc is a group company of Kyocera Document Solutions Inc., a global leading provider of total document solutions based in Osaka, Japan. The company’s portfolio includes reliable and eco-friendly MFPs and printers, as well as business applications and consultative services which enable customers to optimize and manage their document workflow, reaching new heights of efficiency. With professional expertise and a culture of empathetic partnership, the objective of the company is to help organizations put knowledge to work to drive change. Kyocera is looking for enthusiastic and innovative people to help our customers run their businesses more efficiently and more profitably. We offer a generous benefits package including medical, dental and vision plans, a 401k match, flexible spending, disability and life insurance, plus paid time off and holidays.
KYOCERA Document Solutions America, Inc. is an Equal Opportunity Employer, a VEVRRA Federal Contractor, and complies with all applicable federal, state, and local laws regarding nondiscrimination. Kyocera provides equal employment opportunities to all employees and applicants for employment without regard to race, color, religion, gender, sexual orientation, national origin, age, disability, or protected veteran status. Qualified minorities, women, protected veterans and/or individuals with disabilities are encouraged to apply.
#KDS123
Show more
Show less","Data Analysis, eAutomate, CEOJuice, Excel, Pivot Tables, V Look Ups, Calculations, Business Data Analysis, Presentations, Management","data analysis, eautomate, ceojuice, excel, pivot tables, v look ups, calculations, business data analysis, presentations, management","business data analysis, calculations, ceojuice, dataanalytics, eautomate, excel, management, pivot tables, presentations, v look ups"
Sr. Data Engineer,Advanced Knowledge Tech LLC,"Jersey City, NJ",https://www.linkedin.com/jobs/view/sr-data-engineer-at-advanced-knowledge-tech-llc-3667471987,2023-12-17,Morristown,United States,Mid senior,Onsite,"Role:
Sr. Data Engineer / Data Engineer
Location:
Columbus, OH, Jersey City, NJ and Houston, TX.
Remote Preference:
Flexible between 4-6 weeks
Experience Level:
8+ years
Job Description
These resources are AWS Glue developers who will develop the pipelines required for data extraction, ingestion, and transformation for the data lake. These resources will develop the pipeline based on the reverse engineering information provided by the Sr ETL developer. This role will do the development and unit testing of the pipelines for the target state.
Pure Big Data sr developers on AWS Cloud : design build pipelines ( batch & streaming) in Apache Spark, AWS Glue, EMR , Kafka, Airflow etc
10+ for sr data engineer with data lake architecture, design experience.
5+ for data engineer with strong spark skills.
Show more
Show less","Apache Spark, AWS Glue, EMR, Kafka, Airflow, Data lake architecture, Data engineering, Data transformation, Data extraction, Data ingestion, Pipeline development, Pipeline testing, Unit testing, Big data","apache spark, aws glue, emr, kafka, airflow, data lake architecture, data engineering, data transformation, data extraction, data ingestion, pipeline development, pipeline testing, unit testing, big data","airflow, apache spark, aws glue, big data, data engineering, data extraction, data ingestion, data lake architecture, data transformation, emr, kafka, pipeline development, pipeline testing, unit testing"
Junior Data Engineer,TickPick,"New York, United States",https://www.linkedin.com/jobs/view/junior-data-engineer-at-tickpick-3778718017,2023-12-17,Rockhampton, Australia,Associate,Hybrid,"Who We Are
Same Seats. Better Prices.
Among the fastest growing technology companies, TickPick is reshaping the ticketing industry, putting money back in the wallets of live event-goers. Since inception, we have saved our customers over $100 million in service fees. Our BestPrice Guarantee backs up our promise to deliver better prices than our competition.
For the last five years, TickPick has been named a Deloitte Technology Fast 500 award winner and has landed on lists of Inc. 5000’s and Crain's New York Business’ Fast 50.
If you are passionate about concerts, sports, theater or all of them, and want to see your skills and experience have a direct impact on a fast growing company, TickPick is the place for you. We are building a diverse team, committed to providing the most innovative, transparent, and cost-effective ticket marketplace in the industry.
Who You Are
You are: a data engineer, backend software engineer, or other data specialist with strong desire and ability to implement high-impact data movement and management within a growing, technology-first team.
In this core role, you will be responsible for building and maintaining data pipelines touching a wide array of tools from the modern data stack, including but not limited to Snowflake, Spark, Dagster, dbt, and Azure Cloud offerings. You’ll be working closely with our current Data Engineer and other Analytics-focussed team members, as well as stakeholders from across the whole business.
If you value continual learning and are looking to join a high-visibility and high-impact team at a growing company that’s making data a priority, then this role is for you.
Core Responsibilities
Discover opportunities for data acquisition and implementation solutions
Develop production processes and solutions to model, mine and surface data
Improve and ensure data reliability, quality and efficiency
Requirements
BS or above in Computer Science or a related field, or equivalent personal or professional experience
Communication ability is paramount: you understand the value of open communication and have a track record of interacting effectively with stakeholders and team members
Experience and competency with at least one cloud services provider (Azure preferred; AWS or GCP also worthwhile)
Strong Python and SQL skills, as well as at least general competency with web languages (HTML/Javascript)
Python data competency – knowledge of and experience with, eg, Pandas or other Dataframe libraries. Polars, PySpark, or DuckDB are a big plus
Experience and competency with at least one general data orchestration tool, eg Airflow, Dagster, Prefect, or similar other experience. Dagster specifically is a big plus
dbt or other data modeling experience is a plus
Kafka experience is a plus
Experience with web scraping tools is a plus, eg BeautifulSoup, Playwright, requests, or Selenium
Experience with a dashboarding tool (eg Looker, Tableau, Superset, Metabase, or Streamlit) is a plus, but not most centrally important
Experience with managed ETL tools (eg Fivetran, Hightouch) is worth mentioning if you have it, but not most centrally important
Diversity at TickPick
At TickPick, we know that diversity of all types, in an environment that pursues equity and inclusion, strengthens our organization’s culture. When our employees are representative of the communities we serve, with diversity in demographics and a broad set of backgrounds, we provide a superior experience for both our customers and our employees. Fostering an open and supportive environment where our employees are empowered and encouraged to bring their whole selves to the table enables TickPick to thrive. The diverse approaches and collaborative problem solving that result enable us to provide an innovative, nimble, and creative marketplace for our customers and sellers. This belief is central to who we are and what we do, and we are proud of it.
TickPick, LLC is proud to be an equal opportunity employer open to all qualified candidates regardless of race, color, religion, national origin, gender, sexual orientation, gender identity or expression, age, mental or physical disability, marital status, citizenship status, military status, protected veteran status or any other category protected
Benefits
Per the NYC pay transparency law, the hiring range for this position is $90,000 to $110,000.
As a candidate for this position, your salary will be contingent upon your work experience, education, skills and any other factors TickPick considers relevant to the hiring decision. In addition to your salary, TickPick believes in providing a competitive benefits package for its employees. TickPick offers:
A hybrid in-office approach, enabling remote work a portion of each week
Health Care Plan (Medical, Dental & Vision)
Retirement Plan Contribution (401k, IRA)
Life Insurance (Basic, Voluntary & AD&D)
Paid Time Off (Vacation, Sick & Holidays)
Family Leave (Maternity, Paternity)
Training & Development
$100 Monthly Stipend to Attend Live Events
Employee Outings
Free Lunch & Snacks
Show more
Show less","Data engineering, Backend software engineering, Data movement, Data management, Data pipelines, Snowflake, Spark, Dagster, DBT, Azure Cloud, Python, SQL, HTML, JavaScript, Pandas, Polars, PySpark, DuckDB, Airflow, Kafka, BeautifulSoup, Playwright, Requests, Selenium, Looker, Tableau, Superset, Metabase, Streamlit, Fivetran, Hightouch","data engineering, backend software engineering, data movement, data management, data pipelines, snowflake, spark, dagster, dbt, azure cloud, python, sql, html, javascript, pandas, polars, pyspark, duckdb, airflow, kafka, beautifulsoup, playwright, requests, selenium, looker, tableau, superset, metabase, streamlit, fivetran, hightouch","airflow, azure cloud, backend software engineering, beautifulsoup, dagster, data engineering, data management, data movement, datapipeline, dbt, duckdb, fivetran, hightouch, html, javascript, kafka, looker, metabase, pandas, playwright, polars, python, requests, selenium, snowflake, spark, sql, streamlit, superset, tableau"
Junior Data Engineer,goop,"Santa Monica, CA",https://www.linkedin.com/jobs/view/junior-data-engineer-at-goop-3775680912,2023-12-17,Rockhampton, Australia,Associate,Hybrid,"About The Company
Gwyneth Paltrow launched goop from her kitchen as a homespun weekly newsletter. It’s grown a lot since then; goop is a lifestyle platform encompassing curated products and content about beauty, wellness, fashion, food, and home. Pioneering the contextual commerce platform, goop allows readers to shop with meaning. goop is one of the rare places on the web where food, shopping, and mindfulness collide—where the ever-evolving intent is to make every choice count. We’re all resource-strapped, so goop hopes to surface the very best experiences, recipes, products, and advice.
About You
You possess technical knowledge in SQL, Python, and wrangling large amounts of data, but you are also a strong collaborator and a passionate advocate for data.
About The Role
goop is looking for a talented and innovative Jr. Data Engineer to join our team. In this role you will help unify and bring to life our ever-growing data from all areas of business – e-commerce, retail, web analytics, logistics, marketing, and more. You will develop, deploy, and maintain data systems that will allow the company to use data in impactful and exciting ways.
Responsibilities Include:
Architect new data solutions that will allow us to process batched and real-time data.
Build and maintain ETLs from 3rd party data sources.
Ensure data quality through automated testing and alerting.
Identify, design, and implement internal process improvements such as automating manual processes and optimizing infrastructure.
Qualifications & Experience
2+ years of experience working on backend software using modern scripting languages and frameworks (Python)
2+ years of engineering experience relating to data engineering and large-scale data transformations; strong experience with relational databases (PostgreSQL, Redshift, MySQL)
Hands-on experience in cloud computing (AWS, EC2, S3, Athena, Lambda)
Experience with batch and real-time data processing (Airflow, Kinesis, Kafka)
Proficiency in Agile development process preferred.
Skillful problem solver, detail oriented with experience and ability to QA multiple data sources.
Strategic, cross-functional thinking with ability to gather business partners’ buy-in for projects.
FAQ
Compensation:
$100,000-$115,000 + Equity. This is a full time, exempt role. Please note that this range represents the low and high end of the anticipated base salary range for the Los Angeles, CA based position. Goop, in good faith, reasonably expects to pay the position within this salary range. Goop provides the salary range in compliance with all applicable federal, state and local laws. The actual base salary will depend on numerous factors such as: experience, training, knowledge and skills, and if the location of the job changes.
Benefits: Generous health benefits package, fertility benefits and paid parental leave.
Perks: “goopcation” paid company summer break, generous goop discounts, special offers with brand partners, access to custom lifestyle resources and events, and a beautiful work space in Santa Monica
Work Philosophy: At goop we believe that creativity, innovation and camaraderie are essential to our business, our culture and our employee’s growth and development. With our Hybrid Work Policy, we are committed to promoting collaboration, productivity and employee well-being by maximizing the benefits of both in-person and remote work. We are in office Tuesdays and Thursdays, as well as Mondays for those within a 15-mile radius of our Santa Monica office.
goop is an Equal Opportunity Employer. goop does not discriminate on the basis of race, religion, color, sex, gender identity, sexual orientation, age, non-disqualifying physical or mental disability, national origin, veteran status or any other basis covered by appropriate law. All employment is decided on the basis of qualifications, merit, and business need. All information provided by the applicant is collected, stored and processed in accordance with the terms of our CCPA Notice for Job Candidates.
Job Disclosures: No applicant disclosures related to physical requirements or ADA-related considerations are relevant for this role
Application Requirements: All applicants will be reviewed through Greenhouse submission. Direct submissions to the People Operations Team members will not be reviewed separately.
Candidate Requirements: Applicant must have US work authorization.
Show more
Show less","Python, SQL, PostgreSQL, Redshift, MySQL, AWS, EC2, S3, Athena, Lambda, Airflow, Kinesis, Kafka, ETL, Data engineering, Agile development, Data wrangling, Data quality assurance, Data transformation, Cloud computing, Machine learning, Data visualization","python, sql, postgresql, redshift, mysql, aws, ec2, s3, athena, lambda, airflow, kinesis, kafka, etl, data engineering, agile development, data wrangling, data quality assurance, data transformation, cloud computing, machine learning, data visualization","agile development, airflow, athena, aws, cloud computing, data engineering, data quality assurance, data transformation, data wrangling, ec2, etl, kafka, kinesis, lambda, machine learning, mysql, postgresql, python, redshift, s3, sql, visualization"
Data Engineer,Walmart,"Dallas, TX",https://www.linkedin.com/jobs/view/data-engineer-at-walmart-3768739077,2023-12-17,Rockhampton, Australia,Associate,Hybrid,"Our organization, i.e., Enterprise Business Services – Global Governance -Digital Citizenship Tech focuses on managing and delivering world-class products, driving policy compliance, creating partnerships, and developing pipelines and self-service tools. Responsible for Global Privacy Technology Product implementation(s) so that we can earn customers and associate trust.
This Principal Software Engineer will help develop the strategy for solution architecture including end state vision of PaaS/SaaS offering and support architecture initiatives across Digital Citizenship and Global Governance organization. If you are passionate about working in ambiguous spaces, solving complex architectural problems, collaborating across enterprise leaders to drive consensus, influencing horizontally, and are experienced in architecting PaaS/SaaS offerings, then this role is tailor-made for you!
What you’ll do:
Provide deep technical experts and thought leaders that help accelerate adoption of the very best engineering practices, while maintaining knowledge on industry innovations, trends, and practices
Visionaries, collaborating on Walmart’s toughest issues, to deliver on business needs that directly impact the lives of our customers and associates.
Mentor, coach and strengthen the technical expertise within the team.
Evangelists, both internally and externally, helping to elevate the Engineering community and establish themselves as a go-to resource on given technologies and technology-enabled capabilities.
Provide guidance and drive engineering excellence covering site reliability, alerts & monitoring, product health and portability.
What you’ll bring:
Experience providing technical and architectural guidance and help develop cost optimization strategies by deploying the right architecture and pricing models.
Full stack development experience with Java, Node, React, Angular and microservices.
Experience building long term strategy of building integrated packageable platforms (PaaS) scalable to Walmart global needs, complexity, and scale.
Create and capture best practices, technical content and new reference architectures (e.g., white papers, code samples, blog posts).
Develop areas of depth in technical domains relevant to your interests and your customer's outcomes.
Provide guidance and drive engineering excellence covering site reliability, alerts & monitoring, product health and portability.
About Walmart Global Tech
Imagine working in an environment where one line of code can make life easier for hundreds of millions of people and put a smile on their face. That’s what we do at Walmart Global Tech. We’re a team of 15,000+ software engineers, data scientists and service professionals within Walmart, the world’s largest retailer, delivering innovations that improve how our customers shop and empower our 2.3 million associates. To others, innovation looks like an app, service, or some code, but Walmart has always been about people. People are why we innovate, and people power our innovations. Being human-led is our true disruption. We train our team in the skillsets of the future and bring in experts like you to help us grow. We have roles for those chasing their first opportunity as well as those looking for the opportunity that will define their career. Here, you can kickstart a great career in tech, gain new skills and experience for virtually every industry, or leverage your expertise to innovate at scale, impact millions and reimagine the future of retail.
Flexible, hybrid work
We use a hybrid way of working that is primarily virtual, while remaining near the locations Global Tech calls home. This approach helps us make quicker decisions, remove location barriers across our global team, be more flexible in our personal lives and spend less time commuting. Of course, being together in person is an important part of our culture and shared success. We use our campuses to collaborate and be together in person, as business needs require and for development and networking opportunities.
Benefits
Beyond our great compensation package, you can receive incentive awards for your performance. Other great perks include 401(k) match, stock purchase plan, paid maternity and parental leave, PTO, multiple health plans, and much more. Equal Opportunity Employer Walmart, Inc. is an Equal Opportunity Employer – By Choice. We believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing diversity- unique styles, experiences, identities, ideas and opinions – while being inclusive of all people.
The above information has been designed to indicate the general nature and level of work performed in the role. It is not designed to contain or be interpreted as a comprehensive inventory of all responsibilities and qualifications required of employees assigned to this job. The full Job Description can be made available as part of the hiring process.
Show more
Show less","Java, Node, React, Angular, Microservices, PaaS, SaaS, Site reliability, Alerts & monitoring, Product health & Portability, Technical & architectural guidance, Cost optimization strategies, Full stack development, Longterm strategy building, Integrated packageable platforms, White papers, Code samples, Blog posts, Technical domains","java, node, react, angular, microservices, paas, saas, site reliability, alerts monitoring, product health portability, technical architectural guidance, cost optimization strategies, full stack development, longterm strategy building, integrated packageable platforms, white papers, code samples, blog posts, technical domains","alerts monitoring, angular, blog posts, code samples, cost optimization strategies, full stack development, integrated packageable platforms, java, longterm strategy building, microservices, node, paas, product health portability, react, saas, site reliability, technical architectural guidance, technical domains, white papers"
Junior Data Engineer,Dale WorkForce Solutions,"Coppell, TX",https://www.linkedin.com/jobs/view/junior-data-engineer-at-dale-workforce-solutions-3782273397,2023-12-17,Rockhampton, Australia,Associate,Hybrid,"Our client is seeking a
Junior Data Engineer
with 1-3 years of experience.
Job Title:
Junior Data Engineer
Location:
On-site in Coppell, TX 1x/week (must currently live in the area)
Duration:
Direct hire/Full-time
Key Responsibilities:
Data Pipeline Development: Design, build, and maintain efficient data pipelines using Python, Apache Spark, and Apache Kafka.
Big Data Processing: Work on large-scale data processing and engineering tasks, ensuring performance and scalability.
Data Quality and Integration: Ensure the integrity, reliability, and quality of data. Integrate data from various sources for analytical and operational purposes.
Collaborative Development: Collaborate with cross-functional teams, including data scientists and analysts, to support data infrastructure needs and contribute to ongoing projects.
Problem-Solving: Tackle complex data-related problems and engineer scalable solutions.
Continuous Learning: Stay updated with the latest trends and technologies in big data engineering and apply them to improve existing systems.
Qualifications:
Bachelor's degree in Computer Science, Engineering, or a related field.
1-3 years of experience in a Data Engineering role.
Strong proficiency in Python, Java, Spark programming.
Hands-on experience with Confluent Kafka, Snowflake, BigQuery
Demonstrated experience in working with big data technologies and large-scale data engineering projects.
Familiarity with cloud platforms like AWS, Azure, or GCP is a plus.
Excellent problem-solving and analytical skills.
Strong communication and teamwork abilities.
Show more
Show less","Python, Java, Apache Spark, Apache Kafka, Data Pipeline Development, Big Data Processing, Data Quality, Data Integration, Confluent Kafka, Snowflake, BigQuery, AWS, Azure, GCP, Cloud Platforms","python, java, apache spark, apache kafka, data pipeline development, big data processing, data quality, data integration, confluent kafka, snowflake, bigquery, aws, azure, gcp, cloud platforms","apache kafka, apache spark, aws, azure, big data processing, bigquery, cloud platforms, confluent kafka, data integration, data pipeline development, data quality, gcp, java, python, snowflake"
Data Engineer,Medical Mutual,"Brooklyn, OH",https://www.linkedin.com/jobs/view/data-engineer-at-medical-mutual-3775487039,2023-12-17,Rockhampton, Australia,Associate,Hybrid,"This position will be Hybrid (In office and work at home)
Data Engineer I
Software developer for the design and development of the company’s data lake, data warehouse, and data marts, and development of related business intelligence dashboards and visualizations. Job duties involve profiling data, developing data transformation jobs and processes, measuring data quality, and designing data warehouse data models, evaluating usability and testing data models, and developing business intelligence visualizations.
Data Engineer II
Software developer for the design and development of the company’s data lake, data warehouse, and data marts, and support for applied use of data for business insights. Job duties involve profiling data, developing data transformation jobs and processes, measuring data quality, and designing data warehouse data models, evaluating usability and testing data models, and developing processes for applied use of enterprise data (data analysis, business intelligence, reporting).
Data Engineer III
Data engineer responsible for the design and development of data integration pipelines. Build data solutions for business problems and support the applied use of data to enable business insights and action. Job duties involve profiling data, developing data transformation pipelines, measuring data quality, designing data models, and supporting applied use of data to solve problems (exploratory analysis, business intelligence/dashboards, reporting, alerts).
Data Engineer IV
Data engineer responsible for the design and development of data integration pipelines. Build data solutions for business problems and support the applied use of data to enable business insights and action. Job duties involve profiling data, developing data transformation pipelines, measuring data quality, designing data models, and supporting applied use of data to solve problems (exploratory analysis, business intelligence/dashboards, reporting, alerts).
Data Engineer V
Data engineer responsible for the design and development of data integration pipelines. Build data solutions for business problems and support the applied use of data to enable business insights and action. Job duties involve profiling data, developing data transformation pipelines, measuring data quality, designing data models, and supporting applied use of data to solve problems (exploratory analysis, business intelligence/dashboards, reporting, alerts).
Responsibilities
Data Engineer I
Develop data pipelines to integrate and transform data for analysis
Support other data users and enable applied use of data to solve business problems
Business meetings to understand data and business needs, socialize knowledge of data products and insights, and generate consensus
Data profiling, data documentation, measuring and enforcing data quality
Developing data models to represent information
Performs other duties as assigned.
Learns new technical concepts and software languages as needed.
Data Engineer II
Developing and sequencing jobs and processes to transform data into data lake and data warehouse
Support analytic projects, data extracts, and other work to apply data in the warehouse, test and prove capabilities, evaluate usability, and perform demonstrations to other analysts for how to use data to meet objectives
Business meetings to understand data, capture agreement on business rules, understand analyst and stakeholder objectives, and support usage of data to solve business problems
Data profiling, data documentation, and measuring data quality with manual verification and
development of automated data quality tests
Developing data models to represent data
Data Engineer III
Develop data pipelines to integrate and transform data for analysis
Support other data users and enable applied use of data to solve business problems
Business meetings to understand data and business needs, socialize knowledge of data products and insights, and generate consensus
Data profiling, data documentation, measuring and enforcing data quality
Developing data models to represent information
Performs other duties as assigned.
Learns new technical concepts and software languages as needed
Data Engineer IV
Develop data pipelines to integrate and transform data for analysis
Support other data users and enable applied use of data to solve business problems
Business meetings to understand data and business needs, socialize knowledge of data products and insights, and generate consensus
Data profiling, data documentation, measuring and enforcing data quality
Developing data models to represent information
Performs other duties as assigned.
Learns new technical concepts and software languages as needed.
Data Engineer V
Develop data pipelines to integrate and transform data for analysis
Support other data users and enable applied use of data to solve business problems
Business meetings to understand data and business needs, socialize knowledge of data products and insights, and generate consensus
Data profiling, data documentation, measuring and enforcing data quality
Developing data models to represent information
Performs other duties as assigned.
Learns new technical concepts and software languages as needed.
Qualifications
Data Engineer I
Education and Experience:
Bachelors degree in Computer Science, Computer Information Systems, business degree with focus on data transformation or analysis, or equivalent work experience
Technical Skills and Knowledge:
Basic understanding of programming design and coding techniques
Familiarity with systems development life cycle
Understanding of database technologies and query semantics
Experience with public cloud database technology (e.g. RedShift, SnowFlake, DataBricks, Azure Synapse, BigQuery)
Experience programming in ETL or ELT technology
Understanding of data warehouse data model theory and techniques
Experience with Python programming language
Data Engineer II
Education and Experience:
Bachelors degree in Computer Science, Computer Information Systems, business degree with focus on data transformation or analysis, or equivalent work experience
Technical Skills and Knowledge:
Two or more years professional experience developing software to solve business problems
Experience with data engineering system design and software implementation
Experience with all parts of the systems development life cycle
Experience with database technologies and query semantics
Experience with public cloud database technology (e.g. RedShift, SnowFlake, DataBricks, Azure Synapse, BigQuery)
Experience programming in ETL or ELT technology
Understanding of data warehouse data model theory and techniques
Experience with Python programming language
Data Engineer III
Education and Experience:
Bachelors degree in Computer Science, Computer Information Systems, business degree with focus on data transformation or analysis, or equivalent work experience
Technical Skills and Knowledge:
Four or more years professional experience developing software to solve business problems
Three or more years experience with data engineering system design and software implementation
Experience with all parts of the systems development life cycle
Experience with database technologies and query semantics
Experience with public cloud database technology (e.g. RedShift, SnowFlake, DataBricks, Azure Synapse, BigQuery)
Experience programming in ETL or ELT technology
Understanding of data warehouse data model theory and techniques
Experience with Python programming language
Data Engineer IV
Education and Experience:
Bachelors degree in Computer Science, Computer Information Systems, business degree with focus on data transformation or analysis, or equivalent work experience
Technical Skills and Knowledge:
Seven or more years professional experience developing software to solve business problems
Five or more years experience with data engineering system design and software implementation
Proficient with all parts of the systems development life cycle
Experience with database technologies and query semantics
Experience leading and coordinating technical initiatives
Experience with public cloud database technology (e.g. RedShift, SnowFlake, DataBricks, Azure Synapse, BigQuery)
Experience programming in ETL or ELT technology
Understanding of data warehouse data model theory and techniques
Experience with Python programming language
Data Engineer V
Education and Experience:
Bachelors degree in Computer Science, Computer Information Systems, business degree with focus on data transformation or analysis, or equivalent work experience
Technical Skills and Knowledge:
Seven or more years professional experience developing software to solve business problems
Five or more years experience and demonstrated excellence with data engineering system design and software implementation
Demonstrated excellence with all parts of the systems development life cycle
Expert in database technologies and query semantics
Demonstrated excellence leading and coordinating technical initiatives
Experience with public cloud database technology (e.g. RedShift, SnowFlake, DataBricks, Azure Synapse, BigQuery)
Experience programming in ETL or ELT technology
Expert understanding of data warehouse data model theory and techniques
Experience with Python programming language
At Medical Mutual and its family of companies we celebrate differences and are mutually invested in our employees and our community. We are proud to be an Equal Employment Opportunity and Affirmative Action Employer. Qualified applicants will receive consideration for employment regardless of race, color, religion, sex, sexual orientation, gender perception or identity, national origin, age, marital status, veteran status, or disability status.
We maintain a drug-free workplace and perform pre-employment substance abuse and nicotine testing.
Show more
Show less","Data transformation, ETL pipelines, Data lakes and warehouses, Data Marts, Business intelligence, Data models, Data quality, Data analysis, Data visualizations, Systems development life cycle, Python, Programming, Software development, Software implementation, Data architecture, Cloud database technologies, Database design, Query semantics, Business intelligence dashboards, Data migration, Data integration, Data governance, Data visualization","data transformation, etl pipelines, data lakes and warehouses, data marts, business intelligence, data models, data quality, data analysis, data visualizations, systems development life cycle, python, programming, software development, software implementation, data architecture, cloud database technologies, database design, query semantics, business intelligence dashboards, data migration, data integration, data governance, data visualization","business intelligence, business intelligence dashboards, cloud database technologies, data architecture, data governance, data integration, data lakes and warehouses, data marts, data migration, data models, data quality, data transformation, data visualizations, dataanalytics, database design, etl pipelines, programming, python, query semantics, software development, software implementation, systems development life cycle, visualization"
Data Engineer,Tier4 Group,"Lehi, UT",https://www.linkedin.com/jobs/view/data-engineer-at-tier4-group-3779255947,2023-12-17,Rockhampton, Australia,Associate,Hybrid,"Job Title: Data Engineer
Location: Lehi, UT
Employment Type: Full-time, Hybrid 3 days onsite
Our client is seeking a skilled Data Engineer to join their dynamic and growing team, contributing to the forefront of technological innovation. This role involves leveraging cutting-edge AWS tools and technologies to handle large volumes of data, design and manage datasets, implement machine learning models, and drive insightful data-driven solutions.
Key Responsibilities:
Utilize AWS tools such as Kinesis, Glue, and Data Pipeline to collect, store, and process extensive data volumes efficiently.
Design, manage, and analyze large datasets, identifying patterns, anomalies, and ensuring data quality using services like Amazon S3, RDS, and Redshift.
Develop, train, and deploy machine learning models, employing ML Ops methodologies (classification, regression, clustering, NLP, CNN) through AWS SageMaker or related services.
Implement analytical algorithms focusing on predictive and prescriptive modeling.
Create visualizations and dashboards, either using tools like Amazon QuickSight or integrating with platforms such as Tableau and PowerBI.
Develop and manage DB objects like tables, procedures, triggers, and functions.
Proficiently write complex SQL queries, optimizing performance and tuning queries as necessary.
Extract data from SQL/NoSQL databases for generating ad hoc reports and analysis.
Script and program using Python and R.
Document procedures, workflows, and best practices related to data processing and modeling.
Job Specifications:
5+ years of experience as a Data Engineer, demonstrating proficiency with large datasets and data science methodologies.
3+ years of experience with SQL-based database technologies (Oracle, SQL, MySQL, Postgres), including store procedures, functions, triggers, etc.
3+ years of experience with data visualization tools such as Power BI, AWS QuickSight, etc.
3+ years of hands-on experience with cloud services, preferably AWS (SageMaker, S3, EC2, EMR, RDS, Airflow, Redshift).
Familiarity with big data tools like Hadoop, Spark, Kafka, etc.
Strong programming skills in languages like Python, Java, or Scala.
Experience with machine learning frameworks like TensorFlow or PyTorch and ETL tools is a plus.
Exceptional communication and teamwork skills.
This role offers an exciting opportunity to work in a collaborative environment, leveraging advanced technologies, and contributing to impactful data-driven solutions. If you are passionate about data engineering and thrive in a dynamic, cutting-edge tech environment, we encourage you to apply!
Show more
Show less","AWS, Kinesis, Glue, Data Pipeline, Amazon S3, RDS, Redshift, SageMaker, ML Ops, Python, R, SQL, Oracle, MySQL, Postgres, Power BI, Tableau, Airflow, Hadoop, Spark, Kafka, TensorFlow, PyTorch, ETL","aws, kinesis, glue, data pipeline, amazon s3, rds, redshift, sagemaker, ml ops, python, r, sql, oracle, mysql, postgres, power bi, tableau, airflow, hadoop, spark, kafka, tensorflow, pytorch, etl","airflow, amazon s3, aws, data pipeline, etl, glue, hadoop, kafka, kinesis, ml ops, mysql, oracle, postgres, powerbi, python, pytorch, r, rds, redshift, sagemaker, spark, sql, tableau, tensorflow"
Data Engineer,Chartway Credit Union,"Virginia Beach, VA",https://www.linkedin.com/jobs/view/data-engineer-at-chartway-credit-union-3760254106,2023-12-17,Rockhampton, Australia,Associate,Hybrid,"Job Title: Data Engineer
Job Type: Full-time
Location: Virginia Beach, Virginia
All of us at Chartway stand united in our strong culture of listening and learning. We treat people with respect, integrity, and kindness, and are committed to continuously developing a diverse and inclusive workplace where team members are empowered to be their authentic selves. All backgrounds, viewpoints, and life experiences are welcome, knowing this commitment strengthens our credit union family and communities.
The Data Engineer would be responsible for development and operations of the data warehouse, and overall data environment management for the credit union. The engineer will work closely with Chartway’s analysts, business owners, and stakeholders across the business to identify, recommend, develop, implement, and support analytical solutions. Once developed, solutions will deliver essential data to satisfy their informational needs and the needs of our members. Engineers will train and develop both internal resources as well as Subject Matter Experts in different departments on proper ways to query, analyze and use data.
Responsible and Accountable for:
Building and supporting elements of data infrastructure to help the business insights, product analytics, and all business functions to use data to drive decisions
Communicate complex information, ideas, and findings to non-technical key stakeholder
Interfaces across the organization to share vision and provide guidance on usage of the data product/solution
Collaborate with teams to identify, prioritize, and develop key features to meet business needs.
Supports Requirements Analysis, Presenting Technical Information, and Problem Solving. Able to quickly determine the best way to put information together
Participates actively in data design; data standards; and policy as it pertains to data management and analytic solutions
Knowledge, Skills, and Abilities:
Proficient with SQL scripting
Working knowledge of a data visualization tool, preferably Power BI
Working knowledge of an orchestration tool, preferably SSIS
Proficient with Microsoft Office
Ability to troubleshoot, problem solve, and think critically
Ability to be self-directed and demonstrate strong initiative
Ability to work independently and support team processes and responsibilities
Ability to articulate and interpret results and processes in a compelling manner
Technical knowledge of data management systems, data warehouse methodology, data quality, and data modelling
Strong written and oral communication skills as well as superior interpersonal skills
Exceptional customer service orientation
Ability to work in a team-oriented, collaborative environment, using an agile methodology
Self-motivated and self-directed. Keen attention to detail. Proven analytical, evaluative, and problem-solving abilities
Education and Experience:
Education: BS Degree in Economics, Information Systems, Finance, Accounting, Statistics, Mathematics, Computer Science, or related area
Experience: entry to mid-level experience with professional data use and manipulation
Data visualization platforms such as Microsoft Power BI and SSRS
Leveraging data quality, monitoring, and governance infrastructure
Experience supporting a data analytics solution within an organization
Familiarity with project management skills and able to participate in complex projects
Chartway offers a competitive salary and a comprehensive benefits package. Candidates must have a good credit history for consideration. Salary requirements must be included and resumes must be accompanied by a Chartway Employment Application for consideration.
Chartway Federal Credit Union is an EOE/M/F/ Disability/Vet Employer.
Show more
Show less","SQL scripting, Power BI, SSIS, Microsoft Office, Data visualization, Data warehousing, Data quality, Data modeling, Project management, Data analytics","sql scripting, power bi, ssis, microsoft office, data visualization, data warehousing, data quality, data modeling, project management, data analytics","data quality, dataanalytics, datamodeling, datawarehouse, microsoft office, powerbi, project management, sql scripting, ssis, visualization"
Data Engineer,Fortune,"New York, NY",https://www.linkedin.com/jobs/view/data-engineer-at-fortune-3759839264,2023-12-17,Rockhampton, Australia,Associate,Hybrid,"Fortune Media
Data Engineer
Job description
We are looking for a Data Engineer to join our growing, versatile Data Engineering team. This person will be a key member of analytical projects and will contribute to data discovery, pipeline implementation, and analytics modeling. This role requires a person with hands on technical experience who is comfortable with delivering scalable solutions that meet (and exceed!) business needs and is comfortable working with various tools and methodologies.
Responsibilities
Work closely with Data Engineering Lead to scope, document, and ingest data from structured and unstructured sources into Fortune’s data warehouse.
Collaborate with BI & Analytics stakeholders to fully understand business and technical requirements, and be able to communicate a clear path of work.
Design, develop, and deploy data models to meet BI & Analytics use cases.
Explore and structure web analytics data capture with our Product Analytics lead.
Identify opportunities for optimization with our existing pipelines and data warehouse automations.
Document and provide support for existing implemented solutions.
Collaborate across the Data Engineering team to add value where necessary
Requirements
2+ years relevant professional experience in a technical, data centric role (analyst, modeler, engineer). Preferably in digital media or publishing.
Technical Skills
Experience with web analytics platforms (ex. Google Analytics, Amplitude) – Piano Analytics is a plus.
Deep knowledge of SQL fundamentals – PostgreSQL and PLPGSQL are a plus.
Proficiency in Python and ETL / data processing dependent libraries (think psycopg2, requests, pyspark / dask / pandas etc.).
Experience in Data Modeling – taking raw data and surfacing as a usable data object by adding structure and implementing business logic.
Experience with structuring ETL / ELTs and/or using ETL tools (ex. AWS Glue, DBT
Experience with at least one of the major cloud platforms (ex. AWS, GCP) – we operate on an AWS stack
Familiarity with OLTP databases
Familiarity with Jira or other agile project management platforms
Bonus – familiarity with NOSQL databases
Bonus – familiarity with Shell / Bash scripting
Bonus – familiarity with Google Tag Manager.
Soft Skills
Experience in digesting business requirements and translating those needs into technical implementation.
Communicating architecture and implemented solutions in understandable terms to non-technical partners.
Agency – owning and delivering on your responsibilities.
Coachability and willingness to learn.
Team player and champion for collaboration.
Transparent in communicating work.
Location:
NYC (hybrid). Must be commutable to our office in Lower Manhattan
A few of Fortune’s perks and benefits:
22 vacation days, 11 paid holidays and an honor-based sick leave policy
Health, dental, and vision coverage (90% paid for individuals and families), plus a high-deductible health plan option where Fortune contributes to a Health Savings Account (HSA)
401(k) plan
Generous parental leave
Dependent care/health care/commuter FSAs, and cell phone benefits
Tuition reimbursement program
A commitment to an open, inclusive and diverse work culture
For Residents of California:
Our California Resident Applicant and Prospective Applicant Privacy Notice is located at this link:
https://boards.greenhouse.io/fortune/jobs/4766493004.
Salary:
$90,000 - $140,000
Show more
Show less","PostgreSQL, PLPGSQL, Python, Psycopg2, Requests, PySpark, Dask, Pandas, Data Modeling, ETL, ELT, AWS Glue, DBT, AWS, GCP, OLTP, Jira, Agile, NOSQL, Shell, Bash, Google Tag Manager, SQL","postgresql, plpgsql, python, psycopg2, requests, pyspark, dask, pandas, data modeling, etl, elt, aws glue, dbt, aws, gcp, oltp, jira, agile, nosql, shell, bash, google tag manager, sql","agile, aws, aws glue, bash, dask, datamodeling, dbt, elt, etl, gcp, google tag manager, jira, nosql, oltp, pandas, plpgsql, postgresql, psycopg2, python, requests, shell, spark, sql"
Azure Data Engineer,Sogeti,"Raritan, NJ",https://www.linkedin.com/jobs/view/azure-data-engineer-at-sogeti-3768086033,2023-12-17,Rockhampton, Australia,Associate,Hybrid,"Responsibilities & Requirements:
Work with business and technology to establish a data solution in Azure
Work with the customer to understand the requirements and client environment
Perform data engineering to translate data into useful forms for analysis for a customer data program
Collect, prepare and analyze data
Analyze System Architecture with heterogenous sources
Must have solid experience with SQL and Azure Data Factory (Azure SQL, Synapse, Databricks)
Must have 3+ years of data engineering experience
Experience with python and data pipelines is nice to have
Ability to work in a global delivery model in an agile environment
Must have excellent communication skills
AWS or Azure certification is nice to have
Show more
Show less","Data Solution, Data Engineering, Data Analysis, SQL, Azure Data Factory, Azure SQL, Synapse, Databricks, Python, Data Pipelines, Data Architecture, Heterogeneous Data Sources, Agile, Communication, AWS, Azure","data solution, data engineering, data analysis, sql, azure data factory, azure sql, synapse, databricks, python, data pipelines, data architecture, heterogeneous data sources, agile, communication, aws, azure","agile, aws, azure, azure data factory, azure sql, communication, data architecture, data engineering, data solution, dataanalytics, databricks, datapipeline, heterogeneous data sources, python, sql, synapse"
Big Data Engineer,Insight Global,"Phoenix, AZ",https://www.linkedin.com/jobs/view/big-data-engineer-at-insight-global-3779376062,2023-12-17,Rockhampton, Australia,Associate,Hybrid,"Title:
Big
Data Engineer
Duration:
Up to 12 month contract w potential for hire/extensions
Desired Skills:
-4 to 5+ years of Big Data Engineering experience with managing, designing, performance tuning relational and non-relational databases.
-Experience working in Hadoop ecosystem, distributed system architecture HBase, HDFS, Map-reduce programming model, Hive, PIG, etc.
-Proficient in data-warehousing, requirement driven data-modeling, data-modeling techniques, and scalable database programming.
-Expertise in ETL methodology with large data sets
-Comfortable with creating structured data solutions using various programming languages such as Python or Java.
-Public Cloud Experience with GCP or AWS
-Big Data exposure: Parquet, Avro, etc.
Nice to Have Skills and Experience:
-Big Data certification (any)
-Public Cloud certification
-Spark, Flink
Job Description:
A large financial enterprise in Phoenix, AZ is looking for 3 Big Data Engineers to join their Database Management team to help with a large finance initiative. This initiative is centered around their Big Data Platforms, relational and non-relational databases, and working within their Hadoop ecosystem. You will be responsible for scaling large data analytics clusters, creating data solutions that are structured and scalable, along with working with data sets in a public cloud environment. The finance initiative is expediting their modernization of their finance platforms. 90% of the data they are already supporting so it is only about 10% of new data, but it is about speeding up the modernization of databases/data models/data structures.
Show more
Show less","Big Data Engineering, Hadoop, HBase, HDFS, MapReduce, Hive, PIG, Data warehousing, Data modeling, ETL, Python, Java, GCP, AWS, Parquet, Avro, Spark, Flink","big data engineering, hadoop, hbase, hdfs, mapreduce, hive, pig, data warehousing, data modeling, etl, python, java, gcp, aws, parquet, avro, spark, flink","avro, aws, big data engineering, datamodeling, datawarehouse, etl, flink, gcp, hadoop, hbase, hdfs, hive, java, mapreduce, parquet, pig, python, spark"
Data Engineer IT,JetBlue,"New York, NY",https://www.linkedin.com/jobs/view/data-engineer-it-at-jetblue-3732582931,2023-12-17,Rockhampton, Australia,Associate,Hybrid,"Position Summary
The Data Engineer is responsible for integrating and modeling data in JetBlue’s modern data stack to support analysts, business intelligence users, data scientists, and decision-makers across the company. The Data Engineer must have a deep understanding of Structured Query Language (SQL) and be familiar with Snowflake, dbt (data build tool), and git version control. In this role, the Data Engineer collaborates with stakeholders across the company to understand and address their needs. The Data Engineer must enjoy working with others and be comfortable expressing ideas in a public setting. The Data Engineer reports to the Manager of IT Data Engineering.
Essential Responsibilities:
Work with business stakeholders to understand problems and data sources, playing the pivotal role of architecting data models to address Crewmember needs
Gather requirements, build roadmaps, and write great documentation for data sources that need to be modeled
Build Extract, Load, Transform (ELT) jobs based on jointly defined requirements
Perform end-to-end unit testing & code reviews to promote data integrity across a variety of products built by the development team
Support bug fixing and performance analysis along the data pipeline, including writing tests & coordinating with Quality Assurance teams
Be comfortable presenting to large groups & expressing ideas in public settings with high visibility
Be a strong advocate for a culture of process and data quality across development teams
Follow an agile development methodology
Other duties as assigned
Minimum Experience and Qualifications:
Bachelor Degree in Mathematics, Operations Research, Statistics, Computer Science, Engineering, or Physics; OR demonstrated capability to perform job responsibilities with a High School Diploma/GED and at least four (4) years of previous relevant work experience
Three (3) years of relevant experience in a data role working with data warehouses and business intelligence tools
Proven experience with dbt (data build tool) & Snowflake
Strong SQL skills
Experience with modern ELT orchestration tools like Azure Data Factory or Airflow
Experience with git and git-based workflows
Available for occasional overnight travel (10%)
Must pass a pre-employment drug test
Must be legally eligible to work in the country in which the position is located
Authorization to work in the US is required. This position is not eligible for visa sponsorship
Preferred Experience and Qualifications:
Experience implementing best-practices for data modelling, especially with regards to dimensional modelling for business intelligence
Basic Python skills
Proven track record of successfully contributing to a project that transitioned a large enterprise to a new cloud data warehouse, like Snowflake
Prior airline experience
Crewmember Expectations:
Regular attendance and punctuality
Potential need to work flexible hours (including nights & weekends) and be available to respond on short-notice
Able to maintain a professional appearance
When working or traveling on JetBlue flights, and if time permits, all capable Crewmembers are asked to assist with light cleaning of the aircraft
Must be an appropriate organizational fit for the JetBlue culture, that is, exhibit the JetBlue values of Safety, Caring, Integrity, Passion and Fun
Promote JetBlue’s #1 value of safety as a Safety Ambassador, supporting JetBlue’s Safety Management System (SMS) components, Safety Policy and behavioral standards
Identify safety and/or security concerns, issues, incidents or hazards that should be reported and report them whenever possible and by any means necessary including JetBlue’s confidential reporting systems (Aviation Safety Action Program (ASAP) or Safety Action Report (SAR))
Equipment:
Computer and other office equipment
Work Environment:
Traditional office environment
Physical Effort:
Generally not required, or up to 10 pounds occasionally, 0 pounds frequently. (Sedentary)
Compensation:
The base pay range for this position is between $100,000.00 and $128,600.00 per year. Base pay is one component of JetBlue’s total compensation package, which may also include access to healthcare benefits, a 401(k) plan and company match, crewmember stock purchase plan, short-term and long-term disability coverage, basic life insurance, free space available travel on JetBlue, and more.
Show more
Show less","SQL, Snowflake, DBT, Git, Azure Data Factory, Airflow, Python, Data Warehouse, Dimensional Modelling, ELT","sql, snowflake, dbt, git, azure data factory, airflow, python, data warehouse, dimensional modelling, elt","airflow, azure data factory, datawarehouse, dbt, dimensional modelling, elt, git, python, snowflake, sql"
Data Engineer II - Enterprise Analytics (On-site/Hybrid),The Venetian Resort Las Vegas,"Las Vegas, NV",https://www.linkedin.com/jobs/view/data-engineer-ii-enterprise-analytics-on-site-hybrid-at-the-venetian-resort-las-vegas-3782061787,2023-12-17,Rockhampton, Australia,Associate,Hybrid,"Position Overview
The primary responsibility of the Data Engineer II – Enterprise Analytics is assisting in designing, developing, and deploying data-driven solutions as part of Enterprise Analytics data strategy and goals. Data Engineer II – Enterprise Analytics is responsible for creating reliable ETLs and scalable data pipelines to support Analytics and BI environment (including modeling and machine learning, visualizations, reports, cubes & applications, etc.). Data Engineer II – Enterprise Analytics participates in data modeling and development of data models and data marts by interpreting business logic required to turn complex ideas into a sustainable value add processes. All duties are to be performed in accordance with departmental and The Venetian Resort’s policies, practices, and procedures.
Essential Duties & Responsibilities
Collaborate with Enterprise Analytics BI Analysts and Data Scientists, and other business stakeholders to understand business problems and build/automate data structures ingested by analytics products (e.g.: reports, dashboards, cubes, etc.).
Create BI solutions, including dashboards (Power BI) and reports (SSRS, Excel, and Cognos).
Development of logic for KPIs requested by the business leadership.
Troubleshoot existing and create new ETLs, SSIS packages, SQL stored procedures and jobs.
Assist in Star and Snowflake modeling, creating dimensional models and ETL processes.
Write an efficient SQL code for use in data pipelines and data processing.
Drive data quality processes like data profiling, data cleansing, etc.
Develop best practices and approaches to support continuous process automation for data ingestion and data pipelines.
Use innovative problem solving and critical thinking approaches to troubleshoot challenging data obstacles.
Test, optimize, troubleshoot, and fine-tune queries for maximum efficiency.
Maintain existing and create new Microsoft Power Apps solutions (including but not limited to Power Apps, Power Automate, Power BI, Flows, etc.) according to business needs.
Perform QA and UAT processes to foster an agile development cycle.
Create documentation on table design, mapping out steps and underlying logic within data marts to facilitate data adoption with minimum guidance from the Enterprise Analytics management.
Identify areas of improvement not just in owned work, but also other areas of the business.
Mentor and train junior Data Engineers on best practices, query and ETL optimization techniques.
Create and maintain daily, weekly, monthly, quarterly reports and dashboards.
Consolidate fractured enterprise reporting into a standardized product for easy visualization and cross-departmental understanding.
Create reporting structures that accurately link cross-departmental data, which allows for on demand delivery of ad-hoc reports.
Safety is an essential function of this job.
Consistent and regular attendance is an essential function of this job.
Performs other related duties as assigned.
Company Standards of Conduct
All The Venetian Resort Team Members are expected to conduct and carry themselves in a professional manner at all times. Team Members are required to observe the Company’s standards, work requirements and rules of conduct.
Minimum Qualifications
21 years of age.
Proof of authorization/eligibility to work in the United States.
Bachelor’s degree in Computer Science, Information Systems, Engineering, Analytics, or related field is required.
Master’s degree in related discipline is preferred.
Must be able to obtain and maintain a Nevada Gaming Control Board registration and any other certification or license, as required by law or policy.
2+ years of experience in building data pipelines and ETL processes is required.
2+ years of experience creating visualizations and reports (Power BI, Tableau, MicroStrategy, Google Analytics) is required.
2+ years of experience in writing advanced SQL, data mining and working with traditional relational databases (tables, views, window functions, scalar and aggregate functions, primary/foreign keys, indexes DML/DDL statements, joins and unions) and/or distributed systems (Hadoop, Big Query) is required.
1+ years of experience with programming/scripting languages such as Python, R or Big Query is required.
Experience in either Microsoft Power Suite (Power Apps, Power Automate, Power BI, etc.), Microsoft Azure, Google Cloud Platform, or RPA tools is preferred.
Excellent understanding of data types, data structures and database systems and their specific use cases is required.
Strong understanding of data modeling principles including Dimensional modeling, and Data Normalization principles is required.
Extensive knowledge of Microsoft Excel (Excel formulas, data wrangling, VBA macros, graphs, and pivot tables) is required.
Excellent critical thinker and effective problem solver with creative solutions.
Physical Requirements
Must be able to:
Lift or carry 10 pounds, unassisted, in the performance of specific tasks, as assigned.
Physically access all areas of the property and drive areas with or without a reasonable accommodation.
Maintain composure under pressure and consistently meet deadlines with internal and external customers and contacts.
Ability to interact appropriately and effectively with guests, management, other team members, and outside contacts.
Ability for prolonged periods of time to walk, stand, stretch, bend and kneel.
Work in a fast-paced and busy environment.
Work indoors and be exposed to various environmental factors such as, but not limited to, CRT, noise, dust, and cigarette smoke.
Show more
Show less","Data Engineering, ETL, Data Pipelines, Business Intelligence (BI), Data Modeling, Star Schema, Snowflake Schema, SQL, Data Quality, Process Automation, Data Profiling, Data Cleansing, Power BI, SSRS, Excel, Cognos, Power Apps, Power Automate, Power BI Flows, QA, UAT, Agile Development, Documentation, Mentoring, Training, Reporting, Data Visualization, CrossDepartmental Collaboration, AdHoc Reporting, Computer Science, Information Systems, Engineering, Analytics, Python, R, Big Query, Microsoft Azure, Google Cloud Platform, RPA, Data Types, Data Structures, Database Systems, Dimensional Modeling, Data Normalization, Microsoft Excel, VBA Macros","data engineering, etl, data pipelines, business intelligence bi, data modeling, star schema, snowflake schema, sql, data quality, process automation, data profiling, data cleansing, power bi, ssrs, excel, cognos, power apps, power automate, power bi flows, qa, uat, agile development, documentation, mentoring, training, reporting, data visualization, crossdepartmental collaboration, adhoc reporting, computer science, information systems, engineering, analytics, python, r, big query, microsoft azure, google cloud platform, rpa, data types, data structures, database systems, dimensional modeling, data normalization, microsoft excel, vba macros","adhoc reporting, agile development, analytics, big query, business intelligence bi, cognos, computer science, crossdepartmental collaboration, data engineering, data normalization, data profiling, data quality, data structures, data types, database systems, datacleaning, datamodeling, datapipeline, dimensional modeling, documentation, engineering, etl, excel, google cloud platform, information systems, mentoring, microsoft azure, microsoft excel, power apps, power automate, power bi flows, powerbi, process automation, python, qa, r, reporting, rpa, snowflake schema, sql, ssrs, star schema, training, uat, vba macros, visualization"
Senior Data Engineer / Architect - Hybrid,CVS Health,Greater Phoenix Area,https://www.linkedin.com/jobs/view/senior-data-engineer-architect-hybrid-at-cvs-health-3786526351,2023-12-17,Rockhampton, Australia,Associate,Hybrid,"Bring your heart to CVS Health.
Every one of us at CVS Health shares a single, clear purpose: Bringing our heart to every moment of your health. This purpose guides our commitment to deliver enhanced human-centric health care for a rapidly changing world. Anchored in our brand — with heart at its center — our purpose sends a personal message that how we deliver our services is just as important as what we deliver.
Our Heart At Work Behaviors™ support this purpose. We want everyone who works at CVS Health to feel empowered by the role they play in transforming our culture and accelerating our ability to innovate and deliver solutions to make health care more personal, convenient and affordable.
Position Summary
Join the Data and Analytics Engineering team to drive data, analytics, and insights transforming the healthcare industry. As a core member of the data engineering team, you will be responsible to build and deliver best in class data products and analytics solutions on Google Cloud / Azure Platform and on-prem infrastructure for the Pharmacy Specialty Service Analytics & Behavior Change group.
Collaborate with business partners to identify opportunities to leverage structured, semi structured, and unstructured data to make analytics faster, more insightful, and more efficient.
Build the next generation data solutions using core data technologies in support of next best actions.
Execute a highly scalable data architecture and data platforms which enables data integration, data management, and analysis of massive data sets from multiple channels.
Work closely with data scientists on preparing feature sets, deep exploration of data elements, integrate ML and NLP based models on the data pipeline and productionize models.
Re-engineer existing on-prem solutions to the cloud platform as well as address performance optimization of the data pipelines.
Bring a DevOps mindset to enable big data and batch/real-time analytical solutions that leverage emerging technologies.
You will be working in a hybrid role (3 days a week onsite) in one of the CVS Health or Aetna offices in Irving, TX; Phoenix, AZ; Boston, MA; Wellesley, MA; Hartford, CT; NYC or Chicago, IL.
Required Qualifications
5+ years experience in data analytics, including hands-on experience on data integration
5+ years experience ETL pipelines, data architecture and design patterns, data security, cloud engineering and automation
2+ years experience with Python and/or PySpark
2+ years experience with cloud platforms ( GCP preferred , AWS or Azure acceptable )
Preferred Qualifications
Ability to communicate technical ideas and results to non-technical clients in written and verbal form. Collaboration and communication skills within and across teams.
Proven ability to create innovative solutions to highly complex technical problems.
Able to translate analytical problems and business requirements into technical requirements and further into implementation.
Strong analytical, conceptual thinking and problem-solving skills.
Experience building and implementing data transformation and processing solutions.
Ability to understand and build complex systems and solve challenging analytical problems.
Education
Bachelor's degree or equivalent work experience in Computer Science, Engineering, Machine Learning, or related discipline
Master’s degree preferred
Pay Range
The typical pay range for this role is:
$90,000.00 – $130,000.00
This pay range represents the base hourly rate or base annual full-time salary for all positions in the job grade within which this position falls. The actual base salary offer will depend on a variety of factors including experience, education, geography and other relevant factors. This position is eligible for a CVS Health bonus, commission or short-term incentive program in addition to the base pay range listed above.
In addition to your compensation, enjoy the rewards of an organization that puts our heart into caring for our colleagues and our communities. The Company offers a full range of medical, dental, and vision benefits. Eligible employees may enroll in the Company’s 401(k) retirement savings plan, and an Employee Stock Purchase Plan is also available for eligible employees. The Company provides a fully-paid term life insurance plan to eligible employees, and short-term and long term disability benefits. CVS Health also offers numerous well-being programs, education assistance, free development courses, a CVS store discount, and discount programs with participating partners. As for time off, Company employees enjoy Paid Time Off (“PTO”) or vacation pay, as well as paid holidays throughout the calendar year. Number of paid holidays, sick time and other time off are provided consistent with relevant state law and Company policies.
For more detailed information on available benefits, please visit jobs.CVSHealth.com/benefits
CVS Health requires certain colleagues to be fully vaccinated against COVID-19 (including any booster shots if required), where allowable under the law, unless they are approved for a reasonable accommodation based on disability, medical condition, religious belief, or other legally recognized reasons that prevents them from being vaccinated.
You are required to have received at least one COVID-19 shot prior to your first day of employment and to provide proof of your vaccination status or apply for a reasonable accommodation within the first 10 days of your employment. Please note that in some states and roles, you may be required to provide proof of full vaccination or an approved reasonable accommodation before you can begin to actively work.
CVS Health is committed to recruiting, hiring, developing, advancing, and retaining individuals with disabilities. As such, we strive to provide equal access to the benefits and privileges of employment, including the provision of a reasonable accommodation to perform essential job functions. CVS Health can provide a request for a reasonable accommodation, including a qualified interpreter, written information in other formats, translation or other services through ColleagueRelations@CVSHealth.com If you have a speech or hearing disability, please call 7-1-1 to utilize Telecommunications Relay Services (TRS). We will make every effort to respond to your request within 48 business hours and do everything we can to work towards a solution.
Show more
Show less","Data engineering, Analytics, ETL, Machine learning, Natural language processing, Data architecture, Data Integration, Cloud computing, AWS, Azure, Python, PySpark, GCP, Hadoop, Big data, Business intelligence, Data visualization, DevOps, SQL, NoSQL, Data mining, Data warehousing, Data science","data engineering, analytics, etl, machine learning, natural language processing, data architecture, data integration, cloud computing, aws, azure, python, pyspark, gcp, hadoop, big data, business intelligence, data visualization, devops, sql, nosql, data mining, data warehousing, data science","analytics, aws, azure, big data, business intelligence, cloud computing, data architecture, data engineering, data integration, data mining, data science, datawarehouse, devops, etl, gcp, hadoop, machine learning, natural language processing, nosql, python, spark, sql, visualization"
Sr. Data Engineer,Harnham,Los Angeles Metropolitan Area,https://www.linkedin.com/jobs/view/sr-data-engineer-at-harnham-3776283100,2023-12-17,Rockhampton, Australia,Associate,Hybrid,"Data Engineer
6-Month Contract
$70 to $75/hour
Location:
Onsite presence required 2 days a week in either Los Angeles or New York City
Overview:
Join the dynamic Data Technology team at the large entertainment, focusing on advertising and marketing initiatives. As a Data Engineer, you will be an integral part of a 12-member team, including analysts and engineers, contributing to customer data management with a strong emphasis on data collection and privacy.
Project Focus:
The primary project involves the development of a consent data management platform, specifically examining user consent and data-sharing practices. The objective is to transition this platform onto a new partner. The initial phase has entailed scoping out the project's requirements.
Day-to-Day Responsibilities:
Facilitate data movement
Build frameworks for data deletion requests
Essential Skills:
AWS (Lambda, Step Functions, API Management)
Experience with distributed databases (Snowflake, Redshift, Bitquery, etc.)
Proficient in SQL and Python
Minimum of 4 years of experience in data engineering, with flexibility for strong candidates
Nice-to-Have Skills:
Specific experience in building frameworks and server-less computations
Experience in building APIs, with some exposure to legacy APIs on Spark
Background in software engineering (versus ETL background)
Experience in marketing/advertising or consent data, or a background in entertainment/media
Show more
Show less","AWS Lambda, AWS Step Functions, AWS API Management, Distributed Databases, SQL, Python, Data Engineering, Data Movement, Data Deletion Requests, Frameworks, Serverless Computations, APIs, Legacy APIs, Spark, Software Engineering, ETL, Marketing, Advertising, Consent Data, Entertainment, Media","aws lambda, aws step functions, aws api management, distributed databases, sql, python, data engineering, data movement, data deletion requests, frameworks, serverless computations, apis, legacy apis, spark, software engineering, etl, marketing, advertising, consent data, entertainment, media","advertising, apis, aws api management, aws lambda, aws step functions, consent data, data deletion requests, data engineering, data movement, distributed databases, entertainment, etl, frameworks, legacy apis, marketing, media, python, serverless computations, software engineering, spark, sql"
Data Software Engineer,C2R Ventures,New York City Metropolitan Area,https://www.linkedin.com/jobs/view/data-software-engineer-at-c2r-ventures-3775618610,2023-12-17,Rockhampton, Australia,Associate,Hybrid,"Hedge Fund managing 8 Billion in AUM is seeking an Software Developer who will sit on the Data team and will work closely with the front office to improve data ingestion / analysis.
Responsibilities
Deliver data quickly including thoughtful design, unit testing, integration testing, and development
Maintain and enhance the data platform
Partner with front office analysts and quants, on variety of financial, macroeconomic, and alternative data
Qualifications
Bachelor's degree or equivalent experience in Computer Science or related field
Development experience with programming languages (Python
Strong SQL experience (Microsoft SQL Server a plus)
Experience with data sets that are specific to an industry (financial data preferred)
2+ Years of Experience
Show more
Show less","Python, SQL, Microsoft SQL Server, Data Analysis, Data Ingestion, Data Platform, Testing, Financial Data, Macroeconomic Data, Alternative Data","python, sql, microsoft sql server, data analysis, data ingestion, data platform, testing, financial data, macroeconomic data, alternative data","alternative data, data ingestion, data platform, dataanalytics, financial data, macroeconomic data, microsoft sql server, python, sql, testing"
Data Engineer - Associate,Sumitomo Mitsui Banking Corporation,"New York, NY",https://www.linkedin.com/jobs/view/data-engineer-associate-at-sumitomo-mitsui-banking-corporation-3717298077,2023-12-17,Rockhampton, Australia,Associate,Hybrid,"SMBC Group is a top-tier global financial group. Headquartered in Tokyo and with a 400-year history, SMBC Group offers a diverse range of financial services, including banking, leasing, securities, credit cards, and consumer finance. The Group has more than 130 offices and 80,000 employees worldwide in nearly 40 countries. Sumitomo Mitsui Financial Group, Inc. (SMFG) is the holding company of SMBC Group, which is one of the three largest banking groups in Japan. SMFG’s shares trade on the Tokyo, Nagoya, and New York (NYSE: SMFG) stock exchanges.
In the Americas, SMBC Group has a presence in the US, Canada, Mexico, Brazil, Chile, Colombia, and Peru. Backed by the capital strength of SMBC Group and the value of its relationships in Asia, the Group offers a range of commercial and investment banking services to its corporate, institutional, and municipal clients. It connects a diverse client base to local markets and the organization’s extensive global network. The Group’s operating companies in the Americas include Sumitomo Mitsui Banking Corp. (SMBC), SMBC Nikko Securities America, Inc., SMBC Capital Markets, Inc., SMBC Rail Services LLC, Manufacturers Bank, JRI America, Inc., SMBC Leasing and Finance, Inc., Banco Sumitomo Mitsui Brasileiro S.A., and Sumitomo Mitsui Finance and Leasing Co., Ltd.
The anticipated salary range for this role is between $75,000.00 and $150,000.00. The specific salary offered to an applicant will be based on their individual qualifications, experiences, and an analysis of the current compensation paid in their geography and the market for similar roles at the time of hire. The role may also be eligible for an annual discretionary incentive award. In addition to cash compensation, SMBC offers a competitive portfolio of benefits to its employees.
Role Description
SMBC is in the process of leading a Digital Transformation across our Americas Division as we continue to modernize our technology, focus on our data-driven approach, grow, and plan for the future. As a result of this expansion, we are seeking an experienced software engineer, with 3-7 years of experience to support the design and development of a strategic data platform for SMBC Capital Markets and Nikko Securities Group.
Role Objectives
This role will be part of the data strategy team spanning across the SMBC Capital Markets and Nikko securities teams, SMBC Americas Division’s broker-dealer and swap-dealer entities
This role will be involved in the active development of the data platform, beginning with the establishment of a reference data system for securities and pricing data, and later moving to other data domains
This role will be part of the SMBC CM/Nikko development team, and will need to follow internal developments standards to contribute to the overall agenda of the team
Qualifications And Skills
3-7 years of experience in a large & complex application development environment, preferably in the financial services industry
Basic understanding of equities and fixed-income instruments
Strong SQL skills and experience with RDMS
3+ years of experience programming in Python and Java,
Experience with the DevOps lifecycle (git, Jenkins, etc.) is preferred
Experience with Jira/Confluence
Experience with REST web services and microservice architecture
Good understanding of ETL/ELT
Experience with Cloud solutions (AWS or Azure) is preferred
Additional Requirements
SMBC’s employees participate in a hybrid workforce model that provides employees with an opportunity to work from home, as well as, from an SMBC office. SMBC requires that employees live within a reasonable commuting distance of their office location. Prospective candidates will learn more about their specific hybrid work schedule during their interview process.
SMBC is an EO employer – M/F/Veteran/Disability. SMBC provides reasonable accommodations for employees and applicants with disabilities consistent with applicable law. If you need reasonable accommodation during the application process, please let us know at accommodations@smbcgroup.com.
SMBC’s employees participate in a hybrid workforce model that provides employees with an opportunity to work from home, as well as, from an SMBC office. SMBC requires that employees live within a reasonable commuting distance of their office location. Prospective candidates will learn more about their specific hybrid work schedule during their interview process.
We are an equal employment opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, national origin, disability status, protected veteran status or any other characteristic protected by law. SMBC provides reasonable accommodations for employees and applicants with disabilities consistent with applicable law. If you need a reasonable accommodation during the application process, please let us know at accommodations@smbcgroup.com.
Show more
Show less","Software Engineer, Data Platform, Design, Development, SQL, RDMS, Python, Java, DevOps, Git, Jenkins, Jira, Confluence, REST web services, Microservice architecture, ETL/ELT, Cloud solutions, AWS, Azure, Equities, Fixedincome instruments","software engineer, data platform, design, development, sql, rdms, python, java, devops, git, jenkins, jira, confluence, rest web services, microservice architecture, etlelt, cloud solutions, aws, azure, equities, fixedincome instruments","aws, azure, cloud solutions, confluence, data platform, design, development, devops, equities, etlelt, fixedincome instruments, git, java, jenkins, jira, microservice architecture, python, rdms, rest web services, software engineer, sql"
Data Engineer - Hybrid,Vertex Education,"Chandler, AZ",https://www.linkedin.com/jobs/view/data-engineer-hybrid-at-vertex-education-3775477072,2023-12-17,Rockhampton, Australia,Associate,Hybrid,"Be the one who makes a difference!
At Vertex Education we are a team of high achievers, courageous leaders, and passionate believers in changing lives through education. As a purpose-led education services provider, our mission is destined to benefit many and yet it starts with just one person inspired to work together with us to make a memorable and meaningful difference for our clients, schools, students, and communities. Be the one who makes a difference—with us.
Data Engineer
At Vertex Education, we understand the critical role that data plays in unlocking
insights, driving innovation, and making informed business decisions. As we expand
and refine our Business Intelligence capabilities, we're on the lookout for a Data
Engineer who can elevate our data infrastructure to the next level. This position is
more than just managing databases—it's about creating an environment where data
is easily accessible, flawlessly accurate, and primed for analysis. As a pivotal
member of our BI team, the Data Engineer will collaborate closely with BI analysts
and the BI manager, owning the entire lifecycle of our data processes. From
designing new pipelines to ensuring the integrity of our data, this role is essential to
ensuring our analysts have the tools they need to deliver rapid insights.
Design & Implementation of Data Pipelines
Design & Architecture: Design the data flow and architecture for new
pipelines, ensuring scalability and compatibility with the existing
infrastructure.
Integration with Sources: Establish connections to various structured data
sources, utilizing tools like Fivetran for seamless integration or write custom
Python as necessary.
Data Transformation & Modeling: Implement and refine data
transformation processes in Snowflake to convert structured data into
optimized models suitable for analysis using DBT. Ensure efficient data
retrieval and alignment with analytical needs.
Tool Evaluation & Integration: Explore and integrate new tools or
technologies that can enhance the overall efficiency and capability of the
data engineering ecosystem.
Quality Assurance & Data Integrity
Quality Assurance: Implement initial data quality checks to ensure
accuracy and integrity of data ingested from new sources.
Audit & Evaluation: Periodically review existing pipelines to identify
performance bottlenecks, data quality issues, or outdated practices.
Monitoring & Alerting: Implement robust, automated monitoring solutions
for the entire data engineering environment, ensuring timely alerts and
prompt resolution of any data quality issues.
Data Quality Enhancement: Introduce or refine data validation, cleansing,
and deduplication processes to elevate data accuracy and reliability.
Infrastructure & Environment Management
Infrastructure Optimization: Regularly evaluate and optimize the Azure,
Fivetran, and Snowflake environments for performance, cost-efficiency, and
scalability.
Data Security & Compliance: Prioritize and enforce robust data security
measures, ensuring data privacy regulations are adhered to and the
infrastructure remains compliant with industry standards.
Best Practices & Training: Stay updated with industry best practices and
ensure that the BI team is trained and aligned with the latest data
engineering standards.
Documentation & Continuous Improvement
Comprehensive Documentation: Produce and update documentation for
both new and existing pipelines, capturing source details, transformation
logic, and any known limitations, ensuring it remains current and accurate.
Stakeholder Engagement: Foster close collaboration with the BI analysts,
understanding their evolving data needs and adapting pipelines accordingly.
Feedback Mechanism: Establish and sustain a robust feedback loop with
the BI team and potential future data scientists, ensuring rapid identification
and rectification of performance bottlenecks.
Innovation & Best Practices: Stay abreast of industry trends, proactively
introducing innovative techniques, tools, or methodologies that can enhance
the efficiency and effectiveness of the data engineering ecosystem.
Education and Experience
Education:
Bachelor’s degree in Computer Science, Information Systems, Engineering, or
a related field.
Master’s degree in a related field is a plus.
Experience:
Minimum of 3-5 years of experience in data engineering or a related role.
Proven expertise in working with cloud platforms, preferably Azure.
Hands-on experience with data pipeline tools such as Fivetran, DBT, and
Snowflake.
Expertise in Python and basic software engineering best practices
Familiarity with BI tools, with Tableau experience being a distinct advantage.
Demonstrated ability to design, build, and maintain scalable and robust data
pipelines.
Prior experience working in close collaboration with BI analysts or data
scientists is a plus.
Certifications (optional but advantageous):
Azure Data Engineer Associate certification or other relevant cloud
certifications.
Certifications related to specific data engineering tools or platforms, such as
Snowflake SnowPro Core Certification, SnorPro Advanced Data Engineer Cert,
or other comparable.
Be excited to be a part of our team and grow your career with us!
Be the one who enables us to positively impact over 25,000 students across multiple states while driving our growth forward so we can enrich even more lives. Be the one who helps us achieve excellence for the 26 schools we support with academics, finance, technology, human resources, communications, marketing, facilities, construction, and food services. Be the one who is a diverse thinker, a team player, a smart risk taker, an innovator, and a difference maker by encouraging others to climb higher and reach farther to further education.
Be yourself surrounded by wonderful people who care about you, value your unique skills, and lift you up.
Be supported in your work by caring leaders and team members who want you to succeed.
Be empowered to make a difference and climb higher and reach farther to change lives through education.
Be well in all aspects of your life from your physical, mental, and emotional wellbeing to your finances.
Enjoy industry-leading pay, rewards, referral bonuses, with unlimited flexible paid time-off for performance.
Be able to care for your health and your family with comprehensive medical, dental and vision benefits and invest in your future with 401(k) plans with a 6% employer match on your contributions.
Enhance your growth and development with mentoring and money to take training classes.
Thrive in a welcoming, supportive, and inclusive environment where we treat others with fairness and respect, celebrate diversity, and elevate equality and inclusion as an equal opportunity employer.
Be the one who makes a difference!
With an innovative mind, a hungry heart, and engaging spirit you can change lives through education. Be a part of Vertex Education and let’s make a difference together.
Apply Today!
Show more
Show less","Data Engineering, Data Pipelines, Data Transformation, Data Modeling, Data Quality, Data Integrity, Data Security, Data Compliance, Azure, Fivetran, Snowflake, DBT, Python, Software Engineering, BI Tools, Tableau, Data Analytics, Data Science, Cloud Platforms, Certifications, Computer Science, Information Systems, Engineering","data engineering, data pipelines, data transformation, data modeling, data quality, data integrity, data security, data compliance, azure, fivetran, snowflake, dbt, python, software engineering, bi tools, tableau, data analytics, data science, cloud platforms, certifications, computer science, information systems, engineering","azure, bi tools, certifications, cloud platforms, computer science, data compliance, data engineering, data integrity, data quality, data science, data security, data transformation, dataanalytics, datamodeling, datapipeline, dbt, engineering, fivetran, information systems, python, snowflake, software engineering, tableau"
Big Data Developer,Phaxis,"Dallas, TX",https://www.linkedin.com/jobs/view/big-data-developer-at-phaxis-3780191547,2023-12-17,Rockhampton, Australia,Associate,Hybrid,"Big Data Developer and Analysis Engineer
Dallas TX-Hybrd!
MANDARIN SPEAKING is preferred!
$85,000-110,000 Base + Great Benefits!
We are seeking a Big Data Developer and Analysis Engineer to join our team based in the Dallas, TX, United States. You will be responsible for developing and maintaining high-quality data pipelines and data analytics and visualization tools to support the company's business decision-making and strategic planning.
Responsibilities:
1. Collaborate with business teams to understand their data requirements and provide appropriate solutions.
2. Collect, organize, and analyze data to support business decision-making and strategic planning.
3. Identify trends and patterns in the business through data mining and analysis and provide recommendations for business growth and efficiency improvements.
4. Assist in defining best practices for data collection, pipeline, and analytics development, and ensure data accuracy and consistency.
5. Monitor and maintain existing data pipelines, analytics, and visualization tools, promptly addressing issues and providing support.
Requirements:
1. Bachelor's degree or higher in Computer Science, Data Analytics, or a related field preferred.
2. Experience in data management and developing data pipelines with Airflow, Apache Spark, Flink and other data ETL tools strongly recommended.
3. Proficiency in SQL for data querying and analysis, with the ability to clean and process data.
4. Proficiency in Python for data mining.
5. Strong communication and teamwork skills, able to effectively collaborate and coordinate with different departments and teams.
6. Strong problem-solving and self-learning abilities, able to quickly adapt to new technologies and tools.
Show more
Show less","Data Pipelines, Data Analytics, Visualization Tools, Business DecisionMaking, Data Mining, Business Growth, Efficiency Improvements, Data Collection, Data Consistency, Data Accuracy, Airflow, Apache Spark, Flink, Data ETL Tools, SQL, Python, ProblemSolving, Communication, Teamwork","data pipelines, data analytics, visualization tools, business decisionmaking, data mining, business growth, efficiency improvements, data collection, data consistency, data accuracy, airflow, apache spark, flink, data etl tools, sql, python, problemsolving, communication, teamwork","airflow, apache spark, business decisionmaking, business growth, communication, data accuracy, data collection, data consistency, data etl tools, data mining, dataanalytics, datapipeline, efficiency improvements, flink, problemsolving, python, sql, teamwork, visualization tools"
Machine Learning Data Engineer,Tier4 Group,"Lehi, UT",https://www.linkedin.com/jobs/view/machine-learning-data-engineer-at-tier4-group-3783947825,2023-12-17,Rockhampton, Australia,Associate,Hybrid,"Job Title: Data Engineer (Machine Learning)
Location: Lehi, UT
Employment Type: Full-time, Hybrid 3 days onsite
Our client is seeking a skilled Data Engineer to join their dynamic and growing team, contributing to the forefront of technological innovation. This role involves leveraging cutting-edge AWS tools and technologies to handle large volumes of data, design and manage datasets, implement machine learning models, and drive insightful data-driven solutions.
Key Responsibilities:
Utilize AWS tools such as Kinesis, Glue, and Data Pipeline to collect, store, and process extensive data volumes efficiently.
Design, manage, and analyze large datasets, identifying patterns, anomalies, and ensuring data quality using services like Amazon S3, RDS, and Redshift.
Develop, train, and deploy machine learning models, employing ML Ops methodologies (classification, regression, clustering, NLP, CNN) through AWS SageMaker or related services.
Implement analytical algorithms focusing on predictive and prescriptive modeling.
Create visualizations and dashboards, either using tools like Amazon QuickSight or integrating with platforms such as Tableau and PowerBI.
Develop and manage DB objects like tables, procedures, triggers, and functions.
Proficiently write complex SQL queries, optimizing performance and tuning queries as necessary.
Extract data from SQL/NoSQL databases for generating ad hoc reports and analysis.
Script and program using Python and R.
Document procedures, workflows, and best practices related to data processing and modeling.
Job Specifications:
5+ years of experience as a Data Engineer, demonstrating proficiency with large datasets and data science methodologies.
Machine Learning experience
3+ years of experience with SQL-based database technologies (Oracle, SQL, MySQL, Postgres), including store procedures, functions, triggers, etc.
3+ years of experience with data visualization tools such as Power BI, AWS QuickSight, etc.
3+ years of hands-on experience with cloud services, preferably AWS (SageMaker, S3, EC2, EMR, RDS, Airflow, Redshift).
Familiarity with big data tools like Hadoop, Spark, Kafka, etc.
Strong programming skills in languages like Python, Java, or Scala.
Experience with machine learning frameworks like TensorFlow or PyTorch and ETL tools
Exceptional communication and teamwork skills.
This role offers an exciting opportunity to work in a collaborative environment, leveraging advanced technologies, and contributing to impactful data-driven solutions. If you are passionate about data engineering and thrive in a dynamic, cutting-edge tech environment, we encourage you to apply!
Show more
Show less","AWS tools, Hadoop, Spark, SQL, NoSQL, Python, R, Power BI, AWS QuickSight, Tableau, SQL Server, Oracle, MySQL, Postgres, TensorFlow, PyTorch, Data Pipeline, Kinesis, Glue, SageMaker, Redshift, RDS, EMR, Airflow, Kafka, Machine Learning, Data visualization, Tableau, PowerBI, NLP, CNN, Classification, Regression, Clustering","aws tools, hadoop, spark, sql, nosql, python, r, power bi, aws quicksight, tableau, sql server, oracle, mysql, postgres, tensorflow, pytorch, data pipeline, kinesis, glue, sagemaker, redshift, rds, emr, airflow, kafka, machine learning, data visualization, tableau, powerbi, nlp, cnn, classification, regression, clustering","airflow, aws quicksight, aws tools, classification, clustering, cnn, data pipeline, emr, glue, hadoop, kafka, kinesis, machine learning, mysql, nlp, nosql, oracle, postgres, powerbi, python, pytorch, r, rds, redshift, regression, sagemaker, spark, sql, sql server, tableau, tensorflow, visualization"
Database Engineer (Salesforce),KnowBe4,"Clearwater, FL",https://www.linkedin.com/jobs/view/database-engineer-salesforce-at-knowbe4-3774258965,2023-12-17,Rockhampton, Australia,Associate,Hybrid,"About KnowBe4
KnowBe4, the provider of the world's largest security awareness training and simulated phishing platform, is used by tens of thousands of organizations around the globe. KnowBe4 enables organizations to manage the ongoing problem of social engineering by helping them train employees to make smarter security decisions, every day.
Fortune has ranked us as a best place to work for women, for millennials, and in technology for four years in a row! We have been certified as a ""Great Place To Work"" in 8 countries, plus we've earned numerous other prestigious awards, including Glassdoor's Best Places To Work.
Our team values radical transparency, extreme ownership, and continuous professional development in a welcoming workplace that encourages all employees to be themselves. Whether working remotely or in-person, we strive to make every day fun and engaging; from team lunches to trivia competitions to local outings, there is always something exciting happening at KnowBe4.
The Database Engineer is a key IT professional responsible for the design and efficient use of an organization’s databases through collaboration with stakeholders. This role is critical in optimizing database architecture and integrations while maintaining best practices in data integrity, security, and performance.
Responsibilities
Manage and optimize our Salesforce database, ensuring data integrity, availability, and performance.
Leverage DemandTools to schedule, run, and monitor data management tasks and cleanup procedures.
Manage and maintain our database integrations, ensuring seamless data flow between Salesforce and external systems.
Design and implement database structures, procedures, and workflows that align with the company's needs and objectives.
Develop and manage database integrations
Work closely with cross-functional teams to understand data requirements, and provide technical expertise.
Monitor system health, and perform regular backups, restorations, and updates as necessary.
Resolve database-related incidents and problems in a timely and efficient manner.
Develop and maintain documentation for database designs, processes, and best practices.
Provide training and support to users to ensure optimal system utilization.
Stay current with the latest Salesforce updates, tools, and best practices to drive continuous improvement.
Act as lead on duplicate management and prevention
Minimum Qualifications
Bachelor's degree in Computer Science, Information Technology, or a related field.
Minimum of 3 years of experience as a Database Engineer or similar role with significant Salesforce exposure.
Experience with SaaS CRM/Lead Management data structures a big plus
Proven familiarity with DemandTools and Celigo integrations or equivalent tools.
Strong SQL skills and a deep understanding of relational database principles.
Excellent problem-solving abilities and attention to detail.
Strong communication skills, both written and verbal.
Familiarity with Salesforce best practices, trends, and the wider Salesforce ecosystem.
Ability to work both independently and in a team-oriented environment.
Database Management Systems (DBMS): Proficiency in working with various DBMS software, such as MySQL, PostgreSQL, Oracle, SQL Server, or MongoDB, depending on the organization's requirements.
Database Design: Understanding of database design principles, including data modeling, schema design, and normalization, to create efficient and effective databases.
SQL: Strong knowledge of SQL (Structured Query Language) to create, modify, and query databases. This includes writing complex queries, stored procedures, and triggers.
Data Security: Implementing and managing security measures to protect sensitive data, such as user access control, encryption, and auditing.
Our Fantastic Benefits
We offer company-wide monthly bonuses, employee referral bonuses, 401k matching (US), fully paid medical insurance (US), open/generous paid time off (length varies by country), parental leave (length varies by country), adoption assistance, tuition reimbursement, certification reimbursement, certification completion bonuses, gym benefits, and a relaxed dress code - all in a modern, high-tech, and fun work environment. For more details about our benefits, visit www.knowbe4.com/careers/benefits.
Note: An applicant assessment and background check may be part of your hiring procedure.
Individuals seeking employment at KnowBe4 are considered without prejudice to race, color, religion, national origin, age, sex, marital status, ancestry, physical or mental disability, veteran status, gender identity, sexual orientation or any other characteristic protected under applicable federal, state, or local law. If you require reasonable accommodation in completing this application, interviewing, completing any pre-employment testing, or otherwise participating in the employee selection process, please visit www.knowbe4.com/careers/request-accommodation.
No recruitment agencies, please.
Show more
Show less","Salesforce, SQL, Relational database principles, Data modeling, Schema design, Normalization, Data security, User access control, Encryption, Auditing, MySQL, PostgreSQL, Oracle, SQL Server, MongoDB","salesforce, sql, relational database principles, data modeling, schema design, normalization, data security, user access control, encryption, auditing, mysql, postgresql, oracle, sql server, mongodb","auditing, data security, datamodeling, encryption, mongodb, mysql, normalization, oracle, postgresql, relational database principles, salesforce, schema design, sql, sql server, user access control"
Data Engineer III,ZoomInfo,"Waltham, MA",https://www.linkedin.com/jobs/view/data-engineer-iii-at-zoominfo-3782591665,2023-12-17,Rockhampton, Australia,Associate,Hybrid,"At ZoomInfo, we encourage creativity, value innovation, demand teamwork, expect accountability and cherish results. We value your take charge, take initiative, get stuff done attitude and will help you unlock your growth potential. One great choice can change everything. Thrive with us at ZoomInfo.
The Responsibilities Of This Opportunity Include
Develop Data Pipelines using Java/Scala and Apache Beam Dataflow to move data or transform data.
Consolidate/join datasets to create easily consumable, performant, and consistent information
Look for ways to improve processes and take initiative to implement them
Evaluate new technology and advise on our data lake ecosystem
Working with stakeholders including data, design, product and executive teams and assisting them with data-related technical issues.
Experience on Data pipelines and workflow management tools ( Airflow) Cloud-based solutions (e.g. AWS Google Cloud).
Creates and maintains documentation for business end users and other data analysts.
Determine where in our infrastructure we should house our data based on the use case and data model
Collaborate with Data Scientists in order to design scalable implementations of their models
A Successful Candidate Will Have The Following Qualifications
Degree (Masters preferred) in Computer Science, Information Systems, Data Science, or related field and 3-5 years of experience in data engineering, or an equivalent combination of education and experience.
Expert and Hands-On development in distributed computing environment using Hadoop, Java/Scala, Apache Beam [Dataflow] and Apache Airflow
Advanced SQL knowledge and understanding
Experience architecting solutions in collaboration with development and data science teams.
Experience working with third party APIs for data collection
Ability to communicate effectively with, and provide excellent customer service to, stakeholders at all levels of the organization.
Experience mentoring/coaching business end users and junior analysts
Self-motivated; able to work independently to complete tasks and collaborate with others to identify and implement solutions.
Expertise and Hand-On working knowledge of AWS and GCP
Experience with Git/Github or other version control systems
Familiarity with Snowflake or DataBricks is a plus.
The US base salary range for this position is between $110,400.00- $148,000.00 + bonus + equity + benefits.
Actual compensation offered will be based on factors such as the candidate’s work location, qualifications, skills, experience and/or training. Your recruiter can share more information about the specific salary range for your desired work location during the hiring process. We want our employees and their families to thrive.
In addition to comprehensive benefits, we offer holistic mind, body and lifestyle programs designed for overall well-being. Learn more about ZoomInfo benefits here.
About Us
ZoomInfo (NASDAQ: ZI) is the trusted go-to-market platform for businesses to find, acquire, and grow their customers. It delivers accurate, real-time data, insights, and technology to more than 35,000 companies worldwide. Businesses use ZoomInfo to increase efficiency, consolidate technology stacks, and align their sales and marketing teams — all in one platform.
ZoomInfo may use a software-based assessment as part of the recruitment process. More information about this tool, including the results of the most recent bias audit, is available here.
ZoomInfo is proud to be an Equal Opportunity employer. We are committed to equal employment opportunities for applicants and employees regardless of sex, race, age, color, national origin, sexual orientation, gender identity, marital status, disability status, religion, protected military or veteran status, medical condition, or any other characteristic or status protected by applicable law. At ZoomInfo, we also consider qualified candidates with criminal histories, consistent with legal requirements.
Show more
Show less","Java, Scala, Apache Beam Dataflow, Data Pipelines, Airflow, Hadoop, Apache Airflow, AWS, Google Cloud, SQL, Git/Github, Snowflake, DataBricks","java, scala, apache beam dataflow, data pipelines, airflow, hadoop, apache airflow, aws, google cloud, sql, gitgithub, snowflake, databricks","airflow, apache airflow, apache beam dataflow, aws, databricks, datapipeline, gitgithub, google cloud, hadoop, java, scala, snowflake, sql"
Data Analyst,Alpine Solutions Group,"Dallas, TX",https://www.linkedin.com/jobs/view/data-analyst-at-alpine-solutions-group-3777303108,2023-12-17,Rockhampton, Australia,Associate,Hybrid,"MUST:
2+ years of very strong with SQL querying
ETL knowledge , we write them in SQL everything runs off of database
hands on experience with BI tools
understanding with platform maintenance within MYSQL (they are Maria DB in MYSQL)
ability to communicate with the c suite
experience building out dashboards and reports, could be within tableau or any other BI tools.
Ability to help automate manual reporting/manual data entry
PLUS:
Masters in Data Science
DAY TO DAY:
Our client in Dallas, a fast growing consulting firm, is looking to hire a BI Analyst to come aboard their team as a full time employee. This position will be hybrid in Dallas and report into the Lead BI Analyst. This person will be helping to set up reporting through a newly selected BI tool (we are looking for this person to help spearhead the selection of that tool and help maintain is at well). We are looking for someone who can help standardize reporting and work to help various departments in the company who still do manual data entry/manual reporting to automate those processes. This person will play a role in analyzing and transforming the data into insights for our company, and will work cross departmental. Other responsibilities could be: gathering, cleaning, and analyzing data to identify trends, develop dashboards/reports, monitor KPI’s, and conduct ad-hoc analysis to support stakeholders as needed.
Show more
Show less","SQL, ETL, Business Intelligence (BI) tools, Tableau, Data Science, Data Analysis, Data Visualization, Data Transformation, Data Mining, Machine Learning, Data Warehousing, MYSQL, Maria DB","sql, etl, business intelligence bi tools, tableau, data science, data analysis, data visualization, data transformation, data mining, machine learning, data warehousing, mysql, maria db","business intelligence bi tools, data mining, data science, data transformation, dataanalytics, datawarehouse, etl, machine learning, maria db, mysql, sql, tableau, visualization"
Data Analyst,Integrated Healthcare Association,"California, United States",https://www.linkedin.com/jobs/view/data-analyst-at-integrated-healthcare-association-3780007524,2023-12-17,Rockhampton, Australia,Associate,Hybrid,"About the job
Reporting to the Director of Analytics, IHA is looking for a data analyst to conduct data validation, develop reasonability checks, and provide insights in support of recurring program needs, business development efforts, and health policy technical assistance. The ideal candidate has a keen attention to detail, a deep intuition for anticipating data issues, and can efficiently and accurately handle multiple tasks. This is a unique opportunity to work on challenging projects that move the needle on key health care industry pain points in California.
About IHA
At Integrated Healthcare Association (IHA), we bring the healthcare community together to overcome barriers to high-value care. For more than 25 years, we have championed a more integrated care system that improves quality and affordability for patients in California and beyond. As a non-profit business league, we are funded by the healthcare industry to help make the system work better for everyone. Our mission-driven executive team and staff work with cross-industry leaders to solve big, systematic challenges such as performance measurement and provider data management. With core values of Collaboration, Candor, and Impact as our guide, we align healthcare around shared goals—and new possibilities. We use data and insights to help everyone improve. We build what is needed to drive lasting change. Because we envision a healthcare system where patients get the best possible care at an affordable price.
Core Job Duties and Responsibilities:
Support the data intake validation process and ensure effective translation of technical requirements for internal and external stakeholders
Document, implement, and continuously improve reporting and analysis. This work will include data file transformation, validation of a variety of data sources, and comprehension of clinical quality, utilization, and cost data.
Support engineering and operations teams in data validation, data ingestion, file layout creation, benchmarking, and tailored analysis
Clearly and concisely summarize data and provide analytic insights to inform decision making for internal and external clients
Query datasets from highly normalized databases
Generate program results and deliverables based on a variety of IHA developed methodologies
Any other duties or ad hoc modeling as required to ensure the analytics team goals are met
Qualifications:
BA/BS in STEM discipline, public health, statistics, or related field with emphasis on analytical or quantitative skills.
1-3 years relevant experience
Passion for healthcare data and the healthcare industry
Intermediate knowledge of Python or R to analyze large datasets and support automation
Intermediate knowledge of Tableau
Proficiency in SQL
Have exceptional analytical and critical thinking skills
Well versed in combing through datasets, performing thorough validations and building impactful data visualizations.
Can communicate technical concepts clearly and effectively to various stakeholder personas
Bonus if you have:
A Master’s degree in Public Health or related discipline
Experience with using Hex
Experience with HEDIS measurement, claims-based data, or health information systems
Benefits:
Competitive salary
Great work environment (Lake Merrit in Oakland)
Free snacks, tea, and coffee
Subsidized gym membership
o ClassPass
Mental health support resources:
o Optum Counselors 24/7 (Online Therapy) – Talkspace
o Meditation Apps: Headspace and Calm
401K contribution
20 paid days off a year + 18 holidays
Medical/dental/vision coverage
Commuter benefits
Permanent US work authorization is a prerequisite to employment for this position.
Show more
Show less","SQL, Python, Tableau, R, Hex, HEDIS measurement, Claimsbased data, Health information systems, Data validation, Reporting, Analysis, Data visualization, Critical thinking, Analytical skills, Communication skills, Data intake, Data ingestion, File layout creation, Benchmarking, Tailored analysis, Program results, Deliverables, Datasets, Databases","sql, python, tableau, r, hex, hedis measurement, claimsbased data, health information systems, data validation, reporting, analysis, data visualization, critical thinking, analytical skills, communication skills, data intake, data ingestion, file layout creation, benchmarking, tailored analysis, program results, deliverables, datasets, databases","analysis, analytical skills, benchmarking, claimsbased data, communication skills, critical thinking, data ingestion, data intake, data validation, databases, datasets, deliverables, file layout creation, health information systems, hedis measurement, hex, program results, python, r, reporting, sql, tableau, tailored analysis, visualization"
Data Engineer (ETL),Hartford Funds,"Wayne, PA",https://www.linkedin.com/jobs/view/data-engineer-etl-at-hartford-funds-3704221317,2023-12-17,Rockhampton, Australia,Associate,Hybrid,"The ETL Data Engineer will be responsible for the analysis, design, and development of a highly efficient, highly scalable ETL process to address Hartford Funds’ data movement needs. This resource will draw on their past ETL experience and ability to think big-picture to create the most optimal solution to meet the business requirements. The resource will be a team player and will use their strong communications skills in everyday interaction with business analysts, the Business Intelligence team, and the project manager. The resource will be responsible for hands-on development as well as troubleshooting.
*This position will adhere to Hartford Funds’ hybrid work model, in which employees are expected to work in our Wayne, PA office location 3 days per week (Tuesdays, Wednesdays and Thursdays). Employees have the option of working remotely on Mondays and Fridays if they choose.
RESPONSIBILITIES:
Conduct data profiling, source-target mappings, ETL development, SQL tunings and optimization, testing and implementation
Provide business and technical analysis for initiatives focusing on Data Warehouse and Business Intelligence solutions
Influence and provide input on data warehouse design with (experienced based) recommendations for optimal structuring of data to support efficient processing
Develop detailed technical specifications and ETL documentation in collaboration with data warehouse architect and Business Intelligence developers
Perform peer reviews and provide feedback on ETL design and implementation
Design and develop ETL code, ETL audit control and scheduling processes
Develop testing strategy and conduct appropriate testing to identify and resolve data quality issues
Collaborate with Business Intelligence Developers through dashboard and application development process for requirements gathering, feedback on proposed designs and models, and acceptance testing
Build custom? Complex? Advanced? solutions independently, sometimes without documented/concrete requirements to work from or specific guidance from other developers
REQUIRED SKILLS/EXPERIENCE
Minimum of 3 years’ experience, with hands-on ETL experience in supporting data warehousing solutions
Bachelor's degree in computer science, information technology or another computer-based discipline
Must possess the following technical skills:
ETL Tools: Enterprise class ETL tool (Talend is a plus)
Databases & Utilities: Experience with enterprise relational databases (Snowflake experience preferred)
Expertise and fluency in SQL language is required
Knowledge of scripting languages and job schedulers is required (Powershell, etc.)
Experience with various integration patterns (e.g., Flat Files, Web Services, etc. is required
Knowledge of fundamental data modeling concepts (e.g., ER Diagrams, normalization, etc.) is required
Knowledge of data warehousing techniques (dimensional modeling / star schema, etc.) is required
Experience writing technical documentation is required
Must be comfortable/excited working in a fast-paced, high impact environment, with
Excellent troubleshooting and problem-solving skills; able to root cause and debug complex code in and efficient manner/with appropriate urgency
Ability to obtain a good understanding of the business context of a problem in order to ensure the solution is optimal in solving the business need
PREFERRED SKILLS/EXPERIENCE:
Familiarity with Python is a plus
XML/XSLT experience is a plus
Snowflake / Talend
Cloud platform experience (AWS/Azure)
Show more
Show less","ETL Tools, Talend, Snowflake, SQL, Powershell, Scripting Languages, Job Schedulers, Integration Patterns, Flat Files, Web Services, ER Diagrams, Normalization, Data Modeling, Data Warehousing, Dimensional Modeling, Star Schema, Technical Documentation, Troubleshooting, ProblemSolving, Python, XML/XSLT, Cloud Platforms, AWS, Azure","etl tools, talend, snowflake, sql, powershell, scripting languages, job schedulers, integration patterns, flat files, web services, er diagrams, normalization, data modeling, data warehousing, dimensional modeling, star schema, technical documentation, troubleshooting, problemsolving, python, xmlxslt, cloud platforms, aws, azure","aws, azure, cloud platforms, datamodeling, datawarehouse, dimensional modeling, er diagrams, etl tools, flat files, integration patterns, job schedulers, normalization, powershell, problemsolving, python, scripting languages, snowflake, sql, star schema, talend, technical documentation, troubleshooting, web services, xmlxslt"
Senior Data Engineer,Plante Moran,"Colorado, United States",https://www.linkedin.com/jobs/view/senior-data-engineer-at-plante-moran-3707160023,2023-12-17,Rockhampton, Australia,Associate,Hybrid,"Location: United States, Michigan
Location: United States, Colorado
Make your mark.
Plante Moran’s technology services team has been awarded
Insider Pro
and
Computerworld’s “100 Best Places to Work in IT”
for five consecutive years. We are also previous recipients of the
InformationWeek IT Excellence
award and the
CIO 100
award. If you’re seeking professional growth, like being innovative and challenged, and have a desire to work on impactful business technology solutions, we want to hear from you!
What does diversity, equity, and inclusion mean to Plante Moran? It means that all staff members have equitable and fair opportunities to succeed, in an inclusive environment, with their individual, unique identities.
Plante Moran enjoys a “Workplace for Your Day” model which, simply put, means no one knows the best place for you to physically work better than you. Our staff choose their work location based on their individual day’s needs, and gives them the flexibility they need when they need it. If staff need to go into the office to collaborate, the space is there, and if they have a day where they need to focus alone, then staying home works just fine, too.
We are looking for a Data Engineer to be a senior member of the Data Services team which includes all data warehouse, data engineering and visualizations, database operations, and data science capabilities. This role will be a team member on our data engineering team supporting all data integrations activities at Plante Moran.
Your role.
Your work will include, but not be limited to:
Develop new innovative Azure data technologies to design and develop innovative POC’s and Project to address complex business issues.
Develop data solutions, POC to address current and future information system needs, and create process improvements and controls for defined solutions
Works closely with customers and colleagues to identify opportunities to utilize information systems to improve business processes, promote the strategic use of information while enabling seamless access to information.
Interacts with the staff to produce data mapping and requirements, deliver high quality solutions utilizing Microsoft Data Integration Stack
Provides prototyping solutions, prepares test scripts, and conducts tests and for data replication, extraction, loading, cleansing, and data modeling.
Possesses working knowledge of Relational Database Management Systems (DBMS) and data warehouse front-end tools.
Be proficient in creating and maintain SQL data structures and store procedures.
Works closely with the technical and business team lead to drive solution options analysis, development and implementation of BI solutions.
Contributing team member to the design and support of data architecture, database design and integration, transformations, and load processes.
The Qualifications.
Associate degree required
Bachelor’s Degree preferred in computer science or related field preferred
Extensive Azure Data Technology design and implementation experience: ADF, Azure SQL, Azure Data Bricks, Azure Analysis Services, Data Lakes and Power BI
Informatica Experience (IICS) – Using Data integrator, Application Integration, Data Quality, Data Lineage
Experience creating data flows, executing stored procedures within SSIS
Experience setting up and troubleshooting SQL Agent Jobs
Azure Dev Ops and GitHub development management experience
SSMS and T-SQL Experience – Strong T-SQL Skills
3-5 years of experience using the Microsoft Azure Data integration tools including SSIS/IR, ADF, and ADO
Familiarity with database-centric applications
Proven experience with data warehousing and data modeling, specifically building/understanding Microsoft data storage structures
Proven ability to build out, enhance, and maintain the company data environment, including modifications and maintenance of various ETLs, transactional data sets, and company data assets.
Manage data access, data quality, and data delivery tasks within the framework of specific client BI and IT projects.
Ability to validate & ensure production level functionality/style is thoroughly tested, and the code reflects the current standards.
Preferred Qualifications
Problem Solver – Interested in finding solutions to/supporting existing solutions, as well as, continuing to advance through new development
Analytical approach to problem-solving; ability to use technology to solve business problems
Passionate about learning new technologies
Strong verbal and written communication skills; strong time-management and organizational skills
Our difference.
We are a nationally recognized public accounting, consulting, and wealth management firm, consistently ranked as one of FORTUNE magazine’s “100 Best Companies to Work For.” At Plante Moran, we live by the Golden Rule, fostering a relatively “jerk-free” culture with the lowest staff turnover rate in the industry. Our supportive network of well-rounded professionals is excited to catapult your growth and help pave your pathway to professional excellence.
Apply now. Make your mark.
Th is is an exempt p osition, so you may have to work hours that exceed the standard 40-hour work week.
Plante Moran is committed to a diverse workplace. We strive to create a culture where each person feels accepted and valued. We believe that each person’s ultimate potential begins with first acknowledging their inherent dignity. When we can recognize — and celebrate — our many human differences, we’re able to create a workplace where all staff feel a sense of belonging and an opportunity to succeed. This allows us to attract and retain the best talent, serve clients through diverse thinking, and better represent and support the various communities in which we live and work. Plante Moran is an Equal Opportunity Employer.
Plante Moran maintains a drug-free workplace.
Interested applicants must submit their resume for consideration using our applicant tracking system. Due to the high volume of applications received, only candidates selected for interviews will be contacted. Candidates must be legally authorized to work in the United States without sponsorship, with the exception of candidates that are bilingual in Japanese and English. Unsolicited resumes from search firms or employment agencies, or similar, will not be paid a fee and become the property of Plante Moran.
The specific statements above are not intended to be all-inclusive.
Show more
Show less","Azure Data Technologies, ADF, Azure SQL, Azure Data Bricks, Azure Analysis Services, Data Lakes, Power BI, Informatica Experience (IICS), Data integrator, Application Integration, Data Quality, Data Lineage, Data flows, SSIS, SQL Agent Jobs, Azure Dev Ops, GitHub, SSMS, TSQL, ETLs, BI, ADO","azure data technologies, adf, azure sql, azure data bricks, azure analysis services, data lakes, power bi, informatica experience iics, data integrator, application integration, data quality, data lineage, data flows, ssis, sql agent jobs, azure dev ops, github, ssms, tsql, etls, bi, ado","adf, ado, application integration, azure analysis services, azure data bricks, azure data technologies, azure dev ops, azure sql, bi, data flows, data integrator, data lakes, data lineage, data quality, etls, github, informatica experience iics, powerbi, sql agent jobs, ssis, ssms, tsql"
Data Integration Engineer (ODI),The Wonderful Company,"Los Angeles, CA",https://www.linkedin.com/jobs/view/data-integration-engineer-odi-at-the-wonderful-company-3782544493,2023-12-17,Rockhampton, Australia,Associate,Hybrid,"Company Description
From farm to table, The Wonderful Company is a privately held $5 billion company committed to offering high-quality, healthy, and iconic brands such as Wonderful Pistachios, Wonderful Halos, FIJI Water, and POM Wonderful. We’re looking to make the world a healthier place through an uncompromising commitment to the well-being of our employees and their families.
We are seeking a talented and experienced
Data Integration Engineer
to join our team and play a crucial role in designing, building, and maintaining our ETL (Extract, Transform, Load) processes. As a Data Integration Engineer, you will work closely with data analysts, data scientists, and other stakeholders to ensure the efficient extraction, transformation, and loading of data from various sources into our data warehouse. Your expertise in data integration, data quality, and performance optimization will be essential in supporting our organization's data-driven decision-making efforts.
This position will be based at our West Los Angeles, CA office and working a hybrid schedule with 3 days per week in the office.
Job Description
Design, develop, and maintain data warehouses and data marts to store and organize data from the Oracle ERP and other sources.
Design, model and document the logical and conceptual relationship of data and implement database changes for Business Intelligence (BI) applications in ODI and Oracle Fusion Analytics Warehouse (FAW).
Understand the data related challenges, nuances, and requirements to identify and recommend the optimal technical approach.
Understand project timelines for IT Projects and lead development and training efforts for IT Projects using Agile methodology.
Perform proof of concepts for innovation and to continuously improve and enhance the capabilities of the data analytics platform.
Develop and maintain strong effective working relationships with team members and internal customers.
Train and educate internal team members as well as partner consultants about best practices in data engineering and governance.
Qualifications
Datawarehouse Design Skills: Focus area includes exceptional skills with data design and management.
Strong Experience with data warehousing ETL process design/development/support (ODI, Informatica).
Experience working with ERP systems data such as Oracle EBS/Fusion (Purchasing, Order Management, Manufacturing and/or Financials modules).
Extensive experience in SQL/PLSQL, Logical SQL Queries and Performance Tuning.
Proven experience working with REST APIs and bringing data from REST APIs from multiple sources.
Excellent understanding of star/snow-flake schema, SCDs and de-normalized operations.
Experience in leading requirements gathering, gap analysis and create user requirements documentation.
Solid exposure to varied cloud databases such as Snowflake, Oracle Autonomous DW (ADW), etc.
Oracle Fusion Analytics Warehouse (FAW) experience is a plus.
Pay Range: $130,000 - $145,000 and may include a discretionary bonus. Final compensation will be dependent upon skills & experience.
Additional Information
Competitive benefits package including Medical (including 24/7 online access to a physician), Vision, Dental and 401k with match eligibility
Opportunities for development and internal mobility
Manager and leadership training, biweekly L&OD webinars, and eLearning offerings
Companywide problem solving and continuous improvement training
Wonderful Giving (wonderfulgiving.com) - allowing you to donate company money to a cause of your choice
Company prioritizes wellness through its Wonderful NOW (Nourish Our Wellness) initiative, which provides a robust suite of wellness offerings such as access to mental health resources and life coaching, employee community groups, cash rewards for healthy habits, and on-demand fitness videos
Headquartered in Los Angeles, The Wonderful Company is a privately held $5 billion global company dedicated to harvesting health and happiness around the world through its iconic consumer brands. The company’s 10,000 employees worldwide are committed to bringing consumers everywhere the freshest, most wholesome pistachios, citrus and pomegranates; bottling the finest water and wines; and creating colorful bouquets that are sure to touch the heart. This commitment is reflected in the company’s market share: Wonderful Pistachios is America’s No. 1 tree nut and America’s fastest-growing snack; Wonderful Halos is the No. 1 mandarin orange in America; POM Wonderful is the No. 1 100% pomegranate brand in America; FIJI Water is America’s No. 1 premium imported bottled water brand; JUSTIN Wine has the No. 1 Cabernet Sauvignon in California; and Teleflora is the world’s leading floral delivery service.
The Wonderful Company’s connection to consumers has health at its heart and giving back in its DNA. The company has a long-standing commitment to corporate social responsibility, including more than $2.5BN invested in education, health and wellness, community development, and sustainability initiatives across California’s Central Valley, Fiji, and the world.
To learn more about The Wonderful Company, its products, and its core values, please visit wonderful.com, or follow The Wonderful Company on LinkedIn, Facebook, Instagram, and Twitter. To learn more about the company’s corporate social responsibility impact, visit csr.wonderful.com.
The Wonderful Company is an Equality Opportunity Employer that provides opportunities for advancement. We are committed to creating a diverse workforce that embodies a deep culture of acceptance, equity, and inclusion. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, age, protected veteran status, or other protected categories.
EEO is the law - click here for more information
Show more
Show less","Oracle ERP, ODI, Informatica, Oracle Fusion Analytics Warehouse (FAW), SQL, PLSQL, Logical SQL Queries, Performance Tuning, REST APIs, Star/Snowflake Schema, SCDs, DeNormalized Operations, Cloud Databases, Snowflake, Oracle Autonomous DW (ADW)","oracle erp, odi, informatica, oracle fusion analytics warehouse faw, sql, plsql, logical sql queries, performance tuning, rest apis, starsnowflake schema, scds, denormalized operations, cloud databases, snowflake, oracle autonomous dw adw","cloud databases, denormalized operations, informatica, logical sql queries, odi, oracle autonomous dw adw, oracle erp, oracle fusion analytics warehouse faw, performance tuning, plsql, rest apis, scds, snowflake, sql, starsnowflake schema"
Software Engineer – Quantitative Data,"Susquehanna International Group, LLP (SIG)",Greater Philadelphia,https://www.linkedin.com/jobs/view/software-engineer-%E2%80%93-quantitative-data-at-susquehanna-international-group-llp-sig-3762277228,2023-12-17,Rockhampton, Australia,Associate,Hybrid,"Overview:
SIG is a proprietary trading firm that’s dependent on a strong and creative technology team. We sit next to our stakeholders, allowing for a quick feedback cycle and a firsthand look at how our user interacts with our product. We value decision making and asking questions, and we pride ourselves on the fact that working at SIG is more than just a job.
In this role, you will
Research, design, develop and improve systems used in backtesting and data analysis for automated option trading strategies
Work with quantitative analysts, trading technologists and traders to build both ad-hoc and systematic tools and data which help in optimizing trading behavior and results
Work with infrastructure developers to scale computations for high throughput
What we’re looking for
Bachelor’s degree in Computer Science, Engineering, Mathematics or related discipline or its foreign equivalent; advanced degree is preferred
5 years of professional experience developing software and data systems, or advanced graduate work
Fluency in at least one low level language (C++, C#, Java) and one higher level language (Python, Lisp, Matlab, Julia)
Strong understanding of data structures and algorithms, and operating systems concepts
Facility working with large object-oriented code bases, using multiple languages and tools
Experience building large scale data transformation pipelines
Enthusiasm for working with data, especially large sets of data, automating operations. Good aesthetics for cleanliness and correctness in data
Experience in Linux shell environment and with text processing tool chains, Windows development platform experience a plus
Experience with some higher level data analysis/manipulations languages/tools (R, Pandas, Julia)
Excellent analytical and communication skills
An interest in large scale computing problems and solutions, and understanding of performance trade-offs is plus
Experience in a data or compute intensive role required
SIG does not accept unsolicited resumes from recruiters or search firms. Any resume or referral submitted in the absence of a signed agreement will become the property of SIG and no fee will be paid.
Show more
Show less","Backtesting, Data analysis, Automated option trading, Quantitative analysis, Trading technology, C++, C#, Java, Python, Lisp, Matlab, Julia, Linux shell, R, Pandas","backtesting, data analysis, automated option trading, quantitative analysis, trading technology, c, c, java, python, lisp, matlab, julia, linux shell, r, pandas","automated option trading, backtesting, c, dataanalytics, java, julia, linux shell, lisp, matlab, pandas, python, quantitative analysis, r, trading technology"
Data Analytics Engineer,QualDerm Partners,"Brentwood, TN",https://www.linkedin.com/jobs/view/data-analytics-engineer-at-qualderm-partners-3778221831,2023-12-17,Rockhampton, Australia,Associate,Hybrid,"With 150+ locations in 17 states, QualDerm Partners is the largest multi-state female-founded and owned dermatology network in the U.S. At QualDerm Partners, we are united in our purpose: to educate, protect, and care for your skin. We are committed to bringing you the very best in comprehensive skin care so that you can achieve a confident, healthier, and more beautiful you! Our mission is to educate communities and healthcare professionals about the importance of skin health; including skin cancer prevention, risk factors and how to achieve healthy skin through annual skin exams and dermatologic care. We strive to provide the latest proven, proactive and comprehensive dermatologic care to populations in the expanding geographies we serve by making high-quality skin care timelier and more accessible all while creating a rewarding work environment for our providers and employees.
Purpose:
Data Analytics Engineer is responsible for both maintaining and evolving the data platform (pipelines, models, etc.) as well as dashboard and report creation and data analysis. This position is instrumental in the development and tracking of KPIs and business metrics by leveraging internal and external data sources and utilizing visualization and reporting tools. This position is responsible for both the technical aspects of the data platform and data analysis and visualization.
ESSENTIAL DUTIES AND RESPONSIBILITIES
:
Help build analytics solutions focused on answering critical business questions to help inform business decisions and drive operational efficiency.
Collaborate with cross-functional teams to gather and refine the data requirements, and design/develop data deliverables.
· Design, build and optimize the platform, ETL data pipelines for efficient data extraction, transformation, and loading from various sources while ensuring data quality and integrity.
· Develop, and maintain reports, dashboards and visualizations to eﬀectively communicate data driven insights to users for a variety of audiences e.g. visualization solutions of data into reports and graphics to illustrate facts, trends, and insights.
· Develop and foster relationships with internal/external partners to manage work-streams and cross-functional deliverables.
· Support ad hoc data analysis projects and tasks as needed.
· Identify areas to increase eﬃciency and process automation.
· Partner with senior leaders to ensure data governance. Develops stakeholder trust in data accuracy and cleanliness.
· Learn the business quickly.
OTHER FUNCTIONS:
• Maintains regular and predictable attendance.
• Performs other essential duties as assigned.
• Maintain professional licensure/certifications as applicable.
• Ability to work remotely as well as be on-site at the Corporate office in Brentwood, TN
SKILLS AND QUALIFICATIONS:
• Skilled in identifying and resolving problems.
• Ability to interpret, adapt and apply guidelines and procedures.
• Ability to multitask with interruptions.
• Ability to use good reasoning and judgment and react calmly in emergency situations.
• Ability to establish and maintain effective working relationships with all levels of the organization
• Ability to read, write and communicate effectively orally and in writing.
• Proficiency in the operation of a computer keyboard and ability to work effectively with Microsoft Office Products.
EDUCATION AND/OR EXPERIENCE REQUIREMENTS:
Bachelor Degree in related field
2-4 years of previous experience as a data analyst, data engineer, or business analyst.
· Azure, Synapse, PowerBI (Microsoft) technology and infrastructure experience.
· Experience in data management disciplines including data integration, modeling, optimization and data quality, and/or other areas directly relevant to data engineering responsibilities and tasks.
· Experience working in cross-functional teams and collaborating with business stakeholders in support of data management and analytics initiatives.
· Experience in healthcare a plus.
Benefits of Joining QualDerm Partners
Competitive Pay
Medical, dental, and vision
401(k) - The company match is 100% of the first 3%; and 50% of the next 2%; immediately vested
Paid Time Off - accrual starts upon hire, plus 6 Paid Holidays and 2 floating days
Company paid life insurance and additional coverage available
Short-term and long-term disability, accident and critical illness, and identity theft protection plans
Employee Assistance Program (EAP)
Employee Discounts
Employee Referral Bonus Program
QualDerm Partners, LLC is proud to be an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees.
Show more
Show less","Data Analytics, Data Platform Maintenance, Data Pipeline Development, Data Quality Management, Data Visualization, Reporting, ETL, Data Extraction, Data Transformation, Data Loading, Dashboard Creation, Data Analysis, Problem Solving, Reasoning, Judgment, Communication, Microsoft Office, Azure, Synapse, PowerBI, Data Management, Data Integration, Data Modeling, Data Optimization, Data Quality, CrossFunctional Collaboration, Business Stakeholder Collaboration, Data Engineering, Healthcare Experience","data analytics, data platform maintenance, data pipeline development, data quality management, data visualization, reporting, etl, data extraction, data transformation, data loading, dashboard creation, data analysis, problem solving, reasoning, judgment, communication, microsoft office, azure, synapse, powerbi, data management, data integration, data modeling, data optimization, data quality, crossfunctional collaboration, business stakeholder collaboration, data engineering, healthcare experience","azure, business stakeholder collaboration, communication, crossfunctional collaboration, dashboard creation, data engineering, data extraction, data integration, data loading, data management, data optimization, data pipeline development, data platform maintenance, data quality, data quality management, data transformation, dataanalytics, datamodeling, etl, healthcare experience, judgment, microsoft office, powerbi, problem solving, reasoning, reporting, synapse, visualization"
Data Engineer II,UW Credit Union,"Madison, WI",https://www.linkedin.com/jobs/view/data-engineer-ii-at-uw-credit-union-3774804061,2023-12-17,Rockhampton, Australia,Associate,Hybrid,"UW Credit Union is adding a talented
Data Engineer II
to our dynamic and developing Data Analytics team. Are you a skilled and motivated Data Engineer with expert SQL skills and looking to make a significant impact?
As a Data Engineer II, you'll play a pivotal role in building and operationalizing data pipelines using DBT and Snowflake technologies within our cloud-native data platform. If you're passionate about working with cutting-edge technologies, collaborating with cross-functional teams, and driving data-driven initiatives from the ground up, this role could be your perfect opportunity.
Join UW Credit Union and be at the forefront of our developing Data Analytics team, driving the creation and operationalization of data pipelines necessary for our data and analytics initiatives. You'll work closely with our business units, data analysts and data scientists to design, develop, and optimize data pipelines, ensuring high-quality, reliable data for various analytics use cases. This role requires a mix of technical expertise, creative problem-solving, and collaboration.
Why Work for UW Credit Union?
Join one of Wisconsin’s premier financial institutions, a 2023 Top Workplaces USA and multi-year recipient of Madison Magazine’s Best Places to Work, Wisconsin State Journal’s Top Workplaces, and Milwaukee Journal Sentinel’s Top Workplaces to receive:
21.5+ days of annual paid time off
2 weeks paid caregiver leave
2.5 weeks paid new child parental leave
2 days paid volunteer time
11 paid holidays (includes your birthday!)
401k company match of up to 5%, plus approximately 4% discretionary match
Variable bonus reward
Competitive Medical, Dental and Vision plans, including domestic partner eligibility
Free bus pass and bublr/ BCycle membership
Employee Assistance Program
Hybrid work environment
Salary: $105,000 - $125,000; mid-point $118,000
And more!
Responsibilities
What You’ll Do
Engage in scrum ceremonies including Daily Standups, Sprint Planning, Retrospectives, and Demos.
Collaborate daily with your development team, sharing progress and offering support to one another.
Design, develop, and maintain highly scalable and extensible data pipelines from internal and external sources.
Test and validate models developed by fellow team members to ensure solutions have high data quality and follow development standards.
Collaborate with cross-functional teams to design, develop, and deploy data-driven applications and products.
Optimize data queries, models, and storage formats to support common usage patterns.
Work with the data science team to enhance machine learning algorithms and platforms.
Define and automate data quality checks and develop data solutions, including data visualization tools.
Contribute to the definition and management of data governance standards and policies.
Participate in prototyping emerging technologies for data ingestion, transformation, and distributed file systems within a cloud-native big data platform.
Stay up-to-date with industry trends and best practices in data engineering and cloud computing.
Support nightly and weekend on-call support activities.
Qualifications
What You’ll Need to Succeed
These skills and experiences are essential to your success:
Bachelor’s degree in Information Systems, Computer Science, Data Science, Business, or related field; relevant work experience may be considered in lieu of educational requirements
4-5 years of relevant work experience in data engineering space in a cloud data platform
Demonstrated experience providing customer-drive solutions, support and service
In-depth knowledge of SQL and demonstrated ability to write SQL optimized for MPPS
Ability to employ design patterns and generalize code to address common use cases
Capable of authoring robust, high quality, reusable code and contributing to the division’s inventory of libraries
Expertise and demonstrated experience developing distributed data processing solutions including batch and streaming
Knowledge of object-oriented programming languages and scripting languages
Ability to work 3 days a week on-prem (Monday, Wednesday & Thursday)
These attributes and knowledge are preferred but not required:
Experience working with the following tools:
Snowflake
DBT
Azure (Data Factory, Storage Accounts)
Python
Azure DevOps Team Boards and Repos
Experience working in the financial services industry
UW Credit Union is proud to be an Equal Opportunity and Affirmative Action Employer of Minorities, Protected Veterans, Women, and individuals with Disabilities. We strive to cultivate an inclusive workplace that is home to a diverse workforce where employees at all levels model knowledge of, belief in and a commitment to cultural competence.
Show more
Show less","SQL, DBT, Snowflake, Cloudnative data platform, Data pipelines, Data analytics, Data engineering, Data science, Machine learning, Data governance, Data visualization, Azure Data Factory, Azure Storage Accounts, Python, Azure DevOps Team Boards, Azure DevOps Repos, Objectoriented programming languages, Scripting languages, Scrum ceremonies, Daily Standups, Sprint Planning, Retrospectives, Demos, Crossfunctional teams, Datadriven applications, Data products, Data quality checks, Data solutions, Industry trends, Best practices","sql, dbt, snowflake, cloudnative data platform, data pipelines, data analytics, data engineering, data science, machine learning, data governance, data visualization, azure data factory, azure storage accounts, python, azure devops team boards, azure devops repos, objectoriented programming languages, scripting languages, scrum ceremonies, daily standups, sprint planning, retrospectives, demos, crossfunctional teams, datadriven applications, data products, data quality checks, data solutions, industry trends, best practices","azure data factory, azure devops repos, azure devops team boards, azure storage accounts, best practices, cloudnative data platform, crossfunctional teams, daily standups, data engineering, data governance, data products, data quality checks, data science, data solutions, dataanalytics, datadriven applications, datapipeline, dbt, demos, industry trends, machine learning, objectoriented programming languages, python, retrospectives, scripting languages, scrum ceremonies, snowflake, sprint planning, sql, visualization"
Data Dashboard Analyst,Cypress HCM,"Chicago, IL",https://www.linkedin.com/jobs/view/data-dashboard-analyst-at-cypress-hcm-3764332860,2023-12-17,Rockhampton, Australia,Associate,Hybrid,"Data Dashboard Analyst
The Data Dashboard Analyst is responsible for designing, developing, and maintaining data dashboards and reports that provide insights into the company's operations. The ideal candidate will have 2+ years of experience in data analysis and visualization, as well as strong communication and presentation skills.
Responsibilities:
Work with stakeholders to gather requirements and define data dashboard and report specifications
Design and develop data dashboards and reports using data visualization tools
Publish and maintain data dashboards and reports. Incorporate existing dashboards.
Troubleshoot data issues and ensure the accuracy of data dashboards and reports
Collaborate with other data analysts and developers to improve data dashboard and report capabilities
Qualifications
:
Bachelor's degree in a related field (e.g., computer science, statistics, business)
2+ years of experience in data analysis and visualization
Strong proficiency in data visualization tools (e.g., Tableau, Power BI)
Proficiency in pulling data from existing business systems like Salesforce, AWS S3 and Vendor systems
Excellent communication and presentation skills
Ability to work independently and as part of a team
Strong analytical and problem-solving skills
Compensation: $37.00 - $50.00 per hour
Show more
Show less","Data Analysis, Data Visualization, Business Intelligence, Tableau, Power BI, Salesforce, AWS S3, SQL, Communication, Presentation","data analysis, data visualization, business intelligence, tableau, power bi, salesforce, aws s3, sql, communication, presentation","aws s3, business intelligence, communication, dataanalytics, powerbi, presentation, salesforce, sql, tableau, visualization"
Data Center Engineer,Cloudflare,"Phoenix, AZ",https://www.linkedin.com/jobs/view/data-center-engineer-at-cloudflare-3732382790,2023-12-17,Chandler,United States,Associate,Onsite,"About Us
At Cloudflare, we have our eyes set on an ambitious goal: to help build a better Internet. Today the company runs one of the world’s largest networks that powers approximately 25 million Internet properties, for customers ranging from individual bloggers to SMBs to Fortune 500 companies. Cloudflare protects and accelerates any Internet application online without adding hardware, installing software, or changing a line of code. Internet properties powered by Cloudflare all have web traffic routed through its intelligent global network, which gets smarter with every request. As a result, they see significant improvement in performance and a decrease in spam and other attacks. Cloudflare was named to Entrepreneur Magazine’s Top Company Cultures list and ranked among the World’s Most Innovative Companies by Fast Company.
We realize people do not fit into neat boxes. We are looking for curious and empathetic individuals who are committed to developing themselves and learning new skills, and we are ready to help you do that. We cannot complete our mission without building a diverse and inclusive team. We hire the best people based on an evaluation of their potential and support them throughout their time at Cloudflare. Come join us!
Data Center Operations Engineer
About the department
In this role, you will be focused on maintaining the Clou dflare global network. You 'll work closely with Cloudflare’s SRE (Site Reliability Engineering) team, Network Engineering team, Network Deployment Engineering team and with various vendors and partners (including hardware vendors, datacenter and network providers, and ISPs) to maintain and improve our global infrastructure. You will further be responsible for the development and implementation of consistent processes and visibility measurements for consistent and effective management of our infrastructure. This is a highly visible position that requires deep technical understanding of datacenter infrastructure, networking (physical), and basic experience with data analysis and project management.
To be successful in this position, you should have excellent technical skills, communication skills, and be able to navigate a range of challenges and constraints (e.g. schedule adherence, time zones, and cultures). You will have the opportunity to (literally) build a faster, safer Internet for our millions of users and the billions of web surfers that visit their sites each month.
Who You Are
You will thrive in a hypergrowth engineering environment and be self driven with a keen attention to detail. You will come with a deep technical understanding of Data Center colocation environments, network architecture and server technologies. You will be used to working through partners to support infrastructure delivery to a number of remote locations. You will have had experience managing operational environments, and used to developing new approaches to improve delivery efficiency or operational stability.
What You'll Do
Collaborating with internal teams (Infrastructure, Network Engineering and SRE). Create documentation and manage remote contractors to complete datacenter tasks, working with hardware manufacturers, datacenter and network providers, logistics partners and other service providers in support of our 300+ datacenter locations
Maintain Data Center environment operational availability
Creating and maintaining documentation, plans, SOP’s, MOP’s etc.
Support and configure network infrastructure where required
Providing feedback to internal teams to support internal tools and external vendor partnerships
Required Experience
Minimum of 5 yrs of Linux systems administration
Experience with Juniper, Cisco and DWDM network equipment
Experience managing and instructing remote contractors
Familiarity with work required to stand up infrastructure in remote colocation facilities
Experience running and improving operational processes, including automation tooling, in a rapidly changing environment
Familiarity with day-to-day tasks and projects common to Data Center Operations (deployment, migration, decommissioning etc.)
Comfortable handling basic program management responsibilities (prioritization, planning, scheduling, status reporting) such as JIRA
Incident management
Other Responsibilities May Include
Aggressively seek opportunities to introduce cutting-edge technology and automation solutions that are effective, efficient and scalable in order to improve our ability to deploy and maintain our global infrastructure
Assist with the definition, documentation and implementation of consistent processes across all region
Limited travel
Examples Of Desirable Skills, Knowledge And Experience
Bachelor’s degree; technical background in engineering, computer science, or MIS
Direct experience executing on complex data center/infrastructure projects
Previous experience installing / maintaining data center (and other IT) infrastructure and DCIM tools
Experience running and improving operational processes in a rapidly changing environment
Strong verbal and written communication skills, problem-solving skills, attention to detail, and interpersonal skills
Must be proactive with proven ability to learn fast and execute on multiple tasks simultaneously
Ability to manage MS excel and Google spreadsheets
Comfortable handling basic program management responsibilities (prioritization, planning, scheduling, status reporting) such as JIRA
Must be a team player
Bonus Points
Multi-lingual; experience working with infrastructure in multiple countries
Comfortable with remote “lights-out” and out-of-band access to data center resources
Linux certifications (RHCSA etc.)
Network certifications (CCNA, JNCIA or higher)
Compensation
Compensation may be adjusted depending on work location.
For Colorado-based hires: Estimated annual salary of $ 111,000 - $ 135,000 .
For New York City, Washington, and California (excluding Bay Area) based hires: Estimated annual salary of $ 135,000 - $ 165,000
For Bay Area-based hires: Estimated annual salary of $ 142,000 - $ 174,000 .
Equity
This role is eligible to participate in Cloudflare’s equity plan.
Benefits
Cloudflare offers a complete package of benefits and programs to support you and your family. Our benefits programs can help you pay health care expenses, support caregiving, build capital for the future and make life a little easier and fun! The below is a description of our benefits for employees in the United States, and benefits may vary for employees based outside the U.S.
Health & Welfare Benefits
Medical/Rx Insurance
Dental Insurance
Vision Insurance
Flexible Spending Accounts
Commuter Spending Accounts
Fertility & Family Forming Benefits
On-demand mental health support and Employee Assistance Program
Global Travel Medical Insurance
Financial Benefits
Short and Long Term Disability Insurance
Life & Accident Insurance
401(k) Retirement Savings Plan
Employee Stock Participation Plan
Time Off
Flexible paid time off covering vacation and sick leave
Leave programs, including parental, pregnancy health, medical, and bereavement leave
What Makes Cloudflare Special?
We’re not just a highly ambitious, large-scale technology company. We’re a highly ambitious, large-scale technology company with a soul. Fundamental to our mission to help build a better Internet is protecting the free and open Internet.
Project Galileo
: We equip politically and artistically important organizations and journalists with powerful tools to defend themselves against attacks that would otherwise censor their work, technology already used by Cloudflare’s enterprise customers--at no cost.
Athenian Project
: We created Athenian Project to ensure that state and local governments have the highest level of protection and reliability for free, so that their constituents have access to election information and voter registration.
Path Forward Partnership
: Since 2016, we have partnered with Path Forward, a nonprofit organization, to create 16-week positions for mid-career professionals who want to get back to the workplace after taking time off to care for a child, parent, or loved one.
1.1.1.1
: We released 1.1.1.1 to help fix the foundation of the Internet by building a faster, more secure and privacy-centric public DNS resolver. This is available publicly for everyone to use - it is the first consumer-focused service Cloudflare has ever released. Here’s the deal - we don’t store client IP addresses never, ever. We will continue to abide by our privacy commitment and ensure that no user data is sold to advertisers or used to target consumers.
Sound like something you’d like to be a part of? We’d love to hear from you!
This position may require access to information protected under U.S. export control laws, including the U.S. Export Administration Regulations. Please note that any offer of employment may be conditioned on your authorization to receive software or technology controlled under these U.S. export laws without sponsorship for an export license.
Cloudflare is proud to be an equal opportunity employer. We are committed to providing equal employment opportunity for all people and place great value in both diversity and inclusiveness. All qualified applicants will be considered for employment without regard to their, or any other person's, perceived or actual race, color, religion, sex, gender, gender identity, gender expression, sexual orientation, national origin, ancestry, citizenship, age, physical or mental disability, medical condition, family care status, or any other basis protected by law. We are an AA/Veterans/Disabled Employer.
Cloudflare provides reasonable accommodations to qualified individuals with disabilities. Please tell us if you require a reasonable accommodation to apply for a job. Examples of reasonable accommodations include, but are not limited to, changing the application process, providing documents in an alternate format, using a sign language interpreter, or using specialized equipment. If you require a reasonable accommodation to apply for a job, please contact us via e-mail at hr@cloudflare.com or via mail at 101 Townsend St. San Francisco, CA 94107.
Show more
Show less","Linux systems administration, Juniper Cisco and DWDM network equipment, Remote contractors, Colocation facilities, Automation tooling, Microsoft Excel, Google spreadsheets, JIRA, Team player, Bachelor's degree in engineering computer science or MIS, Data center/infrastructure projects, DCIM tools, RHCSA certification, CCNA certification, JNCIA certification","linux systems administration, juniper cisco and dwdm network equipment, remote contractors, colocation facilities, automation tooling, microsoft excel, google spreadsheets, jira, team player, bachelors degree in engineering computer science or mis, data centerinfrastructure projects, dcim tools, rhcsa certification, ccna certification, jncia certification","automation tooling, bachelors degree in engineering computer science or mis, ccna certification, colocation facilities, data centerinfrastructure projects, dcim tools, google spreadsheets, jira, jncia certification, juniper cisco and dwdm network equipment, linux systems administration, microsoft excel, remote contractors, rhcsa certification, team player"
Database Engineer,U-Haul,"Phoenix, AZ",https://www.linkedin.com/jobs/view/database-engineer-at-u-haul-3773737897,2023-12-17,Chandler,United States,Associate,Onsite,"Job Description
Job Description
Join our dynamic team as a Database Engineer and be at the forefront of developing and optimizing cutting-edge data solutions. Harness the power of SQL and other data management systems to shape and manage our robust customer and user data, unlock valuable insights, and drive impactful decision-making. Take your expertise to new heights in a collaborative environment, where your technical prowess and problem-solving skills will impact our organization's success.
Responsibilities
Fully architect and build databases based on business requirements, collaborating with programming staff for design changes and innovation.
Manage and interface with multiple databases across distributed environments including conducting performance tuning and optimization.
Monitor and maintain database systems, troubleshooting issues that may arise.
Collaborate with development teams to design efficient database structures.
Ensure data integrity and accuracy through regular maintenance tasks.
Stay up to date with industry best practices and emerging trends in database management systems and approaches.
Maintain data integrity and security and manage roles and permissions of database users.
Use profiling tools to identify and resolve performance issues, deadlocks, and contention.
Develop strategies and implement solutions for back-ups, restoration, and replication.
Proactively perform housekeeping, archiving, and shrinking of databases.
Manage production, QA, and development database environments.
Create detailed documentation, including diagrams of database infrastructure where needed.
Understand relevant business processes, data flow, criticality, and dependencies.
Coordinate and communicate effectively with internal teams and clients.
Qualifications
Bachelor's or master's degree in computer science or equivalent education and experience.
5+ years of experience in administering relational and/or non-relational databases, including SQL server databases.
Extensive experience in writing Transact-SQL, stored procedures, and query tuning.
Experience in extract-transform-load development and data integration.
Familiarity with clustering and high availability approaches, such as mirroring, log shipping, fail-over clustering, and replication technologies.
Ability to document procedures and create diagrams illustrating processes and database structures.
Proactive mindset in upgrading systems for improved efficiency and effectiveness.
Preferred Qualifications
Experience working with SQL in multiple data centers.
Experience running SQL in Azure.
Familiarity with Grafana.
Preferred certifications: SQL MCSA.
Experience with non-relational database management systems including Elasticsearch and Redis.
Some of the programs U-Haul has available:
Full Medical coverage
Prescription plans
Dental & Vision Plans
New indoor fitness gym
Gym Reimbursement Program
Registered Dietitian Program
Weight Watchers
Onsite medical clinic for you and your family
Career stability
Opportunities for advancement
Valuable on-the-job training
Tuition reimbursement program
Free online courses for personal and professional development at U-Haul University®
Business and travel insurance
You Matter Employee Assistance Program
Paid holidays, vacation, and sick days
Employee Stock Ownership Plan (ESOP)
401(k) Savings Plan
Life insurance
Critical Illness/Group Accident
24-hour physician available for kids
Subsidized gym/ membership
MetLaw Legal program
MetLife auto and home insurance
Mindset App Program
Discounts on cell phone plans, hotels, and more
LifeLock identity Theft
Savvy consumer wellness programs- from health care tips to financial wellness
Dave Ramsey’s SmartDollar Program
U-Haul federal credit union
Wellness Program
Show more
Show less","SQL, Database management, Data analysis, Data optimization, Data structures, Database architecture, TransactSQL, Stored procedures, Query tuning, ETL development, Data integration, Clustering, High availability, Mirroring, Log shipping, Failover clustering, Replication, Documentation, System upgrading, SQL MCSA, Elasticsearch, Redis, Azure, Grafana","sql, database management, data analysis, data optimization, data structures, database architecture, transactsql, stored procedures, query tuning, etl development, data integration, clustering, high availability, mirroring, log shipping, failover clustering, replication, documentation, system upgrading, sql mcsa, elasticsearch, redis, azure, grafana","azure, clustering, data integration, data optimization, data structures, dataanalytics, database architecture, database management, documentation, elasticsearch, etl development, failover clustering, grafana, high availability, log shipping, mirroring, query tuning, redis, replication, sql, sql mcsa, stored procedures, system upgrading, transactsql"
Jr Data Analyst,DriveTime,"Tempe, AZ",https://www.linkedin.com/jobs/view/jr-data-analyst-at-drivetime-3771243458,2023-12-17,Chandler,United States,Associate,Onsite,"What’s Under The Hood
DriveTime Family of Brands is the largest privately owned used car sales finance & servicing company in the nation. Headquartered in Tempe, Arizona and Dallas, Texas, we create opportunities and improve the lives of our customers and our employees by placing a focus on putting the right customer, in the right vehicle, on the right terms and on their path to ownership.
The DriveTime Family of Brands spans across DriveTime, Bridgecrest and SilverRock. You can find us at the intersection of technology and innovation as we use our proprietary tools and over two decades of industry knowledge to redefine the process of purchasing, financing, and protecting your vehicle.
That’s Nice, But What’s the Job?
This is not a position for which sponsorship will be provided. Individuals with temporary visas or who need sponsorship now or in the future are not eligible for hire at this time.
In short, as an Jr Data Analyst, you will be responsible for providing consumable data points to department leaders to assist in the day to day operations across DriveTime. You will review data trends and work with department leaders on identifying what is causing changes in trends, and collaborating on resolutions. You will apply your analytical and strategic skills as you initiate, build, and develop the business case for major company initiatives.
In long you will:
Develop KPI dashboards and reporting suite for the business and provide detailed review of data as necessary based on KPIs.
Bringing the strategy to life: our analysts are responsible for analyzing and developing the full life cycle of a project including implementation/execution Effectively communicate with concise insights and/or visualizations utilizing software such as Tableau while partnering collaboratively with colleagues and senior leaders in the organization.
Leveraging emerging technologies and critical thinking skills to continually innovate and automate existing methodologies, processes, and reporting. Design, develop, test, and implement new products and solutions.
What do you need?
Graduating December 2023 or May 2024
You are concentrating with the major(s) of Statistics, Physics, Mathematics, Operations Management, Economics, Business Analytics, Actuarial/Risk Analysis, Finance and/or Financial Management majors
You have knowledge of SQL, Excel, Snowflake & Visualization Tools such as Tableau or PowerBI
Excellent verbal and written communication skills. The ability to talk and write with confidence, charisma and competence for a wide variety of audiences including management.
Intellectual curiosity. Why? What? How? Do you find yourself always wanting to learn more and broaden your knowledge base? If so, this could be the role for you.
#Dice
So What About the Perks? Perks matter
Medical, dental, and vision, oh my! DriveTime Family of Brands covers a sizable amount of insurance premiums to ensure our employees receive top-tier healthcare coverage.
But Wait, There’s More. 401(K), Company paid life insurance policy, short and long-term disability coverage to name a few.
Growth Opportunities. You grow, I grow, we all grow! But seriously, DriveTime Family of Brands is committed to providing its employees with every opportunity to grow professionally with roughly over 1,000 employees promoted year over year.
Tuition Reimbursement. We’re as passionate about your professional development as you are. With that, we’ll put our money where our mouth is.
Wellness Program. Health is wealth! This program includes self-guided coaching and journeys, cash incentives and discounts on your medical premiums through engaging in fun activities!
Gratitude is Green. We offer competitive pay across the organization, because, well… money matters!
In-House Gym. We want our employees to be the best versions of themselves. So come early, take a break in your day or finish strong with a workout!
Give Us a Reason (or not), and We’ll Celebrate. Regardless of whether there is a holiday or not, we are finding ways to kick back and enjoy each other’s company outside of day-to-day work.
Smart-Casual Dress. Come dressed in jeans (you’ll fit right in with the rest of us).
Paid Time Off & Paid Holidays. Not just lip service: we work hard, to play hard.
Anything Else? Absolutely.
DriveTime Family of Brands is Great Place to Work Certified! And get this: 90% of our rockstar employees say they feel right at home here. We could spend a lot of time having you read about ALL our awards, but we’ll save time (and practice some humility) just naming a few others; Comparably Awards: Best Company for Diversity, Best Company Culture and Best Company Leadership, oh and don’t forget Phoenix Business Journal Healthiest Employers (okay, we’ll stop there)!
Hiring is contingent upon successful completion of our background and drug screening process. DriveTime is a drug-free, tobacco-free workplace and an Equal Opportunity Employer.
And when it comes to hiring, we don't just look for the right person for the job, we seek out the right person for DriveTime. Buckle up for plenty of opportunities to grow in a professional, fun, and high-energy environment!
Show more
Show less","SQL, Excel, Snowflake, Tableau, PowerBI, Data Analysis, Business Analytics, KPI Dashboards, Data Visualization, Data Mining, Reporting, Project Management, Strategic Planning, Communication, Problem Solving, Critical Thinking, Innovation, Automation","sql, excel, snowflake, tableau, powerbi, data analysis, business analytics, kpi dashboards, data visualization, data mining, reporting, project management, strategic planning, communication, problem solving, critical thinking, innovation, automation","automation, business analytics, communication, critical thinking, data mining, dataanalytics, excel, innovation, kpi dashboards, powerbi, problem solving, project management, reporting, snowflake, sql, strategic planning, tableau, visualization"
Data Analyst,Insight Global,"Phoenix, AZ",https://www.linkedin.com/jobs/view/data-analyst-at-insight-global-3781755840,2023-12-17,Chandler,United States,Associate,Onsite,"A company in Phoenix, AZ is looking for a Process Technician to join their team. This person will be responsible for assisting in the analysis of process improvement data in an industrial manufacturing environment. They will be required to work at a computer for 12 hours a day on a compressed work week schedule (Night shifts receive a 16% shift differential).
Must Haves:
High School Degree, GED, or Associate in STEM Program (Sciences, Technologies, Engineering, Mathematics, etc.)
2-8 Years of professional working experience
MS Excel (Data Analytics, Pivot Tables)
MS Outlook (email, calendar, navigation)
MS PowerPoint (Create and make presentations - to show data)
Use of mathematics, science, technology, or engineering principles in responsibilities, or in work environment
Works well in a flexible culture, not always have structure in daily work duties
Visual recognition of next action steps, code/process memory
Work history is analytical or has analysis in roles
Show more
Show less","MS Excel (Data Analytics Pivot Tables), MS Outlook (email calendar navigation), MS PowerPoint, Data Analytics, Pivot Tables, Mathematics, Science, Technology, Engineering, Visual recognition, Code/process memory, Analytical thinking","ms excel data analytics pivot tables, ms outlook email calendar navigation, ms powerpoint, data analytics, pivot tables, mathematics, science, technology, engineering, visual recognition, codeprocess memory, analytical thinking","analytical thinking, codeprocess memory, dataanalytics, engineering, mathematics, ms excel data analytics pivot tables, ms outlook email calendar navigation, ms powerpoint, pivot tables, science, technology, visual recognition"
Data Analyst,Insight Global,"Phoenix, AZ",https://www.linkedin.com/jobs/view/data-analyst-at-insight-global-3778826078,2023-12-17,Chandler,United States,Associate,Onsite,"Job Title:
Process Technician
Location:
Phoenix, AZ
Salary:
$49,000-55,000
Hours:
12-hour shifts (3 days on, 4 days off, 4 days on, 3 days off)
Duration:
Full time
Required Skills and Experience:
High School Degree, GED, or Associates in STEM Program (Sciences, Technologies, Engineering, Mathematics, etc.)
*NOTE: We are NOT accepting candidate's with Bachelor's or Master's degrees at this time.*
2-8 Years of professional working experience
MS Excel (Data Analystics, Pivot Tables)
MS Outlook (email, calendar, navigation)
MS PowerPoint (Create and make presentations - to show data)
Use of mathematics, science, technology, or engineering principles in responsibilities, or in work environment
Works well in flexible culture, not always have structure in daily work duties
Visual recognition of next action steps, code/process memory
Work history is analytical or has analysis in roles
Job Description:
One of Insight Global's large semiconductor manufacturing clients in Phoenix, AZ is looking for a Data Process Technician to join their team. This person will be responsible for assisting in the analysis of process improvement data in a industrial manufacturing environment. They will be required to work at a computer for 12 hours a day on a compressed work week schedule.
Show more
Show less","STEM, MS Excel, Data Analysis, Pivot Tables, MS Outlook, MS PowerPoint, Mathematics, Science, Technology, Engineering, Visual Recognition, Code, Process Memory, Analytical Thinking","stem, ms excel, data analysis, pivot tables, ms outlook, ms powerpoint, mathematics, science, technology, engineering, visual recognition, code, process memory, analytical thinking","analytical thinking, code, dataanalytics, engineering, mathematics, ms excel, ms outlook, ms powerpoint, pivot tables, process memory, science, stem, technology, visual recognition"
Data Analytics Intern (Summer 2024),DriveTime,"Tempe, AZ",https://www.linkedin.com/jobs/view/data-analytics-intern-summer-2024-at-drivetime-3706629047,2023-12-17,Chandler,United States,Associate,Onsite,"What’s Under The Hood
DriveTime Family of Brands is the largest privately owned used car sales finance & servicing company in the nation. Headquartered in Tempe, Arizona and Dallas, Texas, we create opportunities and improve the lives of our customers and our employees by placing a focus on putting the right customer, in the right vehicle, on the right terms and on their path to ownership.
The DriveTime Family of Brands spans across DriveTime, Bridgecrest and SilverRock. You can find us at the intersection of technology and innovation as we use our proprietary tools and over two decades of industry knowledge to redefine the process of purchasing, financing, and protecting your vehicle.
That’s Nice, But What’s the Job?
We are looking for skilled Analytics Interns for our 2024 summer internship program. Asan Analytics Intern, you will have the ability to work within one of our analytics groups that are dedicated to supporting a vertical within our complex business. These verticals range from operational analytics to risk analytics to servicing analytics (just to name a few).
The Anlaytics Intern will be responsible for things like:
Developing and maintaining forecasting models
Providing ad hoc analysis and special projects as required.
Working with department leaders to understand changes needed and collaborate on solutions for change.
Manipulating, analyzing, and presenting data using programs like SQL, Snowflake, Excel, Tableau, and Python
Innovating on existing products and processes while using emerging technologies and data analysis to identify new opportunities to improve customer experience and profitability.
Creating in-depth analyses, collaborating with stakeholders, and leveraging resources skillfully to capitalize on those new opportunities.
Demonstrating strong communication skills in making recommendations to senior executives across DriveTime’s family of companies.
So what are we looking for?
Killer analytical and reporting abilities. You’ll need the capability to analyze data and in return, prepare timely reports on your findings.
A mind for the details. Okay we know “detail-oriented” is on about every job description – but we really mean it!
Fantastic problem solver. Your job is not only to find the problem, but more importantly, find the solution.
Agile in a fast-past environment. We move, and we move quickly. Thriving in an environment that never stops, is a must.
Excellent verbal and written communication skills. The ability to talk and write with confidence, charisma and competence for a wide variety of audiences including management.
Intellectual curiosity. Why? What? How? Do you find yourself always wanting to learn more and broaden your knowledge base? If so, this could be the role for you.
The Specifics.
You are currently pursuing a bachelor's or master’s with a graduation date of December 2024 – May 2025
A quantitative degree in either; economics, finance, analytics, math, engineering, computer science or other STEM field.
Data analysis experience
Knowledge of Excel, SQL, Tableau, Python or R
Excel skills
Our internship program will be 11 weeks starting May 2024 to August 2024
Internships will be based out of our Home Office in Tempe, AZ and Dallas, TX
You can expect a regular on-site schedule Monday-Friday, 40 hours per week
So What About the Perks? Perks matter.
Not Just Coffee Runs. We’re talking real world experience. You’ll walk away from our internship program with hands-on experience, completed projects, full portfolios, and newly developed skills.
Who Says You Have to Walk Away? Across both our Tempe, AZ and Dallas, TX offices, over 60% of our interns stay through the fall semester, return the following summer, or are brought on full-time after graduation.
Growth & Development. You will be mentored by industry professionals, be given guidance along the way, and the tools to be successful.
Philanthropy: Give for Good. We are proud to be difference makers in our communities. We dedicate time for our Interns to give back with us.
Gratitude is Green. Out Internship Program is paid, because, well… money matters!
In-House Gym. We want our employees to be the best versions of themselves. So come early, take a break in your day or finish strong with a workout!
Give Us a Reason (or not), and We’ll Celebrate. Regardless of whether there is a holiday or not, we are finding ways to kick back and enjoy each other’s company outside of day-to-day work.
Smart-Casual Dress. Come dressed in jeans (you’ll fit right in with the rest of us).
Anything Else? Absolutely.
DriveTime Family of Brands is Great Place to Work Certified! And get this: 90% of our rockstar employees say they feel right at home here. We could spend a lot of time having you read about ALL our awards, but we’ll save time (and practice some humility) just naming a few others; Comparably Awards: Best Company for Diversity, Best Company Culture and Best Company Leadership, oh and don’t forget Phoenix Business Journal Healthiest Employers (okay, we’ll stop there)!
Hiring is contingent upon successful completion of our background and drug screening process. DriveTime is a drug-free, tobacco-free workplace and an Equal Opportunity Employer.
And when it comes to hiring, we don't just look for the right person for the job, we seek out the right person for DriveTime. Buckle up for plenty of opportunities to grow in a professional, fun, and high-energy environment!
Show more
Show less","Data Analysis, Forecasting, SQL, Snowflake, Excel, Tableau, Python, R, Communication, Problem Solving, Intellectual Curiosity, Economics, Finance, Analytics, Math, Engineering, Computer Science, STEM","data analysis, forecasting, sql, snowflake, excel, tableau, python, r, communication, problem solving, intellectual curiosity, economics, finance, analytics, math, engineering, computer science, stem","analytics, communication, computer science, dataanalytics, economics, engineering, excel, finance, forecasting, intellectual curiosity, math, problem solving, python, r, snowflake, sql, stem, tableau"
Sr. Data Analyst (Research),Exos,"Phoenix, AZ",https://www.linkedin.com/jobs/view/sr-data-analyst-research-at-exos-3779278119,2023-12-17,Chandler,United States,Associate,Onsite,"The Role:
If you enjoy research, managing and exploring data, and making discoveries within an applied environment, come join the Exos Human Performance Lab (HPL)! The HPL works on both internal and external projects, with our customers including a broad range of disciplines across both internal and external stakeholders. The
Senior Data Analyst
will enable efficient data-driven operations and will foster a data-centric environment.
This role will leverage knowledge, problem-solving skills, and a passion for research and analytics to inform the evolution of our data strategy and analytics functional unit and keep our operations running as efficiently as possible. The ideal candidate should have a strong ability to combine technical expertise with entrepreneurial spirit to evaluate study outcomes, provide meaningful insights, and create compelling stories and/or visualizations for non-technical audiences.
This is an onsite role, based out of our Phoenix, AZ headquarters, reporting to the Vice President of Research.
RESPONSIBILITIES
Work on a cross-functional team to manage and coordinate the prioritization and execution of analysis, statistical methods and reporting, interpret the implementation of statistical requests, identify areas for improving the statistical approach, and share in the success of the team.
Play a key role in the analysis and visualization of data to evaluate study outcomes and provide meaningful insights about Exos research initiatives.
Communicate all status updates to the project manager/senior research coordinator, participate in project team meetings, and communicate the status of the analytic phases per project to the leadership team.
Execute and evolve the data and device management approach and create procedures for reference and use as a training tool for research staff.
Train research staff on data and device related procedures, and evaluate task-based proficiencies, as necessary.
Lead the Data Strategy and Analytics (DSA) Functional Unit within the Human Performance Lab
The data strategy and analytics unit plays a vital role in unlocking the value of data within the HPL. Duties and responsibilities span across data strategy development, data governance, data analysis, reporting, predictive analytics, stakeholder collaboration, and continuous improvement, ultimately enabling data-driven decision-making and fostering a data-driven culture.
Partner with the Performance Innovation Team on internal research projects evolving Exos methodology, products, and services and helping us maintain our position as industry thought-leaders.
Work on externally funded (contract) research projects that broaden our reach throughout the industry with forward-thinking collaborations and contributions to the organization’s revenue.
Other tasks, as assigned.
QUALIFICATIONS
Passion for our mission: human performance
Bachelor’s degree from an accredited university in a quantitative and/or health related field AND 5+ years of experience
2+ years of professional experience with statistical analysis and reporting
A strong understanding of statistical concepts and substantial knowledge in analytical approaches for population health and human performance
Demonstrated expertise with research methods (quantitative and qualitative)
Proficiency with complex statistical analysis and quality assurance techniques, preferably in STATA or R
Working knowledge in data management, coding, and visualization techniques
Working knowledge of programming languages, preferably Python
Ability to design reports, analyses, and insights with appropriate audience in mind
Experience presenting information to executive stakeholders
Familiarity with command line interfaces, preferably Android Debug Bridge (adb)
Proficient using statistical analysis software
Working knowledge of human subjects research and regulatory compliance
Ability to earn trust, maintain positive and professional relationships, and contribute to a culture of inclusion
Willingness to promote the goals of Exos and support the expansion of its methodology
Must be legally eligible to work in the United States (currently not sponsoring visas for this role).
Preferred:
Master’s degree from an accredited university in a quantitative and/or health related field
Proficient with STATA or R
Experience with Qualtrics
Understanding of data management and automation techniques
Experience implementing data quality and assurance procedures
Experience with wearable devices
Experience with biometrics and sensor data
Show more
Show less","Data Analysis, Statistical Methods, Data Interpretation, Research, Data Visualization, Quantitative Analysis, Health Informatics, Statistical Concepts, Population Health, Human Performance, STATA, R, Python, Data Management, Coding, Programming Languages, Data Quality, Data Assurance, Stakeholder Collaboration, Predictive Analytics, Data Governance, Methodology Development, Qualitative Research, Reporting, DataDriven Decision Making, DataDriven Culture, ThoughtLeadership, Human Subjects Research, Regulatory Compliance, Inclusion, Qualtrics, Data Automation, Wearable Devices, Biometrics, Sensor Data","data analysis, statistical methods, data interpretation, research, data visualization, quantitative analysis, health informatics, statistical concepts, population health, human performance, stata, r, python, data management, coding, programming languages, data quality, data assurance, stakeholder collaboration, predictive analytics, data governance, methodology development, qualitative research, reporting, datadriven decision making, datadriven culture, thoughtleadership, human subjects research, regulatory compliance, inclusion, qualtrics, data automation, wearable devices, biometrics, sensor data","biometrics, coding, data assurance, data automation, data governance, data interpretation, data management, data quality, dataanalytics, datadriven culture, datadriven decision making, health informatics, human performance, human subjects research, inclusion, methodology development, population health, predictive analytics, programming languages, python, qualitative research, qualtrics, quantitative analysis, r, regulatory compliance, reporting, research, sensor data, stakeholder collaboration, stata, statistical concepts, statistical methods, thoughtleadership, visualization, wearable devices"
Data Engineer,Skiltrek,"Phoenix, AZ",https://www.linkedin.com/jobs/view/data-engineer-at-skiltrek-3772271142,2023-12-17,Chandler,United States,Mid senior,Onsite,"Job Title: Data Engineer
Location: Phoenix, AZ(Onsite)
Duration: 12 Months
Rate $60/hr W2
Need Consultants With Minimum 9 Years Overall Experience
Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies
Partner with business teams and communicate with product managers to automate business processes, build visualizations on key performance indicators, and provide real-time business insights to improve business performance
Perform ad hoc analysis on data and provide recommendations on ways to refine existing data models and improve both management and governance of data in cooperation with backend teams
Utilize database tools such as PostgreSQL with RDBMS, NoSQL databases, and Cloud-based data warehousing services such as RDS, Redshift, and Snowflake
Write in programming languages like Python with high-quality unit tests. Conducting reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance Basic Qualifications
Data Engineering Experience
Previous project experience with big data technologies
Knowledge of Agile engineering practices
PostgreSQL 11.3 or later
Application Development Experience
UNIX/Linux including basic commands and shell scripting
Strong programming fundamentals using Python 3
Working knowledge of Python libraries like pandas, NumPy, SQL alchemy, and dask
AWS Cloud Experience
Experience with visualization tools like Tableau and Quick sight.
Data warehousing experience (Redshift or Snowflake) Preferred Qualifications
Certifications
AWS Associate Developer
AWS Specialty in Big Data
Prior Projects
Hands-on experience with building end-to-end data pipelines for operational dashboards
Hands-on experience building interactive dashboards for operational reporting and decision making
Show more
Show less","PostgreSQL, RDBMS, NoSQL, AWS RDS, AWS Redshift, AWS Snowflake, Python programming, Unit testing, Tableau, QuickSight, Data warehousing, NumPy, SQL Alchemy, Dask, pandas, Big data technologies, AWS Cloud, UNIX/Linux, Shell scripting, Agile engineering, AWS Associate Developer, AWS Specialty in Big Data","postgresql, rdbms, nosql, aws rds, aws redshift, aws snowflake, python programming, unit testing, tableau, quicksight, data warehousing, numpy, sql alchemy, dask, pandas, big data technologies, aws cloud, unixlinux, shell scripting, agile engineering, aws associate developer, aws specialty in big data","agile engineering, aws associate developer, aws cloud, aws rds, aws redshift, aws snowflake, aws specialty in big data, big data technologies, dask, datawarehouse, nosql, numpy, pandas, postgresql, python programming, quicksight, rdbms, shell scripting, sql alchemy, tableau, unit testing, unixlinux"
Data Center Engineer,Extend Information Systems Inc.,"Phoenix, AZ",https://www.linkedin.com/jobs/view/data-center-engineer-at-extend-information-systems-inc-3726332451,2023-12-17,Chandler,United States,Mid senior,Onsite,"Hi
Satyam this side. We do have a new an excellent opportunity for you. This opportunity is a full time onsite position as
Data Center Engineer.
Please have a look at the job description below and let me know if you or someone you know is interested in this role. You can mail me at
satyam@extendinfosys.com
.
Job Title Data Center Engineer
Location Phoenix,AZ
Job Type Full Time
Experience 5+years
Job Description
Need resource to be onsite in Data Center- Phoenix, Arizona
Provide support in troubleshooting Hardware related problems.
Should be familiar with and work on deployment and decommission of hardware.
Communicating & Co-ordinating with vendors on hardware related issues and follow-up till resolution.
Able to understand the activities/migrations related to data center.
Work in a collaborative environment learning with other IT specialists.
L2 knowledge on Unix and Windows
Must have excellent communication, Customer Management, and interpersonal skills.
Have a good knowledge Nagios and experience in monitoring tool like Splunk, AppDynamics etc.
Thanks
Satyam Prajapati | Technical Recruiter| Extend Information Systems
Cell:
(571) 547-2880
Email:
satyam@extendinfosys.com
Address:
44258 Mercure Circle, UNIT 102 A, Sterling VA, USA - 20166
Web:
www.extendinfosys.com
Show more
Show less","Data Center Engineer, Phoenix AZ, Full Time, 5+ years, Hardware troubleshooting, Deployment and decommission of hardware, Unix and Windows, Nagios, Splunk, AppDynamics","data center engineer, phoenix az, full time, 5 years, hardware troubleshooting, deployment and decommission of hardware, unix and windows, nagios, splunk, appdynamics","5 years, appdynamics, data center engineer, deployment and decommission of hardware, full time, hardware troubleshooting, nagios, phoenix az, splunk, unix and windows"
Bigdata Developer,Extend Information Systems Inc.,"Phoenix, AZ",https://www.linkedin.com/jobs/view/bigdata-developer-at-extend-information-systems-inc-3690598743,2023-12-17,Chandler,United States,Mid senior,Onsite,"Hi Jobseekers,
I hope you are doing well!
We have an opportunity for
Bigdata Developer
with one of our clients for
Phoenix, AZ
Please see the job details below and let me know if you would be interested in this role.
If interested, please send me a copy of your resume, contact details, availability, and a good time to connect with you.
Title: Bigdata Developer
Location: 10004, Phoenix, AZ
Terms: Full-time
Job Details
8+ Experience as Big data Developer
Experience in design, development, and maintenance and support of Big Data Analytics using Hadoop Ecosystem components like HDFS, Hive, Pig, HBase, Sqoop, Zookeeper, Map Reduce, and oozie.
Strong working experience with ingestion, storage, querying, processing and analysis of big data.
Extensive Experience on working with Hadoop Architecture and the components of Hadoop - Map Reduce, HDFS, Job Tracker, Task Tracker, Name Node and Data Node.
Having exposure to faster processing of large datasets using Apache Spark and distributed messaging and multi-processing of Kafka.
Hands on experience in writing and tweaking Map Reduce programs according to the requirement.
Good experience with ""Productionalizing"" Apache Hadoop, Apache Spark and Apache Kafka applications (such as monitoring, debugging and performance tuning).
Experience in writing Pig scripts, hive scripts, pig UDF, hive UDF for ETL process workflows.
Familiar with importing and exporting data using Sqoop into HDFS and have experience in fine tuning the sqoop actions for terabytes of data.
Experience with SQL, PL/SQL and database concepts and NoSQL databases.
Good Experience with job workflow scheduling like Oozie, D-series and Autosys
Good understanding of Experience on creating databases, tables and views in HIVE, IMPALA
Thanks & Regards
Monika Singh
Extend Information System Inc
Phone:571-622-3980
Email: monika@extendinfosys.com
44258 Mercure Circle, UNIT 102 A, Sterling VA, USA 20166
Show more
Show less","Big Data, Hadoop, HDFS, Hive, Pig, HBase, Sqoop, Zookeeper, Map Reduce, Oozie, SQL, PL/SQL, NoSQL, Oozie, Dseries, Autosys, HIVE, IMPALA, Pig scripts, Hive scripts, Pig UDF, Hive UDF, ETL, Sqoop, Kafka, Spark","big data, hadoop, hdfs, hive, pig, hbase, sqoop, zookeeper, map reduce, oozie, sql, plsql, nosql, oozie, dseries, autosys, hive, impala, pig scripts, hive scripts, pig udf, hive udf, etl, sqoop, kafka, spark","autosys, big data, dseries, etl, hadoop, hbase, hdfs, hive, hive scripts, hive udf, impala, kafka, map reduce, nosql, oozie, pig, pig scripts, pig udf, plsql, spark, sql, sqoop, zookeeper"
Lead Data Engineer-DE,Zortech Solutions,"Scottsdale, AZ",https://www.linkedin.com/jobs/view/lead-data-engineer-de-at-zortech-solutions-3778569516,2023-12-17,Chandler,United States,Mid senior,Onsite,"Hello Professionals, We have urgent requirement for
Lead Data Engineer-DE
Please have a look at the below job description, if interested please share your updated resume, email me, pavan@zortechsolutions.ca or share any References.
Role: Lead Data Engineer-DE
Location: Scottsdale AZ (day 1 onsite)
Duration: Fulltime
Job Description
Must have skill set: Java, Scala , Python, Spark, S3, Glue, Redshift
You have 6-8 years of relevant software development experience.
You have hands-on experience in Java/Scala/Python, Spark, S3, Glue, Redshift. This is critical
Highly analytical and data oriented.
Experience in SQL, NoSql Database
Data masking of on prem PII data.
Develop API calls with using secure data transfer.
Take standard output data to lower environments for pre prod testing!
Enable secured channels for data models and data science activities.
Need a solution that can scale to handle millions of data. i.e., masking 10 million records in 15 mins
You have experience with development tools and agile methodologies.
Show more
Show less","Java, Scala, Python, Spark, S3, Glue, Redshift, SQL, NoSQL, Data masking, API calls, Agile methodologies","java, scala, python, spark, s3, glue, redshift, sql, nosql, data masking, api calls, agile methodologies","agile methodologies, api calls, data masking, glue, java, nosql, python, redshift, s3, scala, spark, sql"
"ONSITE: Data Engineer GCP Phoenix, AZ( 100% Onsite day 1 )","Conch Technologies, Inc","Phoenix, AZ",https://www.linkedin.com/jobs/view/onsite-data-engineer-gcp-phoenix-az-100%25-onsite-day-1-%C2%A0-at-conch-technologies-inc-3784584591,2023-12-17,Chandler,United States,Mid senior,Onsite,"HI,
Greetings from Conch Technologies Inc
Position: Data Engineer ( More hands-on exp on GCP )
Experience: 9+ years
Location: Phoenix, AZ( 100% Onsite day 1 )
Duration: 12+ Months Contract
Required Skills
9+ Years experience as a GCP
A/ML experience needed
Good to have experience in Machine Learning
Good to have experience in Artificial Intelligence.
Good Communication Skills.
Thanks and Regards,
Chanakya
[IT Recruiter]
Direct : 214-247-7117
chanakya@conchtech.com
linkedin.com/in/nameischanikya
Show more
Show less","Google Cloud Platform (GCP), Artificial Intelligence (AI), Machine Learning (ML), Communication Skills","google cloud platform gcp, artificial intelligence ai, machine learning ml, communication skills","artificial intelligence ai, communication skills, google cloud platform gcp, machine learning ml"
Data Engineer – Insurance Group Domain client,IVY TECH SOLUTIONS INC,"Phoenix, AZ",https://www.linkedin.com/jobs/view/data-engineer-%E2%80%93-insurance-group-domain-client-at-ivy-tech-solutions-inc-3787776114,2023-12-17,Chandler,United States,Mid senior,Onsite,"This role is a Data Engineer focused on Information and Kafka Streaming. 12 month contract – remote initially but will need to be onsite in Phoenix when things open back up.
Data Engineer – Insurance Group Domain client
The data engineer is a critical role that will provide data engineering design, ETL development and technical expertise to an ETL scrum team. They will work on a team with other ETL developers, QA engineers and analysts to support data streams for payment, claim and customer service operations. The team is responsible for a large catalog of jobs that use a mixture of batch ETL architecture and real-time data streaming with API-integrated data services. The data engineer will spend their time doing hands on development, designing future data processes, conducting data analysis, consulting with other teams and interacting throughout the Agile process in a stable scrum team environment.
Essential Functions / Principal Responsibilities
Develops data pipelines in both batch ETL and real-time streaming architectures.
Develops data models to define new or modify existing data structures in support of data integration initiatives.
Provides expert technical knowledge of data solutions for business projects.
Provides source system analysis, data discovery, complex transformation assessment and target system exploration to understand information data requirements and anticipate user needs.
Contributes to data pipeline design, coding, and technical / functional reviews while collaborating with source system developers, data engineers and functional subject matter experts.
Develops effective data pipeline solutions to deliver business features.
Adheres to best practices for data movement, data quality, data profiling, data cleansing and other data pipeline related activities.
Applies tuning and optimization for continuous improvement.
Presents technical information in easily understood terms (written, verbal and visual).
Communicates effectively within the Agile team and to external stakeholders and management.
Follows Agile best practices and adheres to internal IT processes like change management and problem management.
Skills that will Ensure Success:
Specialist in ETL development with a demonstrated understanding of transactional data processing, streaming data and data pipeline best practices.
Experience in build, unit test, and deployment of Informatica ETL processes.
Knowledgeable in making REST API calls within data processes.
Familiar with real-time data pipeline platforms, preferably StreamSets, AWS Glue or similar platform.
Hands on experience with data streaming in Apache Kafka.
Able to interpret business needs and turn them into a technical plan of attack with pros and cons of various approaches to the data processing options.
Demonstrates a solid understanding of technical standards and processes related to batch and real-time data pipeline development.
Excellent team player, able to work with product owners, technical developers, DBAs, system administrators, BI professional services, data warehouse operations and functional experts.
Expertise in SQL query transactions and optimization, especially T-SQL.
Understand nulls, cardinality, joins, data types to develop technical ETL specifications and technical metadata.
Ability to integrate an application solution into the broader business and IT ecosystem in which it will operate.
Firm understanding of quality assurance activities and automation in data pipeline and ETL processing.
Desire experience working with financial and/or claims data requiring compliance, balancing and integrity checks, especially payment-related data, PCI compliant data and banking industry formats such as NACHA.
Desire a firm understanding of cloud data processing and data streaming architectures, especially in AWS.
Please feel free to share your resume to
Powered by JazzHR
5v894PvtuH
Show more
Show less","Data Engineering, ETL Development, Kafka Streaming, Informatica ETL, REST API, StreamSets, AWS Glue, Apache Kafka, SQL, TSQL, Data Pipeline Design, Data Modeling, Data Quality, Data Profiling, Data Cleansing, Data Warehousing, Business Intelligence, Agile Development, Change Management, Problem Management, Cloud Data Processing, Cloud Data Streaming, AWS","data engineering, etl development, kafka streaming, informatica etl, rest api, streamsets, aws glue, apache kafka, sql, tsql, data pipeline design, data modeling, data quality, data profiling, data cleansing, data warehousing, business intelligence, agile development, change management, problem management, cloud data processing, cloud data streaming, aws","agile development, apache kafka, aws, aws glue, business intelligence, change management, cloud data processing, cloud data streaming, data engineering, data pipeline design, data profiling, data quality, datacleaning, datamodeling, datawarehouse, etl development, informatica etl, kafka streaming, problem management, rest api, sql, streamsets, tsql"
Lead Data Engineer-DE - US,Zortech Solutions,"Fountain Hills, AZ",https://www.linkedin.com/jobs/view/lead-data-engineer-de-us-at-zortech-solutions-3782816737,2023-12-17,Chandler,United States,Mid senior,Onsite,"Role: Lead Data Engineer-DE
Location: Scottsdale AZ (day 1 onsite)
Duration: Fulltime
Job Description
Must have skill set: Java, Scala , Python, Spark, S3, Glue, Redshift
You have 6-8 years of relevant software development experience.
You have hands-on experience in Java/Scala/Python, Spark, S3, Glue, Redshift. This is critical
Highly analytical and data oriented.
Experience in SQL, NoSql Database
Data masking of on prem PII data.
Develop API calls with using secure data transfer.
Take standard output data to lower environments for pre prod testing!
Enable secured channels for data models and data science activities.
Need a solution that can scale to handle millions of data. i.e., masking 10 million records in 15 mins
You have experience with development tools and agile methodologies.
Show more
Show less","Java, Scala, Python, Spark, S3, Glue, Redshift, SQL, NoSql, Data masking, Secure data transfer, Agile methodologies","java, scala, python, spark, s3, glue, redshift, sql, nosql, data masking, secure data transfer, agile methodologies","agile methodologies, data masking, glue, java, nosql, python, redshift, s3, scala, secure data transfer, spark, sql"
"100% ONSITE: AI Machine Learning Data Engineer, Phoenix, AZ","Conch Technologies, Inc","Phoenix, AZ",https://www.linkedin.com/jobs/view/100%25-onsite-ai-machine-learning-data-engineer-phoenix-az-at-conch-technologies-inc-3777993750,2023-12-17,Chandler,United States,Mid senior,Onsite,"HI,
Greetings from Conch Technologies Inc
Position: GCP (AI/ML)
Experience: 9 years
Location: Phoenix, AZ
100% onsite
Required Skills
9+ Years experience as a GCP
A/ML experience needed
Good to have experience in Machine Learning
Good experience in Artificial Intelligence.
Good Communication Skills.
Thank you &
Regards
V S Durga Prasad - I
T Recruiter
Direct: 901-466-4708
Mobile:
615-922-1491
Email: vprasad@conchtech.com
LinkedIn: www.linkedin.com/in/shunmukh-durga
Show more
Show less","Google Cloud Platform (GCP), Cloud Computing, Artificial Intelligence (AI), Machine Learning (ML), Communication Skills","google cloud platform gcp, cloud computing, artificial intelligence ai, machine learning ml, communication skills","artificial intelligence ai, cloud computing, communication skills, google cloud platform gcp, machine learning ml"
Data Engineer I,Motion Recruitment,"Phoenix, AZ",https://www.linkedin.com/jobs/view/data-engineer-i-at-motion-recruitment-3774938148,2023-12-17,Chandler,United States,Mid senior,Remote,"Our client is a globally renowned industry leader and has garnered a reputation for excellence. Headquartered in Phoenix, AZ, they are currently seeking a
Data Engineer I
to be an independent contributor on a fast-growing Data Engineering team pursuing a vision of analytics-driven excellence. This role is full-time, permanent, and remote with occasional travel to the Phoenix corporate offices.
Your expertise in data engineering and software engineering will enable and empower our organization to build and deploy data driven solutions to production. You will work in close collaboration with operations, subject matter experts, data scientists, and software engineers to develop advanced, highly automated data products. You will be a champion of DataOps, and agile practices; actively participating in project teams to drive value.
Required Skills & Experience
Bachelor’s degree in engineering, computer science, analytical field (Statistics, Mathematics, etc.) or related discipline and three (3) years of relevant work experience –OR-
Master’s or Ph.D. in engineering, computer science, analytical field (Statistics, Mathematics, etc.) or related discipline and one (1) year of relevant work experience
Strong experience in at least two areas:
Knowledgeable Practitioner of SQL development with experience designing high quality, production SQL codebases
Knowledgeable Practitioner of Python development with experience designing high quality, production Python codebases
Knowledgeable Practitioner in data engineering, software engineering, and ML systems architecture
Knowledgeable Practitioner of data modeling
Experience applying software development best practices in data engineering projects, including Version Control, P.R. Based Development, Schema Change Control, CI/CD, Deployment Automation, Test Driven Development/Test Automation, Shift left on Security, Loosely Coupled Architectures, Monitoring, Proactive Notifications using Python and SQL
Data science experience wrangling data, model selection, model training, modeling validation, e.g., Operational Readiness Evaluator and Model Development and Assessment Framework, and deployment at scale
Desired Skills & Experience
Working knowledge of Azure Stream Architectures, DBT, Schema Change tools, Data Dictionary tools, Azure Machine Learning Environment, GIS Data
Working knowledge of Software Engineering and Object Orient Programming Principles
Working knowledge of Distributed Parallel Processing Environments such as Spark or Snowflake
Working knowledge of problem solving/root cause analysis on Production workloads
Working knowledge of Agile, Scrum, and Kanban
Working knowledge of workflow orchestration using tools such as Airflow, Prefect, Dagster, or similar tooling
Working knowledge with CI/CD and automation tools like Jenkins or Azure DevOps
Experience with containerization tools such as Docker
Strong verbal and written communication skills in English language
Daily Responsibilities
Agile Project Work: Work in cross-functional, geographically distributed agile teams of highly skilled data engineers, software/machine learning engineers, data scientists, DevOps engineers, designers, product managers, technical delivery teams, and others to continuously innovate analytic solutions.
Design, develop, and review real-time/bulk data pipelines from a variety of sources (streaming data, APIs, data warehouse, messages, images, video, etc)
Follow established design patterns for data ingest, transformation, and egress.
Develop documentation of Data Lineage and Data Dictionaries to create a broad awareness of the enterprise data model and its applications.
Apply best practices within DataOps (Version Control, P.R. Based Development, Schema Change Control, CI/CD, Deployment Automation, Test Automation, Shift left on Security, Loosely Coupled Architectures, Monitoring, Proactive Notifications)
Problem Solving/Project Management: Constructively challenge while soliciting participation in problem solving to enrich possible solutions.
Architecture: Utilize modern cloud technologies and employ best practices from DevOps/DataOps to produce enterprise quality production Python and SQL code with minimal errors. Participate in regular code review sessions and collaboratively discuss opportunities for continuous improvement in all solutions.
Self-Development: Flexibly seek out new work or training opportunities to broaden experience. Independently research latest technologies and openly discuss applications within the department.
Perform other duties as requested.
The Offer
Bonus eligible
You Will Receive The Following Benefits
Medical, Dental, and Vision Insurance
Vacation Time
Stock Options
Applicants must be currently authorized to work in the US on a full-time basis now and in the future.
Posted By:
Debbie Jasper
Show more
Show less","SQL, Python, Data engineering, Data modeling, Software development, Version Control, Schema Change Control, CI/CD, Deployment Automation, Test Driven Development, Test Automation, Monitoring, Proactive Notifications, ML systems architecture, Azure Stream Architectures, DBT, Schema Change tools, Data Dictionary tools, Azure Machine Learning Environment, GIS Data, Software Engineering, Object Orient Programming Principles, Distributed Parallel Processing Environments, Spark, Snowflake, Problem solving, Root cause analysis, Agile, Scrum, Kanban, Workflow orchestration, Airflow, Prefect, Dagster, CI/CD, Automation tools, Jenkins, Azure DevOps, Docker, Communication skills, Agile Project Work, Data pipelines, Data Lineage, Data Dictionaries, Best practices, Problem Solving, Architecture, SelfDevelopment","sql, python, data engineering, data modeling, software development, version control, schema change control, cicd, deployment automation, test driven development, test automation, monitoring, proactive notifications, ml systems architecture, azure stream architectures, dbt, schema change tools, data dictionary tools, azure machine learning environment, gis data, software engineering, object orient programming principles, distributed parallel processing environments, spark, snowflake, problem solving, root cause analysis, agile, scrum, kanban, workflow orchestration, airflow, prefect, dagster, cicd, automation tools, jenkins, azure devops, docker, communication skills, agile project work, data pipelines, data lineage, data dictionaries, best practices, problem solving, architecture, selfdevelopment","agile, agile project work, airflow, architecture, automation tools, azure devops, azure machine learning environment, azure stream architectures, best practices, cicd, communication skills, dagster, data dictionaries, data dictionary tools, data engineering, data lineage, datamodeling, datapipeline, dbt, deployment automation, distributed parallel processing environments, docker, gis data, jenkins, kanban, ml systems architecture, monitoring, object orient programming principles, prefect, proactive notifications, problem solving, python, root cause analysis, schema change control, schema change tools, scrum, selfdevelopment, snowflake, software development, software engineering, spark, sql, test automation, test driven development, version control, workflow orchestration"
Sr. Data Engineer - Remote AZ,Blue Cross Blue Shield of Arizona,"Phoenix, AZ",https://www.linkedin.com/jobs/view/sr-data-engineer-remote-az-at-blue-cross-blue-shield-of-arizona-3762886226,2023-12-17,Chandler,United States,Mid senior,Remote,"Awarded a Healthiest Employer, Blue Cross Blue Shield of Arizona aims to fulfill its mission to inspire health and make it easy. BCBSAZ offers a variety of health insurance products and services to meet the diverse needs of individuals, families, and small and large businesses as well as providing information and tools to help individuals make better health decisions.
This position is remote within the state of AZ only.
This remote work opportunity requires residency, and work to be performed, within the State of Arizona.
About The Job
8+ years of Experience in advanced SQL techniques, including analytic functions
5+ Expertise of relational database performance structures and techniques; specifically indexing , materialized views, and partitioning
3+ Experience working with Big Data technology and cloud such as Hadoop , Spark, Hive, Microsoft Azure
5+ Hands-on coding experience with any scripting language Python
Experience with technical problem solving and debugging skill
Experience designing and implementing automated tests, including unit tests and integration tests.
Experience In HealthCare Payor Preferred.
Roles and Responsibilities
Mentors other team members in areas of expertise and ETL processes and standards and participates in code reviews
Develop and maintain ETL (Extract, Transform, Load) processes to extract, transform, and load data into the data warehouse, ensuring data accuracy and quality.
Prepares ETL packages for deployment to development, test, and production environments following all Release Management policies
ETL design and development knowledge using the Microsoft BI tool set, preferably SSIS
Purpose of the job
Designs and implements business intelligence and extract, transform, and load (ETL) solutions using programming, performance tuning, data modeling. Creates databases optimized for performance, implementing schema changes, and maintaining data architecture standards across all of the business’s databases. Serves as a liaison between the Database Administration department and development teams.
Qualifications
REQUIRED QUALIFICATIONS
Required Work Experience
4 years of experience in computer programming, query design, and databases
6 years of experience in computer programming, query design, and databases (Senior level)
Required Education
High-School Diploma or GED in general field of study
Required Licenses
N/A
Required Certifications
N/A
Preferred Qualifications
Preferred Work Experience
4+ years of experience building and managing complex Data Integration solutions.
4+ years of experience in computer programming, query design, and databases
4+ years of experience Database administration with SQL Server
6+ years of experience building and managing complex Data Integration solutions (Senior)
6+ years of experience in computer programming, query design, and databases (Senior)
6+ years of experience Database administration with SQL Server (Senior)
Preferred Education
Bachelor’s Degree in Information Technology or related field preferred
Master’s Degree in Information Technology or related field (Senior)
Preferred Licenses
N/A
Preferred Certifications
MS SQL Certification or other certification in current programming languages
ESSENTIAL Job Functions AND RESPONSIBILITIES
Data Engineer –
Performs job functions with general supervision and peer review
Learn area’s direct flow; and how it affects surrounding systems and operational areas.
Architect, design, construct, test, tune, deploy, and support Data Integration solutions for various data management systems.
Contribute to the team’s knowledge base with useful information such as adopted standards, procedure documentation, problem resolution advice, etc.
Participate in the promotion of SQL Server best practices
Collaborate with other technology teams and architects to define and develop solutions.
Research and experiment with emerging Data Integration technologies and tools.
Work with the team to establish and reinforce disciplined software development, processes, standards, and error recovery procedures are deployed; ensuring a high degree of data quality.
Support Enterprise database clustering, mirroring, replication among other SQL Server technologies.
Develop, write and implement processing requirements and post implementation review
Facilitate and/or create new procedures and processes that support advancing technologies or capabilities
Design & Implement Extract, Transform, and Load (ETL) solutions utilizing SSIS
Apply data mining rules
Create logic, system, and program flows for complex systems, including interfaces and metadata
Write and execute unit test plans. Track and resolve any processing issues.
Implement and maintain operational and disaster-recovery procedures.
Participate in the review of code and/or systems for proper design standards, content and functionality.
Participate in all aspects of the Software Development Life Cycle
Analyze files and map data from one system to another
Adhere to established source control versioning policies and procedures
Meet timeliness and accuracy goals.
Communicate status of work assignments to stakeholders and management.
Responsible for technical and production support documentation in accordance with department standards and industry best practices.
Maintain current knowledge on new developments in technology-related industries
Participate in corporate quality and data governance programs
The position requires a full-time work schedule. Full-time is defined as working at least 40 hours per week, plus any additional hours as requested or as needed to meet business requirements.
Senior -
Performs job functions with minimal supervision in a Lead capacity
Research, analyze, track and resolve complex production problems.
Build, support, and maintain complex processes, programs, and data.
Evaluate high-level project information and assess project components to forecast work effort required.
Provide peer-level review and mentoring to peers.
Participate and/or lead complex technical projects.
Act as a subject matter expert in two or more of areas as assigned:
BI Production Reporting
Database Architecture
Database Architecture and Support
Training
Act as primary operational contact for internal and external customers when needed or in the absence of manager.
Ensure Service Level Agreements between department and operational or technical areas are met.
Lead, develop and mentor staff by providing opportunities for growth through delegation, training, and assignment to various project teams.
Inform manager of any issues impacting the efficient and effective performance of the department including system, resource, and informational barriers; Provide timely feedback to team member on performance.
Assist the manager in the day-to-day operations of the department.
ALL LEVELS
Each progressive level includes the ability to perform the essential functions of any lower levels and mentor employees in those levels.
Maintain current on new developments in technology-related industries.
Participate in corporate quality and data governance programs.
Participate in on-call rotation.
Perform all other duties as assigned.
The position requires a full-time work schedule. Full-time is defined as working at least 40 hours per week, plus any additional hours as requested or as needed to meet business requirements. Participate in on-call rotation.
Required Competencies
Required Job Skills (Applies to All Levels)
Intermediate skill in use of office equipment, including copiers, fax machines, scanner and telephones
Intermediate PC proficiency in spreadsheet, database and word processing software
Advanced knowledge of business intelligence, programming, and data analysis software
Intermediate knowledge of Microsoft SQL databases and database administration
Intermediate proficiency in T-SQL, NZ-SQL, PostgreSQL, NoSQL, Hadoop, data tuning, enterprise data modeling and schema change management.
Working technical knowledge of current software protocols and Internet standards to the extent that they apply to database administration.
Excellent database troubleshooting skills
Working technical knowledge of PowerShell.
Strong object-oriented design and analysis skills
Experience consuming, organizing and analyzing JSON and XML messages as data.
Required Professional Competencies (Applies to All Levels)
Knowledge of agile development practices
Strong analytical skills to support independent and effective decisions
Ability to prioritize tasks and work with multiple priorities, sometimes under limited time constraints.
Perseverance in the face of resistance or setbacks.
Effective interpersonal skills and ability to maintain positive working relationship with others.
Verbal and written communication skills and the ability to interact professionally with a diverse group, executives, managers, and subject matter experts.
Systems research and analysis. Ability to write and present business intelligence documentation
Demonstrate the ability to stay current on global threats and vulnerabilities.
Maintain confidentiality and privacy
Required Leadership Experience and Competencies
Build synergy with a diverse team in an ever changing environment
Facilitate and resolve customer requests and inquiries for all levels of management within the Corporation (Senior only)
Participation in one or more groups that aids in setting/enforcing standards and/or providing educational opportunities across the IT organization. (Senior only)
Preferred Competencies
Preferred Job Skills
(Applies to All Levels)
Knowledge of HIPAA regulations
Advanced proficiency in spreadsheet, SQL queries, database, flow charting, and word processing software
Advanced knowledge of data mapping techniques
Advanced knowledge of computer operating systems
Advanced knowledge of decision support systems
Advanced knowledge of programming, database systems, and data management.
Advanced knowledge of decision support systems
Advanced knowledge of Business Objects
Preferred Professional Competencies (Applies To All Levels)
Advanced systems research and analysis expertise
Impeccable project management skills
Solid technical ability and problem solving skills
Knowledge of internal departments and operations
Strong technical documentation skills and a strong ability to translate technical concepts so that they are easily understood by laypersons
Preferred Leadership Experience And Competencies (Applies To Senior Level)
Ability to provide mentoring and peer review to junior team members
Ability to build lesson plans and deliver lessons to junior team members
Our Commitment
BCBSAZ does not discriminate in hiring or employment on the basis of race, ethnicity, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, protected veteran status or any other protected group.
Thank you for your interest in Blue Cross Blue Shield of Arizona. For more information on our company, see azblue.com. If interested in this position, please apply.
Show more
Show less","SQL, Data Warehousing, Big Data, Cloud Computing, Python, ETL, SSIS, Microsoft Azure, Hadoop, Spark, Hive, Data Mining, Data Modeling, Data Analysis, Data Integration, TSQL, NZSQL, PostgreSQL, NoSQL, PowerShell, JSON, XML, Agile Development, HIPAA, Business Objects, Decision Support Systems","sql, data warehousing, big data, cloud computing, python, etl, ssis, microsoft azure, hadoop, spark, hive, data mining, data modeling, data analysis, data integration, tsql, nzsql, postgresql, nosql, powershell, json, xml, agile development, hipaa, business objects, decision support systems","agile development, big data, business objects, cloud computing, data integration, data mining, dataanalytics, datamodeling, datawarehouse, decision support systems, etl, hadoop, hipaa, hive, json, microsoft azure, nosql, nzsql, postgresql, powershell, python, spark, sql, ssis, tsql, xml"
"Staff Software Engineer - Data Infrastructure - (Permanent Remote, US)","Aledade, Inc.","Phoenix, AZ",https://www.linkedin.com/jobs/view/staff-software-engineer-data-infrastructure-permanent-remote-us-at-aledade-inc-3768363693,2023-12-17,Chandler,United States,Mid senior,Remote,"As a Staff Software Engineer - Data Infrastructure , you will build technology that saves lives. At Aledade we use data to save lives. This opportunity is to improve millions of people's mental and physical health, that's what we do here at Aledade. We empower primary care physicians to use technology to keep their patients healthy, by preventing unnecessary hospitalizations.
At Aledade, you will sit at the intersection of:
A huge problem
; amounting to trillions of dollars of waste in U.S. Healthcare spending.
An approach that moves the needle
; In the U.S., we wait until someone has a stroke, then are hospitalized. From there, no expense is spared. Using technology, Aledade changes that by enabling doctors to intervene with the proper preventive care, like blood pressure control, before it becomes a stroke.
A platform that works at scale
; last year 11,000 physicians across 36 states served ~1.7M patients, saving 24,000 unnecessary hospitalizations and 120,000 visits to the emergency room using Aledade’s technology.
But the journey is only 1% done,
come join us to build a world-class team that will not quit till it addresses the trillions of dollars of suffering in U.S. healthcare spending.
As a Staff Software Engineer , you will take us beyond traditional monolithic SQL engines and batch pipelines. You will build the next generation of distributed data storage and processing systems. You will build systems that can scale indefinitely and surpass traditional query performance while making the interfaces for that data simple, expressive, and cleanly abstracted. Your interfaces will support a broad array of data consumers, from our web application to business analytics and artificial intelligence.
Qualifications:
BS/BTech (or higher) in Computer Science or a related field required
5+ years of production-level experience as an engineer building highly scalable and reliable infrastructure
Experience architecting, developing, and deploying large-scale distributed systems at scale
Experience with cloud APIs (e.g., a public cloud such as AWS, Azure, GCP, or an advanced private cloud such as Google or Facebook)
Experience with security and systems that handle sensitive data
3+ years of experience working with SQL or other database querying languages on large multi-table data sets.
3+ years of experience acting as a trusted technical decision-maker in a team setting, solving for short-term and long-term business value
Bonus
Working knowledge of health-tech systems, like Electronic Health Records, Clinical data, etc.
Responsibilities:
Architect abstractions to allow for ingesting and processing multiple clinical and patient data feeds.
Use the latest tools to harvest knowledge from data.
Re-design systems to meet the growing data and query needs.
Develop automated build, test, and release infrastructures.
Set and uphold the standard for engineering processes to support high-quality engineering, including style and code checking, test harnesses, and release packaging.
Balance speed and scalability, dealing with millions of records per client and pesky data integration and data quality issues, while optimizing for time to market.
Maintain the security of protected patient health information.
As the engineering team grows, you will lift the people around you:
Mentoring and galvanizing new engineers to do their best work.
Working across disciplines to shape a product strategy and execution.
Who We Are:
Aledade, a public benefit corporation, exists to empower the most transformational part of our health care landscape - independent primary care. We were founded in 2014, and since then, we've become the largest network of independent primary care in the country - helping practices, health centers and clinics deliver better care to their patients and thrive in value-based care. Additionally, by creating value-based contracts across a wide variety of payers, we aim to flip the script on the traditional fee-for-service model. Our work strengthens continuity of care, aligns incentives, and ensures primary care physicians are paid for what they do best - keeping patients healthy. If you want to help create a health care system that is good for patients, good for practices and good for society - and if you're eager to join a collaborative, inclusive and remote-first culture - you've come to the right place.
What Does This Mean for You?
At Aledade, you will be part of a creative culture that is driven by a passion for tackling complex issues with respect, open-mindedness and a desire to learn. You will collaborate with team members who bring a wide range of experiences, interests, backgrounds, beliefs and achievements to their work - and who are all united by a shared passion for public health and a commitment to the Aledade mission.
In addition to time off to support work-life balance and enjoyment, we offer the following comprehensive benefits package designed for the overall well-being of our team members:
Flexible work schedules and the ability to work remotely are available for many roles
Health, dental and vision insurance paid up to 80% for employees, dependents, and domestic partners Robust time off plan 21 days of PTO in your first year 2 Paid Volunteer Days & 11 paid holidays
12 weeks paid Parental Leave for all new parents
6 weeks paid sabbatical after 6 years of service
Educational Assistant Program & Clinical Employee Reimbursement Program
401(K) with up to 4% match
Stock options
And much more!
At Aledade, we don’t just accept differences, we celebrate them! We strive to attract, develop, and retain highly qualified individuals representing the diverse communities where we live and work. Aledade is committed to creating a diverse environment and is proud to be an equal opportunity employer. Employment policies and decisions at Aledade are based on merit, qualifications, performance, and business needs. All qualified candidates will receive consideration for employment without regard to age, race, color, national origin, gender (including pregnancy, childbirth or medical conditions related to pregnancy or childbirth), gender identity or expression, religion, physical or mental disability, medical condition, legally protected genetic information, marital status, veteran status, or sexual orientation.
Privacy Policy: By applying for this job, you agree to Aledade's Applicant Privacy Policy available at https://www.aledade.com/privacy-policy-applicants
Show more
Show less","SQL, Distributed Systems, Cloud APIs (AWS Azure GCP), Security, Data integration, Data quality, Agile, DevOps, Mentorship","sql, distributed systems, cloud apis aws azure gcp, security, data integration, data quality, agile, devops, mentorship","agile, cloud apis aws azure gcp, data integration, data quality, devops, distributed systems, mentorship, security, sql"
Sr. Data Analyst,Aston Carter,"Phoenix, AZ",https://www.linkedin.com/jobs/view/sr-data-analyst-at-aston-carter-3788742988,2023-12-17,Chandler,United States,Mid senior,Remote,"Description:*
Essential Job Functions:*
Support clients: Work and communicate closely with their clients throughout the report/dashboard development process. Provide status updates. Help clients understand their report/dashboard. Provide on-going maintenance and support.
Support projects: Participate in outcome evaluation, program evaluation, and quality improvement studies. Attend meetings, answer data-related questions, and offer suggestions.
Create reports and dashboards: Create ad-hoc and routine reports. Design and develop dashboards to display key metrics and trends.
Manage data: Collect, organize, store, and share a wide variety of data.
Transform data: Clean and optimize data for analyses.
Ensure data quality: Audit data, data transformation processes, workflow, deliverables and outputs.
Perform analyses: Perform statistical analyses (descriptive and inferential analyses).
Present findings: Present data and findings in a clear and concise manner, using appropriate reporting and data visualization tools.
Create and maintain documentation: Create FDD, document report requirement, business logic and workflow. Create data dictionaries. Ensure documentation is up-to-date.
Maintain up-to-date knowledge on information management systems, processes and data.
Manage compliance reporting: Maintain up-to-date knowledge of CMS, DHCS and internal compliance reporting requirement. Translate reporting requirement into reports. Work with clients to ensure accuracy of data. Submit report to external and internal agencies in a timely manner. Attend compliance trainings, meetings, and data validation webinars.
Support system enhancement/implementation: Perform data-related research and testing. Stay informed of system and process changes. Identify impact on existing reports and dashboards. Modify existing reports and dashboards accordingly.
Prioritize work and keep supervisor informed: Work on multiple projects at the same time. Organize and prioritize work effectively. Inform management when requirement or due date cannot be met.
Adheres to all quality, compliance and regulatory standards to achieve organization outcomes.
Required Education: *
Bachelor’s or Master’s degree in Analytics, Healthcare Informatics, Statistics, Computer Science, or related field.
Required Experience:*
At least 5 years of experience analyzing and compiling data, preferably in a health plan setting.
Required Skills/Abilities:*
Ability to manipulate and analyze data to produce accurate results. Present results in data visualizations, dashboards, and reports.
Knowledge in CMS Medicare Advantage (Part C), CMS Prescription Drug Coverage (Part D), and CMS Special Needs Plan (SNP), and DHCS Medi-Cal Managed Care reporting requirements.
Knowledge in authorization, claims, and encounter data. Clinical code knowledge (ICD, CPT, etc) related to utilization data.
Advanced skills in Microsoft Office, SQL Transactional SQL (T-SQL), SQL Server Reporting Services (SSRS), and Tableau.
Experience in SQL Server Integration Services (SSIS), Visual Basic for Applications (VBA).
Must have analytical, communication, documentation, interpersonal, planning, presentation, problem-solving and research skills.
About Aston Carter:
Please Note: Scammers are posing as Aston Carter. We'll never contact you via Gmail, Telegram, or WhatsApp and we'll never solicit money from you.
At Aston Carter, we’re dedicated to expanding career opportunities for the skilled professionals who power our business. Our success is driven by the talented, motivated people who join our team across a range of positions – from recruiting, sales and delivery to corporate roles. As part of our team, employees have the opportunity for long-term career success, where hard work is rewarded and the potential for growth is limitless. Established in 1997, Aston Carter is a leading staffing and consulting firm, providing high-caliber talent and premium services to more than 7,000 companies across North America. Spanning four continents and more than 200 offices, we extend our clients’ capabilities by seeking solvers and delivering solutions to address today’s workforce challenges. For organizations looking for innovative solutions shaped by critical-thinking professionals, visit [AstonCarter.com.](AstonCarter.com) Aston Carter is a company within Allegis Group, a global leader in talent solutions. The company is an equal opportunity employer and will consider all applications without regards to race, sex, age, color, religion, national origin, veteran status, disability, sexual orientation, gender identity, genetic information or any characteristic protected by law. If you would like to request a reasonable accommodation, such as the modification or adjustment of the job application process or interviewing process due to a disability, please call 888-237-6835 or email [astoncarteraccommodation@astoncarter.com](mailto:%20astoncarteraccommodation@astoncarter.com) for other accommodation options. However, if you have questions about this position, please contact the Recruiter located at the bottom of the job posting. The Recruiter is the sole point of contact for questions about this position.
Show more
Show less","SQL, Data Analysis, SQL Transactional SQL (TSQL), Data Visualization, Microsoft Office, Tableau, Data Quality, Data Management, Data Audit, Data Extraction, Reporting, Dashboarding, Data Maintenance, Statistical Analysis, Communication, Documentation, Interpersonal Skills, Research, SQL Server Reporting Services (SSRS), SQL Server Integration Services (SSIS), Visual Basic for Applications (VBA), Analytics, CMS Medicare Advantage (Part C), CMS Prescription Drug Coverage (Part D), CMS Special Needs Plan (SNP), DHCS MediCal Managed Care, Authorization, Claims, Encounter Data, Clinical Code Knowledge, ICD, CPT","sql, data analysis, sql transactional sql tsql, data visualization, microsoft office, tableau, data quality, data management, data audit, data extraction, reporting, dashboarding, data maintenance, statistical analysis, communication, documentation, interpersonal skills, research, sql server reporting services ssrs, sql server integration services ssis, visual basic for applications vba, analytics, cms medicare advantage part c, cms prescription drug coverage part d, cms special needs plan snp, dhcs medical managed care, authorization, claims, encounter data, clinical code knowledge, icd, cpt","analytics, authorization, claims, clinical code knowledge, cms medicare advantage part c, cms prescription drug coverage part d, cms special needs plan snp, communication, cpt, dashboard, data audit, data extraction, data maintenance, data management, data quality, dataanalytics, dhcs medical managed care, documentation, encounter data, icd, interpersonal skills, microsoft office, reporting, research, sql, sql server integration services ssis, sql server reporting services ssrs, sql transactional sql tsql, statistical analysis, tableau, visual basic for applications vba, visualization"
Data Engineer | Contract to Hire  | Remote,Spanco Solutions,United States,https://www.linkedin.com/jobs/view/data-engineer-contract-to-hire%C2%A0-remote-at-spanco-solutions-3728263830,2023-12-17,Chandler,United States,Mid senior,Remote,"Title: Data Engineer
Job Type: Contract to Hire
Remote
Top 3 Skills: Data Manipulation (DBX, Spark, SQL)
Job Description
Supply Chain Data Strategy and Cloud Operation are core to the clients operations. With sales of over $100 B, the capability of delivering critical data and building cloud first solutions our teams offer are one of the most critical capabilities of the company. We’re looking for individuals who can bring their core set of knowledge as well as learn new tools to provide new data and capabilities to support growing Supply Chain.
Accountable for developing and delivering technological responses to targeted business outcomes.
Analyze, design and develop enterprise data and information architecture deliverables, focusing on data as an asset for Supply Chain and the overall enterprise.
Understand and follow reusable standards, design patterns, guidelines, and configurations to deliver valuable data and information across the enterprise, including direct collaboration with 84.51, where needed.
Demonstrate the company’s core values of respect, honesty, integrity, diversity, inclusion and safety.
Key Responsibilities
Create and leverage Databricks notebooks to source, shape and store data using DBX, SQL, Python, PySpark
Utilize enterprise standards for data domains and data solutions, focusing on simplified integration and streamlined operational and analytical uses
Ensure there is clarity between ongoing projects, escalating when necessary, including direct collaboration with 84.51
Define high-level migration plans to address the gaps between the current and future state
Analyze technology environments to detect critical deficiencies and recommend solutions for improvement
Promote the reuse of data assets, including the management of the data catalog for reference
Show more
Show less","Data Manipulation, DBX, Spark, SQL, Data Engineering, Cloud Operation, Data Architecture, Design Patterns, Data Warehousing, Data Catalog, Databricks, Python, PySpark, Enterprise Standards, Migration Plans, Data Analysis, Data Quality, Data Security","data manipulation, dbx, spark, sql, data engineering, cloud operation, data architecture, design patterns, data warehousing, data catalog, databricks, python, pyspark, enterprise standards, migration plans, data analysis, data quality, data security","cloud operation, data architecture, data catalog, data engineering, data manipulation, data quality, data security, dataanalytics, databricks, datawarehouse, dbx, design patterns, enterprise standards, migration plans, python, spark, sql"
Big Data Engineer,IntraEdge,Greater Phoenix Area,https://www.linkedin.com/jobs/view/big-data-engineer-at-intraedge-3778867577,2023-12-17,Chandler,United States,Mid senior,Hybrid,"Based in PHoenix, AZ, hybrid.
GCP Big Data Engineer
GCP Data Engineer with Bigdata skills
Experience working in GCP based Big Data deployments (Batch/Real-Time) leveraging Big Query, Big Table, Google Cloud Storage, PubSub, Data Fusion,
8+ years of application development experience required
In-depth knowledge about SQL databases and ability to execute queries quickly.
Knowledge of data warehousing and data modeling.
Build and maintain data structures and databases.
Design data processing systems.
Analyze data and enable machine learning.
Developing comprehensive abilities for Google data structures and matrix for upcoming product development
Good to have: Experience and understanding of Cornerstone DAM = Data access manage, Automated Case workflow mgmt (Case Management tools) , ISP (Intuitive Servicing Platform).
Inventory Management tools
Show more
Show less","GCP Big Data, BigQuery, BigTable, Google Cloud Storage, PubSub, Data Fusion, SQL, Data warehousing, Data modeling, Data structures, Databases, Data processing systems, Data analysis, Machine learning, Google data structures, Matrix, Cornerstone DAM, Data access management, Automated Case workflow management, Case Management tools, ISP (Intuitive Servicing Platform), Inventory Management tools","gcp big data, bigquery, bigtable, google cloud storage, pubsub, data fusion, sql, data warehousing, data modeling, data structures, databases, data processing systems, data analysis, machine learning, google data structures, matrix, cornerstone dam, data access management, automated case workflow management, case management tools, isp intuitive servicing platform, inventory management tools","automated case workflow management, bigquery, bigtable, case management tools, cornerstone dam, data access management, data fusion, data processing systems, data structures, dataanalytics, databases, datamodeling, datawarehouse, gcp big data, google cloud storage, google data structures, inventory management tools, isp intuitive servicing platform, machine learning, matrix, pubsub, sql"
Senior Data Engineer,Motion Recruitment,"Chandler, AZ",https://www.linkedin.com/jobs/view/senior-data-engineer-at-motion-recruitment-3781903942,2023-12-17,Chandler,United States,Mid senior,Hybrid,"Outstanding long-term contract opportunity! A well-known Financial Services Company is looking for a Senior Data Engineer in Chandler, AZ (Hybrid).
Work with the brightest minds at one of the largest financial institutions in the world. This is long-term contract opportunity that includes a competitive benefit package! Our client has been around for over 150 years and is continuously innovating in today's digital age. If you want to work for a company that is not only a household name, but also truly cares about satisfying customers' financial needs and helping people succeed financially, apply today.
Contract Duration: 12- Months
Required Skills & Experience
Demonstrated innovation in proposing and implementing structural enhancements for increased efficiency and reduced error susceptibility.
Strong presentation skills to articulate and advocate for novel ideas and strategies.
Aptitude for analytical thinking and comfortable recommending enhancements in processes.
Proficiency in both Python and SAS.
Experience working within the Teradata Vantage environment.
This role demands a seasoned Data Engineer with substantial expertise in both model development and the foundational aspects of data engineering.
The ideal candidate will possess a proactive, innovative mindset, capable of presenting and implementing ideas for process enhancement and efficiency.
If you meet these qualifications and are enthusiastic about contributing to a dynamic team focused on innovation, please apply with your updated resume and any relevant work samples.
What You Will Be Doing
Proficient in model development with a primary focus on data engineering.
Expertise in aggregating and integrating diverse datasets, establishing robust processes supporting model execution, and facilitating the deployment of model outputs to relevant destinations.
Specialized in automation development, thorough testing, and ensuring comprehensive documentation outputs.
Managing and refining processes integral to executing the model while emphasizing developmental aspects.
Posted By:
Judy Jett
Show more
Show less","Python, SAS, Teradata Vantage, Data engineering, Model development, Automation development, Testing, Documentation, Agile, Data integration, Data aggregation, Process management, Process refinement, Deployment","python, sas, teradata vantage, data engineering, model development, automation development, testing, documentation, agile, data integration, data aggregation, process management, process refinement, deployment","agile, automation development, data aggregation, data engineering, data integration, deployment, documentation, model development, process management, process refinement, python, sas, teradata vantage, testing"
Data Analyst - International Trade,Intel Corporation,"Phoenix, AZ",https://www.linkedin.com/jobs/view/data-analyst-international-trade-at-intel-corporation-3784833601,2023-12-17,Chandler,United States,Mid senior,Hybrid,"Job Description
Intel is in the midst of a transformational journey to deliver on its ambitious IDM 2.0 strategy which includes significant manufacturing expansions and the creation of a foundry to serve internal and external customers, as well as a product group to invent the next generation of silicon products to support AI and the future of compute. To support the IDM 2.0- strategy, Intel is embarking on a transformative ERP project. To support this critical initiative, the International Trade Group is seeking a dynamic Data Analyst with deep expertise in implementing harmonized data and reporting solutions.
At Intel, the International Trade Group (ITG) is responsible for ensuring that Intel can seamlessly transfer millions of items across borders in compliance with trade regulations. The data analyst and reporting team is enabling access to high-quality data, metrics, and reports for trade.
If you are looking for a role that gives you the opportunity to influence and shape Intel's future, then this is the role for you.
Responsibilities May Include But Are Not Limited To
Works directly with subject matter experts, vendors, and partner organizations to deliver project milestones including requirements gathering, business process mapping, testing, documentation, and training.
Design and implement the capability to ingest, harmonize, and analyze trade data from multiple disparate sources e.g., SAP GTS, internal shipments, and government trade data to improve visibility, and establish trends and metrics that provide insight into trade operations for decision-making.
Engage with teams like Supply Chain, Finance, and IT to integrate and optimize data sources, ensuring high-quality, consistent, and accessible data solutions.
Gather data/reporting requirements by collaborating with internal and external stakeholders, understanding AS-IS and TO-BE processes, business requirements, key gaps, and objectives with a focus on data design and integration.
Develop data flows and diagrams, transition/change management plans and perform fit-gap analyses to ensure effective integration of data into business processes and capabilities.
Communicate, explain, and clarify data issues with the customer/business groups, and translate impacts to business requirements.
Validates feasibility of an idea, understands expectations from the solution, conducts design due diligence, and evaluates business value and risks based on solution expertise.
Use predictive modeling, statistics, machine learning, data mining, and other data analysis techniques to collect, explore, and extract insights from structured data and where appropriate, unstructured data.
The Ideal Candidate Should Exhibit The Following Behavioral Traits
Express concepts and data concisely verbally and in writing.
Build/foster relationships with business partners and collaboration through influencing and negotiation skills.
Attention to detail, exceptional analytics skills, and willingness to go depth in understanding International Trade processes/tools.
Qualifications
You must possess the below minimum qualifications to be initially considered for this position. Preferred qualifications are in addition to the minimum requirements and are considered a plus factor in identifying top candidates.
Minimum Qualifications
The candidate must possess a Bachelor's or Master's degree in, International Trade, Business, Management, Information Systems, Computer Science, Industrial Engineering, Data Science, or a related field.
5+ years of relevant International Trade experience, business analysis, and/or Data related roles.
Strong technical background with an understanding of data structures/design and experience in analysis utilizing standard query language and visualization.
Preferred Qualifications
Familiarity with data visualization tools such as PowerBI or similar.
Expert in data wrangling and analyzing massive and complex datasets, structured or unstructured.
Experience in requirement analysis, design, and prototyping.
Hands on experience with Analytics tools like Sql server analysis services, Azure Databricks, Snowflake.
Designing and deploying solutions for Global Trade.
The ideal candidate will have the following skills in addition to the qualifications listed below:
Experience in project management with organizational skills and a track record for delivering a set of objectives on schedule and within budget.
Must be a team player, with a demonstrated experience technically influencing others.
Strong problem-solving skills and the ability to tackle complex data-related challenges.
Excellent communication skills and the ability to collaborate effectively with customers and team members.
A proactive and self-driven approach.
This position is not eligible for employment-based visa/immigration sponsorship.
Intel sponsors individuals for employment-based visas for positions where we experience a shortage of US Workers. These skills shortage roles are typically STEM contributing positions requiring a Master's or PhD degree, or a Bachelor's degree with three years related job experience. This position does not qualify for Intel Sponsorship because it is either (1) a non-STEM contributing position, or (2) a STEM position that only requires a bachelor's degree and less than three years' experience.
Inside this Business Group
Intel’s Growth Acceleration Team (GAT) offers unique opportunities to work in a variety of areas, including counsel to Intel businesses; technology and intellectual property licensing; patent prosecution; trademarks and brands; litigation, mergers, acquisitions and investing; public policy, legislative and regulatory lobbying; global trade, export, import, and customs; and corporate compliance.
Other Locations
US, NM, Albuquerque; US, OR, Hillsboro; US, CA, Folsom; US, CA, Santa Clara
Posting Statement
All qualified applicants will receive consideration for employment without regard to race, color, religion, religious creed, sex, national origin, ancestry, age, physical or mental disability, medical condition, genetic information, military and veteran status, marital status, pregnancy, gender, gender expression, gender identity, sexual orientation, or any other characteristic protected by local law, regulation, or ordinance.
Benefits
We offer a total compensation package that ranks among the best in the industry. It consists of competitive pay, stock, bonuses, as well as, benefit programs which include health, retirement, and vacation. Find more information about all of our Amazing Benefits here.
Annual Salary Range for jobs which could be performed in US, California: $108,050.00-$161,630.00
Salary range dependent on a number of factors including location and experience
Working Model
This role will be eligible for our hybrid work model which allows employees to split their time between working on-site at their assigned Intel site and off-site.
In certain circumstances the work model may change to accommodate business needs.
Show more
Show less","SAP GTS, PowerBI, Sql server analysis services, Azure Databricks, Snowflake, Data wrangling, Data visualization, Predictive modeling, Statistics, Machine learning, Data mining, International Trade, Business analysis, Data analysis, Data structures, Data design, Standard query language, Requirement analysis, Design, Prototyping, Project management, Communication skills, Collaboration, Problemsolving, RDBMS","sap gts, powerbi, sql server analysis services, azure databricks, snowflake, data wrangling, data visualization, predictive modeling, statistics, machine learning, data mining, international trade, business analysis, data analysis, data structures, data design, standard query language, requirement analysis, design, prototyping, project management, communication skills, collaboration, problemsolving, rdbms","azure databricks, business analysis, collaboration, communication skills, data design, data mining, data structures, data wrangling, dataanalytics, design, international trade, machine learning, powerbi, predictive modeling, problemsolving, project management, prototyping, rdbms, requirement analysis, sap gts, snowflake, sql server analysis services, standard query language, statistics, visualization"
SQL Database Administrator/Architect,The Intersect Group,"Phoenix, AZ",https://www.linkedin.com/jobs/view/sql-database-administrator-architect-at-the-intersect-group-3777096742,2023-12-17,Chandler,United States,Mid senior,Hybrid,"***Only USC or GC Holders Accepted***
About the Position
The Sr SQL DBA/Data Architect functions as a technical lead overseeing the architectural design, development, implementation, maintenance support, and management of complex data service systems including design and management of data warehouse and databases using database platform tools and systems, such as Microsoft SQL Server, Power BI, and other Microsoft suite of data products and Power Platform capabilities. This position provides technical solutions, expertise, guidance, and direction to database administrators, database developers, data analysts, report writers, and IT executive management on a broad range of complex information technology matters and projects.
Position Qualifications
We recognize your time is valuable, so please apply if you meet the following required qualifications:
Education
Bachelor’s Degree in Computer Science, Computer Information Systems, or related field.
Experience
Seven years (7)
in an information technology role that involves responsibility, accountability, and hands-on experience with database & data warehousing architecture, design, development, and implementation, and includes reviewing and managing complex information technology projects using Microsoft suite of data products and Power Platform.
OR
Combined education and experience qualifications
Other combinations of education or experience may substitute for the minimum qualifications on a year for year basis.
Other requirements
Training that may be provided on-the-job: FEMA NIMS – IS100 and 700; Safety; Payroll; PREA.
All candidates pass a background review.
Our Preferred Candidate has
Master’s degree in Computer Science, Computer Information Systems, or related field.
Experience in database, data warehousing and business intelligence architect role.
Experience building databases, data warehouses, business intelligence solutions, including high availability and disaster recovery with the Microsoft technology stack using the best practices with Microsoft suite of data products and Power Platform.
Proven experience with Microsoft SQL Server, T-SQL, Stored Procedures & Triggers, SSIS, SSRS, PowerShell, Power BI, Python, R.
Experience with Team Foundation Server, or Git administration / configuration or build and release management.
Experience working with Microsoft Dynamics, SharePoint, and Power Platform.
Experience working in Agile Methodology.
Essential Job Tasks
(This is not an all-inclusive list of all job duties that may be required; employees will be required to perform other related duties as assigned.)
Provides technical and analytical guidance; develops and defines business and technical requirements and solution designs as part of specific projects.
Leads technical design and development with key role in technical decision process; evaluates and troubleshoots functional design and system architecture, component, and framework programming.
Develops and recommends strategies and specifications for data solutions based on the analysis of the business goals, objectives, needs, and existing data systems infrastructure.
Oversees the design, assessment, selection, application and maintenance of major databases, data warehousing & business intelligence solutions and metadata structures.
Leads the Data and Data Warehousing & Business Intelligence teams to maximize existing systems, ensure data integrity / data consistency, chain of custody, and develop analysis to be used in mission critical reporting.
Formulates data and metadata policies and procedures for functional design and development to build, maintain and upgrade the data model, ensuring compliance with organization data standards.
Administers technical troubleshooting and integration of new technology or new capabilities into the metadata and data standards and structures and completes assessment of the technical specifications of proposals and alternatives recommended to optimize database performance.
Develops a logical/physical database design by applying data design standards and guidelines.
Develops and facilitates resolution of data related project issues.
Creates and proposes technical design documentation which includes database design models, specifications, and diagrams to provide direction to development teams on the physical implementation of the data model.
Creates and maintains documentation as it relates to data configuration, mapping, standards, and processes.
Builds processes supporting data transformation, data structures, metadata, dependencies, and model management.
Designs, develops, and tests data migration solutions as needed.
Assists in the design of databases and data warehouses to ensure interoperability with BI solutions.
Monitors emerging data warehouse technologies and recommends applications appropriate to the organization’s environment and its users to ensure the data warehousing solution remains current.
Designs and delivers end-user training and training materials; provides technical support as necessary.
Develops and presents conceptual and functional innovation prototypes for assessment and evaluation.
Shapes the strategy, architecture, methodology and standards for delivering business intelligence and data visualization solutions.
Plans, prepares, defines, structures, develops, and implements complex database systems.
Manages data integrity, security, availability, recoverability to ensure optimal performance.
Participates in multiple, technically complex, mission-critical, and/or high-profile data projects or initiatives.
Partners with internal customers to conceptualize new projects, recommends technical resources and strategies, and designs project requirements.
Conducts detailed reviews of all major development projects undertaken by information technology functions within the MCSO, within area of expertise or as requested by management.
Identifies opportunities to re-engineer business practices to enhance the MCSO’s information technology strategies and plans.
Maintains state-of the-art knowledge in emerging technologies, methodologies, and best practices to enhance the enterprise’s image as a first-class organization.
Provides expertise regarding the integration of applications across the business.
Determines specifications; plans, designs, and develops complex and business critical data solutions individually or in partnership with project team.
Acts as evangelist for BI benefits across the organization; promotes BI usage to relevant departments.
Provides expert level support for the most difficult support problems; resolves unique or non-routine data and BI problems.
Analyzes short and long-term strategic data or BI/reporting problems and develops solutions.
Performs other duties as assigned.
Knowledge, Skills, and Abilities
Knowledge of
Computer applications program design, development, and implementation.
Big data tools and capabilities as well as data modeling.
Industry best practices and a deep understanding of application development, continuous integration and continuous delivery, current data structure / modeling, and DevOps.
SharePoint information architectures, workflows, and mobile applications best practices.
Cloud Based platforms and methodologies.
Skill in
Communicating effectively with all levels of clients, including business stakeholders, technical staff, and vendors.
Maintaining and updating technical documents for application and database support, including Tier 1 support personnel
Providing technical leadership, with consultative application of technical and management principles and software architecture / design skills to others.
Designing database architecture that manages data and joins diverse sets of data and sources.
Using REST APIs, HTML5, CSS and other core web technology concepts.
Datawarehouse design and implantation.
Team Foundation Server / DEVOPS.
PowerBI or other reporting platforms such as tableau or Qlik.
Microsoft Power Platform, Microsoft Dynamics, and SharePoint.
Ability to
Address and resolve unique situations or obstacles creatively.
Plan, architect, design, organize, monitor, control, and implement information systems projects.
Coach application administrators, developers, database administrators, and other technical staff.
Lead people from many different disciplines with varying degrees of technical expertise.
Provide technical oversight across systems and application and leverage support skills across multiple teams and departments.
Interact with APIs, utilizing SDKs and other tools.
Show more
Show less","SQL, Data Warehousing, Microsoft SQL Server, Microsoft Dynamics, Power BI, Python, R, TSQL, Stored Procedures, Triggers, SSIS, SSRS, PowerShell, SharePoint, Team Foundation Server, Git, REST APIs, HTML5, CSS, Data Modeling, Big Data Tools, Cloud Based Platforms, DevOps, Agile Methodology, Tableau, Qlik","sql, data warehousing, microsoft sql server, microsoft dynamics, power bi, python, r, tsql, stored procedures, triggers, ssis, ssrs, powershell, sharepoint, team foundation server, git, rest apis, html5, css, data modeling, big data tools, cloud based platforms, devops, agile methodology, tableau, qlik","agile methodology, big data tools, cloud based platforms, css, datamodeling, datawarehouse, devops, git, html5, microsoft dynamics, microsoft sql server, powerbi, powershell, python, qlik, r, rest apis, sharepoint, sql, ssis, ssrs, stored procedures, tableau, team foundation server, triggers, tsql"
Senior Data Integration Developer,"Sunrise Systems, Inc.","Phoenix, AZ",https://www.linkedin.com/jobs/view/senior-data-integration-developer-at-sunrise-systems-inc-3729930047,2023-12-17,Chandler,United States,Mid senior,Hybrid,"Title: Senior Data Integration Developer
Location: Phoenix AZ 85004
duration: 6-12 months Contract
Weekly hybrid schedule – 3 days onsite Tuesday, wednesday, Thursday and 2 days Remote (Monday, Friday)
Job Description
Works within a team that manages a public health data warehouse (Lakehouse) platform deployed in a cloud environment.
Plans, prepares, defines, structures, installs, and deploys complex databases; deploys timely upgrades and patches, manages performance and resources, manages system security, and provisions data access controls.
Performs data modeling for OLAP applications. Creates physical data models based on logical models. Creates optimal database objects for both data store and transformation / access.
Develops ELT / ETL data pipelines and data ingestion procedures. Creates processes for automation leveraging cloud provider native toolsets and technologies.
Troubleshoots data-related issues using SQL and other scripting / high-level languages (e.g., R, Python)
Develops test execution plans and designs processes for automating quality assurance. Aligns data integrity and quality checks throughout the data life cycle with County security standards and data governance rules.
Develops processes for stress testing the system to measure for optimal performance. Defines and configures required autoscaling rules that meet business needs. Creates automated alerts to monitor system performance and trigger notifications when system resources reach a pre-defined threshold.
Establishes guidelines for backup and restore. Creates a plan for recovery to ensure business continuity in case of a disaster or data corruption.
Creates rules for change management and deployment. Works with business users and developers to ensure stakeholder alignment, conducts impact analyses, tests changes prior to promoting into production, and validates after successful deployment. Develops a detailed rollback plan in case of any failures and unsuccessful validation after the deployment.
Develops and maintains appropriate documentation for system design, code, procedures, security, and protocols.
Maintains professional and technical knowledge through education, benchmarking state-of-the-art practices, and publications. Shares subject matter expertise to support Public Health staff with the enterprise data warehouse.
Experience Required:
7+ years of progressively responsible programming experience or an equivalent combination of training and experience. Specialized in implementing ELT applications using Snowflake and Azure cloud (Data Factory, Storage containers, On-premises data gateways, Integration Runtime environments, Hub-spoke network architecture etc.). Should be familiar with Active directory, SAML SSO, Okta integration
Education Required:
Bachelor`s degree in Information Technology or Computer Science
If you're interested, Please contact below,
Jani. S | Team Lead Recruitment
Sunrise Systems Inc.
Desk: 732-837-0402
Email: jani.s@sunrisesys.com | URL: www.sunrisesys.com
Show more
Show less","Data Integration, Data Warehouse, Cloud Computing, SQL, R, Python, ELT, ETL, Snowflake, Azure, Data Factory, Storage Containers, Onpremises Data Gateways, Integration Runtime Environments, HubSpoke Network Architecture, Active Directory, SAML SSO, Okta, Information Technology, Computer Science","data integration, data warehouse, cloud computing, sql, r, python, elt, etl, snowflake, azure, data factory, storage containers, onpremises data gateways, integration runtime environments, hubspoke network architecture, active directory, saml sso, okta, information technology, computer science","active directory, azure, cloud computing, computer science, data factory, data integration, datawarehouse, elt, etl, hubspoke network architecture, information technology, integration runtime environments, okta, onpremises data gateways, python, r, saml sso, snowflake, sql, storage containers"
Sr. Software Engineer - Data Engineer (Contract),Early Warning®,"Scottsdale, AZ",https://www.linkedin.com/jobs/view/sr-software-engineer-data-engineer-contract-at-early-warning%C2%AE-3778901202,2023-12-17,Chandler,United States,Mid senior,Hybrid,"Come build the next-gen fintech at Early Warning, network operator of Zelle®, where we’re relentlessly focused on empowering prosperity in all its forms.
From fast money movement for over 100 million people who can access Zelle® directly through their banking app to new account opening and beyond – we make a difference in the lives of consumers and businesses every day and enable them to live their best financial lives. And we’re only getting started.
With new state-of the-art offices in Scottsdale, AZ (Headquarters), Chicago, IL, San Francisco, CA – plus a growing presence in New York City – we’re entering our next big chapter. We focus all hiring efforts in a few states and within driving distance of an office location to enable in-person collaboration when and where possible. Priority hiring locations are Scottsdale, AZ, Chicago, IL, NY tri-state metro area (New Jersey, New York and Connecticut) and San Francisco, CA. We are not actively recruiting in Rhode Island.
People matter to us, so we think our best work is done when we’re together, in-person. From informal interactions, to sharing ideas, to mentoring and beyond. We also believe in workplace flexibility and empowering teams to determine the rhythm of work to create an inclusive culture. Through the power of innovative collaboration, we offer hybrid and (when necessary) virtual (“remote”) workplace models.
Join us and make your mark on what’s next in fintech.
Applicants must be authorized to work for any employer in the United States. We are unable to sponsor an employment Visa for this position.
Engineering at Early Warning (EWS) is a blend of teams organized around many different platforms, capabilities and products that are brought together to power core capabilities at the biggest banks in America – this includes ubiquitous products like Zelle and Paze.
These capabilities are typically provided behind a customer-facing API or integration point which enables the EWS teams to innovate aggressively where big wins can be found. The teams aligned behind these efforts drive their own innovation in partnership with stakeholders.
If you are hungry for large scale challenges and crave opportunities to learn and contribute in a big way (in an environment that is predominately Java) – we’d love to talk to you!
Overall Purpose
Develops, documents, tests, debugs and maintains software product applications; provides expertise and familiarity in software tools, frameworks and best practices in secure software engineering. Collaborates effectively with other members of the team to deliver on the team’s backlog.
Essential Functions
Participates in the discussion and implementation around complex capabilities.
Develops complex features and capabilities to enhance the Early Warning core product offerings.
Managing source code using Git.
Receiving, reviewing, and approving pull requests ensuring that code quality remains high.
Querying platform data using SQL-based DBs.
Working on complex Java Web Services using Spring Boot.
Document complex details and decisions in Confluence.
Building automated tests alongside sprint work that is integrated into the CI/CD pipeline, considering environment variables, and providing recommendations.
Able to troubleshoot complex bugs that may arise during development.
Support the company’s commitment to risk management and protecting the integrity and confidentiality of systems and data.
The above job description is not intended to be an all-inclusive list of duties and standards of the position. Incumbents will follow instructions and perform other related duties as assigned by their supervisor.
Minimum Qualifications
Education and/ or experience typically obtained through completion of a bachelor’s degree in computer science, Computer Science Engineering, Computer Information Systems, Electrical Engineering, or related field.
Minimum 5 years direct experience in Software Engineering or related experience.
Proficient with Git source control and Git flow branching model.
Spring / Spring boot framework is required.
Familiarity with modern DevOps practices.
Familiarity writing automated tests.
Proficient with SQL.
Proficient in RESTful service design.
Experience with Docker/ Kubernetes.
Experience with Kafka.
Excellent written and spoken communication.
Background and drug screen
Preferred Qualifications
Computer language experience (Python, PySpark, Spark)
Knowledge of HDFS, Linux must
Preferable experience with HBase, Aerospike
Experience with test automation is beneficial
Experience with Gitlab, Harness
Monitoring and Alerting systems experience (AppDynamics & Splunk)
Knowledge of mature engineering practices (CI/CD, testing, secure coding, etc.).
Knowledge of Software Development Lifecycle (SDLC) best practices, software development methodologies (Agile, Scrum, LEAN etc.) and DevOps practices.
Physical Requirements
Working conditions consist of a normal office environment. Work is primarily sedentary and requires extensive use of a computer and involves sitting for periods of approximately four hours. Work may require occasional standing, walking, kneeling, and reaching. Must be able to lift 15 pounds occasionally and/or negligible amount of force frequently. Requires visual acuity and dexterity to view, prepare, and manipulate documents and office equipment including personal computers. Requires the ability to communicate with internal and/or external customers.
Employee must be able to perform essential functions and physical requirements of position with or without reasonable accommodation.
Some of the Ways We Prioritize Your Health and Happiness
Healthcare Coverage – Competitive medical (PPO/HDHP), dental, and vision plans as well as company contributions to your Health Savings Account (HSA) or pre-tax savings through flexible spending accounts (FSA) for commuting, health & dependent care expenses.
401(k) Retirement Plan – Featuring a 100% Company Safe Harbor Match on your first 6% deferral immediately upon eligibility.
Paid Time Off – Unlimited Time Off for Exempt (salaried) employees, as well as generous PTO for Non-Exempt (hourly) employees, plus 11 paid company holidays and a paid volunteer day.
12 weeks of Paid Parental Leave
Maven Family Planning – provides support through your Parenting journey including egg freezing, fertility, adoption, surrogacy, pregnancy, postpartum, early pediatrics, and returning to work.
And SO much more! We continue to enhance our program, so be sure to check our Benefits page here for the latest. Our team can share more during the interview process!
Early Warning Services, LLC (“Early Warning”) considers for employment, hires, retains and promotes qualified candidates on the basis of ability, potential, and valid qualifications without regard to race, religious creed, religion, color, sex, sexual orientation, genetic information, gender, gender identity, gender expression, age, national origin, ancestry, citizenship, protected veteran or disability status or any factor prohibited by law, and as such affirms in policy and practice to support and promote equal employment opportunity and affirmative action, in accordance with all applicable federal, state, and municipal laws. The company also prohibits discrimination on other bases such as medical condition, marital status or any other factor that is irrelevant to the performance of our employees.
Show more
Show less","Java, Spring Framework, Git, SQL, RESTful, Jenkins, Docker, Kubernetes, Kafka, Python, PySpark, Spark, HDFS, Linux, HBase, Aerospike, AppDynamics, Splunk, CI/CD, Agile, Scrum, LEAN","java, spring framework, git, sql, restful, jenkins, docker, kubernetes, kafka, python, pyspark, spark, hdfs, linux, hbase, aerospike, appdynamics, splunk, cicd, agile, scrum, lean","aerospike, agile, appdynamics, cicd, docker, git, hbase, hdfs, java, jenkins, kafka, kubernetes, lean, linux, python, restful, scrum, spark, splunk, spring framework, sql"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Mesa, AZ",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3759707776,2023-12-17,Chandler,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto,
San
Francisco or Chicago
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Java, Bash, SQL, Git, Snowflake, Airflow, Kubernetes, Docker, Helm, PySpark, Spark, Kafka, Storm, SparkStreaming, DynamoDB, Machine Learning, Data Mining, Data Cleaning, Normalization, Modeling, Data Visualization, Data Pipelines, Data Processing, Data Infrastructure, Data Governance, Risk and Compliance, Data Engineering, DataOps, Agile, LLMs, NLP, Conversational AI, Recommender Systems, Distributed Systems, Microservices, ETL, Big Data","python, java, bash, sql, git, snowflake, airflow, kubernetes, docker, helm, pyspark, spark, kafka, storm, sparkstreaming, dynamodb, machine learning, data mining, data cleaning, normalization, modeling, data visualization, data pipelines, data processing, data infrastructure, data governance, risk and compliance, data engineering, dataops, agile, llms, nlp, conversational ai, recommender systems, distributed systems, microservices, etl, big data","agile, airflow, bash, big data, conversational ai, data cleaning, data engineering, data governance, data infrastructure, data mining, data processing, dataops, datapipeline, distributed systems, docker, dynamodb, etl, git, helm, java, kafka, kubernetes, llms, machine learning, microservices, modeling, nlp, normalization, python, recommender systems, risk and compliance, snowflake, spark, sparkstreaming, sql, storm, visualization"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Phoenix, AZ",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3759707775,2023-12-17,Chandler,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto,
San
Francisco or Chicago
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Azure, Pandas, Snowflake, Kafka, Docker, KubeFlow, Bash, Helm, Python, SQL, Airflow, Kubernetes, Java, Spark, Git, AWS, GCP, pySpark, ETL, Hadoop, NoSQL, DynamoDB, Machine learning, Data engineering, Data mining, Data modeling, Data governance, Data analysis, Data processing, Data visualization, Data pipelines, Data infrastructure, Data security, Data compliance, Cloud computing, Distributed systems, Microservices, Stream processing, NLP, Recommendation systems, LLMs","azure, pandas, snowflake, kafka, docker, kubeflow, bash, helm, python, sql, airflow, kubernetes, java, spark, git, aws, gcp, pyspark, etl, hadoop, nosql, dynamodb, machine learning, data engineering, data mining, data modeling, data governance, data analysis, data processing, data visualization, data pipelines, data infrastructure, data security, data compliance, cloud computing, distributed systems, microservices, stream processing, nlp, recommendation systems, llms","airflow, aws, azure, bash, cloud computing, data compliance, data engineering, data governance, data infrastructure, data mining, data processing, data security, dataanalytics, datamodeling, datapipeline, distributed systems, docker, dynamodb, etl, gcp, git, hadoop, helm, java, kafka, kubeflow, kubernetes, llms, machine learning, microservices, nlp, nosql, pandas, python, recommendation systems, snowflake, spark, sql, stream processing, visualization"
"Senior Analyst, Data Science",Whirlpool Corporation,"Benton Harbor, MI",https://www.linkedin.com/jobs/view/senior-analyst-data-science-at-whirlpool-corporation-3761474194,2023-12-17,Benton Harbor,United States,Mid senior,Hybrid,"Requisition ID:
59696
About Whirlpool Corporation
Whirlpool Corporation (NYSE: WHR) is committed to being the best global kitchen and laundry company, in constant pursuit of improving life at home. In an increasingly digital world, the company is driving purposeful innovation to meet the evolving needs of consumers through its iconic brand portfolio, including Whirlpool, KitchenAid, Maytag, Consul, Brastemp, Amana, Bauknecht, JennAir, Indesit, Yummly and InSinkErator. In 2022, the company reported approximately $20 billion in annual sales, 61,000 employees and 56 manufacturing and technology research centers. Additional information about the company can be found at WhirlpoolCorp.com.
This Role In Summary
Whirlpool is seeking a qualified candidate to fill a Senior Analyst, Data Scientist position within the Supply Chain Analytics team. This role will be located at our Riverview Campus, in Benton Harbor MI. Employees in this role can work a hybrid schedule of three core days in office and two days remote. This enables employees to come together to collaborate in-person and gives employees the added flexibility to work remotely.
This role is critical to the development of self-service analytics for the Integrated Supply Chain (ISC) organization so that all levels of the organization can quickly understand performance and drive intelligent business decisions using data. This role is responsible for providing insightful analytics through visualization, data modeling, and data preparation necessary to enable data analytics throughout the ISC organization. The ideal candidate will have strong data analytic abilities, exhibit excellent verbal and written communication skills, and have the ability to derive meaningful data-driven insights.
Primary
Your responsibilities will include
Data visualization. Communicates insights and findings through compelling data visualizations, dashboards, and reports. Present insights and findings to both technical and non-technical audiences.
Data reporting. Generates ad hoc and standard datasets or reports for end-users using system tools and databases or data warehouse queries and scripts.
Data analysis. Integrate data from multiple sources and analyze large and complex data sets using statistical methods to extract insights and identify patterns.
Project management. Manage data science projects from conception to delivery, ensuring timely and high-quality results. Communicate with business stakeholders on requirements and updates.
Stakeholder Collaboration. Collaborate with cross functional partners to understand key performance indicators (KPI), reporting requirements, and projects to improve data-driven decision-making.
Secondary
Data modeling. Support supply chain efforts to build predictive models to enable data driven decision making and drive business strategy.
Data architect. Designs data architectures and ETL workflows to support supply chain data and reporting requirements.
Minimum Requirements
Bachelor’s Degree
3+ years of experience in data visualization, data analytics, and/or data management
1+ year of experience with dashboard development in Tableau
1+ year of experience with relational databases
Preferred Skills And Experiences
Master’s degree in Business, Supply Chain, Engineering, Data Analytics, Mathematics, Statistics or Computer Science
Tableau Developer Certification
3+ years experience working in supply chain functions
Experience with programming languages, preferably SQL, Python and/or R
Experience with ETL tools, preferably Alteryx
Demonstrated strong analytical and problem-solving skills
Demonstrated strong communications skills to all levels of the organization
Work both independently and in a team environment
Excellent time management, prioritization abilities, and process management
Consistently deliver results through direct and indirect influence
RSRWH
Connect with us and learn more about Whirlpool Corporation
See what it's like to work at Whirlpool by visitingWhirlpool Careers. Additional information about the company can be found onFacebook,Twitter,LinkedIn,InstagramandYouTube.
At Whirlpool Corporation, we value and celebrate diversity. Whirlpool Corporation is committed to equal employment opportunity and prohibits any discrimination on the basis of race or ethnicity, religion, sex, pregnancy, gender expression or identity, sexual orientation, age, physical or mental disability, veteran status, or any other category protected by applicable law.
Show more
Show less","Data Visualization, Data Reporting, Data Analysis, Project Management, Stakeholder Collaboration, Data Modeling, Data Architect, Tableau, Relational Databases, SQL, Python, R, Alteryx, ETL, Analytical and ProblemSolving Skills, Communications Skills, Team Work, Time Management, Prioritization Abilities, Process Management, Datadriven Insights","data visualization, data reporting, data analysis, project management, stakeholder collaboration, data modeling, data architect, tableau, relational databases, sql, python, r, alteryx, etl, analytical and problemsolving skills, communications skills, team work, time management, prioritization abilities, process management, datadriven insights","alteryx, analytical and problemsolving skills, communications skills, data architect, data reporting, dataanalytics, datadriven insights, datamodeling, etl, prioritization abilities, process management, project management, python, r, relational databases, sql, stakeholder collaboration, tableau, team work, time management, visualization"
Senior Data Infrastructure & AV Engineer,NETbuilder Education,"West Midlands, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-infrastructure-av-engineer-at-netbuilder-education-3779680760,2023-12-17,West Midlands, United Kingdom,Mid senior,Onsite,"Company Description
Netbuilder Education is a specialist provider of EdTech and infrastructure services for schools and businesses in Worcestershire and the surrounding areas. Our one-stop-shop provides peace of mind through a wide range of services including unified communications, audio visual equipment, networking equipment, data and fibre cabling, and much more. Our team is passionate about delivering the highest quality of work and outstanding customer services to all our clients.
Role Description
Netbuilder Education is seeking a Senior Data Infrastructure & AV Engineer for a full-time position in West Midlands. The successful candidate will be responsible for day-to-day responsibility for data, VoIP, Audio-Visual network installations and maintenance for Education, Local Government and Small to Medium size Enterprises.
Installation, testing and maintenance of Cat5e, Cat6, Cat6a (STP and FTP) data cabling in accordance with all relevant industry standards. This includes installation of containment, data cabinets, cabinet upgrades, network switches, routers, WiFi access points and patch panels as appropriate.
Installation, termination, testing and maintenance of single-mode and multi-mode (OS1, OS2, OM3 and OM4) fibre optic cabling including containment and bonding.
Installation of Audio-Visual cabling and systems including but not limited Smart Screens, Projectors, HDMI, Audio amplifier systems, Access Control and CCTV etc.
To take ownership of potentially large and complex data installations including onsite supervision of cabling sub-contractors.
To always work within the company’s health and safety guidelines.
Knowledge and experience working on Construction sites under CDM requirements.
Qualifications
Must have at least 3 years’ experience within an appropriate discipline – preferably as an installation engineer within the telecoms or data areas.
Must have excellent knowledge and practical application of all copper and fibre installation and testing principles.
Be fully conversant with the setup and operation of Fluke Analysers and validation tools to ensure compliance and installation certification as well as for fault diagnosis.
Experience of working within Local Education and Local Government locations would be desirable but not mandatory.
Must have an excellent and demonstrable technical understanding of Data, Voice and AV systems particularly within an Educational/Local Government environment.
Must have attained the minimum qualification - Data Cabling City & Guilds 3667, Certified Network Cable Installer (CNCI) – Desirable.
Ability to read and interpret CAD or similar drawing packages to determine a co-ordinated plan of action.
Strong analytical and problem-solving skills with attention to detail
Excellent verbal and written communication skills and ability to work collaboratively with cross-functional teams.
Person Specification
Self-motivated individual with the ability and desire to act on one’s own initiative.
A natural planner who can break down complex tasks into achievable key component parts.
A passion for accuracy and quality in terms of infrastructure installation.
A completer-finisher who likes to see the project through to completion.
A person who is proud of their work and acts as a good ambassador for the company.
Holds a clean and full UK driving licence.
Must hold a current enhanced CRB/DBS certificate.
Must hold CSCS/ECS accreditation.
IOSH Working Safely – Desirable
PASMA/IPAF - Desirable
Certified Network Cable Installer (CNCI) - Desirable
UKATA (CAT A) Asbestos Awareness - Desirable
Show more
Show less","Data Cabling City & Guilds 3667, Certified Network Cable Installer (CNCI), Cat5e/6/6a (STP/FTP) copper cabling, Singlemode and multimode fiber optic cabling (OS1/OS2/OM3/OM4), AudioVisual (AV) systems and cabling, Smart Screens, Projectors, HDMI, Audio amplifier systems, Access Control, CCTV, Fluke Analysers, CAD or similar drawing packages, Troubleshooting, Analytical and problemsolving skills, Verbal and written communication skills, Crossfunctional teamwork, Selfmotivation and initiative, Planning and task management, Accuracy and quality in infrastructure installation, Project completion, Pride in work and ambassadorship","data cabling city guilds 3667, certified network cable installer cnci, cat5e66a stpftp copper cabling, singlemode and multimode fiber optic cabling os1os2om3om4, audiovisual av systems and cabling, smart screens, projectors, hdmi, audio amplifier systems, access control, cctv, fluke analysers, cad or similar drawing packages, troubleshooting, analytical and problemsolving skills, verbal and written communication skills, crossfunctional teamwork, selfmotivation and initiative, planning and task management, accuracy and quality in infrastructure installation, project completion, pride in work and ambassadorship","access control, accuracy and quality in infrastructure installation, analytical and problemsolving skills, audio amplifier systems, audiovisual av systems and cabling, cad or similar drawing packages, cat5e66a stpftp copper cabling, cctv, certified network cable installer cnci, crossfunctional teamwork, data cabling city guilds 3667, fluke analysers, hdmi, planning and task management, pride in work and ambassadorship, project completion, projectors, selfmotivation and initiative, singlemode and multimode fiber optic cabling os1os2om3om4, smart screens, troubleshooting, verbal and written communication skills"
Senior Data Insights Analyst,AtkinsRéalis,"West Midlands, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-insights-analyst-at-atkinsr%C3%A9alis-3779378306,2023-12-17,West Midlands, United Kingdom,Mid senior,Onsite,"You might know us for the great work AtkinsRéalis does across our wide variety of projects. We're proud to say it's thanks to our people's diversity of thought, expertise, and knowledge. When you join us, you'll be a part of this genuinely collaborative environment, where everyone's voice is valued and treated equally.
This role is within the Transportation Consultancy business which has an excellent record in engineering and delivering to a wide range of clients, and, particularly within the Transport Data Insights practice. Our clients include some of the largest public and private sector organisations in the UK, particularly in the Rail, Highways and Local Transport markets. We create a culture and environment to inspire our people, ensuring collaboration with our clients and continuing to shape the future of Transportation. We are passionate about the importance of Net Zero and the role that we must play in meeting the global requirements.
Read More About How You Can Thrive With Us.
The increased use of data within organisations, the disruption of the digital era and the increasing pace, volatility, variety, and scale of change is a reality. The need to make data more accessible, actionable, and easy to extract value from is growing as more of our clients look to unlock efficiencies and develop more effective business processes.
AtkinsRéalis is aware of this new paradigm and has invested in innovative digital developments to keep the leadership on Data & Technology; playing key roles in Transportation projects to upgrade existing transport networks, including major road schemes, high-speed rail, and extensive public transport enhancements. We are at the forefront of national and international policy making, supporting key decision makers at all levels.
We are also working closely with our clients to adapt to the evolving environment, providing remote working patterns that allow us to conciliate with our life outside of work.
We are currently looking for talented professionals who are experienced Data Insights Analysts to work alongside a team of Data Scientists, Engineers, and Consultants.
The purpose of this role is to help our clients understand and improve their business performance using data-driven approaches. Successful candidates will be providing expertise in the design and application of data analytics solutions for a variety of clients and sectors.
Our Team
You will be a part of a growing, multidisciplinary team who is well-established in the space of data consulting services. In your role you will:
Work with clients to integrate best practices into their processes and decision-making.
Handle, merge, and clean multi-source data so they can be further analysed.
Effectively apply a variety of data visualisation techniques to bring life to business problems.
Collaborate with teams of technical and non-technical users of various experience levels.
Contribute to decision-making that will steer to successful delivery of projects.
Be responsible for expanding your technical development through appropriate training and mentorship.
Participate in external activities related to your specialism such as client engagement, conferences, and research papers.
The Role Of The Data Insights Analyst
The role of the Data Insights Analyst is to support bid and live project teams, by developing and implementing appropriate methodologies for the tasks and ensuring quality and accuracy through technical reviews. The role holder will lead a team of analysts, must be able to communicate well with clients and accurately capture their requirements. The role holder must effectively work and communicate with the delivery team and work collaboratively to deliver in a fast-paced environment.
You will work on projects end-to-end, covering all aspects of multidisciplinary design and management from inception to options appraisal through to detailed scheme design and delivery support. You will be communicating with stakeholders at varying levels, shaping an approach for the deployment of analytics solutions in large organisations. This role requires an understanding of data and its complexities and the ability to create compelling, easily understandable insight from data.
The role is very hands-on and is often busy; time and self-management skills are essential.
Requirements
Qualifications:
Degree ideally in a numerate subject such as Computer Science, Mathematics, Statistics, Physics or Engineering preferred.
Equivalent experience in a similar role will also be considered in lieu of a degree.
Essential Criteria
Experience and ability to conduct/undertake some of the following:
engaging with client teams and understanding their needs.
gathering requirements with end-users to create solutions supported by costed delivery programmes.
work in an agile delivery environment, being flexible and take on a wide range of duties.
supporting the developing specifications and implementation of tools and technologies
working/collaboratively with multi-disciplinary teams and liaising with team members (data engineers, analysts, engineers) to deliver projects on time and to budget.
extract and manage data from different sources.
create Extract, Transform, Load (ETL) processes.
code in programming languages (Python or similar).
undertake database programming (SQL or similar).
work in an Information Technology environment (Azure or similar)
create interactive and attractive data visualisations (Microsoft Power BI or similar)
understand the governance of a project.
interpret outputs of analyses and make recommendations on findings of analyses.
apply critical and analytical thinking into problem solving.
work in a team or work as an individual as appropriate.
articulate complex solutions and effectively communicate them in writing, orally and visually using engaging techniques.
Desirable Criteria
Experience of:
Highways, Rail, and other transport sector sectors.
data modelling techniques.
traffic management or lifecycle planning tools.
statistical analysis
lifecycle assurance and quality management
creating tools and solutions accessible to clients.
Ability To
work with disparate data types (GIS, JSON, images, delimited text files etc.) and use of FME/Azure Data Factory as an ETL tool.
transform data, undertake analysis, and implement models with Python or R languages.
conduct statistical analyses with SPSS.
test systems, units, and their integration.
We would like applicants to demonstrate they are committed to continuous professional development and are engaged with the wider professional community in their discipline. This could be through membership or qualifications with professional bodies e.g., the British Computing Society, the OR Society, the Institute of Mathematics and its Applications, or the Royal Statistical Society.
Responsibilities
The Data Insights Analyst is responsible for:
leading a team of analysts and data engineers.
supporting the analysis and prioritisation of client requirements into solution options and delivery plans.
supporting the engagement with clients to understand and document their current needs and requirements.
supporting the delivery of projects to quality, time, and budget requirements
collaborating with technical leads/subject matter experts
using and enhancing existing toolsets to prepare, load and validate data.
analysing data outputs and presentation of findings.
supporting the development of both new and existing applications.
supporting bid and proposal work, including early engagement with clients.
The Individual
You will have a strong sense of data and analytics, and the analytical capacity to design, develop and deliver solutions to support the management of transportation infrastructure.
You will have strong communications and engagement skills with demonstrable experience of working with a range of client teams to understand their requirements and the environment in which they work to shape the solution options to address their key challenges.
You are a creative and critical thinker, eager to expand further your skills according to project needs, with the ability to innovate and passion for keeping abreast of developments and their applications, shaping deliveries within tightly constrained technical and business environments.
You are results-oriented with the ability to understand wider business strategies and problems and with the technical know-how to drive outcomes by drawing insight and visualising the 'so what' in data using the appropriate tools.
Additionally, You Will Have
Ability to develop solid working relationships with clients from a varied background and gain confidence and buy-in to proposed solutions.
Ability to work as a part of a team adding expertise and value to project deliverables as needed.
Consulting skills, knowledge, and experience of a variety of techniques and methods to capture, elaborate and understand client problems (structured thinking, effective report writing and presentations, and strong stakeholder engagement).
Good negotiation skills to influence business recommendations and formulate positive relationships with customers, strategic partners and colleges.
A high-level of self-initiation, organisation, enthusiasm, and eagerness to learn and mentor junior staff to develop the team further.
If you are interested in finding out more before applying, then please feel free to contact our recruitment lead who would be more than happy to talk to you.
Why work for AtkinsRéalis?
Our Transportation teams are reinventing transport for a challenging yet exciting future. We're creating new ideas and smart solutions for cities, people and the environment. You'll be working alongside some of the top people in our industry, solving challenges that bring together traditional engineering and new technologies. As part of our organisation, you'll be in a privileged position, delivering what's right for governments and communities. You'll be helping us to improve our leading environmental performance while enabling others to do the same.
Meeting Your Needs
To help you get the most out of life in and outside of work, we offer employees ""Total Reward."" It's a package that can be tailored to your changing lifestyle, career and personal needs. You'll enjoy benefits such as partner discounts, leadership training, professional, accreditations, and career planning for any stage of your career. Making sure you're supported is important to us. So, if you identify as having a disability, tell us ahead of your interview. And let's talk through any adjustments you might need.
Benefits
New name, new era. We are AtkinsRéalis. It will take some time for all our information to update – you can still refer to our fantastic benefits via the link below:
Your Reward at SNC-Lavalin (snclavalin.com).
More About Us
AtkinsRéalis, is a leading global consultancy with over 100 years of design and engineering expertise. With us, you'll be surrounded by the skills, knowledge, and support to help you succeed. Together, we can deliver projects which have a positive, sustainable impact on the world.
Check out some of our recent, most exciting projects.
Training
AtkinsRéalis is committed to developing its people both personally and professionally. Our colleagues have the advantage of access to a high ranging training portfolio and development activities designed to help make the best of individual’s abilities and talents. We also actively support staff in achieving corporate membership of relevant institutions.
Additional Information
This role may require security clearance and offers of employment will be dependent on obtaining the relevant level of clearance. If this is necessary, it will be discussed with you at interview. The vetting process is delivered by United Kingdom Security Vetting (UKSV) and may require candidates to provide proof of residency in the UK of 5 years or longer. If applying to this role please do not make reference to (in conversation) or include in your application or CV, details of any current or previously held security clearance.
We are committed to promoting a diverse and inclusive community - a place where we can all be ourselves, thrive and develop. To help embed inclusion for all, from day one, we offer a range of family friendly, inclusive employment policies, flexible working arrangements and employee networks to support staff from different backgrounds. As an Equal Opportunities Employer, we value applications from all backgrounds, cultures and ability.
Show more
Show less","Data Analytics, Data Extraction, Data Visualization, Data Management, Database Programming, SQL, Python, R, Azure Data Factory, FME, SPSS, Statistical Analysis, Lifecycle Planning Tools, GIS, JSON, Microsoft Power BI, Data Modelling, Agile Development, Traffic Management, Project Management, Requirements Gathering, Consulting, Problem Solving, Critical Thinking, Communication, Presentation, Negotiation, Stakeholder Engagement, Team Leadership, Mentoring, Microsoft Office Suite","data analytics, data extraction, data visualization, data management, database programming, sql, python, r, azure data factory, fme, spss, statistical analysis, lifecycle planning tools, gis, json, microsoft power bi, data modelling, agile development, traffic management, project management, requirements gathering, consulting, problem solving, critical thinking, communication, presentation, negotiation, stakeholder engagement, team leadership, mentoring, microsoft office suite","agile development, azure data factory, communication, consulting, critical thinking, data extraction, data management, data modelling, dataanalytics, database programming, fme, gis, json, lifecycle planning tools, mentoring, microsoft office suite, microsoft power bi, negotiation, presentation, problem solving, project management, python, r, requirements gathering, spss, sql, stakeholder engagement, statistical analysis, team leadership, traffic management, visualization"
Senior BI Developer / Data Engineer,Intec Select,"West Midlands, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-bi-developer-data-engineer-at-intec-select-3770489567,2023-12-17,West Midlands, United Kingdom,Mid senior,Hybrid,"A leading financial services corporation is currently recruiting a Senior BI Developer / Data Engineer with strong capabilities in SQL Server Backend 2016+, SSIS, SSRS and Azure to work alongside a number of data professionals within data driven organisation. The selected
Senior BI Developer / Data Engineer must be able to demonstrate recent project experience within on-premise and cloud environments to be eligible for this exciting opportunity.
Our client is looking to pay up to £67,000 + 15% bonus with an excellent benefits package to be based in Chatham, Wolverhampton or London on occasions.
This is a truly exciting position where you will be part of a team responsible for shaping our client data ecosystem transition into the latest cloud technologies and trends.
Core Responsibilities
Leading solutions for BI / Data engineering
Maintain the integrity of both the design and the data that is held within the architecture
Champion and educate people in the development and use of data engineering best practices
Support the Head of Data Engineering and lead by example
Contribute to the development of database management services and associated processes relating to the delivery of data solutions
Provide requirements analysis, documentation, development, delivery and maintenance of data platforms.
Develop database requirements in a structured and logical manner ensuring delivery is aligned with business prioritisation and best practice
Design and deliver performance enhancements, application migration processes and version upgrades across a pipeline of BI environments.
Provide support for the scoping and delivery of BI capability to internal users.
Experience Requirements
5 years Data Engineering / ETL development experience (must have)
Experience working within a regulated environment (must have)
5 years data design experience in an MI / BI / Analytics environment (Kimball, lake house, data lake) (must have)
Excellent Data Warehouse with substantial experience in extracting, reporting and manipulating data from a data warehouse environment (must have)
Significant technical skills such as Transact SQL language, relational database (must have).
Evidence of delivering complex data platforms and solutions (must have)
Experience with cloud data platforms (Microsoft Azure) (nice to have)
Microsoft SQL Server 2019 certification (nice to have)
£67,000/ 15% bonus / Flexible working / 28 Days Holiday / Medical Cover / Life Cover / 13% Pension / Flexible Benefits
Senior BI Developer / Data Engineer
Show more
Show less","SQL Server Backend, SSIS, SSRS, Azure, OnPremise environments, Cloud environments, Data Warehouse, Kimball, Lake house, Data lake, Transact SQL, Relational database, Microsoft Azure, SQL Server 2019","sql server backend, ssis, ssrs, azure, onpremise environments, cloud environments, data warehouse, kimball, lake house, data lake, transact sql, relational database, microsoft azure, sql server 2019","azure, cloud environments, data lake, datawarehouse, kimball, lake house, microsoft azure, onpremise environments, relational database, sql server 2019, sql server backend, ssis, ssrs, transact sql"
Senior Data Analyst,"Envestnet, Inc","Berwyn, PA",https://www.linkedin.com/jobs/view/senior-data-analyst-at-envestnet-inc-3782754867,2023-12-17,Pottstown,United States,Mid senior,Onsite,"Description
Envestnet is seeking a passionate and talented
Senior Data Analyst
to join our Product team. This position is expected to report to our Berwyn, PA office on a hybrid basis.
Envestnet is transforming the way financial advice and wellness are delivered. Our mission is to empower advisors and financial service providers with innovative technology, solutions, and intelligence to make financial wellness a reality for everyone.
Since our founding 20 years ago, we are fully vested in helping people live an intelligent financial life. If you love the idea of working in a Fintech company with the environment and excitement of a start-up where you are making everyday impact - then read on.
Job Summary
Envestnet is seeking a Senior Data Analyst to join the Wealth and Banking Analytics Product team. As a Senior Data Analyst on the team, you will work with data platform teams, clients, sales and account management, operations, engineering, and product teams across Data & Analytics and Envestnet to continually improve and innovate products for customers across our Enterprise, Asset Manager, RIA and Banking market segments.
As a Senior Data Analyst, you will be responsible for a set of data domains that include wealth portfolio management data, banking and consumer spend, financial planning and retirement, CRM, insurance and annuities, sustainable investing, trust, payroll, compliance, and healthcare to name a few. You will use your passion for data to analyze and prepare the data by adding business context in the form of logic, rules, algorithms, and models. You will work with Product Owners to understand strategic product requirements and turn them into implementable components in partnership with the Engineering teams..
Job Responsibilities
Document, build and maintain data and reporting products to power our three Product pillars of data/connectivity, analytics/intelligence, and experiences for our customers
Identify, define and document clear and complete reporting requirements with product owners and customers
Perform data analysis on multiple data sets to identify sources, verify data quality, and ensure delivery to customers
Partner with Engineering data delivery teams to ensure clear understanding of requirements and accurate sourcing of data
Develop an understanding of the end-to-end data pipelines from source to data warehouse to end users
Coordinate with and contribute to Data Governance and Data Quality management efforts to facilitate sourcing, definitions, and lineage
Write and execute SQL queries for ad-hoc reporting and analysis
Coordinate user acceptance testing with the Quality Management organization
Participate in proof of concept and rapid prototyping projects
Collaborate with product and engineering teams to advise on and build optimal data models, metrics, attributes, KPI’s, dashboards and visualizations
Pull, organize, and analyze relevant data from multiple sources, using tools such as SQL, dbt and PowerBI
Deliver data research results to technical and non-technical audiences.
Document data product functional and technical requirements and collaborate with Engineering to build and implement them.
Collaborate with architects and platform engineering teams to optimize queries for performance and efficiency
Mentor and help junior resources on best practices, code reviews and deployment processes.
Adherence to and application of Envestnet legal, compliance, risk, business continuity and administrative policy within the role and department(s) including the timely completion of training & awareness, affirmations and testing as requested.
As part of the responsibilities for this role, you will understand and readily support Envestnet's established corporate business practices, policies, internal controls and procedures designed to create value or minimize risk
Required Qualifications
5-7 years of experience as a data analyst, BI developer, data engineer, or scientist.
5-7 years of advanced SQL skills with experience using data warehouses
Experience building dashboards and data storytelling in a modern BI tool (Looker, PowerBI, Qlik, Tableau, etc.)
Confidence applying software engineering principals (testing, version control, code reviews, DevOps, CI/CD, etc.) to the analytical development workflow.
Excellent communication with technical and non-technical audiences.
5+ years working with and analyzing structured and unstructured data using SQL.
Advanced understanding of data visualization principles and best practices.
Experience communicating technical results to technical and non-technical audiences across all levels of an organization.
Excellent analytical and problem-solving skills, combined with strong business judgement and an ability to present analyses in a clear and compelling manner.
Strong understanding of data warehouse, relational and columnar databases.
Preferred Qualifications
Experience integrating, analyzing, and applying business rules to wealth management, financial, investment, and/or banking data
Experience working with dbt, Snowflake, APIs, React or similar technologies
AWS and Snowflake experience preferred
Knowledge of data modeling, ETL/ELT, and data pipeline development a plus
About Us
Envestnet is a leading independent provider of technology‐enabled investment and practice management solutions to financial advisors who are independent, as well as those who are associated with small or mid‐sized financial advisory firms and larger financial institutions. Envestnet's technology is focused on addressing financial advisors' front, middle, and back‐office needs while leveraging our platform to grow their businesses and expand client relationships.
We offer a highly competitive compensation and benefits package as well as the excitement, challenges, and rewards of a fast-growing, entrepreneurial company.
Why Choose Envestnet
Be a member of a leading financial services and products innovation company
Competitive Compensation/Total Reward Packages that include:
Health Benefits (Health/Dental/Vision)
Paid Time Off (PTO) & Volunteer Time Off (VTO)
401K – Company Match
Annual Bonus Incentives
Parental Stipend
Tuition Reimbursement
Student Debt Program
Charitable match
Wellness Program
Work on global projects with diverse, energetic, team members who respect each other and celebrate differences
Envestnet is an Equal Opportunity Employer.
Envestnet refers to the family of operating subsidiaries of the public holding company, Envestnet, Inc. (NYSE: ENV).
Show more
Show less","SQL, Data Warehousing, Dashboards, Data Visualization, Software Engineering, Data Storytelling, Data Analysis, Data Modeling, ETL/ELT, Data Pipelines, Data Governance, Data Quality Management, Testing, Version Control, Code Reviews, DevOps, CI/CD, AWS, Snowflake, APIs, React","sql, data warehousing, dashboards, data visualization, software engineering, data storytelling, data analysis, data modeling, etlelt, data pipelines, data governance, data quality management, testing, version control, code reviews, devops, cicd, aws, snowflake, apis, react","apis, aws, cicd, code reviews, dashboard, data governance, data quality management, data storytelling, dataanalytics, datamodeling, datapipeline, datawarehouse, devops, etlelt, react, snowflake, software engineering, sql, testing, version control, visualization"
Senior Data Analyst,Vanguard,"Malvern, PA",https://www.linkedin.com/jobs/view/senior-data-analyst-at-vanguard-3760646325,2023-12-17,Pottstown,United States,Mid senior,Onsite,"Analyzes data, assesses trends and develops actionable insights and recommendations in support of business objectives. Trains and develops junior analysts.
Description:
The Client Experience Analytics department is looking for a talented Senior Data Analyst to join our team! Aligned to the work of Vanguard's Institutional Investment Group, you will work closely with business partners to develop data-driven insights and recommendations to improve our clients' experience with Vanguard. This role requires a high level of technical expertise in data analysis and a strong business acumen.
Core Responsibilities:
Work closely with business partners to develop data-driven insights and recommendations to improve the client experience.
Develop and maintain dashboards and reports to track and monitor key performance indicators.
Collaborate with cross-functional teams to develop and implement solutions to improve the client experience.
Use statistical techniques to analyze large and complex datasets.
Analyze customer data to uncover trends and patterns that impact the client experience.
Collaborate with cross-functional teams to design and execute experiments that test hypotheses and measure the impact of new initiatives on customer experience and business performance.
Communicate insights and recommendations to stakeholders across the organization, including senior management, to inform decision-making.
Stay current with industry trends and emerging technologies related to data analytics, customer experience, and financial services.
Qualifications:
Undergraduate degree in an analytic/quantitative field such as: Data Science, Business Analytics, Applied Mathematics, Statistics, Economics or related quantitative field of study or equivalent combination of training and experience.
4-6 years of relevant work experience
Extraordinary analytical skills, coupled with the ability to form hypotheses and draw meaningful insights from disparate data sources. The ability to understand data, process it, extract value from it, visualize it and communicate insights to audiences of various backgrounds, including senior management.
Demonstrated track record of building strong relationships across technical and non-technical crew.
Strong command of advanced data analysis tools and software (Python, Tableau preferred), and querying languages (SQL)
Experience working within a cloud-based, big data environment (AWS preferred)
Highest standards of accuracy and precision; highly organized
Preferred Qualifications:
A graduate level degree in an analytic/quantitative field such as: Data Science, Business Analytics, Applied Mathematics, Statistics, Economics or related quantitative field of study
Experience with advanced statistical and machine-learning methods such as: regression models, propensity models, segmentation, optimization, experimental test design, time series analysis, cohort analysis, Bayesian methods, decision trees, text analysis, etc.
ent combination of training and experience.
Special Factors
Sponsorship
Vanguard is not offering visa sponsorship for this position.
About Vanguard
We are Vanguard. Together, we’re changing the way the world invests.
For us, investing doesn’t just end in value. It starts with values. Because when you invest with courage, when you invest with clarity, and when you invest with care, you can get so much more in return. We invest with purpose – and that’s how we’ve become a global market leader. Here, we grow by doing the right thing for the people we serve. And so can you.
We want to make success accessible to everyone. This is our opportunity. Let’s make it count.
Inclusion Statement
Vanguard’s continued commitment to diversity and inclusion is firmly rooted in our culture. Every decision we make to best serve our clients, crew (internally employees are referred to as crew), and communities is guided by one simple statement: “Do the right thing.”
We believe that a critical aspect of doing the right thing requires building diverse, inclusive, and highly effective teams of individuals who are as unique as the clients they serve. We empower our crew to contribute their distinct strengths to achieving Vanguard’s core purpose through our values.
When all crew members feel valued and included, our ability to collaborate and innovate is amplified, and we are united in delivering on Vanguard's core purpose.
Our core purpose: To take a stand for all investors, to treat them fairly, and to give them the best chance for investment success.
How We Work
Vanguard has implemented a hybrid working model for the majority of our crew members, designed to capture the benefits of enhanced flexibility while enabling in-person learning, collaboration, and connection. We believe our mission-driven and highly collaborative culture is a critical enabler to support long-term client outcomes and enrich the employee experience.
Show more
Show less","Data Analysis, DataDriven Insights, Dashboards, Reports, Performance Indicators, Statistical Techniques, Customer Data, Trends, Patterns, Experiments, Hypotheses, Business Performance, Communication, Stakeholder Engagement, DecisionMaking, Industry Trends, Emerging Technologies, Data Science, Business Analytics, Applied Mathematics, Statistics, Economics, Data Visualization, CloudBased Environments, Big Data, AWS, Accuracy, Precision, Organization, Regression Models, Propensity Models, Segmentation, Optimization, Experimental Test Design, Time Series Analysis, Cohort Analysis, Bayesian Methods, Decision Trees, Text Analysis","data analysis, datadriven insights, dashboards, reports, performance indicators, statistical techniques, customer data, trends, patterns, experiments, hypotheses, business performance, communication, stakeholder engagement, decisionmaking, industry trends, emerging technologies, data science, business analytics, applied mathematics, statistics, economics, data visualization, cloudbased environments, big data, aws, accuracy, precision, organization, regression models, propensity models, segmentation, optimization, experimental test design, time series analysis, cohort analysis, bayesian methods, decision trees, text analysis","accuracy, applied mathematics, aws, bayesian methods, big data, business analytics, business performance, cloudbased environments, cohort analysis, communication, customer data, dashboard, data science, dataanalytics, datadriven insights, decision trees, decisionmaking, economics, emerging technologies, experimental test design, experiments, hypotheses, industry trends, optimization, organization, patterns, performance indicators, precision, propensity models, regression models, reports, segmentation, stakeholder engagement, statistical techniques, statistics, text analysis, time series analysis, trends, visualization"
Lead Data Analyst,Vanguard,"Malvern, PA",https://www.linkedin.com/jobs/view/lead-data-analyst-at-vanguard-3765318763,2023-12-17,Pottstown,United States,Mid senior,Onsite,"The Wealth Management (WM) Analytics team is looking for an experienced analyst to join us on our analytic expedition. The WM Analytics team supports the Wealth Management business in all their business endeavors from Service Operations, Trusts, Self-Directed, Advised, and Family Office.
Partnership The ability to work in cross-functional teams is paramount as our team partners with a variety of other CDAO (Chief Data Analytics Office) teams (e.g., Marketing CXD, CAI) and other groups which support WM.
Analytic Work The type of work ranges from data reporting, dashboard development to advanced analytics.
Vertical Alignment This analyst will be aligned to a business function within the WM to help establish a deeper level of understanding of the day-to-day challenges facing the business. The candidate for this role will be responsible for proactively identifying opportunities within the business, contributing to its growth and success.
Horizontal Alignment The analyst will also have the responsibility of developing expertise in key subject areas of the WM business (e.g., Advice, Asset Consolidation, Demand & Capacity). The candidate will be considered a subject matter expert for our team, CDAO and the rest of the organization.
Core Responsibilities
Engages with internal partners to understand business strategy, questions and goals. Brings structure to business requests, translates requirements into an analytical project approach, and leads complex projects through completion. Delegates tasks and provides tactical and strategic guidance to peers. Serves as the analytics expert on cross-functional teams for large strategic initiatives.
Acquires and compiles structured and unstructured data and verifies its quality, accuracy and reasonableness.
Performs analyses of historical data to surface trends and insights using advanced analytical methods. Validates analytical techniques employed by other analysts.
Prepares and delivers expert level visualizations and internal presentations that translate analytic insights into tangible, actionable solutions for business partners to implement.
Develops, owns and manages recurring analytic or reporting processes.
Actively develops the analytics community at Vanguard by mentoring, coaching and connecting others with resources and training. Participates and presents during scheduled analytics seminars.
Participates in special projects and performs other duties as assigned.
Qualifications
Minimum of five years related work experience.
Undergraduate degree or equivalent combination of training and experience. Graduate degree preferred.
Strong command of advanced data analysis tools and software (Python, Tableau preferred), and querying languages (SQL)
Special Factors
Sponsorship
Vanguard is not offering visa sponsorship for this position.
About Vanguard
We are Vanguard. Together, we’re changing the way the world invests.
For us, investing doesn’t just end in value. It starts with values. Because when you invest with courage, when you invest with clarity, and when you invest with care, you can get so much more in return. We invest with purpose – and that’s how we’ve become a global market leader. Here, we grow by doing the right thing for the people we serve. And so can you.
We want to make success accessible to everyone. This is our opportunity. Let’s make it count.
Inclusion Statement
Vanguard’s continued commitment to diversity and inclusion is firmly rooted in our culture. Every decision we make to best serve our clients, crew (internally employees are referred to as crew), and communities is guided by one simple statement: “Do the right thing.”
We believe that a critical aspect of doing the right thing requires building diverse, inclusive, and highly effective teams of individuals who are as unique as the clients they serve. We empower our crew to contribute their distinct strengths to achieving Vanguard’s core purpose through our values.
When all crew members feel valued and included, our ability to collaborate and innovate is amplified, and we are united in delivering on Vanguard's core purpose.
Our core purpose: To take a stand for all investors, to treat them fairly, and to give them the best chance for investment success.
How We Work
Vanguard has implemented a hybrid working model for the majority of our crew members, designed to capture the benefits of enhanced flexibility while enabling in-person learning, collaboration, and connection. We believe our mission-driven and highly collaborative culture is a critical enabler to support long-term client outcomes and enrich the employee experience.
Show more
Show less","Python, Tableau, SQL, Data analysis, Data reporting, Dashboard development, Advanced analytics, Data visualization, Internal presentations, Analytic insights, Analytical project approach, Complex projects, Delegating tasks, Tactical guidance, Strategic guidance, Analytics expertise, Crossfunctional teams, Structured data, Unstructured data, Data quality, Data accuracy, Data reasonableness, Historical data, Analytical methods, Analytic techniques, Advanced analytical methods, Visualization, Tangible solutions, Actionable solutions, Recurring analytic processes, Reporting processes, Analytics community, Mentoring, Coaching, Connecting others, Resources, Training, Special projects","python, tableau, sql, data analysis, data reporting, dashboard development, advanced analytics, data visualization, internal presentations, analytic insights, analytical project approach, complex projects, delegating tasks, tactical guidance, strategic guidance, analytics expertise, crossfunctional teams, structured data, unstructured data, data quality, data accuracy, data reasonableness, historical data, analytical methods, analytic techniques, advanced analytical methods, visualization, tangible solutions, actionable solutions, recurring analytic processes, reporting processes, analytics community, mentoring, coaching, connecting others, resources, training, special projects","actionable solutions, advanced analytical methods, advanced analytics, analytic insights, analytic techniques, analytical methods, analytical project approach, analytics community, analytics expertise, coaching, complex projects, connecting others, crossfunctional teams, dashboard development, data accuracy, data quality, data reasonableness, data reporting, dataanalytics, delegating tasks, historical data, internal presentations, mentoring, python, recurring analytic processes, reporting processes, resources, special projects, sql, strategic guidance, structured data, tableau, tactical guidance, tangible solutions, training, unstructured data, visualization"
Lead Data Analyst,Vanguard,"Malvern, PA",https://www.linkedin.com/jobs/view/lead-data-analyst-at-vanguard-3777192501,2023-12-17,Pottstown,United States,Mid senior,Onsite,"In partnership with the Financial Advisor Services Organization (FAS), Vanguard's Chief Data and Analytics Office is embarking on a critical transformation to reimagine how data and analytics drive Vanguard FAS's vision of being the Advisor to the Advisor.
Vanguard's FAS business provides services and investment products, such as exchange-traded funds (ETFs), to advisor clients, including broker-dealers, registered investment advisors (RIAs), and bank wealth managers.
Our vision is to deliver data-driven solutions and insights that inform every decision and enable us to know our clients and anticipate their needs - to provide the right products and services at the right time. Data and analytics are at the heart of activating this vision, enabling us to enhance the advisor and distribution experience, streamline operations, and expand our suite of products and services.
This role will provide strategic support to elevate, expand, and optimize our advisor experience leveraging data and analytics capabilities for our Sales Division – enabling us to increase the depth and breadth of actionable insights delivered. You will work closely with business partners to develop data-driven insights and recommendations to improve our clients' experience with Vanguard. This role requires a high level of technical expertise in data analysis and a strong business acumen.
Core Responsibilities
Engages with internal partners to understand business strategy, questions and goals. Brings structure to business requests, translates requirements into an analytical project approach, and leads complex projects through completion. Delegates tasks and provides tactical and strategic guidance to peers. Serves as the analytics expert on cross-functional teams for large strategic initiatives.
Acquires and compiles structured and unstructured data and verifies its quality, accuracy and reasonableness.
Performs analyses of historical data to surface trends and insights using advanced analytical methods. Validates analytical techniques employed by other analysts.
Prepares and delivers expert level visualizations and internal presentations that translate analytic insights into tangible, actionable solutions for business partners to implement.
Develops, owns and manages recurring analytic or reporting processes.
Actively develops the analytics community at Vanguard by mentoring, coaching and connecting others with resources and training. Participates and presents during scheduled analytics seminars.
Participates in special projects and performs other duties as assigned.
Qualifications
Minimum of five years related work experience.
Undergraduate degree or equivalent combination of training and experience. Graduate degree preferred.
Strong command of advanced data analysis tools and software (Python, Tableau preferred), and querying languages (SQL)
Experience working within a cloud-based, big data environment (AWS preferred)
Special Factors
Sponsorship
Vanguard is not offering visa sponsorship for this position.
About Vanguard
We are Vanguard. Together, we’re changing the way the world invests.
For us, investing doesn’t just end in value. It starts with values. Because when you invest with courage, when you invest with clarity, and when you invest with care, you can get so much more in return. We invest with purpose – and that’s how we’ve become a global market leader. Here, we grow by doing the right thing for the people we serve. And so can you.
We want to make success accessible to everyone. This is our opportunity. Let’s make it count.
Inclusion Statement
Vanguard’s continued commitment to diversity and inclusion is firmly rooted in our culture. Every decision we make to best serve our clients, crew (internally employees are referred to as crew), and communities is guided by one simple statement: “Do the right thing.”
We believe that a critical aspect of doing the right thing requires building diverse, inclusive, and highly effective teams of individuals who are as unique as the clients they serve. We empower our crew to contribute their distinct strengths to achieving Vanguard’s core purpose through our values.
When all crew members feel valued and included, our ability to collaborate and innovate is amplified, and we are united in delivering on Vanguard's core purpose.
Our core purpose: To take a stand for all investors, to treat them fairly, and to give them the best chance for investment success.
How We Work
Vanguard has implemented a hybrid working model for the majority of our crew members, designed to capture the benefits of enhanced flexibility while enabling in-person learning, collaboration, and connection. We believe our mission-driven and highly collaborative culture is a critical enabler to support long-term client outcomes and enrich the employee experience.
Show more
Show less","Python, Tableau, SQL, AWS, Data analysis, Data visualization, Data mining, Business intelligence, Cloud computing, Big data, Machine learning, Artificial intelligence, Datadriven decision making, Data governance, Data management, Data quality, Data integration, Data architecture, Data warehousing, Data lake, Data pipeline, Data security, Data privacy, Data ethics, Data science, Data engineering, Data analytics, Data visualization tools","python, tableau, sql, aws, data analysis, data visualization, data mining, business intelligence, cloud computing, big data, machine learning, artificial intelligence, datadriven decision making, data governance, data management, data quality, data integration, data architecture, data warehousing, data lake, data pipeline, data security, data privacy, data ethics, data science, data engineering, data analytics, data visualization tools","artificial intelligence, aws, big data, business intelligence, cloud computing, data architecture, data engineering, data ethics, data governance, data integration, data lake, data management, data mining, data pipeline, data privacy, data quality, data science, data security, data visualization tools, dataanalytics, datadriven decision making, datawarehouse, machine learning, python, sql, tableau, visualization"
Master Data Functional Analyst,Dorman Products,"Colmar, PA",https://www.linkedin.com/jobs/view/master-data-functional-analyst-at-dorman-products-3751614233,2023-12-17,Pottstown,United States,Mid senior,Hybrid,"Dorman Products, a nearly $2B global supplier of aftermarket motor vehicle parts with 3,500+ Contributors and 28 sites across the globe, is seeking a Master Data Functional Analyst to join our growing team! This role will leverage data analysis and development skills to identify opportunity areas for expanded master data process and system efficiencies. This role leads efforts to define, coordinate, and oversee master data process improvement and strategic capabilities development, implementing new systems to support the cross-functional business. As the Master Data Functional Analyst at Dorman, you will support internal and external customers with broad base knowledge of all system impacts and processes and collaborate with both Business Analysts and the IT technical team to ensure the overall solutions are well designed, planned, tested, and executed. If you are a result-oriented individual with the ability to influence all levels of an organization with a strong process and data background, we want to hear from you!
This is a hybrid role with the expectation to come into our corporate office in suburban Philadelphia (Colmar, PA) roughly twice a week.
Primary Duties / Key Accountabilities
Applies significant knowledge of industry trends and developments to improve product data systems.
Translates identified business challenges or opportunities into actionable project efforts with defined scope and goals.
Creates visual dashboards and metrics that foster a shared understanding of business challenges or opportunities.
Represents Master Data Office and leads best practice solutions for cross functional groups.
Easily recognizes system deficiencies and implements effective solutions including researching and developing new system capabilities as needed.
Consistently delivers high-quality systems/solutions by maintaining Master Data Office system data architecture.
Ensures the quality and timeliness of all project deliverables.
Develops, monitors, and manages project metrics including benefits capture.
Ability to multitask and deliver on several simultaneous initiatives.
Self-motivated, independent contributor with the ability to learn new business areas, tools and concepts quickly.
Ability to effectively communicate the practical results of various analyses to business stakeholders using illustrations and practical language.
Works with Master Data Office Leadership to improve overall data delivery and support processes.
Ability to think strategically and analyze trends, situations, and industry specific data to identify business and technical solution opportunities.
Assessing tests and implementing new or upgraded software and assisting with strategic decisions on new systems.
Maintains alignment with business process strategies through interaction with functional business process owners.
Troubleshooting the reporting database environment and reports and provide technical solutions.
Oversight, strategic development and technical support of end-to-end product data processes related to product launch and ongoing maintenance.
Performs data analysis of key problem areas to assist in root cause analysis.
Supports Acquisition team on ensuring that current system and processes can account for new businesses.
Qualifications / Education / Experience
B.S. degree in IT, Business Management, or equivalent experience
7+ years of relative work experience
SAP and Winshuttle - strongly desired
Proven working experience as a data analyst or business data analyst
Experience with managing large volumes of data and ensuring quality of data
Experience managing and developing new system capabilities as needed
Experience leading successful cross functional projects
Strong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy
Knowledgeable of master data management strategies and best practices
Strong knowledge of and experience with reporting packages (Business Objects etc) and databases (SQL etc)
Excellent verbal and written communications skills with an ability to present and discuss technical information in a way that establishes rapport, persuades others and gains understanding
#DormanProducts
Dorman Products is an equal opportunity employer; we value a combination of ideas, perspectives, and cultures at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sex, gender identity, sexual orientation, age, marital status, veteran status, or disability status. EEO/AA Employer M/F/D/V. www.DormanProducts.com
Show more
Show less","SAP, Winshuttle, Business Objects, SQL, Master data management, Data analysis, System capabilities management, Crossfunctional project management, Analytical skills, Data quality management, Reporting packages, Databases","sap, winshuttle, business objects, sql, master data management, data analysis, system capabilities management, crossfunctional project management, analytical skills, data quality management, reporting packages, databases","analytical skills, business objects, crossfunctional project management, data quality management, dataanalytics, databases, master data management, reporting packages, sap, sql, system capabilities management, winshuttle"
(Sr.) Lead Data Architect,PJM Interconnection,"Audubon, PA",https://www.linkedin.com/jobs/view/sr-lead-data-architect-at-pjm-interconnection-3577415598,2023-12-17,Pottstown,United States,Mid senior,Hybrid,"Flexible Work Arrangements:
Hybrid
The (Sr.) Lead Data Architect is responsible for working with PJM business and technical stakeholders to define, collaborate, implement, deliver and maintain the architecture for PJM's data initiatives and projects to meet strategic directions and intended business benefit. The data architect drives the architecture discovery, analysis and overall execution of the assigned projects and initiatives. The data architect must possess sufficient knowledge and experience to understand the modelling practices in different PJM systems and drive success of PJM's data management initiatives and projects.
The data architect is key member of data architecture team and will play a peer role in engaging with the data governance lead and business data Stewards to provide input regarding technology opportunities and implications for data governance. Additionally, support the business in data governance change management across the enterprise by leading and coordinating analysis engaging the appropriate departments.
Essential Functions
Build and develop strong relationships with clients and data stakeholders by providing superior customer support as demonstrated by clearly owning, resolving and communicating issues and problems, and being responsive to needs, requirements, and deadlines
Cultivate visibility and respect from all levels of the organization; must have credibility and track record of success in the business
Communicate effectively with people at all organizational levels, from clerical positions to senior management team
Prioritize work to produce maximum results in order to meet short deadlines and deliverables
Collaborate with information governance board members to define, receive and understand policy directives. Support and inform data governance practices and assist in consolidated reporting into the data governance board
Independently schedule, organize and facilitate (run) meetings with clients, technology SMEs, business SMEs, data owners and other internal business stakeholders
Develop and maintain high quality documentation, such as requirements documents, data flow diagrams and other project / initiative documentation as needed
Apply facilitation and conflict resolution skills to assist in resolving the different points of view and requirements that invariably collide during cross-team and cross organization efforts
Assist data architecture team in developing work plans, including scope, milestones, schedule, releases, resources and deliverables
Gain and maintain an in-depth understanding of how master and transactional data is used within business processes and its impact on desired business process outcomes and on PJM members and stakeholders
Define, implement, and maintain data architecture framework, policies and data standards for PJM business units and systems, including security, reference data, metadata, and master data
Understand and apply the security, privacy and quality requirements for critical data entities
Demonstrate and apply a deep and comprehensive understanding of the importance and impact of data (or bad data) on the ability of the business to meet its own objectives
Apply data architecture techniques in the context of data management initiatives and projects coordinated by the team to drive success of PJM’s data management initiatives and projects
Defining data flows, i.e., which parts of the organization generate data, which require data to function, how data flows are managed, and how data changes in transition
Build algorithms and prototypes to support data integration and other data related functions
Identify opportunities for data acquisition and determine data update velocity
Collaborating with other teams within the organization to devise and implement data strategies, build models, and assess business partners needs and goals
Creating patterns that data engineers, developers and others can follow
Required
Characteristics & Qualifications:
BS degree in Computer Science, Management Information Systems, Computer Engineering or equivalent work experience 5+ years of experience in a data engineer, data modeler or data architect role
At least 5 years of experience with of SQL and data analysis
Experience in data architecture, engineering and cloud data architecture and processing services (example - Azure ,Google or AWS Cloud Platforms)
At least 5 years of experience Hands on Experience in Data Warehousing & ETL tools
At least 5 years of experience in Big Data technologies (example - Hadoop, Spark, Databricks, Snowflake), Logical Data Warehouse (LDW), Data Fabric
At least 5 years of experience building and optimizing 'big data' data pipelines, architectures, and data sets
At least 5 years of experience with Programming e.g. Python, PySpark, R, Oracle PL/ SQL
At least 5 years of experience with written, and verbal communication skills
Experience with conceptual and logical data modeling including developing and maintaining data model diagrams (ERD), facilitating structured data modeling meetings
Experience designing and building business intelligence architectures including data marts, data warehouses, and reporting and analysis applications
Experience with SQL Tools, such as SQL Navigator, TOAD, SQL*Plus etc
Experience in developing applications using Oracle PL/SQL
Experience with DBMS concepts (including SQL queries, tables, views, DB links, etc.)
Preferred
MS degree in Computer Science, Computer Engineering, Management Information Systems or equivalent work experience
5+ years of experience as a data architect with an additional 5+ years of experience as a data engineer, data modeler or other related data field
At least 5 years of experience Performance/security-oriented data modeling and data design, including partitions, different type of indexes, views, and most effective data architecture management practices on-premises, edge, and cloud
0-5 years of experience with Azure Synapse Analytics, Azure DevOps
0-5 years of experience with Purview, Snowflake, Alteryx, Qlik, or Tableau
0-5 years of experience working with Master Data Management systems
At least 5 years of experience with data governance and data structures
At least 5 years of experience with support and data modelling tools including but not limited to: Oracle Enterprise Manager (OEM), SQL*Plus, Toad
0-5 years of experience Experience in writing policies, standards and procedures
Experience with PJM operations, markets, and planning functions
Experience with PJM operations, markets, and planning functions
Show more
Show less","SQL, Data modeling, Data architecture, Data governance, Data warehousing, ETL tools, Big data, Hadoop, Spark, Databricks, Snowflake, Logical Data Warehouse, Data Fabric, Data pipelines, Python, PySpark, R, Oracle PL/SQL, SQL Navigator, TOAD, SQL*Plus, Performance/securityoriented data modeling, Data design, Oracle Enterprise Manager, Toad, Azure Synapse Analytics, Azure DevOps, Purview, Alteryx, Qlik, Tableau, Master Data Management systems, PJM operations, PJM markets, PJM planning functions","sql, data modeling, data architecture, data governance, data warehousing, etl tools, big data, hadoop, spark, databricks, snowflake, logical data warehouse, data fabric, data pipelines, python, pyspark, r, oracle plsql, sql navigator, toad, sqlplus, performancesecurityoriented data modeling, data design, oracle enterprise manager, toad, azure synapse analytics, azure devops, purview, alteryx, qlik, tableau, master data management systems, pjm operations, pjm markets, pjm planning functions","alteryx, azure devops, azure synapse analytics, big data, data architecture, data design, data fabric, data governance, databricks, datamodeling, datapipeline, datawarehouse, etl tools, hadoop, logical data warehouse, master data management systems, oracle enterprise manager, oracle plsql, performancesecurityoriented data modeling, pjm markets, pjm operations, pjm planning functions, purview, python, qlik, r, snowflake, spark, sql, sql navigator, sqlplus, tableau, toad"
Clinical Data Analyst Manager (US),CluePoints,"King of Prussia, PA",https://www.linkedin.com/jobs/view/clinical-data-analyst-manager-us-at-cluepoints-3782249622,2023-12-17,Pottstown,United States,Mid senior,Hybrid,"Summary
CluePoints is an innovative, fast-growing company and the premier provider of Risk-Based Monitoring and Data Quality Oversight solutions to clinical research organizations. Our products enable both risk planning and operational risk monitoring and utilize advanced statistical algorithms to determine the quality of clinical trial data both during and following study execution.
CluePoints is seeking outstanding candidates with strong data analytics aptitude and a passion for delivery excellence, to help drive customer success and broad adoption of the CluePoints solution.
As Manager, Clinical Data Analyst, you will join CluePoints’ Operations team and report to the Vice President, Data Services.
Purpose and Goals
Play a key role in data analyst attraction, retention and motivation
Assist in developing individualized growth plans for the team
Work with extended Operations team to identify areas of strength and improvement
Assist the VP of Data Services in directing the team on the overall vision and plans
Focus on simplicity and develop the team’s critical thinking abilities
Drive standardization and simplification throughout the analyst team
Work with the HR group to deliver a people strategy that enables analysts to thrive in their role and to meet CluePoints’ values and objectives
Responsibilities
Provide coaching and line management to data analysts as a direct line manager
Support team with workload management and general support
Identify areas of growth for the team and communicate common areas to the VP of Data Services to develop plans for strengthening the team
Develop an optimal team culture focusing on the goals of the organization while developing the individuals
Support the VP of Data Services in initiatives and strategy
Implement guidance and best practices in conjunction with the vision of the VP of Data Services
Ensure the team is delivering successfully with high customer satisfaction level
Drive innovative behaviours and encourage analysts to add customer value through continuous improvement
Work with the team to assist individuals in setting goals
Provide feedback in a motivational way to further leverage what is working well and develop plans to address any challenges
Assist in recruitment of team including resume review and interviewing of candidates
Ensure good communication and knowledge sharing across the Analyst team(s)
A reasonable understanding of analyst day to day job
Assist in resourcing and staffing of analysts on projects both externally with customers and internally with initiatives
Further develop skills needed within analyst positions such as customer communication and presentation, analytical development, programming skills and most importantly critical thinking
Requirements
3-10 + years of experience with a leadership role in Data Management or similar discipline
3+ years of experience in managing, coaching and team building
Demonstrated ability to lead the team by positive influence and motivation
Ability to travel occasionally
Experience working in a clinical research organization, with knowledge and understanding of the regulations and standards related to clinical trial conduct and clinical data management
College degree, preferably in a scientific or technical discipline
Some background in statistics and/or programming with respect to clinical data
Skills Needed
Experience with people management
Experience in coaching, influencing and building strong working relationships
Ability to successfully resolve conflict / de-escalate issues and to coach team members on how to successfully de-escalate issues (internal and client)
Ability to establish professional credibility quickly with colleagues and to gain the confidence and trust of others through honesty, integrity, and authenticity
Open-minded and collaborative when looking for solutions
Capable of effectively delegating and monitoring and adjusting workload as necessary
Effective English communication skills, including verbal, written and presentation skills
Experience working in a team environment under time and resource constraints
A can-do attitude, flexible and adaptable to changing priorities in a fast-growing organization
Ability to both set goals for the team and ensure successful completion
Aptitude for data analytics and basic statistical concepts
What We Offer
Flexibility is part of our DNA and you’ll find at CluePoints that you can work 100% from home or chose to come to the office any day
If you choose to come to the office, you’ll discover a fun atmosphere with a kicker, PS4, arcade machine, nerf battles, and our newly added dartboard
If you feel like a change of scenery, we have recently added the option to work at co-working spaces throughout Belgium.
Many activities are organized during the year, such as soccer, team lunches, happy hours, LAN parties, and board games nights! We also have running groups, language lessons, and much more!
At CluePoints, learning, training and personal growth are part of the game: you’ll have access to online training materials, certifications sponsored by the company, personal growth plans, and career paths to explore new opportunities.
You’ll work with a fast-growing, multi-disciplinary and international team representing over 20 different nationalities, in an English-speaking working environment.
You’ll have a challenging and rewarding job in an ambitious and fast-paced technology scale-up that has received many national and international awards including “Scale-Up of the year 2019”!
Last but not least, we offer competitive salary and benefits.
How To Apply
Please simply click the button on this page!
Show more
Show less","Data analytics, Statistical algorithms, Clinical data management, Data visualization, Data interpretation, Team management, Coaching, Leadership, Customer service, Communication, Presentation, Conflict resolution, Problemsolving, Critical thinking, Analytical development, Programming, Flexibility, Adaptability","data analytics, statistical algorithms, clinical data management, data visualization, data interpretation, team management, coaching, leadership, customer service, communication, presentation, conflict resolution, problemsolving, critical thinking, analytical development, programming, flexibility, adaptability","adaptability, analytical development, clinical data management, coaching, communication, conflict resolution, critical thinking, customer service, data interpretation, dataanalytics, flexibility, leadership, presentation, problemsolving, programming, statistical algorithms, team management, visualization"
"Staff Data Solutions Engineer- Denver, CO",findhelp,"Denver, CO",https://www.linkedin.com/jobs/view/staff-data-solutions-engineer-denver-co-at-findhelp-3748647935,2023-12-17,Colorado,United States,Associate,Onsite,"We’re changing the way people connect to social care.
At findhelp, we’ve built a comprehensive platform of products and services that make it easy for you to connect people to resources, follow them on their journey, and track your impact in a fast and reliable way. Our industry-leading social care network includes more than half a million local, state, and national programs that serve every ZIP Code in the country, from rural areas to major metropolitan centers.
Findhelp is headquartered in Austin, Texas and has been enabling healthcare, government, education, and other organizations to connect people with the social care resources that serve them, with privacy and security, since 2010.
As a mission driven organization, we are focused on creating a positive impact by connecting people in need to the programs that serve them with dignity and ease. Powered by our proprietary technology that enables people to find the resources available in their area, we have helped millions of Seekers find food, health, housing and employment programs.
Job Overview:
We are seeking a highly skilled Staff Data Solution Engineer to join our team. As a Staff Data Solution Engineer, you will coordinate and contribute to the design, development, and implementation of data solutions for our customers and team. You will be responsible for working closely with our product team and customers to understand the data needs of today and tomorrow. This role offers an exciting opportunity to work on diverse projects, solve complex problems, and contribute to the success of our data-driven initiatives.
Responsibilities and Duties:
Client Collaboration: Engage with clients to understand their data requirements and challenges. Serve as a strong technical counterpart and trusted advisor to customer success and government relations teams.
Data Solution Design: Translate business requirements to analytical dashboard designs and operational data delivery solutions that meet the needs of a diverse and growing user audience.
Data Management: Leverage knowledge of relational databases to analyze and optimize schemas, queries, and data movement (ETL).
Problem Solving: Tackle new and complex data-related problems with creativity and a structured approach. Identify opportunities for simplification, process improvement and efficiency gains.
Continuous Learning: Stay up-to-date with industry trends, emerging technologies, and best practices in data engineering, data modeling, and use of AI.
Architecture analysis: Understand data from source to target and inform design, considering optimal methods for data capture, logging, transformation, warehousing, and delivery.
Qualifications:
Bachelor’s degree in Computer Science, Engineering, or related field ( or proven experience (7+ years) in data engineering, dashboard development, or a related role)
Experience with data visualization tools to develop dashboards and reports.
Proficiency in database systems such as MySQL and BigQuery
Strong SQL programming skills with the ability to write complex queries and optimize database performance.
Excellent problem-solving skills and the ability to work on new and challenging data problems.
Exceptional communication and customer-facing skills, with the ability to explain technical concepts to non-technical audiences.
Preferred: Experience with data management, ETL processes, and data movement techniques in Cloud infrastructure.
Preferred: Experience with Python for data manipulation, scripting, or automation.
$89,280 - $133,920 a year
The compensation for this position will be based on a candidate’s job-related skills, experience, education or training, and location.
This job is based on-site in Austin, TX, Denver, CO, or Madison, WI with the opportunity to work remotely on Fridays.
Perks at findhelp
401k & stock options
Free food and onsite gym at our Austin HQ
Paid parental leave
Competitive PTO & 10 paid holidays
Health, dental, and vision insurance
Pet-friendly office with attached dog park at our Austin HQ
24/7 access to telemedicine and counseling
Book Purchasing Program
We’re building a diverse, inclusive team
You’re welcome here. We want everyone to be able to easily connect to the help they need, and we want our teams to reflect and represent our communities. It is our policy to recruit, hire, train, and promote individuals, as well as administer any and all Company policies, without regard to age, color, disability, ethnicity, family or marital status, gender identity or expression, language, national origin or ancestry, physical and mental ability, political affiliation, race, religion, creed, sexual orientation, socio-economic status, veteran status, or any other protected class, in accordance with applicable laws . Accommodations are available for applicants with disabilities.
Here are some of the ways we support our staff:
Culture Committee
Leadership Development Training
Paid Volunteering Time
Show more
Show less","Data engineering, Dashboard development, Data visualization, SQL, MySQL, BigQuery, Cloud infrastructure, Python, ETL processes, Data movement techniques, Data management","data engineering, dashboard development, data visualization, sql, mysql, bigquery, cloud infrastructure, python, etl processes, data movement techniques, data management","bigquery, cloud infrastructure, dashboard development, data engineering, data management, data movement techniques, etl, mysql, python, sql, visualization"
Data Center Building Operations Engineer,JLL,"Aurora, CO",https://www.linkedin.com/jobs/view/data-center-building-operations-engineer-at-jll-3738035310,2023-12-17,Colorado,United States,Associate,Onsite,"A Data Center Building Operations Engineer will be responsible to operate all building systems and equipment safely and efficiently, troubleshoot issues, and assist in the hands-on maintenance of those building systems.
ESSENTIAL DUTIES & RESPONSIBILITIES include, but not limited to the following:
Operate systems in a safe and efficient manner in accordance with government regulations, company policies and standard operating procedures.
Perform the installation, maintenance, repair and operation of mechanical, electrical and power generation equipment and systems to support the critical environment.
Quickly learn building controls, electrical and UPS systems, utilizing on the job training, operations manuals and documentation.
Perform predictive, planned, preventative and routine maintenance as well as equipment rounds, and service requests as directed. Perform maintenance in accordance with the JLL Preventative Maintenance Program. Assist with the development and improvement of preventative maintenance programs, schedules, work instructions, SOP’s and operational procedures.
Inspect and repair pumps, fans, valves, and motors ensuring proper operation of the equipment and systems.
Perform visual and operational inspection of associated equipment and interpret specifications, blueprints and job standards to perform assigned duties.
Determine and/or assist with analyzing and resolving work problems and achieving work goals
Maintain time and production records and customer service requests.
Must be comfortable and flexible enough to work with a wide-ranging and changing scope of responsibilities while adhering to the constraints of procedures put in place to minimize the risk for human error.
Team player with the desire to foster teamwork to help create a positive working environment.
Monitor operation and maintain refrigeration, water cooling and air conditioning equipment, boilers, heating, ventilating and hot water equipment, pumps, valves, piping and filters, other mechanical and electrical equipment
Perform and/or complete all tenant service requests accurately and expediently.
Complies with all policies and procedures established for the building, including safe storage, usage, and disposal of hazardous materials while maintaining a clean and safe workplace
Periodically acts as the on-call engineer as requested by supervisor.
Actively participate in emergency response procedures, technical and safety training programs.
Perform all work using the proper safety equipment and in a safe manner.
Performs additional job duties as requested.
The Data Center Building Operations Engineer will also assist with implementation and compliance with JLL’s Key Performance Indicators (KPI’s).
Iii. Minimum Requirements
Must have 4+ years of hands-on experience in a data center/critical facility, including UPS Systems, emergency generators, switch-gear, and mechanical cooling systems.
High School diploma or GED equivalent is required. Training and certification from an accredited trade school or Union apprentice program in HVAC, electronics, electrical is preferred.
State and local licensures, where required.
Certification as a Universal Technician for CFC’s (or within 90 days of employment), if required by local jurisdiction.
Should be knowledgeable in the safe and proper use of the following tools: ladders, lifts, basic hand tools, voltmeter, manual drain auger, safety goggles, ear protection, and fire extinguisher.
Working knowledge of computer applications including Word and Excel, and business applications, such as CMMS and Change Management is preferred.
Physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this job. The employee frequently is required to stand; walk; reach with hands and arms; climb or balance; and stoop, kneel, crouch, or crawl. The employee must regularly lift and/or move up to 10 pounds, frequently lift and/or move up to 25 pounds, and occasionally lift and/or move up to 50 pounds.
The work environment characteristics described here are representative of those an employee encounters while performing the essential functions of this job. While performing the duties of this job, the employee is regularly exposed to risk of electrical shock, moving mechanical parts and heights up to 30ft. The employee is occasionally exposed to wet and/or humid conditions as well as very cold and uncomfortable heat while inside and out of the facility. The noise level in the work environment is at times moderately loud.
Demonstrated strong verbal/written communication skills.
Proven record of excellent internal and external customer service.
Show more
Show less","Data Center Operations, Building Systems Maintenance, Electrical Systems, Mechanical Systems, Power Generation Equipment, HVAC, Refrigeration, Water Cooling, Boilers, Heating, Ventilation, Piping, Filters, Pumps, Valves, Motors, UPS Systems, Emergency Generators, Switchgear, Mechanical Cooling Systems, CMMS, Change Management, Microsoft Word, Microsoft Excel","data center operations, building systems maintenance, electrical systems, mechanical systems, power generation equipment, hvac, refrigeration, water cooling, boilers, heating, ventilation, piping, filters, pumps, valves, motors, ups systems, emergency generators, switchgear, mechanical cooling systems, cmms, change management, microsoft word, microsoft excel","boilers, building systems maintenance, change management, cmms, data center operations, electrical systems, emergency generators, filters, heating, hvac, mechanical cooling systems, mechanical systems, microsoft excel, microsoft word, motors, piping, power generation equipment, pumps, refrigeration, switchgear, ups systems, valves, ventilation, water cooling"
Data Center Building Operating Engineer,JLL,"Broomfield, CO",https://www.linkedin.com/jobs/view/data-center-building-operating-engineer-at-jll-3675623055,2023-12-17,Colorado,United States,Associate,Onsite,"What This Job Involves –
Our Data Center Building Operating Engineer will be responsible to operate all building systems and equipment safely and efficiently, troubleshoot issues, and assist in the hands-on maintenance of those building systems.
What is your day to day?
Operate systems in a safe and efficient manner in accordance with government regulations, company policies and standard operating procedures.
Perform the installation, maintenance, repair and operation of mechanical, electrical and power generation equipment and systems to support the critical environment.
Quickly learn building controls, electrical and UPS systems, utilizing on the job training, operations manuals, and documentation.
Perform predictive, planned, preventative and routine maintenance as well as equipment rounds, and service requests as directed. Perform maintenance in accordance with the JLL Preventative Maintenance Program. Assist with the development and improvement of preventative maintenance programs, schedules, work instructions, SOP’s and operational procedures.
Inspect and repair pumps, fans, valves, and motors ensuring proper operation of the equipment and systems.
Perform visual and operational inspection of associated equipment and interpret specifications, blueprints, and job standards to perform assigned duties.
Determine and/or assist with analysing and resolving work problems and achieving work goals
Maintain time and production records and customer service requests.
Must be comfortable and flexible enough to work with a wide-ranging and changing scope of responsibilities while adhering to the constraints of procedures put in place to minimize the risk for human error.
Monitor operation and maintain refrigeration, water cooling and air conditioning equipment, boilers, heating, ventilating and hot water equipment, pumps, valves, piping and filters, other mechanical and electrical equipment
Perform and/or complete all tenant service requests accurately and expediently.
Complies with all policies and procedures established for the building, including safe storage, usage, and disposal of hazardous materials while maintaining a clean and safe workplace
Periodically acts as the on-call engineer as requested by supervisor.
Actively participate in emergency response procedures, technical and safety training
Required
Desired experience and technical skills
Must have 4+ years of hands-on experience in a data center/critical facility, including UPS Systems, emergency generators, switch-gear, and mechanical cooling systems.
State and local licensures, where required
Certification as a Universal Technician for CFC’s (or within 90 days of employment), if required by local jurisdiction.
Should be knowledgeable in the safe and proper use of the following tools: ladders, lifts, basic hand tools, voltmeter, manual drain auger, safety goggles, ear protection, and fire extinguisher.
Preferred
High School diploma or GED equivalent is required. Training and certification from an accredited trade school or Union apprentice program in HVAC, electronics, electrical is preferred.
Working knowledge of computer applications including Word and Excel, and business applications, such as CMMS and Change Management is preferred.
Demonstrated strong verbal/written communication skills.
Show more
Show less","Building systems operation, Troubleshooting, Maintenance, Electrical systems, UPS systems, Building controls, Predictive maintenance, Preventative maintenance, Routine maintenance, Equipment rounds, Service requests, JLL Preventative Maintenance Program, Pumps, Fans, Valves, Motors, Refrigeration, Water cooling, Air conditioning, Boilers, Heating, Ventilating, Hot water equipment, Piping, Filters, Tenant service requests, Hazardous materials, Emergency response procedures, Technical training, Safety training, UPS Systems, Emergency generators, Switchgear, Mechanical cooling systems, State and local licensures, CFC certification, Ladders, Lifts, Basic hand tools, Voltmeter, Manual drain auger, Safety goggles, Ear protection, Fire extinguisher, CMMS, Change Management","building systems operation, troubleshooting, maintenance, electrical systems, ups systems, building controls, predictive maintenance, preventative maintenance, routine maintenance, equipment rounds, service requests, jll preventative maintenance program, pumps, fans, valves, motors, refrigeration, water cooling, air conditioning, boilers, heating, ventilating, hot water equipment, piping, filters, tenant service requests, hazardous materials, emergency response procedures, technical training, safety training, ups systems, emergency generators, switchgear, mechanical cooling systems, state and local licensures, cfc certification, ladders, lifts, basic hand tools, voltmeter, manual drain auger, safety goggles, ear protection, fire extinguisher, cmms, change management","air conditioning, basic hand tools, boilers, building controls, building systems operation, cfc certification, change management, cmms, ear protection, electrical systems, emergency generators, emergency response procedures, equipment rounds, fans, filters, fire extinguisher, hazardous materials, heating, hot water equipment, jll preventative maintenance program, ladders, lifts, maintenance, manual drain auger, mechanical cooling systems, motors, piping, predictive maintenance, preventative maintenance, pumps, refrigeration, routine maintenance, safety goggles, safety training, service requests, state and local licensures, switchgear, technical training, tenant service requests, troubleshooting, ups systems, valves, ventilating, voltmeter, water cooling"
Quality Data Analyst - Claims/Insurance,Compri Consulting,"Denver, CO",https://www.linkedin.com/jobs/view/quality-data-analyst-claims-insurance-at-compri-consulting-3774824249,2023-12-17,Colorado,United States,Associate,Hybrid,"*Colorado Candidates Only*
Client located in Denver, Colorado is seeking a Quality Control Data Analyst for a direct hire position. This position will report to the Data Quality Manager and participate on a three-person team. Responsibilities include reviewing SQL code and digging into database systems to validate data. Required: -Healthcare claims / medical billing experience. -Experience assessing data accuracy. -Solid SQL query skills. -Ability to navigate database systems and read complex stored procedures. Desired: -CPT codes / ICD Classifications.
$70-$78K
Show more
Show less","SQL, Data Analysis, Data Validation, Data Quality Management, Database Systems, Healthcare Claims, Medical Billing, CPT Codes, ICD Classifications","sql, data analysis, data validation, data quality management, database systems, healthcare claims, medical billing, cpt codes, icd classifications","cpt codes, data quality management, data validation, dataanalytics, database systems, healthcare claims, icd classifications, medical billing, sql"
"Director, Data Scientist - Biopharma",Pfizer,"Boulder, CO",https://www.linkedin.com/jobs/view/director-data-scientist-biopharma-at-pfizer-3729747091,2023-12-17,Colorado,United States,Associate,Hybrid,"ROLE SUMMARY:
The Commercial Analytics team at Pfizer is looking for a Director, Data Science who is passionate about crafting and implementing predictive modeling and statistical analysis to build end-to-end solutions and insights that have a direct impact on patient's lives and the future of Pfizer as a data-driven organization. You will be a thought partner to the business, understand strategic goals, and then use your skills and subject matter expertise to surface impactful insights that drive business decisions and patient benefits. With colleagues across the globe, our rigorous analytical expertise is relied upon as the compass and decision support for the enterprise. Our dynamic, exciting team of subject-matter experts comes from diverse backgrounds and experiences, including market research, data science, digital analytics, finance, and consulting. Our culture is about getting things done iteratively and rapidly, with open feedback and debate along the way. We believe Data Science is a team sport, but we strive for independent decision-making and taking smart risks.
ROLE RESPONSIBILITIES:
This role is accountable for delivering data science driven support for an assigned brand and will partner with US Commercial, business, and digital teams, to develop and implement models, insights, and data products that drive brands’ strategic priorities. This is an individual contributor role.
Take deep dives in large-scale data to identify key insights that will shape future product/brand strategy for a specific therapeutic area.
Collaborate with cross-functional teams to identify new growth opportunities, develop data requirements, establish critical metrics, and evangelize data products.
Design, deploy, and evaluate experiments that help define opportunities for higher adoption, improved business performance, and better patient experience.
Conduct hypothesis-driven exploratory analyses, select appropriate ML algorithms, and build complex optimization engines to deliver impactful data solutions!
Research new technologies and methods across data science and data engineering to improve the technical capabilities of the team.
Communicate insights to senior management by distilling complex analysis and concepts into concise business-focused takeaways.
QUALIFICATIONS:
Bachelor’s degree with 10+ years of experience OR Masters Degree with 9+ years of experience OR PhD with 7+ years of experience
Degree preferably in engineering, economics, statistics, computer science, or related quantitative field.
Preferred experience in Applied Econometrics, Statistics, Data Mining, Machine Learning, Analytics, Mathematics, Operations Research, Industrial Engineering, or related field preferred.
Working knowledge of relational databases, including SQL, and large-scale distributed systems such as Hadoop and or working in Snowflake/Databricks
Ability to implement data science pipelines and applications in a general programming language such as Python, Scala, Java, or R.
Practical experience with and theoretical understanding of ML algorithms for classification, regression, clustering, and anomaly detection
Well versed with and have experience applying various statistical methodologies including Bayesian and non-parametric techniques, hypothesis testing, ANOVA, Regression, fixed and random effects etc. to measure the impact of experiments
Hands on experience working in big data environments such as Hadoop, Spark, and using Python / SQL or comparable languages for manipulating and analyzing complex clickstream and or unstructured data
Ability to extract significant business insights from data and identify the roots behind the patterns
Experience working with a data visualization tool/package, including Dash, Tableau, and Angular etc.
Communication skills for communicating complex quantitative analyses to senior business executives
Candidate demonstrates a breadth of diverse leadership experiences and capabilities including: the ability to influence and collaborate with peers, develop and coach others, oversee and guide the work of other colleagues to achieve meaningful outcomes and create business impact.
Other Job Details:
Last Date to Apply for Job: October 31, 2023
The annual base salary for this position ranges from $144,900.00 to $241,500.00.* In addition, this position is eligible for participation in Pfizer’s Global Performance Plan with a bonus target of 20.0% of the base salary and eligibility to participate in our share based long term incentive program. We offer comprehensive and generous benefits and programs to help our colleagues lead healthy lives and to support each of life’s moments. Benefits offered include a 401(k) plan with Pfizer Matching Contributions and an additional Pfizer Retirement Savings Contribution, paid vacation, holiday and personal days, paid caregiver/parental and medical leave, and health benefits to include medical, prescription drug, dental and vision coverage. Learn more at Pfizer Candidate Site – U.S. Benefits | (uscandidates.mypfizerbenefits.com). Pfizer compensation structures and benefit packages are aligned based on the location of hire. The United States salary range provided does not apply to Tampa, FL or any location outside of the United States.
The annual base salary for this position in Tampa, FL ranges from $130,400.00 to $217,300.00.
Relocation assistance may be available based on business needs and/or eligibility.
Sunshine Act
Pfizer reports payments and other transfers of value to health care providers as required by federal and state transparency laws and implementing regulations. These laws and regulations require Pfizer to provide government agencies with information such as a health care provider’s name, address and the type of payments or other value received, generally for public disclosure. Subject to further legal review and statutory or regulatory clarification, which Pfizer intends to pursue, reimbursement of recruiting expenses for licensed physicians may constitute a reportable transfer of value under the federal transparency law commonly known as the Sunshine Act. Therefore, if you are a licensed physician who incurs recruiting expenses as a result of interviewing with Pfizer that we pay or reimburse, your name, address and the amount of payments made currently will be reported to the government. If you have questions regarding this matter, please do not hesitate to contact your Talent Acquisition representative.
EEO & Employment Eligibility
Pfizer is committed to equal opportunity in the terms and conditions of employment for all employees and job applicants without regard to race, color, religion, sex, sexual orientation, age, gender identity or gender expression, national origin, disability or veteran status. Pfizer also complies with all applicable national, state and local laws governing nondiscrimination in employment as well as work authorization and employment eligibility verification requirements of the Immigration and Nationality Act and IRCA. Pfizer is an E-Verify employer.
Marketing and Market Research
Show more
Show less","SQL, Python, R, Scala, Java, Hadoop, Spark, Snowflake, Databricks, Machine Learning, Data Mining, Applied Econometrics, Statistics, Analytics, Mathematics, Operations Research, Industrial Engineering, Regression, Bayesian techniques, Nonparametric techniques, ANOVA, Hypothesis testing, Dash, Tableau, Angular","sql, python, r, scala, java, hadoop, spark, snowflake, databricks, machine learning, data mining, applied econometrics, statistics, analytics, mathematics, operations research, industrial engineering, regression, bayesian techniques, nonparametric techniques, anova, hypothesis testing, dash, tableau, angular","analytics, angular, anova, applied econometrics, bayesian techniques, dash, data mining, databricks, hadoop, hypothesis testing, industrial engineering, java, machine learning, mathematics, nonparametric techniques, operations research, python, r, regression, scala, snowflake, spark, sql, statistics, tableau"
Senior Data Analytics Engineer,Fastly,"Denver, CO",https://www.linkedin.com/jobs/view/senior-data-analytics-engineer-at-fastly-3775560420,2023-12-17,Colorado,United States,Mid senior,Onsite,"Fastly helps people stay better connected with the things they love. Fastly’s edge cloud platform enables customers to create great digital experiences quickly, securely, and reliably by processing, serving, and securing our customers’ applications as close to their end-users as possible — at the edge of the Internet. The platform is designed to take advantage of the modern internet, to be programmable, and to support agile software development. Fastly’s customers include many of the world’s most prominent companies, including Vimeo, Pinterest, The New York Times, and GitHub.
We're building a more trustworthy Internet. Come join us.
Senior Data Analytics Engineer
The Analytics team is building a modern data stack to cultivate data-driven decision-making across our high growth business. We are looking for a Senior Analytics Engineer to join our Analytics team, who is inquisitive, curious, and passionate about solving complex business problems with analysis. This role will provide the opportunity to work closely with business partners and build KPIs, dashboards, pipelines, and exploratory analysis to empower a data-driven culture across the company.
What You'll Do
You’ll collaborate and partner with executives, stakeholders, and decision-makers to prioritize, design, and fulfill their data analytics needs
You’ll identify KPIs driving business growth and performance and work closely with the business teams to define them
You’ll work across the data stack to design and build various data products (data models, dashboards, analysis) that enable the business to make data backed decisions
You’ll help garner trust in data from the business by documenting analytics best practices, assisting with data discoverability, governance, and accuracy
You’ll promote and drive a self-service data culture by building easy to use analytics products and teaching the business users how to use them
You’ll communicate progress, risks, and completion of projects to stakeholders
What We're Looking For
You have 5+ years of hands-on experience in BI or Analytics teams.
You have hands-on experience in building and managing data transformations (dbt experience is a plus)
You are able to craft functional and user-friendly data products (data models, dashboards, etc..) for both technical and non-technical stakeholders
You have experience with cloud data warehouse technologies (BigQuery is a plus)
You have experience with a data visualization tools (ie. Looker, Tableau, Mode)
You are a SQL expert, capable of complex, efficient, and logical SQL to accomplish data analysis goals
You are driven by logical and analytical thinking to solve problems. And have an eye for detail-oriented analysis and design
You have functional knowledge of financial, sales, or other core business metrics
You are able to manage professional relationships across a variety of stakeholders, including executives
You are able to identify and prioritize multiple requirements and manage expectations accordingly
You are collaborative and willing to traverse and contribute to the entire data stack, from front-end to back-end as needed
Work Hours:
This position will require you to be available during core business hours.
Work Location(s) & Travel Requirements:
This position is open to the following preferred office locations:
San Francisco, CA USA
Denver, CO USA
New York, NY USA
Culver City, CA USA
Fastly currently embraces a largely hybrid model for most roles which allows employees flexibility to split their time between the office and home.
This position may require travel as required by your role or requested by your manager.
Salary
The estimated salary range for this position is $143,860 to $179,820.
Starting salary may vary based on permissible, non-discriminatory factors such as experience, skills, qualifications, and location.
This role may be eligible to participate in Fastly’s equity and discretionary bonus programs.
Benefits
We care about you. Fastly works hard to create a positive environment for our employees, and we think your life outside of work is important too. We support our teams with great benefits that start on the first day of your employment with Fastly. Curious about our offerings?
We offer a comprehensive benefits package including medical, dental, and vision insurance. Family planning, mental health support along with Employee Assistance Program, Insurance (Life, Disability, and Accident), an open vacation policy and up to 18 days of accrued paid sick leave are there to help support our employees. We also offer 401(k) (including company match) and an Employee Stock Purchase Program. For 2023, we offer 10 paid local holidays, 11 paid company wellness days.
Why Fastly?
We have a huge impact. Fastly is a small company with a big reach. Not only do our customers have a tremendous user base, but we also support a growing number of open source projects and initiatives. Outside of code, employees are encouraged to share causes close to their heart with others so we can help lend a supportive hand.
We love distributed teams. Fastly’s home-base is in San Francisco, but we have multiple offices and employees sprinkled around the globe. As a new hire, you will be able to attend our IN-PERSON new hire orientation in our San Francisco office! It is an exciting week-long experience that we offer to new employees to build connections with colleagues across Fastly, participate in hands-on learning opportunities, and immerse yourself in our culture firsthand.
We value diversity. Growing and maintaining our inclusive and diverse team matters to us. We are committed to being a company where our employees feel comfortable bringing their authentic selves to work and have the ability to be successful -- every day.
We are passionate. Fastly is chock full of passionate people and we’re not ‘one size fits all’. Fastly employs authors, pilots, skiers, parents (of humans and animals), makeup geeks, coffee connoisseurs, and more. We love employees for who they are and what they are passionate about.
We’re always looking for humble, sharp, and creative folks to join the Fastly team. If you think you might be a fit please apply!
A fully completed application and resume or CV are required when applying.
Fastly is committed to ensuring equal employment opportunity and to providing employees with a safe and welcoming work environment free of discrimination and harassment. Our employment decisions are based on business needs, job requirements and individual qualifications.
All qualified applicants will receive consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, family or parental status, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by applicable laws, regulations and ordinances.
Consistent with the Americans with Disabilities Act (ADA) and federal or state disability laws, Fastly will provide reasonable accommodations for applicants and employees with disabilities. If reasonable accommodation is needed to participate in the job application or interview process, to perform essential job functions, and/or to receive other benefits and privileges of employment, please contact your Recruiter, or the Fastly Employee Relations team at
candidateaccommodations@fastly.com
or 501-287-4901.
Fastly collects and processes personal data submitted by job applicants in accordance with our Privacy Policy. Please see our privacy notice for job applicants.
Show more
Show less","Data analytics, Business intelligence, Data modeling, Data visualization, SQL, Data transformations, Cloud data warehouse technologies, DBT, Looker, Tableau, Mode, Financial metrics, Sales metrics, Business metrics, Data products, Dashboards, Analysis, KPIs, Data governance, Data accuracy, Data discovery, Data culture, Datadriven decisionmaking, Agile software development","data analytics, business intelligence, data modeling, data visualization, sql, data transformations, cloud data warehouse technologies, dbt, looker, tableau, mode, financial metrics, sales metrics, business metrics, data products, dashboards, analysis, kpis, data governance, data accuracy, data discovery, data culture, datadriven decisionmaking, agile software development","agile software development, analysis, business intelligence, business metrics, cloud data warehouse technologies, dashboard, data accuracy, data culture, data discovery, data governance, data products, data transformations, dataanalytics, datadriven decisionmaking, datamodeling, dbt, financial metrics, kpis, looker, mode, sales metrics, sql, tableau, visualization"
Senior Data Engineer,Bio-Techne,"Centennial, CO",https://www.linkedin.com/jobs/view/senior-data-engineer-at-bio-techne-3705230467,2023-12-17,Colorado,United States,Mid senior,Onsite,"Job Details
Level
Senior
Job Location
Centennial CO - Centennial, CO
Position Type
Full Time
Education Level
Bachelor's Degree
Salary Range
$106,000.00 - $159,000.00 Salary
Job Shift
Day
Job Category
Information Technology
Description
By joining Bio-Techne, you’ll join a company with a powerful and positive purpose of enabling cutting-edge research in Life Sciences and Clinical Diagnostics. Bio-Techne, and all of its brands, provides tools for researchers to further treat and prevent disease worldwide.
Position Summary
We are looking for a Senior Data Engineer to join our growing team. This individual will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross-functional teams. The core function will be driving the creation of master data repositories for critical business data. The ideal candidate is an experienced data pipeline builder who enjoys optimizing data systems and building them from the ground up. The Senior Data Engineer will support our software developers, database architects and data analysts on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems, and products.
Key Responsibilities
Create and maintain master data solutions for critical business data
Create and maintain optimal data pipeline architecture
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and Azure or similar cloud ‘big data’ technologies.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Work with data and analytics experts to strive for greater functionality in our data systems.
Qualifications
Education and Experience:
Bachelor’s Degree in Computer Science and 5-8 years of relevant experience, or a High School Diploma, certificate, or equivalent with 8-10 years relevant experience.
Experience with relational SQL databases required
Experience with data warehousing methodologies required
5+ years working with SQL and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
5+ years of experience in a Data Engineer or Database Developer role
Master Data Management knowledge and experience preferred
Must be local to the Centennial, CO area to meet the hybrid in office requirements of the role.
Knowledge, Skills And Abilities
Experience with data pipeline and workflow management tools: Azure Service Bus, Azure Data Factory, Dell Boomi or similar ETL/ELT technologies a plus
Experience with Azure cloud services a plus
Experience with object-oriented/object function scripting languages: Python,C#, C++, JavaScript, etc. a plus
Experience building and optimizing data pipelines, architectures, and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets a plus
Strong project management and organizational skills.
Experience supporting and working with cross-functional teams in a dynamic environment.
Microsoft Master Data Services and/or Data Vault 2.0 would be a plus
Why Join Bio-Techne
We offer competitive salaries along with extensive medical, vision, and dental plans for you and your family starting on day one!
We invest in our employees’ financial futures through 401k matching and an employee stock purchase plan.
We help our employees develop their careers through mentorship, promotional opportunities, training and development, internship programs, and more.
We offer employee resource groups, volunteer paid time off, employee events, and charity drives to build a culture of caring and belonging.
We foster a culture of empowerment and innovation, where employees feel valued and encouraged to bring their new ideas to the table.
Where permitted by applicable law, candidate must have received or be willing to receive an FDA authorized COVID-19 vaccine by date of hire to be considered for this position
.
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.
Bio-Techne is an E-Verify Employer in the United States.
Show more
Show less","Data Pipeline Architecture, Data Flow, Master Data Repositories, SQL, Azure, Data Warehousing, Data Transformation, ETL/ELT Technologies, Cloud Services, Python, C#, C++, JavaScript, Microsoft Master Data Services, Data Vault 2.0","data pipeline architecture, data flow, master data repositories, sql, azure, data warehousing, data transformation, etlelt technologies, cloud services, python, c, c, javascript, microsoft master data services, data vault 20","azure, c, cloud services, data flow, data pipeline architecture, data transformation, data vault 20, datawarehouse, etlelt technologies, javascript, master data repositories, microsoft master data services, python, sql"
Data Engineer,ALTEN Technology USA,"Westminster, CO",https://www.linkedin.com/jobs/view/data-engineer-at-alten-technology-usa-3774869524,2023-12-17,Colorado,United States,Mid senior,Onsite,"We’re ALTEN Technology USA, an engineering company that provides solutions for engineering, technology, and product development projects. For decades, we’ve been helping our clients develop products that are changing the world, whether that’s by shaping the future of space exploration, saving lives with medical devices that set a new standard of care, or creating the fully autonomous electric taxi of tomorrow. Our team of more than 600 people works across industries including aerospace, medical devices and life sciences, unmanned systems and robotics, automotive OEM and tier 1 suppliers, commercial vehicle, electric vehicles, rail, and more. Our offices across the US have different industry focuses and engage with our clients in different ways, ranging from working on-site at client facilities to performing product development at our delivery centers.
ALTEN Group, our parent company, has been a leader in engineering and information technology for more than 30 years. ALTEN Group operates in 30 countries across Europe, North America, Asia, Africa and the Middle East and employs more than 36,000 people, 88% of whom are engineers. The company covers the whole development cycle and offers a choice of service levels, from technology consulting to complete project outsourcing.
When you join ALTEN Technology USA, you join a group of people passionate about collaborating to solve some of the world’s most technical engineering challenges. Our success is tied to taking care of our employees by building relationships and providing opportunities for mentorship and career growth. We offer comprehensive benefits for all full-time employees, including medical, dental, and vision insurance; a 401(k) plan with employer matching; paid time off (PTO); paid parental leave; and mentorship to help you take your career to the next level. You don’t have to take our word for it that we are a great place to work—we were named on the 2023 Top Workplaces list thanks to feedback from our proud employees.
Essential Duties and Responsibilities include the following. Other duties may be assigned.
· Collaborate with business users to understand analytics requirements.
· Implement data-driven solutions for the tenant accounts.
· Leverage strong data enablement and infrastructure creation skills to support analysis use cases by our customers.
· Develop and maintain data pipelines and jobs for the tenant accounts.
· Utilize expertise in AWS data services, including Glue and Athena, to optimize data processes.
· Provision and configure new tenant AWS accounts.
· Administer shared resources within the administrative account.
· Provide account support to ensure seamless operations.
Education/Experience
· B.S./B.A. degree in engineering or another technical field.
Qualifications: The requirements listed below are representative of the knowledge, skills, and/or abilities required.
· 1-5 years of relevant experience.
· Experience with data services, such as Glue and Athena.
· Familiarity with AWS and Account Administration.
Salary Range: $70k - $100k
The actual salary offered is dependent on various factors including, but not limited to, location, the candidate’s combination of job-related knowledge, qualifications, skills, education, training, and experience.
Location: Remote. Must be able to support EST Business hours.
Show more
Show less","AWS, Data Pipelines, Glue, Athena, Data Services, Account Administration, Engineering, Big Data, Data Analysis, Data Modeling, Data Warehousing, Cloud Computing","aws, data pipelines, glue, athena, data services, account administration, engineering, big data, data analysis, data modeling, data warehousing, cloud computing","account administration, athena, aws, big data, cloud computing, data services, dataanalytics, datamodeling, datapipeline, datawarehouse, engineering, glue"
Business Intelligence Data Developer,TekWissen ®,"Centennial, CO",https://www.linkedin.com/jobs/view/business-intelligence-data-developer-at-tekwissen-%C2%AE-3723226457,2023-12-17,Colorado,United States,Mid senior,Onsite,"Overview
TekWissen Group is a workforce management provider throughout the USA and many other countries in the world. The below client is an American spacecraft launch service provider that manufactures and operates a number of rocket vehicles that are capable of launching spacecraft into orbits around Earth and to other bodies in the Solar System.
Position: Business Intelligence Data Developer
Location: Centennial CO 80112
Duration: 6 Months
Job Type: Contract
Work Type: Hybrid
Job Description
Client is looking at bringing in a Business Intelligence Data Developer with 9+ years in building, developing, and deploying BI Solutions.
Person will be responsible for supporting the Supply Chain Dashboard data modeling.
Other data modeling or warehousing and report development as needed.
Required Skills
Proficient in HANA 2.0 data modeling, XS Classic & XSA.
Experience in project deployments using HANA/BW and SAP Business Objects (BOBJ) tool sets [70%].
Experienced in using Microsoft Power BI for data modeling and dashboard development [30%].
Experienced in Business Objects tools: WebIntelligence (WebI), Lumira Design studio, Analysis for Office.
Good knowledge on ETL tools: SLT and IDT.
Strong analytical and debugging skills.
Excellent oral and written communication skills.
Preferred Skills
Strong functional knowledge of Supply Chain and Manufacturing.
Front end dashboard development in WebIntelligence or Microsoft Power BI.
Experience using SAP HANA data models in Power BI.
Required Experience
Candidate must have a minimum of 9 years of BW / SAP HANA analytics experience.
Software Experience: SAP HANA analytics experienced required as min. – no HANA experience is an automatic DISQUALIFIER.
Required Education
Bachelor’s degree preferred, but NOT required. Manager will accept experience in lieu of degree.
Keywords
Proficient in HANA 2.0 data modeling. (required OR rejected, NO exceptions)
XS Classic & XSA experience highly preferred.
Experienced using Microsoft Power BI for data modeling and dashboard development.
TekWissen® Group is an equal opportunity employer supporting workforce diversity.
TekWissen is an emerging global human capital, recruitment and IT services organization. Operating since 2009, we draw upon more than a decade of staffing experience to deliver critical talent acquisition solutions and IT engagements for our clients. We’re founded on a culture that is passionate about delivering tailored solutions, that create lasting partnerships.
Our global footprint covers six countries: United States, Canada, Australia, India, United Kingdom and the Philippines. This allows us to work in close partnership with organizations and manage everything from global talent needs with demanding resourcing strategies, to single sites with lower recruitment volumes.
TekWissen® is an equal opportunity employer supporting workplace diversity.
Show more
Show less","HANA 2.0 data modeling, XS Classic, XSA, SAP Business Objects (BOBJ) tool sets, Microsoft Power BI, WebIntelligence (WebI), Lumira Design studio, Analysis for Office, SAP HANA data models, BW / SAP HANA analytics, HANA analytics, SQL, SLT, IDT, ETL","hana 20 data modeling, xs classic, xsa, sap business objects bobj tool sets, microsoft power bi, webintelligence webi, lumira design studio, analysis for office, sap hana data models, bw sap hana analytics, hana analytics, sql, slt, idt, etl","analysis for office, bw sap hana analytics, etl, hana 20 data modeling, hana analytics, idt, lumira design studio, microsoft power bi, sap business objects bobj tool sets, sap hana data models, slt, sql, webintelligence webi, xs classic, xsa"
Sr. Data Engineer,SM Energy Company,"Denver, CO",https://www.linkedin.com/jobs/view/sr-data-engineer-at-sm-energy-company-3767212506,2023-12-17,Colorado,United States,Mid senior,Onsite,"Position Summary
The Sr. Data Engineer can work both independently and as part of a team to determine and develop approaches and solutions to complex problems. The position provides a specialized understanding of computer capabilities both cloud and on prem and interfaces between analytic applications or data stores. They develop workflows and solutions across multiple platforms while having the communication skills necessary to advise business users and management on process improvement and best practices. They also partner with business users to resolve issues and provide effective solutions, acting as a liaison between the business units, technology, and support teams. They work closely with members of other IT teams to support and drive a data/analytics first culture. They are viewed as a technical leader and are often consulted by team members and others in the organization for their knowledge/advisement and may train jr staff members.
Essential Roles And Responsibilities
Proficient in SQL and Python development further providing support to the Data and Analytics teams and the business in database design, data flow and analysis activities.
Proficient in the development and deployment of innovative big data platforms for advanced analytics and data processing and work with Data Architect to build innovative solutions.
Proficient in the maintenance, improvement, and manipulation of data in the business’s analytics data views. Provides support and scripts to identify potential data quality issues, analyzes analytics performance, and troubleshoot any data pipeline issues. Is a technical lead for other members on the team.
Proficient in the development and implementation of scripts for database maintenance, monitoring, performance tuning, and so forth. Proactively brings forward optimization opportunities and areas of potential risk.
Skilled in requirements gathering to define and build the end-to-end data pipelines that are cost effective and scalable for the business need.
Proficient in data acquisition problems and providing support and recommendations. Ex: API connection to vendor data or app for data collection
Practiced in designing and developing scalable ETL packages from the business source systems and the development of ETL routines in order to populate databases from sources.
Leader in performing thorough testing and validation in order to support the accuracy of data transformations ,data verification, and educates other team members in best practices in this area
Proficient in code documentation, change release process, and can communicate changes and risks to the team and others effectively.
Proficient in performing ad-hoc analyses of data stored in the company databases and designs, develops and tests SQL scripts, stored procedures, functions, and views
Troubleshoots data issues within the business and across the business and presents solutions to these issues. Can proactively identify potential issues and make recommendations.
Proficient in analyzing complex data elements and systems, data flow, dependencies, and relationships in order to contribute to & define conceptual physical and logical data models and actively mentors others in this area.
Leads and coaches junior data engineering support personnel and provides guidance and direction to third party contract developers to deliver fast and reliable data for advanced analytics solutioning.
Working independently can build and leads data education sessions with the business to support self-service data discovery, analytics, and data science.
Lead innovation through exploration, research, benchmarking, making recommendations, and makes recommendations for implementing big data technologies for platforms in support of leadership strategy
Proficient in data governance and is a leader in the team and organization on best practices for data quality.
A leader that collaboratively works with and provides a continuous feedback loop for the entire Data, AI and Analytics teams, providing support to the entire business for its data centric needs
Proficient in security to support the delivery of data and data delivery systems and privacy concerns and makes recommendations and solutions to leadership for areas that need improvement
Working knowledge of : Software engineering, web app development, applied math and ML frameworks
Machine Learning Data Engineers are proficient in making recommendations in deploying and scaling machine learning models
Demonstrate a commitment to learning by seeking higher learning opportunities to ensure thorough understanding of the current and future capabilities of supported systems
Actively engage in local and on-line development communities building a network of colleagues upon which to share experiences and knowledge.
Actively stays current of industry trends and best practices, advising senior management on new and improved data engineering strategies
Drive excellent, consistent customer service.
Computer/Software Applications
Programming Experience: SQL, R, Python, and/or .Net
Data Visulization: Power BI, Spotfire, Tableau, etc.
Data Architecture: Relational Database design, Warehouse design, Structred/Unstructured, Real-time
Data ETL tools: SSIS, dbt, Data API, Matilion, Boomi, etc.
Cloud Architecture: Azure, AWS, Google, VM, kubernetes, Azure ACI, kafka
Cloud Data Tools: Snowflake, Databricks, Delta Lake
Data Engineering: Complex Data Flows, Data Cleansing, Data Delivery
AI/ML: Data Prep, Scalable Compute, Notebooks, Delivery Tools (React, Vue, Streamlit, Matlab)
Education
Associates or Bachelor’s degree in related field is strongly preferred
Typical Experience
Minimum of 8 years related experience
Location
This is a Denver based hybrid role. Primary work is done from home with in person work done that prioritizes collaboration, networking and team building.
Salary range: $111,000 - $150,000
Show more
Show less","SQL, Python, R, .Net, Power BI, Spotfire, Tableau, SSIS, dbt, Data API, Matilion, Boomi, Azure, AWS, Google, VM, kubernetes, Azure ACI, kafka, Snowflake, Databricks, Delta Lake, Data Prep, Scalable Compute, Notebooks, Delivery Tools, React, Vue, Streamlit, Matlab, Relational Database design, Warehouse design, Structured/Unstructured, Realtime, Complex Data Flows, Data Cleansing, Data Delivery","sql, python, r, net, power bi, spotfire, tableau, ssis, dbt, data api, matilion, boomi, azure, aws, google, vm, kubernetes, azure aci, kafka, snowflake, databricks, delta lake, data prep, scalable compute, notebooks, delivery tools, react, vue, streamlit, matlab, relational database design, warehouse design, structuredunstructured, realtime, complex data flows, data cleansing, data delivery","aws, azure, azure aci, boomi, complex data flows, data api, data delivery, data prep, databricks, datacleaning, dbt, delivery tools, delta lake, google, kafka, kubernetes, matilion, matlab, net, notebooks, powerbi, python, r, react, realtime, relational database design, scalable compute, snowflake, spotfire, sql, ssis, streamlit, structuredunstructured, tableau, vm, vue, warehouse design"
Sr. Data Platform Engineer,EverBright,"Denver, CO",https://www.linkedin.com/jobs/view/sr-data-platform-engineer-at-everbright-3785567251,2023-12-17,Colorado,United States,Mid senior,Onsite,"EverBright is a clean energy company on a mission to accelerate the decarbonization of residences and businesses across the United States. We are a leading provider of financing for residential solar, storage, and energy efficiency products, fueling our rapid growth through innovative software solutions. We understand that the energy systems of tomorrow will be clean, distributed, and powered by advanced technology.
As a Senior Data Platform Engineer, you will play a critical role in designing, building, and maintaining the data infrastructure for our organization, employing a strong combination of data engineering and software engineering skills as we build out our new data platform. You will be responsible for developing and maintaining scalable and reliable data pipelines that enable data science, analytics, software, and business teams to build products and make critical data-driven strategic decisions. Your role will involve collaborating with cross-functional teams to identify business requirements and translate them into technical solutions.
You will also be responsible for designing and implementing data models and warehousesthat can efficiently handle large-scale data. In addition to technical expertise, you should have strong leadership and communication skills to effectively lead and mentor junior data engineers on the team. You should be comfortable working in an agile development environment and have experience with agile methodologies.
WHAT YOU'll DO
Design, build, and maintain scalable and reliable data pipelines, infrastructure and services
Develop and maintain data models and data warehouses
Lead and mentor junior data engineers on the team
Continuously evaluate and improve data infrastructure to ensure scalability and reliability
Qualifications
Bachelor's degree in Computer Science or other related Engineering field
4+ years of experience in data engineering
7+ years experience software engineering or related field
Strong proficiency in SQL and programming languages such as Python, Java or Scala
Hands-on experience with data warehousing and data modeling, with a strong understanding of relational database schema design
Experience building systems with cloud providers such as AWS or Azure
Good knowledge of the current data engineering landscape + available tools
Strong leadership and communication skills
Show more
Show less","Data engineering, Software engineering, Scalable data pipelines, Data science, Analytics, Software, Datadriven strategic decisions, Crossfunctional teams, Data models, Data warehouses, Agile development environment, Agile methodologies, SQL, Python, Java, Scala, Cloud providers, AWS, Azure, Leadership, Communication","data engineering, software engineering, scalable data pipelines, data science, analytics, software, datadriven strategic decisions, crossfunctional teams, data models, data warehouses, agile development environment, agile methodologies, sql, python, java, scala, cloud providers, aws, azure, leadership, communication","agile development environment, agile methodologies, analytics, aws, azure, cloud providers, communication, crossfunctional teams, data engineering, data models, data science, data warehouses, datadriven strategic decisions, java, leadership, python, scala, scalable data pipelines, software, software engineering, sql"
"Senior Data Engineer, Pipeline (486)",Techstars,"Boulder, CO",https://www.linkedin.com/jobs/view/senior-data-engineer-pipeline-486-at-techstars-3565454390,2023-12-17,Colorado,United States,Mid senior,Onsite,"As a Sr. Data Engineer at Techstars you will be an important part of a team building innovative software solutions for entrepreneurs worldwide. Techstars already has one of the largest portfolios in early stage venture capital, with over 2,300 portfolio companies, a combined market cap of more than $193B, and 12 unicorns. In this role, you will build backend streaming data pipelines, integrations with third party SaaS applications, complex analytics features and app facing platform data APIs. You will take part in architecture and solution design and help optimize solution performance and reliability. As a result you will help Techstars attract 10x more founders, and contribute directly to scaling the Techstars footprint to serve more entrepreneurs than ever before.
What You Will Do
As part of a team of engineers working in an agile environment you will build and deploy high quality data ingest, cleansing, analytics, integrations and API solutions
Ensure that all data and software solutions are secure, performant, reliable, observable and testable
Work with team members in a collaborative manner
Continuously improve the quality of the products and solutions delivered
What You Bring
A passion for building data solutions
A deep appreciation for dev/ops
Experience working with distributed systems and cloud infrastructure
Data engineering experience with ETL, streaming data pipelines (Kafka), cleansing and mastery
Diverse experience with languages (ie. SQL, Python, Node, Scala)
Experience working with a variety of relational and non-relational database systems (ie Postgres, MongoDB, Redis)
Experience with cloud based data visualization technologies (ie. Tableau, Domo, Airtable)
Experience developing robust APIs
Experience working in an agile development environment
Excellent communication and collaboration skills, a desire to learn and teach
Nice To Have
Experience in financial/investment data
Experience developing with containerization technologies (Docker, Kubernetes etc.)
Compensation range:
$90,000 - $140,000 + 10% Bonus
US Benefits
About Techstars
Techstars is the worldwide network that helps entrepreneurs succeed. Founded in 2006, Techstars began with three simple ideas - entrepreneurs create a better future for everyone, collaboration drives innovation and great ideas can come from anywhere. Now we are on a mission to enable every person on the planet to contribute to, and benefit from, the success of entrepreneurs. In addition to operating accelerator programs and venture capital funds, we do this by connecting startups, investors, corporations and cities to help build thriving startup communities. Techstars has invested in more than 2,300 companies with a combined market cap of more than $210B.
Techstars’ mission is to help entrepreneurs succeed wherever they are in the world and whatever their background is. Regional accelerator programs all around the world are the cornerstone of the strategy. The investment approach is fundamentally driven by the worldwide network of managing directors, who interact with startup founders daily, guiding, mentoring and cultivating them along the journey. The scale of this reach results in a diversified strategy that provides investors with a uniquely qualified deal flow.
We help Techstars founders connect with other entrepreneurs, experts, mentors, alumni, investors, community leaders, and corporations to grow their companies.
www.techstars.com
Show more
Show less","Data Engineering, ETL, Streaming Data Pipelines, Kafka, Data Cleansing, SQL, Python, Node.js, Scala, Postgres, MongoDB, Redis, Tableau, Domo, Airtable, APIs, Agile Development, Cloud Infrastructure, Distributed Systems, Data Visualization, Financial/Investment Data, Containerization Technologies, Docker, Kubernetes","data engineering, etl, streaming data pipelines, kafka, data cleansing, sql, python, nodejs, scala, postgres, mongodb, redis, tableau, domo, airtable, apis, agile development, cloud infrastructure, distributed systems, data visualization, financialinvestment data, containerization technologies, docker, kubernetes","agile development, airtable, apis, cloud infrastructure, containerization technologies, data engineering, datacleaning, distributed systems, docker, domo, etl, financialinvestment data, kafka, kubernetes, mongodb, nodejs, postgres, python, redis, scala, sql, streaming data pipelines, tableau, visualization"
Senior Data Engineer - IT,Terumo Blood and Cell Technologies,"Lakewood, CO",https://www.linkedin.com/jobs/view/senior-data-engineer-it-at-terumo-blood-and-cell-technologies-3752358927,2023-12-17,Colorado,United States,Mid senior,Onsite,"Requisition ID:
31476
At Terumo Blood and Cell Technologies, our 7,000+ global associates proud to come to work each day, knowing that what we do impacts the lives of patients around the world.
We make medical devices and related products that are used to collect, separate, manufacture and process various components of blood and cells. With our innovative technologies and service offerings, we touch a patient’s life every second of every day and are committed to continuing to increase the number of patients we serve.
With some of the best and brightest minds in the industry, an unmatched global footprint, comprehensive benefits and a distinct culture, Terumo Blood and Cell Technologies is a great place to work, grow and be part of a team that is focused on making a difference. Consider joining our team and unlock your potential.
In this capacity, the Senior Data Engineer designs and develops systems for the maintenance of the business’s data warehouse, ETL processes, and business intelligence. Design, build, and/or modify existing data pipeline(s) for Analytics use, with eye toward best practices in enterprise data architecture. Build solutions including real-time data ingestion, data lake and data warehouse storage, and analytics in alignment with best practices for optimizing performance as well as automated integration with dashboarding products. Design and build systems for both batch and real-time data ingestion, storage, and analytics, for structured and unstructured data.
Essential Duties
Build automations and updates to data pipeline to produce regular reporting products to support business decision making.
Work hand-in-hand with business stakeholders/ analysts/ business partners to deliver data solutions in the form of data preparation, data transformation, dashboarding, automated reporting, and automated dashboard updates that will support Terumo’s digital strategy and strategic vision.
Strong working experience and knowledge of SQL databases building/querying databases/schemas/tables. Use state-of-the-art tools and technologies to enable full cloud optimization, seamless elasticity and end-to-end automation with considerations for both performance and architecture.
Make modifications and updates to existing data pipelines as necessary.
Critically review current and future data structures with an eye towards simplicity & performance. Analytical role in quickly and thoroughly analyzing business requirements for reporting and analysis and subsequently translating the emanating results into good technical data designs.
Other Duties And Responsibilities
Help to foster the data-driven and technology-savvy culture across our business by serving as a tech evangelist and encourage forward thinking development.
Participate in SAAS technology evaluations and/or collaborate with other areas of business on technology strategy—and with an enterprise mindset.
Use knowledge and expertise to effectively and collaboratively work with vendors and other non-Terumo BCT associates on analytics planning and development.
Participate and help guide data governance policy and practice for the data lake, data warehouse and other company repositories.
Conduct full system testing and documentation.
Minimum Qualification Requirements
Education
Bachelor’s degree in Computer Science, Statistics and/or related field, equivalent of education and experience sufficient to successfully perform the essential functions of the job may be considered.
Experience
Minimum 5 years’ experience.
Experience with Python, Java, C++, Data bricks, SQL database, Scala or similar object-oriented programming languages and using software automation/CICD tools.
Experience with R Shiny, Azure data factory, Azure Microsoft BI and/or modern data visualization technologies.
Skills
Ability to transform data using one or more modern cloud-native ETL/ELT tools.
Familiarity in working with cloud native technologies, such as Microsoft Azure or AWS, and in building data systems on cloud.
Demonstrated ability to lead projects, including project management tool experience, leading project teams, and coordination with client personnel.
Demonstrated ability to communicate effectively both verbally and in writing.
Exposure to/ Experience with Alteryx Server a plus.
Experience as being a member of an analytics agile team is a definite plus.
-Or-
An equivalent competency level acquired through a variation of these qualifications may be considered.
Physical Requirements
Typical Office Environment requirements, may include reading, speaking, hearing, close vision, walking, bending, sitting, and occasional lifting up to 20 pounds.
Target Pay Range:
$106,500.00 to $133,100.00 - Salary to be determined by the education, experience, knowledge, skills, and abilities of the applicant, internal equity, and alignment with market data
Target Bonus on Base:
7.0%
At Terumo Blood and Cell Technologies, we provide competitive total reward offerings that consist of compensation, benefits, recognition, along with a wealth of other well-being, work-life and recognition programs which support in unlocking the potential for you and your family. Included in our expansive list of benefits offerings are multiple group medical, dental and vision plans, a robust wellness program, life insurance and disability coverages, also a variety of voluntary programs such as group accident, hospital indemnity, critical illness, pet insurance and much more. To help you save for retirement, we offer a 401(k) plan with a matching contribution and for work-life balance we have vacation and sick time programs for associates. For us, it’s about protecting the personal welfare of our associates and their families, helping to achieve personal goals and offering those extra touches for convenience, security and overall peace of mind.
Terumo Blood and Cell Technologies is part of Terumo Group, founded in 1921 and headquartered in Tokyo, Japan.
In 2019, Terumo Blood and Cell Technologies reached $1 billion in revenue.
We employ nearly 7,000 associates globally, with global headquarters in Lakewood, CO, U.S., and regional headquarters in Brussels, Buenos Aires, Singapore and Tokyo.
We manufacture devices, disposable sets and solutions at our facilities in Belgium, India, Japan, Northern Ireland, the U.S. and Vietnam. Our global presence enables us to serve customers in more than 130 countries.
Our core values help set our direction, guide our actions and keep us true to our corporate mission of contributing to society through healthcare.
Respect – Appreciative of others
Integrity – Guided by our mission
Care – Empathetic to patients
Quality – Committed to excellence
Creativity – Striving for innovation
We contribute to the Leukemia and Lymphoma Society (LLS), raising $1.3 million USD since 2007.
We are proud to be an Equal Opportunity Affirmative Action Employer. All applicants will be afforded equal opportunity without discrimination because of race, color, religion, sex, gender identity or expression, sexual orientation, marital status, order of protection status, national origin or ancestry, citizenship status, age, physical or mental disability unrelated to ability, military status or an unfavorable discharge from military service.
Terumo Blood and Cell Technologies is committed to providing a safe, healthy and secure working environment. Our Colorado campus locations are tobacco-free workplaces, and we maintain a drug-free workplace and perform pre-employment substance abuse testing and detailed background verification.
Show more
Show less","Python, Java, C++, Data bricks, SQL, Scala, R Shiny, Azure data factory, Microsoft BI, cloudnative ETL/ELT tools, Cloud computing, Data visualization, Project management, Alteryx Server, Agile, ETL, ELT, Azure, AWS","python, java, c, data bricks, sql, scala, r shiny, azure data factory, microsoft bi, cloudnative etlelt tools, cloud computing, data visualization, project management, alteryx server, agile, etl, elt, azure, aws","agile, alteryx server, aws, azure, azure data factory, c, cloud computing, cloudnative etlelt tools, data bricks, elt, etl, java, microsoft bi, project management, python, r shiny, scala, sql, visualization"
Data Systems Engineer,Broadcom,"Fort Collins, CO",https://www.linkedin.com/jobs/view/data-systems-engineer-at-broadcom-3745769425,2023-12-17,Colorado,United States,Mid senior,Onsite,"Please Note:
If you are a first time user, please create your candidate login account before you apply for a job. (Click Sign In > Create Account)
If you already have a Candidate Account, please Sign-In before you apply.
Job Description:
Data Systems Engineer
Broadcom’s Fort Collins Colorado semiconductor operation is looking for the right person to join our team of software professionals and data systems engineers who will enhance our ability to deliver highly integrated and highly available manufacturing applications now and in the future. Successful candidates will have a record of sustained technical leadership, software development, data analysis, and professional growth.
Broadcom is a global provider of an extensive range of analog, mixed-signal and optoelectronic components and subsystems to more than 40,000 end customers, including many of the world’s top original equipment manufacturers. The Fort Collins Semiconductor wafer fabrication operation delivers RF filters, power amplifiers and front-end module semiconductor solutions to the wireless communications industry.
Fort Collins Colorado and surrounding communities continue to be recognized nationally as a great place to live. Broadcom offers a competitive compensation and benefits package.
Required Skills
Minimum BS in Software Engineering, Computer/Data Science, Statistics or related degree.
Minimum 5+ years background in statistics, data analysis, reporting, data modelling, and software engineering.
Excellent teamwork, communication, problem solving, customer service, and influencing skills.
Experience with Linux and Python 3.
Advanced experience using SQL (Oracle).
Experience using software revision control systems like Git.
Demonstrated project management, planning, and leadership experience.
Ability to work on site in Fort Collins, Colorado. This is not a remote work opportunity.
Preferred Skills
Experience with semiconductor manufacturing techniques
Experience with Java, C#, Javascript, HTML, CSS or similar libraries and frameworks
Statistical methods for manufacturing process control and data analysis
Manufacturing Execution Systems (MES) software design and development
Business process analysis and software requirements definition
Knowledge of semiconductor Test and Yield analysis principles
Additional Job Description:
Compensation And Benefits
The annual base salary range for this position is $78,000 - $130,000.
This position is also eligible for a discretionary annual bonus in accordance with relevant plan documents, and equity in accordance with equity plan documents and equity award agreements.
Broadcom offers a competitive and comprehensive benefits package: Medical, dental and vision plans, 401(K) participation including company matching, Employee Stock Purchase Program (ESPP), Employee Assistance Program (EAP), company paid holidays, paid sick leave and vacation time. The company follows all applicable laws for Paid Family Leave and other leaves of absence.
Broadcom is proud to be an equal opportunity employer. We will consider qualified applicants without regard to race, color, creed, religion, sex, sexual orientation, gender identity, national origin, citizenship, disability status, medical condition, pregnancy, protected veteran status or any other characteristic protected by federal, state, or local law. We will also consider qualified applicants with arrest and conviction records consistent with local law.
If you are located outside USA, please be sure to fill out a home address as this will be used for future correspondence.
Show more
Show less","Python 3, SQL (Oracle), Git, Linux, Java, C#, JavaScript, HTML, CSS, Semiconductor manufacturing techniques, Statistical methods for manufacturing process control and data analysis, MES software design and development, Business process analysis, Software requirements definition, Test and Yield analysis principles","python 3, sql oracle, git, linux, java, c, javascript, html, css, semiconductor manufacturing techniques, statistical methods for manufacturing process control and data analysis, mes software design and development, business process analysis, software requirements definition, test and yield analysis principles","business process analysis, c, css, git, html, java, javascript, linux, mes software design and development, python 3, semiconductor manufacturing techniques, software requirements definition, sql oracle, statistical methods for manufacturing process control and data analysis, test and yield analysis principles"
Data Architect and Developer,Creative Financial Staffing (CFS),"Centennial, CO",https://www.linkedin.com/jobs/view/data-architect-and-developer-at-creative-financial-staffing-cfs-3785718721,2023-12-17,Colorado,United States,Mid senior,Onsite,"Data Architect and Developer
About The Company
Global innovative manufacturing company!
Well established and stable company
Career growth opportunities
Full benefits package!
The team believes in providing customers with the best and most reliable solutions and products
Want to Serve the world one local market at a time!
Join a strong team that loves to mentor and drive a curiosity to learn
THIS IS AN ONSITE ROLE IN COLORADO/NO SPONORSHIPS,VISAS/ THIS IS 100% IN OFFICE
Day To Day
Participate and occasionally lead custom database and data architecture development solutions, AI, business intelligence, and integration with hardware, software, and data layers with database and software solutions
Expert in RDBMS, SQL, DML, DDL, and related software development layers including front end, back end, middle layers, and full stack development.
Lead, drive, and partake in SDLC methodologies related to the data and database portions including different degrees and combinations of Agile, spiral, pilot programs, and waterfall methods.
Subject Matter Expert leading integration of key business functions into architecture, design, development, testing, and implementation of full stack (UI Frameworks, Business Logic, and Database) software solutions
What You Will Bring
Experience with a variety of SLDC tools and software development languages that should include at least three or more of the following: Java (Spring framework)JavaScript, HTML5, CSS, Angular, React, Python, JSON, RESTfulservices, SQL, Graph QL, Business Intelligence tools, and/or AWS Lambda.
Expert knowledge of RDBMS, Oracle/SQL Server, Data Architecture, DML, DDL, and data integration.
Professional experience with connecting database frameworks on AWS services and Azure Dev/Ops, SQL Server
Ability to read and understand requirements and develop backend, front-end/UI, and integration (APIs) applications.
Experience in managing SDLC enabling a variety of concurrent multi-release software deployments
Bachelor’s Degree in Computer Science, MIS, BCS or related degree +8 years relevant experience
#INDEC2023
#CBDEC2023
Show more
Show less","Data Architect, Database Development, AI, Business Intelligence, SQL, RDBMS, DML, DDL, FrontEnd Development, BackEnd Development, FullStack Development, Agile, Spiral Model, Pilot Programs, Waterfall, UI Frameworks, Business Logic, Java, Spring Framework, JavaScript, HTML5, CSS, Angular, React, Python, JSON, RESTful Services, AWS Lambda, Oracle, Data Architecture, Data Integration, AWS Services, Azure DevOps, APIs, SDLC, MultiRelease Software Deployments, Computer Science, MIS, BCS","data architect, database development, ai, business intelligence, sql, rdbms, dml, ddl, frontend development, backend development, fullstack development, agile, spiral model, pilot programs, waterfall, ui frameworks, business logic, java, spring framework, javascript, html5, css, angular, react, python, json, restful services, aws lambda, oracle, data architecture, data integration, aws services, azure devops, apis, sdlc, multirelease software deployments, computer science, mis, bcs","agile, ai, angular, apis, aws lambda, aws services, azure devops, backend development, bcs, business intelligence, business logic, computer science, css, data architect, data architecture, data integration, database development, ddl, dml, frontend development, fullstack development, html5, java, javascript, json, mis, multirelease software deployments, oracle, pilot programs, python, rdbms, react, restful services, sdlc, spiral model, spring framework, sql, ui frameworks, waterfall"
Staff Database Engineer,Bluestaq,"Colorado Springs, CO",https://www.linkedin.com/jobs/view/staff-database-engineer-at-bluestaq-3721163607,2023-12-17,Colorado,United States,Mid senior,Onsite,"Position Type:
Full Time
Salary Range:
In compliance with Colorado's Equal Pay for Equal Work Act, the salary range for this role is
$106,017 - $159,754
. Final compensation for this role will be determined by various factors such as a candidate's relevant work experience, skills, certifications, and geographic location.
Location
: Colorado Springs, Colorado. This position is not remote.
Additional Compensation:
This position is eligible for an end-of-year bonus.
Employment Benefits:
Bluestaq offers a wide range of competitive benefits including Medical, Dental, Vision, Life Insurance, Disability Insurance, Paid Leave, and 401k. For more information regarding Bluestaq benefits, please visit https://www.bluestaq.com/bennies.
About Bluestaq
Bluestaq starts with our people.
Are you ready for the best job you've ever had?
At Bluestaq, we've rapidly built a culture around ownership, trust, community, and innovation. Bluestaq is dedicated to cultivating a growth environment where our team feels valued, works with a purpose of mission, is personally rewarded, and is professionally challenged.
Bluestaq is an emerging technology company founded in 2018 to streamline global operations using cutting-edge enterprise software and mission analytics. Bluestaq develops and transforms its enterprise systems and secures disparate data to solve our customers' most critical challenges. Today, Bluestaq primarily provides the U.S. Department of Defense and the nation's intelligence community with advanced software systems. Our name, Bluestaq, originates from ""blue"" a military term for friendly forces, and ""staq"" referring to a software stack.
Let's talk about our team's accolades in just four years! Inc. Magazine just recognized Bluestaq as ranking #630 out of the 5000 fastest-growing private companies in America! We've also been recognized as an Inc. Magazine's 2022 and 2021 Best Workplaces and are a 2021 Colorado Inno on Fire Company and a 2020 Colorado Company to Watch. Bluestaq was honored by the Colorado Techology Association as the 2019 Emerging Tech Company of the Year and named Colorado Springs Chamber of Commerce and EDC 2019 New Company of the Year. For the third consecutive year, Bluestaq landed on the Colorado Springs Gazette Best Workplaces in Colorado Springs (2019 - 2021) and Built In's Best Places to Work Lists: Best Places to Work in Colorado, Best Small Companies to Work for in Colorado, and Best Paying Companies in Colorado (2020 - 2022). Our team is award-winning and ahhhmazing!
Looking for a phenomenal quality of life? Colorado Springs was named the #2 Best Places to Live by U.S. News & World Report 2022, #2 Top 10 Cities Where Tech Strength Defies the Pandemic by LinkedIn, and one of the Top 10 Best-Performing Cities 2022 by the Milken Institute.
Why join Bluestaq in Colorado Springs?
Lifestyle Amenities - coffee shops, craft breweries, boutiques, locally-owned restaurants
Nature - more than 100 parks and ample outdoor sporting opportunities
Weather - Colorado Springs boasts over 300 days of sunshine a year
Laidback atmosphere - an accessible alternative to the faced-paced Denver lifestyle
Position Description
The Bluestaq Staff Database Engineer position is a fantastic opportunity for an engineer that loves the end-to-end ownership of the build, testing, and implementation of data warehousing solutions. This position is focused on data movement in and out of our enterprise data management platform. As an experienced and creative Bluestaq Database Engineer, you will collaborate with the engineering team to develop large-scale data architectures across multiple database technologies (SQL and NoSQL) and designs to support some of the country's most critical national security challenges. Are you ready to use your database experience to focus data movement through our software platforms?
What You Will Be Doing
Relational database design, data modeling, DDL, performance tuning, and extensive work with SQL
Work with data/row-level security features such as Postgres RLS.
Become familiar with and work on other NoSQL datastore technologies such as MongoDB, Amazon RDS, Amazon EMR, Amazon Athena
Design and implement large-scale ingest systems in a big data environment.
Develop custom solutions/code to ingest and exploit new and existing data sources.
Work with the Bluestaq development team to improve application performance.
Provide ETL experience simultaneously with enterprise search capabilities to unlock big data hurdles.
Monitor and troubleshoot performance issues in the data pipelines
Proactively identify issues and propose solutions with corresponding context/data.
Stay up to date with industry best practices and trends to apply to future software architectures.
Facilitates mentorship and healthy collaboration across the program.
Basic Qualifications And Skills
8+ years of database engineering experience with a Bachelor of Science degree OR Associate degree with 2 additional years of experience beyond the minimum required may be substituted in lieu of a degree OR a High School Diploma with 4 additional years of experience beyond the minimum required may be substituted in lieu of a degree.
Experience with and managing data across one of the following relational SQL databases: Postgres, MySQL, Oracle, SQL Server.
Experience with and managing data across a non-relational database such as MongoDB.
A general understanding of software development concepts.
Experience working in a Linux environment.
Knowledge of Common Table Expressions concepts.
Knowledge of Dynamic SQL concepts.
Familiarity and/or experience with NoSQL databases such Cassandra, HDFS, Redis, and Elasticsearch.
Experience with database security and best practices.
Scope work on complex projects involving multiple engineers and ensures the team's delivery.
Excellent verbal and written communication skills.
Display a strong sense of ownership for projects.
Required Education:
8+ years of software development and database engineering experience with a Bachelor of Science degree OR Associate degree with 2 additional years of experience beyond the minimum required may be substituted in lieu of a degree OR a High School Diploma with 4 additional years of experience beyond the minimum required may be substituted in lieu of a degree.
Clearance Requirement:
This position requires the ability to obtain a TS/SCI Clearance. To be eligible for a clearance, US Citizenship is required, and an employee must agree to participate in a background screen and credit check.
Relocation
: Relocation may be available for this position.
Bluestaq is an Equal Opportunity Employer. We prohibit unlawful discrimination against applicants or employees on the basis age 40 and over, color, disability, gender identity, genetic information, military or veteran status, national origin, race, religion, sex, sexual orientation, or any other status protected by state or local law.
Bluestaq will make reasonable accommodations for qualified individuals with known disabilities and employees whose work requirements interfere with a religious belief unless doing so would result in an undue hardship to Bluestaq or a direct threat. Employees needing such accommodation are instructed to contact Human Resources immediately at contact.us@bluestaq.com.
Show more
Show less","SQL, NoSQL, MongoDB, Amazon RDS, Amazon EMR, Amazon Athena, ETL, Linux, DDL, Relational databases, Nonrelational databases, Postgres, MySQL, Oracle, SQL Server, Cassandra, HDFS, Redis, Elasticsearch, Common Table Expressions, Dynamic SQL","sql, nosql, mongodb, amazon rds, amazon emr, amazon athena, etl, linux, ddl, relational databases, nonrelational databases, postgres, mysql, oracle, sql server, cassandra, hdfs, redis, elasticsearch, common table expressions, dynamic sql","amazon athena, amazon emr, amazon rds, cassandra, common table expressions, ddl, dynamic sql, elasticsearch, etl, hdfs, linux, mongodb, mysql, nonrelational databases, nosql, oracle, postgres, redis, relational databases, sql, sql server"
Staff Data Engineer,Recruiting from Scratch,"Denver, CO",https://www.linkedin.com/jobs/view/staff-data-engineer-at-recruiting-from-scratch-3744392348,2023-12-17,Colorado,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our client is a dating app.
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, deliver analysis to further improve engagement in existing features, and empower our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Solve technical problems of the highest scope and complexity
Exert significant influence on the company’s analytical long-range goals and data architecture
Define and extend our internal standards for style, maintenance, and best practices for a high-scale data platform
Provide mentorship for all on your team to help them grow in their technical responsibilities
Propose ideas to improve the scale, performance, and capabilities of the Data Platform
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We'll Love About You
7+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field
7+ years experience using Python
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention
Location
: This role will be located in office twice a week minimum in Palo Alto, San Francisco or Chicago.
Salary Range: $180,000 - 250,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Data engineering, Business intelligence, Data science, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, SQL, Kafka, Storm, SparkStreaming, Dimensional data modeling, Schema design, Data Warehouses, ETL, Data management tools, Data classification, Data retention, TDD, Pair Programming, Continuous Integration, Automated testing, Deployment","python, data engineering, business intelligence, data science, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, kafka, storm, sparkstreaming, dimensional data modeling, schema design, data warehouses, etl, data management tools, data classification, data retention, tdd, pair programming, continuous integration, automated testing, deployment","airflow, automated testing, business intelligence, continuous integration, data classification, data engineering, data management tools, data retention, data science, data warehouses, deployment, dimensional data modeling, docker, etl, helm, kafka, kubernetes, pair programming, python, schema design, snowflake, spark, sparkstreaming, sql, storm, tdd"
Treasure Data Sr Developer,NR Consulting,"Boulder, CO",https://www.linkedin.com/jobs/view/treasure-data-sr-developer-at-nr-consulting-3762818468,2023-12-17,Colorado,United States,Mid senior,Onsite,"Title : Treasure Data Sr Developer
Location : Remote
Duration : 6&plus; months (possible extension)
Job Description
A senior Treasure Data developer who has a sound understanding of Customer Data Platform (CDP) products with hands-on experience driving production implementation using the Treasure Data (TD) Platform
Needed Experience
Strong Data Engineering background with 5 to 10 years of experience
Proficient in configuring and implementing the TD Platform
Experience with configuring and using TD modules like Integration hub, and Audience Studio
Proficient in the use of TD Workflows for data integration and segmentation
Ability to work in multiple programming languages (such as Python)
Hands-on experience with AWS Cloud, Hive, Presto, and Real-time APIs
Nice To Have Skills
TD Certification
Knowledge of Treasure Data Boxes and engagement
Show more
Show less","Treasure Data, Customer Data Platform (CDP), Data Engineering, Data Integration, Data Segmentation, Python, AWS Cloud, Hive, Presto, Realtime APIs, Treasure Data Boxes, Engagement","treasure data, customer data platform cdp, data engineering, data integration, data segmentation, python, aws cloud, hive, presto, realtime apis, treasure data boxes, engagement","aws cloud, customer data platform cdp, data engineering, data integration, data segmentation, engagement, hive, presto, python, realtime apis, treasure data, treasure data boxes"
Sr Database Engineer,Russell Tobin,"Aurora, CO",https://www.linkedin.com/jobs/view/sr-database-engineer-at-russell-tobin-3724344252,2023-12-17,Colorado,United States,Mid senior,Onsite,"What are we looking for in our Sr Database Engineer?
Tittle: Database Engineer
Duration: 12+ months
Location: Remote
Job Description:
A Lot About You:
You must be accomplished in database design, engineering, operations, and high availability setups.
You must be proficient at least with one scripting language such as shell script/Perl/Python and must have worked in a Unix environment (preferably Linux).
UI Development skills are an added advantage. Multiple RDBMS environments like Oracle and SQL Server experience is a plus.
Responsibilities & minimum qualifications
:
B.S/M.S in Computer Science or a related technical field.
Database design for new features in existing products and new products
DevOps based model to deploy changes to massively complex systems.
Automation efforts for day to day ease of operations.
Must be familiar with MySQL 5.7 or above.
Server and query performance tuning.
You Must Have
Good knowledge of MySQL Architecture.
Work with Development teams to support new features in a DevOps model.
Knowledge of database implementation on cloud such as AWS.
Strong automation skills using Python, Perl, Ruby and Bash.
Comfortable with various backup/recovery for MySQL specific need.
Show more
Show less","Database design, Database engineering, High availability setups, Scripting languages (shell script/Perl/Python), Unix environment (Linux), UI Development, RDBMS environments (Oracle SQL Server), DevOps, Automation, MySQL 5.7 or above, Server performance tuning, Query performance tuning, MySQL Architecture, Cloud implementation (AWS), Python, Perl, Ruby, Bash, Backup/recovery","database design, database engineering, high availability setups, scripting languages shell scriptperlpython, unix environment linux, ui development, rdbms environments oracle sql server, devops, automation, mysql 57 or above, server performance tuning, query performance tuning, mysql architecture, cloud implementation aws, python, perl, ruby, bash, backuprecovery","automation, backuprecovery, bash, cloud implementation aws, database design, database engineering, devops, high availability setups, mysql 57 or above, mysql architecture, perl, python, query performance tuning, rdbms environments oracle sql server, ruby, scripting languages shell scriptperlpython, server performance tuning, ui development, unix environment linux"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Boulder, CO",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3744389960,2023-12-17,Colorado,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
They are a profitable dating application.
Location: This is a hybrid role based in their Chicago office and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location
: This role is twice a week in a hybrid role minimum.
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Python, SQL, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, Kafka, Storm, SparkStreaming, Data Warehouses, ETL, Hadoop, Data governance, Data security, Automation, Continuous delivery, TDD, Agile engineering practices, Pair Programming, Continuous Integration, Automated Testing, Deployment, R&D, Prototyping","python, sql, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, kafka, storm, sparkstreaming, data warehouses, etl, hadoop, data governance, data security, automation, continuous delivery, tdd, agile engineering practices, pair programming, continuous integration, automated testing, deployment, rd, prototyping","agile engineering practices, airflow, automated testing, automation, continuous delivery, continuous integration, data governance, data security, data warehouses, deployment, docker, etl, hadoop, helm, kafka, kubernetes, pair programming, prototyping, python, rd, snowflake, spark, sparkstreaming, sql, storm, tdd"
Senior Data Engineer,Arrow Search Partners,"Denver, CO",https://www.linkedin.com/jobs/view/senior-data-engineer-at-arrow-search-partners-3772516073,2023-12-17,Colorado,United States,Mid senior,Onsite,"About The Company
Our client is a multi-strategy hedge fund based in Denver, CO with over $3 billion in AUM. They are looking for an experienced Senior Data Engineer to join their growing Engineering team. This person will be leading and owning all things data. This is a unique opportunity to join a dynamic, technology-driven investment firm that provides a challenging and collaborative work environment. This opportunity will provide relocation compensation.
Responsibilities
Work closely with investment and technology professionals, designing, developing, and maintaining their data systems, while also building new
Develop, manage, and continuously improve technology infrastructure with a focus on database design
Contribute to driving the firm’s data platform vision
Work directly with research staff to clean, modify, and ensure data correctness
Understand current data governance policies and audit existing systems
Requirements
5+ years of data engineering experience
Strong experience in software engineering or equivalent project/coursework
Proficient in data Extract-Transform-Load (ETL) processes
Proficient in database design, administration, migrations, SQL, RDMS
Programming capabilities with Python or other languages
Experience with cloud computing services, specifically AWS, AWS Glue, AWS RDS, and AWS Aurora
Experience in security best practices/data governance for software and data platforms
Pluses
Experience working in investment/hedge fund or financial services
Experience with GraphQL
Experience working with financial data sets, managing OneTick and Bloomberg
Salary Range
$100,000-$140,000
Show more
Show less","Data Engineering, Software Engineering, Data ExtractTransformLoad (ETL), Database Design, Database Administration, SQL, RDMS, Python, Cloud Computing Services, AWS, AWS Glue, AWS RDS, AWS Aurora, Security Best Practices, Data Governance","data engineering, software engineering, data extracttransformload etl, database design, database administration, sql, rdms, python, cloud computing services, aws, aws glue, aws rds, aws aurora, security best practices, data governance","aws, aws aurora, aws glue, aws rds, cloud computing services, data engineering, data extracttransformload etl, data governance, database administration, database design, python, rdms, security best practices, software engineering, sql"
Data Engineer III (Remote),Physician Health Partners,"Denver, CO",https://www.linkedin.com/jobs/view/data-engineer-iii-remote-at-physician-health-partners-3727755942,2023-12-17,Colorado,United States,Mid senior,Remote,"Are you looking to work for a company that has been recognized for over a decade as a Top Place to Work? Apply today to become a part of a company that continues to commit to putting our employees first.
Salary Range:
$108,000-$135,000
Job Description Summary:
Job Description:
The Data Engineer III is a subject matter expert within the Software Development team and will support the business intelligence team by implementing new and maintaining existing data flows and dimensional models with a focus on consistent and accurate enterprise data solutions on premise and in the cloud.
Essential Functions:
Takes part in regular Scrum activities (daily standup meetings, weekly planning poker, post-sprint retrospectives, etc.)
Develops foundational patterns to be utilized by other members of the business intelligence team
Addresses issues in data flow logic and data model design, as well as the interoperability of new datasets with existing data models
Participates in and conducts code-reviews for all changes to the codebase and conveys coding standards clearly and concisely
Tests work on each assignment before working with Product Owners to ensure business requirements are fulfilled
Performance tunes ETL/ELT processes, queries, notebooks, and other data flows
Coordinates identification of requirements and recommending new data features in conjunction with development manager, product owners, and department managers
Acts as a subject matter expert by sharing information and providing support and training to others, as well as spearheading team projects and establishing goals and milestones for projects
Ensures goals and commitments to the team are met
Adheres to the company’s Compliance Program and to federal and state laws and regulations
Other duties as assigned
Knowledge, Skills and Abilities:
Extensive experience with the Databricks platform under Microsoft Azure (ADF, ADLS, Databricks Delta Lake, Databricks Unity Catalog, SQL Serverless, Auto Loader, etc.)
Advanced familiarity with on premise ETL frameworks, data modelling tools, relational database design, dimensional modelling, and SQL
Intermediate familiarity with the C# language
In-depth knowledge of data integration strategies and supporting technologies
In-depth knowledge of standards used for electronic data exchange (FHIR, HL7, CSV, EDI, etc.)
Ability to mentor others
Excellent verbal and written communication skills
Great customer service skills
Great teamwork and leadership skills
Independent problem-solving skills
Self-motivated and self-managed
Proficient in Microsoft Office Suite
Qualifications:
Bachelor’s degree in computer science, information systems, or equivalent work experience
7+ years working with the following concepts and technologies:
Relational and Dimensional Data Models, T-SQL, Microsoft SQL Server, etc.
3+ years working with the Databricks Platform under Azure, designing, and implementing a lake house architecture
Experience migrating on premise data warehousing solutions to Azure solutions (preferred)
Experience with Visual Studio and .NET technologies such as C# (preferred)
Experience working on an agile/scrum-driven software development team (preferred)
Experience working with common health care datasets (preferred)
Home office that is HIPAA compliant for all remote or telecommuting positions as outlined by the company policies and procedures
About Physician Health Partners:
PHP is an integrated team of physicians and health care professionals committed to supporting effective patient care throughout the health care continuum. PHP partners with Anthem, Centura Health, and Primary Physician Partners to create Colorado Community Health Alliance (CCHA) and meet the needs of Health First Colorado (Colorado’s Medicaid Program) members.
We believe that our employees are our biggest assets, so we offer a robust benefits package that is not limited to:
Company pays majority of medical, dental, and vision premiums for employees/dependents
HSAs, including employer contribution FSAs, including medical, limited purpose, and dependent care
Paid Time Off
401(k) matching
Short-Term and Long-Term Disability coverage
Tuition Reimbursement
Rewards and Recognition program
If you are interested in working for an innovative and collaborative workplace in the healthcare market, apply now!
Show more
Show less","Databricks, Azure ADF, ADLS, Databricks Delta Lake, Databricks Unity Catalog, SQL Serverless, Auto Loader, ETL frameworks, Data modeling tools, SQL, C#, Data integration strategies, FHIR, HL7, CSV, EDI, TSQL, Microsoft SQL Server, Relational and Dimensional Data Models, Visual Studio, .NET, Healthcare datasets, HIPAA, Agile, Scrum","databricks, azure adf, adls, databricks delta lake, databricks unity catalog, sql serverless, auto loader, etl frameworks, data modeling tools, sql, c, data integration strategies, fhir, hl7, csv, edi, tsql, microsoft sql server, relational and dimensional data models, visual studio, net, healthcare datasets, hipaa, agile, scrum","adls, agile, auto loader, azure adf, c, csv, data integration strategies, data modeling tools, databricks, databricks delta lake, databricks unity catalog, edi, etl frameworks, fhir, healthcare datasets, hipaa, hl7, microsoft sql server, net, relational and dimensional data models, scrum, sql, sql serverless, tsql, visual studio"
Sr. Edge Data Center Engineer,FastTek Global,"Oshkosh, WI",https://www.linkedin.com/jobs/view/sr-edge-data-center-engineer-at-fasttek-global-3784883006,2023-12-17,Saint Cloud,United States,Mid senior,Onsite,"The Sr. Edge Data Center Engineer is a mid-level technical role in the Digital Technology organization that designs, implements, and supports edge (remote) data centers and systems in the enterprise.
This role works as part of the DT Platform Services Facilities team that manages and supports 150+ edge facilities across all business segments.
This role collaborates with other infrastructure teams (server, storage, field services) as well as facilities (physical plant) extensively.
It is expected that this role will operate independently, solve complex problems, and fulfill requests with minimal supervision.
Required
Designs, implements, and supports edge (remote) data centers/server rooms and their systems across the enterprise.
Knowledge of server room power, space, and air handling requirements.
Detect and troubleshoot server room operational issues
Preferred:
Working knowledge of server and virtualization management.
Working knowledge of edge network technologies including LAN, WAN and Wi-Fi.
Ability to lead projects involving technology components of new site deployment, refresh, and migration
Your Impact
Provide day-to-day support for production sites and systems.
Create and maintain site drawings and documentation.
Perform assessments of power, space, and air handling requirements.
Provide proactive recommendations on areas of improvement.
Detect and troubleshoot operational issues.
Configure and manage facility monitoring solutions.
Collaborate with teams providing services such as network, storage, data protection & security.
Resolve incidents and complete assigned tasks.
Coordinate 3rd party support response for managed equipment
Oversee 3rd party technology and trades (i.e., construction) as necessary.
Maintain working knowledge of server and virtualization management.
Maintain a working knowledge of edge network technologies including LAN, WAN and Wi-Fi.
Perform server, storage, and network administration tasks as assigned.
Lead projects involving technology components of new site deployment, refresh, and migration.
Participate in an on-call rotation in order to provide 24/7 support and escalation.
Implement, enhance, and enforce server, storage, power and cooling standards and policies.
Document and follow standards, policies and procedures for change management. Perform
performance and capacity reporting on a regular basis.
Act with a sense of urgency to achieve business outcomes.
Your Skills And Expertise
Must be a US PERSON
Bachelor’s degree or equivalent experience and/or certifications.
Working knowledge of edge technologies including VMware, Windows Server, and Cisco networking.
Ability to interact with teams to build strong relationships at all levels and across all business units.
Strong understanding of the business impact of IT tools, technologies, and policies.
Excellent verbal, written and interpersonal communication skills, including the ability to communicate effectively with the IT organization, management, and business personnel.
Domestic and international travel up to 20%-30%
Standout Qualifications
:
Relevant industry recognized certifications
Experience with scripting and automation tools such as Ansible, Terraform, PowerShell, etc.
Information Technology experience within the manufacturing industry.
Additional Info
At FastTek Global,
Our Purpose
is
Our People
and
Our Planet
. We come to work each day and are reminded we are
helping people find their success stories
. Also,
Doing the right thing is our mantra
. We act responsibly, give back to the communities we serve and have a little fun along the way.
We have been doing this with pride, dedication and plain, old-fashioned hard work for
24 years
!
FastTek Global is financially strong, privately held company that is
100% consultant
and
client focused
.
We've differentiated ourselves by being
fast, flexible,
creative
and
honest
. Throw out everything you've heard, seen, or felt about every other IT Consulting company. We do unique things and we do them for Fortune 10, Fortune 500, and technology start-up companies.
Benefits
Our benefits are second to none and thanks to our
flexible benefit
options you can choose the benefits you need or want, options include:
Medical and Dental (FastTek pays majority of the medical program)
Vision
Personal Time Off (PTO) Program
Long Term Disability (100% paid)
Life Insurance (100% paid)
401(k) with immediate vesting and 3% (of salary) dollar-for-dollar match
Plus, we have a lucrative employee referral program and an employee recognition culture.
FastTek Global was named one of the
Top Work Places
in Michigan by the Detroit Free Press in
2013, 2014, 2015, 2016, 2017, 2018, 2019,
2020, 2021, 2022, and 2023!
To view all of our open positions go to: https://www.fasttek.com/fastswitch/findwork
Follow us on Twitter: https://twitter.com/fasttekglobal
Follow us on Instagram: https://www.instagram.com/fasttekglobal
Find us on LinkedIn: https://www.linkedin.com/company/fasttek
You can become a fan of FastTek on Facebook: https://www.facebook.com/fasttekglobal/
Show more
Show less","Edge Data Center Design, Server Room Management, Power and Cooling Standards, VMware, Windows Server, Cisco Networking, LAN, WAN, WiFi, Ansible, Terraform, PowerShell, Scripting Automation","edge data center design, server room management, power and cooling standards, vmware, windows server, cisco networking, lan, wan, wifi, ansible, terraform, powershell, scripting automation","ansible, cisco networking, edge data center design, lan, power and cooling standards, powershell, scripting automation, server room management, terraform, vmware, wan, wifi, windows server"
"Sr Engineer, Data Management and Analytics",Pacific Dental Services,"Mission Viejo, CA",https://www.linkedin.com/jobs/view/sr-engineer-data-management-and-analytics-at-pacific-dental-services-3743890970,2023-12-17,San Clemente,United States,Mid senior,Onsite,"Now is the time to join Wellfit Office. You will have opportunities to learn new skills from our team of experienced professionals. If you're ready to take your career to the next level and gain valuable experience, apply today!
The Senior Business Intelligence (BI) developer will create and manage BI analytic solutions that turn data into knowledge to drive PDS business decisions with actionable datasets, reports, and dashboards.
Job Description:
Professional who expertly designs, creates, and implements Self-Service BI data models.
Plan, implement and oversee comprehensive BI delivery strategy.
Design, build and deploy EPIC reporting solutions (Clarity, Caboodle, RWB, Radar, Metrics)
Innovator, adept at conceptualizing and developing marketable tools that cater to the needs and demands of multiple clients.
Build relationships with the business users, understand how data can solve business problems, and formulate business requirements for the BI tools.
Work with data warehouse System Analyst and Engineering team to implement data schema to accommodate new analytics requirements.
Manage meta data for BI models, reports, and dashboards (author, usage, data source)
Create technical and functional documentation and provide training for BI tools.
Required Skills:
Experience developing customer BI products including Epic Cogito BI stack and Microsoft Power BI
Proficient in SQL programming
Certification in Epic Clarity PB data model, Caboodle a preferred
A highly effective communicator, proficient in both written and verbal forms, with exceptional presentation abilities, enabling seamless conveyance of complex ideas.
In depth knowledge of relational databases, star schema, data lake, and data modeling
Experiences working with Data Warehouse
Expertise in data and business analysis
A true team asset; consistently exhibit a strong sense of ownership, commitment and accountability towards their tasks and responsibilities.
Strong problem-solving aptitude, debugging and issue resolution
Benefits:
Medical, dental, and vision insurance
Paid time off
Tuition Reimbursement
401K
Paid time to volunteer in your local community
Pacific Dental Services is an Equal Opportunity Employer. We celebrate diversity and are united in our mission to create healthier and happier team members.
$107,088.00-$180,169.00 / Annually
Show more
Show less","Power BI, Epic Cogito BI, SQL, Epic Clarity PB, Caboodle, EDI, Relational databases, Star schema, Data lake, Data modeling, Data warehouse, Data analysis, Business analysis, Problemsolving, Debugging, Issue resolution","power bi, epic cogito bi, sql, epic clarity pb, caboodle, edi, relational databases, star schema, data lake, data modeling, data warehouse, data analysis, business analysis, problemsolving, debugging, issue resolution","business analysis, caboodle, data lake, dataanalytics, datamodeling, datawarehouse, debugging, edi, epic clarity pb, epic cogito bi, issue resolution, powerbi, problemsolving, relational databases, sql, star schema"
Data Science Engineer,Zortech Solutions,"Irvine, CA",https://www.linkedin.com/jobs/view/data-science-engineer-at-zortech-solutions-3667475701,2023-12-17,San Clemente,United States,Mid senior,Onsite,"Role: Data Science Engineer
Location: Irvine, CA (Remote)
Duration: Fulltime
Job Description
Summary :
Develops core versions of software applications for sale to external clients. Identifies clients' purchasing requirements and technical specifications through Product Management and/or client representatives. Interacts with engineering groups to assist in the design changes to equipment and/or software. Trains and communicates with clients on systems application.
Responsibilities
Analyzes, designs, programs, debugs, and modifies software enhancements and/or new products used in local, networked, or Internet-related computer programs. Code must be used in the core version of applications available for sale commercially and not that which is customized to meet a specific client's implementation of the application.
Interacts with product managers and/or users to define system requirements and/or necessary modifications.
Participates in software design meetings and analyzes user needs to determine technical requirements.
Writes technical specifications based on conceptual design and stated business requirements.
Writes code, completes programming, and performs testing and debugging of applications using current programming languages and technologies.
Completes documentation and procedures for installation and maintenance.
Trains and communicates with internal Client Training, Client Relationship, and Sales teams on systems application.
May be responsible to develop or execute project plans, budgets, and schedules and for documenting work and results.
May deliver informational and decision-seeking presentations to technical and business groups to internal and/or external audiences.
Performs additional related duties as assigned.
Minimum Requirements
A Bachelor's degree in Computer Engineering, Computer Science or other related discipline; or equivalent combination of education and experience that is required for the specific job level.
Intermediate professional role. Proficient in at least two higher-level programming languages and knowledge of at least one systems development life cycle model. Understands the products, services, practices, regulations and operations associated with the assigned line of business. Conducts detailed analyses of all defined systems specifications for changes in systems requirements, business requirements or equipment configurations, and develops all levels of logic flow charts. Develops and prepares moderately complex computer programs, prepares program test data, tests and debugs programs. Documents all procedures used throughout the computer program when it is formally established. Receives general supervision and is competent in most phases of programming to work on own, and requires only some general direction for the balance of the activities. May assist and help train Entry-level software engineers. Typically requires five (5) or more years of software engineering work experience or an equivalent combination of education and experience.
Strong coding skills in Java is required.
Experience with Linux and bash scripting.
Experience in Relational Data Bases.
Preferred Qualifications
Coding skills on additional languages: Python, C++ or JavaScript is preferred.
Experience with distributed and Big Data computing tools: Spark, Hadoop/HDFS, NiFi, and Hive is desired.
Knowledge and practical experience on Microservices, Containers (e.g, Docker).
Experience with AWS services or equivalent.
Knowledge of financial services industry.
In-depth understanding of design patterns and their application.
Fluency in developing and understanding sequence diagrams, class models, etc.
Proficiency in solutions design and requirements definition disciplines leveraging model driven design based tools and techniques including conceptual solution component models, business process models, use cases, etc.
Outstanding verbal and written communication skills to technical and non-technical audiences of various levels in the organization (e.g., executive, management, individual contributors).
Excellent analytical, decision-making, problem-solving, team, and time management skills.
Ability to estimate work effort for project sub-plans or small projects and ensure the project is successfully completed.
Positive outlook, strong work ethic, and responsive to internal and external clients and contacts.
EEO Statement
Black Knight is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, disability, age, and protected veteran or military family status. Our employees' diversity is our strength, and when we embrace our differences, it makes us better and brighter. Black Knight's commitment to inclusion is at the core of who we are, and motivates us in how we do business each and every day.
Show more
Show less","Java, Linux, Bash scripting, SQL, Python, C++, JavaScript, Spark, Hadoop, HDFS, NiFi, Hive, Microservices, Containers, Docker, AWS, Design patterns, Model driven design, Sequence diagrams, Class models, Analytical skills, Decisionmaking skills, Problemsolving skills, Team management skills, Time management skills, Communication skills","java, linux, bash scripting, sql, python, c, javascript, spark, hadoop, hdfs, nifi, hive, microservices, containers, docker, aws, design patterns, model driven design, sequence diagrams, class models, analytical skills, decisionmaking skills, problemsolving skills, team management skills, time management skills, communication skills","analytical skills, aws, bash scripting, c, class models, communication skills, containers, decisionmaking skills, design patterns, docker, hadoop, hdfs, hive, java, javascript, linux, microservices, model driven design, nifi, problemsolving skills, python, sequence diagrams, spark, sql, team management skills, time management skills"
"Spark, Scala, Big data developer",Tata Consultancy Services,"Costa Mesa, CA",https://www.linkedin.com/jobs/view/spark-scala-big-data-developer-at-tata-consultancy-services-3758770899,2023-12-17,San Clemente,United States,Mid senior,Onsite,"Job Title
Spark, Scala, Big data developer
Relevant Experience
(in Yrs)
8 – 10 years
Technical/Functional Skills
Experienced engineer with hands on and strong coding skills, preferably with Java, Scala, Python.
AWS Cloud experience – S3, EFS, ECS, EMR, etc.
Knowledge of Hadoop/Spark and various data formats like Parquet, AVRO, CSV, etc.
Knowledge and experience with RDBMS (Aurora MySQL) and No-SQL (Cassandra) is a plus.
Great attention to details with analytical/curious mind.
Organizational and communication skills
Experience Required
8 – 10 years
Roles & Responsibilities
Perform large-scale data processing using Python and Apache Spark.
Utilize AWS services like S3, EMR, EC2 for data storage and processing.
Schedule, monitor, and troubleshoot data pipelines with Apache Airflow.
Ensure the data quality, observability, and integrity across all our data infrastructure.
Generic Managerial Skills
Good communication skills and management skills
Education
Show more
Show less","Apache Spark, Scala, Java, Python, AWS Cloud, S3, EFS, ECS, EMR, Hadoop, Parquet, AVRO, CSV, RDBMS, Aurora MySQL, NoSQL, Cassandra, Apache Airflow, EC2","apache spark, scala, java, python, aws cloud, s3, efs, ecs, emr, hadoop, parquet, avro, csv, rdbms, aurora mysql, nosql, cassandra, apache airflow, ec2","apache airflow, apache spark, aurora mysql, avro, aws cloud, cassandra, csv, ec2, ecs, efs, emr, hadoop, java, nosql, parquet, python, rdbms, s3, scala"
Senior Master Data Analyst,MicroVention-Terumo,"Aliso Viejo, CA",https://www.linkedin.com/jobs/view/senior-master-data-analyst-at-microvention-terumo-3637742902,2023-12-17,San Clemente,United States,Mid senior,Hybrid,"Job Description
Responsible for supporting financial master data set-up and providing technical solutions for issues related to the SAP Global Master Data Management which involves data maintenance, collection, validation, reporting and analysis. Assess and handle all kinds of material types, identify process improvements, and follow approved job aids. Job duties:
Work with business users on all master data management requests and research potential improvements to transaction, data handling and process developing job aids and procedures to enhance SAP usability and deliver value to users.
Evaluate new business requirements, issues and propose appropriate solutions.
Provide technical assistance to escalated issues identifying, evaluating, and developing systems and procedures.
Maintain and/or update bill of materials, routings, work center and production versions.
Execute all regular transaction processes necessary to maintain Operations Finance records and databases (e.g., SAP, MES, and Windchill).
Perform extracting, importing, and exporting of data in various database applications (e.g., SAP, JDE, PLM and Windchill).
Verify that all Global Master Data is setup completely and accurately in accordance with written and controlled procedures.
Develop, coordinate, and implement plans to test business and functional processes during system development.
Identify, recommend, and implement means to continuously improve department processes and Global Master Data.
Participate in software upgrades, design, data migration, implementation, functional and regression testing, and data validation to ensure quality solutions.
Comply with applicable Laws and Regulations, adhere to Quality Management System processes and requirements as well as demonstrate Ethics and Integrity in all matters and at all levels throughout the organization.
Perform additional duties as assigned.
Qualifications
Bachelor’s degree in Computer Science or, equivalent combination of education and experience sufficient to successfully perform the essential functions of the job.
Minimum five (5) years of relevant finance, engineering, or master data/ ERP analysis experience.
Minimum one (1) year of SAP full lifecycle implementation experience in a team environment.
Detailed-oriented.
Strong critical thinking and problem-solving skills.
Excellent written and verbal communication skills.
Ability to work with cross-functional partners.
Ability to juggle multiple tasks and deliverables.
Strong customer service mindset.
Ability to analyze and interpret incomplete or inconclusive information.
Solid working knowledge of Microsoft Office Suite.
Advanced knowledge of Microsoft Excel and creating complex spreadsheets.
Desired Qualifications
Previous experience in business/systems analysis.
Business background in Supply Chain Planning, Business Excellence, Production, or Operations is highly desired.
Experience in building strategic, impactful, business-driven insights from data.
Previous proven experience managing new systems implementations.
Advanced user of SAP transactions.
Posting Country
US - United States
Show more
Show less","SAP, SAP Global Master Data Management, Data maintenance, Data collection, Data validation, Data reporting, Data analysis, Process improvement, Job aids, Procedures, Transaction processing, Data handling, System development, Software upgrades, Data migration, Implementation, Regression testing, Data validation, Quality management, Ethics, Integrity, Computer Science, Finance, Engineering, Master data, ERP analysis, Microsoft Office Suite, Microsoft Excel, Business analysis, Supply Chain Planning, Business Excellence, Production, Operations, Strategic insights, System implementations","sap, sap global master data management, data maintenance, data collection, data validation, data reporting, data analysis, process improvement, job aids, procedures, transaction processing, data handling, system development, software upgrades, data migration, implementation, regression testing, data validation, quality management, ethics, integrity, computer science, finance, engineering, master data, erp analysis, microsoft office suite, microsoft excel, business analysis, supply chain planning, business excellence, production, operations, strategic insights, system implementations","business analysis, business excellence, computer science, data collection, data handling, data maintenance, data migration, data reporting, data validation, dataanalytics, engineering, erp analysis, ethics, finance, implementation, integrity, job aids, master data, microsoft excel, microsoft office suite, operations, procedures, process improvement, production, quality management, regression testing, sap, sap global master data management, software upgrades, strategic insights, supply chain planning, system development, system implementations, transaction processing"
"Sr, Data Analyst - REMOTE",RemoteWorker US,"Shrewsbury, MA",https://www.linkedin.com/jobs/view/sr-data-analyst-remote-at-remoteworker-us-3779915735,2023-12-17,Barre,United States,Mid senior,Onsite,"Overview GENERAL SUMMARY OF POSITION:
Under the general direction of the Manager or designee, the Senior Data Analyst will lead and develop data analyses for assigned projects and develop/deliver all related reports in a team environment. This position will provide information to leadership regarding data management systems and opportunities to enhance analyses, system integration and related databases. The Senior Data Analyst will utilize subject matter expertise as well as exceptional analytic and consulting skills with a customer service focus to design and deliver analytic products that meet or exceed client expectations. Responsibilities MAJOR RESPONSIBILITIES: Lead the development of and conduct analyses using a variety of data in a timely and accurate manner as required by the program/project Develop project-specific analytic plans, data manipulation and collection efforts; may include: human-service related data, health service data, critical incident data, and surveys involving clients, state agency personnel, and other key informants Manipulate and build large datasets from data sources to support analyses through use of MS Access and/or SAS/SQL code which will produce answers to complex analytic questions Create testing measures and/or algorithms to test and analyze large amounts of data, including data cleaning and quality assurance procedures Deploy complex qualitative and quantitative analysis techniques utilizing software packages such as SAS, SQL Provide oversight of data analysis by junior level team members; provide guidance and training as needed to help ensure accurate and timely completion of deliverables; provide QA on data for program/project integrity Oversee the design and development of data collection instruments, databases and procedure manuals based on thorough knowledge of data files, related databases, and systems integration (i.e., knowledge of study populations, types of claims data being analyzed) Design and implement data management and quality control procedures, including writing technical requirements and documentation of data sets Prepare and deliver data analysis reports. Develop and present summary reports, and oral and written presentations as requested by client, stakeholders and senior leaders to better inform decision making Provide interpretation of data to key stakeholders and leadership (including lay audiences); including identifying and reporting data patterns and abnormalities and presenting analyses of key metrics and trends Produce tabular and graphic summaries of analyses in a form suitable for inclusion in manuscripts for publication in peer reviewed scientific journals as well as for presentation at scientific meetings Communicate with all appropriate parties to ensure that the project’s objectives, scope, and expected deliverables of each project phase are understood. Also responsible for communicating project status, resource utilization, issues, and related information Review and monitor compliance with the federal and state regulations Perform other duties as required. Qualifications REQUIRED QUALIFICATIONS: Bachelor’s degree in a business, health sciences, or a related field; or equivalent experience 5 years of related experience; including at least 3 years of applied data analysis projects Advanced knowledge and use of SAS/SQL software Exceptional analytic and problem-solving skills Demonstrated experience/skills in appropriate computer, statistical or database applications Excellent attention to detail and time management/organization skills to manage multiple priorities Demonstrated ability to research issues and resources Demonstrated ability to write reports, journal articles and/ or other technical documents Excellent communication skills, both oral and written Interpersonal skills necessary to interact successfully with a wide range of individuals
Show more
Show less","Data Analysis, SAS, SQL, MS Access, Data Manipulation, Data Collection, Data Cleaning, Data Quality Assurance, Data Management, Data Interpretation, Data Visualisation, Report Writing, Project Management, Communication, Time Management, Problem Solving, Research","data analysis, sas, sql, ms access, data manipulation, data collection, data cleaning, data quality assurance, data management, data interpretation, data visualisation, report writing, project management, communication, time management, problem solving, research","communication, data cleaning, data collection, data interpretation, data management, data manipulation, data quality assurance, data visualisation, dataanalytics, ms access, problem solving, project management, report writing, research, sas, sql, time management"
Data Analytics Analyst - Level 4,Falcon Smart IT (FalconSmartIT),"Dover, DE",https://www.linkedin.com/jobs/view/data-analytics-analyst-level-4-at-falcon-smart-it-falconsmartit-3727975771,2023-12-17,Dover,United States,Mid senior,Onsite,"Job Title:
Data Analytics Analyst - Level 4
Location:
REMOTE
Job Type:
Contract
Job Description
Bachelor's degree in computer science, Information Technology, or a related field
7+ years of experience as an architect
5 years of experience implementing MDM solutions
Proven experience in designing and implementing MDM solutions in an enterprise environment.
Strong knowledge of MDM platforms and technologies (e.g., Reltio MDM, Informatica MDM, SAP Master Data Governance).
Understanding of data modeling, data integration, and data quality best practices.
Familiarity with data governance frameworks and methodologies.
Expertise in creating mappings, trust and validation rules, match & merge process.
Experience in creating and maintaining entity objects, hierarchies, entity types and relationship objects
Excellent analytical, problem-solving, and communication skills.
Own the logical unit of development work “end-to-end” from concept to production, covering each phase of the SDLC.
Strong project management skills and the ability to lead cross-functional teams.
Relevant certifications in MDM or data management are a plus.
Regards
Mark
Sr.IT Recruiter
Mark_US@falconsmartit.com
FalconSmartI
Show more
Show less","Data Analytics, Computer Science, Information Technology, MDM, Reltio MDM, Informatica MDM, SAP Master Data Governance, Data Modeling, Data Integration, Data Quality, Data Governance, Mappings, Trust and Validation Rules, Match & Merge Process, Entity Objects, Hierarchies, Entity Types, Relationship Objects, Analytical Skills, ProblemSolving Skills, Communication Skills, SDLC, Project Management, CrossFunctional Teams, MDM Certifications, Data Management Certifications","data analytics, computer science, information technology, mdm, reltio mdm, informatica mdm, sap master data governance, data modeling, data integration, data quality, data governance, mappings, trust and validation rules, match merge process, entity objects, hierarchies, entity types, relationship objects, analytical skills, problemsolving skills, communication skills, sdlc, project management, crossfunctional teams, mdm certifications, data management certifications","analytical skills, communication skills, computer science, crossfunctional teams, data governance, data integration, data management certifications, data quality, dataanalytics, datamodeling, entity objects, entity types, hierarchies, informatica mdm, information technology, mappings, match merge process, mdm, mdm certifications, problemsolving skills, project management, relationship objects, reltio mdm, sap master data governance, sdlc, trust and validation rules"
Data Science Data Support Engineer,ZoomInfo,"Vancouver, WA",https://www.linkedin.com/jobs/view/data-science-data-support-engineer-at-zoominfo-3741836760,2023-12-17,Effingham,United States,Associate,Hybrid,"At ZoomInfo, we encourage creativity, value innovation, demand teamwork, expect accountability and cherish results. We value your take charge, take initiative, get stuff done attitude and will help you unlock your growth potential. One great choice can change everything. Thrive with us at ZoomInfo.
About Our Team
Our mission is to maximize the value of ZI’s comprehensive B2B database through practical application of ML and Generative AI to solving real-world business challenges. To this end, the team has cultivated a powerful fusion of product thinking, ML, language models, and iterative methodologies which work together to deliver a robust suite of data products that inject data-driven intelligence into every facet of ZI. We actively research existing solutions and adapt them to fit our unique requirements, all while leaving ample room for creativity. We collaborate with other data scientists, product managers, ML engineers, subject-matter experts and key business stakeholders to form a cohesive unit dedicated to identifying critical gaps and defining effective approaches to solving the most valuable problems in our business. The result? An impressive array of both experimental and production-ready Data Science products that have a tangible impact on our customer’s ability to generate revenue with their Go-to-Market motion. Embrace the opportunity to join our accomplished team on this transformative journey, where innovation and teamwork thrive, and where your contributions will shape the future of data-driven excellence.
About The Job
As part of our Innovation Data-Science team, you will be working on delivering solutions for a variety of business problems:
Collaborate with Data Scientists to understand data requirements and translate them into effective data engineering solutions.
Design, implement, and maintain scalable and efficient data pipelines for data processing, transformation, and analysis.
Work with large volumes of structured and unstructured data from various sources, ensuring data quality and reliability.
Optimize and fine-tune data pipelines for performance, throughput, and reliability.
Develop and maintain ETL (Extract, Transform, Load) processes that enable seamless integration of data into analytical platforms.
Ensure data security, privacy, and compliance with relevant regulations throughout the data engineering lifecycle.
Setting up monitoring systems to track data pipeline performance, data quality, and data integrity. Performing data validation to ensure data accuracy and consistency.
Monitor and troubleshoot data pipeline issues, proactively identifying and resolving bottlenecks or inconsistencies.
Manage and scale the computing infrastructure required for robust and scalable machine learning workloads.
Create deployment templates and scripts for model deployment and rollback.
Explore and implement innovative technologies and tools to enhance data engineering capabilities.
Document data pipelines, processes, and best practices for knowledge sharing and team collaboration.
Stay current with industry trends and best practices in data engineering and related technologies.
About You
Bachelor's or Master's degree in Computer Science, Data Engineering, or a related field.
3+ years of experience as a Data Engineer, preferably in a Data Science or analytics-focused environment.
Proficiency in programming languages such as Python, Java, or Scala.
Experience with data integration tools and ETL frameworks (e.g., Apache Spark, Apache Beam, Talend, Airflow, dbt).
Strong SQL skills and experience working with relational and NoSQL databases.
Familiarity with data warehousing concepts and technologies (e.g., Snowflake, Redshift, BigQuery).
Understanding of data modeling, schema design, and data normalization/denormalization.
Knowledge of code version control systems (e.g., Git) and collaborative development practices.
Excellent problem-solving skills and the ability to work in a fast-paced, dynamic environment.
Strong communication skills to collaborate with cross-functional teams and translate technical concepts to non-technical stakeholders.
Preferred:
Experience with cloud platforms such as AWS, Azure, or Google Cloud.
Knowledge of DevOps practices and tools (e.g. Jenkins, Terraform).
Previous exposure to Machine Learning frameworks and concepts.
Contributions to open-source projects or data engineering communities.
The US base salary range for this position is $118,500 to $148,000 + variable compensation + benefits.
Actual compensation offered will be based on factors such as the candidate’s work location, qualifications, skills, experience and/or training. Your recruiter can share more information about the specific salary range for your desired work location during the hiring process.
We want our employees and their families to thrive. In addition to comprehensive benefits, we offer holistic mind, body and lifestyle programs designed for overall well-being. Learn more about ZoomInfo benefits here.
About Us
ZoomInfo (NASDAQ: ZI) is the trusted go-to-market platform for businesses to find, acquire, and grow their customers. It delivers accurate, real-time data, insights, and technology to more than 35,000 companies worldwide. Businesses use ZoomInfo to increase efficiency, consolidate technology stacks, and align their sales and marketing teams — all in one platform.
ZoomInfo may use a software-based assessment as part of the recruitment process. More information about this tool, including the results of the most recent bias audit, is available here.
ZoomInfo is proud to be an Equal Opportunity employer. We are committed to equal employment opportunities for applicants and employees regardless of sex, race, age, color, national origin, sexual orientation, gender identity, marital status, disability status, religion, protected military or veteran status, medical condition, or any other characteristic or status protected by applicable law. At ZoomInfo, we also consider qualified candidates with criminal histories, consistent with legal requirements.
Show more
Show less","Python, Java, Scala, Apache Spark, Apache Beam, Talend, Airflow, dbt, SQL, Snowflake, Redshift, BigQuery, Data modeling, Schema design, Data normalization, Data denormalization, Git, AWS, Azure, Google Cloud, DevOps, Jenkins, Terraform, Machine learning frameworks, Machine learning concepts","python, java, scala, apache spark, apache beam, talend, airflow, dbt, sql, snowflake, redshift, bigquery, data modeling, schema design, data normalization, data denormalization, git, aws, azure, google cloud, devops, jenkins, terraform, machine learning frameworks, machine learning concepts","airflow, apache beam, apache spark, aws, azure, bigquery, data denormalization, data normalization, datamodeling, dbt, devops, git, google cloud, java, jenkins, machine learning concepts, machine learning frameworks, python, redshift, scala, schema design, snowflake, sql, talend, terraform"
Data Analyst,Selby Jennings,"Columbia, MO",https://www.linkedin.com/jobs/view/data-analyst-at-selby-jennings-3784593317,2023-12-17,Effingham,United States,Associate,Hybrid,"* This role being referred to does not provide sponsorship or employment-related benefits
The Data Analyst In this position you are responsible for programming the collection and normalization of data; reviewing, analyzing and mapping data for reporting purposes in systems, supporting and implementing integrations and migrating data between systems. Will also provide basic DBA support.
ESSENTIAL FUNCTIONS
Lead database design, data collection, data analysis/comparison and data conversion in applications to be utilized.
Communicate with the appropriate internal and external personnel to understand their operational data needs and objectives; deliver actionable insights from the operational data
Lead end-to-end data migration and reporting efforts between systems and develop supporting documentation for data migration projects.
Define and document data verification criteria and ensure quality of all data conversion work.
Assist in the development and application of data architectural strategy.
Identify and retrieve information from various data sources and organizes it to create meaningful data visualizations for understanding, operational reporting and decision making.
Drive the development of processes supporting automation of the data extraction process, verification of data, data reconciliation and reporting.
Manage and optimize database inquiries for optimal end-user experience.
Analyze and advise management of workflow and data integrity issues; present resolution recommendations.
Identify, manage, troubleshoot and resolve database and data structure issues.
Support existing integrations and assist in development/setup of new integrations.
Other duties as assigned.
Qualifications
Required Qualifications
Three years of data analysis experience
Excellent communication and customer service skills
SQL programming and query expertise
Experience in dashboarding, reporting, visualizations, data modeling and migration between enterprise systems
EDUCATION QUALIFICATIONS
Bachelor’s degree required in Applied Math, Statistics, Computer Science or other related technical field
Show more
Show less","Data analysis, Data migration, Data modeling, Data visualization, Data extraction, Data reconciliation, Data integrity, SQL programming, Query expertise, Dashboarding, Reporting, Statistics, Computer Science","data analysis, data migration, data modeling, data visualization, data extraction, data reconciliation, data integrity, sql programming, query expertise, dashboarding, reporting, statistics, computer science","computer science, dashboard, data extraction, data integrity, data migration, data reconciliation, dataanalytics, datamodeling, query expertise, reporting, sql, statistics, visualization"
Senior Data Engineer,Plante Moran,"Michigan, United States",https://www.linkedin.com/jobs/view/senior-data-engineer-at-plante-moran-3707153809,2023-12-17,Effingham,United States,Associate,Hybrid,"Location: United States, Michigan
Location: United States, Colorado
Make your mark.
Plante Moran’s technology services team has been awarded
Insider Pro
and
Computerworld’s “100 Best Places to Work in IT”
for five consecutive years. We are also previous recipients of the
InformationWeek IT Excellence
award and the
CIO 100
award. If you’re seeking professional growth, like being innovative and challenged, and have a desire to work on impactful business technology solutions, we want to hear from you!
What does diversity, equity, and inclusion mean to Plante Moran? It means that all staff members have equitable and fair opportunities to succeed, in an inclusive environment, with their individual, unique identities.
Plante Moran enjoys a “Workplace for Your Day” model which, simply put, means no one knows the best place for you to physically work better than you. Our staff choose their work location based on their individual day’s needs, and gives them the flexibility they need when they need it. If staff need to go into the office to collaborate, the space is there, and if they have a day where they need to focus alone, then staying home works just fine, too.
We are looking for a Data Engineer to be a senior member of the Data Services team which includes all data warehouse, data engineering and visualizations, database operations, and data science capabilities. This role will be a team member on our data engineering team supporting all data integrations activities at Plante Moran.
Your role.
Your work will include, but not be limited to:
Develop new innovative Azure data technologies to design and develop innovative POC’s and Project to address complex business issues.
Develop data solutions, POC to address current and future information system needs, and create process improvements and controls for defined solutions
Works closely with customers and colleagues to identify opportunities to utilize information systems to improve business processes, promote the strategic use of information while enabling seamless access to information.
Interacts with the staff to produce data mapping and requirements, deliver high quality solutions utilizing Microsoft Data Integration Stack
Provides prototyping solutions, prepares test scripts, and conducts tests and for data replication, extraction, loading, cleansing, and data modeling.
Possesses working knowledge of Relational Database Management Systems (DBMS) and data warehouse front-end tools.
Be proficient in creating and maintain SQL data structures and store procedures.
Works closely with the technical and business team lead to drive solution options analysis, development and implementation of BI solutions.
Contributing team member to the design and support of data architecture, database design and integration, transformations, and load processes.
The Qualifications.
Associate degree required
Bachelor’s Degree preferred in computer science or related field preferred
Extensive Azure Data Technology design and implementation experience: ADF, Azure SQL, Azure Data Bricks, Azure Analysis Services, Data Lakes and Power BI
Informatica Experience (IICS) – Using Data integrator, Application Integration, Data Quality, Data Lineage
Experience creating data flows, executing stored procedures within SSIS
Experience setting up and troubleshooting SQL Agent Jobs
Azure Dev Ops and GitHub development management experience
SSMS and T-SQL Experience – Strong T-SQL Skills
3-5 years of experience using the Microsoft Azure Data integration tools including SSIS/IR, ADF, and ADO
Familiarity with database-centric applications
Proven experience with data warehousing and data modeling, specifically building/understanding Microsoft data storage structures
Proven ability to build out, enhance, and maintain the company data environment, including modifications and maintenance of various ETLs, transactional data sets, and company data assets.
Manage data access, data quality, and data delivery tasks within the framework of specific client BI and IT projects.
Ability to validate & ensure production level functionality/style is thoroughly tested, and the code reflects the current standards.
Preferred Qualifications
Problem Solver – Interested in finding solutions to/supporting existing solutions, as well as, continuing to advance through new development
Analytical approach to problem-solving; ability to use technology to solve business problems
Passionate about learning new technologies
Strong verbal and written communication skills; strong time-management and organizational skills
Our difference.
We are a nationally recognized public accounting, consulting, and wealth management firm, consistently ranked as one of FORTUNE magazine’s “100 Best Companies to Work For.” At Plante Moran, we live by the Golden Rule, fostering a relatively “jerk-free” culture with the lowest staff turnover rate in the industry. Our supportive network of well-rounded professionals is excited to catapult your growth and help pave your pathway to professional excellence.
Apply now. Make your mark.
Th is is an exempt p osition, so you may have to work hours that exceed the standard 40-hour work week.
Plante Moran is committed to a diverse workplace. We strive to create a culture where each person feels accepted and valued. We believe that each person’s ultimate potential begins with first acknowledging their inherent dignity. When we can recognize — and celebrate — our many human differences, we’re able to create a workplace where all staff feel a sense of belonging and an opportunity to succeed. This allows us to attract and retain the best talent, serve clients through diverse thinking, and better represent and support the various communities in which we live and work. Plante Moran is an Equal Opportunity Employer.
Plante Moran maintains a drug-free workplace.
Interested applicants must submit their resume for consideration using our applicant tracking system. Due to the high volume of applications received, only candidates selected for interviews will be contacted. Candidates must be legally authorized to work in the United States without sponsorship, with the exception of candidates that are bilingual in Japanese and English. Unsolicited resumes from search firms or employment agencies, or similar, will not be paid a fee and become the property of Plante Moran.
The specific statements above are not intended to be all-inclusive.
Show more
Show less","Azure, SQL, ADF, Azure SQL, Azure Data Bricks, Azure Analysis Services, Data Lakes, Power BI, Informatica, SSIS, TSQL, ADO, ETLs, GitHub","azure, sql, adf, azure sql, azure data bricks, azure analysis services, data lakes, power bi, informatica, ssis, tsql, ado, etls, github","adf, ado, azure, azure analysis services, azure data bricks, azure sql, data lakes, etls, github, informatica, powerbi, sql, ssis, tsql"
Data Analyst,"Kelly Science, Engineering, Technology & Telecom","Portage, MI",https://www.linkedin.com/jobs/view/data-analyst-at-kelly-science-engineering-technology-telecom-3780049533,2023-12-17,Effingham,United States,Associate,Hybrid,"Job Title: Senior Analyst, IS ERP
Location: Portage, MI, 49002
Type: ONLY on W2 contract (NO C2C)
Length: 12 months
Pay Rate – ($45 -50 per hour)
What you need:
Bachelor’s degree required in Computer Information Systems, Business Analytics, Data Science, or related specialty.
2+ year of related data analysis and reporting experience required.
PowerBI and Azure tools experience, connecting data models to a variety of technical sources of information preferred.
Experience with various tools and data sources such as Excel, Sharepoint, Oracle DB, SAP Hana DB, SQL Server, DB2 preferred.
B2B experience – using data to solve and predict business problems and provide meaningful insights
Experience writing and understanding SQL & Python/R preferred.
Background working directly with sales and marketing teams preferred.
Knowledge of demand management/forecasting tools, forecasting processes in sales, marketing operations, finance, manufacturing and supply chain issues preferred.
As the Sr. analyst IS ERSP you will provide the sales organization with information, content, and tools that increase sales productivity and effectiveness. You will be the key resource in creating, reporting and identify opportunities for our sales leaders in Emergency Care to drive sales performance.
Who We Want:
Strategic Thinkers: People who enjoy analyzing data or trends for the purposes of planning, forecasting, advising, budgeting, reporting, or sales opportunities.
Collaborative Partners: People who build and leverage cross-functional relationships to bring together ideas, data, and insights to drive continuous improvement in functions.
Self-directed initiators: People who take ownership of their work and need no prompting to drive productivity, change, and outcomes.
What you will do:
Provide data analytics, dashboards, and business intelligence to drive sales and marketing opportunities.
Own and manage the territory alignment process for the Emergency Care business, including micro and macro-economic data and customer penetration by product and zip code related data.
Develop and complete all sales rep and region quota assessment and measurement metrics, and develop materials to provide recommendations, assessment, and tracking.
Support ERP work stream, CRM implementation, and successful migration to Power BI.
Utilize sales tools as a super user to complete analysis and recommendations for territories.
Analysis of internal and external data sources to support sales strategies and programs.
Support monthly business reviews and highlights for the sales organization.
Partner with National Accounts and marketing teams to analyze and identify targeted sales opportunities and customer business reviews.
Liaison between commercial functions and IT/IS, Finance, and Market Development teams – ensuring that key requirements are met, and execution is accomplished in a timely fashion.
*** If this position may be interested to you, please email me back at
somp767@kellyservices.com
(with your most up to date resume in word format) and advise the best time and number at which you can be reached****
#P2
Show more
Show less","Power BI, Azure, Excel, Sharepoint, Oracle DB, SAP Hana DB, SQL Server, DB2, SQL, Python, R, Demand management, Forecasting, Data analytics, Dashboards, Business intelligence, CRM, ERP","power bi, azure, excel, sharepoint, oracle db, sap hana db, sql server, db2, sql, python, r, demand management, forecasting, data analytics, dashboards, business intelligence, crm, erp","azure, business intelligence, crm, dashboard, dataanalytics, db2, demand management, erp, excel, forecasting, oracle db, powerbi, python, r, sap hana db, sharepoint, sql, sql server"
Data Engineer,Zortech Solutions,"Fremont, CA",https://www.linkedin.com/jobs/view/data-engineer-at-zortech-solutions-3769287224,2023-12-17,Skagway,United States,Mid senior,Onsite,"Years of experience: 5 – 7 Years
Skills
Strong in SQL and Python
Data warehousing concepts , Data Modeling and RDBMS concepts
Good to have exposure to big data technologies – Apache Spark, Kafka, Airflow, NoSQL
Good to have exposure on Insurance & Finance business processes.
Show more
Show less","SQL, Python, Data warehousing, Data modeling, RDBMS, Apache Spark, Kafka, Airflow, NoSQL, Insurance, Finance","sql, python, data warehousing, data modeling, rdbms, apache spark, kafka, airflow, nosql, insurance, finance","airflow, apache spark, datamodeling, datawarehouse, finance, insurance, kafka, nosql, python, rdbms, sql"
DATA ENGINEER,Dollar General,"Goodlettsville, TN",https://www.linkedin.com/jobs/view/data-engineer-at-dollar-general-3580202862,2023-12-17,Skagway,United States,Mid senior,Onsite,"Dollar General Corporation has been delivering value to shoppers for more than 80 years. Dollar General helps shoppers Save time. Save money. Every day.® by offering products that are frequently used and replenished, such as food, snacks, health and beauty aids, cleaning supplies, basic apparel, housewares and seasonal items at everyday low prices in convenient neighborhood locations. Dollar General operates more than 18,000 stores in 47 states, and we’re still growing. Learn more about Dollar General at www.dollargeneral.com.
General Summary
Dollar General Corporation has been delivering value to shoppers for more than 80 years. Dollar General helps shoppers Save time. Save money. Every day.® by offering products that are frequently used and replenished, such as food, snacks, health and beauty aids, cleaning supplies, basic apparel, housewares and seasonal items at everyday low prices in convenient neighborhood locations. Dollar General operates more than 18,000 stores in 47 states, and we’re still growing. Learn more about Dollar General at www.dollargeneral.com.
Duties & Responsibilities
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and cloud technologies.
Build analytics tools that utilize the data pipelines to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Knowledge, Skills and Abilities
Knowledge of programming languages (e.g. Java and Python)
Hands-on experience with SQL database design
Great numerical and analytical skills
Degree in Computer Science, IT, or similar field; a Master’s is a plus
Data engineering certification (e.g IBM Certified Data Engineer) is a plus
Experience with big data tools Hadoop, Spark, Kafka, etc.
Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.
Experience with data pipeline and workflow management tools Azkaban, Luigi, Airflow, etc.
Experience with Snowflake/Azure cloud services EC2, EMR, RDS, Redshift
Experience with stream-processing systems Storm, Spark-Streaming, etc.
Experience with object-oriented/object function scripting languages Python, Java, C++, Scala, etc
Work Experience &/or Education
Degree in information technology or computer science with additional vendor-specific certification.
BS or MS degree in Computer Science or a related technical field
4+ years of Python or Java development experience
4+ years of SQL experience (No-SQL experience is a plus)
4+ years of experience with schema design and dimensional data modeling
Ability in managing and communicating data warehouse plans to internal clients
Experience designing, building, and maintaining data processing systems
Experience working with a cloud platform such as Snowflake / Azure or Databricks
Show more
Show less","SQL, Data engineering, Data pipeline management, Cloud services, Data visualization, Data modeling, Data processing, Python, Java, C++, Scala, Hadoop, Spark, Kafka, Postgres, Cassandra, Azkaban, Luigi, Airflow, Snowflake, Azure, EC2, EMR, RDS, Redshift, Storm, SparkStreaming","sql, data engineering, data pipeline management, cloud services, data visualization, data modeling, data processing, python, java, c, scala, hadoop, spark, kafka, postgres, cassandra, azkaban, luigi, airflow, snowflake, azure, ec2, emr, rds, redshift, storm, sparkstreaming","airflow, azkaban, azure, c, cassandra, cloud services, data engineering, data pipeline management, data processing, datamodeling, ec2, emr, hadoop, java, kafka, luigi, postgres, python, rds, redshift, scala, snowflake, spark, sparkstreaming, sql, storm, visualization"
Data Engineer,Laguna Games,"San Francisco, CA",https://www.linkedin.com/jobs/view/data-engineer-at-laguna-games-3548578312,2023-12-17,Skagway,United States,Mid senior,Onsite,"Laguna Games is the developer of Crypto Unicorns, the #1 NFT project on Polygon, and now one of the fastest-growing games. We are looking to hire an experienced Data Engineer to join our team. You are self-motivated, goal-orientated, and a strong team player. As a Data Engineer, you will be responsible for designing, building, and maintaining the data infrastructure that supports our gaming platform. You will work closely with our product, engineering, and data science teams to collect, process, and analyze large amounts of data generated by our gaming platform to inform our business decisions and improve user experience. You will work and innovate at the forefront of web3 gaming, bringing together unique technologies to deliver a new gaming experience.
As a Data Engineer at our Web3 Gaming Startup, Key Responsibilities:
Develop and maintain the data pipeline that ingests, processes and stores large amounts of data generated by our gaming platform
Design and build scalable data solutions that support the real-time analytics and reporting needs of the business
Collaborate with the product, engineering and data science teams to identify and implement data-driven solutions to improve user engagement, retention and monetization
Build and maintain data models, data warehouses and data marts to support business intelligence and reporting needs
Ensure data quality, integrity and security across all data sources and systems
Monitor and optimize data performance and scalability, and identify and resolve any data-related issues and bottlenecks
Keep up-to-date with the latest trends, tools and technologies in data engineering and apply them to improve our data infrastructure
Qualifications
Bachelor's degree in Computer Science, Computer Engineering, or related field
3+ years of experience in data engineering or related field
Experience with big data technologies such as Hadoop, Spark, Kafka, and Elasticsearch
Strong proficiency in SQL, Python and/or Java programming languages
Experience with cloud-based data solutions such as AWS, Azure or Google Cloud
Familiarity with data modeling, ETL/ELT processes, data warehousing, and business intelligence tools
Understanding of blockchain technology and its use in gaming platforms is a plus
Excellent communication skills, both written and verbal, and ability to collaborate with cross-functional teams
If you are passionate about data engineering and excited about the opportunity to work in a fast-paced and dynamic startup environment, we would love to hear from you!
Laguna Games is the developer of Crypto Unicorns, a new blockchain-based game centered around awesomely unique Unicorn NFTs that players use in a fun farming simulation and in a variety of exciting battle loops. As game developers, we are excited to move away from the extractive nature of Free-2-Play to foster and nurture community-run game economies. Crypto Unicorns is our first digital nation and we are extremely excited to build it in tandem with our player community!
We are headquartered in San Francisco, CA but operate as a remote company with locations around the world. We offer competitive compensation along with health benefits (medical, dental, and vision), 401k retirement savings, open paid time off, and a remote work support stipend.
Learn more here!
https://laguna.games
https://www.cryptounicorns.fun
Show more
Show less","Data Engineering, Python, SQL, Java, Hadoop, Spark, Kafka, Elasticsearch, AWS, Azure, Google Cloud, ETL/ELT processes, Data Warehousing, Business Intelligence, Data Modeling, Data Analytics, Cloudbased Data Solutions, Data Infrastructure, Blockchain Technology, Gaming Platforms","data engineering, python, sql, java, hadoop, spark, kafka, elasticsearch, aws, azure, google cloud, etlelt processes, data warehousing, business intelligence, data modeling, data analytics, cloudbased data solutions, data infrastructure, blockchain technology, gaming platforms","aws, azure, blockchain technology, business intelligence, cloudbased data solutions, data engineering, data infrastructure, dataanalytics, datamodeling, datawarehouse, elasticsearch, etlelt processes, gaming platforms, google cloud, hadoop, java, kafka, python, spark, sql"
Data Analyst Part Time,Voxmediallc,"Thunder Bay, Ontario, Canada",https://ca.linkedin.com/jobs/view/data-analyst-part-time-at-voxmediallc-3757244525,2023-12-17,Thunder Bay, Canada,Mid senior,Onsite,"Summary:
The Data Analyst Part Time will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : hiring@jobsai.live
Powered by Webbtree
Show more
Show less","Data analysis, Data mining, Statistical modeling, Hypothesis testing, A/B testing, SQL, R, Python, Tableau, Power BI, Data visualization, ETL processes","data analysis, data mining, statistical modeling, hypothesis testing, ab testing, sql, r, python, tableau, power bi, data visualization, etl processes","ab testing, data mining, dataanalytics, etl, hypothesis testing, powerbi, python, r, sql, statistical modeling, tableau, visualization"
Hybrid Work - Need Sr. Data Analyst in LakeVille MN,Steneral Consulting,"Lakeville, MN",https://www.linkedin.com/jobs/view/hybrid-work-need-sr-data-analyst-in-lakeville-mn-at-steneral-consulting-3703660896,2023-12-17,Minnesota,United States,Associate,Onsite,"Title- Sr. Data Analyst
Location-
Hybrid-Onsite 2-3 days/ week (Client is located in Lakeville, MN)
NO RELOCATION!! MUST BE LOCAL
to Minnesota
JD-
Job Description
Our client is seeking an experienced Data Analytics Specialist to join their team and play a pivotal role in contributing to their success. As a Data Analytics Specialist, you will collaborate closely with our clients and their various teams to understand and identify their data-related requirements, ensuring seamless incorporation into their work plan. Your expertise will be vital in providing valuable insights and contributing to the overall project plan. You will work hand-in-hand with project managers and the Information Management Office (IMO) to provide routine updates on progress, issues, and risk
Key Responsibilities
Collaborate with our client's business and IT teams to map and troubleshoot integrations, ensuring smooth data flow throughout their organization.
Produce data collection and load specifications, executing data loading processes with meticulous attention to accuracy and data integrity.
Demonstrate proficiency in mapping data from source systems to destination systems, including expert data cleansing techniques to maintain the highest data quality.
Support and facilitate the smooth transition of data into multiple target systems, enabling our client to utilize data effectively.
Conduct data cleansing, harmonization, and enrichment activities to enhance data quality and usability, ensuring our client's data is of the utmost quality.
Plan periodic and ongoing data refreshes for transactional and non-transactional data, keeping our client's insights up-to-date and relevant.
Contribute to planning and execution for multiple testing cycles, leading to the final conversion into production environments for our valued client.
Assist in the analysis and design of data-related integrations, providing valuable insights to optimize our client's data strategy.
Qualifications
5+ years of experience in data analytics, data engineering, or a similar role.
2+ years of experience working with data architecture principles and practices.
Proficiency in data analysis tools/programming languages such as Python, R, SQL, and similar, with data visualization experience.
Familiarity with tools and concepts related to data and analytics, such as dimensional modeling, ETL, data warehousing, and structured, unstructured, and transactional data.
Strong expertise in data validation and cleansing techniques, ensuring our client's data is accurate and reliable.
Demonstrated ability to collaborate effectively with our esteemed clients at all levels throughout their organization.
Logical problem-solving skills with the ability to present recommendations clearly in verbal, written, and graphical form.
Preferred Experience (not mandatory):
Experience in implementing Master Data Management (MDM) solutions.
Must HAVE Familiarity with Boomi and JDE.
Hands-on experience with agile and waterfall methodologies.
Show more
Show less","Data Analytics, Data Engineering, Data Architecture, Python, R, SQL, Dimensional Modeling, ETL, Data Warehousing, Data Validation, Data Cleansing, Master Data Management (MDM), Boomi, JDE, Agile, Waterfall","data analytics, data engineering, data architecture, python, r, sql, dimensional modeling, etl, data warehousing, data validation, data cleansing, master data management mdm, boomi, jde, agile, waterfall","agile, boomi, data architecture, data engineering, data validation, dataanalytics, datacleaning, datawarehouse, dimensional modeling, etl, jde, master data management mdm, python, r, sql, waterfall"
Lead Data Engineer,Ledelsea,"Minnesota City, MN",https://www.linkedin.com/jobs/view/lead-data-engineer-at-ledelsea-3777394457,2023-12-17,Minnesota,United States,Mid senior,Onsite,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, Ledelsea, is seeking the following. Apply via Dice today!
Role: Lead Data Engineer
The lead data engineer will design solutions and work with a team of engineers to ensure accurate, consistent, reliable, and sustainable solutions are delivered. The data engineer will also influence data engineering processes that result in highly accurate and trusted data assets for use in Truterra and across the organization. Additionally, the lead data engineer influences stakeholders to ensure investments in data driven solutions are centered on optimizing clearly defined and quantified business outcomes.
Required Education And Experience
Bachelor's degree in Computer Science, MIS, or related field and 7+ years of experience with or Associates Degree and 9+ years of experience
Applicable experience is characterized as advanced SQL, data engineering & data modeling techniques
Ability to function as a technical lead, working closely with developers and data analysts, as well as hands-on implementation
7+ years of experience building data integration solutions using tools like Informatica, Talend, Mulesoft, Qlik, etc.
Strong experience building out data warehouse and/or data lake
3+ years of experience leading engineering resources
2+ years of experience working with cloud-native data solutions on Microsoft Azure, AWS, or Google Cloud platform
Strong experience leading full lifecycle, large, complex reporting or data engineering efforts
Strong experience in working with heterogeneous datasets in building and optimizing data pipelines, pipeline architectures and integrated datasets using various data integration technologies (ETL/ELT, data replication/CDC, message-oriented data movement, API design, etc.)
Experience with DevOps, CI/CD pipelines and automated testing Required Competencies/skills
Implement data structures using standards and best practices in data modeling, ETL/ELT processes, SQL, database, and other technologies
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Thorough experience building and optimizing data pipelines and data sets.
Deep knowledge of the data vault method, model, and architecture
Ability to manage the overall data landscape: meta data, processing patterns, and data quality
A successful history of manipulating, processing and extracting value from large datasets.
Strong working knowledge of message queuing, stream processing, and highly scalable data stores
Ability to obtain a clear understanding of business needs and value, developing a detailed vision for the initiative, mapping out the solution, and guiding its implementation
Develop test-driven solutions that can be deployed quickly and in an automated fashion
Ability to obtain a clear understanding of business needs and value, developing a detailed vision for the initiative, mapping out the solution, and guiding its implementation
Demonstrated ability to collaborate across all levels (Engineers, Management, Architects, etc.) & across all skill sets (Data scientists, Data visualization developers, Salesforce developers etc.) particularly in a Product-oriented culture
Capable of using agile methodology and implementing Continuous Integration/Continuous Delivery (CI/CD) pipelines
Experience working with Databricks (or Spark) & Qlik technologies (Replicate and Compose)
Experience with any scripting languages, preferably Python
Experience working with BI Tools (Power BI, Tableau etc.) to create dashboards and reports
Experience with Snowflake
Experience in Manufacturing or Agriculture industry Preferred
Troubleshooting skills, ability to determine impacts, ability to resolve complex issues, and ability to exercise sound judgment and initiative in challenging situations.
Show more
Show less","Data Engineering, Data Modeling, SQL, Informatica, Talend, Mulesoft, Qlik, Data Warehousing, Data Lake, Microsoft Azure, AWS, Google Cloud Platform, DevOps, CI/CD, Data Vault, Message Queuing, Stream Processing, Scalable Data Stores, Python, Databricks, Spark, BI Tools, Power BI, Tableau, Snowflake, Agile Methodology, Continuous Integration, Continuous Delivery","data engineering, data modeling, sql, informatica, talend, mulesoft, qlik, data warehousing, data lake, microsoft azure, aws, google cloud platform, devops, cicd, data vault, message queuing, stream processing, scalable data stores, python, databricks, spark, bi tools, power bi, tableau, snowflake, agile methodology, continuous integration, continuous delivery","agile methodology, aws, bi tools, cicd, continuous delivery, continuous integration, data engineering, data lake, data vault, databricks, datamodeling, datawarehouse, devops, google cloud platform, informatica, message queuing, microsoft azure, mulesoft, powerbi, python, qlik, scalable data stores, snowflake, spark, sql, stream processing, tableau, talend"
Senior Data Engineer 1/Development Lead,NR Consulting,"Minnesota, United States",https://www.linkedin.com/jobs/view/senior-data-engineer-1-development-lead-at-nr-consulting-3762818444,2023-12-17,Minnesota,United States,Mid senior,Onsite,"Overall, 10 to 12 years of experience. Leads Data integration and automation requirements for Opportunity Analytics working with business and technical teams, business analyst(s) and Data Analyst(s). Reviews Technical Requirements for Data Integration Pipelines and frameworks, validates test plans and tests integration components for Opportunity Assessments MVP's and Analytics design. API development, Cloud deployment activities, DevOPs, CI/CD, Orchestration, Databricks, Snowflake and Python /Spark Experience. SAS and Health Care Payer Domain is desired. Ideally 2-3 years healthcare experience
Experience developing well documented, modular, testable code by setting maintaining/setting coding standards and data quality objectives
Experience successfully delivering and maintaining software solutions
Good Experience (5&plus; years) designing, developing, and implementing Cloud solutions, preferably Azure technologies.
Good Exposure to API (REST) web services development & deployment.
Must have Programming experience – Python, SQL, Spark
Must have Databricks experience
Good Experience (5&plus; years) on DevOps, CI/CD on cloud solution in Azure / AWS etc. and manage cloud infrastructure/container services like Kubernetes/ Docker etc.
Show more
Show less","Data Integration, Automation, Opportunity Analytics, Business Analysis, Data Analysis, Data Integration Pipelines, Technical Requirements Review, Test Plans Validation, Integration Components Testing, Opportunity Assessments MVP's, Analytics Design, API Development, Cloud Deployment, DevOps, CI/CD, Orchestration, Databricks, Snowflake, Python, Spark, SAS, Healthcare Payer Domain, Modular Coding, Testable Code, Data Quality Objectives, Software Solution Delivery, Software Solution Maintenance, Cloud Solution Design, Cloud Solution Development, Cloud Solution Implementation, Azure Technologies, API Development, REST Web Services, Python Programming, SQL Programming, Spark Programming, Databricks Experience, DevOps Experience, CI/CD Experience, Cloud Infrastructure Management, Container Services Management, Kubernetes, Docker","data integration, automation, opportunity analytics, business analysis, data analysis, data integration pipelines, technical requirements review, test plans validation, integration components testing, opportunity assessments mvps, analytics design, api development, cloud deployment, devops, cicd, orchestration, databricks, snowflake, python, spark, sas, healthcare payer domain, modular coding, testable code, data quality objectives, software solution delivery, software solution maintenance, cloud solution design, cloud solution development, cloud solution implementation, azure technologies, api development, rest web services, python programming, sql programming, spark programming, databricks experience, devops experience, cicd experience, cloud infrastructure management, container services management, kubernetes, docker","analytics design, api development, automation, azure technologies, business analysis, cicd, cicd experience, cloud deployment, cloud infrastructure management, cloud solution design, cloud solution development, cloud solution implementation, container services management, data integration, data integration pipelines, data quality objectives, dataanalytics, databricks, databricks experience, devops, devops experience, docker, healthcare payer domain, integration components testing, kubernetes, modular coding, opportunity analytics, opportunity assessments mvps, orchestration, python, python programming, rest web services, sas, snowflake, software solution delivery, software solution maintenance, spark, spark programming, sql, technical requirements review, test plans validation, testable code"
Lead Data Engineer (Java Programming)- US,Zortech Solutions,"Minnesota City, MN",https://www.linkedin.com/jobs/view/lead-data-engineer-java-programming-us-at-zortech-solutions-3736313397,2023-12-17,Minnesota,United States,Mid senior,Onsite,"Hello Professionals, We have urgent requirement for
Lead Data Engineer
Please have a look at the below job description, if interested please share your updated resume, email me, pavan@zortechsolutions.ca or share any References.
Role: Lead Data Engineer
Location: MN / CT-Day one onsite
Duration: C2C/Fulltime-No H1 transfers
Job Description
Ability to design and develop Azure framework which should be able to pull data from any type of unstructured/ structured sources of data, run transformation based on business rules and load in various types of targets
Candidate need strong Java programming and troubleshooting skills.
Expertise with Azure Databricks Services
Should Have
Should have BigData/ ETL experience.
Show more
Show less","Azure, Data Engineering, Java, ETL, BigData, Azure Databricks","azure, data engineering, java, etl, bigdata, azure databricks","azure, azure databricks, bigdata, data engineering, etl, java"
Data Security Engineer - Slack,Slack,"Minnesota, United States",https://www.linkedin.com/jobs/view/data-security-engineer-slack-at-slack-3781952813,2023-12-17,Minnesota,United States,Mid senior,Remote,"To get the best candidate experience, please consider applying for a maximum of 3 roles within 12 months to ensure you are not duplicating efforts.
Job Category
Software Engineering
Job Details
About Salesforce
We’re Salesforce, the Customer Company, inspiring the future of business with AI+ Data +CRM. Leading with our core values, we help companies across every industry blaze new trails and connect with customers in a whole new way. And, we empower you to be a Trailblazer, too — driving your performance and career growth, charting new paths, and improving the state of the world. If you believe in business as the greatest platform for change and in companies doing well and doing good – you’ve come to the right place.
Our Security team supports the unwritten fourth tenet of Slack’s mission: make people’s working lives more secure. We’re serious about protecting our infrastructure, operations, and most importantly, our customers’ data. We take a systemic approach to security, and strive to ensure we provide low friction high-impact security across everything we do.
As a member of the Slack Security Customer Protection team, you are the first line of detection of bad actors using Slack in unwanted and unexpected ways. As Slack’s data, customers, and features grow, protecting customers’ data from unwanted behaviors becomes an ever more important and challenging problem. The Security Customer Protection team develops and uses tooling to tease out high-quality signal from all the noise, to detect unwanted behaviors, such as abuse of users, workspaces, or tokens. Your work directly impacts the way millions of people, teams and businesses get things done.
Slack's API and web backend is built using PHP/Hack, and our backend services are written in Go. We use Airflow, Presto, Hive and Spark to interact with our data infrastructure.
Slack has a positive, diverse, and supportive culture—we look for people who are curious, inventive, and work to be a little better every single day. In our work together we aim to be smart, humble, hardworking and, above all, collaborative. If this sounds like a good fit for you, why not say hello?
What you will be doing
Creating tools for Slack and our customers to do proactive discovery and prevention of threat actors and unwanted activity in Slack
Work closely with Privacy, Policy, and Product teams to understand customers’ security needs and current priorities
Develop new dashboards to visualize and surface data for analysis and reporting
Develop and execute code to: modify data tables, automate database queries, surface and analyze logs, perform password resets
Manage investigations of abuse and discover new abuse cases, occasionally working with customers and legal entities
Bring to bear security-relevant data and log sources to surface unwanted activity in the customer space, both proactively and reactively
Use data and tools to understand and hunt for threats in the environment
Collaborate with product and features teams on security capabilities
Understand the underpinnings of how Slack works, and where bad actors could take advantage, to develop improved detective tools
Expose measurable data to internal and external partners to improve Slack’s ability to detect future threats
What you should have
4-6 years work experience with data analytics in a security or privacy environment, with a focus on customer-facing environments
Proficiency working with data technologies that power analytics (e.g. Airflow, Hive, Spark, Presto, Kafka, Pinot, MySQL or similar technologies)
Practical experience mining and cleaning large, unstructured datasets, then extracting meaningful and actionable insights, presenting results to an audience of various backgrounds
Experience understanding bad actors, threat intelligence, and abuse; involvement remediating abuse or security-related incidents is a plus
Experience with a high-level programming language such as Python or Go
Experience with Linux, Kibana, and engineering fundamentals at scale such as AWS, Chef, and Terraform
You have a Bachelor's degree in Computer Science, Engineering, Mathematics or a related field, equivalent training, fellowship, or work experience is required
A related technical degree is required
Accommodations
If you require assistance due to a disability applying for open positions please submit a request via this Accommodations Request Form.
Posting Statement
At Salesforce we believe that the business of business is to improve the state of our world. Each of us has a responsibility to drive Equality in our communities and workplaces. We are committed to creating a workforce that reflects society through inclusive programs and initiatives such as equal pay, employee resource groups, inclusive benefits, and more. Learn more about Equality at www.equality.com and explore our company benefits at www.salesforcebenefits.com .
Salesforce is an Equal Employment Opportunity and Affirmative Action Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender perception or identity, national origin, age, marital status, protected veteran status, or disability status. Salesforce does not accept unsolicited headhunter and agency resumes. Salesforce will not pay any third-party agency or company that does not have a signed agreement with Salesforce .
﻿Salesforce welcomes all.
For Colorado-based roles, the base salary hiring range for this position is $133,400 to $183,400.
Compensation offered will be determined by factors such as location, level, job-related knowledge, skills, and experience. Certain roles may be eligible for incentive compensation, equity, benefits. More details about our company benefits can be found at the following link: https://www.salesforcebenefits.com.
Show more
Show less","PHP, Hack, Go, Airflow, Presto, Hive, Spark, Linux, Kibana, AWS, Chef, Terraform, Python, Data Analytics, Threat Intelligence, Abuse, Security, Privacy, Engineering Fundamentals, Workspaces, Tokens","php, hack, go, airflow, presto, hive, spark, linux, kibana, aws, chef, terraform, python, data analytics, threat intelligence, abuse, security, privacy, engineering fundamentals, workspaces, tokens","abuse, airflow, aws, chef, dataanalytics, engineering fundamentals, go, hack, hive, kibana, linux, php, presto, privacy, python, security, spark, terraform, threat intelligence, tokens, workspaces"
Associate Data Science Analyst - CDH - Remote,Mayo Clinic,"Rochester, MN",https://www.linkedin.com/jobs/view/associate-data-science-analyst-cdh-remote-at-mayo-clinic-3784860855,2023-12-17,Minnesota,United States,Mid senior,Remote,"Why Mayo Clinic
Mayo Clinic is top-ranked in more specialties than any other care provider according to U.S. News & World Report. As we work together to put the needs of the patient first, we are also dedicated to our employees, investing in competitive compensation and comprehensive benefit plans – to take care of you and your family, now and in the future. And with continuing education and advancement opportunities at every turn, you can build a long, successful career with Mayo Clinic. You’ll thrive in an environment that supports innovation, is committed to ending racism and supporting diversity, equity and inclusion, and provides the resources you need to succeed.
Responsibilities
This position utilizes data analysis, data visualization, reporting, and data management skills. Supports projects under close supervision of senior data analysts or data scientists. Types of work include descriptive analytics, data query scripting, data analysis, data engineering and visualizations. Other responsibilities:
Builds and maintains dashboards in Tableau, Power BI, etc.
Provides data insights for business problems that can be approached with analytics techniques to collect, explore, and extract insights from structured and unstructured data.
Executes analytical procedures in the framework of a specific project work request.
Modifies scripts or software applications to support data management, data extraction and data analysis as required.
Contributes to the interpretation of data analysis and to writing reports.
May help customers understand the data set and provide training and suggestions for improvement on the data request.
Leverages communication and interpersonal skills and works with subject matter experts
Presents findings in easy to understand terms for the business or clinical practice.
This vacancy is not eligible for sponsorship/ we will not sponsor or transfer visas for this position.
Qualifications
Bachelor’s degree in a domain-relevant field such as engineering, mathematics, computer science, statistics, physics, data science, health science, or other analytical/quantitative field.Ability to develop predictive models to address various business problems through leveraging advanced statistical modeling, machine learning, or data mining techniques, is preferred. Demonstrated application of several problem-solving methodologies, planning techniques, continuous improvement methods, and analytical tools and methodologies (e.g. machine learning, statistical packages, modeling, etc.) is required. Incumbent must stay current on healthcare trends and enterprise changes. Interpersonal skills and time management skills are required. Requires strong analytical skills and a commitment to customer service.
Exemption Status
Exempt
Compensation Detail
$74,818 - $104,749/ year. Education, experience and tenure may be considered along with internal equity when job offers are extended.
Benefits Eligible
Yes
Schedule
Full Time
Hours/Pay Period
80
Schedule Details
M-F daytime hours
Weekend Schedule
NA
International Assignment
No
Site Description
Just as our reputation has spread beyond our Minnesota roots, so have our locations. Today, our employees are located at our three major campuses in Phoenix/Scottsdale, Arizona, Jacksonville, Florida, Rochester, Minnesota, and at Mayo Clinic Health System campuses throughout Midwestern communities, and at our international locations. Each Mayo Clinic location is a special place where our employees thrive in both their work and personal lives. Learn more about what each unique Mayo Clinic campus has to offer, and where your best fit is.
Affirmative Action and Equal Opportunity Employer
As an Affirmative Action and Equal Opportunity Employer Mayo Clinic is committed to creating an inclusive environment that values the diversity of its employees and does not discriminate against any employee or candidate. Women, minorities, veterans, people from the LGBTQ communities and people with disabilities are strongly encouraged to apply to join our teams. Reasonable accommodations to access job openings or to apply for a job are available.
Recruiter
Laura Percival
Show more
Show less","Tableau, Power BI, Data analysis, Data visualization, Data reporting, Data management, Descriptive analytics, Data query scripting, Data engineering, Machine learning, Statistical modeling, Data mining, Predictive modeling, Advanced statistical modeling, Statistical packages, Modeling, Healthcare trends, Enterprise changes, Customer service","tableau, power bi, data analysis, data visualization, data reporting, data management, descriptive analytics, data query scripting, data engineering, machine learning, statistical modeling, data mining, predictive modeling, advanced statistical modeling, statistical packages, modeling, healthcare trends, enterprise changes, customer service","advanced statistical modeling, customer service, data engineering, data management, data mining, data query scripting, data reporting, dataanalytics, descriptive analytics, enterprise changes, healthcare trends, machine learning, modeling, powerbi, predictive modeling, statistical modeling, statistical packages, tableau, visualization"
"Data Engineer, Data Platform",Grammarly,"Minnesota, United States",https://www.linkedin.com/jobs/view/data-engineer-data-platform-at-grammarly-3689966136,2023-12-17,Minnesota,United States,Mid senior,Hybrid,"Grammarly is excited to offer a
remote-first hybrid working model
. Team members work primarily remotely in the United States, Canada, Ukraine, Germany, or Poland. Certain roles have specific location requirements to facilitate collaboration at a particular Grammarly hub.
All roles have an in-person component: Conditions permitting, teams meet 2–4 weeks every quarter at one of Grammarly’s hubs in San Francisco, Kyiv, New York, Vancouver, and Berlin, or in a workspace in Kraków.
This flexible approach gives team members the best of both worlds: plenty of focus time along with in-person collaboration that fosters trust and unlocks creativity.
Grammarly team members in this role must be based in the United States or Canada, and they must be able to collaborate in person 2 weeks per quarter, traveling if necessary to the hub(s) where the team is based.
The opportunity
Grammarly is the world’s leading AI writing assistance company trusted by over 30 million people and 70,000 professional teams every day. From instantly creating a first draft to perfecting every message, Grammarly’s product offerings help people at 96% of the Fortune 500 get their point across—and get results. Grammarly has been profitable for over a decade because we’ve stayed true to our values and built an enterprise-grade product that’s secure, reliable, and helps people do their best work—without selling their data. We’re proud to be one of Inc.’s best workplaces, a Glassdoor Best Place to Work, one of TIME’s 100 Most Influential Companies, and one of Fast Company’s Most Innovative Companies in AI.
To achieve our ambitious goals, we’re looking for a Data Engineer to join our Data Engineering Platform team. This person will build highly automated, low latency core datasets that will help data engineers and end users across Grammarly to work with analytical data at scale.
Grammarly’s engineers and researchers have the freedom to innovate and uncover breakthroughs—and, in turn, influence our product roadmap. The complexity of our technical challenges is growing rapidly as we scale our interfaces, algorithms, and infrastructure. Read more about our stack or hear from our team on our technical blog.
Your impact
As a Data Engineer on our Data Engineering Platform team, you will:
Drive improvements to make our analytics effortless by creating and adjusting core data models and storage structures, all while understanding the needs of our users.
Make analytical data and metrics usable within a few minutes of real world events occuring, and build streaming processes for the output derived events and aggregate data.
Model structure, storage, and access of data at very high volumes for our data lakehouse.
Improve developer productivity and self-serve solutions by contributing components to our stream data processing framework(s).
Own data engineering's infrastructure-as-code for provisioning services that allow our engineers to deploy mature software installations within a few hours.
Build a world-class process that will allow our systems to scale.
Mentor other back-end engineers on the team and help them grow.
Build and contribute to AWS high-scale distributed systems on the back-end.
We’re Looking For Someone Who
Embodies our EAGER values—is ethical, adaptable, gritty, empathetic, and remarkable.
Is inspired by our MOVE principles, which are the blueprint for how things get done at Grammarly: move fast and learn faster, obsess about creating customer value, value impact over activity, and embrace healthy disagreement rooted in trust.
Is able to collaborate in person 2 weeks per quarter, traveling if necessary to the hub where the team is based.
Has experience with Python, Scala, or Java.
Has experience with designing database objects and writing relational queries
Has experience designing and standing up APIs and services.
Has experience with system design and building internal tools.
Has experience handling applications that work with data from data lakes.
Has at least some experience building internal Admin sites.
Has good knowledge of and at least some experience with AWS (or, alternatively, has deep expertise in Azure or GCE and is willing to learn AWS in a short time frame).
Can knowledgeably choose an open source or third-party service to accomplish what they need or, alternatively, can devise a quick and simple solution on their own.
Support for you, professionally and personally
Professional growth: We believe that autonomy and trust are key to empowering our team members to do their best, most innovative work in a way that aligns with their interests, talents, and well-being. We support professional development and advancement with training, coaching, and regular feedback.
A connected team: Grammarly builds a product that helps people connect, and we apply this mindset to our own team. Our remote-first hybrid model enables a highly collaborative culture supported by our EAGER (ethical, adaptable, gritty, empathetic, and remarkable) values. We work to foster belonging among team members in a variety of ways. This includes our employee resource groups, Grammarly Circles, which promote connection among those with shared identities, such as BIPOC and LGBTQIA+ team members, women, and parents. We also celebrate our colleagues and accomplishments with global, local, and team-specific programs.
Compensation And Benefits
Grammarly offers all team members competitive pay along with a benefits package encompassing the following and more:
Excellent health care (including a wide range of medical, dental, vision, mental health, and fertility benefits)
Disability and life insurance options
401(k) and RRSP matching
Paid parental leave
Twenty days of paid time off per year, eleven days of paid holidays per year, and unlimited sick days
Home office stipends
Caregiver and pet care stipends
Wellness stipends
Admission discounts
Learning and development opportunities
Grammarly takes a market-based approach to compensation, which means base pay may vary depending on your location. Our US and Canada locations are categorized into compensation zones based on each geographic region’s cost of labor index. For more information about our compensation zones and locations where we currently support employment, please refer to this page. If a location of interest is not listed, please speak with a recruiter for additional information.
Base pay may vary considerably depending on job-related knowledge, skills, and experience. The expected salary ranges for this position are outlined below by compensation zone and may be modified in the future.
United States
Zone 1: $167,000 - $242,000/year (USD)
Zone 2: $150,000 – $218,000/year (USD)
Zone 3: $142,000 – $206,000/year (USD)
Zone 4: $134,000 – $194,000/year (USD)
We encourage you to apply
At Grammarly, we value our differences, and we encourage all—especially those whose identities are traditionally underrepresented in tech organizations—to apply. We do not discriminate on the basis of race, religion, color, gender expression or identity, sexual orientation, ancestry, national origin, citizenship, age, marital status, veteran status, disability status, political belief, or any other characteristic protected by law. Grammarly is an equal opportunity employer and a participant in the US federal E-Verify program (US). We also abide by the Employment Equity Act (Canada).
Please note that EEOC is optional and specific to US-based candidates.
#NA
All team members meeting in person for official Grammarly business or working from a hub location are strongly encouraged to be vaccinated against COVID-19.
Show more
Show less","Java, Apache Kafka, AWS, APIs, Python, Scala, Data Lakes, Open Source Framework, Relational databases, Machine learning, Data mining, Big data analytics, Data visualization, Cloud computing, Data modeling, Streaming data processing, Data warehousing, Data engineering, Software engineering, System design, Technical writing, Communication skills, Problemsolving skills, Analytical skills, Attention to detail, Teamwork skills, Leadership skills","java, apache kafka, aws, apis, python, scala, data lakes, open source framework, relational databases, machine learning, data mining, big data analytics, data visualization, cloud computing, data modeling, streaming data processing, data warehousing, data engineering, software engineering, system design, technical writing, communication skills, problemsolving skills, analytical skills, attention to detail, teamwork skills, leadership skills","analytical skills, apache kafka, apis, attention to detail, aws, big data analytics, cloud computing, communication skills, data engineering, data lakes, data mining, datamodeling, datawarehouse, java, leadership skills, machine learning, open source framework, problemsolving skills, python, relational databases, scala, software engineering, streaming data processing, system design, teamwork skills, technical writing, visualization"
Software/Data Engineer 3 for Backend development,NR Consulting,"Minnesota, United States",https://www.linkedin.com/jobs/view/software-data-engineer-3-for-backend-development-at-nr-consulting-3762821117,2023-12-17,Minnesota,United States,Mid senior,Hybrid,"Develops Data integration and automation requirements for Opportunity Analytics and RWE automation working with business and technical teams, business analyst(s) and Data Analyst(s). Develops Technical Requirements for Data Integration Pipelines and frameworks, build test plans and tests integration components for Opportunity Assessments MVP's and Analytics design. API development, Cloud deployment activities, DevOPs, CI/CD, Orchestration, Databricks, Snowflake and Python /Spark Experience. SAS and Health Care Payer Domain is desired. Must have Programming skills – Python, SQL, Spark (4&plus; years)
At least 1 year1 of Databricks experience
At least 2 years of healthcare experience
At least 1 year of SAS programming experience
Nice to have:
Good Experience (4&plus; years) designing, developing, and implementing Cloud solutions, preferably Azure technologies.
Good Experience (3&plus; years) on DevOps on cloud solution in Azure / AWS etc. and manage cloud infrastructure/container services like Kubernetes/ Docker etc.
Overall, 6-8 years' experience
Experience with designing, configuring, and managing cloud infrastructure (Azure/Databricks)
Good Exposure to API (REST) web services development & deployment
Experienced in Docker container orchestration framework using Kubernetes
Deployments of microservices based applications
Proficient in using Docker for wrapping up final code and setting up Dev/Test environments
Hands on experience in Jenkins for CI/CD, Airflow
Good understanding of Azure
Experience on DevOps on cloud solution in Azure / AWS etc. and manage cloud infrastructure/container services like Kubernetes/ Docker etc.
Demonstrated ability in DevOps / Layers of support & operations, managing code deployments, fixes, updates, and related processes
Must have Programming skills – Python, SQL, Spark
Show more
Show less","Data integration, Automation, Opportunity Analytics, RWE, Technical Requirements, Data Integration Pipelines, Frameworks, Test Plans, Integration Components, MVP, Analytics Design, API Development, Cloud Deployment, DevOps, CI/CD, Orchestration, Databricks, Snowflake, Python, Spark, SAS, Health Care Payer Domain, Programming, SQL, Kubernetes, Docker, Microservices, Jenkins, Airflow, Azure","data integration, automation, opportunity analytics, rwe, technical requirements, data integration pipelines, frameworks, test plans, integration components, mvp, analytics design, api development, cloud deployment, devops, cicd, orchestration, databricks, snowflake, python, spark, sas, health care payer domain, programming, sql, kubernetes, docker, microservices, jenkins, airflow, azure","airflow, analytics design, api development, automation, azure, cicd, cloud deployment, data integration, data integration pipelines, databricks, devops, docker, frameworks, health care payer domain, integration components, jenkins, kubernetes, microservices, mvp, opportunity analytics, orchestration, programming, python, rwe, sas, snowflake, spark, sql, technical requirements, test plans"
Senior Data Engineer with SAS/Python Experience,NR Consulting,"Minnesota, United States",https://www.linkedin.com/jobs/view/senior-data-engineer-with-sas-python-experience-at-nr-consulting-3762820261,2023-12-17,Minnesota,United States,Mid senior,Hybrid,"Develops Data integration and automation requirements for Opportunity Analytics and RWE automation working with business and technical teams, business analyst(s) and Data Analyst(s). Develops Technical Requirements for Data Integration Pipelines and frameworks, build test plans and tests integration components for Opportunity Assessments MVP's and Analytics design. API development, Cloud deployment activities, DevOPs, CI/CD, Orchestration, Databricks, Snowflake and Python /Spark Experience. SAS and Health Care Payer Domain is desired. Must have Programming skills – Python, SQL, Spark (4&plus; years)
At least 1 year1 of Databricks experience
At least 2 years of healthcare experience
At least 1 year of SAS programming experience
Nice to have:
Good Experience (4&plus; years) designing, developing, and implementing Cloud solutions, preferably Azure technologies.
Good Experience (3&plus; years) on DevOps on cloud solution in Azure / AWS etc. and manage cloud infrastructure/container services like Kubernetes/ Docker etc.
Overall, 6-8 years' experience
At least 1 year of SAS experience
Experience with designing, configuring, and managing cloud infrastructure (Azure/Databricks)
Good Exposure to API (REST) web services development & deployment
Experienced in Docker container orchestration framework using Kubernetes
Deployments of microservices based applications
Proficient in using Docker for wrapping up final code and setting up Dev/Test environments
Hands on experience in Jenkins for CI/CD, Airflow
Good understanding of Azure
Experience on DevOps on cloud solution in Azure / AWS etc. and manage cloud infrastructure/container services like Kubernetes/ Docker etc.
Demonstrated ability in DevOps / Layers of support & operations, managing code
Show more
Show less","Data integration, Automation, Opportunity Analytics, RWE automation, Technical Requirements, Data Integration Pipelines, Frameworks, Test plans, Integration components, Opportunity Assessments MVP's, Analytics design, API development, Cloud deployment, DevOps, CI/CD, Orchestration, Databricks, Snowflake, Python, Spark, SAS, Health Care Payer Domain, Programming skills, SQL, Cloud solutions, Azure technologies, Kubernetes, Docker, Jenkins, Airflow, Azure","data integration, automation, opportunity analytics, rwe automation, technical requirements, data integration pipelines, frameworks, test plans, integration components, opportunity assessments mvps, analytics design, api development, cloud deployment, devops, cicd, orchestration, databricks, snowflake, python, spark, sas, health care payer domain, programming skills, sql, cloud solutions, azure technologies, kubernetes, docker, jenkins, airflow, azure","airflow, analytics design, api development, automation, azure, azure technologies, cicd, cloud deployment, cloud solutions, data integration, data integration pipelines, databricks, devops, docker, frameworks, health care payer domain, integration components, jenkins, kubernetes, opportunity analytics, opportunity assessments mvps, orchestration, programming skills, python, rwe automation, sas, snowflake, spark, sql, technical requirements, test plans"
Data Engineer IV - Max Digital (Data Operations),ACV Auctions,"Minnesota, United States",https://www.linkedin.com/jobs/view/data-engineer-iv-max-digital-data-operations-at-acv-auctions-3762878252,2023-12-17,Minnesota,United States,Mid senior,Hybrid,"If you are looking for a career at a dynamic company with a people-first mindset and a deep culture of growth and autonomy, ACV is the right place for you! Competitive compensation packages and learning and development opportunities, ACV has what you need to advance to the next level in your career. We will continue to raise the bar every day by investing in our people and technology to help our customers succeed. We hire people who share our passion, bring innovative ideas to the table, and enjoy a collaborative atmosphere.
Who we are:
ACV is a technology company that has revolutionized how dealers buy and sell cars online. We are transforming the automotive industry. ACV Auctions Inc. (ACV), has applied innovation and user-designed, data driven applications and solutions. We are building the most trusted and efficient digital marketplace with data solutions for sourcing, selling and managing used vehicles with transparency and comprehensive insights that were once unimaginable. We are disruptors of the industry and we want you to join us on our journey. ACV’s network of brands includes ACV Auctions, ACV Transportation, ClearCar, MAX Digital and ACV Capital within its Marketplace Products, as well as, True360 and Data Services.
At ACV we focus on the Health, Physical, Financial, Social and Emotional Wellness of our Teammates and to support this we offer:
Multiple medical plans including a high deductible health plan that costs $0 out of your paycheck
Company-sponsored (paid) Short-Term Disability, Long-Term Disability, and Life Insurance
Comprehensive optional benefits such as Dental, Vision, Supplemental Life/AD&D, Legal/ID Protection, and Accident and Critical Illness Insurance
Generous paid time off options, including vacation time, sick days, Company holidays, floating holidays, parental leave, bereavement leave, jury duty leave, voting leave, and other forms of paid leave as required by applicable law or regulation
Employee Stock Purchase Program with additional opportunities to earn stock in the Company
Retirement planning through the Company’s 401(k)
Who we are looking for:
We are seeking a highly skilled Engineer IV in Data Engineering with a strong foundation in computer science and excellent problem-solving skills. You will be responsible for maintaining and extending our database operations, optimizing SQL queries, and designing scalable data services.
What you will do:
Actively and consistently support all efforts to simplify and enhance the customer experience.
Maintain and extend (as required) existing database operations solution for backups, index defragmentation, data retention, etc.
Troubleshoot any SQL Server or ETL stack (SSIS, C#, Web APIs) outages during our operational support window.
Leverage monitoring tools to ensure high performance and availability; work with operations and engineering to improve as required.
Leverage DMVs and monitoring tools to ensure system performance; work with data operations and engineering to improve as required.
Ensure existing HADR (availability groups) solution is functional and meets requirements.
Support development, integration, and stage SQL Server environments for application development and data science teams.
Ensure that new database development meets company standards for readability, reliability, and performance. Work with internal teams on transactional and analytical schema design.
Collaborate with software and DevOps engineers to design scalable services, plan feature roll-out, and ensure high reliability and performance of our products.
Conduct code reviews, develop high-quality documentation, and build robust test suites.
Own the overall performance of products and services within a defined area of focus.
Respond-to and troubleshoot highly complex problems quickly, efficiently, and effectively. This may include being part of the emergency after-hours on-call rotation.
Mentor junior data engineers.
Perform additional duties as assigned.
What you will need:
Bachelor's degree in Computer Science, Information Technology, Computer Information Systems, Management Information Systems, or similar
5 years' building & supporting the database-tier of SaaS web applications.
Ability to read, write, speak, and understand English.
Expert in SQL Query optimization
ETL workflow implementation (SSIS, Airflow, C#, Python)
Experience working with Cloud Services (AWS RDS, S3, SQS, SNS)
Experience working with NoSQL data stores (e.g., MongoDB)
Experience developing Windows services in C#
Experience writing unit and integration testing
Expert SQL and data-layer development experience; OLTP schema design.
Experience using and integrating with cloud services, specifically: AWS RDS, S3, SQS, SNS.
Nice to Have
Experience with Airflow
Experience with DBT
Our Values
Trust & Transparency | People First | Positive Experiences | Calm Persistence | Never Settling
At ACV, we are committed to an inclusive culture in which every individual is welcomed and empowered to celebrate their true selves. We achieve this by fostering a work environment of acceptance and understanding that is free from discrimination. ACV is committed to being an equal opportunity employer regardless of sex, race, creed, color, religion, marital status, national origin, age, pregnancy, sexual orientation, gender, gender identity, gender expression, genetic information, disability, military status, status as a veteran, or any other protected characteristic. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you have a disability or special need that requires reasonable accommodation, please let us know.
For information on our collection and use of your personal information, please see our Privacy Notice.
Apply Now
Show more
Show less","SQL, ETL, SSIS, Airflow, C#, Python, AWS RDS, S3, SQS, SNS, MongoDB, Windows services, Unit testing, Integration testing, OLTP, DBT","sql, etl, ssis, airflow, c, python, aws rds, s3, sqs, sns, mongodb, windows services, unit testing, integration testing, oltp, dbt","airflow, aws rds, c, dbt, etl, integration testing, mongodb, oltp, python, s3, sns, sql, sqs, ssis, unit testing, windows services"
"Urgent requirement :: Data Analyst with Actimize Development experience :: Fulltime :: JERSEY CITY, NJ (Hybrid).",Apptad Inc.,"Jersey City, NJ",https://www.linkedin.com/jobs/view/urgent-requirement-data-analyst-with-actimize-development-experience-fulltime-jersey-city-nj-hybrid-at-apptad-inc-3780206395,2023-12-17,Westfield,United States,Associate,Onsite,"Position: Data Analyst with Actimize Development experience
Location: JERSEY CITY, NJ (Hybrid 3 Days/Week)
Duration: Full Time
Job Responsibilities
Hands-on experience in the development and/or development of Actimize based Surveillances
Utilize technology skills in Informatica, Oracle, Unix shell scripting, AUTOSYS scheduler
Communicate effectively to team manager
Deal with multiple deadline-driven, customer-sensitive projects and tasks
Manage time and prioritize tasks effectively
Communicate clearly and concisely verbally and in writing
Understand business requirements and convert them into technical problems
Write high level and low-level technical requirement documents
Work in a global technology development model
Have experience in Oracle SQL tuning, Autosys scheduling, Unix shell scripting
Job Description
3+ years of strong technology skills especially in Actimize (custom / in-house developed solution). Experience in Actimize Risk Case Manager, Actimize Visual Modeler, Actimize Risk Case Manager Designer, and Actimize Analytics Intelligence Server is a MUST
The ability to communicate effectively to team manager.
The ability to effectively deal with multiple deadline-driven, customer-sensitive projects and tasks.
Hands on experience in the development and/or development of Actimize based Surveillances.
Well versed with Oracle SQL tuning, Autosys scheduling, Unix shell, scripting
Experience in both Waterfall and Agile software development methodologies
Must Have
Informatica
Oracle
Actimize based Surveillances
Oracle SQL tuning
Autosys scheduling
Unix shell scripting
About Us
Apptad offers strategic consulting, enterprise information management and digital transformation services. With globally connected offices in US and India along with a team of trained and certified IT resources, Apptad ensures quick and effective delivery to its customers.Apptad is relentlessly reinventing the outlook of how companies leverage data.
With an effort to enable our customers the ability to solve biggest problems within their organization.We perceive our clients’ problems and respond with custom solutions instead of handing over boilerplate responses.
OUR MISSION
Customer Focus:
We listen carefully to the needs of our clients so that we know what’s important for their business and can design a customized solution for their business.
Innovation:
As a firm, we believe in constantly upgrading ourselves and improving our solutions to adapt to the changing landscape of technology.
Accountability and Ethics:
We believe in taking our commitments as seriously as our customers and living up to them while building trust for a long term business relationship.
Show more
Show less","Informatica, Oracle, Actimize based Surveillances, Oracle SQL tuning, Autosys scheduling, Unix shell scripting, Actimize Risk Case Manager, Actimize Visual Modeler, Actimize Risk Case Manager Designer, Actimize Analytics Intelligence Server, Waterfall, Agile","informatica, oracle, actimize based surveillances, oracle sql tuning, autosys scheduling, unix shell scripting, actimize risk case manager, actimize visual modeler, actimize risk case manager designer, actimize analytics intelligence server, waterfall, agile","actimize analytics intelligence server, actimize based surveillances, actimize risk case manager, actimize risk case manager designer, actimize visual modeler, agile, autosys scheduling, informatica, oracle, oracle sql tuning, unix shell scripting, waterfall"
Senior Data Engineer,NBCUniversal,New York City Metropolitan Area,https://www.linkedin.com/jobs/view/senior-data-engineer-at-nbcuniversal-3769561551,2023-12-17,Westfield,United States,Mid senior,Hybrid,"Our Direct-to-Consumer (DTC) portfolio is a powerhouse collection of consumer-first brands, supported by media industry leaders, Comcast, NBCUniversal and Sky. When you join our team, you'll work across our dynamic portfolio including Peacock, NOW, Fandango, SkyShowtime, Showmax, and TV Everywhere, powering streaming across more than 70 countries globally. And the evolution doesn't stop there. With unequalled scale, our teams make the most out of every opportunity to collaborate and learn from one another. We're always looking for ways to innovate faster, accelerate our growth and consistently offer the very best in consumer experience. But most of all, we're backed by a culture of respect. We embrace authenticity and inspire people to thrive.
In the Senior Data Engineer role, the Playback Analytics Engineering team at Peacock is looking for a passionate problem solver who’s looking to build the next generation of data pipelines and applications. You will be owning full segments of a data engineering project, managing expectations, and driving delivery with the appropriate stakeholders while maintaining the consensus for engineering architecture. As part of the Global Video Engineering organization, the PAE team collects client-side and server-side telemetry for Peacock and prepares it for analytics workloads.
Essential Functions (Responsibilities):
Design, build, and scale data pipelines across a variety of source systems and streams, distributed/elastic environments, and downstream applications.
Contributor to the overall Data Product roadmap by working closely with our business partners to understand their challenges and develop analytical tools to help drive business decisions.
Implement the appropriate design patterns while optimizing performance, cost, security, and scale and end user experience.
Maintain ownership of a given pipeline or domain and raise flags to senior leadership when appropriate regarding architectural concerns.
Provide mentorship to junior members of the engineering team to align to overall practice standards and approach.
Participate in development sprints, demos, and retrospectives, as well as release and deployment.
Build and manage relationships with supporting IT teams in order to effectively deliver work products to production.
Qualifications:
Bachelor’s degree in Computer Science or a relevant field or equivalent work experience
Minimum of five (5) years of data science, machine learning, advanced analytics experience in high velocity, high-growth companies
Strong coding skills in general purpose languages like Scala or Python, and familiarity with software engineering principles around testing, code reviews and deployment
Experience with
distributed data processing systems like Spark, and proficiency in SQL
Experience owning and managing a full pipeline within a large project, including interfacing with business stakeholders, architects and senior leadership to effectively communicate status and concerns.
Experience collaborating with and understanding the needs of stakeholders from a variety of business functions: Product, Engineering and Technical Operations
Familiarity with ML and MLOps concepts and technologies, such as model training, deployment, and monitoring.
Desired:
Analytical – You have experience in delivering data analytics solutions that promote data discovery.
Experience with Databricks, Snowflake, Amazon Web Services, or related cloud platforms a plus
Understanding of big data technology stacks (Hive / Spark etc.) is a plus.
Media-focused – Strong knowledge/passion for media including broadcast TV, digital, and film.
Communicator – You have excellent verbal and written skills with the ability to communicate ideas effectively across all levels of the organization, both technical and non-technical
Action-oriented – You're constantly figuring out new problems and are regularly showing results with a positive attitude, always displaying ethical behavior, integrity, and building trust.
Strong understanding of Agile principles and best practices
You’ve dealt with ambiguity and can make quality decisions in a dynamic, fast-paced environment.
This position is eligible for company sponsored benefits, including medical, dental and vision insurance, 401(k), paid leave, tuition reimbursement, and a variety of other discounts and perks. Learn more about the benefits offered by NBCUniversal by visiting the Benefits page of the Careers website.
Salary range: $115,000 - $145,000
Show more
Show less","Data Engineering, Data Pipelines, Data Analytics, Machine Learning, Advanced Analytics, Scala, Python, Spark, SQL, Databricks, Snowflake, Amazon Web Services, Hive, Agile, Software Engineering, Testing, Code Reviews, Deployment, Media, Broadcasting, Digital, Film","data engineering, data pipelines, data analytics, machine learning, advanced analytics, scala, python, spark, sql, databricks, snowflake, amazon web services, hive, agile, software engineering, testing, code reviews, deployment, media, broadcasting, digital, film","advanced analytics, agile, amazon web services, broadcasting, code reviews, data engineering, dataanalytics, databricks, datapipeline, deployment, digital, film, hive, machine learning, media, python, scala, snowflake, software engineering, spark, sql, testing"
Sr. Data Engineer,Chubb,"Whitehouse Station, NJ",https://www.linkedin.com/jobs/view/sr-data-engineer-at-chubb-3756335093,2023-12-17,Westfield,United States,Mid senior,Hybrid,"Job Description
We are looking for an experienced and motivated Senior ETL Developer to join our dynamic team. In this role, you will lead the delivery of key projects in support of North America financial reporting from the enterprise data warehouse and associated data marts, including the Business Analytics Repository (BAR). BAR is a strategic application within the business, feeding into multiple systems and applications both up and downstream with data that directly supports business decisions being made each and every day. You will be responsible for leading ETL development projects, coordinating with cross-functional teams to ensure project success, and creating and maintaining data integration solutions to meet business requirements. The ideal candidate will have experience with ETL development solutions such as AWS Glue, Google Dataflow, Azure Data Factory, Snowflake, Informatica/IICS and be able to identify and resolve data quality issues, performance bottlenecks, and other ETL-related problems.
Responsibilities
Lead ETL development projects and coordinate with cross-functional teams to ensure project success
Create and maintain data integration solutions to meet business requirements
Identify and resolve data quality issues, performance bottlenecks, and other ETL-related problems
Design and develop scalable ETL workflows and data pipelines using ETL tools such as Informatica/IICS
Ensure compliance with data governance and security policies
Develop and maintain documentation such as technical design documents, data lineage, and ETL runbooks
Mentor junior ETL developers and provide technical guidance to the team
Evaluate modern technologies and tools, and recommend solutions to improve ETL processes and performance
Contribute to the architecture, design, and development of data warehousing and business intelligence solutions
Understands data mapping and data modeling methodologies including normal form, star, and snowflake to reduce data redundancy and improve data integrity.
Participate in analysis, design, and ETL development as part of Agile development methodologies and provide status updates to the management
Maintains knowledge on current and emerging developments/trends for assigned area(s) of responsibility, assesses the impact, and collaborates with Scrum Team and Leadership to incorporate current trends and developments in current and future solutions
Qualifications
5 Year/bachelor’s degree or equivalent work experience (4 years of experience in lieu of Bachelors)_Minimum Required in Computer Science, Computer Information Systems, Information Systems, Information Technology or Computer Engineering or equivalent work experience
At least 5+ years of Strong understanding of ETL development concepts and tools such as ETL development solutions (e.g., AWS Glue, Google Dataflow, Azure Data Factory, Snowflake, Informatica/IICS)
Experience with Data Warehousing and Business Intelligence concepts and technologies
Strong knowledge of SQL and advanced programming languages such as Python and Java
Demonstrated critical thinking skills and the ability to identify and resolve data quality issues, performance bottlenecks, and other ETL-related problems
Experience with Agile methodologies and project-management skills
Excellent communication and interpersonal skills
Ability to mentor and provide technical guidance to junior ETL developers
Experience with cloud based environment required.
2+ years of experience in scheduling jobs using Autosys (or comparable distributed scheduler)
3+ years of experience writing Unix/Linux or Windows Scripts in tools such as PERL, Shell script, Python, etc.
3+ years of experience in creating complex technical specifications from business requirements/specifications
About Us
Chubb is the world’s largest publicly traded property and casualty insurer. With operations in 54 countries, Chubb provides commercial and personal property and casualty insurance, personal accident and supplemental health insurance, reinsurance, and life insurance to a diverse group of clients. The company is distinguished by its extensive product and service offerings, broad distribution capabilities, exceptional financial strength, underwriting excellence, superior claims handling expertise and local operations globally.
At Chubb, we are committed to equal employment opportunity and compliance with all laws and regulations pertaining to it. Our policy is to provide employment, training, compensation, promotion, and other conditions or opportunities of employment, without regard to race, color, religious creed, sex, gender, gender identity, gender expression, sexual orientation, marital status, national origin, ancestry, mental and physical disability, medical condition, genetic information, military and veteran status, age, and pregnancy or any other characteristic protected by law. Performance and qualifications are the only basis upon which we hire, assign, promote, compensate, develop and retain employees. Chubb prohibits all unlawful discrimination, harassment and retaliation against any individual who reports discrimination or harassment.
Show more
Show less","ETL development, Data integration, Data quality, Data governance, Data security, Documentation, Mentoring, Agile methodologies, Project management, SQL, Python, Java, Autosys, Unix/Linux, Windows, PERL, Shell script","etl development, data integration, data quality, data governance, data security, documentation, mentoring, agile methodologies, project management, sql, python, java, autosys, unixlinux, windows, perl, shell script","agile methodologies, autosys, data governance, data integration, data quality, data security, documentation, etl development, java, mentoring, perl, project management, python, shell script, sql, unixlinux, windows"
Data Engineer,SDG Group USA,"Bedminster, NJ",https://www.linkedin.com/jobs/view/data-engineer-at-sdg-group-usa-3783582076,2023-12-17,Westfield,United States,Mid senior,Hybrid,"Who Are We?
SDG Group is a global consulting firm specialized in Data & Analytics. We are committed to unlocking organizations’ potential and hidden value by offering in-depth analytics expertise that empowers our clients’ business models to become successful data-driven enterprises.
Innovation is in our DNA. We constantly innovate our value proposition with cutting-edge laboratories and transformational models to provide the ultimate analytics practices and solutions.
SDG Group works on the pillars of a Triple Expertise, incorporating business process skills by domain with a technological knowledge and a partnership with the most important and innovative software providers. Our portfolio also features cutting-edge laboratories and the development of
Advanced Analytics & AI technologies.
Why Join Us?
We’re looking for problem solvers, innovators, and collaborators who want to be a part of the mission to help us to further grow our company! You will have the ability to learn the latest technologies in the market and build strong relationships with technical and business leaders, all
while deepening your expertise and enhancing your skills. We will work with you to create customized career plans which will allow you to grow independently, as well as with your colleagues. If you value inclusivity and want to join a culture that empowers you to show up as your authentic self, join us. Our goal is to move through the interview process in a timely manner while staying highly communicative. We are committed to ensuring that you are equipped with a
comprehensive understanding of SDG Group and the role you have applied to.
About the Data Engineer role
If you have 3+ years of experience implementing data analytics solutions and would like to take your career to the next level within a rapidly growing and dynamic organization, join the SDG family! SDG is a company full of opportunities and committed to the success of our customers and
the development of our employees. We are looking for smart, hardworking candidates who
excel in fast-paced, results oriented environments. We look for candidates that take initiative and have a strong sense of responsibility. The candidate must be able to work as a team, communicate openly and honestly and be interested in taking on growing responsibilities within the company.
What you will be doing
· Lead projects in complex environments ensuring timely delivery of quality solutions
· Architect innovative solutions to modern data architecture challenges facing enterprises today including:
· Migration to cloud solutions
· Processing massive amounts of data efficiently and with optimal performance
· Automating complex integrations involving many systems
· Modelling data structures according to business needs that follow industry best practice
· Supporting analytics and data visualization technologies and designing solutions to support them
What We Look For
· Relevant Bachelor’s Degree (Computer Science, Engineering, Mathematics, Physics, Business Technology, etc.)
· 3+ years experience with Relational Database
Management Systems (RDBMS)
· 3+ years of Business Intelligence / Analytics related work experience in challenging environments
· Strong understanding of modern data modelling techniques
· Strong understanding of Cloud services providers (AWS, Google, Microsoft) and how to architect solutions around them
· Experience working with modern Business Intelligence, Analytics or Data Management technologies and a history of recommending good practices and developing innovative and sustainable solutions
· Outstanding oral and written communication skills
· Willingness to travel to clients throughout the NJ, NY and PA area
Desired
· Experience building solutions with cloud data warehousing platforms like Snowflake or Amazon Redshift
· Experience working in the Pharmaceuticals, Manufacturing, Consumer Products or Retail industry
· Possesses qualities such as entrepreneurial, ambitious, proactive, detail oriented, collaborative.
Benefits of Working With Us
· Full-time employment in an exciting and growing high-tech industry with a competitive salary
· Professional career with opportunities for personal and professional growth and rapid career advancement based on meritocracy
· A creative and innovative work environment with Smart working options (flexible working hours, good mix of on-site and remote)
· Robust Healthcare and Benefits package including Medical, Dental, Vision, Disability coverage and various other benefit options
· 401k Retirement plan with Employer match
· 3+ weeks paid vacation and flexible holiday schedule
Show more
Show less","Cloud services, Relational Database Management Systems, Data visualization, Data modeling, Mathematics, Business Intelligence, Analytics, Software development, Enterprise resource planning, Problemsolving, Data warehousing, Communication skills, AWS, Microsoft, Google, Snowflake, Amazon Redshift","cloud services, relational database management systems, data visualization, data modeling, mathematics, business intelligence, analytics, software development, enterprise resource planning, problemsolving, data warehousing, communication skills, aws, microsoft, google, snowflake, amazon redshift","amazon redshift, analytics, aws, business intelligence, cloud services, communication skills, datamodeling, datawarehouse, enterprise resource planning, google, mathematics, microsoft, problemsolving, relational database management systems, snowflake, software development, visualization"
Data Engineering Manager,Vivante Health,"Chicago, IL",https://www.linkedin.com/jobs/view/data-engineering-manager-at-vivante-health-3780556450,2023-12-17,Wheaton,United States,Mid senior,Remote,"Job Title:
Data Engineering Manager
Remote – US and British Columbia Only
About Vivante
Vivante Health is an innovative startup reinventing the way chronic conditions are managed. We’re filling the unmet needs of people with chronic conditions that are invisible, neglected or stigmatized, starting with digestive disease.
Why digestive? Because an astonishing 70 million people in the US are affected—that’s twice the number with diabetes and more than many other chronic conditions combined. Unlike other chronic conditions, though, digestive diseases often go untreated or misdiagnosed…because of stigma.
At Vivante, we think it’s time to bring digestive health to the forefront while providing REAL solutions to the millions who are struggling and don’t know where to turn. Our health management ecosystem, GIThrive, empowers people to spend less time worrying about their digestive symptoms and more time living life.
GIThrive works with our members (patients) to help identify and manage their digestive health conditions with personalized insights & recommendations. Our platform also facilitates interactions with our world-class clinical team to blend our technology with real person-to-person relationships that support each patient throughout their individual journeys.
With a remote-first workforce, backed by leading digital health investors, we’re changing the way healthcare is delivered.
Position Summary
As a Data Engineering Manager at Vivante Health, you will play a crucial role in leading the strategy and execution for our data engineering organization. Your team will ""own"" our Data Warehouse, collaborating closely with our Data Science and Software Engineering teams to enhance our GIThrive product, and supporting the operational data needs of our Business Intelligence, Finance and Clinical Operations teams. There are many external data sources to incorporate, including patient health records, lab results, medical claims, and other population health data. The technology stack leverages GCP's Data Analytics and AI/ML services, along with DBT, Airflow and other open source data engineering standards. Expertise in these areas is a plus, but having familiarity with similar technologies can be fine substitutes.
Why is this a great opportunity for a Data Engineering Manager?
The problem domain and our approach to solving it is super compelling. We're bringing together data sources that have never been joined, building models of a GI patient that have never been built, and discovering new GI health insights that help real people to manage and support their real medical conditions.
It's still early enough that there’s a lot of decisions and new discoveries to be made, so this is your chance to get in early and help shape our future.
Other things about our environment you should be aware of:
We're still a small company, which means that everyone still wears a lot of hats and we need engineers who are comfortable with ambiguity and working across a wide variety of challenges. That said, our existing engineering team has built an extremely efficient working environment based on best-in-class hosted SaaS frameworks that greatly minimize the overhead in deploying and supporting software in production.
We're all really passionate about improving clinical outcomes for our members and truly advancing the GI health space. Most of us also have personal (direct/family/friend) connections to GI health problems. Whatever it is that fuels you, we're on a mission to build something much larger than ourselves here and looking for another partner in driving that mission forward.
Responsibilities Include:
Technical Leadership: Provide strong technical leadership and vision for the Data Engineering organization, ensuring alignment with the company's strategic goals.
Technical Strategy: Collaborate with product management and other stakeholders to define and prioritize the technical roadmap, ensuring alignment with business objectives.
Organization Maturation: Implement processes, tools, and practices to mature the team, including establishing clear career growth paths for all team members.
Staffing: Lead staffing efforts as the team continues to grow
Execution: Drive the successful execution of data engineering projects across multiple teams, including planning, resource allocation, and project management.
Quality Assurance: Grow and foster our adoption of SQA best practices to ensure high-quality deliverables.
Cross-Functional Collaboration: Work closely with cross-functional teams (Product/Clinical/ClientSuccess/etc..) and communicate effectively to distill technical details into business terms
Innovation: Encourage a culture of innovation and experimentation, staying abreast of industry trends and emerging technologies to drive product innovation.
Risk Management: Identify and mitigate technical risks and challenges, making data-driven decisions to achieve project success.
Continuous Improvement: Monitor and drive improvements to organizational performance
You'll be a good fit here if you are:
A team player. You like collaborating closely with other engineering and product managers
Empathetic to our members’ GI health conditions and are driven to improve their outcomes
Security oriented. We take the stewardship of our customer's healthcare data seriously, and take no shortcuts to protect it
Comfortable with a distributed workforce. We interact with each other via video chat, Slack, and other collaboration tools
A self motivated, creative problem solver
We are proud of the team culture that we foster today, which is extremely friendly and supportive while constantly reaching to raise our own standards of engineering. We're really excited about what we're building, and usually having fun building it together. If all of that sounds fun to you too, we'd love to meet you.
Desired Qualifications:
Proven experience (5+ years in Data or other Software Engineering with 2+ years in Data Engineering leadership roles), with a track record of successfully delivering complex software projects.
Exceptional leadership and team-building skills, with the ability to motivate, mentor and inspire technical teams
Excellent communication and interpersonal skills, with the ability to collaborate effectively with cross-functional teams, business partners, and vendors alike
Strong problem-solving and decision-making abilities.
Demonstrated experience building and supporting Data Warehouse solutions
An addiction to automated testing
Bonus:
Experience in a startup environment
Experience with medical, healthcare and/or population health data. Double bonus for familiarity with OHDSI, FHIR and/or other healthcare data standards
Experience working in a HIPAA regulated environment (or other regulated industry) and supporting data security and privacy controls
Familiarity with Google Cloud Platform (GCP) and/or Cloud Native ecosystems
Bachelor's or Master's degree in Computer Science, Data/Software/Other Engineering, or related field (or equivalent experience)
Vivante Health is an equal opportunity employer.
We believe safe spaces where everyone can be their authentic selves is the key to a successful team so we welcome and embrace all identities, cultures, and backgrounds.
Interested? Apply here
Show more
Show less","Data Engineering, Data Warehouse, Data Science, Software Engineering, Data Analytics, AI/ML, DBT, Airflow, GCP, SaaS, SQA, FHIR, OHDSI","data engineering, data warehouse, data science, software engineering, data analytics, aiml, dbt, airflow, gcp, saas, sqa, fhir, ohdsi","aiml, airflow, data engineering, data science, dataanalytics, datawarehouse, dbt, fhir, gcp, ohdsi, saas, software engineering, sqa"
JAVA / Sr Cloud Data Developer - REMOTE,Addison Group,"Chicago, IL",https://www.linkedin.com/jobs/view/java-sr-cloud-data-developer-remote-at-addison-group-3575205813,2023-12-17,Wheaton,United States,Mid senior,Remote,"LOCATION REMOTE
SALARY 140K + BENEFITS + Stability and a great work life balance
NO VISA TRANSFERS AT THIS TIME -
MUST BE LOCATED IN THE US & ABLE TO CST HOURS
Looking for role to grow and take on additional responsibility while being part of the organizations Digital Transformation? Our client is seeking a Sr Cloud Developer to join their team and be groomed to be the their technical lead. Responsible for the technical delivery, enhancement, and maintenance of multiple custom-built Java/J2EE web applications on-premises and in the cloud. THIS ROLE IS 100% REMOTE
Essential Job Responsibilities & Duties
Perform the analysis, architecture, design, development, and implementation of Distributed Systems applications using Web based, Java, Java EE, Web Services, MS SQL Server, Security, authentication, and authorization technologies.
Migrate on-premises corporate applications and related data to the cloud.
Collaborate with different IT functional departments including Operations, DevOps, and QA.
Improve architecture by studying emerging technologies, evaluating their applicability to operational requirements.
Work on the deploy of cloud applications following defined architectures and leadership guidelines.
Technical Skills
1+ year of experience developing cloud solutions or training in Azure or AWS.
Min of 4+ years experience developing for enterprise n-tier technical environments involving Java/Java EE, security, application server architecture.
Expertise with REST services (message queues experience is a plus).
Experience in large scale systems involving reporting, transactional systems, and integration with other enterprise systems APIs.
Min 4 years experience with JavaEE containers and/or Docker.
Experience in analysis and design of web-based client facing applications using at least one of the following development frameworks: Spring, JSF, JavaScript, and/or XML Beans.
Source control management experience using tools such as SVN or Git.
Commitment and understanding of DevOps best practices such as CI/CD deploying using tools such Jenkins, Maven, GitHub repos.
Relevant relational database experience using MS SQL Server and/or Oracle, Hibernate, and JDBC.
Comfortable using Eclipse or IntelliJ IDE.
PHP and MySQL experience would be a plus.\
Education, Licenses And/or Certifications
Bachelor's degree in Engineering, Computer Science, or Information Systems or equivalent.
Show more
Show less","Cloud solutions development (Azure AWS), Java Java EE, Software architecture, Web Services, MS SQL Server, Security, Authentication, Authorization, DevOps, Reporting systems, Transactional systems, Enterprise systems APIs, JavaEE containers, Docker, Spring, JSF, JavaScript, XML Beans, Maven, GitHub, SQL Server, Oracle, Hibernate, JDBC, Eclipse, IntelliJ, PHP, MySQL","cloud solutions development azure aws, java java ee, software architecture, web services, ms sql server, security, authentication, authorization, devops, reporting systems, transactional systems, enterprise systems apis, javaee containers, docker, spring, jsf, javascript, xml beans, maven, github, sql server, oracle, hibernate, jdbc, eclipse, intellij, php, mysql","authentication, authorization, cloud solutions development azure aws, devops, docker, eclipse, enterprise systems apis, github, hibernate, intellij, java java ee, javaee containers, javascript, jdbc, jsf, maven, ms sql server, mysql, oracle, php, reporting systems, security, software architecture, spring, sql server, transactional systems, web services, xml beans"
Expression of Interest: Data Scientist,Fingerprint for Success (F4S),"Chicago, IL",https://www.linkedin.com/jobs/view/expression-of-interest-data-scientist-at-fingerprint-for-success-f4s-3787769939,2023-12-17,Wheaton,United States,Mid senior,Remote,"We are inviting professionals in high-growth industries who are thinking about their next move or looking for a new opportunity to join our expanding talent pool.
The F4S Talent Pool is a pilot project designed to:
Help job seekers get discovered by our partners based on their anticipated hiring needs.
Provide optional support and resources for job seekers in their career endeavors.
Help individuals understand, and bring out the best in themselves and each other.
The F4S Talent Pool process:
Once you express your interest, you will be asked to complete the F4S work style assessment which measures 48 key attitudes and motivations in the context of work. On completion, you will be automatically added to our growing talent pool and contacted as new opportunities arise.
About Fingerprint For Success (F4S)
Backed by 20+ years of research, F4S’s revolutionary predictive analytics have achieved over 90% reliability in forecasting personal and team motivations, behaviors, and performance. Ultimately, we help people find purpose and fulfillment at work, and help build and scale high performing teams.
Keep in mind, joining our talent pool does not guarantee a job offer. We aim to balance your technical skills with the results of your F4S work style assessment to match the hiring needs of our partners.
Your feedback is a gift! Write to us via:
Powered by JazzHR
JNnpORryzp
Show more
Show less","Work style assessment, Predictive analytics, Motivations, Behaviors, Performance, Talent pool, Technical skills, Hiring needs","work style assessment, predictive analytics, motivations, behaviors, performance, talent pool, technical skills, hiring needs","behaviors, hiring needs, motivations, performance, predictive analytics, talent pool, technical skills, work style assessment"
Assistant Marketing Data Analyst,HomeServe USA,"Norwalk, CT",https://www.linkedin.com/jobs/view/assistant-marketing-data-analyst-at-homeserve-usa-3750875315,2023-12-17,Connecticut,United States,Associate,Onsite,"Position Overview/ Responsibilities:
To assist and support the Marketing Database team by carrying out data selections and analysis for direct mail, email, and outbound telemarketing campaigns.
Timely and accurate production of counts and data files for acquisition, cross-sell and retention campaigns using SAS programming.
Cleansing and standardizing partner files for use in marketing campaigns using specialist data cleansing software.
Assisting with guidance on data briefs/matrices through a consultative process with the strategy and campaign management teams.
Gathering and supporting the responses to customer and partner complaints about data use through data mining and investigations.
Supporting the data infrastructure of the Marketing Database team through table updates, database maintenance and recommending opportunities for data process improvements. –
Managing all data work with due skill, care and diligence, including effective quality controls and risk management/mitigation.
Inquires and suggests implementing ways to automate the production of data and counts for campaign activity.
Job Requirements:
Bachelor’s degree (mathematics/statistics/economics/computer science or other numerical-based degree preferred) or 1-2 years’ relevant work experience.
Highly numerate and analytical
Some work experience in a commercially data driven/analysis role.
SAS, R, SQL or other programming language experience
Intermediate to Advanced Microsoft Excel skills
Strong written communication skills
Attention to detail and accuracy.
What We Are Looking For
At HomeServe USA Corp., we put our people and our customers at the heart of everything we do, and we’re looking for someone who loves the work of compliance to join our team. A compliant organization is good for everyone so join us in supporting a culture that creates great employee experiences at an organization that cares. The right people for our team (1) care about what we do, the people on our teams, and the customers we serve, (2) are open and honest when communicating, (3) engage actively in work and office life, (4) are committed to growth, learning, and improving both self and the organization, (5) take ownership and collaborate well with to get to great outcomes, (6) share in the full team’s success. We need people who challenge convention, think differently, solve problems, and strive for continuous growth.
This job description is intended to provide guidelines for job expectations and the employee's ability to perform the position described. It is not intended to be construed as an exhaustive list of all functions, responsibilities, skills and abilities. Additional functions and requirements may be assigned by supervisors as deemed appropriate.
In Return, We Offer:
Competitive compensation
Career development and advancement opportunities
Friendly, open and team-oriented work atmosphere
Excellent benefits including generous medical, vision, dental and life & disability insurance.
401(k) plan with a company match
Eligibility to enroll in up to two HomeServe coverage plans paid for by the company.
The salary range for this position is $60,290 - $80,386 + 5% annual incentive bonus eligibility.
We are an equal opportunity employer.
Company Overview:
HomeServe USA Corp. is a Great Place to Work, and while we’re biased, we’re not just saying that. We’re proud to have been certified as a Great place to Work the last four years. What do we do and what makes it so great? Well, we’re glad you asked!
We put people at the heart of everything we do. That’s priority number one for all of us. For the 5 million Customers we serve, that means being there when they have an emergency home repair need, such as getting their a/c working, clearing their clogged pipes, or fixing broken electrical systems. With over 1,000 municipal and utility Partners, that means providing
their
customers with access to affordable home repair plans, making it easier, faster and less expensive to have their home repair needs met. And for the nearly 3,000 People working alongside us, it means fostering a rewarding, inclusive and challenging career experience that we think is second to none. At HomeServe USA Corp., everyone is welcome. We know that having diverse teams has a positive impact on our work and ultimately helps us better serve our customers.
Show more
Show less","SAS, R, SQL, Microsoft Excel, Data mining, Data cleansing, Data analytics, Data processing, Data analysis, Direct mail, Email marketing, Outbound telemarketing, Data quality control, Risk management, Database maintenance, Table updates, Data infrastructure, Data standardization, Data normalization, Data integrity, Data consistency, Data accuracy, Attention to detail, Numerical analysis, Statistics, Mathematics, Economics, Computer science","sas, r, sql, microsoft excel, data mining, data cleansing, data analytics, data processing, data analysis, direct mail, email marketing, outbound telemarketing, data quality control, risk management, database maintenance, table updates, data infrastructure, data standardization, data normalization, data integrity, data consistency, data accuracy, attention to detail, numerical analysis, statistics, mathematics, economics, computer science","attention to detail, computer science, data accuracy, data consistency, data infrastructure, data integrity, data mining, data normalization, data processing, data quality control, data standardization, dataanalytics, database maintenance, datacleaning, direct mail, economics, email marketing, mathematics, microsoft excel, numerical analysis, outbound telemarketing, r, risk management, sas, sql, statistics, table updates"
AWS Data Engineer,LTI - Larsen & Toubro Infotech,"Hartford, CT",https://www.linkedin.com/jobs/view/aws-data-engineer-at-lti-larsen-toubro-infotech-3718986845,2023-12-17,Connecticut,United States,Mid senior,Onsite,"A little about us...
LTIMindtree is a global technology consulting and digital solutions company that enables enterprises across industries to reimagine business models, accelerate innovation, and maximize growth by harnessing digital technologies. As a digital transformation partner to more than 750 clients, LTIMindtree brings extensive domain and technology expertise to help drive superior competitive differentiation, customer experiences, and business outcomes in a converging world.
· We provide our employees with a learning environment that promotes growth and creativity.
· To learn more plase visit us at https://www.ltimindtree.com/Twitter @ https://twitter.com/ltimindtree
Role: Data Migration Engineer
Experience 4-6 Years
Location: USA – Hartford, CT
Must Have: AWS (S3), Python, Databricks
Good to have: Snowflake, Spark
Overview:
Data Engineering team constructs pipelines that contextualize and provide easy access to data by the entire enterprise. As a Data Engineer, you will play a key role in growing and transforming analytics landscape.
Looking for Data Migration Engineer having an experience migrating data from on prem to cloud.
Requirements:
• Strong python skills
• AWS/Cloud infrastructure knowledge (commonly used AWS services, IAM)
• Experience with building data pipelines using Databricks on AWS
• Knowledge and Hands-on Snowflake
• Experience in Agile methodologies and Atlassian tools like JIRA.
• Expertise in using version control tools like Git, Bitbucket
• Experience on CI/CD using Kubernetes, GIT and Monitoring and Alerting tools
• Experience on data migration from On-Prem databases to AWS Cloud on S3
Roles & resposibilities:
• Acts as a single point of contact for data migration to AWS projects for customer
• Provides innovative and cost-effective solution using AWS, Spark, python & databricks
• Develops solutions to meet business needs that reflect a clear understanding of the objectives, practices and procedures of the corporation, department and business unit
• As a leader in the Cloud Engineering you will be responsible for the overseeing development
• Learn/adapt quickly to new Technologies as per the business need
• Develop a team of Operations Excellence, building tools and capabilities that the Development teams leverage to maintain high levels of performance, scalability, security and availability
• Understand where to obtain information needed to make the appropriate decisions
• Demonstrate ability to break down a problem to manageable pieces and implement effective, timely solutions
• Identify the problem versus the symptoms
• Develop solutions to meet business needs that reflect a clear understanding of the objectives, practices and procedures of the corporation, department and business unit
Skills:
• The Candidate must have 3-5 yrs of experience in AWS, Python, Databricks
• Hands on experience on AWS Cloud platform especially S3, lamda, EC2, EMR
• Experience on spark scripting
• Has working knowledge on migrating relational and dimensional databases on AWS Cloud platform
• Relevant experience with ETL methods and with retrieving data from dimensional data models and data warehouses.
• Strong experience with relational databases and data access methods, especially SQL.
• Knowledge of Amazon AWS architecture and design
Minimum Qualifications
• Bachelor’s degree or equivalent training with data tools, techniques, and manipulation.
• Four years of data engineering or equivalent experience.
How will you grow?
· Role-based Training programs
· Continuing Education Programs (CEP) to enhance your knowledge, skills, and attitude as a professional
· We encourage you to acquire various beneficial international certifications, with costs s reimbursed
· Our role-based workshop helps us groom future leaders for LTI
What's in it for you?
· Excellent benefits plan: medical, dental, vision, life, FSA, & PTO
· Roll over vacation days
· Commuter benefits
· Excellent growth and advancement opportunities
· Certification reimbursement
· Rewards and recognition programs
· Innovative and collaborative company culture
We are an
Equal Opportunity/Affirmative Action employer
. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, gender identity, sexual orientation, disability status, protected veteran status, or any other characteristic protected by law.
Show more
Show less","AWS, Python, Databricks, Snowflake, Spark, S3, Lambda, EC2, EMR, SQL, Git, Bitbucket, Kubernetes, Jira, Agile, ETL, Data Engineering, Cloud Infrastructure, Data Migration, Data Pipelines, Data Warehouses, Data Access Methods","aws, python, databricks, snowflake, spark, s3, lambda, ec2, emr, sql, git, bitbucket, kubernetes, jira, agile, etl, data engineering, cloud infrastructure, data migration, data pipelines, data warehouses, data access methods","agile, aws, bitbucket, cloud infrastructure, data access methods, data engineering, data migration, data warehouses, databricks, datapipeline, ec2, emr, etl, git, jira, kubernetes, lambda, python, s3, snowflake, spark, sql"
Junior Data Engineer,Bridgeton Research Group LLC,"Westport, CT",https://www.linkedin.com/jobs/view/junior-data-engineer-at-bridgeton-research-group-llc-3761064348,2023-12-17,Connecticut,United States,Mid senior,Onsite,"We are a technology-based firm that utilizes our proprietary platform to develop and implement quantitative strategy overlays for our clients. We deploy our strategies across equities, futures, and foreign exchange instruments. Our clients include institutional investors and commercial hedgers who use our data driven insights to complement their existing investment process and fundamental market expertise.
We are looking for a motivated and focused individual with a passion for solving complex problems to join our infrastructure development team.
You will have the opportunity to work on a high-volume data ingestion system, setting the foundation for our high-level analysis.
Requirements
Proficiency in Python and SQL
Familiarity with Python libraries such as Pandas, Numpy, Matplotlib, SQLAlchemy, etc.
Experience with numerical data manipulation and processing
Experience with Microsoft Excel
Tireless attention to detail and a sense of perfectionism
A bachelor’s degree with a GPA above 3.0 (STEM preferred)
Ability to communicate clearly and concisely, verbally and in writing
Ability to solve unfamiliar problems under pressure
Desire to take ownership and work autonomously in a fast-paced and dynamic environment
Job duties include:
Support, maintain, configure, and automate data ingestion infrastructure and documentation.
Ensure data is clean and client reports are 100% accurate
Provide technical support to clients and other project team members.
Assist the team in resolving production issues on nights and weekends as needed.
Perform administrative tasks as needed.
Collaborate with the data science team in the creation and distribution of client reports.
Show more
Show less","Python, SQL, Pandas, Numpy, Matplotlib, SQLAlchemy, Numerical data manipulation, Numerical data processing, Microsoft Excel, Data ingestion, Data infrastructure, Data documentation, Data cleaning, Client reports, Technical support","python, sql, pandas, numpy, matplotlib, sqlalchemy, numerical data manipulation, numerical data processing, microsoft excel, data ingestion, data infrastructure, data documentation, data cleaning, client reports, technical support","client reports, data cleaning, data documentation, data infrastructure, data ingestion, matplotlib, microsoft excel, numerical data manipulation, numerical data processing, numpy, pandas, python, sql, sqlalchemy, technical support"
Data Engineer - Business Intelligence,Subway,"Shelton, CT",https://www.linkedin.com/jobs/view/data-engineer-business-intelligence-at-subway-3785720345,2023-12-17,Connecticut,United States,Mid senior,Remote,"Be a part of the Subway® Group – the world’s largest quick service restaurant franchisor and recognized leader in the Quick Service Restaurant industry. With more than 44,000 restaurants in over 100 countries, we continue to add talent to our team at our global headquarters in Shelton, Connecticut. Subway seeks to hire an experienced Business Intelligence (BI) Data Engineer to work closely with our BI Team and Business Users to continue to evolve our cloud enabled data ecosystem.
The BI Engineer will be responsible for the design and development of datasets and Reports that support our BI Platforms. The BI Engineer will work as part of a cross functional team to deliver Reporting solutions for enterprise initiatives.
A successful candidate will deliver scalable and flexible solutions that conform to Subway’s data design and governance strategies utilizing our AWS environment and Amazon Redshift data warehouse and other cloud-based technologies. Responsibilities include, but are not limited to:
Design and develop Datasets (Tableau or other tools) that are performant, scalable and flexible.
Design and develop Reports that are performant and flexible.
Design and develop data pipelines and integrations that are performant, scalable and flexible.
Work with project teams to deliver Datasets and reports, ensuring a high degree of reliability and resiliency.
Work with business users and project teams to provide SME guidance, finalize requirements and develop level of effort estimates.
Conduct Unit testing and UAT with the business counterparts.
Provide database support by coding utilities, responding to user questions, and resolving problems.
Create and maintains documentation of the datasets and reports including data flow and data lineage documentation.
Responsible for tier 3 support and assisting our Operations Team as required to provide a great customer experience for our users.
Responsible for overall data quality in the aggregate layer in Data warehouse. Provide data analysis as required to troubleshoot data issues.
Assist with Dataset and reporting related code reviews and mentorship of junior and offshore developers.
The successful candidate will have:
5 years’ experience in creating Tableau Datasets and Tableau workbooks.
5 years’ experience in creating SSRS reports.
Bachelor’s degree in Computer or Information Science or related field or equivalent combination of education and experience.
Knowledge of Matillion and/or other ELT systems preferred.
Experience with creating SQL DDL’s and DML’s in Redshift.
Expert level data analysis and data management skills with highly effective problem-solving skills that can transform problems into solutions.
Knowledge of integration methodologies and best practices.
Expert level SQL programming experience.
Python programming experience.
Data Modeling skills and familiarity with BI Reporting Tools.
Understanding of DevOps, Continuous Integration and Continuous Delivery technologies.
Experience which would set the candidate apart:
Tableau certification is a plus (Architect or Developer track).
Practical understanding of public cloud architecture.
Show more
Show less","SQL, AWS, Python, Redshift, Matillion, Tableau, SSRS, Data Analysis, DevOps, Data Modeling, Data Pipelines, Data Lineage, Business Intelligence, Cloud Computing, Data Warehousing, Data Integration, Data Governance","sql, aws, python, redshift, matillion, tableau, ssrs, data analysis, devops, data modeling, data pipelines, data lineage, business intelligence, cloud computing, data warehousing, data integration, data governance","aws, business intelligence, cloud computing, data governance, data integration, data lineage, dataanalytics, datamodeling, datapipeline, datawarehouse, devops, matillion, python, redshift, sql, ssrs, tableau"
Data Engineer,"Bexorg, Inc.","New Haven, CT",https://www.linkedin.com/jobs/view/data-engineer-at-bexorg-inc-3766966259,2023-12-17,Connecticut,United States,Mid senior,Remote,"About Us
Make an impact with Bexorg - a pioneering discovery company dedicated to shaping the future of brain research. Together, we are committed to creating a world without brain disease. On our mission to construct this groundbreaking understanding of the brain, we need team members that will be as ambitious and innovative as our technology. At Bexorg we value individuals who can think aspirationally and start practically, who are eager to collaborate and dedicated to their research, and who can act with the conviction needed to actualize our mission. Join our team and see what a future without brain disease holds.
About the Job
As a key member of our data team, you will play a pivotal role in designing and building data pipelines, databases, and knowledge graphs on Google Cloud Platform (GCP) and other services. Your responsibilities will include creating robust and scalable data architectures, and leveraging GCP services such as BigQuery. You will also ensure data quality, and security, while collaborating closely with data scientists and analysts to provide high-quality, actionable data for our organization's decision-making processes.
Key Responsibilities
Design, build, and operationalize data engineering solutions
Implement CI/CD and other automation techniques to streamline data ingestion and delivery processes.
Develop self-service data and analytics capabilities
Improve data flow architecture, emphasizing data quality, maintainability, and extensibility.
Implement software engineering and DataOps standards and best practices for data analytics team, including code modularization, versioning, testing, automation of CI/CD workflows, code reviews.
Work with data teams to drive enterprise taxonomy and ontology development to connect disparate data across team and enable 360 insights.
Gain an understanding of core business processes and align data development with business strategy.
Preferred Education and Experience
Bachelor degree in Data Science/ Computer Science/ Applied Mathematics or similar field with at least three years of industry experience.
or Master's Degree in Data Science/Computer Science/Applied Mathematics or similar field with no industry experience.
Preferred Skills, Qualifications, or Technical Proficiencies
Architecture design
Data modeling
Data warehousing concepts
Data transformations and standardizations
ETL processes & strategies
SDLC & workflow best practices
Version control & branching strategies, Github a plus
Proficient in languages: SQL, Python, SPARQL a plus
Data governance, security, and compliance concepts
Data Ingestion - REST APIs
Experience working with cloud, Google Cloud Services a plus
Experience querying massive datasets
Securely source external data from partners and outside sources
Design and develop new systems in partnership with software engineers and scientists to enable quick and easy consumption of data
Project management experience
Bexorg is an equal opportunity employer. We thrive on diversity and collaboration.
Show more
Show less","Data engineering, Data pipelines, Databases, Knowledge graphs, Google Cloud Platform, BigQuery, Data architecture, Data quality, Data security, DataOps, Version control, Branching strategies, CI/CD, Code reviews, Enterprise taxonomy, Ontology development, Data governance, Data compliance, ETL, SQL, Python, REST APIs, Cloud computing, Massive datasets, Project management","data engineering, data pipelines, databases, knowledge graphs, google cloud platform, bigquery, data architecture, data quality, data security, dataops, version control, branching strategies, cicd, code reviews, enterprise taxonomy, ontology development, data governance, data compliance, etl, sql, python, rest apis, cloud computing, massive datasets, project management","bigquery, branching strategies, cicd, cloud computing, code reviews, data architecture, data compliance, data engineering, data governance, data quality, data security, databases, dataops, datapipeline, enterprise taxonomy, etl, google cloud platform, knowledge graphs, massive datasets, ontology development, project management, python, rest apis, sql, version control"
Sr Data Movement Engineer,Harvey Nash,"New Haven, CT",https://www.linkedin.com/jobs/view/sr-data-movement-engineer-at-harvey-nash-3785820175,2023-12-17,Connecticut,United States,Mid senior,Remote,"Job Title: Sr Data Movement Engineer
Location: New Haven, CT 06473 (Remote option available)
Duration: 12+ months contract - will go long term
Summary of Job:
Must Haves:
- Strong Background in API development
- Strong SQL background
- Strong Data Engineering Background, must understand best practices
- Cloud experience
- Experience with Talend is highly preferred
Nice to Haves:
- Power BI
- Ideally has Snowflake experience. Background in Oracle/SQL is acceptable
Job Description
Key Job Responsibilities.
Experience in Data Engineering in AWS/Azure Cloud
Experience with API data integration
Experience with Talend or other data movement tools Informatica, Azure Data Factory
Snowflake or other relational database experience such as Oracle or SQL Server
CI/CD Experience either using Git or Azure Dev Ops
Team Lead responsibility Looking for the resource to become the overall Data Engineering lead
Job Description
Overview
Embarking on the modernization of its core data platforms.
We are currently seeking a Senior Data Movement Engineer who can assist us with modernizing our current ETL environment, as well as begin building new ETL/ELT workflows and data pipelines as we move towards a cloud environment.
The Data Movement Engineer will support our data architects, data analysts, and business intelligence developers on various data initiatives.
They must be self-directed and comfortable supporting the data requirements of multiple teams and systems.
The ideal candidate will possess experience in the data management domain and be willing to contribute their technical skills and experience towards the development of elegant and efficient data processes.
Core Responsibilities
Complete full life cycle of ETL/ELT development to address business needs or resolve issues including design, mappings, data transformations, scheduling and testing.
Translate data movement requirements into technical ETL design.
Develop ETL/ELT workflows and mappings to extract, transform, and load data into target environments.
Develop data extraction and transmissions to external vendors.
Develop test plans and perform unit testing.
Create supporting documentation for new ETL processes.
Work closely with data analysts to gain understanding of business processes and corporate data.
Determine impacts to data warehouse structures and information flows due to changes in business or technical requirements.
Contribute to architectural decisions to support business processes.
Provide production support for data solutions.
Complete root cause analysis and contribute to remediation planning and implementation.
Perform data quality analysis, report data quality issues and propose solutions for data quality management.
Learn and expand upon internal controls and participate in customer support.
Perform design and code reviews in support of organizational standards.
Mentor less experienced team members.
Prepare effort estimation including researching and estimating costs of software development, unit testing. May provide estimates for upgrades of vendor packages upgrades and integration with existing systems.
On Call and/or after hours work required.
Essential Competencies
Professionalism/Personal Accountability, Collaboration and Teamwork, Communication, Flexible and Adapts to Change, Service to Customers and Clients
Skill Qualifications
Required:
Minimum of 8 years of relevant experience in data warehousing, business intelligence tools, and the analysis of data
Minimum of 5 years of SQL query development, preferably across multiple database management platforms, and working with dimensional and star schema data warehouse implementations
Proficiency in ETL/ELT concepts and tools (Informatica IICS, Talend, Boomi and PowerCenter preferred)
Working knowledge of data warehousing concepts, structures and ETL best practices
Experience using query tools (e.g.. AQT, MS Query)
Ability to problem solve using analytical thinking skills
Must work well independently - must be inquisitive and seek answers to complex questions without being prompted
Strong organizational and time management skills
Strong communication skills including verbal and written to communicate effectively with clients and management
Strong project management skills to ensure that projects get done on time and within budget
Effectively participates in teams and moves the team toward completion of goals
Ability to resolve conflicts
Preferred:
Some experience with cloud database technologies preferred
Some experience with data visualization tools (e.g. Tableau) desirable
Education and Experience Qualifications
Required:
BA or BS in Computer Science, Information Systems or related field.
10+ years experience
Show more
Show less","API development, SQL, Data engineering, Cloud, Talend, Power BI, Snowflake, Oracle, Azure Dev Ops, Git, AWS, Azure Cloud, API data integration, Informatica, Azure Data Factory, ETL/ELT, Data extraction, Data transformation, Data loading, Data quality analysis, Data visualization, Tableau, dimensional data warehouse, star schema data warehouse, AQT, MS Query, Computer Science, Information Systems","api development, sql, data engineering, cloud, talend, power bi, snowflake, oracle, azure dev ops, git, aws, azure cloud, api data integration, informatica, azure data factory, etlelt, data extraction, data transformation, data loading, data quality analysis, data visualization, tableau, dimensional data warehouse, star schema data warehouse, aqt, ms query, computer science, information systems","api data integration, api development, aqt, aws, azure cloud, azure data factory, azure dev ops, cloud, computer science, data engineering, data extraction, data loading, data quality analysis, data transformation, dimensional data warehouse, etlelt, git, informatica, information systems, ms query, oracle, powerbi, snowflake, sql, star schema data warehouse, tableau, talend, visualization"
Insurance - Data Analyst - REMOTE,Wahve LLC,"Stamford, CT",https://www.linkedin.com/jobs/view/insurance-data-analyst-remote-at-wahve-llc-3785427367,2023-12-17,Connecticut,United States,Mid senior,Remote,"Put your Insurance Experience to work - FROM HOME!
At
Wahve
, we value significant insurance experience and want to revolutionize the way people think about
phasing into
retirement
by offering qualified candidates the opportunity to continue their career working from home. As we say -
retire from the office but not from work
. Our unique platform provides you with
real
work/life balance and allows you to customize your own work schedule while continuing to utilize your insurance expertise in
a remote, long-term position
.
What You’ll Love About Wahve
We created a welcoming place to work with friendly and professional leadership. We are known for the great care we take with our staff and our clients. We are passionate and determined about delivering the best customer service, preserving insurance industry knowledge, and making a difference by the work that we do.
What We Are Seeking
We have assignments available to help our
insurance industry
clients in
Data Analyst positions. Responsibilities include:
Build and maintain data warehouse, new reports, and ad hoc reports.
Work with user groups to identify reporting issues/enhancements and document business requirements.
Will serve as a member of a project team and/or work independently on projects.
Support and train internal users as needed.
Compile and prepare data for customer analysis.
Experience in C#, Visual Studio, JavaScript, CSS, and current web technologies such as .NET, ASP, JSON, and XML.
Experience with ANY of the following technologies: SQL Server Reporting Services (SSRS), SSIS Reporting, Power BI, Dynamics CRM, Dynamics GP, Share point, Excel, Power Query, Power Pivot.
Ability to compile data results and author commentary on industry studies is a plus.
Insurance or financial services industry experience required.
TO BECOME A WORK-AT-HOME VINTAGE EXPERT, WE REQUIRE
25 years of full-time work experience
Experience working in a data analysis role in the insurance or financial services industry - required
Benefits Of Becoming a Wahve Vintage Expert
Retire from the office but not from work.
Eliminate the office stress and the commute.
Choose the work you would like to do now.
Customize your schedule - full or part time.
Continue to earn an income.
Utilize your years of insurance industry knowledge.
Be part of our dynamic yet virtual team environment and connect with other experienced insurance professionals like yourself!
How To Get Started
Click
APPLY NOW
to complete our simple preliminary profile. Be sure to include your preferred contact information as one of our Qualification Specialists will connect with you promptly.
WE LOOK FORWARD TO MEETING YOU!
Show more
Show less","Data Analysis, C#, Visual Studio, JavaScript, CSS, .NET, ASP, JSON, XML, SQL Server Reporting Services (SSRS), SSIS Reporting, Power BI, Dynamics CRM, Dynamics GP, Sharepoint, Excel, Power Query, Power Pivot, Industry Studies, Insurance, Financial Services","data analysis, c, visual studio, javascript, css, net, asp, json, xml, sql server reporting services ssrs, ssis reporting, power bi, dynamics crm, dynamics gp, sharepoint, excel, power query, power pivot, industry studies, insurance, financial services","asp, c, css, dataanalytics, dynamics crm, dynamics gp, excel, financial services, industry studies, insurance, javascript, json, net, power pivot, power query, powerbi, sharepoint, sql server reporting services ssrs, ssis reporting, visual studio, xml"
Senior Investment Data Analyst,Energy Jobline,"Norwalk, CT",https://www.linkedin.com/jobs/view/senior-investment-data-analyst-at-energy-jobline-3786345877,2023-12-17,Connecticut,United States,Mid senior,Remote,"Job Description SummaryNorth American Life and Health (NALH) is the reinsurance business unit under GE Aerospace’s Corporate Holdings; primarily focused on Long Term Care (LTC) and Life reinsurance segments. The NALH Investment organization manages a diverse asset portfolio of ~$45B to support reinsurance obligations.
The Senior Investment Data Analyst is a key role within NALH Investment Operations organization, reporting to the Director – Investment Data Strategy & Management. This role will support the overall investment data strategy including design, development and continuous improvement of NALH investment data infrastructure and processes. This role will serve as the subject matter expert on investment data management and involve data integrity, data integration, data reconciliation, data governance, as well as enabling investment and risk analytics reporting across the organization. This role will partner with key external and internal stakeholders to support investment data operations as well as investment data strategic initiatives.
Job Description
Roles and Responsibilities
Oversee and monitor data flows to ensure timely receipt of investment data and delivery of production reports from third party sources/vendors and work closely with internal and external partners to address any issues that might arise
Drive consistency and integrity of investment data by 1) developing and maintaining data definitions and dictionaries and 2) designing and implementing analysis tools to reconcile data obtained from multiple sources and 3) enable periodic/ad hoc data validations and tests of reasonability to ensure accurate, complete and on-time availability of investment data reporting & analytics
Research and analyze data exceptions, working closely with internal and external partners to resolve issues
Analyze trends and patterns in observed data exceptions and outliers to improve upon the design and effectiveness of existing processes and recommend and implement additional control processes as appropriate
Develop and maintain core investment data production reports, analytics and dashboards (including portfolio holdings, weekly transactions, advanced investment metrics etc.) for downstream investment, financial and risk reporting and related processes
Develop/serve as a subject matter expert in investment data management, providing support for ongoing data operations as well as our investment data strategic initiatives. Provide guidance and technical coaching to other team members as necessary
Partner with internal stakeholders (including but not limited to: Investment Management, Treasury, Investment Accounting, Finance, Risk) and external partners (e.g., investment data/risk analytics platforms, investment accounting provider, bank custodians, etc.) to support investment-related data activities as necessary
Required Qualifications
Bachelor’s degree, preferably in a quantitative field such as Finance, Economics, Statistics, Mathematics, or equivalent
4+ years of relevant experience; 1+ years of experience in investment analytics, financial systems and process design and/or market/investment risk
Working understanding of financial markets and instruments, including security reference data and attributes. Familiarity with fixed income reference data and risk analytics preferred
Experience working directly with an investment data/risk analytics platform such as Blackrock Aladdin and an investment accounting platform such as Clearwater, including understanding of data transmission and loading, risk production and reporting, and downstream integration
Advanced technical skills, including proficiency working with large data sets and using Microsoft Office Suite. Familiarity with one or more programming language is a plus (e.g., SQL, VBA, Python, R, etc.)
OPEN TO REMOTE IN US
Desired Characteristics
CFA or progress towards a CFA designation a plus
Familiarity with designing efficient business processes and related tools to optimize workflows, reduce manual effort, and mitigate potential errors.
Detail-oriented, with strong analytical, critical thinking, and problem-solving skills
Quick learner, can-do-attitude, self-starter, strategically prioritizes work
Additional Information
GE offers a great work environment, professional development, challenging careers, and competitive compensation. GE is an Equal Opportunity Employer. Employment decisions are made without regard to race, color, religion, national or ethnic origin, sex, sexual orientation, gender identity or expression, age, disability, protected veteran status or other characteristics protected by law.
GE will only employ those who are legally authorized to work in the United States for this opening. Any offer of employment is conditioned upon the successful completion of a drug screen (as applicable).
Relocation Assistance Provided: No
This is a remote position
Show more
Show less","Investment data, Data management, Data integration, Data reconciliation, Data governance, Data analytics, SQL, VBA, Python, R, Portfolio holdings, Weekly transactions, Investment metrics, Financial reporting, Risk reporting, Blackrock Aladdin, Clearwater, Microsoft Office Suite","investment data, data management, data integration, data reconciliation, data governance, data analytics, sql, vba, python, r, portfolio holdings, weekly transactions, investment metrics, financial reporting, risk reporting, blackrock aladdin, clearwater, microsoft office suite","blackrock aladdin, clearwater, data governance, data integration, data management, data reconciliation, dataanalytics, financial reporting, investment data, investment metrics, microsoft office suite, portfolio holdings, python, r, risk reporting, sql, vba, weekly transactions"
Senior Data Engineer,STAND 8 Technology Services,"Southbury, CT",https://www.linkedin.com/jobs/view/senior-data-engineer-at-stand-8-technology-services-3782064148,2023-12-17,Connecticut,United States,Mid senior,Hybrid,"STAND 8 provides end to end IT solutions to enterprise partners across the United States and with offices in LA, Atlanta, New York, Raleigh, and more.
Are you a seasoned professional passionate about transforming data movement processes? We're actively seeking a
Senior Data Movement Engineer
to play a pivotal role in revamping our ETL infrastructure and spearheading the construction of innovative ETL/ELT workflows in our journey towards a cloud-centric environment. The ideal candidate will bring a wealth of expertise in data management, contributing nuanced technical skills and a proactive mindset to craft sophisticated and streamlined data processes. If you thrive in driving transformative changes and optimizing data flow, this opportunity awaits your leadership and innovation.
Core Responsibilities
Complete full life cycle of ETL/ELT development to address business needs or resolve issues including design, mappings, data transformations, scheduling, and testing.
Translate data movement requirements into technical ETL design.
Develop ETL/ELT workflows and mappings to extract, transform, and load data into target environments.
Develop data extraction and transmissions to external vendors.
Develop test plans and perform unit testing.
Create supporting documentation for new ETL processes.
Work closely with data analysts to gain understanding of business processes and corporate data.
Determine impacts to data warehouse structures and information flows due to changes in business or technical requirements.
Contribute to architectural decisions to support business processes.
Provide production support for data solutions.
Complete root cause analysis and contribute to remediation planning and implementation.
Perform data quality analysis, report data quality issues, and propose solutions for data quality management.
Learn and expand upon internal controls and participate in customer support.
Perform design and code reviews in support of organizational standards.
Mentor less experienced team members.
Prepare effort estimation including researching and estimating costs of software development, unit testing. May provide estimates for upgrades of vendor packages upgrades and integration with existing systems.
Required
Minimum of 8 years of relevant experience in data warehousing, business intelligence tools, and the analysis of data
Minimum of 5 years of SQL query development, preferably across multiple database management platforms, and working with dimensional and star schema data warehouse implementations
Proficiency in ETL/ELT concepts and tools (Informatica IICS, Talend, Boomi and PowerCenter preferred)
Working knowledge of data warehousing concepts, structures and ETL best practices
Experience using query tools (e.g.. AQT, MS Query)
Ability to problem solve using analytical thinking skills.
Must work well independently - must be inquisitive and seek answers to complex questions without being prompted.
Strong organizational and time management skills
Strong communication skills including verbal and written to communicate effectively with clients and management.
Strong project management skills to ensure that projects get done on time and within budget.
Effectively participates in teams and moves the team toward completion of goals
Ability to resolve conflicts.
Preferred:
Some experience with cloud database technologies preferred.
Some experience with data visualization tools (e.g. Tableau) desirable
Education and Experience Qualifications
BA or BS in Computer Science, Information Systems, or related field.
10+ years' experience
The US base range for this contract position is $60-67/hour. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training.
Show more
Show less","ETL, ELT, Data Management, Data Warehousing, Business Intelligence, SQL, Informatica IICS, Talend, Boomi, PowerCenter, AQT, MS Query, Data Visualization, Tableau, Cloud Database Technologies","etl, elt, data management, data warehousing, business intelligence, sql, informatica iics, talend, boomi, powercenter, aqt, ms query, data visualization, tableau, cloud database technologies","aqt, boomi, business intelligence, cloud database technologies, data management, datawarehouse, elt, etl, informatica iics, ms query, powercenter, sql, tableau, talend, visualization"
"Senior Data Engineer - SQL, Python, Kafka.",The Recruitment Company Pty Ltd,"East Haddam, CT",https://www.linkedin.com/jobs/view/senior-data-engineer-sql-python-kafka-at-the-recruitment-company-pty-ltd-3773061984,2023-12-17,Connecticut,United States,Mid senior,Hybrid,"Respected financial organization seeks an end-to-end Senior Data Engineer (SQL, Python, Kafka) to join a growing team and take ownership of the distributed streaming application suite and messaging and integration components.
Super interesting and varied role! You will work with a talented and friendly team of Data Engineers. You will play a central role in the growing team and a pivotal part of a key project to in deploying Kafka to Confluent Cloud.
You will be working heavily with Kafka; development, support, administration and configuration of the Kafka platform & other messaging and distribution systems.
Key Experience Needed
SQL, Python
Successfully deployed and operated a Kafka platform in a Production environment.
Experience in building, deploying and managing services in the Cloud (GCP or AWS are desirable)
Experience in managing messaging and integration platforms.
Excellent technical problem-solving skills.
Solutions mindset
Bonus Experience
Fintech experience or Start Up Experience
Java
T
o be considered please APPLY today, interviews this week.
Or for a confidential discussion please text Chipo Riva on 0420 410 976 and we will call you back!
Show more
Show less","SQL, Python, Kafka, Confluent Cloud, Data Engineering, Messaging, Integration, Cloud Computing (GCP AWS), Technical Problem Solving, Java (Bonus), Fintech (Bonus), Start Up Experience (Bonus)","sql, python, kafka, confluent cloud, data engineering, messaging, integration, cloud computing gcp aws, technical problem solving, java bonus, fintech bonus, start up experience bonus","cloud computing gcp aws, confluent cloud, data engineering, fintech bonus, integration, java bonus, kafka, messaging, python, sql, start up experience bonus, technical problem solving"
Azure Data Engineer - Remote/Contract,Emtec Inc.,United States,https://www.linkedin.com/jobs/view/azure-data-engineer-remote-contract-at-emtec-inc-3785860163,2023-12-17,Smith Center,United States,Mid senior,Onsite,"Company Overview
Emtec is a Global consulting company that provides technology-empowered business solutions for world-class organizations. Our Global Workforce of over 1000 consultants provide best in class services to our clients to realize their digital transformation journey. Our clients span the emerging, mid-market and enterprise space. With multiple offices worldwide, we are uniquely positioned to deliver digital solutions to our clients leveraging Microsoft, Java and Open Source with a focus on Mobility, Cloud, Data Engineering and Intelligent Automation. Emtec’s singular mission is to create “Clients for Life” - long-term relationships that deliver rapid, meaningful, and lasting business value.
At Emtec, we have a unique blend of Corporate and Entrepreneurial cultures. This is where you would have an opportunity to drive business value for clients while you innovate and continue to grow and have fun while doing it. You would work with team members who are vibrant, smart and passionate and they bring their passion to all that they do – whether it’s learning, giving back to our communities or always going the extra mile for our client.
Position Description
We are looking for a talented and experienced Azure Data Engineer who will support various data projects within the organization. We are looking from someone with a strong background with Azure Data Factory, Azure Databricks, Azure SQL Database, Azure Data Lake Storage,etc. Responsibilites may vary depending on the project; however, ETL will likely be a major responsibility across the board.
Core Tech Stack:
Azure Data Factory, Azure Databricks, Azure SQL Database, Azure Data Lake Storage,etc.
Location:
Fully Remote
Employment Type:
This is a 6 month contract, with potential extention.
Responsibilities Include But Are Not Limited To
Develop and implement ETL processes using Azure Data Factory, Azure Databricks, and other relevant tools.
Implement and manage Azure SQL Database, Azure Data Lake Storage, and other Azure data storage solutions.
Ensure data integrity, security, and compliance with industry best practices and organizational standards.
Utilize Azure Databricks and other tools for data transformation and processing.
Optimize queries and data processing workflows for performance and efficiency.
Must Have Skills
Bachelor's degree in Computer Science, Information Technology, or a related field.
4+ Years of Experience in Data Integration platform on cloud (Azure preferred)
In-depth knowledge of Azure Data Factory, Azure SQL Database, Azure Databricks, and other relevant Azure services.
Strong proficiency in SQL, Python, and/or other scripting languages.
Proven experience as a Data Engineer with a focus on Azure cloud technologies
Knowledge on GitHub Integration with Azure DevOps
Emtec is an Equal Opportunity Employer
US citizens and those authorized to work in the US are encouraged to apply
Applicants for employment in the US must possess work authorization which does not require sponsorship now or in the future by the employer for a visa
Show more
Show less","Azure Data Factory, Azure Databricks, Azure SQL Database, Azure Data Lake Storage, ETL, Data Integration, Cloud Computing, Microsoft Azure, Data Engineering, Data Transformation, Data Processing, Data Quality, Data Security, Data Compliance, SQL, Python, Scripting Languages, GitHub Integration, Azure DevOps","azure data factory, azure databricks, azure sql database, azure data lake storage, etl, data integration, cloud computing, microsoft azure, data engineering, data transformation, data processing, data quality, data security, data compliance, sql, python, scripting languages, github integration, azure devops","azure data factory, azure data lake storage, azure databricks, azure devops, azure sql database, cloud computing, data compliance, data engineering, data integration, data processing, data quality, data security, data transformation, etl, github integration, microsoft azure, python, scripting languages, sql"
"Senior Data Engineer - Python, AWS, Databricks",Wizeline,United States,https://www.linkedin.com/jobs/view/senior-data-engineer-python-aws-databricks-at-wizeline-3774839186,2023-12-17,Smith Center,United States,Mid senior,Onsite,"The Company
Wizeline is a global digital services company helping mid-size to Fortune 500 companies build, scale, and deliver high-quality digital products and services.
We thrive in solving our customer’s challenges through human-centered experiences, digital core modernization, and intelligence everywhere (AI/ML and data). We help them succeed in building digital capabilities that bring technology to the core of their business.
Our People
At Wizeline, we are a team of near 2,000 people spread across 25+ countries. We understand that great technology begins with outstanding talent and diversity of thought. Our business was built on doing well and doing good, and our values of Ownership, Innovation, Community, and Diversity & Inclusion are embedded within our company’s DNA. We are committed to offering our Wizeliners the opportunity to create their career path and develop the skills needed to achieve their personal goals.
Community Impact
We are proud to contribute to local economies by developing technology ecosystems in places like Mexico, Colombia, and Vietnam. We also created Wizeline Academy, a free, community-based education program that teaches high-value skills to workers looking to advance their tech industry careers. As of 2022, Academy has served more than 28,000 students across 675 courses. Wizeliners have the opportunity to upskill by taking Wizeline Academy courses and can also share their expertise by delivering classes to students.
Your Day-to-Day
This job posting is for Data Engineer on our team. Here's what you'll be doing in your day-to-day work:
Design and implement product features in collaboration with product owners, report developers, product analysts, architects, and business partners within an Agile / Scrum methodology.
Design and implement data platforms for large-scale, high-performance, and scalable requirements, integrating data from several data sources, and managing structured and unstructured data while melding existing warehouse structures.
Analyze, diagnose and identify bottlenecks in data workflows
Participate in demos to clients and requirements elicitation and translation to systems requirements (functional and nonfunctional).
Constantly monitor, refine and report on the performance of data management systems.
Are You a Fit?
To Be Successful In This Role, You Must Have
Strong Programming Skills with Python.
Solid experience with EMR and Databricks.
Solid engineering foundations (good coding practices, good architectural design skills).
Experience working with SQL in advanced scenarios that require heavy optimization.
7+ years of experience with large-scale data engineering with an emphasis on analytics and reporting
Experience building cloud-scalable, real-time and high-performance Data Lake solutions.
Proficiency in designing and implementing ETL (Extract, Transform, load) processes, dealing with big volumes of data (terabytes of data which required distributed processing)
Experience developing solutions within Cloud Services (AWS)
Experience in data streams processing technologies including Kafka, Spark Streaming, etc (nice to have)
Advanced English level.
Why You Should Apply
Still Not Convinced You Should Apply? Here Are Some Of The Things That Make Wizeline Different From Other Technology Services Companies
Our Values
At Wizeline, we value innovation, community, and ownership. Our commitment to diversity, inclusion, and respect fosters an environment where everyone does well and does good. We're proud to be recognized by the Human Rights Campaign Foundation in response to our inclusive corporate policies and best practices for LGBTQ+ employees.
Our Culture
We offer exceptional career growth and learning opportunities to our employees with skill development workshops, mentoring programs, and support for side projects or entrepreneurial work. We ranked 7th on Expansión’s Super Companies 2021 list.
Our Benefits And Perks
We offer competitive compensation and employee-centric benefits, including industry-leading maternity and paternity leave, wellness programs, and remote work opportunities. All Wizeliners have access to continuous learning opportunities through Wizeline Academy, including cloud certifications, mentorship, LinkedIn Learning, Udemy, and in-house technical bootcamps developed by our experts in the field.
In recognition of our superb employee benefits, we debuted on Quartz’s list of the Best Companies for Remote Workers and ranked 5th on Expansión’s Super Workspaces 2021 list.
Our Global Family
To support the global nature of our business and help our employees grow beyond their technical skills, we offer free virtual English and Spanish language classes and provide immigration support when applicable. Before COVID-19, we had a robust Work Abroad program in place, which we plan to continue once it’s safe.
#AllAreWelcomeHere
Wizeline creates and fosters a diverse, inclusive, and harassment-free workplace where everyone can achieve their potential. All applicants will be considered for employment regardless of race, color, religion, sex, sexual orientation, gender identity, national origin, veteran, or disability status.
Show more
Show less","Python, EMR, Databricks, SQL, AWS, Kafka, Spark Streaming, ETL, Cloud Services, Data Lake, Agile / Scrum methodology","python, emr, databricks, sql, aws, kafka, spark streaming, etl, cloud services, data lake, agile scrum methodology","agile scrum methodology, aws, cloud services, data lake, databricks, emr, etl, kafka, python, spark streaming, sql"
Senior Data Systems Engineer - BI,VISTRADA,United States,https://www.linkedin.com/jobs/view/senior-data-systems-engineer-bi-at-vistrada-3169754037,2023-12-17,Smith Center,United States,Mid senior,Onsite,"Vistrada is seeking highly motivated candidates to serve as senior data systems engineers to work as part of our Business Intelligence team supporting multiple clients. Vistrada’s clients look to us to provide technical leadership and assist them in solving many of today’s most challenging digital transformation issues and complex data needs. In this role, you will help address operational challenges associated with data modeling, system architectures, and reporting requirements by applying advanced capabilities, modern technology, and best practices to real-world scenarios.
Key Responsibilities:
Work closely with clients to develop technology innovation plans and enhancements on a broad range of areas, including:
Model-based system engineering
Relational data architectures
Data modeling
Data integration processes and ETL development
Report / Dashboard creation using leading visualization technology platforms (i.e., Tableau; Qlik; Power BI)
Data analytics platform design, development, and testing
Machine Learning and/or Statistical Analysis.
Required Qualifications:
Bachelor’s Degree and 8+ years of related experience
5+ years of experience with SQL databases, such as MSSQL, Oracle, MySQL, and/or PostgreSQL
5+ years of experience with leading visualization technology platforms (i.e., Tableau; Qlik; Power BI)
Ability to work independently and eager to learn new technologies, techniques, processes, software languages, platforms, and systems
Expertise to provide unbiased advice, formulate courses of action, analyze programs, and make recommendations across a wide spectrum of issues
Passionate, goal driven, team-oriented, and outgoing
Flexible, self-starter, and demonstrated ability to operate effectively with ambiguous and evolving objectives in a client-facing environment
Effective communication skills.
Preferred Qualifications
2+ years of experience with cloud architecture systems, such as Azure, Google Cloud, or AWS
Competency with source code management systems
Competency with Office365
Strong written communication skills.
Show more
Show less","Modelbased system engineering, Relational data architectures, Data modeling, Data integration processes, ETL development, Tableau, Qlik, Power BI, Data analytics platform design, Machine Learning, Statistical Analysis, SQL, MSSQL, Oracle, MySQL, PostgreSQL, Azure, Google Cloud, AWS, Source code management systems, Office365","modelbased system engineering, relational data architectures, data modeling, data integration processes, etl development, tableau, qlik, power bi, data analytics platform design, machine learning, statistical analysis, sql, mssql, oracle, mysql, postgresql, azure, google cloud, aws, source code management systems, office365","aws, azure, data analytics platform design, data integration processes, datamodeling, etl development, google cloud, machine learning, modelbased system engineering, mssql, mysql, office365, oracle, postgresql, powerbi, qlik, relational data architectures, source code management systems, sql, statistical analysis, tableau"
Senior Data Engineer - Spark,Wizeline,United States,https://www.linkedin.com/jobs/view/senior-data-engineer-spark-at-wizeline-3774838576,2023-12-17,Smith Center,United States,Mid senior,Onsite,"The Company
Wizeline is a global digital services company helping mid-size to Fortune 500 companies build, scale, and deliver high-quality digital products and services.
We thrive in solving our customer’s challenges through human-centered experiences, digital core modernization, and intelligence everywhere (AI/ML and data). We help them succeed in building digital capabilities that bring technology to the core of their business.
Our People
At Wizeline, we are a team of near 2,000 people spread across 25+ countries. We understand that great technology begins with outstanding talent and diversity of thought. Our business was built on doing well and doing good, and our values of Ownership, Innovation, Community, and Diversity & Inclusion are embedded within our company’s DNA. We are committed to offering our Wizeliners the opportunity to create their career path and develop the skills needed to achieve their personal goals.
Community Impact
We are proud to contribute to local economies by developing technology ecosystems in places like Mexico, Colombia, and Vietnam. We also created Wizeline Academy, a free, community-based education program that teaches high-value skills to workers looking to advance their tech industry careers. As of 2022, Academy has served more than 28,000 students across 675 courses. Wizeliners have the opportunity to upskill by taking Wizeline Academy courses and can also share their expertise by delivering classes to students.
Your Day-to-Day
This job posting is for Data Engineer on our team. Here's what you'll be doing in your day-to-day work:
Design and implement product features in collaboration with product owners, report developers, product analysts, architects, and business partners within an Agile / Scrum methodology.
Design and implement data platforms for large-scale, high-performance, and scalable requirements, integrating data from several data sources, and managing structured and unstructured data while melding existing warehouse structures.
Analyze, diagnose and identify bottlenecks in data workflows
Participate in demos to clients and requirements elicitation and translation to systems requirements (functional and nonfunctional).
Constantly monitor, refine and report on the performance of data management systems.
Are You a Fit?
To Be Successful In This Role, You Must Have
Good Programming Skills with Python.
Strong experience with Spark
Experience with Scala.
Solid engineering foundations (good coding practices, good architectural design skills).
Experience working with SQL in advanced scenarios that require heavy optimization.
Experience building cloud-scalable, real-time and high-performance Data Lake solutions.
Good experience with Airflow.
Experience developing solutions within Cloud Services (AWS)
Good experience with AWS Athena, S3, Glue.
Experience in data streams processing technologies including Kafka, Spark Streaming, etc (nice to have)
Advanced English level.
Why You Should Apply
Still Not Convinced You Should Apply? Here Are Some Of The Things That Make Wizeline Different From Other Technology Services Companies
Our Values
At Wizeline, we value innovation, community, and ownership. Our commitment to diversity, inclusion, and respect fosters an environment where everyone does well and does good. We're proud to be recognized by the Human Rights Campaign Foundation in response to our inclusive corporate policies and best practices for LGBTQ+ employees.
Our Culture
We offer exceptional career growth and learning opportunities to our employees with skill development workshops, mentoring programs, and support for side projects or entrepreneurial work. We ranked 7th on Expansión’s Super Companies 2021 list.
Our Benefits And Perks
We offer competitive compensation and employee-centric benefits, including industry-leading maternity and paternity leave, wellness programs, and remote work opportunities. All Wizeliners have access to continuous learning opportunities through Wizeline Academy, including cloud certifications, mentorship, LinkedIn Learning, Udemy, and in-house technical bootcamps developed by our experts in the field.
In recognition of our superb employee benefits, we debuted on Quartz’s list of the Best Companies for Remote Workers and ranked 5th on Expansión’s Super Workspaces 2021 list.
Our Global Family
To support the global nature of our business and help our employees grow beyond their technical skills, we offer free virtual English and Spanish language classes and provide immigration support when applicable. Before COVID-19, we had a robust Work Abroad program in place, which we plan to continue once it’s safe.
#AllAreWelcomeHere
Wizeline creates and fosters a diverse, inclusive, and harassment-free workplace where everyone can achieve their potential. All applicants will be considered for employment regardless of race, color, religion, sex, sexual orientation, gender identity, national origin, veteran, or disability status.
Show more
Show less","Python, Spark, Scala, SQL, Airflow, AWS, AWS Athena, AWS S3, AWS Glue, Kafka, Spark Streaming","python, spark, scala, sql, airflow, aws, aws athena, aws s3, aws glue, kafka, spark streaming","airflow, aws, aws athena, aws glue, aws s3, kafka, python, scala, spark, spark streaming, sql"
Senior Data Engineer,Orange Quarter,United States,https://www.linkedin.com/jobs/view/senior-data-engineer-at-orange-quarter-3755843809,2023-12-17,Smith Center,United States,Mid senior,Onsite,"We are currently looking for Data people of all shapes and sizes to join an Italian giant who offer 100% remote options. They were founded in 2015 and have since raised over €170M in funding through organisations such as Goldman Sachs. Their product operates in the insurance space and since the first policy was sold, they have acquired over 2.2 million customers and grown the team to over 800 people. Their advantage came through redesigning and digitising the entire insurance value chain, powered by the most advanced tech stack and data analytics capabilities.
Industry
Fintech
OQ-industries Insurance
What To Expect
You will be involved in shaping the requirements and designing well engineered data pipelining and warehousing solutions that will support the business as it grows from inception to scale. This will not just involve working with the engineering teams but also with all business functions from Product to Finance, so you must also want to be an entrepreneur from within the business. Their current Data team consists of around 12 people and as their Software teams grow, the same needs to happen with Data.
Perks
100% remote options
Consistent internal and external trainings
Welfare and company benefits
The latest, state of the art tech
Requirements
Requirements:
Extensive experience programming with Python
Exposure to working with Data processing using Spark and PySpark
Knowledge in SQL, ETL design and development and pipeline orchestration
Familiarity with infrastructure platforms such as AWS
Sounds good?
Apply now
Show more
Show less","Python, Spark, PySpark, SQL, ETL, Data warehousing, Data pipelining, AWS, Data analytics","python, spark, pyspark, sql, etl, data warehousing, data pipelining, aws, data analytics","aws, dataanalytics, datapipeline, datawarehouse, etl, python, spark, sql"
Lead Cloud Database Engineer,Ultimate Medical Academy,United States,https://www.linkedin.com/jobs/view/lead-cloud-database-engineer-at-ultimate-medical-academy-3774908948,2023-12-17,Smith Center,United States,Mid senior,Onsite,"Purpose of the Position
The Lead Cloud Database Engineer leads in the systems implementation and management of UMA cloud and on-prem database systems in support of day-to-day 24x7 IT operations. The Lead Cloud Database Engineer is responsible for hands-on delivery and leadership of a small database team that supports, maintains, administers our enterprise data platform solutions at UMA. This position operates as the leader of UMA’s Data Platform engineering initiative as we migrate to a could architecture.
The Lead Cloud Database Engineer reports to the Director, Enterprise Data Engineering.
UMA is a nonprofit healthcare educational institution with a national presence. Headquartered in Tampa, Florida and founded in 1994, UMA offers content-rich, interactive online programs as well as hands-on training at our Clearwater, Florida campus.
Anticipated starting salary of $125,000; pay rate negotiable based on experience and qualifications.
Medical (including prescription), Dental, Vision (UMA subsidized)
FSA/HSA (Depending on Medical Plan chosen)
$50,000 Life Insurance (UMA paid)
Additional Voluntary Life Insurance (Team Member paid)
Employee Assistance Program – EAP (UMA paid)
Long Term Disability (UMA paid)
Short Term Disability (Team Member paid)
Supplemental Insurance such as Critical Illness, Accident, and Hospital (Team Member Paid)
Paid Time Off – 15 days accrued in year 1, 9 holidays, and 1 day of Volunteering Time Off
401k (eligible upon completion of 90 days of employment and must be at least 21 years of age)
Pet Insurance
Identity Theft Protection
Key Responsibilities
Functional
Own the outcome. Execute as one, together with IT peers.
Lead projects and team to create and maintain optimal database architecture.
Guide development to assemble large, complex databases that meet enterprise requirements for both OLTP and analytical workloads.
Steer expansion and optimization of UMA’s database architecture for cross functional teams.
Provide operational and functional leadership for creating, storing, managing, and maintaining enterprise data, including the ability to incorporate policies and procedures for centrally managing and sharing data through the data life cycle.
Partner with application and product teams to define and deliver on database needs.
Participate in short-and intermediate-term planning for data initiatives and projects including identifying dependencies, capacity planning and resource allocation.
Identify, design, and implement internal process improvements automating manual processes, optimizing data delivery, and re-designing for flexibility and scalability.
Design and implement proactive database monitoring and guide rapid issue resolution of issues. Ensure databases are performing to SLAs,
Define infrastructure required for a variety of cloud and on-premises databases.
Work with stakeholders including to assist with database technical issues and support their data infrastructure needs.
Work closely with Infrastructure, DevOps, system engineering, data architecture, data governance and data analytics teams to ensure databases adhere to enterprise standards, usability, and performance.
Guide and mentor team members, peers, and business partners on functional requirements, best practices, effective design patterns and data maturity.
Be responsible for implementation of database safeguards and security adherent to UMA enterprise standards.
Assist with implementation of DataOps and DevSecOps for data initiatives using UMA’s preferred processes and tools. Participate as team member / leader in Agile/Scrum team.
Work on continuous improvement initiatives and effectively address root cause analysis outcomes to deliver sustainable excellence.
Mentor, coach and support direct reports to excel at team deliverables.
Ensure data engineering design and deliverables adhere to regulatory and compliance requirements.
Participate in On-Call rotation for after-hours support and troubleshooting
Perform other duties as assigned.
Technical
Ensure overall performance, reliability, stability, and availability of UMA’s Cloud and on-prem database systems and infrastructure.
Lead the configuration, management, and implantation of cloud and on-prem data systems.
Perform proactive maintenance, monitoring, and troubleshooting of SQL Server, Azure SQL, Databrick, Cosmos and all other database systems.
Troubleshoot database service outages as they occur, including after-hours and weekends.
Create documentation on database recovery and architecture effective provisioning, installation/configuration, operation, and maintenance of systems hardware and software and related infrastructure.
Work to ensure that the associated hardware resources allocated to the databases are configured for high resilience and are tuned for optimum performance.
When performance issues arise, determine the most effective way to increase performance server configuration changes, or index/query changes.
Identify and partner with developers and leaders to remediate inefficient or suboptimal performance of queries, processes, or data structures.
Guide complex and critical data/database management initiatives that typically involve multiple disciplines and multiple business groups, including responsibility for database architecture, design, integration, and/or data modeling.
Responsible for improvement and maintenance of the databases to include rollout and upgrades.
Work with IT Engineering and DevOps to ensure all database systems are backed up in a way that meets the business’s Recovery Point Objectives (RPO).
Test backups to ensure we can meet the business’ Recovery Time Objectives (RTO).
Lead architectural decisions for cloud and on-prem data system implementations and upgrades including clustering, log shipping, mirroring, SQL Database servers, Azure SQL Databases, Datalakes, Data Factory, Databricks, or other technologies.
Lead the configuration, tuning, and management SQL Always-On Availability Groups.
Deploy database change scripts provided by third party vendors.
Perform load and execution testing.
Develop and deploy methodologies for testing database performance and providing performance statistics and reports.
Responsible for assisting developers in creating stored procedures and SQL commands to properly query databases.
Continued development of database administration skills; keep abreast of latest techniques, best practices, and trends and able to apply to project work.
Lead developing SQL queries/scripts and similar artifacts to validate the completeness, integrity, and accuracy of data within data load/refresh, backup and recovery processes.
Lead in data analysis efforts to resolve complex database performance issues.
Lead with the design and development of automated solutions (scripts, functions, programs, processes) to increase the efficiency of the database administration and monitoring processes.
Document designs and specifications that adhere to company practices.
Communicate needs as well as deployment and operations standards to Infrastructure Services.
Support Institutional Initiatives
Keep current on institutional goals, objectives, and progress.
Engage in institutional sponsored activities and initiatives as appropriate.
Lead with courage in support of change initiatives that impact organization.
Manages and assists in coordinating efforts between departments within UMA.
Compliance
Demonstrates knowledge of, and carefully follows all applicable federal and state compliance requirements and regulations including those prescribed by the Department of Education, accrediting agencies, CIE, and internal UMA policies and procedures.
Effectively communicates compliance requirements to students and other staff as appropriate and quickly escalates any compliance concerns to the Compliance department.
Work Experience, Skills & Abilities
Minimum Requirements
Bachelor’s degree or equivalent combination of education and experience.
2+ years of leadership experience.
9+ years of experience with database design, and administration experience with Microsoft SQL Server.
9+ years of experience writing T-SQL and stored procedures and query tuning on high transaction systems.
5+ years of experience with Microsoft SQL Server in a high availability environment including Failover Clusters, Availability Groups and Transactional Replication.
5+ years of experience with Microsoft Azure cloud database services such as Azure SQL Databases, SQL Server Stretch Databases.
5+ years in CDC and Data Replication techniques, processes, and best practices and able to provide guidance to other development teams (BI, ETL, Software Engineering, etc.).
Deep knowledge of both OLTP and Analytical databases.
Possess in-depth SQL knowledge to understand and review SQL and host language programs and to recommend changes for optimization.
Ability to prioritize in the face of demanding time frames.
Proficient at collecting, storing, and managing the ability to query the organization's metadata.
Able to integrate database administration requirements and tasks with general systems management requirements and tasks (such as job scheduling, network management, transaction processing, etc.).
Able to professionally communicate fluently in verbal and written English and effectively communicate with all level of the organization.
Able to work in a team focused environment.
Excellent customer service mindset and approach.
Self-motivated; strong work ethic.
Proficient in MS Office (Word, Excel, PowerPoint) and other business tools such as Skype and Microsoft Teams.
Able to support a diverse and inclusive work environment.
Preferred Requirements
5+ years of experience with Microsoft Azure cloud data services such as ADLS, DataBricks, Power BI, and Data Factory.
Experience with COSMOS DB
Proficient experience with Database monitoring tool sets such as SolarWinds DPA.
Proficient experience with Database Modeling and Design Tools (e.g., Erwin, Oracle Designer, ER Studio, etc.).
Ability to translate a data model or logical database design into an actual physical database implementation and to manage that database once it has been implemented.
Proficient experience with SQL Source control tools such as RedGate or VSTS.
Possess procedural skills to help design, debug, implement, and maintain stored procedures, triggers, and user-defined functions that are stored in the DBMS.
Possess understanding of the advanced tenets of relational database technology and be able to accurately communicate them to others.
Working Environment
Ability to work full-remote, on-site, or a hybrid attendance model.
Minimal travel required-Valid drivers license required.
Flexibility to work evenings and weekends as needed.
Physical Demands
Requires long periods of sitting at a desk working on a computer.
Requires ability to travel.
Requires occasional bending, stooping and squatting.
Requires occasional lifting of up to 10 lbs.
OUR VALUES
Our institutional values are shaped and validated by our team members. They describe how we strive to operate and are the standards of behavior we look to embody.
ACT WITH INTEGRITY
We operate honestly and ethically in a compliant fashion. We are fair and trustworthy in our interactions with all we serve. Our team members, at all levels, lead by example and strive to do the right thing for our students and for each other. We are disciplined professionals who strive to be straightforward and dependable.
CHAMPION STUDENT OUTCOMES
We live by a strong commitment to our students and are passionate about preparing them for meaningful careers. We are deeply dedicated to ensuring students’ educational and career success. We excel at building our students’ confidence and empowering them to reach their full potential.
COMMIT TO TEAM MEMBER SUCCESS
We are committed to our team members’ success. We strive to create an environment that attracts and retains the best talent while offering continuous learning, professional development, and career growth opportunities. We recognize and reward our team members for their contributions to the organization and to our students.
PURSUE RESULTS WITH PURPOSE
We pursue results with a sense of purpose reflective of our student-centered mission. We take responsibility for supporting ambitious student outcomes and hold each other accountable to this end. We think strategically and critically, greet new ideas and challenges openly, and look for innovative solutions to challenges.
HAVE FUN. BUILD ENERGY
Enjoying what we do is central to achieving our goals. Building energy, having fun, being optimistic, and creating a positive working environment are all critical to our success and that of our students. We strive to be inspired and to inspire others. We consistently show appreciation and celebrate our successes, both large and small.
WORK AS ONE
We believe that diverse, inclusive teams produce breakthrough results. We strive to build and maintain positive relationships with team members from all types of backgrounds by showing respect and humility when interacting with each other and resolving conflicts in a constructive manner. By working together, we win together, ensuring that the goals of the institution are the focal point of our efforts.
Acknowledgement
This job description is intended to be an accurate reflection of the current position. It may not include every job responsibility. Job duties may fluctuate based on needs of the business. UMA reserves the right to revise the job or require that additional or different tasks be performed.
I have read and understand all the responsibilities and accountabilities in my job description. This job description is a tool used by me and management to help assess, evaluate and provide guidelines and expectations for the position named herein. I understand that it is not a contract and it does not constitute any guarantee of future employment.
UMA will NEVER ask you to send money or ask you to provide bank account information in order for you to get reimbursed for tools to work. If you have been contacted by someone claiming to be from UMA about a job posting, you can always verify the position at https//careers-ultimatemedical.icims.com/
Show more
Show less","SQL Server, Azure SQL, Databricks, Cosmos DB, OLTP, Analytics, TSQL, Stored procedures, Query tuning, Failover Clusters, Availability Groups, Transactional Replication, CDC, Data Replication, MS Office, Skype, Microsoft Teams, ADLS, Power BI, Data Factory, SolarWinds DPA, RedGate, VSTS, ER Studio","sql server, azure sql, databricks, cosmos db, oltp, analytics, tsql, stored procedures, query tuning, failover clusters, availability groups, transactional replication, cdc, data replication, ms office, skype, microsoft teams, adls, power bi, data factory, solarwinds dpa, redgate, vsts, er studio","adls, analytics, availability groups, azure sql, cdc, cosmos db, data factory, data replication, databricks, er studio, failover clusters, microsoft teams, ms office, oltp, powerbi, query tuning, redgate, skype, solarwinds dpa, sql server, stored procedures, transactional replication, tsql, vsts"
Healthcare Data Analyst III-PPSQ,Boston Children's Hospital,United States,https://www.linkedin.com/jobs/view/healthcare-data-analyst-iii-ppsq-at-boston-children-s-hospital-3767056545,2023-12-17,Smith Center,United States,Mid senior,Onsite,"The Healthcare Data Analyst III Is Responsible For
Researches, designs, develops, implements, and maintains reporting and analytical facilities in the context of the hospital's clinical and/or financial systems.
Collaborates with clinical and administrative staff to ensure that reports are timely, accurate, meaningful, and interpretable.
Ensures that analyses and reports provide detailed, specific data regarding the performance of individuals, teams, programs, divisions, and the department as a whole.
Generate reports from a variety of internal and external sources, including benchmarking databases e.g. PHIS, PACT, and specialty-specific registries.
Acts as a subject matter expert for visualization tools (e.g. Microstrategy, Tableau, etc.) and techniques for insightful display of data for business decision-makers.
Builds new databases as needed, and maintains a valid and accurate (validated) data repository to ensure availability of required information.
Advises on the development of data collection and management systems.
Defines specifications for key data elements, data collection procedures, and integrity of databases. Improves and maintains the production environment for tasks and applications.
Analyzes data and generates reports of performance indicators, providing trended data using statistical process control or other appropriate epidemiological techniques.
Aids team in the interpretation of data and provides team with data-driven recommendations.
Creates new benchmarks (when none are available) using large data sets, both internal (e.g. patient registries) and external (e.g. PHIS, national/international learning collaboratives).
Assists in publishing performance to add benchmarks where none currently exist.
Maintains sufficient knowledge of biostatistical techniques to ensure effective collaboration with biostatisticians and epidemiologists regarding appropriate data analysis.
Participates in regular ‘Report Development’ Users Group within CHB and adheres to appropriate technical standards and guidelines.
To Qualify, you must have:
A Bachelor's Degree and 4 years using Data Analytics tools in a clinical setting.
Experience with Data Analytics tools High level of knowledge of SQL syntax, and coding practices. Expert with dimensional data modeling concepts used in data warehousing, business intelligence, and business analytics applications.
Ability to translate reporting needs expressed in clinical or operational terms to technical solutions based on existing data, data structures, and query tool functionality.
Show more
Show less","Data Analytics, Statistics, Biostatistics, Epidemiology, Data Warehousing, Business Intelligence, Business Analytics, Statistical Process Control, SQL, Dimensional Data Modeling, Data Visualization, Trend Analysis, Data Interpretation, DataDriven Recommendations, Benchmarking, Microstrategy, Tableau","data analytics, statistics, biostatistics, epidemiology, data warehousing, business intelligence, business analytics, statistical process control, sql, dimensional data modeling, data visualization, trend analysis, data interpretation, datadriven recommendations, benchmarking, microstrategy, tableau","benchmarking, biostatistics, business analytics, business intelligence, data interpretation, dataanalytics, datadriven recommendations, datawarehouse, dimensional data modeling, epidemiology, microstrategy, sql, statistical process control, statistics, tableau, trend analysis, visualization"
Lead Data Engineer,Orange Quarter,United States,https://www.linkedin.com/jobs/view/lead-data-engineer-at-orange-quarter-3755842924,2023-12-17,Smith Center,United States,Mid senior,Onsite,"Orange Quarter is currently working with an insurance-tech startup and Italy’s first tech unicorn. With the goal of providing customers with the highest quality experience and best price value, they have revolutionized the insurance value chain through digitalization and data analytics. Founded in 2015, this company received 100 million in investments back in 2018 and boasts over 2 and a half million users. They are now expanding across the EU + UK and growing their engineering team.
Industry
OQ-industries Insurtech
What To Expect
If successful, you will be key in establishing the data landscape and ensuring that is scalable throughout the European market. You will be key in influencing the technical roadmap and ensuring the technical solutions align with the business demands and vision of the company. You would be working on the data capabilities and data platform, whilst collaborating closely with the engineering and data science teams to develop long-term data-oriented solutions for partners.
Perks
High impact in establishing the data landscape
Chance to be part of a Unicorn scale-up
Private healthcare insurance
Fully remote within the EU
Competitive pension
Requirements
Requirements:
Stregnths in distributed big data processing using Spark SQL and PySpark, or Databricks
Experience in building scalable data solutions on AWS
Expert in coding in SQL, as well as pipeline orchestration, data modelling, and data quality testing methods
4+ years of Data Engineering experience
Right to work in the EU
Sounds good?
Apply now
Show more
Show less","Spark SQL, PySpark, Databricks, AWS, SQL, Pipeline orchestration, Data modelling, Data quality testing, Data Engineering","spark sql, pyspark, databricks, aws, sql, pipeline orchestration, data modelling, data quality testing, data engineering","aws, data engineering, data modelling, data quality testing, databricks, pipeline orchestration, spark, spark sql, sql"
Data Analyst I - Windreich Department of Artificial Intelligence & Human Health,Mount Sinai Health System,United States,https://www.linkedin.com/jobs/view/data-analyst-i-windreich-department-of-artificial-intelligence-human-health-at-mount-sinai-health-system-3760367139,2023-12-17,Smith Center,United States,Mid senior,Onsite,"Description
Strength Through Diversity
Ground breaking science. Advancing medicine. Healing made personal.
Roles & Responsibilities:
The Windreich Department of Artificial Intelligence and Human Health within the Icahn School of Medicine at Mount Sinai is searching for a Data Analyst to join Dr. Ipek Ensari’s research laboratory. Our group conducts research in the intersection of biomedical informatics, artificial intelligence, and women’s health. We have several studies that use patient-generated data obtained from various sources (e.g., wearables, sensors, electronic health records) and digital health technologies to investigate women’s reproductive disorders and design disease management strategies. The ideal candidate will possess a combination of technical, domain-specific, and soft skills to analyze complex datasets, evaluate data quality, and communicate insights clearly to technical and non-technical stakeholders.
Responsibilities
Acquire and manage patient-generated health data from various sources, ensuring data integrity, privacy, and security.
Process, clean, and organize large datasets, making them suitable for analysis and modeling.
Evaluate quality and integrity of the study data obtained from various sources (e.g., wearables, mobile phone Apps, electronic health records)
Employ statistical techniques to interpret and analyze healthcare data.
Work closely with healthcare professionals to understand the context and implications of data findings within the realm of women’s health.
Aid the research team in formulating questions and hypotheses by providing insights from preliminary data analyses.
Contribute to the design of research methods, ensuring that data-related aspects are adequately addressed.
Review the latest literature and integrate findings to improve and innovate the lab’s approaches and methodologies.
Clearly convey data findings, insights, and implications to both technical and non-technical stakeholders through reports, presentations, and discussions.
Qualifications
Bachelors degree in computer science, statistics and/or related field, or combination of equivalent work experience and education. Masters degree in relevant field of study preferred.
2-3 years database application/management experience, preferably in a research or healthcare setting.
Proficiency in statistical software (e.g. R, Python) and database management skills.
Strong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy.
Knowledge of data privacy and security.
Experience with mixed-level modeling (or other similar models) and generalized additive models is a strong plus.
Must be able to
work closely with others.
communicate effectively, both written and verbal.
learn/grow skill sets.
Employer Description
Strength Through Diversity
The Mount Sinai Health System believes that diversity, equity, and inclusion are key drivers for excellence. We share a common devotion to delivering exceptional patient care. When you join us, you become a part of Mount Sinai’s unrivaled record of achievement, education, and advancement as we revolutionize medicine together. We invite you to participate actively as a part of the Mount Sinai Health System team by:
Using a lens of equity in all aspects of patient care delivery, education, and research to promote policies and practices to allow opportunities for all to thrive and reach their potential.
Serving as a role model confronting racist, sexist, or other inappropriate actions by speaking up, challenging exclusionary organizational practices, and standing side-by-side in support of colleagues who experience discrimination.
Inspiring and fostering an environment of anti-racist behaviors among and between departments and co-workers.
We work hard to acquire and retain the best people and to create an inclusive, welcoming and nurturing work environment where all feel they are valued, belong and are able to professional advance. We share the belief that all employees, regardless of job title or expertise contribute to the patient experience and quality of patient care.
Explore more about this opportunity and how you can help us write a new chapter in our history!
“About the Mount Sinai Health System:
Mount Sinai Health System is one of the largest academic medical systems in the New York metro area, with more than 43,000 employees working across eight hospitals, more than 400 outpatient practices, more than 300 labs, a school of nursing, and a leading school of medicine and graduate education. Mount Sinai advances health for all people, everywhere, by taking on the most complex health care challenges of our time — discovering and applying new scientific learning and knowledge; developing safer, more effective treatments; educating the next generation of medical leaders and innovators; and supporting local communities by delivering high-quality care to all who need it. Through the integration of its hospitals, labs, and schools, Mount Sinai offers comprehensive health care solutions from birth through geriatrics, leveraging innovative approaches such as artificial intelligence and informatics while keeping patients’ medical and emotional needs at the center of all treatment. The Health System includes approximately 7,400 primary and specialty care physicians; 13 joint-venture outpatient surgery centers throughout the five boroughs of New York City, Westchester, Long Island, and Florida; and more than 30 affiliated community health centers. We are consistently ranked by U.S. News & World Report's Best Hospitals, receiving high ""Honor Roll"" status, and are highly ranked: No. 1 in Geriatrics and top 20 in Cardiology/Heart Surgery, Diabetes/Endocrinology, Gastroenterology/GI Surgery, Neurology/Neurosurgery, Orthopedics, Pulmonology/Lung Surgery, Rehabilitation, and Urology. New York Eye and Ear Infirmary of Mount Sinai is ranked No. 12 in Ophthalmology. U.S. News & World Report’s “Best Children’s Hospitals” ranks Mount Sinai Kravis Children's Hospital among the country’s best in several pediatric specialties. The Icahn School of Medicine at Mount Sinai is ranked No. 14 nationwide in National Institutes of Health funding and in the 99th percentile in research dollars per investigator according to the Association of American Medical Colleges. Newsweek’s “The World’s Best Smart Hospitals” ranks The Mount Sinai Hospital as No. 1 in New York and in the top five globally, and Mount Sinai Morningside in the top 20 globally.
The Mount Sinai Health System is an equal opportunity employer. We comply with applicable Federal civil rights laws and does not discriminate, exclude, or treat people differently on the basis of race, color, national origin, age, religion, disability, sex, sexual orientation, gender identity, or gender expression. We are passionately committed to addressing racism and its effects on our faculty, staff, students, trainees, patients, visitors, and the communities we serve. Our goal is for Mount Sinai to become an anti-racist health care and learning institution that intentionally addresses structural racism.”
EOE Minorities/Women/Disabled/Veterans
Compensation
The Mount Sinai Health System (MSHS) provides a salary range to comply with the New York City Law on Salary Transparency in Job Advertisements. The salary range for the role is $58661 - $74250 Annually. Actual salaries depend on a variety of factors, including experience, education, and hospital need. The salary range or contractual rate listed does not include bonuses/incentive, differential pay or other forms of compensation or benefits.
Show more
Show less","Data Analysis, Data Management, Data Security, Statistical Software (R Python), SQL, Machine Learning, Data Visualization, Data Mining, PatientGenerated Health Data, Wearables, Mobile Health Apps, Electronic Health Records, Bioinformatics, Healthcare Informatics, Women's Health, Reproductive Disorders, Disease Management, MixedLevel Modeling, Generalized Additive Models","data analysis, data management, data security, statistical software r python, sql, machine learning, data visualization, data mining, patientgenerated health data, wearables, mobile health apps, electronic health records, bioinformatics, healthcare informatics, womens health, reproductive disorders, disease management, mixedlevel modeling, generalized additive models","bioinformatics, data management, data mining, data security, dataanalytics, disease management, electronic health records, generalized additive models, healthcare informatics, machine learning, mixedlevel modeling, mobile health apps, patientgenerated health data, reproductive disorders, sql, statistical software r python, visualization, wearables, womens health"
Sr Lead Data Engineer (MarTech),Dice,United States,https://www.linkedin.com/jobs/view/sr-lead-data-engineer-martech-at-dice-3788301137,2023-12-17,Smith Center,United States,Mid senior,Onsite,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, BayOne Solutions, is seeking the following. Apply via Dice today!
Job Title: Sr Lead Data Engineer
Duration: 6-12 Month/Contract
Location: Remote (PST Hours)
Skill Set:
Area of Expertise :
MarTech
Orchestration Tool Knowledge :
Adobe Campaign or similar.
Big Data:
Scala, Databricks, Spark SQL, Spark Streaming, Python
Cloud
: Azure , ADF
System Design
: Event Driven Architecture, Design Patterns
Sr Lead Data Engineer (MarTech)
Show more
Show less","Data Engineering, MarTech, Adobe Campaign, Scala, Databricks, Spark SQL, Spark Streaming, Python, Azure, ADF, Event Driven Architecture, Design Patterns","data engineering, martech, adobe campaign, scala, databricks, spark sql, spark streaming, python, azure, adf, event driven architecture, design patterns","adf, adobe campaign, azure, data engineering, databricks, design patterns, event driven architecture, martech, python, scala, spark sql, spark streaming"
Data Analyst I-Addictions Institute,Mount Sinai Health System,United States,https://www.linkedin.com/jobs/view/data-analyst-i-addictions-institute-at-mount-sinai-health-system-3680312780,2023-12-17,Smith Center,United States,Mid senior,Onsite,"Description
Strength Through Diversity
Ground breaking science. Advancing medicine. Healing made personal.
Roles & Responsibilities:
The Data Analyst I is responsible for implementing best information technology data management practices in an effort to improve and ensure quality, streamline data cleaning efforts and deliver analyzable databases within agreed upon project timelines.
Responsibilities
Designs and writes custom applications needed to ensure the database meets requirements for the entry, management and reporting of data.
Assists in the development of data management plans and guidelines that will deliver accurate, timelines consistent with quality data. Assures the quality control compliance process.
Identifies and implements solutions to data management issues and concerns that arise during the conduct of research studies.
Assists staff with technical issues related to electronic and remote record keeping systems; trains staff to help improve quality of data collected.
Assists medical students, interns, residents and/or program faculty with queries and generating statistical analyses.
Maintains knowledge of the current regulations and technologies related to data management.
May write and prepare manuscripts and other materials for internal and external audiences.
Performs other related duties.
Qualifications
Bachelors degree in computer science, statistics and/or related field, or combination of equivalent work experience and education. Masters degree in relevant field of study preferred.
2-3 years database application/management experience, preferably in a large medical center or healthcare environment.
Employer Description
Strength Through Diversity
The Mount Sinai Health System believes that diversity, equity, and inclusion are key drivers for excellence. We share a common devotion to delivering exceptional patient care. When you join us, you become a part of Mount Sinai’s unrivaled record of achievement, education, and advancement as we revolutionize medicine together. We invite you to participate actively as a part of the Mount Sinai Health System team by:
Using a lens of equity in all aspects of patient care delivery, education, and research to promote policies and practices to allow opportunities for all to thrive and reach their potential.
Serving as a role model confronting racist, sexist, or other inappropriate actions by speaking up, challenging exclusionary organizational practices, and standing side-by-side in support of colleagues who experience discrimination.
Inspiring and fostering an environment of anti-racist behaviors among and between departments and co-workers.
We work hard to acquire and retain the best people and to create an inclusive, welcoming and nurturing work environment where all feel they are valued, belong and are able to professional advance. We share the belief that all employees, regardless of job title or expertise contribute to the patient experience and quality of patient care.
Explore more about this opportunity and how you can help us write a new chapter in our history!
“About the Mount Sinai Health System:
Mount Sinai Health System is one of the largest academic medical systems in the New York metro area, with more than 43,000 employees working across eight hospitals, more than 400 outpatient practices, more than 300 labs, a school of nursing, and a leading school of medicine and graduate education. Mount Sinai advances health for all people, everywhere, by taking on the most complex health care challenges of our time — discovering and applying new scientific learning and knowledge; developing safer, more effective treatments; educating the next generation of medical leaders and innovators; and supporting local communities by delivering high-quality care to all who need it. Through the integration of its hospitals, labs, and schools, Mount Sinai offers comprehensive health care solutions from birth through geriatrics, leveraging innovative approaches such as artificial intelligence and informatics while keeping patients’ medical and emotional needs at the center of all treatment. The Health System includes approximately 7,400 primary and specialty care physicians; 13 joint-venture outpatient surgery centers throughout the five boroughs of New York City, Westchester, Long Island, and Florida; and more than 30 affiliated community health centers. We are consistently ranked by U.S. News & World Report's Best Hospitals, receiving high ""Honor Roll"" status, and are highly ranked: No. 1 in Geriatrics and top 20 in Cardiology/Heart Surgery, Diabetes/Endocrinology, Gastroenterology/GI Surgery, Neurology/Neurosurgery, Orthopedics, Pulmonology/Lung Surgery, Rehabilitation, and Urology. New York Eye and Ear Infirmary of Mount Sinai is ranked No. 12 in Ophthalmology. U.S. News & World Report’s “Best Children’s Hospitals” ranks Mount Sinai Kravis Children's Hospital among the country’s best in several pediatric specialties. The Icahn School of Medicine at Mount Sinai is ranked No. 14 nationwide in National Institutes of Health funding and in the 99th percentile in research dollars per investigator according to the Association of American Medical Colleges. Newsweek’s “The World’s Best Smart Hospitals” ranks The Mount Sinai Hospital as No. 1 in New York and in the top five globally, and Mount Sinai Morningside in the top 20 globally.
The Mount Sinai Health System is an equal opportunity employer. We comply with applicable Federal civil rights laws and does not discriminate, exclude, or treat people differently on the basis of race, color, national origin, age, religion, disability, sex, sexual orientation, gender identity, or gender expression. We are passionately committed to addressing racism and its effects on our faculty, staff, students, trainees, patients, visitors, and the communities we serve. Our goal is for Mount Sinai to become an anti-racist health care and learning institution that intentionally addresses structural racism.”
EOE Minorities/Women/Disabled/Veterans
Compensation
The Mount Sinai Health System (MSHS) provides a salary range to comply with the New York City Law on Salary Transparency in Job Advertisements. The salary range for the role is $58661 - $74250 Annually. Actual salaries depend on a variety of factors, including experience, education, and hospital need. The salary range or contractual rate listed does not include bonuses/incentive, differential pay or other forms of compensation or benefits.
Show more
Show less","data management, database applications, data cleaning, data analysis, statistical analysis, data quality control, electronic record keeping systems, computer science, statistics, manuscripts, Microsoft Office Suite","data management, database applications, data cleaning, data analysis, statistical analysis, data quality control, electronic record keeping systems, computer science, statistics, manuscripts, microsoft office suite","computer science, data cleaning, data management, data quality control, dataanalytics, database applications, electronic record keeping systems, manuscripts, microsoft office suite, statistical analysis, statistics"
Lead Data Engineer/Data Architect,Dice,United States,https://www.linkedin.com/jobs/view/lead-data-engineer-data-architect-at-dice-3779715105,2023-12-17,Smith Center,United States,Mid senior,Onsite,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, Luxoft USA Inc, is seeking the following. Apply via Dice today!
Project Description:
We have an ambitious goal to migrate big legacy Retail Platform based on Oracle ATG framework and Informix Database to Spring Boot microservices for a big retail company.
Solution is implemented by Team Dataverse. This team is mainly focused on Elasticsearch Data Ingestion, focusing on processing all the data so that the website has a fast and consumable data for the web and mobile experiences. In addition, this team also helps to manage data file exports for third parties and SEO pages.
Responsibilities:
We are looking for DB developer who wants to lead data migration portion in various big modernization projects. Knowledge on different DBMS and experience with wide variety of platforms is highly desirable. Individual should be able to come up with data migration approach and must understand the ins and outs of database physical and logical replications, clustering, mirroring, and high availability. Analytic skill in solving problems of building robust and efficient data processing routines both on premise and in cloud is a must.
Mandatory Skills Description:
Strong expertise in designing and implementing data models, including conceptual, logical, and physical data models, to support efficient data storage and retrieval.
Strong knowledge of Microsoft Azure, including Azure Data Lake Storage, Azure Synapse Analytics, Azure Data Factory, and Azure Databricks, for building scalable and reliable data solutions.
Proficiency in distributed computing frameworks such as Apache Spark for processing and analyzing large-scale datasets.
Extensive experience with building robust and scalable ETL/ELT pipelines to extract, transform, and load data from various sources into data lakes or data warehouses.
Ability to integrate data from disparate sources, including databases, APIs, and external data providers, using appropriate techniques such as API integration or message queuing.
Proficiency in designing and implementing data warehousing solutions, including dimensional modeling, star schemas, and data aggregation techniques.
Experience with Azure Cosmos DB, Azure Analysis Services, and Informix.
Experience with Python and Python-based ETL tools.
Experience with shell scripting in Bash, Unix or windows shell is preferable.
Nice-to-Have Skills:
Experience with Elasticsearch
Familiarity with containerization and orchestration technologies (Docker, Kubernetes).
Troubleshooting and Performance Tuning: Ability to identify and resolve performance bottlenecks in data processing workflows and optimize data pipelines for efficient data ingestion and analysis.
Collaboration and Communication: Strong interpersonal skills to collaborate effectively with stakeholders, data engineers, data scientists, and other cross-functional teams.
Lead Data Engineer/Data Architect
Show more
Show less","Data Migration, Data Modeling, Microsoft Azure, Apache Spark, ETL/ELT Pipelines, Data Warehousing, Dimensional Modeling, Data Aggregation, Python, Shell Scripting, Elasticsearch, Containerization, Docker, Kubernetes, Performance Tuning, Collaboration, Communication","data migration, data modeling, microsoft azure, apache spark, etlelt pipelines, data warehousing, dimensional modeling, data aggregation, python, shell scripting, elasticsearch, containerization, docker, kubernetes, performance tuning, collaboration, communication","apache spark, collaboration, communication, containerization, data aggregation, data migration, datamodeling, datawarehouse, dimensional modeling, docker, elasticsearch, etlelt pipelines, kubernetes, microsoft azure, performance tuning, python, shell scripting"
Data Science Analyst I- Environmental Medicine & Public Health Research,Mount Sinai Health System,United States,https://www.linkedin.com/jobs/view/data-science-analyst-i-environmental-medicine-public-health-research-at-mount-sinai-health-system-3784212433,2023-12-17,Smith Center,United States,Mid senior,Onsite,"Description
Strength Through Diversity
Ground breaking science. Advancing medicine. Healing made personal.
Roles & Responsibilities:
The Data Science Analyst I collaborates with stakeholders from across the organization to develop sophisticated analytics to provide information, insights and BI (Business Intelligence) solutions that contribute to sound strategic planning, decision-making, goal setting, and effective performance measurement. This individual demonstrates sound understanding of the healthcare domain, technical data manipulation and analytic development skills and impact the patient community of the Mount Sinai Health System.
Responsibilities
Analyzes data requests using information technology, enrollment, claims, pharmacy, clinical, contract, medical management, financial, administrative and other corporate data from both modeled and disparate internal and external sources.
Works with departmental staff to identify requirements for reporting and / or business intelligence tools.
Identifies necessary data, data sources and methodologies.
Collects, organizes, integrates, analyzes and interprets data.
Leverages advanced statistical analysis methods to create insightful recommendations and conclusions that may be communicated to the stakeholder.
Identifies and addresses expected and unforeseen data complexities to mitigate their impact on the analytic outcome and associated business decisions. Works to improve data quality where possible within created analytical models. Feeds data quality issues back to IT or identified data stewards to facilitate creation of high quality metrics.
Develops and may present reports, analyses and findings to senior management and others as scheduled or requested.
Research and Development: help with developing predictive forecasting models and tools to improved R&D workflows in the Environmental Medicine & Public Health department
Acts as liaison/analyst for internal stakeholders, understands their needs and translates them into reporting and analytic solutions.
Effectively communicates with stakeholders and customers and ensures all requests are properly triaged, recorded and tracked.
Adheres to corporate standards for performance metrics, data collection, data integrity, query design, and reporting format to ensure high quality, meaningful analytic output.
Helps identify and understand data from internal and external sources for competitive, scenario and performance analyses, and financial modeling to gain member/provider insight into new and existing processes and business opportunities.
Works closely with IT on the ongoing improvement of Mount Sinai’s integrated data warehouse, driven by strategic and business needs, and designed to ensure data and reporting consistency throughout the organization.
Develops and maintains project work plans, including critical tasks, milestones, timelines, interdependencies and contingencies. Tracks and reports progress. Keeps stakeholders apprised of project status and implications for completion.
Provides technical support to data analytics functions as they relate to varied business units, and technical expertise on the selection, development and implementation of various reporting and BI tools tied to business unit reporting requirements. Creates new BI reports and interactive dashboards as required.
Prepares clear, well-organized project-specific documentation, including, at a minimum, analytic methods used, key decision points and caveats, with sufficient detail to support comprehension and replication.
Ensures customers are adequately trained to use self-service BI tools and dashboards.
Assists and may teach others within the organization on how to a) define meaningful process and performance measures, b) develop BI queries, and c) generate and use management reports effectively.
Shares development and process knowledge with other analysts in order to assure redundancy and continuously builds a core of analytical strength within the organization.
Demonstrates proficiency with the principles and methodologies of process improvement. Applies these in the execution of responsibilities in support of a process focused approach.
Other duties as assigned.
Qualifications
BA or BS degree minimum, in a relevant field of study; Masters degree preferred.
2 years minimum is preferred in analytics development expertise, preferably in health care, or for a health provider, health plan or accountable care organization, including either:
Working knowledge of Natural Language Processing algorithms.
Working knowledge of Deep Learning Models
Experience working in predictive forecasting and modeling is highly desirable
Employer Description
Strength Through Diversity
The Mount Sinai Health System believes that diversity, equity, and inclusion are key drivers for excellence. We share a common devotion to delivering exceptional patient care. When you join us, you become a part of Mount Sinai’s unrivaled record of achievement, education, and advancement as we revolutionize medicine together. We invite you to participate actively as a part of the Mount Sinai Health System team by:
Using a lens of equity in all aspects of patient care delivery, education, and research to promote policies and practices to allow opportunities for all to thrive and reach their potential.
Serving as a role model confronting racist, sexist, or other inappropriate actions by speaking up, challenging exclusionary organizational practices, and standing side-by-side in support of colleagues who experience discrimination.
Inspiring and fostering an environment of anti-racist behaviors among and between departments and co-workers.
We work hard to acquire and retain the best people and to create an inclusive, welcoming and nurturing work environment where all feel they are valued, belong and are able to professional advance. We share the belief that all employees, regardless of job title or expertise contribute to the patient experience and quality of patient care.
Explore more about this opportunity and how you can help us write a new chapter in our history!
“About the Mount Sinai Health System:
Mount Sinai Health System is one of the largest academic medical systems in the New York metro area, with more than 43,000 employees working across eight hospitals, more than 400 outpatient practices, more than 300 labs, a school of nursing, and a leading school of medicine and graduate education. Mount Sinai advances health for all people, everywhere, by taking on the most complex health care challenges of our time — discovering and applying new scientific learning and knowledge; developing safer, more effective treatments; educating the next generation of medical leaders and innovators; and supporting local communities by delivering high-quality care to all who need it. Through the integration of its hospitals, labs, and schools, Mount Sinai offers comprehensive health care solutions from birth through geriatrics, leveraging innovative approaches such as artificial intelligence and informatics while keeping patients’ medical and emotional needs at the center of all treatment. The Health System includes approximately 7,400 primary and specialty care physicians; 13 joint-venture outpatient surgery centers throughout the five boroughs of New York City, Westchester, Long Island, and Florida; and more than 30 affiliated community health centers. We are consistently ranked by U.S. News & World Report's Best Hospitals, receiving high ""Honor Roll"" status, and are highly ranked: No. 1 in Geriatrics and top 20 in Cardiology/Heart Surgery, Diabetes/Endocrinology, Gastroenterology/GI Surgery, Neurology/Neurosurgery, Orthopedics, Pulmonology/Lung Surgery, Rehabilitation, and Urology. New York Eye and Ear Infirmary of Mount Sinai is ranked No. 12 in Ophthalmology. U.S. News & World Report’s “Best Children’s Hospitals” ranks Mount Sinai Kravis Children's Hospital among the country’s best in several pediatric specialties. The Icahn School of Medicine at Mount Sinai is ranked No. 14 nationwide in National Institutes of Health funding and in the 99th percentile in research dollars per investigator according to the Association of American Medical Colleges. Newsweek’s “The World’s Best Smart Hospitals” ranks The Mount Sinai Hospital as No. 1 in New York and in the top five globally, and Mount Sinai Morningside in the top 20 globally.
The Mount Sinai Health System is an equal opportunity employer. We comply with applicable Federal civil rights laws and does not discriminate, exclude, or treat people differently on the basis of race, color, national origin, age, religion, disability, sex, sexual orientation, gender identity, or gender expression. We are passionately committed to addressing racism and its effects on our faculty, staff, students, trainees, patients, visitors, and the communities we serve. Our goal is for Mount Sinai to become an anti-racist health care and learning institution that intentionally addresses structural racism.”
EOE Minorities/Women/Disabled/Veterans
Compensation
The Mount Sinai Health System (MSHS) provides a salary range to comply with the New York City Law on Salary Transparency in Job Advertisements. The salary range for the role is $72473 - $108709 Annually. Actual salaries depend on a variety of factors, including experience, education, and hospital need. The salary range or contractual rate listed does not include bonuses/incentive, differential pay or other forms of compensation or benefits.
Show more
Show less","Data Analysis, Predictive Forecasting, Advanced Statistical Analysis, Natural Language Processing, Deep Learning Models, Business Intelligence, Reporting, Data Integration, Information Technology, Healthcare Domain, Health Informatics","data analysis, predictive forecasting, advanced statistical analysis, natural language processing, deep learning models, business intelligence, reporting, data integration, information technology, healthcare domain, health informatics","advanced statistical analysis, business intelligence, data integration, dataanalytics, deep learning models, health informatics, healthcare domain, information technology, natural language processing, predictive forecasting, reporting"
Developer - Database III_US,Dice,United States,https://www.linkedin.com/jobs/view/developer-database-iii-us-at-dice-3773136864,2023-12-17,Smith Center,United States,Mid senior,Onsite,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, Pinnacle Group, is seeking the following. Apply via Dice today!
NO C2C Only W2
Job Description:
The Role:
Quantitative Data Engineer, Research Signals
The Team:
The Research Signals group provides independent research and investment consulting services to the institutional asset management community.
The group researches and develops individual factors and multi-factor models across a range of asset classes by applying a systematic evaluation process to a variety of fundamental, technical, and industry-specific data sources.
The Impact:
The Quantitative Data Engineer develops and supports new and existing quantitative products and datasets.
The Research Signals team's research and datasets are widely used across the investment management industry.
This position will partner with stakeholders in technology, research and operations and the broader product management team to build a data platform, deliver robust production systems and ensure high data integrity of client quantitative products which include alpha factors, risk models and quantitative investing software solutions.
What's in it for candidate:
Build technology workflows to deliver new data products and enhance the Research Signals suite of quantitative factors and models.
Work with client specialized quantitative research team to deliver a variety of proprietary and unique datasets.
Exposure to quantitative research, strategy development, and other innovation initiatives across client Market Intelligence.
Responsibilities:
Develop both on-premises and cloud-based data ingestion processes using SQL,
Python
Engineer data models and infrastructure for a wide variety of market and alternative dataset.
Author tests to validate data quality and measure the stability of the data acquisition processes.
Work directly with Analysts, Product Specialists, and the technology team to understand requirements and provide end-to-end data solutions.
Communicate with data providers to onboard new datasets and troubleshoot technical issues.
Investigate and defuse time-sensitive data incidents.
What Client is Looking For:
Bachelor/Master's degree in Computer Science, Information Systems, or related field.
Strong analytical, data and programming skills (Python/SQL/NoSQL/JavaScript).
3 plus years of experience with large data sets ETL and techniques to architect them for performance, experience using alternative unstructured data is a plus.
1 plus year of experience with cloud computing services, AWS preferred.
Aptitude for designing infrastructure, and data products for Quant/Data Scientists is a plus.
Ability to work effectively in an agile environment with numerous stakeholders on complex research and new development projects.
A genuine interest in investment strategies, equities, and fixed income.
Asset management industry experience is a plus.
Strong verbal and written communication skill, must be a team player.
Note:
Req. SPGJP00001448 aged out.
TECHNICAL EVALUATION WILL BE ADMINISTERED AS PART OF THE INTERVIEWING PROCESS.
Duration = 12 months
Candidates should have a minimum of 3 plus years.
Strong exp. overall data engineering (SQL, data modeling, data ATL.
Position will be 100% remote for the duration of the assignment.
Interview process:
1 to 2 rounds of interviews.
Round one approx. 1 hour long, coding test administered; potential 2nd interview is panel interview with dept. leadership.
Assignment has potential to extend, but no conversion planned at this time.
Laptop to be shipped to worker - Indicate candidate's time zone in comments or on resume.
Biz. Hours based on CT.
Candidate expected to start immediately upon bkgd. clearance.
DISCLOSE ANY UPCOMING VACATION, PENDING OFFERS, SPECIAL NEEDS, ETC.
AT TIME OF SUBMISSION In candidate submission in comments section.
*No IC's or visa holders (even if they're your own W2 employee).
Pay Range: $60 - $63/hour on W2
The specific compensation for this position will be determined by a number of factors, including the scope, complexity and location of the role as well as the cost of labor in the market; the skills, education, training, credentials and experience of the candidate; and other conditions of employment. Our full-time consultants have access to benefits including medical, dental, vision as well as 401K contributions as well as any other PTO, sick leave, and other benefits mandated by appliable state or localities where you reside or work. Developer - Database III_US
Show more
Show less","Python, SQL, NoSQL, JavaScript, AWS, ETL, Data modeling, Data ingestion, Data quality, Data integrity, Quantitative research, Strategy development, Investment strategies, Equities, Fixed income, Asset management, Agile environment, Cloud computing, Coding test","python, sql, nosql, javascript, aws, etl, data modeling, data ingestion, data quality, data integrity, quantitative research, strategy development, investment strategies, equities, fixed income, asset management, agile environment, cloud computing, coding test","agile environment, asset management, aws, cloud computing, coding test, data ingestion, data integrity, data quality, datamodeling, equities, etl, fixed income, investment strategies, javascript, nosql, python, quantitative research, sql, strategy development"
Principal Data Engineer,Rapid7,United States,https://www.linkedin.com/jobs/view/principal-data-engineer-at-rapid7-3739106224,2023-12-17,Smith Center,United States,Mid senior,Hybrid,"Rapid7 seeks an experienced, highly driven, and dynamic Principal Data Engineer to take our data engineering function to the next level. Come and join our efforts in unlocking the value of data through industry-leading innovation, cutting edge modern tooling, democratization at scale and building exceptional and trusted data products for the company!
About The Team
As we spearhead a cultural shift to a data-driven business, Data Engineering serves as the Hub for all teams at Rapid7 from ML Ops, to Sales and Operations to Platform and Engineering. Our team is a highly skilled yet egoless group of data magicians (and humorists) with a penchant for innovation and a knack for problem solving.
About The Role
As a Principal Data Engineer, you will be responsible for the technical advancement of our data engineering function tackling our most complex challenges, mentoring our highly motivated core of data engineers and senior data engineers, and building strong partnerships with data consumers across the organization. You’ll have the liberty to drive business value through thought leadership and innovation as we continue to work towards the high standard we’ve set for ourselves - a best-in-class data platform.
In This Role, You Will
Scope and stage larger scale initiatives into well-defined milestones to avoid a monolithic deliverables
Provide thought leadership and contribute to the vision of our data engineering function
Lead team processes such as on-call rotations, bug triage, technical direction, standards, and execution
Own delivery architecture/execution of major component(s) from conception to release
Looked up to for technical mentorship within the data engineering team. Make others better through code reviews, focus on documentation, and technical guidance
Act as a resource sought for technical advice and weigh in on technical decisions that impact other teams as well
Understand the tradeoffs between technical and business needs, interact and negotiate with key stakeholders, and deliver solutions that take all of these needs into account
Regularly take complex designs / codebases and simplify them without being asked
Regularly contribute improvements to team’s existing SDLC (ie: CICD) or other methods, programs, etc
The Skills You’ll Bring Include
8+ years of hands on data engineering experience and at least 4 years in a senior-level data engineering role
8+ years of experience in at least one programming language such as Python, Java, Scala is required (Python is our most commonly used language); Advanced SQL expertise is required
Experience working in a modern lakehouse is required (Snowflake is preferred); Modern warehousing best practices should be second nature
Cloud experience is required (AWS is strongly preferred); Terraform is highly preferred
Knowledge and ideally hands on experience working with container services is required (ECS, Kubernetes, etc)
Experience working in a mature SDLC environment (ie: CICD) is required
Modern tech stack experience is a plus (dbt, Fivetran, Snowflake. Airflow)
Experience as a leader within a data engineering team and ability to mentor teammates
Strong work ethic, resiliency, persistence, and urgency; Data Engineering holds itself to a high standard so you’ll need to keep up!
Sharp business and interpersonal skills; ability to influence at senior levels across business units to drive change and achieve common goals
BS or MS in Computer Science, Analytics, Statistics, Informatics, Information Systems or
Another Quantitative Field Or Equivalent Experience
We know that the best ideas and solutions come from multi-dimensional teams. That’s because these teams reflect a variety of backgrounds and professional experiences. If you are excited about this role and feel your experience can make an impact, please don’t be shy - apply today.
About Rapid7
At Rapid7, we are on a mission to create a secure digital world for our customers, our industry, and our communities. We do this by embracing tenacity, passion, and collaboration to challenge what’s possible and drive extraordinary impact.
Here, we’re building a dynamic workplace where everyone can have the career experience of a lifetime. We challenge ourselves to grow to our full potential. We learn from our missteps and celebrate our victories. We come to work every day to push boundaries in cybersecurity and keep our 10,000 global customers ahead of whatever’s next.
Join us and bring your unique experiences and perspectives to tackle some of the world’s biggest security challenges.
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, age, national origin, disability, protected veteran status or any other status protected by applicable national, federal, state or local law.
Show more
Show less","Data Engineering, Python, Java, Scala, SQL, Snowflake, AWS, Terraform, Kubernetes, SDLC, dbt, Fivetran, Airflow","data engineering, python, java, scala, sql, snowflake, aws, terraform, kubernetes, sdlc, dbt, fivetran, airflow","airflow, aws, data engineering, dbt, fivetran, java, kubernetes, python, scala, sdlc, snowflake, sql, terraform"
Sr Data Engineer - Data Warehousing Technologies,Meijer,"Grand Rapids, MI",https://www.linkedin.com/jobs/view/sr-data-engineer-data-warehousing-technologies-at-meijer-3746295268,2023-12-17,Grand Rapids,United States,Mid senior,Onsite,"As a family company, we serve people and communities. When you work at Meijer, you’re provided with career and community opportunities centered around leadership, personal growth and development. Consider joining our family – take care of your career and your community!
Meijer Rewards
Weekly pay
Scheduling flexibility
Paid parental leave
Paid education assistance
Care.com/back-up care assistance
Team member discount
Development programs for advancement and career growth
Please review the job profile below and apply today!
The Meijer Enterprise Data and Analytics train is seeking a Senior Data Engineer with experience designing architecture aspiring to be a Solution Architect. The Senior Data Engineer designs, develops, tests, maintains and enhances enterprise data and analytic solutions as a member of an Agile or Operations development team working closely with all areas of the Meijer business to implement best-in-class data and analytics solutions.
Remote and Hybrid Work Schedules Available for those living within our footprint. Must live within the Meijer 6-state footprint to be eligible for the Remote or Hybrid work schedule [Michigan, Ohio, Indiana, Illinois, Wisconsin, Kentucky] Or, be willing to relocate… (Comprehensive Relocation assistance is available)
What You'll Be Doing (Responsibilities)
Designs, codes, tests, and implements data movement, dashboarding and analytical assets; develops system documentation according to SAFe Agile principles and industry standards.
Evaluates architectural options and defines overall architecture of enterprise Data Lake and Data Warehouse.
Partnering with all areas of the business to gather requirements for Data and Analytics and designing solutions.
Defines functional and non-functional requirements including performance monitoring, alerting and code management and ensuring alignment with technology best practices and SLAs.
Determines major elements and subsystems and defines interfaces as well as architectural runway.
Providing subject matter expertise and technical consulting support on either vendor or internal applications and interfaces including Azure - Data Factory, Log Analytics, Databricks, Synapse, Power BI, ADLS Gen2, Polybase and Machine Learning/AI.
Mentoring and coaching other members of the agile and\or Run team.
Driving engagement with ITS Security and Infrastructure teams to ensure secure development and deployment of solutions.
Interfaces with the Product Manager and IT partners at the Program level and within other Release Trains to define and estimate features for agile teams.
Conducting industry research, facilitates new product and vendor evaluations, and assists in vendor selection.
What You Bring With You (Qualifications)
Bachelor’s degree in Computer Science, Computer Information Systems, Business Information Systems, Engineering or related discipline or equivalent work experience and technical training is required.
8+ years industry experience (business application design, development, implementation, and/or solution architecture)
Understanding of architecture practices and execution for large projects / programs.
Excellent written and oral communications skills.
Previous experience in Power BI, Data Modeling, Data Classification and zones, data movement, Data architecture and reporting
Experience building and designing data and analytics on enterprise solutions such as Azure - Data Factory, Log Analytics, Databricks, Synapse, Power BI, ADLS Gen2, Polybase and Machine Learning/AI.
Databricks experience is required.
In-depth understanding of computer, storage, network components including backup, monitoring and DR environment requirements
Experience designing data pipelines, ingestion, storage, prep-train, model and serve using above technologies, Automating Azure Workloads, Data quality, Governance/standards, Security and legal compliance in the Azure architecture
Preferred knowledge and experience on Python and API architecture in Azure
Any SAFe certification or training.
Experience with multiple, diverse technical configurations, technologies, and processing environments.
Exceptional interpersonal skills, including teamwork, facilitation, and negotiation
Show more
Show less","Data Architecture, Data Modeling, Data Analytics, Data Engineering, Data Warehouse, Data Factory, Databricks, Synapse, Power BI, Azure Log Analytics, ADLS Gen2, Polybase, Machine Learning, Artificial Intelligence, Python, API Architecture, SAFe","data architecture, data modeling, data analytics, data engineering, data warehouse, data factory, databricks, synapse, power bi, azure log analytics, adls gen2, polybase, machine learning, artificial intelligence, python, api architecture, safe","adls gen2, api architecture, artificial intelligence, azure log analytics, data architecture, data engineering, data factory, dataanalytics, databricks, datamodeling, datawarehouse, machine learning, polybase, powerbi, python, safe, synapse"
Staff Data Engineer,Recruiting from Scratch,"Kentwood, MI",https://www.linkedin.com/jobs/view/staff-data-engineer-at-recruiting-from-scratch-3744393698,2023-12-17,Grand Rapids,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our client is a dating app.
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, deliver analysis to further improve engagement in existing features, and empower our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Solve technical problems of the highest scope and complexity
Exert significant influence on the company’s analytical long-range goals and data architecture
Define and extend our internal standards for style, maintenance, and best practices for a high-scale data platform
Provide mentorship for all on your team to help them grow in their technical responsibilities
Propose ideas to improve the scale, performance, and capabilities of the Data Platform
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We'll Love About You
7+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field
7+ years experience using Python
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention
Location
: This role will be located in office twice a week minimum in Palo Alto, San Francisco or Chicago.
Salary Range: $180,000 - 250,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Business Intelligence, Data Science, Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, SQL, Kafka, Storm, SparkStreaming, Dimensional Data Modeling, Schema Design, Data Warehouses, ETL, Data Classification, Data Retention, TDD, Pair Programming, Continuous Integration, Automated Testing, Deployment","data engineering, business intelligence, data science, python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, kafka, storm, sparkstreaming, dimensional data modeling, schema design, data warehouses, etl, data classification, data retention, tdd, pair programming, continuous integration, automated testing, deployment","airflow, automated testing, business intelligence, continuous integration, data classification, data engineering, data retention, data science, data warehouses, deployment, dimensional data modeling, docker, etl, helm, kafka, kubernetes, pair programming, python, schema design, snowflake, spark, sparkstreaming, sql, storm, tdd"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Kentwood, MI",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3748831277,2023-12-17,Grand Rapids,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our next evolution is underway as a newly public company looking to expand and continue to build meaningful experiences for our users. From social issues to original content, we’re blazing innovative paths with impact for our community, all while leveraging the latest tech stacks and striving for engineering excellence. At the heart of our work in this new chapter is a shared set of core values: openness and exploration, a bias for action, and strong support of the LGBTQ community.
With a track record of strong financial performance and plans for continued headcount growth, we’re looking to build a team of talented, passionate, and open-minded people who believe in our mission, align with our values, and are excited to work at the intersection of innovative technology and social impact. Come be a part of this exciting journey with us.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location: Palo Alto, San Francisco or Chicago
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Data Science, Business Intelligence, Python, Spark, Snowflake, Kafka, Airflow, Kubernetes, Docker, Helm, SQL, TDD, Pair Programming, Continuous Integration, Automated Testing, Deployment, Data Warehouses, ETL, Data Classification","data engineering, data science, business intelligence, python, spark, snowflake, kafka, airflow, kubernetes, docker, helm, sql, tdd, pair programming, continuous integration, automated testing, deployment, data warehouses, etl, data classification","airflow, automated testing, business intelligence, continuous integration, data classification, data engineering, data science, data warehouses, deployment, docker, etl, helm, kafka, kubernetes, pair programming, python, snowflake, spark, sql, tdd"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Kentwood, MI",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3744392633,2023-12-17,Grand Rapids,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
They are a profitable dating application.
Location: This is a hybrid role based in their Chicago office and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location
: This role is twice a week in a hybrid role minimum.
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, SQL, TDD, Pair Programming, Continuous Integration, Automated Testing, ETL, Data Warehouses, Kafka, Storm, SparkStreaming, Dimensional Data Modeling, Data Classification, Data Retention","python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, tdd, pair programming, continuous integration, automated testing, etl, data warehouses, kafka, storm, sparkstreaming, dimensional data modeling, data classification, data retention","airflow, automated testing, continuous integration, data classification, data retention, data warehouses, dimensional data modeling, docker, etl, helm, kafka, kubernetes, pair programming, python, snowflake, spark, sparkstreaming, sql, storm, tdd"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Kentwood, MI",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3759707764,2023-12-17,Grand Rapids,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto,
San
Francisco or Chicago
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Machine Learning, Data Mining, Data Cleaning, Data Normalization, Data Modeling, Statistical Analysis, Data Visualization, Pandas, R, Airflow, KubeFlow, Natural Language Processing, Large Language Models, Python, Java, Bash, SQL, Git, Snowflake, Kubernetes, Docker, Helm, Spark, PySpark, AWS, GCP, Azure, DynamoDB, ETL, Kafka, Storm, SparkStreaming, Applied Machine Learning, Data Classification, Data Retention, Data Management Tools","data engineering, machine learning, data mining, data cleaning, data normalization, data modeling, statistical analysis, data visualization, pandas, r, airflow, kubeflow, natural language processing, large language models, python, java, bash, sql, git, snowflake, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, dynamodb, etl, kafka, storm, sparkstreaming, applied machine learning, data classification, data retention, data management tools","airflow, applied machine learning, aws, azure, bash, data classification, data cleaning, data engineering, data management tools, data mining, data normalization, data retention, datamodeling, docker, dynamodb, etl, gcp, git, helm, java, kafka, kubeflow, kubernetes, large language models, machine learning, natural language processing, pandas, python, r, snowflake, spark, sparkstreaming, sql, statistical analysis, storm, visualization"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Kentwood, MI",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3773091277,2023-12-17,Grand Rapids,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto
or
San
Francisco
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Data Mining, Data Modeling, Pandas, R, Airflow, KubeFlow, NLP, Python, Java, Bash, SQL, Git, Snowflake, Kubernetes, Docker, Helm, Spark, PySpark, AWS, GCP, Azure, DynamoDB, ETL, Data Pipelines, Kafka, Storm, SparkStreaming, Machine Learning, Data Compliance, Data Classification, Data Retention","data engineering, data mining, data modeling, pandas, r, airflow, kubeflow, nlp, python, java, bash, sql, git, snowflake, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, dynamodb, etl, data pipelines, kafka, storm, sparkstreaming, machine learning, data compliance, data classification, data retention","airflow, aws, azure, bash, data classification, data compliance, data engineering, data mining, data retention, datamodeling, datapipeline, docker, dynamodb, etl, gcp, git, helm, java, kafka, kubeflow, kubernetes, machine learning, nlp, pandas, python, r, snowflake, spark, sparkstreaming, sql, storm"
Data Analysis,Accroid Inc,"Warrendale, PA",https://www.linkedin.com/jobs/view/data-analysis-at-accroid-inc-3774783306,2023-12-17,Brighton,United States,Mid senior,Onsite,"Hybrid
Sr. Data Analysis
PA
Extract data from the Finders Oracle database
Prepare data ingestion templates to upload into Boomi Master Data Hub (MDH)
Validate data ingestion results in target databases (MDH/SQL Server)
Experience using Oracle SQL Developer and Microsoft SQL Server Management Studio - 5+ years""
Show more
Show less","Data Analysis, Oracle SQL Developer, Microsoft SQL Server Management Studio, Boomi Master Data Hub (MDH), SQL Server, Oracle","data analysis, oracle sql developer, microsoft sql server management studio, boomi master data hub mdh, sql server, oracle","boomi master data hub mdh, dataanalytics, microsoft sql server management studio, oracle, oracle sql developer, sql server"
Senior Data Engineer,Dice,"Toronto, OH",https://www.linkedin.com/jobs/view/senior-data-engineer-at-dice-3783000664,2023-12-17,Brighton,United States,Mid senior,Onsite,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, Luxoft USA Inc, is seeking the following. Apply via Dice today!
Project Description:
Applying to the position, you may join one of the most respected and fastest growing institutional investors in the world. Our client is a professional investment management organization that globally invests the funds to ensure long-term sustainability. The company invests in all major asset classes, including public equity, private equity, real estate, infrastructure and fixed-income instruments, and currently has offices in Toronto, London, Hong Kong, New York, São Paulo, Luxembourg and Mumbai.
Client attracts and selects high-calibre individuals from top-tier institutions around the globe. Join our team and look forward to:
Diverse and inspiring colleagues and approachable leaders
Stimulating work in a fast-paced, intellectually challenging environment
Accelerated exposure and responsibility
Global career development opportunities
Being motivated every day by important social purpose and unshakable principles
A deeply rooted culture of Integrity, Partnership and High Performance
If you share a passion for performance, value a collegial and collaborative culture, and approach everything with the highest integrity, here's an opportunity.
Responsibilities:
Manage timelines/deliverables within the team towards the successful delivery of projects.
Design software solutions by interacting with portfolio managers, traders, operations staff and peers to understand requirements.
Develop solutions that are in line with client's technology biases, deliver efficiency and scalability, and enable new trading activities.
Provide knowledge transfer to team members and support staff through application demos, walkthroughs, and documentation.
Mandatory Skills Description:
Airflow
PySpark
AWS (EMR, S3, Hive/Glue data catalog)
Python
Microservice
spark optimization (code and cluster level)
Knowing Terraform and AWS code build
Hudi (big plus)
Languages:
English: B2 Upper Intermediate
Senior Data Engineer
Show more
Show less","Airflow, PySpark, AWS, EMR, S3, Hive or Glue data catalog, Python, Microservice, Spark optimization, Terraform, AWS code build, Hudi, English (B2 Upper Intermediate)","airflow, pyspark, aws, emr, s3, hive or glue data catalog, python, microservice, spark optimization, terraform, aws code build, hudi, english b2 upper intermediate","airflow, aws, aws code build, emr, english b2 upper intermediate, hive or glue data catalog, hudi, microservice, python, s3, spark, spark optimization, terraform"
Lead Big Data Developer,Biogensys,"Pittsburgh, PA",https://www.linkedin.com/jobs/view/lead-big-data-developer-at-biogensys-3781153284,2023-12-17,Brighton,United States,Mid senior,Hybrid,"We are hiring a
Lead Big Data Developer
for one of our clients in
Pittsburgh, PA
.
Job Description
10+ years' experience
Linux experience/Red Hat Linux
Trouble shooting skills
Experience with the following tools: Big Data Hadoop Ecosystem (Sqoop, Spark, Hive, HDFS, Impala, Kafka), Python, JupyterHub, RStudio, SQL, Oracle.
Candidate will be able to apply modern principles, methodologies and tools to advance business initiatives and capabilities.
Position requires candidate to have excellent leadership qualities, possess strong communication and troubleshooting skills and work with diverse teams.
Additional Information
Location: Hybrid Pittsburgh or Birmingham or Dallas or Cleveland
Duration: 6 months Contract
About Us
We are specialized in recruiting and deliver the best professional talent of industry and we are committed to deliver best experience for our clients and job seekers. With over two decades of experience in the recruitment industry, we proudly help you to find the next job that matches your professional skills. Our team understands your needs or requirement before starting the recruitment, that enables to find the high quality of talent with high success rate of talent delivery, keeps us continue to be the best in the industry. By responding to this job posting, you are consenting to receive text/SMS messages from us. Thank you.
Show more
Show less","Linux, Red Hat Linux, Big Data Hadoop Ecosystem, Sqoop, Spark, Hive, HDFS, Impala, Kafka, Python, JupyterHub, RStudio, SQL, Oracle, Leadership, Communication, Troubleshooting","linux, red hat linux, big data hadoop ecosystem, sqoop, spark, hive, hdfs, impala, kafka, python, jupyterhub, rstudio, sql, oracle, leadership, communication, troubleshooting","big data hadoop ecosystem, communication, hdfs, hive, impala, jupyterhub, kafka, leadership, linux, oracle, python, red hat linux, rstudio, spark, sql, sqoop, troubleshooting"
"Senior Software Engineer: Data Management, Linux, Networks",Lockheed Martin,"Andover, MA",https://www.linkedin.com/jobs/view/senior-software-engineer-data-management-linux-networks-at-lockheed-martin-3779429987,2023-12-17,Amesbury,United States,Mid senior,Onsite,"At the Advanced Energy Storage division of Lockheed Martin, you will have an opportunity to contribute to the development of energy storage for the future. You will work alongside a team of world-class engineers and scientists, with the backing of a global technology leader known for its history of innovation. You will develop technical solutions supporting the operation of a revolutionary grid-connected flow battery system. As a Software Engineer, you will use your knowledge of software development in Java and Python to contribute to the design of Human-Machine Interfaces (HMIs) and data management tools. You will participate in the entire software development lifecycle, from conceptual design to detailed implementation, testing, code release and field support. You will be relied upon for providing engineering support, testing, and debugging of software functional components and subsystems, and documentation of results. You will see your designs evolve from the conceptual stage to detailed implementation to high-volume production.
Show more
Show less","Software Development, Java, Python, HMIs, Data Management Tools, Engineering Support, Software Functional Components, Subsystems, Testing, Debugging, Documentation","software development, java, python, hmis, data management tools, engineering support, software functional components, subsystems, testing, debugging, documentation","data management tools, debugging, documentation, engineering support, hmis, java, python, software development, software functional components, subsystems, testing"
Staff Data Engineer,Recruiting from Scratch,"Andover, MA",https://www.linkedin.com/jobs/view/staff-data-engineer-at-recruiting-from-scratch-3744392927,2023-12-17,Amesbury,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our client is a dating app.
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, deliver analysis to further improve engagement in existing features, and empower our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Solve technical problems of the highest scope and complexity
Exert significant influence on the company’s analytical long-range goals and data architecture
Define and extend our internal standards for style, maintenance, and best practices for a high-scale data platform
Provide mentorship for all on your team to help them grow in their technical responsibilities
Propose ideas to improve the scale, performance, and capabilities of the Data Platform
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We'll Love About You
7+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field
7+ years experience using Python
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention
Location
: This role will be located in office twice a week minimum in Palo Alto, San Francisco or Chicago.
Salary Range: $180,000 - 250,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Python, SQL, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, Kafka, Storm, SparkStreaming, ETL, TDD, Pair Programming, Continuous Integration, Automated Testing, Deployment, Data Warehouses, Data Governance, Data Classification, Data Retention, Data Management Tools, Data Science, Business Intelligence, Data Engineering","python, sql, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, kafka, storm, sparkstreaming, etl, tdd, pair programming, continuous integration, automated testing, deployment, data warehouses, data governance, data classification, data retention, data management tools, data science, business intelligence, data engineering","airflow, automated testing, business intelligence, continuous integration, data classification, data engineering, data governance, data management tools, data retention, data science, data warehouses, deployment, docker, etl, helm, kafka, kubernetes, pair programming, python, snowflake, spark, sparkstreaming, sql, storm, tdd"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Andover, MA",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3748832239,2023-12-17,Amesbury,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our next evolution is underway as a newly public company looking to expand and continue to build meaningful experiences for our users. From social issues to original content, we’re blazing innovative paths with impact for our community, all while leveraging the latest tech stacks and striving for engineering excellence. At the heart of our work in this new chapter is a shared set of core values: openness and exploration, a bias for action, and strong support of the LGBTQ community.
With a track record of strong financial performance and plans for continued headcount growth, we’re looking to build a team of talented, passionate, and open-minded people who believe in our mission, align with our values, and are excited to work at the intersection of innovative technology and social impact. Come be a part of this exciting journey with us.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location: Palo Alto, San Francisco or Chicago
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Realtime Streaming Technologies, TDD, Automation, Continuous Delivery, Data Science, Machine Learning, Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, SQL, Agile Engineering Practices, StreamProcessing Systems, Kafka, Storm, SparkStreaming, Dimensional Data Modeling, Schema Design, Data Warehouses, ETL Pipelines, Data Compliance, Data Classification, Data Retention","data engineering, realtime streaming technologies, tdd, automation, continuous delivery, data science, machine learning, python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, agile engineering practices, streamprocessing systems, kafka, storm, sparkstreaming, dimensional data modeling, schema design, data warehouses, etl pipelines, data compliance, data classification, data retention","agile engineering practices, airflow, automation, continuous delivery, data classification, data compliance, data engineering, data retention, data science, data warehouses, dimensional data modeling, docker, etl pipelines, helm, kafka, kubernetes, machine learning, python, realtime streaming technologies, schema design, snowflake, spark, sparkstreaming, sql, storm, streamprocessing systems, tdd"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Andover, MA",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3744396614,2023-12-17,Amesbury,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
They are a profitable dating application.
Location: This is a hybrid role based in their Chicago office and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location
: This role is twice a week in a hybrid role minimum.
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Python, SQL, TDD, Continuous Integration, Data Engineering, Data Science, Big Data, Snowflake, Apache Airflow, Kubernetes, Docker, Helm, Apache Spark, PySpark, Kafka, Storm, Spark Streaming, Distributed Databases, Agile Engineering, Pair Programming, Automated Testing, ETL, Dimensional Data Modeling, Legal Compliance","python, sql, tdd, continuous integration, data engineering, data science, big data, snowflake, apache airflow, kubernetes, docker, helm, apache spark, pyspark, kafka, storm, spark streaming, distributed databases, agile engineering, pair programming, automated testing, etl, dimensional data modeling, legal compliance","agile engineering, apache airflow, apache spark, automated testing, big data, continuous integration, data engineering, data science, dimensional data modeling, distributed databases, docker, etl, helm, kafka, kubernetes, legal compliance, pair programming, python, snowflake, spark, spark streaming, sql, storm, tdd"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Andover, MA",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3759707773,2023-12-17,Amesbury,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto,
San
Francisco or Chicago
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Machine Learning (ML), Data Science, Data Mining, Data Cleaning, Data Normalization, Data Modeling, Python, Java, Bash, SQL, Git, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, AWS, GCP, Azure, DynamoDB, ETL, Conversational AI, Recommender Systems, Distributed Systems, Microservices, Kafka, Storm, SparkStreaming, Applied Machine Learning, Data Management Tools, Data Classification, Data Retention","data engineering, machine learning ml, data science, data mining, data cleaning, data normalization, data modeling, python, java, bash, sql, git, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, dynamodb, etl, conversational ai, recommender systems, distributed systems, microservices, kafka, storm, sparkstreaming, applied machine learning, data management tools, data classification, data retention","airflow, applied machine learning, aws, azure, bash, conversational ai, data classification, data cleaning, data engineering, data management tools, data mining, data normalization, data retention, data science, datamodeling, distributed systems, docker, dynamodb, etl, gcp, git, helm, java, kafka, kubernetes, machine learning ml, microservices, python, recommender systems, snowflake, spark, sparkstreaming, sql, storm"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Andover, MA",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3773087759,2023-12-17,Amesbury,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto
or
San
Francisco
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Machine Learning, Data Pipelines, Big Data, Python, Java, Bash, SQL, Git, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, NoSQL, DynamoDB, Kafka, Storm, SparkStreaming, Data Preprocessing, Data Mining, Data Normalization, Data Modeling, Data Visualization, Data Governance, Data Security, Data Compliance, ETL Pipelines, Conversational AI, Recommender Systems, Microservices, Distributed Systems, Applied Machine Learning","data engineering, machine learning, data pipelines, big data, python, java, bash, sql, git, snowflake, airflow, kubernetes, docker, helm, spark, nosql, dynamodb, kafka, storm, sparkstreaming, data preprocessing, data mining, data normalization, data modeling, data visualization, data governance, data security, data compliance, etl pipelines, conversational ai, recommender systems, microservices, distributed systems, applied machine learning","airflow, applied machine learning, bash, big data, conversational ai, data compliance, data engineering, data governance, data mining, data normalization, data preprocessing, data security, datamodeling, datapipeline, distributed systems, docker, dynamodb, etl pipelines, git, helm, java, kafka, kubernetes, machine learning, microservices, nosql, python, recommender systems, snowflake, spark, sparkstreaming, sql, storm, visualization"
Data Analyst III,WinMax,"Cupertino, CA",https://www.linkedin.com/jobs/view/data-analyst-iii-at-winmax-3766802812,2023-12-17,Redwood City,United States,Associate,Onsite,"Title:Data Analyst III, Req# 28195128
Location:Cupertino,CA(Half onsite half remote)
Contract:5+ Month
Job Description
The ideal candidate is one who has been a top performer in his or her career with a consistent track record of accomplishment. The candidate should feel comfortable dealing with complex and ambiguous situations.
As The Content Strategy Analyst, Your Responsibilities Will Include
Blend quantitative insights with qualitative research of games to draw actionable recommendations internally and externally
Analyze, collect, organize, and disseminate significant amount of information with attention to detail and accuracy
Key Qualifications
5+ years of relevant experience across product management, business intelligence, strategy, and management consulting, ideally prior work experience in the game industry
Deep knowledge of and passion for mobile games, and a hunger to help define the future of gaming
Self-motivated, driven individual who is comfortable working a fast-paced and dynamic environment
Proven ability to synthesize insights from data and industry sources and package those insights into persuasive, impactful proposals
Comfortable in managing KPIs for live and long term engagement for mobile games
Extraordinary quantitative skills with high proficiency in Excel (Tableau & SQL experience is a plus)
PyRate: $60-63/hr, W2
Show more
Show less","Product Management, Business Intelligence, Strategy Consulting, Mobile Games, Data Analysis, Excel, Tableau, SQL, KPIs, Engagement, Quantitative Analysis","product management, business intelligence, strategy consulting, mobile games, data analysis, excel, tableau, sql, kpis, engagement, quantitative analysis","business intelligence, dataanalytics, engagement, excel, kpis, mobile games, product management, quantitative analysis, sql, strategy consulting, tableau"
Data Analyst III,WinMax,"Cupertino, CA",https://www.linkedin.com/jobs/view/data-analyst-iii-at-winmax-3766064332,2023-12-17,Redwood City,United States,Associate,Onsite,"Title:Data Analyst III, Req# 28196581
Location:Cupertino,CA(Hybrid)
Contract:12+ Month
Job Description
Position: Pricing Data Analyst
Essential Responsibilities
Review and validate monthly service quotes from OEMs in alignment with Financial Calendar and client costing schedule, and ensure accuracy in BOM structure, materials cost, MVA, and Attrition
Submit monthly service pricing to management approval
Publish approved pricing to SAP and WW buyers
Interact with WW buyers for price troubleshooting
Support NPI BOM and service cost placeholder set-up
Support ad-hoc OEM cost claim validation
System admin for iCost tool
Maintenance of KPI database and KPI reporting
Skills Required
Strong data analytical capabilities with superior attention to details
Multitasking with time-sensitive assignments
Excellent written and oral communication skills
Demonstrated experience with cross-functional collaboration
High level of personal and professional integrity
Strong technical skills (Excel)
SAP skills a plus
Education & Experiences
BS or BA degree from an accredited 4-year college or university. MBA, MS, or professional credential is preferred.
Experience in Supply Chain Management a plus.
Previous experience in managing vendors is a plus.
A second language Chinese is a preferred.
PayRate: $63-69/hr,W2
Show more
Show less","Data Analytical Skills, Attention to Detail, Multitasking, Time Management, Written Communication, Oral Communication, CrossFunctional Collaboration, Personal and Professional Integrity, Excel, SAP, iCost Tool, KPI Database and Reporting, Supply Chain Management, Vendor Management, Chinese Language","data analytical skills, attention to detail, multitasking, time management, written communication, oral communication, crossfunctional collaboration, personal and professional integrity, excel, sap, icost tool, kpi database and reporting, supply chain management, vendor management, chinese language","attention to detail, chinese language, crossfunctional collaboration, data analytical skills, excel, icost tool, kpi database and reporting, multitasking, oral communication, personal and professional integrity, sap, supply chain management, time management, vendor management, written communication"
Data Analysis Specialist,Welocalize,"Sunnyvale, CA",https://www.linkedin.com/jobs/view/data-analysis-specialist-at-welocalize-3780112769,2023-12-17,Redwood City,United States,Associate,Onsite,"Overview
Welocalize is seeking a Data Analyst to contribute to an AI technology project. The ideal candidate will have a foundational understanding of machine learning, data annotation, data handling, quality assurance, and natural language processing. They will play a pivotal role in updating our machine-learning models and ensuring their efficacy.
Project Details
Job Title: Data Analysis Specialist
Location: Onsite in one of the following locations:
Redmond, WA
Burlingame, CA
Sunnyvale, CA
Hours: 40 hours weekly, 1 year commitment
Start date: ASAP
Employment Type: W2 Full-Time Employee
Pay rate: $40 hrly, negotiable based on experience
Preferred Qualifications
Bachelor's degree in Computer Science, Data Science, Linguistics or Computational Linguistics or a related field.
Experience with machine learning models, data annotation, and quality assurance.
Strong analytical skills with the ability to identify patterns and anomalies.
Familiarity with command-line tools and interfaces.
Ability to work in a fast-paced, collaborative environment.
Excellent communication skills.
Additional Information:
This role primarily focuses on English US data sets; however, familiarity with translation or multi-lingual data sets can be a plus for future projects.
Key Responsibilities
Machine Learning Model Updates:
Update training and test model databases with new or amended synthetic textual and image data.
Modify and refine machine learning data creation, annotation, and rating guidelines.
Model Training and Evaluation:
Initiate model training processes using internal tools and command-line interfaces.
Evaluate the performance of trained models to gauge their efficacy and readiness for deployment.
Data Management and Annotation:
Design and develop test and training datasets as per the criteria provided by the project manager and other full-time employees.
Handle data efficiently, ensuring its integrity throughout the workflow.
Engage in data relevance tasks, ensuring data sets are aligned with project goals.
Annotate data accurately, ensuring it adheres to set guidelines.
Quality Assurance and Analysis:
Conduct manual quality analysis of model results.
Recognize error patterns and report anomalies for further investigation.
Deliver detailed reports on findings, including aspects such as utterance quality, LLM evaluation, ASR bug tracking, and customer pain points to be reviewed by the User Experience Research team.
Implement basic quality control measures and ensure the reliability of processed data.
Utilize intermediate data analysis techniques to extract insights and inform decision-making.
Arbitrate discrepancies effectively, ensuring consistent data quality.
Linguistic and NLP Tasks:
Apply basic knowledge of natural language processing and linguistics to data processing tasks.
Ensure linguistic accuracy in all processed and annotated data.
Requirements
Fluency in English, both written and spoken
Must be located in the United States and be 18+ years old
Proficient in web navigation and able to work efficiently in a fast-paced environment
Ability to follow instructions in English and comply with the project conventions and rules expected by the client
Must sign a Non-Disclosure Agreement to protect client confidentiality
Must successfully complete a quality test designed by our client before starting work
Benefits
Paid Sick Time
Employee Assistance Program
Following eligibility requirements: Medical Insurance
Dental Insurance
Vision Insurance
HSA
Voluntary Life Insurance
Accident, Critical Illness,Hospital Indemnity Insurance
401(k) Retirement Plan
Please note that in order to verify work authorization as is required by Federal law (I-9 process), all new employees must complete a live video verification with their selected IDs and provide photos of these selected IDs within their first 3 days of employment.
To know more details (Click here)
In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification form upon hire. In addition, we employ anti-fraud checks to ensure all candidates meet the requirements of the program.
As a trusted global transformation partner, Welocalize accelerates the global business journey by enabling brands and companies to reach, engage, and grow international audiences. Welocalize delivers multilingual content transformation services in translation, localization, and adaptation for over 250 languages with a growing network of over 400,000 in-country linguistic resources. Driving innovation in language services, Welocalize delivers high-quality training data transformation solutions for NLP-enabled machine learning by blending technology and human intelligence to collect, annotate, and evaluate all content types. Our team works across locations in North America, Europe, and Asia serving our global clients in the markets that matter to them. www.welocalize.com
To perform this job successfully, an individual must be able to perform each essential duty satisfactorily. The requirements listed below are representative of the knowledge, skill, and/or ability required. Reasonable accommodations may be made to enable individuals with disabilities to perform essential functions.
Show more
Show less","Machine learning, Data annotation, Data handling, Quality assurance, Natural language processing (NLP), Commandline tools and interfaces, Python, R, SQL, Hadoop, Spark, Tableau, Power BI, Data visualization, Data analysis, Data mining, Statistics, Linguistics, Computational linguistics","machine learning, data annotation, data handling, quality assurance, natural language processing nlp, commandline tools and interfaces, python, r, sql, hadoop, spark, tableau, power bi, data visualization, data analysis, data mining, statistics, linguistics, computational linguistics","commandline tools and interfaces, computational linguistics, data annotation, data handling, data mining, dataanalytics, hadoop, linguistics, machine learning, natural language processing nlp, powerbi, python, quality assurance, r, spark, sql, statistics, tableau, visualization"
AI / ML Data Science Engineer,Harnham,"San Francisco County, CA",https://www.linkedin.com/jobs/view/ai-ml-data-science-engineer-at-harnham-3782581367,2023-12-17,Redwood City,United States,Mid senior,Onsite,"AI / ML Data Science Engineer
195,000 - 215,000 base + bonus + benefits
Location:
South Bay Area
Type:
Hybrid or Remote
Company Overview:
Heath Care Medical Device Company
Job Responsibilities:
* Collaborate with business and technology teams to understand the company's data and AI
needs
* Develop machine learning models and AI solutions based on business requirements
* Integrate machine learning models into business processes and workflows, develop and
maintain machine learning development and deployment pipelines
* Design and develop internal tools related to training and evaluation datasets creation
* Implement evaluation strategies to support machine learning development, implement
and maintain metrics for tracking ML models performance
* Model quality and performance optimization, monitoring, maintenance, and reporting
* Ensuring compliance with all relevant data regulations and AI/ML Standards &
Guidelines
Job Requirements:
* Bachelor's or Master's degree in computer science, data science, stat, math or a related
field
* Proven experience as a Data Science Engineer or a similar role,
* Strong understanding of machine learning concepts and algorithms, and experience with
developing and deploying machine learning models in production
* Working machine learning knowledge of Computer Vision models or NLP
* Strong programming skills in Python (3+ years), SQL
* Experience with optimizing models for production deployments
* Experience with a variety of ML frameworks and libraries, such as PyTorch,
TensorFlow, and scikit-learn
* Proficiency with cloud platforms such as Microsoft Azure, Google Cloud or AWS
* Familiarity with a cloud-based data warehousing platform such as Snowflake and data
analytics platforms such as Databricks
* Knowledge of data management and visualization techniques
* Strong problem-solving skills
* Strong communication skills
Benefits:
Competitive salary and performance-based bonuses.
Comprehensive health, dental, and vision insurance.
401(k) retirement savings plan with company matching.
Generous vacation and paid time off policy.
Continuous learning and professional development opportunities.
Collaborative work environment in a state-of-the-art facility.
Relocation assistance for eligible candidates.
Desired Skills and Experience
python, machine learning, google cloud, GCP
Show more
Show less","Python, Machine Learning, Computer Vision, NLP, PyTorch, TensorFlow, ScikitLearn, Cloud Computing, Azure, Google Cloud, AWS, Snowflake, Databricks, Data Warehousing, Data Analytics, Data Management, Data Visualization, Problem Solving, Communication","python, machine learning, computer vision, nlp, pytorch, tensorflow, scikitlearn, cloud computing, azure, google cloud, aws, snowflake, databricks, data warehousing, data analytics, data management, data visualization, problem solving, communication","aws, azure, cloud computing, communication, computer vision, data management, dataanalytics, databricks, datawarehouse, google cloud, machine learning, nlp, problem solving, python, pytorch, scikitlearn, snowflake, tensorflow, visualization"
Senior Data Engineer,Mercury,"San Francisco, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-mercury-3768476602,2023-12-17,Redwood City,United States,Mid senior,Onsite,"In the 1880s, Herman Hollerith noticed the US Census was taking over 8 years to calculate. To solve this, he invented a tabulating machine using punch cards that dramatically sped up the process and served as the foundation for innovation in high-quality data gathering.
We’re looking for Senior Data Engineers who can help us build our high-quality data engine that informs how we invest in and build Mercury’s future. You’ll be early to building a data-informed culture across Mercury so that we can all determine what’s happening, react quickly, and invest intelligently.
Here are some things you’ll do on the job:
Partner with leadership, engineers, and data scientists to understand data needs and build systems that deliver high-quality and reliable data
Own and maintain the data systems that extract, transform, and load data into internal and external tooling
Apply proven expertise and build high-performance scalable data warehouses.
Design, build and launch efficient & reliable data pipelines to move and transform data (both large and small amounts).
Design and develop new systems in partnership with software engineers to enable quick and easy consumption of data.
You should:
Have 2+ years of experience working with analytics teams on building high-quality and reliable data infrastructure.
Be able to navigate from architecture and implementation decisions related to data infrastructure to guide teams towards building reliable and accurate pipelines and company-critical data sets.
Have familiarity with postgres backend data, Snowflake, and data transformation tools like dbt.
Value quality in data tools, testing and innovation.
The total rewards package at Mercury includes base salary, equity (stock options), and benefits.
Our salary and equity ranges are highly competitive within the SaaS and fintech industry and are updated regularly using the most reliable compensation survey data for our industry. New hire offers are made based on a candidate’s experience, expertise, geographic location, and internal pay equity relative to peers.
Our target new hire base salary ranges for this role are the following:
US employees (any location): $173,600 - $204,200 USD
Canadian employees (any location): CAD 158,000-185,800
Show more
Show less","Data Engineering, Data Warehousing, Data Pipelines, Data Modeling, Data Transformation, Postgres, Snowflake, DBT, SQL, Python, Java, Scala, Hadoop, Spark, Kafka, AWS, Azure, GCP","data engineering, data warehousing, data pipelines, data modeling, data transformation, postgres, snowflake, dbt, sql, python, java, scala, hadoop, spark, kafka, aws, azure, gcp","aws, azure, data engineering, data transformation, datamodeling, datapipeline, datawarehouse, dbt, gcp, hadoop, java, kafka, postgres, python, scala, snowflake, spark, sql"
Data Engineer - Databricks,Diverse Lynx,"San Jose, CA",https://www.linkedin.com/jobs/view/data-engineer-databricks-at-diverse-lynx-3764425094,2023-12-17,Redwood City,United States,Mid senior,Hybrid,"Role – Data Engineer - Databricks
Location – San Francisco, CA (Preferred) or O Fallon, Missouri - (Day 1 WFO)
Type – Full-Time
Mandatory Skills
Python
DataBricks
PySpark
AWS
SQL
Job Description
5&plus; years in a customer facing Technical architecture, consulting role with the expertise in The following technology.
Developing modern Data warehouse solutions using Databricks and AWS/Azure stack.
Drive technical discussion with client architect and team members.
Knowledge in Databricks DELTA lake for the Analytical data lake use case
Hand on experience in create MLOPS data pipeline creation.
AIML Models develop, train and implement for the AIML use cases.
Knowledge in Banking domain on card and payment areas.
Any relation database – Data classification/ Data profiling for MLOPS use cases
Good experience in offshore onsite coordination.
Experience in translating a customer’s business needs.
Diverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company.
Show more
Show less","Python, Databricks, PySpark, AWS, SQL, Data warehouse, Cloud computing, MLOps, Big data, Machine learning, Artificial intelligence, Banking, Data classification, Data profiling","python, databricks, pyspark, aws, sql, data warehouse, cloud computing, mlops, big data, machine learning, artificial intelligence, banking, data classification, data profiling","artificial intelligence, aws, banking, big data, cloud computing, data classification, data profiling, databricks, datawarehouse, machine learning, mlops, python, spark, sql"
Aviation Pilot Data Analyst,Garmin,"Olathe, KS",https://www.linkedin.com/jobs/view/aviation-pilot-data-analyst-at-garmin-3774014239,2023-12-17,Palmyra,United States,Mid senior,Onsite,"Overview
We are seeking a full-time
Aeronautical Data Analyst 1
in our Olathe, KS location. In this role, you will be responsible for the attribution, processing, evaluation, and maintenance of aeronautical data and the execution of Quality Assurance processes.
Essential Functions
Edit, manipulate and maintain aeronautical data using custom and 3rd party software
Ensure the application of appropriate aeronautical specifications to the data and products
Help evaluate and process database updates according to established methodology
Identify and resolve basic issues or risks using proper aeronautical data capture techniques
Conduct testing and evaluation of GARMIN aeronautical data products
Demonstrate fundamental understanding of Garmin’s Aeronautical Data development process
Document work thoroughly in an organized manner
Demonstrate ability to learn, understand, provide input and apply established or new processes and projects
Demonstrate the ability to recognize and process multiple source types; translate/culminate necessary information for tasks within standard framework
Incorporate field knowledge within documented standards framework to identify and creatively solve problems
Work periodic over-time, holidays, and weekends as necessary
Data Collection Team:
Input or update, perform a QC process on, and maintain the features in the National Airspace System Resources (NASR) database
Maintain a working knowledge of the functionality of the National Airspace System (NAS), to include the direct correlation in support of Instrument Flight Rules (IFR) and on-board Flight Management Systems (FMS)
Maintain a working knowledge of the development, review and analysis of airspace structure to include new design processes and procedures
Production Team:
Acquire and maintain DO-200A associate qualifications according to applicable sections of the Personnel Qualification Manual
Acquire and maintain DO-200A initial and follow-on training requirements according to applicable sections of the Training Plan
Insert, update, and delete records in AeroNavData’s DO-200A qualified database
Process aeronautical source from the Standard and Non-Standard Aeronautical Information Publications (AIPs) listed in the Data Source Manual
Acquire a basic understanding of the ARINC 424 specifications
Interface with NFDC and AeroNav Products associates via e-mail or phone when directed
Interface with the Civil Aviation Authorities (CAA) via e-mail or phone when directed
Use and understand database queries written for our Microsoft SQL Server database using the Structured Query Language (SQL)
Basic Qualifications
Bachelor’s Degree in Aviation OR instrument rated pilot, Navigator, Air Traffic Controller, Aeronautical Analyst, or Military TERPS Specialist OR 2 years relevant experience OR substantial demonstrated aeronautical knowledge
Excellent academics (cumulative GPA greater than or equal to 3.0 as a general rule)
Demonstrate a basic working knowledge and understanding of aeronautical charting principles relevant to GARMIN’s business needs
Desired Qualifications
Outstanding academics (cumulative GPA greater than or equal to 3.5)
Previous work experience as an air traffic controller, military TERPS Specialist or aeronautical information specialist
Private Pilot’s License with instrument rating strongly preferred
Demonstrate an understanding of instrument flight rules, instrument procedures, and the United States National Airspace System
Familiarity with Cartography, GIS principles and methodologies
Bilingual skills
Garmin International is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to race, religion, color, national origin, citizenship, sex, sexual orientation, gender identity, veteran’s status, age or disability.
This position is eligible for Garmin's benefit program. Details can be found here: Garmin Benefits
Show more
Show less","Aeronautical Data, Garmin Aeronautical Data, National Airspace System Resources (NASR), National Airspace System (NAS), Instrument Flight Rules (IFR), Flight Management Systems (FMS), DO200A, Standard and NonStandard Aeronautical Information Publications (AIPs), ARINC 424, Microsoft SQL Server, Structured Query Language (SQL), Cartography, GIS, Bilingual, Garmin Benefits","aeronautical data, garmin aeronautical data, national airspace system resources nasr, national airspace system nas, instrument flight rules ifr, flight management systems fms, do200a, standard and nonstandard aeronautical information publications aips, arinc 424, microsoft sql server, structured query language sql, cartography, gis, bilingual, garmin benefits","aeronautical data, arinc 424, bilingual, cartography, do200a, flight management systems fms, garmin aeronautical data, garmin benefits, gis, instrument flight rules ifr, microsoft sql server, national airspace system nas, national airspace system resources nasr, standard and nonstandard aeronautical information publications aips, structured query language sql"
Senior Data Engineer,Workforce Australia for Individuals,"Perth, Western Australia, Australia",https://au.linkedin.com/jobs/view/senior-data-engineer-at-workforce-australia-for-individuals-3772467402,2023-12-17,Western Australia, Australia,Mid senior,Onsite,"Your New Company
An exciting opportunity to work for a state government entity within their Data & Analytics team.
Your new role
Design, develop data pipelines in Azure platform or Databricks
Data models development - conceptual, logical, and physical data models
Implementation of data marts in data lakes on target platforms (Azure)
What You'll Need To Succeed
Experience with tools like Azure Data Factory, Databricks, Python
Data modelling, Data warehousing, Data migration
What You'll Get In Return
Exposure to advanced technologies
Inclusive and collaborative team
Great career progression
What You Need To Do Now
If you're interested in this role, please forward an up-to-date copy of your CV to Lakita.Tarachandani@Hays.com.au or reach out for a confidential discussion today!
Please click here to apply.
Show more
Show less","Azure Data Factory, Databricks, Python, Data modeling, Data warehousing, Data migration, Data lakes, Data marts, SQL, ETL, AWS, Big Data, Cloud Computing","azure data factory, databricks, python, data modeling, data warehousing, data migration, data lakes, data marts, sql, etl, aws, big data, cloud computing","aws, azure data factory, big data, cloud computing, data lakes, data marts, data migration, databricks, datamodeling, datawarehouse, etl, python, sql"
Lead Data Engineer,BHP,"Perth, Western Australia, Australia",https://au.linkedin.com/jobs/view/lead-data-engineer-at-bhp-3785899511,2023-12-17,Western Australia, Australia,Mid senior,Onsite,"About BHP
At BHP we support our people to grow, learn, develop their skills and reach their potential. With a global portfolio of operations, we offer a diverse and inclusive environment with extraordinary career opportunities. Our strategy is to focus on creating a safe work environment where our employees feel strongly connected to our values and objectives, and where the capability of our people is key to our success.
Come and be a part of this success.
About The Role
The role can be based in Perth or Brisbane.
As a Data Engineer, you will play a key role supporting Digital projects that fundamentally change BHP’s performance across safety, productivity and people performance.
You will also contribute to the architectural design patterns for data engineering across our organization in collaboration with the Enterprise teams.
Your responsibilities
Define the data lifecycle (including data models and data sources for analytics platforms).
Gather and clean business data from the business in order to provide ready-to-work inputs for Data Scientists and other consumers.
Work closely with internal BHP customers to understand their data requirements, develop and model data structure, and design and build the ingestion process to provide access to data from operational and enterprise source systems.
Be involved in the design and development of data integration and data pipelines (ETL).
Plan and deliver secure, good practice data integration strategies and approaches.
Work closely with database teams on topics related to data requirements, cleanliness, quality etc.
About You
Master’s degree in Computer Science, MIS, or Engineering.
Strong professional experience working in data engineering or architecture role.
Demonstrated experience working across structured, semi-structured, and unstructured data.
Experienced in multiple database technologies such as Distributed Processing, Traditional RDBMS, MPP, NoSQL.
Strong understanding of understanding of traditional ETL tools & RDBMS, End to End Data Pipeline.
Knowledge of data modelling (i.e. ERDs, Star Schemas, Snowflake schemas, etc.).
Experience with AWS cloud services: S3, EC2, EMR, RDS, Redshift and Kinesis.
Strong experience with Python and at least two of the following technologies: Scala, SQL, Java.
Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.
Experience working in DevOps, Agile, Scrum, Continuous Delivery and/or Rapid Application Development environments.
Mining domain exposure would be highly regarded.
About Our Process
At BHP, we are committed to employing individuals who align with the BHP Charter Values and meet the requirements of the role. As part of the recruitment process, there are a number of checks which may be conducted to demonstrate applicants suitability for a role including police / criminal background checks, medical, drug and alcohol testing, due diligence checks, right to work checks, and/or reference checks.
If you are already employed directly by BHP, please log in using your BHP email address or apply via our internal jobs portal.
Supporting a Diverse Workforce
The size, stability and magnitude of our business not only provides significant opportunity for professional development, but also attractive salary packages with performance-based bonuses and a best-in-class employee share program. We know there are many aspects of our employees' lives that are important, and work is only one of these, so we offer benefits to enable your work to fit with your life. These benefits include flexible working options, a generous paid parental leave policy, other extended leave entitlements and parent rooms.
At BHP, we know that we are strengthened by diversity. We are an Equal Opportunity employer that is committed to making BHP a safe and inclusive workplace where everyone can thrive and be at their best every day. We are focused on creating a workforce that’s more diverse and represents the communities where we work and live. providing a work environment in which everyone is included, treated fairly and with respect. We are an Equal Opportunity employer and recognise that true diversity includes gender, age, race, disability status, sexual orientation, religion, neurodiversity, education levels, and many more aspects of your identity.
BHP is committed to providing a recruitment process that is fair, equitable and accessible for all. If you have a disability, we know that it may be helpful for us to adjust our process to make it equitable for your individual situation. If you would like to reach out to someone about your situation and our recruitment process, please email us at
[email protected]
.
Show more
Show less","Data engineering, Data architecture, Distributed processing, Traditional RDBMS, MPP, NoSQL, ETL tools, ERDs, Star Schemas, Snowflake schemas, AWS cloud services, S3, EC2, EMR, RDS, Redshift, Kinesis, Python, Scala, SQL, Java, Data pipeline, Workflow management tools, Azkaban, Luigi, Airflow, DevOps, Agile, Scrum, Continuous Delivery, Rapid Application Development, Mining","data engineering, data architecture, distributed processing, traditional rdbms, mpp, nosql, etl tools, erds, star schemas, snowflake schemas, aws cloud services, s3, ec2, emr, rds, redshift, kinesis, python, scala, sql, java, data pipeline, workflow management tools, azkaban, luigi, airflow, devops, agile, scrum, continuous delivery, rapid application development, mining","agile, airflow, aws cloud services, azkaban, continuous delivery, data architecture, data engineering, data pipeline, devops, distributed processing, ec2, emr, erds, etl tools, java, kinesis, luigi, mining, mpp, nosql, python, rapid application development, rds, redshift, s3, scala, scrum, snowflake schemas, sql, star schemas, traditional rdbms, workflow management tools"
Senior Data Engineer,Telstra,"Perth, Western Australia, Australia",https://au.linkedin.com/jobs/view/senior-data-engineer-at-telstra-3781772947,2023-12-17,Western Australia, Australia,Mid senior,Onsite,"We're an iconic Aussie brand with a global footprint. Our purpose is to build a connected future so everyone can thrive. We're all about providing the best experience and delivering the best tech on the best network.
This includes making Telstra the place you want to work.
For you, that means a having career that grows with you, and working with a team powered by human connection that prioritizes well-being and choice.
Here's what you can expect from us
Flexible working. Wherever you live, choose where, when, and how you work so you can be at your best.
There when life happens. From Gender Affirmation Leave to Telstra's Additional Leave Day, we give you time to spend away from work when you need it most.
Pay for performance. We recognize outstanding contributions through our generous incentive programs.
Parental Leave. A gender-equal policy for parents so you can choose how and when you take your leave (16 weeks full pay or 32 weeks half pay). Receive superannuation contributions on paid and unpaid parental leave.
Unlimited learning. Level up your credentials with access to 17,000 learning programs. Learn 'on the job' and achieve university credits toward degrees and master's programs.
Global presence. With a global presence across 22 countries, there are many opportunities to work where we do business.
Extra perks. Receive 25% off Telstra products and services, plus unlock exclusive discounts from over 240 brands and partners.
Hi, we're Telstra Purple
👋 People bringing purpose to technology.
We are Australia's largest owned technology services business, focused on outcome-based, transformative tech solutions that help our customers cut through IT complexity to achieve their bespoke goals, faster. If solving unique problems sparks your interest to help change a business, a community or the environment for the better - we're in sync.
The opportunity
We're on the lookout for a Senior Data Engineer to join our team in Perth
.
We solve real problems for the biggest clients in Western Australia, South Australia, and Northern Territory. We work in Health, Mining, Industry, Education, Commercial, and Government.
The technology landscape we operate in is both diverse and dynamic. One day, you might find yourself crafting a report using Power BI, and the next, engineering an ELT pipeline within AWS. Alternatively, you could be contributing to a customer's data governance journey. The possibilities are endless. If you're passionate about delivering value to customers through cutting-edge AI solutions, that's certainly a plus, but it's not a requirement.
About you
We offer some flexibility regarding the level of this role, but certain core requirements include:
Experience in data engineering and analytics,
Be a problem solver with data,
An automation-first mindset. CI/CD tooling, scripting, and DevOps are in your DNA,
A knack for listening and being able to dissect our customers' needs,
Good communication skills,
An ability to work well in a team environment,
An interest in professional development and a thirst for knowledge
​Experience working with customers to improve data architecture, data governance, and/or data maturity.
Ability to challenge norms and embrace new technologies.
Here's some of the technology stack we work with and on for our customers.
Azure DevOps, Git, Data Factory, Purview Synapse, and/or Machine Learning
AWS Glue/ LakeFormation and/or SageMaker
ElasticSearch
PowerBI
Google BigQuery
Databricks
SQL Server
Hadoop
Python, Spark, Jupyter, SQL and PowerShell
What we offer
15 dedicated days per year, for professional development; meaning you don't have to use your holiday allowance to up-skill + study.
A Toolkit allowance of $4200, to spend on your personalised toolkit, renewed every 2 years.
Access to the $10,000 Short-Term Incentive plan
16 weeks of paid parental leave for both primary and secondary carers.
Work/life enablement; so, you can balance work + life's other responsibilities. We work hybrid without set days in the office.
An epic (free) Telstra mobile phone and plan; you'll never receive a bill.
An extra day off per year, for you to spend as you like
Purchased Annual Leave scheme.
30% discount on Telstra products and services
If you are passionate about succeeding as part of an agile and experienced team, we welcome you to apply!
-------------------------------------
We encourage applications from people of all abilities and backgrounds - including Aboriginal and Torres Strait Islander peoples, the LGBQTI+ community, linguistically diverse, and people living with disability. When you apply, you can choose to note the pronouns you use and /or any reasonable adjustments needed to take part equitably during the interview process. Visit
www.telstra.com.au/careers/diversity-and-inclusion or email us at noting your preferred method of contact.
Recruitment Start Date: 15/12/2023
Recruitment End Date: 12/01/2024
Show more
Show less","Data Engineering, Data Analytics, AWS, Azure, Data Factory, Purview Synapse, Machine Learning, Glue/ LakeFormation, SageMaker, ElasticSearch, PowerBI, Google BigQuery, Databricks, SQL Server, Hadoop, Python, Spark, Jupyter, SQL, PowerShell, CI/CD, Scripting, DevOps","data engineering, data analytics, aws, azure, data factory, purview synapse, machine learning, glue lakeformation, sagemaker, elasticsearch, powerbi, google bigquery, databricks, sql server, hadoop, python, spark, jupyter, sql, powershell, cicd, scripting, devops","aws, azure, cicd, data engineering, data factory, dataanalytics, databricks, devops, elasticsearch, glue lakeformation, google bigquery, hadoop, jupyter, machine learning, powerbi, powershell, purview synapse, python, sagemaker, scripting, spark, sql, sql server"
Senior Data Analyst,Workforce Australia for Individuals,"Perth, Western Australia, Australia",https://au.linkedin.com/jobs/view/senior-data-analyst-at-workforce-australia-for-individuals-3767984106,2023-12-17,Western Australia, Australia,Mid senior,Onsite,"Are you a seasoned Data Analyst seeking the next big challenge? We have an exclusive opportunity for you to join a prestigious Health, Safety, Environment, and Sustainability project with a major global resources company! As their recruitment partner, we're on the lookout for a dynamic and experienced
Senior Data Analyst
to lead the charge in data governance and management within this critical domain.
Role Highlights
Data Governance Leadership: Spearhead data governance and management activities in the Health, Safety, Environment, and Sustainability project.
Compliance Expertise: Ensure adherence to Data and Records Management Standards, setting the gold standard in compliance.
Business Definitions Mastery: Maintain business definitions of key data elements and propose innovative attribute requirements.
Record File Plan Custodianship: Take charge of the Record File Plan within the assigned domain for streamlined data organization.
Change Management Guru: Lead data change management and conduct impactful related impact analysis.
Qualifications
Technical Prowess: Demonstrate a highly developed understanding of database structures, showcasing the ability to construct and manipulate databases for insightful analytics and reporting.
Database Navigation: Navigate databases with finesse, identifying and rectifying errors in the data.
Tech Stack Mastery (Mandatory):
Relational Databases
SQL Server
Advanced Excel skills
Power BI
DAX
Desirable Skills
Master Data Management expertise
Enterprise Data Quality Control experience
Familiarity with Enablon, Cority, SafeDay, Forwood CRM, RTBS (SAP), BW4Hana, Azure Integration Platform
Shortlisted candidates will be contacted immediately.
To be considered for the role click the 'apply' button or for more information about this and other opportunities please contact Nik Stojanov on 08 61511702 or email: nstojanov@paxus.com.au and quote the above job reference number.
Paxus values diversity and welcomes applications from Indigenous Australians, people from diverse cultural and linguistic backgrounds and people living with a disability. If you require an adjustment to the recruitment process please contact me on the above contact details.
Show more
Show less","Data Governance, Data Management, Compliance, Data Standards, Business Definitions, Record File Plan, Change Management, Impact Analysis, Database Structures, SQL Server, Advanced Excel, Power BI, DAX, Master Data Management, Data Quality Control, Enablon, Cority, SafeDay, Forwood CRM, SAP, BW4Hana, Azure Integration Platform","data governance, data management, compliance, data standards, business definitions, record file plan, change management, impact analysis, database structures, sql server, advanced excel, power bi, dax, master data management, data quality control, enablon, cority, safeday, forwood crm, sap, bw4hana, azure integration platform","advanced excel, azure integration platform, business definitions, bw4hana, change management, compliance, cority, data governance, data management, data quality control, data standards, database structures, dax, enablon, forwood crm, impact analysis, master data management, powerbi, record file plan, safeday, sap, sql server"
Data Analyst Project Lead,"Department of Planning, Lands and Heritage","Perth, Western Australia, Australia",https://au.linkedin.com/jobs/view/data-analyst-project-lead-at-department-of-planning-lands-and-heritage-3784292359,2023-12-17,Western Australia, Australia,Mid senior,Onsite,"Want to be a part of a once in a lifetime digital transformation program for WA Government?
Interested in a role that offers exposure to a wide range of stakeholders and the opportunity to be a part of streamlining solutions for the future?
The Opportunity
The Data Analyst Project Lead will be a key participant, from our Department, in Landgate’s Spatial WA Program, assisting in delivering a whole of government Advanced Spatial Digital Twin for Western Australia.
The Data Analyst Project Lead will support the Project Manager Spatial WA with the implementation of the program’s Digital Approval Workflows use case, engaging with work across the department and partner agencies involved in Spatial WA to ensure the Spatial Digital Twin platform has the required capability.
To be successful for this position, you need to demonstrate:
Experience in implementing data management practices, working with geospatial data, formats and services supporting the delivery of geospatial solutions.
Experience in data modelling, analysis and visualisation platforms and processes.
A tertiary qualification in spatial sciences or another relevant discipline is highly desirable.
Experience in and ability to undertake business process modelling.
Experience working with 3D geospatial data, data formats, and services is highly desirable.
Demonstrate the behaviours within the leadership context of Leading Others as outlined below.
Core leadership behaviours outlined in Leadership Expectations are applicable to this role and should be demonstrated in the context of Leading Others. Leading Others context is about motivating and enabling others to deliver high quality work that contributes to the agency, focused on leading and managing specialized knowledge and technical expertise. The expected behaviours may be assessed during the recruitment and selection process.
This position is initially a fixed term contract up to the 30 June 2024, with the possibility of 12 month extension contracts up until 2028.
About us
The Department of Planning, Lands and Heritage (DPLH) is responsible for planning and managing land and heritage for all West Australians – now and into the future.
To find out more about the Department and what we have to offer, please visit the Careers at the Department page on the DPLH website.
The Department's values are an integral part of how we do business, and we expect all successful candidates to demonstrate that you are the right cultural fit with a positive and can-do attitude, therefore alignment with our values should be incorporated into your application.
Our Values:
Integrity – Respect – Collaboration – Professionalism – Innovation
Public Sector Code of Ethics:
Personal Integrity – Relationships with others - Accountability
Further Information:
If you would like to know more, please give
John Chortis
a call on
0408 951 906 or email john.chortis@dplh.wa.gov.au
to discuss the role in further detail.
Please do not send your application to John or seek his assistance with lodging your application.
How to apply:
Apply now by submitting the following;
A maximum two (2) page covering letter, addressing the requirements outlined above;
A comprehensive CV that clearly shows your skills, experiences, and key achievements, including the details of two referees.
This process may be used to fill future similar permanent or fixed-term positions that may arise across the WA public sector in the next 12 months from the initial date of appointment.
Special Requirements for this position:
The Department will conduct National Police Clearances for applicants successful to this recruitment process.
To be eligible for a permanent appointment to the Western Australian public sector it is essential that you are an Australian citizen or have permanent residency status in Australia. To be eligible for a fixed term appointment you must have documentary evidence of your entitlement to live and work in Australia for the period of the contract.
Please ensure you have saved your documents in MS Word (.docx files only) or PDF format and have allowed plenty of time to submit your application, as
late, emailed and proforma applications will not be accepted.
The Department is an equal opportunity employer and as a measure to achieve equality under sections 51, 66R and 66ZP of the Equal Opportunity Act 1984, Aboriginal and Torres Strait Islander people, young people (24 years of age and under) and people with disability who meet the requirements of the position, may be prioritised for appointment.
Aboriginal and/or Torres Strait Islander applicants – If you would like to have a confidential discussion with Denika Seelander, our Aboriginal Employment Officer, about the recruitment process please contact her on 0461 552 881 or email aboriginalemployment@dplh.wa.gov.au
Accessibility
: If you have any communication or access needs that require adjustment to allow you to participate in this recruitment process, please contact our HR Services team on (08) 6551 9419 or email recruitment@dplh.wa.gov.au
If you experience technical issues when submitting your application, contact ramshelpdesk@bigredsky.com, or please call (08) 6551 9419 for any other queries.
Closing Date – Friday 5th January 2024 @ 4:00pm
If you are interested in applying for this opportunity, please click the link to Jobs WA.
Show more
Show less","Data Management, Geospatial Data, Spatial Sciences, Business Process Modelling, 3D Geospatial Data, Leadership, Data Modelling, Data Analysis, Data Visualization, Data Platforms","data management, geospatial data, spatial sciences, business process modelling, 3d geospatial data, leadership, data modelling, data analysis, data visualization, data platforms","3d geospatial data, business process modelling, data management, data modelling, data platforms, dataanalytics, geospatial data, leadership, spatial sciences, visualization"
Lead Data Engineer,Zelis,"St Petersburg, FL",https://www.linkedin.com/jobs/view/lead-data-engineer-at-zelis-3767524944,2023-12-17,Bradenton,United States,Mid senior,Onsite,"Summary
Build High level technical design both for Streaming and batch processing systems
Design and build reusable components, frameworks and libraries at scale to support analytics data products
Perform POCs on new technology, architecture patterns
Design and implement product features in collaboration with business and Technology stakeholders
Anticipate, identify, and solve issues concerning data management to improve data quality
Clean, prepare and optimize data at scale for ingestion and consumption
Drive the implementation of new data management projects and re-structure of the current data architecture
Implement complex automated workflows and routines using workflow scheduling tools
Build continuous integration, test-driven development and production deployment frameworks
Drive collaborative reviews of design, code, test plans and dataset implementation performed by other data engineers in support of maintaining data engineering standards
Analyze and profile data for the purpose of designing scalable solutions
Troubleshoot complex data issues and perform root cause analysis to proactively resolve product and operational issues
Lead, Mentor and develop offshore Data Engineers in adopting best practices and deliver data products.
Partner closely with product management to understand business requirements, breakdown Epics,
Partner with Engineering Managers to define technology roadmaps, align on design, architecture, and enterprise strategy
Requirements
Minimum of 8+ years experience with the following:
Snowflake (Columnar MPP Cloud data warehouse)
DBT (ETL tool)
Python
Experience designing and implementing Data Warehouse
Preferred Skills
Azure/AWS cloud technology
SQL objects (procedures, triggers, views, functions) in SQL Server. SQL query optimizations
Understanding of T-SQL, indexes, stored procedures, triggers, functions, views, etc.
Design and development of Azure/AWS Data Factory Pipelines preferred.
Design and development of data marts in Snowflake preferred
Working knowledge of Azure/AWS Architecture, Data Lake, Data Factory
Business analysis experience to analyze data to write code and drive solutions
Knowledge of: Git, Azure DevOps, Agile, Jira and Confluence
Independence/ Accountability
Requires minimal daily supervision
Receives detailed instruction on new assignments and determines next steps with guidance
Regularly reviews goals and objectives with supervisor
Demonstrates competence in relevant job responsibilities which allows for increasing level of independence
Ability to manage and prioritize multiple tasks
Ability to work under pressure and meet deadlines
Problem Solving
Makes logical suggestions of likely causes of problems and independently suggests solutions
Excellent organizational skills are required to prioritize responsibilities, thus completing work in a timely fashion
Outstanding ability to multiplex tasks as required
Excellent project management and/or business analysis skills.
Attention to detail and concern for impact is essential
As a leading payments company in healthcare, we guide, price, explain, and pay for care on behalf of insurers and their members. We’re Zelis in our pursuit to align the interests of payers, providers, and consumers to deliver a better financial experience and more affordable, transparent care for all. We partner with more than 700 payers, including the top-5 national health plans, BCBS insurers, regional health plans, TPAs and self-insured employers, over 4 million providers, and 100 million members, enabling the healthcare industry to pay for care, with care. Zelis brings adaptive technology, a deeply ingrained service culture, and a comprehensive navigation through adjudication and payment platform to manage the complete payment process.
Commitment to Diversity, Equity, Inclusion, and Belonging
At Zelis, we champion diversity, equity, inclusion, and belonging in all aspects of our operations. We embrace the power of diversity and create an environment where people can bring their authentic and best selves to work. We know that a sense of belonging is key not only to your success at Zelis, but also to your ability to bring your best each day.
Equal Employment Opportunity
Zelis is proud to be an equal opportunity employer. All applicants will receive consideration for employment without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.
We encourage members of traditionally underrepresented communities to apply, even if you do not believe you 100% fit the qualifications of the position, including women, LGBTQIA people, people of color, and people with disabilities.
Accessibility Support
We are dedicated to ensuring our application process is accessible to all candidates. If you are a qualified individual with a disability or a disabled veteran and require a reasonable accommodation with any part of the application and/or interview process, please email TalentAcquisition@zelis.com
SCAM ALERT: There is an active nationwide employment scam which is now using Zelis to garner personal information or financial scams. This site is secure, and any applications made here are with our legitimate partner. If you’re contacted by a Zelis Recruiter, please ensure whomever is contacting you truly represents Zelis Healthcare. We will never asked for the exchange of any money or credit card details during the recruitment process. Please be aware of any suspicious email activity from people who could be pretending to be recruiters or senior professionals at Zelis.
Show more
Show less","Data Engineering, Data Analytics, Data Warehouse Design, Data Management, Data Integration, Data Quality, Data Architecture, Snowflake, DBT, Python, SQL, TSQL, Azure Data Factory, Azure DevOps, Git, Jira, Confluence, Agile, AWS, Data Lake, Data Factory, Business Analysis, Project Management","data engineering, data analytics, data warehouse design, data management, data integration, data quality, data architecture, snowflake, dbt, python, sql, tsql, azure data factory, azure devops, git, jira, confluence, agile, aws, data lake, data factory, business analysis, project management","agile, aws, azure data factory, azure devops, business analysis, confluence, data architecture, data engineering, data factory, data integration, data lake, data management, data quality, data warehouse design, dataanalytics, dbt, git, jira, project management, python, snowflake, sql, tsql"
Senior BI/Data Engineer,Robert Half,"Sarasota, FL",https://www.linkedin.com/jobs/view/senior-bi-data-engineer-at-robert-half-3778720095,2023-12-17,Bradenton,United States,Mid senior,Onsite,"Description
Robert Half has an exciting new opening for a Senior BI/Data Engineer with a reputable client in Sarasota, FL!
Position is Hybrid, being 3 days in-office and 2 days WFH.
Compensation is flexible $80-90K depending on experience.
Interviews actively being scheduled - Apply NOW!
Job Description
This position is responsible for managing web applications, analyzing requirements for web and third-party applications, maintaining, and modifying existing web applications, designing, and developing new web applications, and reporting project status, as well as maintaining third-party applications.
Actively participates as project lead in software and hardware upgrades and keeps manager informed of progress, obstacles.
Provides consultation on complex projects and assist team accomplish goals by providing solutions to problems.
Individual is the top-level contributor/specialist. Ability to apply independent judgment and creativity to resolution of contract issues.
Act as a consultant and liaison for users and ensure that problems are handled appropriately by prioritizing internally and working with staff/vendors to insure timely resolution. Provide feedback to customers and to IT managers as necessary.
Application Support, Development Assist in the specification, installation, testing, and support of new versions of operating systems, databases, and other customized or purchased software as it relates to supported applications.
Develop and maintain SQL Server Database, Tables, Views, Stored Procedures, SSIS packages.
Develop and maintain in-house web applications using the approved application development platform and tools: ASP.NET, ASP.NET MVC, MS SQL, XML, ASP, JavaScript, AJAX, in a MS Windows environment.
Administration of large enterprise applications, configures and maintain all elements related to the applications including configuration, monitoring, tuning and import/export data to/from the systems.
Requirements
Looking for 5+ years experience:
MS Office 365
Sharepoint
SQL Server/Database files
Data related work/stored procedures
SSIS
Dashboards
PowerBI
Must be available for full-time hours, HYBRID schedule on W2*
Technology Doesn't Change the World, People Do.®
Robert Half is the world’s first and largest specialized talent solutions firm that connects highly qualified job seekers to opportunities at great companies. We offer contract, temporary and permanent placement solutions for finance and accounting, technology, marketing and creative, legal, and administrative and customer support roles.
Robert Half works to put you in the best position to succeed. We provide access to top jobs, competitive compensation and benefits, and free online training. Stay on top of every opportunity - whenever you choose - even on the go.
All applicants applying for U.S. job openings must be legally authorized to work in the United States. Benefits are available to contract/temporary professionals, including medical, vision, dental, and life and disability insurance. Hired contract/temporary professionals are also eligible to enroll in our company 401(k) plan. Visit
© 2024 Robert Half. An Equal Opportunity Employer. M/F/Disability/Veterans. By clicking “Apply Now,” you’re agreeing to
Show more
Show less","MS Office 365, SharePoint, SQL Server, Data Manipulation, SSIS, Dashboards, PowerBI, ASP.NET, ASP.NET MVC, MS SQL, XML, JavaScript, AJAX, Windows","ms office 365, sharepoint, sql server, data manipulation, ssis, dashboards, powerbi, aspnet, aspnet mvc, ms sql, xml, javascript, ajax, windows","ajax, aspnet, aspnet mvc, dashboard, data manipulation, javascript, ms office 365, ms sql, powerbi, sharepoint, sql server, ssis, windows, xml"
Senior Data Management / Solix Developer (Hybrid),FCCI Insurance Group,"Sarasota, FL",https://www.linkedin.com/jobs/view/senior-data-management-solix-developer-hybrid-at-fcci-insurance-group-3767343098,2023-12-17,Bradenton,United States,Mid senior,Onsite,"Join our team of creative IT professionals within our F&A (Finance & Accounting) Data & Reporting team where you can truly focus on the delivery of meaningful data management, security and governance using the Solix application. Support the Data Warehouse ETL, reporting and data analytics, utilizing available & emerging technologies. Concurrent with our existing Data warehouse practices, you will have the opportunity to work on key projects alongside a great team providing data security, retention and governance across our IT platform. Our focus is to produce secured, documented, high quality applications software in support of FCCI’s various & expanding business needs. This role will configure applications to meet NAIC model security regulations for data retention and anonymization as well as supporting our migration to data lakes/lakehouse. Working in an Agile development environment, you will enjoy collaborating with our business & technical leaders using your business intelligence, data warehouse and system development skills.
A commitment to collaborative problem solving, sophisticated design, and delivering a quality product is essential. As part of our collaborative culture, the selected individual will provide on call support on a rotational basis to applications as necessary. This position will provide the opportunity to work a hybrid work schedule of 3 days in the office and 2 days remote, based at either our state of the art, Sarasota, FL corporate office campus or one of the following locations: Lake Mary, Florida, Lawrenceville, GA or Richardson, TX.
In exchange for your talents, FCCI offers competitive salaries and an excellent benefits package.
FCCI values the contributions of a diverse workforce.
We're committed to being fair and equitable to all employees and applicants for employment. FCCI prohibits discrimination on the basis of race, color, sex, age, marital status, religion, national origin, sexual orientation, handicap, disability and other legally protected classifications.
Please apply via our website at www.fcci-group.com.
Qualifications
Experience
Required
Bachelor’s Degree in Computer Science or equivalent; NOTE - Relevant, progressively responsible work experience may be substituted on a year for year basis for the required education
Minimum four years working knowledge BI, Data Warehouse ETL processes and Data Governance concepts, (preferably including experience with Data Modeling Methodologies focused on Data Standardization.)
Four or more years with SQL programming language;
Two years working with business intelligence tools as a lead (or leading) development teams for both projects and support
Experience working in an Agile environment
Solid working knowledge of Data Governance (including data retention and anonymization)
Advanced understanding of testing and documentation skills
Solid working knowledge of business design and analysis
Solid working knowledge of object-oriented programming concepts
Solid working knowledge of SOA concepts
Preferred
Two or more years developing Solix knowledge base, reports and retention rules, in addition, must be able to support the server infrastructure applying hot fixes and patches from vendor support
Two or more years’ experience with SAS & Cognos
Working knowledge of AWS Cloud envronment
Show more
Show less","Data Management, Data Security, Data Governance, Solix, Data Warehouse ETL, Data Analytics, Business Intelligence, Data Modeling, SQL, Agile Development, Testing, Documentation, Business Design, Analysis, ObjectOriented Programming, SOA","data management, data security, data governance, solix, data warehouse etl, data analytics, business intelligence, data modeling, sql, agile development, testing, documentation, business design, analysis, objectoriented programming, soa","agile development, analysis, business design, business intelligence, data governance, data management, data security, data warehouse etl, dataanalytics, datamodeling, documentation, objectoriented programming, soa, solix, sql, testing"
Senior Data Analyst,NCSA College Recruiting,"Bradenton, FL",https://www.linkedin.com/jobs/view/senior-data-analyst-at-ncsa-college-recruiting-3782536362,2023-12-17,Bradenton,United States,Mid senior,Onsite,"Education
IMG Academy is the world’s leading sports education brand, providing a holistic education model that empowers student-athletes to win their future, preparing them for college and for life. IMG Academy offers an innovative suite of on-campus and online experiences, providing growth opportunities for all student-athletes through:
Boarding school and camps, via a 600-acre state-of-the-art campus in Bradenton, Fla. ( IMGAcademy.com )
Online coaching via the IMG Academy+ brand, with a focus on personal development through the lens of sport and performance ( IMGAcademy.com/Plus )
Online college recruiting, via the NCSA brand, providing content, tools, coaching and access to a network of 40,000 college coaches ( ncsasports.org )
Our team has a deep appreciation for the transformative power of sports and holistic personal development. Our leadership is actively investing in the growth of the organization. We continue to broaden and deepen our technology platform and team in pursuit of our vision for empowering youth sports and the path-to-college for student-athletes.
Position Summary
The
S
enior Data Analyst
will be responsible for leading the design and implementation of analytics solutions as part of the Data T eam. The team provides data, analysis, insights and recommendations in support of the company's fiercely data-driven decision-making culture in sales, operational, financial and product domains. The Sr. Data Analyst is responsible for partnering cross-functionally across the organization to understand business goals, define questions and methods for answering them. The position makes use of multiple data sources, analysis tools and statistical frameworks to construct analytical outputs, reporting and dashboards in support of daily operations, new initiatives, and the development of proprietary predictive models. Working closely with the Data Engineering team and the Platform Development Manager for data infrastructure, the Sr. Data Analyst both makes use of and provides ongoing recommendations for improving the company’s platform components for data collection, aggregation, processing and storage. The position serves as an internal thought leader for decision-making around one or more major areas of business focus, serving as subject matter expert for senior management, operational leaders, and the Product and Technology teams.
Candidates should have experience leading analytics teams, working cross-functionally with business stakeholders and data engineering teams, sustaining mission-critical reporting and insights delivery. They should be very comfortable providing insights to business leaders using descriptive, diagnostic , and predictive analytics. They should demonstrate a rigorous scientific approach, using experimental methods to test hypotheses across multiple data sets using peer-reviewed statistical methods to ensure optimal business performance. Above all else , they should be a servant leader who understands their primary role to be supporting and enabling their team by maintaining a healthy, supportive culture.
Position Responsibilities
Help the Data T eam cultivate cross-functional partnerships across departments, bringing insights to every corner of our operation as part of a collaborative, collegial learning culture.
Support the career development of data analysts , and reporting specialists on team building and delivering Internet-scale analytics solutions.
Provide leadership and guidance to an individual analytics team and its members, offering thoughtful and timely coaching and effective feedback.
Partner with leadership in Marketing, Sales Operations, Sales, Operations, Product and Technology to solicit analytics business requirements and frame actionable questions.
Facilitate the acquisition and integration of data from multiple data sources, performing exploratory analysis to identify data relationships including trends, patterns and correlations; mature these analyses into more formalized frameworks and models.
Use data processing, querying and analysis tools to answer business questions and provide actionable insights.
Use visualization tools to communicate stories with data, creating reporting, dashboards, KPI monitoring and self-service tools to make data and insights available for stakeholders.
Partner with other data analysts and data engineers to identify and address limitations in data sources, data quality, tooling, or performance bottlenecks in analytics delivery.
Develop and maintain increasing awareness of the business context and dynamics surrounding each of the company’s business units, their products and services, the data they generate and the relationships between them.
Lead data analysts, data engineers and database administrators from the initial discovery and design stages of new data science initiatives all the way through implementation and operationalization, including the construction of required data structures, extracts and analytical data stores.
Seek opportunities to perform 'data pioneering' for identifying new opportunities for analysis and insight generation while maintaining focus on reuse and repurposing of frameworks across domains.
Knowledge, Skills And Abilities
Bachelor's or master's degree in a quantitative discipline (Mathematics or Social Science).
5+ years' data analytics experience in a commercial or scientific setting.
Experience supporting critical analytics delivery in a fast-paced environment.
Experience working across functional, product and engineering teams.
Experience using task management and collaboration software such as Jira or Trello.
Experience with distributed version control and software development lifecycle methods.
Exposure to Agile Development or the Scrum project management methodology a plus.
Experience and understanding of how modern analytics frameworks are constructed to define questions and answers using applied statistical methods.
Strong data process definition and refinement skills, with an instinctive understanding for identifying areas of opportunity or challenge and methods for providing a clear analysis.
Ability to construct models for forecasting and tracking business performance in support of sales, marketing, operations, product or financial domains.
Expertise in a data science programming language like R or Python.
Expertise composing complex SQL queries for relational databases.
Hands-on experience building custom visualizations using tools like Tableau or Power BI.
Experience with ELT and data integration tools like Fivetran or Stitch Data.
Experience with modern data warehouse tools like Snowflake, Big Query, or Redshift.
Experience with data modeling and schema definition for data lakes and warehouses.
Experience leading cross-functionally in situations requiring collaboration, negotiation, and communication of complex analyses to non-technical or senior management audiences.
Experience working as a team player with other data scientists and engineers; always looking to make things better and easier for co-workers.
Excellent visual, verbal, and textual communication and presentation skills; extremely strong skills for using data to tell stories, especially counter-intuitive or controversial ones.
Highly self-motivated, self-directed, resourceful, and autonomous; capable of managing multiple simultaneous deliverables with proactive status updates and requests for support.
Strong problem-solving skills with the ability to pinpoint and resolve problematic issues quickly and efficiently .
Background Requirements
Requires a background check upon offer
Requires a drug check upon offer
Don’t meet every single requirement? We are dedicated to building a diverse, inclusive, authentic workplace, so if you’re excited about this role but your past experience doesn’t align perfect with every qualification in the job description, we encourage you to apply anyway. You may be just the right candidate for this or other roles.
Get To Know Us Better
www.imgacademy.com
IMG Academy provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.
Apply Now
Show more
Show less","Data Analytics, Business Intelligence, Tableau, Power BI, Fivetran, Stitch Data, Snowflake, Big Query, Redshift, Data Modeling, Schema Definition, R, Python, SQL, Jira, Trello, Agile Development, Scrum, Experimental Methods, Statistical Methods, Data Visualization, Data Warehousing, Data Engineering, Data Science, Machine Learning, Predictive Analytics, Data Mining, Data Integration, Data Quality, Data Governance, Data Security, Data Privacy","data analytics, business intelligence, tableau, power bi, fivetran, stitch data, snowflake, big query, redshift, data modeling, schema definition, r, python, sql, jira, trello, agile development, scrum, experimental methods, statistical methods, data visualization, data warehousing, data engineering, data science, machine learning, predictive analytics, data mining, data integration, data quality, data governance, data security, data privacy","agile development, big query, business intelligence, data engineering, data governance, data integration, data mining, data privacy, data quality, data science, data security, dataanalytics, datamodeling, datawarehouse, experimental methods, fivetran, jira, machine learning, powerbi, predictive analytics, python, r, redshift, schema definition, scrum, snowflake, sql, statistical methods, stitch data, tableau, trello, visualization"
Software Engineer - Data Services and Modernization - 5044353,Accenture,"St Petersburg, FL",https://www.linkedin.com/jobs/view/software-engineer-data-services-and-modernization-5044353-at-accenture-3775608989,2023-12-17,Bradenton,United States,Mid senior,Onsite,"Accenture Flex
offers you the flexibility of local fixed-duration project-based work powered by Accenture, a leading global professional services company. Accenture is consistently recognized on FORTUNE's 100 Best Companies to Work For and Diversity Inc's Top 50 Companies For Diversity lists.
As an
Accenture Flex
employee, you will apply your skills and experience to help drive business transformation for leading organizations and communities. In addition to delivering innovative solutions for Accenture's clients, you will work with a highly skilled, diverse network of people across Accenture businesses who are using the latest emerging technologies to address today's biggest business challenges.
You will receive competitive rewards and access to benefits programs and world-class learning resources.
Accenture Flex
employees work in their local metro area onsite at the project, significantly reducing and/or eliminating the demands to travel.
Key Responsibilities:
The Software Engineer that will focus on aligning Common Data Services with enterprise digital capabilities and moving those capabilities from mainframe IDB to an Oracle based Participant MDM in the cloud. This includes current state mainframe analysis, code dissection and documentation, identifying current state data input and retrieval capabilities, and working with peers to set strategic technical direction for our mainframe replacement and modernization efforts. We are looking for an innovative and outspoken technologist with a deep passion for data and application design to help migrate data and capabilities off of the mainframe and into the future. It's a great chance to work with innovation driven agile development teams to help build strategic and business value solutions around common data.
Basic Qualifications:
Minimum 5 years of experience with analysis, designing, and development in complex mainframe environments utilizing SQL and JCL technologies
Minimum 5 years of experience analyzing current mainframe capabilities, data flows, interconnections. and creating nonfunctional technical requirements based off of findings
Minimum 3 years of experience with cloud data migration, cloud infrastructure architecture, and Oracle cloud infrastructure
Minimum 3 years of experience with SRE (site reliability engineering) assessments
High School Diploma or GED
Preferred Qualifications:
An analytical and consulting mindset, being fearless in asking engaging questions to help tease out requirements details and to offer up technical implementation options across technical team.
A strong focus on stability and resiliency through the application of Site Reliability Engineering (SRE) principles
Experience with Azure Cloud data technologies
Familiarity with data pipeline and workflow management tools such as Kafka, data engineering
Strong knowledge of data warehousing concepts, ETL processes, and data architecture
A proven track record as an outspoken peer technical leader with peer coaching and strong communications and presentation skills working across collaborative teams to deliver high quality data solutions.
Compensation at Accenture varies depending on a wide array of factors, which may include but are not limited to the specific office location, role, skill set, and level of experience. As required by local law, Accenture provides a reasonable range of compensation for roles that may be hired in California, Colorado, New York, or Washington as set forth below.
Information on benefits is here.
Role Location
California – $62.98 - $72.98/hour
Colorado – $62.98 - $72.98/hour
New York – $62.98 - $72.98/hour
Washington –
$62.98 - $72.98/hour
What We Believe
We have an unwavering commitment to diversity with the aim that every one of our people has a full sense of belonging within our organization. As a business imperative, every person at Accenture has the responsibility to create and sustain an inclusive environment.
Inclusion and diversity are fundamental to our culture and core values. Our rich diversity makes us more innovative and more creative, which helps us better serve our clients and our communities. Read more here
Equal Employment Opportunity Statement
Accenture is an Equal Opportunity Employer. We believe that no one should be discriminated against because of their differences, such as age, disability, ethnicity, gender, gender identity and expression, religion or sexual orientation.
All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.
Accenture is committed to providing veteran employment opportunities to our service men and women.
For details, view a copy of the Accenture Equal Employment Opportunity and Affirmative Action Policy Statement.
Requesting An Accommodation
Accenture is committed to providing equal employment opportunities for persons with disabilities or religious observances, including reasonable accommodation when needed. If you are hired by Accenture and require accommodation to perform the essential functions of your role, you will be asked to participate in our reasonable accommodation process. Accommodations made to facilitate the recruiting process are not a guarantee of future or continued accommodations once hired.
If you would like to be considered for employment opportunities with Accenture and have accommodation needs for a disability or religious observance, please call us toll free at 1 (877) 889-9009, send us an email or speak with your recruiter.
Other Employment Statements
Applicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States.
Candidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.
Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.
The Company will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. Additionally, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the Company's legal duty to furnish information.
Show more
Show less","Mainframe programming, Java, Site Reliability Engineering (SRE), Analysis, Design, Development, Current state mainframe analysis, Code dissection, Documentation, Data input and retrieval capabilities, Strategic technical direction, Data migration, Cloud infrastructure architecture, Oracle cloud infrastructure, Kafka, Data engineering, Data warehousing, ETL processes, Data architecture, Peer technical leadership, Peer coaching, Communications, Presentation skills, Team Collaboration, High School Diploma or GED, SQL, JCL","mainframe programming, java, site reliability engineering sre, analysis, design, development, current state mainframe analysis, code dissection, documentation, data input and retrieval capabilities, strategic technical direction, data migration, cloud infrastructure architecture, oracle cloud infrastructure, kafka, data engineering, data warehousing, etl processes, data architecture, peer technical leadership, peer coaching, communications, presentation skills, team collaboration, high school diploma or ged, sql, jcl","analysis, cloud infrastructure architecture, code dissection, communications, current state mainframe analysis, data architecture, data engineering, data input and retrieval capabilities, data migration, datawarehouse, design, development, documentation, etl, high school diploma or ged, java, jcl, kafka, mainframe programming, oracle cloud infrastructure, peer coaching, peer technical leadership, presentation skills, site reliability engineering sre, sql, strategic technical direction, team collaboration"
Senior Data Analyst,IMG Academy,"Bradenton, FL",https://www.linkedin.com/jobs/view/senior-data-analyst-at-img-academy-3782538156,2023-12-17,Bradenton,United States,Mid senior,Onsite,"Education
IMG Academy is the world’s leading sports education brand, providing a holistic education model that empowers student-athletes to win their future, preparing them for college and for life. IMG Academy offers an innovative suite of on-campus and online experiences, providing growth opportunities for all student-athletes through:
Boarding school and camps, via a 600-acre state-of-the-art campus in Bradenton, Fla. ( IMGAcademy.com )
Online coaching via the IMG Academy+ brand, with a focus on personal development through the lens of sport and performance ( IMGAcademy.com/Plus )
Online college recruiting, via the NCSA brand, providing content, tools, coaching and access to a network of 40,000 college coaches ( ncsasports.org )
Our team has a deep appreciation for the transformative power of sports and holistic personal development. Our leadership is actively investing in the growth of the organization. We continue to broaden and deepen our technology platform and team in pursuit of our vision for empowering youth sports and the path-to-college for student-athletes.
Position Summary
The
S
enior Data Analyst
will be responsible for leading the design and implementation of analytics solutions as part of the Data T eam. The team provides data, analysis, insights and recommendations in support of the company's fiercely data-driven decision-making culture in sales, operational, financial and product domains. The Sr. Data Analyst is responsible for partnering cross-functionally across the organization to understand business goals, define questions and methods for answering them. The position makes use of multiple data sources, analysis tools and statistical frameworks to construct analytical outputs, reporting and dashboards in support of daily operations, new initiatives, and the development of proprietary predictive models. Working closely with the Data Engineering team and the Platform Development Manager for data infrastructure, the Sr. Data Analyst both makes use of and provides ongoing recommendations for improving the company’s platform components for data collection, aggregation, processing and storage. The position serves as an internal thought leader for decision-making around one or more major areas of business focus, serving as subject matter expert for senior management, operational leaders, and the Product and Technology teams.
Candidates should have experience leading analytics teams, working cross-functionally with business stakeholders and data engineering teams, sustaining mission-critical reporting and insights delivery. They should be very comfortable providing insights to business leaders using descriptive, diagnostic , and predictive analytics. They should demonstrate a rigorous scientific approach, using experimental methods to test hypotheses across multiple data sets using peer-reviewed statistical methods to ensure optimal business performance. Above all else , they should be a servant leader who understands their primary role to be supporting and enabling their team by maintaining a healthy, supportive culture.
Position Responsibilities
Help the Data T eam cultivate cross-functional partnerships across departments, bringing insights to every corner of our operation as part of a collaborative, collegial learning culture.
Support the career development of data analysts , and reporting specialists on team building and delivering Internet-scale analytics solutions.
Provide leadership and guidance to an individual analytics team and its members, offering thoughtful and timely coaching and effective feedback.
Partner with leadership in Marketing, Sales Operations, Sales, Operations, Product and Technology to solicit analytics business requirements and frame actionable questions.
Facilitate the acquisition and integration of data from multiple data sources, performing exploratory analysis to identify data relationships including trends, patterns and correlations; mature these analyses into more formalized frameworks and models.
Use data processing, querying and analysis tools to answer business questions and provide actionable insights.
Use visualization tools to communicate stories with data, creating reporting, dashboards, KPI monitoring and self-service tools to make data and insights available for stakeholders.
Partner with other data analysts and data engineers to identify and address limitations in data sources, data quality, tooling, or performance bottlenecks in analytics delivery.
Develop and maintain increasing awareness of the business context and dynamics surrounding each of the company’s business units, their products and services, the data they generate and the relationships between them.
Lead data analysts, data engineers and database administrators from the initial discovery and design stages of new data science initiatives all the way through implementation and operationalization, including the construction of required data structures, extracts and analytical data stores.
Seek opportunities to perform 'data pioneering' for identifying new opportunities for analysis and insight generation while maintaining focus on reuse and repurposing of frameworks across domains.
Knowledge, Skills And Abilities
Bachelor's or master's degree in a quantitative discipline (Mathematics or Social Science).
5+ years' data analytics experience in a commercial or scientific setting.
Experience supporting critical analytics delivery in a fast-paced environment.
Experience working across functional, product and engineering teams.
Experience using task management and collaboration software such as Jira or Trello.
Experience with distributed version control and software development lifecycle methods.
Exposure to Agile Development or the Scrum project management methodology a plus.
Experience and understanding of how modern analytics frameworks are constructed to define questions and answers using applied statistical methods.
Strong data process definition and refinement skills, with an instinctive understanding for identifying areas of opportunity or challenge and methods for providing a clear analysis.
Ability to construct models for forecasting and tracking business performance in support of sales, marketing, operations, product or financial domains.
Expertise in a data science programming language like R or Python.
Expertise composing complex SQL queries for relational databases.
Hands-on experience building custom visualizations using tools like Tableau or Power BI.
Experience with ELT and data integration tools like Fivetran or Stitch Data.
Experience with modern data warehouse tools like Snowflake, Big Query, or Redshift.
Experience with data modeling and schema definition for data lakes and warehouses.
Experience leading cross-functionally in situations requiring collaboration, negotiation, and communication of complex analyses to non-technical or senior management audiences.
Experience working as a team player with other data scientists and engineers; always looking to make things better and easier for co-workers.
Excellent visual, verbal, and textual communication and presentation skills; extremely strong skills for using data to tell stories, especially counter-intuitive or controversial ones.
Highly self-motivated, self-directed, resourceful, and autonomous; capable of managing multiple simultaneous deliverables with proactive status updates and requests for support.
Strong problem-solving skills with the ability to pinpoint and resolve problematic issues quickly and efficiently .
Background Requirements
Requires a background check upon offer
Requires a drug check upon offer
Don’t meet every single requirement? We are dedicated to building a diverse, inclusive, authentic workplace, so if you’re excited about this role but your past experience doesn’t align perfect with every qualification in the job description, we encourage you to apply anyway. You may be just the right candidate for this or other roles.
Get To Know Us Better
www.imgacademy.com
IMG Academy provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.
Apply Now
Show more
Show less","Quantitative Discipline, Data Analytics, Modeling, Forecasting, R, Python, SQL, Tableau, Power BI, Fivetran, Stitch Data, Snowflake, Big Query, Redshift, Data Lakes, Data Warehouses, Team Leadership, Collaboration, Presentation, ProblemSolving","quantitative discipline, data analytics, modeling, forecasting, r, python, sql, tableau, power bi, fivetran, stitch data, snowflake, big query, redshift, data lakes, data warehouses, team leadership, collaboration, presentation, problemsolving","big query, collaboration, data lakes, data warehouses, dataanalytics, fivetran, forecasting, modeling, powerbi, presentation, problemsolving, python, quantitative discipline, r, redshift, snowflake, sql, stitch data, tableau, team leadership"
Space Data Analyst,JLL,"West Lafayette, IN",https://www.linkedin.com/jobs/view/space-data-analyst-at-jll-3768836196,2023-12-17,Sheffield,United States,Associate,Onsite,"We are looking for an Space Data Manager, to
join our Occupancy Planning and Management team
.
About JLL -
We’re JLL - a leading professional services and investment management firm specializing in real estate. We shape the future of real estate for a better world by using the most advanced technology to create rewarding opportunities, amazing spaces and sustainable real estate solutions for our clients, our people, and our communities. As a Fortune 500 company, we help real estate owners, occupiers and investors achieve their business ambitions. We have operations in over 80 countries and a global workforce of over 92,000 individuals. We believe a diverse and inclusive culture is one where everyone succeeds. That’s why we’re committed to creating an environment where we all feel welcomed, valued, and empowered to achieve our full potential.
Confidence can sometimes hold us back from applying for a job. But we’ll let you in on a secret: there’s no such thing as a “perfect” candidate. JLL is a place where everyone can grow no matter how they identify or what background they bring with them. If you’re a leader of any level or experience and this job description resonates with you, let’s talk.
What this job involves:
As Space Data Manager, you will be responsible for delivering accurate data and drawings, and occupancy reporting, within an assigned client portfolio.
Tech-savvy
You have critical thinking skill sets and can easily tell a story through data. You will be successful on the job if you have skills in Microsoft Excel (pivot table, vlook-up, cell calculations), and understand floorplans. Proficiency in Microsoft Office Suite and the ability to consolidate data and floor plans into PowerPoint Presentations for clients will be highly valuable for the job.
Reporting
Major requirements for you will be to get familiar with space utilization methods and the presentation of utilization data. You’ll work closely with clients to reconcile discrepancies from top-down projections, as well as track and report actual seat demand at the group level.
Managing client portfolio drawings and data, providing critical data reporting to support strategic planning functions. This role also involves sharing critical data with other relevant technology solutions as appropriate.
Single point of contact for the CAFM/IWMS database, drawing, and application related support. You will drive consistent processes for submitting, reviewing, and approving data change requests on client file sharing platforms.
Lead recurring site data validation (space audits) program to ensure seat assignments and floor plans are
Drawing management support to maintain compliance with drawing data standards for AI Occupancy Planning system.
Comprehensive understanding of the relevant space data management systems. As the Subject Matter Expert for the Occupancy Planning & Management Platform, the Space Data Manager will interpret portfolio changes and assist with financial forecasts.
Key member for allocation/chargeback cycle(s), to include all related reporting requirements.
Key partner for move coordination and facility management teams assisting with move and space processes and implementation request as well as conduct facility audits.
Sound like you? Before you apply, it’s also worth knowing what we’re looking for
Bachelor’s degree and previous CAFM experience required.
Previous experience utilizing and extracting data from FM Systems, and/or comparable experience with other systems (i.e. Archibus, Serraview, Manhattan, Tririga or similar).
Previous experience in space/occupancy planning, highly desirable.
Proficient in Microsoft Excel to assist with organize data.
Inquisitive and curious by nature, asking questions to better understand the underlying needs of our clients.
Proactive and innovative to determine and develop the right solutions for your stakeholders.
Dynamic and technologically savvy, preferably with some level of AutoCAD skills and experiences.
Excellent communicator – both written and oral.
Independent, self-starter with the ability to prioritize multiple tasks and manage deadlines in an agile environment.
Strong interpersonal skills with an ability to interact with both external and internal clients working to develop and cultivate diverse and inclusive teams.
What you can expect from us:
At JLL, we make sure that you become the best version of yourself by helping you realize your full potential in an entrepreneurial and inclusive work environment.
Our Total Rewards program reflects our commitment to helping you achieve your career ambitions, recognizing your contributions, investing in your well-being, and providing competitive benefits and pay.
We can’t wait to see where your ambitions take you at JLL.
Apply today!
Show more
Show less","Microsoft Excel, Microsoft PowerPoint, Microsoft Office Suite, AutoCAD, Space utilization methods, CAFM/IWMS, Space audits, Portfolio change interpretation, Chargeback cycle reporting, Facility management, FM Systems, Archibus, Serraview, Manhattan, Tririga","microsoft excel, microsoft powerpoint, microsoft office suite, autocad, space utilization methods, cafmiwms, space audits, portfolio change interpretation, chargeback cycle reporting, facility management, fm systems, archibus, serraview, manhattan, tririga","archibus, autocad, cafmiwms, chargeback cycle reporting, facility management, fm systems, manhattan, microsoft excel, microsoft office suite, microsoft powerpoint, portfolio change interpretation, serraview, space audits, space utilization methods, tririga"
Senior Data/BI Engineer,Energy Jobline,"West Lafayette, IN",https://www.linkedin.com/jobs/view/senior-data-bi-engineer-at-energy-jobline-3773701730,2023-12-17,Sheffield,United States,Mid senior,Onsite,"Duties And Responsibilities
Design data solutions to meet business, technical and user requirements. This includes building modern data pipelines that meet that meet functional/non-functional business requirements and provide end to end data solutions.
Build reports and data visualizations, using data from the data warehouse and other sources. Produce scalable, replicable code and engineering solutions that help automate repetitive data management tasks.
Consolidate all Business Intelligence dashboards and datasets coming from different data sources such as Finance, Quality, Pursuit, Engineering, Supply Chain, and Sales Modules into a single data visualization platform.
Solve technical problems of the highest scope and complexity.
Propose changes and improvements to the systems or tools.
Design and implement automation solutions.
Report customization
Any other minor customization
Deployment of customization and testing
Troubleshoot configuration issues: Connect, Bulk processing, Reports.
Interactions
PSS Americas and other world area executive management
Qualifications
Minimum Educational Qualifications: Systems Engineering Degree or related.
10 years of relevant work experience in Data Management & Data Analytics.
Knowledgeable and Expertise in Python, Power apps, Java, C#, Aspx, DAX, Visual Basic, HTML, ASP.NET, SSRS and CSS.
Strong analytical and data visualization skills. Proficiency in SQL, excel MACRO, BI tools (power bi, etc.), and data warehouse.
Expertise in database systems, distributed computing systems, and big data technologies.
Troubleshooting skills: Analyzing database error logs.
Basics of server administration and network understanding.
Show more
Show less","Data solutions, Data pipelines, Reports, Data visualizations, Automation solutions, Customization, Deployment, Troubleshooting, Python, Power apps, Java, C#, ASP.NET, SSRS, CSS, SQL, Excel MACRO, Power BI, Database systems, Distributed computing systems, Big data technologies, Server administration, Networking","data solutions, data pipelines, reports, data visualizations, automation solutions, customization, deployment, troubleshooting, python, power apps, java, c, aspnet, ssrs, css, sql, excel macro, power bi, database systems, distributed computing systems, big data technologies, server administration, networking","aspnet, automation solutions, big data technologies, c, css, customization, data solutions, data visualizations, database systems, datapipeline, deployment, distributed computing systems, excel macro, java, networking, power apps, powerbi, python, reports, server administration, sql, ssrs, troubleshooting"
Senior Data Engineer IRC203967,Energy Jobline,"West Lafayette, IN",https://www.linkedin.com/jobs/view/senior-data-engineer-irc203967-at-energy-jobline-3773714317,2023-12-17,Sheffield,United States,Mid senior,Onsite,"Description
We are GlobalLogic, a Hitachi Group Company.
Based across 19 countries, GlobalLogic is a leader in digital engineering. We help brands across the globe design and build innovative products, platforms, and digital experiences for the modern world.
By integrating experience design, complex engineering, and data expertise - we help our clients imagine what's possible and accelerate their transition into tomorrow's digital businesses. At GlobalLogic, we have a phenomenal track record of successfully delivering ground-breaking Cloud & Digital Transformation programmes and we have an international reach working with a wide variety of projects and customers. Our approach enables companies ""build the exceptional"" and be fit for purpose in the 21st century.
We hire based on personality, potential, and enthusiasm to make a difference, then we give you the tools and skills you need to follow your own path. You'll benefit by gaining exposure to a wide range of tools and technologies that you can then put into practice and become certified on various Cloud (and related) technologies that will help you to develop your own toolkit.
Our client is the world's leading global information services company, unlocking the power of data to create more opportunities for consumers, businesses, and society. FORTUNE named them in one of the 100 Best Companies to work for. In addition, for the last five years they've been name in the 100 ""World's Most Innovative Companies"" by Forbes Magazine.
As a Senior Data Engineer you'll use your skills to transform data as well as developing data pipelines and visualisation solutions using industry recognised common tooling and expertise.
Requirements
DBT core experience is must
Container Images: Docker
Container Registry: TBD
Compute Orchestrator: Kubernetes (Managed version EKS)
Data Pipeline Orchestrator: Airflow | Kestra
Data Transformation Frameworks: Spark & DBT
Programing Language (Data Pipeline): Scala & sbt
Configuration Management: AWS App Config | AWS Parameter Store
Code Repository: Github / git
Object Storage: S3
Table Metastore: AWS Glue
Open Table Format: Iceberg
DBT Compute Engine: AWS Athena (possibility of also covering Snowflake)
Visualization: Looker | Tableau
Job Responsibilities
Development of a File Ingestion Framework (Spark/Scala)
Read files from object storage (landing zone).
Convert file to open table format Iceberg.
Perform some column label categorization.
Schema Inference, pushed to central repository and data services.
Development of Transformation Application (Spark/Scala)
Read Iceberg tables (raw/refined zone).
Translate legacy application code into next-gen tech stack (see above).
Write datasets back into proper lake zone.
Development of DBT data models/macros, setup standards of development for other less-technical teams to continue expanding the code-library following proper procedures.
Work collaboratively with Software Engineers around the Orchestration component of our applications.
Development of Export Framework
Easily export files from our lake into agreed upon channels
IAM Controls.
What We Offer
Why work at GlobalLogic
Our goal is to build an inclusive positive culture where everyone can feel comfortable being themselves, empowering our people to create their own high standards and therefore more value. We work together to promote fairness while recognising, valuing and embracing differences - providing a transparent support structure and generous training budget to help our people develop skills to progress their career. Our region also supports a hybrid model which can flex across a wide spectrum of working options determined by our business, customer and individual needs.
You'll benefit from a comprehensive health and wellness plan, private healthcare (clinical and mental wellbeing), and discounted gym memberships. We offer a fantastic benefits package including a competitive pension scheme and recognition schemes through bonus/reward initiatives. Colleagues are entitled to an annual volunteering day - so you can take time to support a cause close to your heart. We also love to stay social at our trips to the zoo, quiz nights, sports events, theatre trips and much more.
We are an equal opportunities employer. It is our policy to promote an environment free from discrimination, harassment and victimisation. We are proud signatories to the Tech Talent Charter which was created because solving the diversity problem in tech requires a collective effort across companies and sectors - https://www.techtalentcharter.co.uk/.
About GlobalLogic
GlobalLogic is a leader in digital engineering. We help brands across the globe design and build innovative products, platforms, and digital experiences for the modern world.
By integrating experience design, complex engineering, and data expertise-we help our clients imagine what's possible, and accelerate their transition into tomorrow's digital businesses.
Headquartered in Silicon Valley, GlobalLogic operates design studios and engineering centers around the world, extending our deep expertise to customers in the automotive, communications, financial services, healthcare and life sciences, manufacturing, media and entertainment, semiconductor, and technology industries.
GlobalLogic is a Hitachi Group Company operating under Hitachi, Ltd. (TSE: 6501) which contributes to a sustainable society with a higher quality of life by driving innovation through data and technology as the Social Innovation Business.
Show more
Show less","DBT, Docker, Kubernetes, Airflow, Kestra, Spark, Scala, sbt, AWS App Config, AWS Parameter Store, GitHub, git, S3, AWS Glue, Iceberg, AWS Athena, Snowflake, Looker, Tableau, Spark/Scala, Apache Iceberg, AWS Lake Formation, IAM Controls","dbt, docker, kubernetes, airflow, kestra, spark, scala, sbt, aws app config, aws parameter store, github, git, s3, aws glue, iceberg, aws athena, snowflake, looker, tableau, sparkscala, apache iceberg, aws lake formation, iam controls","airflow, apache iceberg, aws app config, aws athena, aws glue, aws lake formation, aws parameter store, dbt, docker, git, github, iam controls, iceberg, kestra, kubernetes, looker, s3, sbt, scala, snowflake, spark, sparkscala, tableau"
Staff Data Engineer,Recruiting from Scratch,"West Lafayette, IN",https://www.linkedin.com/jobs/view/staff-data-engineer-at-recruiting-from-scratch-3744388858,2023-12-17,Sheffield,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our client is a dating app.
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, deliver analysis to further improve engagement in existing features, and empower our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Solve technical problems of the highest scope and complexity
Exert significant influence on the company’s analytical long-range goals and data architecture
Define and extend our internal standards for style, maintenance, and best practices for a high-scale data platform
Provide mentorship for all on your team to help them grow in their technical responsibilities
Propose ideas to improve the scale, performance, and capabilities of the Data Platform
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We'll Love About You
7+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field
7+ years experience using Python
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention
Location
: This role will be located in office twice a week minimum in Palo Alto, San Francisco or Chicago.
Salary Range: $180,000 - 250,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, SQL, Data engineering, Business intelligence, Data science, TDD, Pair programming, Continuous integration, Automated testing, Deployment, Streamprocessing systems, Kafka, Storm, SparkStreaming, Dimensional data modeling, Schema design, Data warehouses, ETL, Data management tools, Data classification, Data retention","python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, data engineering, business intelligence, data science, tdd, pair programming, continuous integration, automated testing, deployment, streamprocessing systems, kafka, storm, sparkstreaming, dimensional data modeling, schema design, data warehouses, etl, data management tools, data classification, data retention","airflow, automated testing, business intelligence, continuous integration, data classification, data engineering, data management tools, data retention, data science, data warehouses, deployment, dimensional data modeling, docker, etl, helm, kafka, kubernetes, pair programming, python, schema design, snowflake, spark, sparkstreaming, sql, storm, streamprocessing systems, tdd"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"West Lafayette, IN",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3744387832,2023-12-17,Sheffield,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
They are a profitable dating application.
Location: This is a hybrid role based in their Chicago office and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location
: This role is twice a week in a hybrid role minimum.
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data engineering, Data science, Python 7+ years, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, SQL, Kafka, Storm, SparkStreaming, Dimensional data modeling, Schema design, ETL, Data management tools, Data classification, Data retention","data engineering, data science, python 7 years, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, kafka, storm, sparkstreaming, dimensional data modeling, schema design, etl, data management tools, data classification, data retention","airflow, data classification, data engineering, data management tools, data retention, data science, dimensional data modeling, docker, etl, helm, kafka, kubernetes, python 7 years, schema design, snowflake, spark, sparkstreaming, sql, storm"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"West Lafayette, IN",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3748830437,2023-12-17,Sheffield,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our next evolution is underway as a newly public company looking to expand and continue to build meaningful experiences for our users. From social issues to original content, we’re blazing innovative paths with impact for our community, all while leveraging the latest tech stacks and striving for engineering excellence. At the heart of our work in this new chapter is a shared set of core values: openness and exploration, a bias for action, and strong support of the LGBTQ community.
With a track record of strong financial performance and plans for continued headcount growth, we’re looking to build a team of talented, passionate, and open-minded people who believe in our mission, align with our values, and are excited to work at the intersection of innovative technology and social impact. Come be a part of this exciting journey with us.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location: Palo Alto, San Francisco or Chicago
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Realtime Streaming Technologies, TDD (Testdriven Development), Automation, Continuous Delivery, Data Products, Data Governance, Security, Scalability, Big Data Architectures, ETL (Extract Transform Load), S3 (Scalable Cloud Storage), Snowflake (Cloudbased Data Warehouse), Kafka (Distributed Streaming Platform), Spark (Big Data Computing Platform), Python, SQL (Structured Query Language), Airflow (Orchestration and Scheduling Platform), Kubernetes (Container Orchestration Platform), Docker (Containerization Platform), Helm (Kubernetes Package Manager), pySpark (Python API for Apache Spark), Agile Engineering Practices, Streamprocessing Systems, Dimensional Data Modeling, Schema Design, Data Warehouses, ETL Pipelines, Legal Compliance, Data Management Tools, Data Classification, Data Retention","data engineering, realtime streaming technologies, tdd testdriven development, automation, continuous delivery, data products, data governance, security, scalability, big data architectures, etl extract transform load, s3 scalable cloud storage, snowflake cloudbased data warehouse, kafka distributed streaming platform, spark big data computing platform, python, sql structured query language, airflow orchestration and scheduling platform, kubernetes container orchestration platform, docker containerization platform, helm kubernetes package manager, pyspark python api for apache spark, agile engineering practices, streamprocessing systems, dimensional data modeling, schema design, data warehouses, etl pipelines, legal compliance, data management tools, data classification, data retention","agile engineering practices, airflow orchestration and scheduling platform, automation, big data architectures, continuous delivery, data classification, data engineering, data governance, data management tools, data products, data retention, data warehouses, dimensional data modeling, docker containerization platform, etl extract transform load, etl pipelines, helm kubernetes package manager, kafka distributed streaming platform, kubernetes container orchestration platform, legal compliance, pyspark python api for apache spark, python, realtime streaming technologies, s3 scalable cloud storage, scalability, schema design, security, snowflake cloudbased data warehouse, spark big data computing platform, sql structured query language, streamprocessing systems, tdd testdriven development"
Senior Data Engineer IRC203099,Energy Jobline,"West Lafayette, IN",https://www.linkedin.com/jobs/view/senior-data-engineer-irc203099-at-energy-jobline-3773725660,2023-12-17,Sheffield,United States,Mid senior,Remote,"Description
Experian is the world's leading global information services company, unlocking the power of
data to create more opportunities for consumers, businesses, and society. We are thrilled to
share that FORTUNE has named Experian one of the 100 Best Companies to work for. In
addition, for the last five years we've been name in the 100 ""World's Most Innovative
Companies"" by Forbes Magazine.
Wage: From 3400€ gross monthly. Final salary will reflect your skills and experience
Requirements
Cloud Ecosystem: AWS
IaC Tool: Terraform
Security: AWS IAM / Policy Management
Network: AWS VPC
Container Images: Docker
Container Registry: TBD
Compute Orchestrator: Kubernetes (Managed version EKS)
Data Pipeline Orchestrator: Airflow | Kestra
Data Transformation Frameworks: Spark & DBT
Programing Language (Microservices): Java/Scala
Programing Language (Data Pipeline): Scala & sbt
Configuration Management: AWS App Config | AWS Parameter Store
Code Repository: Github / git
Object Storage: S3
Table Metastore: AWS Glue
Open Table Format: Iceberg
DBT Compute Engine: AWS Athena (possibility of also covering Snowflake)
Visualization: Looker | Tableau
Job Responsibilities
We look for individuals who are motivated by complex and challenging work. We want to work
with people who share compelling solutions to those challenges, solutions informed by their
unique experiences, passions, and expertise. We plan to build a Data Platform on top of AWS
Ecosystem to port-over some of EMS Legacy applications. We plan to use Scala and Java to
build microservices that will provide self-onboarding functionality to our customer and internal
users. We, furthermore, plan to build data pipelines on Spark (Scala) and other open-source
technologies with the goal of having robust/scalable and resilient data services. The
application/pipelines and microservices we build will run on top of managed Kubernetes.
We're open-minded about new technologies, we're passionate about what we do, and we
make time for everyone to learn and grow as the industry changes. Engineers on the team are
approachable and ambitious people who think outside the box and together solve big
problems.
What We Offer
Empowering Projects:
With 500+ clients spanning diverse industries and domains, we provide an exciting opportunity to contribute to groundbreaking projects that leverage cutting-edge technologies. As a team, we engineer digital products that positively impact people's lives.
Empowering Growth:
We foster a culture of continuous learning and professional development. Our dedication is to provide timely and comprehensive assistance for every consultant through our dedicated Learning & Development team, ensuring their continuous growth and success.
DE&I Matters:
At GlobalLogic, we deeply value and embrace
diversity
. We are dedicated to providing
equal
opportunities for all individuals, fostering an
inclusive
and empowering work environment.
Career Development:
Our corporate culture places a strong emphasis on career development, offering abundant opportunities for growth. Regular interactions with our teams ensure their engagement, motivation, and recognition. We empower our team members to pursue their career goals with confidence and enthusiasm.
Comprehensive Benefits:
In addition to equitable compensation, we provide a comprehensive benefits package that prioritizes the overall well-being of our consultants. We genuinely care about their health and strive to create a positive work environment.
Flexible Opportunities:
At GlobalLogic, we prioritize work-life balance by offering flexible opportunities tailored to your lifestyle. Explore relocation and rotation options for diverse cultural and professional experiences in different countries with our company.
About GlobalLogic
GlobalLogic is a leader in digital engineering. We help brands across the globe design and build innovative products, platforms, and digital experiences for the modern world.
By integrating experience design, complex engineering, and data expertise-we help our clients imagine what's possible, and accelerate their transition into tomorrow's digital businesses.
Headquartered in Silicon Valley, GlobalLogic operates design studios and engineering centers around the world, extending our deep expertise to customers in the automotive, communications, financial services, healthcare and life sciences, manufacturing, media and entertainment, semiconductor, and technology industries.
GlobalLogic is a Hitachi Group Company operating under Hitachi, Ltd. (TSE: 6501) which contributes to a sustainable society with a higher quality of life by driving innovation through data and technology as the Social Innovation Business.
Show more
Show less","Cloud Ecosystem, AWS, Terraform, AWS IAM, AWS VPC, Docker, Kubernetes (EKS), Airflow, Kestra, Spark, DBT, Java, Scala, sbt, AWS App Config, AWS Parameter Store, Github, Git, S3, AWS Glue, Iceberg, AWS Athena, Snowflake, Looker, Tableau","cloud ecosystem, aws, terraform, aws iam, aws vpc, docker, kubernetes eks, airflow, kestra, spark, dbt, java, scala, sbt, aws app config, aws parameter store, github, git, s3, aws glue, iceberg, aws athena, snowflake, looker, tableau","airflow, aws, aws app config, aws athena, aws glue, aws iam, aws parameter store, aws vpc, cloud ecosystem, dbt, docker, git, github, iceberg, java, kestra, kubernetes eks, looker, s3, sbt, scala, snowflake, spark, tableau, terraform"
Senior Staff AI Data Engineer,Recruiting from Scratch,"West Lafayette, IN",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3773090507,2023-12-17,Sheffield,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto
or
San
Francisco
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Airflow, AWS, Azure, Bash, Big data, Data engineering, Data pipelines, Data science, Data visualization, Docker, ETL, GCP, Git, Java, Kubernetes, KubeFlow, Machine learning, NLP, Pandas, Python, R, SQL, Snowflake, Spark, Statistics","airflow, aws, azure, bash, big data, data engineering, data pipelines, data science, data visualization, docker, etl, gcp, git, java, kubernetes, kubeflow, machine learning, nlp, pandas, python, r, sql, snowflake, spark, statistics","airflow, aws, azure, bash, big data, data engineering, data science, datapipeline, docker, etl, gcp, git, java, kubeflow, kubernetes, machine learning, nlp, pandas, python, r, snowflake, spark, sql, statistics, visualization"
Senior Staff AI Data Engineer,Recruiting from Scratch,"West Lafayette, IN",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3759711059,2023-12-17,Sheffield,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto,
San
Francisco or Chicago
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Apache Airflow, Git, Kubernetes, Kafka, Docker, Apache Spark, PySpark, Hadoop, Machine Learning, Natural Language Processing, Big Data, Data Lakes, Data Warehouses, Data Mining, Data Modeling, Data Pipelines, Data Preprocessing, Data Postprocessing, Data Quality, Data Governance, Statistical Analysis, Data Visualization, R, Pandas, Python, Java, Bash, SQL, NoSQL, DynamoDB, Snowflake, AWS, GCP, Azure, Relational Databases, ETL, Data Engineering","apache airflow, git, kubernetes, kafka, docker, apache spark, pyspark, hadoop, machine learning, natural language processing, big data, data lakes, data warehouses, data mining, data modeling, data pipelines, data preprocessing, data postprocessing, data quality, data governance, statistical analysis, data visualization, r, pandas, python, java, bash, sql, nosql, dynamodb, snowflake, aws, gcp, azure, relational databases, etl, data engineering","apache airflow, apache spark, aws, azure, bash, big data, data engineering, data governance, data lakes, data mining, data postprocessing, data preprocessing, data quality, data warehouses, datamodeling, datapipeline, docker, dynamodb, etl, gcp, git, hadoop, java, kafka, kubernetes, machine learning, natural language processing, nosql, pandas, python, r, relational databases, snowflake, spark, sql, statistical analysis, visualization"
Data Center Engineer,"Integrated Resources, Inc ( IRI )","Grand Rapids, MI",https://www.linkedin.com/jobs/view/data-center-engineer-at-integrated-resources-inc-iri-3770496018,2023-12-17,Michigan,United States,Associate,Onsite,"Position: Accountant
Location: Carson, CA
Duration: 6 Months+
JOB DESCRIPTION:
Accountant I with 2-4 years of experience* in the field or in a related area. Familiar with standard concepts, practices, and procedures within a particular field.
Relies on judgment and limited experience to plan and accomplish goals.
Performs a variety of tasks.
Works under general supervision; typically reports to a supervisor or manager.
A certain degree of creativity and attitude is required.
Responsibility:
Reconciliation of daily meter and tank gauging tickets
Rack Customer allocation
Customer reporting on allowed volumes and throughputs
Complies with confidentiality of the data per regulatory requirements to protect financial data. o Report monthly terminal volumes and throughputs.
Monitor Transmix generation.
Reconciliation of daily tank movements with financial control team in Manila.
Reconcile tank to tank movements.
Get approvals for daily folios that are outside the limits.
Reconcile tickets with Houston Control Center logs on daily basis.
Entering Synthesis tickets and book movement in TMS on daily basis.
Resolve custody transfer issues related to meter by coordinating with measurement team.
Support financial controls audit.
Part of document controls, all documents must be uploaded in Hexagon per the guidelines. Identify opportunities for efficiency, cost savings and continuous Improvement of workflow
Show more
Show less","Accounting, Customer reporting, Data confidentiality, Financial data protection, Hexagon, Measurement, Synthesis tickets, Transmix generation","accounting, customer reporting, data confidentiality, financial data protection, hexagon, measurement, synthesis tickets, transmix generation","accounting, customer reporting, data confidentiality, financial data protection, hexagon, measurement, synthesis tickets, transmix generation"
BI Data Analyst,"Amick Brown - SAP, Cloud Technologies and Business Intelligence Staffing & Consulting","Michigan, United States",https://www.linkedin.com/jobs/view/bi-data-analyst-at-amick-brown-sap-cloud-technologies-and-business-intelligence-staffing-consulting-3715672718,2023-12-17,Michigan,United States,Associate,Onsite,"Amick Brown is seeking for an experienced
BI Data Analyst
for our direct client.
Location
: Remote
Duration
: 1 year
Estimated pay range
: $40 - 50 per hour per client contract and candidate skills, experience and work location.
Job Description
The BI Data Analyst will be a member of the Quality data analytics team and use technical expertise and a solid understanding of business processes and needs to turn data into useful information.
In this role, the analyst will work closely with various business teams to understand, collect, clarify, and translate business data requirements into reports and analytics as required.
The analyst will be heavily involved with the use, distribution, and successful delivery of reports on a periodic basis.
The ideal candidate would have medical device post market experience and have the ability to explore and integrate different data sources using tools such as SQL and Tableau.
Responsibilities
Partner with various Quality Management System (QMS) business teams to support data needs
Translate non-technical data requirements into metrics, reports, and visualization dashboards
Partner with business teams to define and document business requirements for new metrics and reports
Ability to execute deep dives into available data sets to provide insight on product quality performance
Develop user documentation and a variety of visual representation of data analysis and ensures consistency and accuracy of data and analysis provided
Support business system integrations for data needs and report outputs
Perform ongoing monitoring and refinement of reports and analytical solutions to ensure a good user experience in support reports and dashboards
Partner with architecture and solution delivery teams to enable reports that have long-term value
Perform functional, regression, usability test, etc. as applicable for both new and existing datasets, reports and dashboards. Document and summarize test results and findings as required
Skill/Job Requirements
Education: Bachelor's or higher degree in Statistics, Mathematics, Computer and Information Science or equivalent blend of work and certification experience required
Minimum 5 years of data analytics experience, with 3+ years hands on using Tableau or another BI Tool required
Minimum 3+ years of experience with Microsoft SQL or equivalent required
Programming skills and experience, especially for data exploration and presentation to management and other users of data required. Familiarity with Python preferred.
Highly organized, self-starter and highly motivated with strong prioritization skills managing multiple projects required
Creative problem solver that is able to look at an issue from a variety of angles required
Ability to work in fast-paced teams with a great work ethic and an enthusiasm for problem solving and data analysis required
Working experience in medical device, pharmaceutical, regulatory or other related industries highly preferred
Experience and capability using visualization software such as Tableau/ Power BI, and familiarity with SAP, CRM or other ERP system preferred
Knowledge of the business of medical device development and manufacture preferred
Excellent writing skills and documentation skills that enable analyses to be reliably reproduced using the same data years later preferred
We are an AA/EEO/Veterans/Disabled employer.
We will consider for employment qualified applicants with arrest and conviction records in accordance with fair chance laws.
Amick Brown is an Information Technology consulting company specializing in ERP, Data Analytics, Information Security, Application Development, Networking, and Cloud Computing. The company was founded in 2010 and is headquartered in San Ramon, California.
Regular full-time employees are eligible for the following Amick Brown provided benefits:
Health
Vision
Dental
401k with company match
Paid time off.
Sick Leave
Long -Term Disability
Life Insurance
Wellness & Discount Programs
Show more
Show less","BI Tools, Tableau, Power BI, SAP, ERP, Data Analytics, SQL, Python, Programming, Mathematics, Statistics, Tableau, SQL, Python, Medical Device Development, Pharmaceutical, Regulatory","bi tools, tableau, power bi, sap, erp, data analytics, sql, python, programming, mathematics, statistics, tableau, sql, python, medical device development, pharmaceutical, regulatory","bi tools, dataanalytics, erp, mathematics, medical device development, pharmaceutical, powerbi, programming, python, regulatory, sap, sql, statistics, tableau"
"Voice/Data Communications Engineer/Design Specialist  Lansing, MI  Only Locals",Software People Inc.,"Lansing, MI",https://www.linkedin.com/jobs/view/voice-data-communications-engineer-design-specialist-lansing-mi-only-locals-at-software-people-inc-3759913538,2023-12-17,Michigan,United States,Associate,Hybrid,"Phone/Skype Hire. Onsite from day 1 / Hybrid
Location: Lansing, MI
Duration: 12+ months
Provide administrative, technical, design, programming, and engineering support for multiple types of IP Telephony (IPT) Systems, Voicemail/Call handler systems, IP contact center systems, local & long-distance call connectivity, and video conferencing systems. Platforms also include of a diverse mixture of Cisco routers & switches, and a wide variety of adjunct servers. Employee is expected to have in-depth knowledge of NTSD supported service offerings and to mentor peer or entry level team members. Additionally, employee will analyze and resolve tier 2 operational problems and identify system and operational efficiencies.
Responsibilities
Duty 1
General Summary of Duty 1 % of Time 70
Configure, deliver, maintain, and support any of the cataloged Telephony services offerings.
Individual tasks related to the duty.
Access the Remedy Change Request system to obtain a list of current incident tickets, project tasks, and C906 requests and process according to established SLEs.
Analyzes and resolves problems to identify network hardware system and operational efficiencies and failure patterns.
Utilizes Standard Operating Procedures (SOPs) to perform routine tasks.
Duty 2
General Summary of Duty 2 % of Time 15
Provide administrative support for all related unified communications platforms and associated tools. This includes Cisco as well as third party support tools.
Individual tasks related to the duty.
Validates and finalizes Local Change Board (LCAB) and Enterprise Change Board documents.
Perform maintenance and upgrade systems as a result of approved systems design or request for change (RFC).
Reviews and approves updates to systems documentation.
Evaluate and provide recommended changes to Design documents and requests.
Performs implementation and maintenance of network systems hardware and operating systems.
Utilizes Standard Operating Procedures (SOPs) to perform routine tasks.
Perform other related assignments as directed my management.
Duty 3
General Summary of Duty 3 % of Time 10
Projects as assigned by management
Individual tasks related to the duty.
As IT Service Requests are submitted by Agencies, the Analyst 11 may be asked to bulk build new Cisco phones or assist in large configuration changes.
Utilizes Standard Operating Procedures (SOPs) to perform routine tasks.
Other projects as assigned by management.
Duty 4
General Summary of Duty 4 % of Time 5
Reviews, creates, and maintains Standard Operating Procedures (SOPs).
Individual tasks related to the duty.
Reviews SOPs to assure accuracy and compliance.
Prepare SOPs as new products and services are added.
Knowledge, Skills, And Abilities
Must be familiar with Cisco Systems IOS, Cisco IPT, Session Manager, Session Border Controllers, and have solid knowledge of enterprise networking and telephony principals including various types of technologies and connectivity types to the Public Switched Telephone Network. Knowledge must be specific to telephony networks and related components. Ability to highly perform job duties on a consistent basis and engage in knowledge transfer of standard, troubleshooting techniques, division work practices/standards and technical skills to apprentice level technicians.
Certifications from Cisco and Microsoft are highly desirable.
Skills Needed
Cisco Unified Call Manager 1.5+ years
Cisco Unity Connections 1.5+ years
Cisco ATA and VGs 1.5+ years
Cisco UCCX
Cisco Emergency Responder
RedSky
Cisco ISR Routers
Ribbon SBC
Akkadian Labs
Remedy
Informacast
CapTel
ITIL or CMMI
Show more
Show less","Cisco Systems IOS, Cisco IPT, Session Manager, Session Border Controllers, Enterprise networking, Telephony principals, Telephony networks, Cisco Unified Call Manager, Cisco Unity Connections, Cisco ATA and VGs, Cisco UCCX, Cisco Emergency Responder, RedSky, Cisco ISR Routers, Ribbon SBC, Akkadian Labs, Remedy, Informacast, CapTel, ITIL, CMMI","cisco systems ios, cisco ipt, session manager, session border controllers, enterprise networking, telephony principals, telephony networks, cisco unified call manager, cisco unity connections, cisco ata and vgs, cisco uccx, cisco emergency responder, redsky, cisco isr routers, ribbon sbc, akkadian labs, remedy, informacast, captel, itil, cmmi","akkadian labs, captel, cisco ata and vgs, cisco emergency responder, cisco ipt, cisco isr routers, cisco systems ios, cisco uccx, cisco unified call manager, cisco unity connections, cmmi, enterprise networking, informacast, itil, redsky, remedy, ribbon sbc, session border controllers, session manager, telephony networks, telephony principals"
Data Integrity Analyst - Hybrid,Stryker,"Portage, MI",https://www.linkedin.com/jobs/view/data-integrity-analyst-hybrid-at-stryker-3771372939,2023-12-17,Michigan,United States,Associate,Hybrid,"Why join Stryker?
We are proud to be named one of the World’s Best Workplaces and a Best Workplace for Diversity by Fortune Magazine! Learn more about our award-winning organization by visiting stryker.com
Who We Want
Data translators.
Highly effective communicators who can transform data findings into recommendations to compose reports and executive level presentations.
Strategic thinkers.
People who enjoy analyzing data or trends for the purposes of planning, forecasting, advising, budgeting, reporting, or sales opportunities.
Collaborative partners.
People who build and leverage cross-functional relationships to bring together ideas, information, use cases, and industry analyses to develop best practices.
What You Will Do
We are looking for a highly motivated and experienced Senior PMO Analyst to join our team. The ideal candidate will have a strong understanding of project management principles and practices, as well as experience with data, analytics, and reporting.
As the Senior PMO Analyst, you will work with cross functional teams as you drive data integrity as you enter, review, and analyze data in the resource capacity platform. In this role, you will be responsible for supporting and maintaining the resource capacity platform associated with data collection, retrieval, accessibility, and usage for internal resource planning and activities. You will play a crucial role in ensuring data integrity, generating reports, and providing training to users. Additionally, you will have the opportunity to recommend changes in application development, maintenance, and system standards.
Other daily activities include but are not limited to:
Support and maintain resource capacity platform/tool for data collection and usage.
Partner with cross-functional teams SME’s on entering and ensuring clean and consistent data inputs into established tables, fields, and system databases.
Acts as the PMO point of contact for cross-functional users for all platform questions, request, and modifications.
Responsible for processing and validating platform changes in accordance with platform guidance and governance.
Partner with Lead PMO Analyst triage issues and determine if root cause is process, people, or system.
Set up user access and provide training on platform usage.
Create and maintain seamless end-user experience for all reporting needs.
Partner with Lead PMO Analyst to drive visibility, understanding, trust, and improvements within the resource capacity platform and reports.
Partner with cross-functional users to identify and resolve data gaps and issues.
Partner with the Lead PMO Analyst to ensure business readiness for minor and major releases and updates.
Build and produce reports using query and flexible reporting tools to meet the requirements of cross-functional users and resource managers.
Investigate reporting issues and provide expertise to resolve and follow-up in a timely manner.
Identify opportunities to gain efficiencies, automate, and improve data quality.
In addition to the activities listed above, the Senior PMO Analyst must be able to build strong relationships with stakeholders, communicate effectively with stakeholders, stay calm under pressure, think critically and make sound decisions.
The ideal candidate will also be highly motivated and experienced, passionate about data, analytics, project portfolio management software, and project management, able to work independently with minimal supervision, be able to understand and apply procedures and concepts of their discipline, pay attention to detail in making evaluative judgments based on the analysis of factual information.
What You Will Need
Bachelor's Degree is required
Minimum of 2 years of professional experience
Strong analytical and problem-solving skills.
Experience with data analysis tools such as Excel, PowerBI and Python.
Ability to communicate data insights to stakeholders.
Contribute to the development of the resource capacity platform, recommend improvements through data and reporting.
Assist in the development of the PMO tools and reports. Alert Lead PMO Analyst of variances or gaps.
Complete assigned tasks according to established timeline. Assist in the development of the resource capacity platform rollout and training timelines.
Participate in the development of PMO tools that support data quality.
Partner in the establishment of risk management plans and support effective risk analysis and risk mitigation planning for the PMO.
Provide support on rescheduling, and resource allocation in the program plan and support program gating processes.
Establish and maintain partnerships with internal and external PMO stakeholders.
Produce weekly and monthly PMO metric reports using dashboards and metrics to measure success.
Contributes data analysis expertise for short- and long-term strategic planning process activities.
Leads projects. May support a high complexity program.
Builds productive working relationships with departments that are key partners (e.g., R&D and Marketing).
About Stryker
Our benefits:
12 paid holidays annually
Health benefits include: Medical and prescription drug insurance, dental insurance, vision insurance, critical illness insurance, accident insurance, hospital indemnity insurance, personalized healthcare support, wellbeing program and tobacco cessation program.
Financial benefits include Health Savings Account (HSA), Flexible Spending Accounts (FSAs), 401(k) plan, Employee Stock Purchase Plan (ESPP), basic life and AD&D insurance, and short-term disability insurance.
For a more detailed overview of our benefits or time off, please follow this link to learn more: US Stryker employee benefits
About Stryker
Stryker is one of the world’s leading medical technology companies and, together with its customers, is driven to make healthcare better. The company offers innovative products and services in Medical and Surgical, Neurotechnology, Orthopaedics and Spine that help improve patient and healthcare outcomes. Alongside its customers around the world, Stryker impacts more than 130 million patients annually. More information is available at stryker.com.
Know someone at Stryker?
Be sure to have them submit you as a referral prior to applying for this position. Learn more about our employee referral program on our referral page
Stryker is driven to work together with our customers to make healthcare better. Employees and new hires in sales and field roles that require access to customer accounts as a function of the job may be required, depending on customer requirements, to obtain various vaccinations as an essential function of their role.
R503899_4
Show more
Show less","Project Management, Data Analysis, Analytics, Reporting, Data Integrity, Data Collection, Data Retrieval, Accessibility, Usage, Internal Resource Planning, Activities, System Standards, Query, Flexible Reporting Tools, CrossFunctional Teams, Communication, Critical Thinking, Decision Making, Excel, PowerBI, Python, Risk Management, Risk Analysis, Risk Mitigation Planning, Dashboards, Metrics, Strategic Planning, Project Leadership, Partnerships, Healthcare, Medical Technology","project management, data analysis, analytics, reporting, data integrity, data collection, data retrieval, accessibility, usage, internal resource planning, activities, system standards, query, flexible reporting tools, crossfunctional teams, communication, critical thinking, decision making, excel, powerbi, python, risk management, risk analysis, risk mitigation planning, dashboards, metrics, strategic planning, project leadership, partnerships, healthcare, medical technology","accessibility, activities, analytics, communication, critical thinking, crossfunctional teams, dashboard, data collection, data integrity, data retrieval, dataanalytics, decision making, excel, flexible reporting tools, healthcare, internal resource planning, medical technology, metrics, partnerships, powerbi, project leadership, project management, python, query, reporting, risk analysis, risk management, risk mitigation planning, strategic planning, system standards, usage"
Data Analyst (GIS Advanced Methane Detection),Consumers Energy,"Michigan, United States",https://www.linkedin.com/jobs/view/data-analyst-gis-advanced-methane-detection-at-consumers-energy-3769539683,2023-12-17,Michigan,United States,Associate,Hybrid,"Location:
This position is open to being assigned to any Consumers Energy Service Center’s located throughout Michigan’s lower peninsula. The candidate must be within a comfortable daily commutable distance to any of the locations or willing to relocate (relocation package is available for those who qualify).
We are looking for:
Data Analyst (GIS Advanced Methane Detection)
The Data Analyst position builds and maintains large-scale statistical models and algorithms by leveraging skill set in predictive modeling, statistics, and modeling methods to align with business strategy. Enhances ethics required to build products that function properly while adhering to the privacy standards of consumers. They bring technical expertise to ensure the quality and accuracy of that data, then process, design, and present it in ways to help people, businesses, and organizations make better decisions.
The Data Analyst (Advanced Methane Detection) will provide quality analysis from Advanced Methane Detection (AMD) surveys. In addition, this person will be responsible for generating reports, providing data, and working closely with other departments in the gas business. The analyst will communicate with contractors on a regular basis and provide them with their work responsibilities.
The knowledge and skills involved with this position include but are not limited to:
Experience using ArcGIS Pro and ArcGIS Online
Leak survey GIS boundary identification
Analyze and summarize coverage of AMD surveys using GIS software
Experience with data analytics
Experience with geospatial analytics
Ability to work with large data sets from multiple data sources
Ability to present to a variety of audiences both technical and non-technical
Knowledge and understanding of data modeling, machine learning algorithms, statistical analysis, data visualization, and data engineering
Ability to recognize, analyze, and solve a variety of problems
Excellent communication skills both written and verbal
We encourage you to apply if you have:
Bachelor’s degree in Information Technology, Data Analytics or related discipline with two (2) or more years applicable experience with a proven track record of turning data into meaningful analysis, presenting to business stakeholders, and driving value for the business
[OR]
Associate degree in Information Technology, Data Analytics or related discipline with six (6) or more years applicable experience with a proven track record of turning data into meaningful analysis, presenting to business stakeholders, and driving value for the business
[OR]
High School Diploma with ten (10) or more years of applicable experience with a proven track record of turning data into meaningful analysis, presenting to business stakeholders, and driving value for the business
Ability to see the big picture and to focus on details when necessary
Advanced knowledge and experience in topics of data and information management
Hands on experience with Big Data/NoSQL Platforms with experience delivering production projects at scale -
Preferred
In this role, you will:
Proposes and evaluates innovative solutions for analyzing and portraying GIS data
Develop and maintain GIS data from Advanced Methane Detection (AMD) surveying
Provide daily survey plans to contractors
Provide platforms for AMD data collection for future studies and pilots
Assist in data transfer between systems
Perform quality control analysis for each AMD survey
Proposes, implements, and evaluates innovative solutions for analyzing, clustering, associating, and classifying data
Leads the development and validates algorithms via analysis, computer simulation, and prototyping
Extends analytics platform for connected devices and sensors
Writes and debugs production code that spans the vertical from database to computational layer to web service
Work with cross functional teams to identify and cover every edge case imaginable
Help maintain large-scale analytics infrastructure, including distributed storage and computation clusters
Other non-essential duties as assigned or may be necessary
All qualified applicants will not be discriminated against and will receive consideration for employment without regard to protected veteran status, disability, race, color, religion, sex, age, sexual orientation, gender identity or national origin.
Show more
Show less","ArcGIS Pro, ArcGIS Online, GIS, Leak survey, Geospatial analytics, Data analytics, Data modeling, Machine learning, Statistical analysis, Data visualization, Data engineering, Problem solving, Communication, Data management, Big Data, NoSQL, Production projects, Innovative solutions, Data analysis, Data transfer, Quality control, Algorithm development, Connected devices, Database, Computational layer, Web service, Distributed storage, Computation clusters","arcgis pro, arcgis online, gis, leak survey, geospatial analytics, data analytics, data modeling, machine learning, statistical analysis, data visualization, data engineering, problem solving, communication, data management, big data, nosql, production projects, innovative solutions, data analysis, data transfer, quality control, algorithm development, connected devices, database, computational layer, web service, distributed storage, computation clusters","algorithm development, arcgis online, arcgis pro, big data, communication, computation clusters, computational layer, connected devices, data engineering, data management, data transfer, dataanalytics, database, datamodeling, distributed storage, geospatial analytics, gis, innovative solutions, leak survey, machine learning, nosql, problem solving, production projects, quality control, statistical analysis, visualization, web service"
PMO Data Analyst - Hybrid,Stryker,"Portage, MI",https://www.linkedin.com/jobs/view/pmo-data-analyst-hybrid-at-stryker-3771377425,2023-12-17,Michigan,United States,Associate,Hybrid,"Why join Stryker?
We are proud to be named one of the World’s Best Workplaces and a Best Workplace for Diversity by Fortune Magazine! Learn more about our award-winning organization by visiting stryker.com
Who We Want
Data translators.
Highly effective communicators who can transform data findings into recommendations to compose reports and executive level presentations.
Strategic thinkers.
People who enjoy analyzing data or trends for the purposes of planning, forecasting, advising, budgeting, reporting, or sales opportunities.
Collaborative partners.
People who build and leverage cross-functional relationships to bring together ideas, information, use cases, and industry analyses to develop best practices.
What You Will Do
We are looking for a highly motivated and experienced Senior PMO Analyst to join our team. The ideal candidate will have a strong understanding of project management principles and practices, as well as experience with data, analytics, and reporting.
As the Senior PMO Analyst, you will work with cross functional teams as you drive data integrity as you enter, review, and analyze data in the resource capacity platform. In this role, you will be responsible for supporting and maintaining the resource capacity platform associated with data collection, retrieval, accessibility, and usage for internal resource planning and activities. You will play a crucial role in ensuring data integrity, generating reports, and providing training to users. Additionally, you will have the opportunity to recommend changes in application development, maintenance, and system standards.
Other daily activities include but are not limited to:
Support and maintain resource capacity platform/tool for data collection and usage.
Partner with cross-functional teams SME’s on entering and ensuring clean and consistent data inputs into established tables, fields, and system databases.
Acts as the PMO point of contact for cross-functional users for all platform questions, request, and modifications.
Responsible for processing and validating platform changes in accordance with platform guidance and governance.
Partner with Lead PMO Analyst triage issues and determine if root cause is process, people, or system.
Set up user access and provide training on platform usage.
Create and maintain seamless end-user experience for all reporting needs.
Partner with Lead PMO Analyst to drive visibility, understanding, trust, and improvements within the resource capacity platform and reports.
Partner with cross-functional users to identify and resolve data gaps and issues.
Partner with the Lead PMO Analyst to ensure business readiness for minor and major releases and updates.
Build and produce reports using query and flexible reporting tools to meet the requirements of cross-functional users and resource managers.
Investigate reporting issues and provide expertise to resolve and follow-up in a timely manner.
Identify opportunities to gain efficiencies, automate, and improve data quality.
In addition to the activities listed above, the Senior PMO Analyst must be able to build strong relationships with stakeholders, communicate effectively with stakeholders, stay calm under pressure, think critically and make sound decisions.
The ideal candidate will also be highly motivated and experienced, passionate about data, analytics, project portfolio management software, and project management, able to work independently with minimal supervision, be able to understand and apply procedures and concepts of their discipline, pay attention to detail in making evaluative judgments based on the analysis of factual information.
What You Will Need
Bachelor's Degree is required
Minimum of 2 years of professional experience
Strong analytical and problem-solving skills.
Experience with data analysis tools such as Excel, PowerBI and Python.
Ability to communicate data insights to stakeholders.
Contribute to the development of the resource capacity platform, recommend improvements through data and reporting.
Assist in the development of the PMO tools and reports. Alert Lead PMO Analyst of variances or gaps.
Complete assigned tasks according to established timeline. Assist in the development of the resource capacity platform rollout and training timelines.
Participate in the development of PMO tools that support data quality.
Partner in the establishment of risk management plans and support effective risk analysis and risk mitigation planning for the PMO.
Provide support on rescheduling, and resource allocation in the program plan and support program gating processes.
Establish and maintain partnerships with internal and external PMO stakeholders.
Produce weekly and monthly PMO metric reports using dashboards and metrics to measure success.
Contributes data analysis expertise for short- and long-term strategic planning process activities.
Leads projects. May support a high complexity program.
Builds productive working relationships with departments that are key partners (e.g., R&D and Marketing).
About Stryker
Our benefits:
12 paid holidays annually
Health benefits include: Medical and prescription drug insurance, dental insurance, vision insurance, critical illness insurance, accident insurance, hospital indemnity insurance, personalized healthcare support, wellbeing program and tobacco cessation program.
Financial benefits include Health Savings Account (HSA), Flexible Spending Accounts (FSAs), 401(k) plan, Employee Stock Purchase Plan (ESPP), basic life and AD&D insurance, and short-term disability insurance.
For a more detailed overview of our benefits or time off, please follow this link to learn more: US Stryker employee benefits
About Stryker
Stryker is one of the world’s leading medical technology companies and, together with its customers, is driven to make healthcare better. The company offers innovative products and services in Medical and Surgical, Neurotechnology, Orthopaedics and Spine that help improve patient and healthcare outcomes. Alongside its customers around the world, Stryker impacts more than 130 million patients annually. More information is available at stryker.com.
Know someone at Stryker?
Be sure to have them submit you as a referral prior to applying for this position. Learn more about our employee referral program on our referral page
Stryker is driven to work together with our customers to make healthcare better. Employees and new hires in sales and field roles that require access to customer accounts as a function of the job may be required, depending on customer requirements, to obtain various vaccinations as an essential function of their role.
R503899_5
Show more
Show less","Project management, Data analysis, Analytics, Reporting, Microsoft Excel, PowerBI, Python, Stakeholder communication, Data integrity, Resource capacity platform, Crossfunctional collaboration, Strategic planning, Risk management, Program gating processes, Project portfolio management software","project management, data analysis, analytics, reporting, microsoft excel, powerbi, python, stakeholder communication, data integrity, resource capacity platform, crossfunctional collaboration, strategic planning, risk management, program gating processes, project portfolio management software","analytics, crossfunctional collaboration, data integrity, dataanalytics, microsoft excel, powerbi, program gating processes, project management, project portfolio management software, python, reporting, resource capacity platform, risk management, stakeholder communication, strategic planning"
Senior Data Engineer,MGIC,"Michigan, United States",https://www.linkedin.com/jobs/view/senior-data-engineer-at-mgic-3735206294,2023-12-17,Michigan,United States,Mid senior,Remote,"Why work at MGIC?
Are you someone who wants to play a critical role in our company’s success? Do you enjoy solving puzzles and finding a better way to get things done? Are you someone who likes to Take The Lead and make an impact? If so, then imagine yourself at MGIC. At MGIC we are a team of dedicated professionals on a fearless mission. A team that fosters a culture of career development and continuous learning opportunities to help you rise to new heights. We are passionate about providing outstanding customer service and making a difference in our community. #WeAreMGIC
Preferred location: Milwaukee based – hybrid (3 days office, 2 days remote)
Other locations: Madison, Minnesota, Illinois, and Michigan (occasional travel into the office to support team engagements).
How will you make an impact?
We are currently looking for a Senior Data Engineer to deliver high quality, maintainable and scalable data solutions. The Senior Data Engineer will partner with solution architects and other data engineers to develop our enterprise analytics data platform through new and updated data pipelines leveraging shared components and aligning to standards and best practices. This role is focused on collecting data from internal and external sources and transforming it into usable information for the business including data scientists and data analysts.
Deliver high quality data assets to be used by the business to transform business processes and to enable leaders to make data-driven decisions
Continuously improve data solutions to increase quality, speed of delivery and trust of data engineering team’s deliverables to enable business outcomes
Reduce total cost of ownership of solutions by developing shared components and implementing best practices and coding standards
Do you have what it takes?
Ability to translate data engineering designs into working code
Data analysis and data engineering pipeline experience including design, development, and support
Experience with AWS services or cloud data offerings including S3, Lambda, EMR, Dynamo DB, Glue, Snowflake and/or other data technologies.
Experience with coding in Python, PySpark, and Terraform
Experience with DevOps practices including Continuous Integration, Continuous Delivery, and Infrastructure as Code to deliver end-to-end automation of data delivery.
Ability to train and mentor junior data engineers.
Experience with Agile engineering practices including the scrum framework.
Enjoy these benefits from day one:
Competitive Salary & pay-for-performance bonus
Financial Benefits (401k with company match, profit sharing, HSA, wellness rewards program)
On-site Fitness Center and classes (corporate office)
Paid-time off and paid company holidays
Business casual dress
For additional information about MGIC and to apply, please visit our website at www.mgic.com/careers.
Show more
Show less","AWS, Snowflake, Python, PySpark, Terraform, DevOps, S3, Lambda, EMR, Dynamo DB, Glue, Agile, Scrum","aws, snowflake, python, pyspark, terraform, devops, s3, lambda, emr, dynamo db, glue, agile, scrum","agile, aws, devops, dynamo db, emr, glue, lambda, python, s3, scrum, snowflake, spark, terraform"
"Data Engineer with GCP (Google Cloud Platform) Experience  -  (Strictly W2, No Third Party / C2C accepted)","Systems Technology Group, Inc. (STG)","Michigan, United States",https://www.linkedin.com/jobs/view/data-engineer-with-gcp-google-cloud-platform-experience-strictly-w2-no-third-party-c2c-accepted-at-systems-technology-group-inc-stg-3533899680,2023-12-17,Michigan,United States,Mid senior,Remote,"Title:
Data Engineer with GCP (Google Cloud Platform) Experience --
(Strictly W2, No Third Party / C2C accepted)
Description:
STG is a fast-growing Digital Transformation services company providing Fortune 500 companies with Digital Transformation, Mobility, Analytics and Cloud Integration services in both information technology and engineering product lines. STG has a 98% repeat business rate from existing clients and have achieved industry awards and recognition for our services.
Crain’s Detroit Business
named STG to Michigan’s Fastest Growing Companies list in both 2020 and 2019, Top IT Services Company’s List in 2020, 2019, 2018, and Top Minority Business Enterprise List in 2020, 2019, and 2019.
STG puts company CULTURE at the forefront of every business decision and employees are EMPOWERED and MEASURED for RESULTS. Both TEAMWORK and INDIVIDUAL Performance is recognized and rewarded.
This is a full-time position that includes responsibilities of a
Data Engineer with GCP (Google Cloud Platform) Experience.
The right person for this position will have high energy, a positive attitude, and a high attention to detail, and an unwavering commitment to success.
Position Description:
The GDIA Data Factory Platform covers all business processes and technical components involved in ingesting a wide range of enterprise data into the GDIA Data Factory (Data Lake) and the transformation of that data into consumable data sets in support of analytics.
The Data Factory Enablement Team, as the name suggests enables teams build their solutions in the GCP Data Factory Platform by proving Tools, Guidelines, processes and support.
We are looking for candidates who have a broad set of technology skills across areas and come from a background of DevOps, with exposure to infrastructure and solution monitoring. This person will be expected to provide consultative services to the Software Development and Database Engineering teams.
Key responsibilities include:
· Work as part of an implementation team from concept to operations, providing deep technical subject matter expertise for successfully deployment of Data Platform
· Implement methods for standardization of all parts of the pipeline to maximize data usability and consistency
· Test and compare competing solutions and report out a point of view on the best solution
· Design and Build CICD Pipelines for Google Cloud Platform (GCP) services: BigQuery, DataFlow, Pub/Sub, Data Fusion and others.
· Work with stakeholders including Analytics, Product, and Design teams to assist with data related technical issues and support their data infrastructure needs.
· Develop IAC tekton pipelines to execute pattern playbooks and templates
· Designing cloud performance and monitoring strategies
· Designing and implementing workflows to automate the infrastructure release and upgrade process for applications in Dev, UAT and Production environments.
· Mentor and grow technical skills of engineers across multiple sprint teams by giving high quality feedback in design and code reviews and providing training for new methods, tools, and patterns.
Skills Required:
· Someone who understands Cloud as being a way to operate and not a place to host systems.
· In-depth understanding of GCP product technology and underlying architectures.
· Experience and very strong with development eco-system such as Git, Jenkins, Terraform and Tekton for CI/CD
· Experience in working with Agile and Lean methodologies.
Experience Required:
· At least 2 years of tekton experience
· At least 5 years of Terraform experience or 3 years with terraform certification.
· At least 3 years of experience in Google cloud and Google cloud professional architect certification
Data Engineer with GCP (Google Cloud Platform) Experience
is based at our corporate office located in Michigan. A great opportunity to experience the corporate environment leading personal career growth.
Resume Submittal Instructions:
Interested/qualified candidates should email their word formatted resumes to
Vasavi Konda –
vasavi.konda(@)stgit.com
and/or contact @
(Two-Four-Eight) Seven- One-Two – Six-Seven-Two-Five (@248.712.6725)
.
In the subject line of the email please include:
First and Last Name
-
Data Engineer with GCP (Google Cloud Platform) Experience.
For more information about STG, please visit us at
www.stgit.com.
Sincerely,
Vasavi Konda| Recruiting Specialist
“Opportunities don't happen, you create them.”
Systems Technology Group (STG)
3001 W. Big Beaver Road, Suite 500
Troy, Michigan 48084
Phone: 248.712.6725(O)
Email: vasavi.konda(@)stgit.com
Show more
Show less","Google Cloud Platform (GCP), BigQuery, DataFlow, Pub/Sub, Data Fusion, Git, Jenkins, Terraform, Tekton, Agile, Lean, Data Lake, Data Factory Enablement Team, Data Factory Platform, CICD Pipelines, Cloud performance monitoring, Infrastructure release and upgrade, Automation, Cloud as a way to operate","google cloud platform gcp, bigquery, dataflow, pubsub, data fusion, git, jenkins, terraform, tekton, agile, lean, data lake, data factory enablement team, data factory platform, cicd pipelines, cloud performance monitoring, infrastructure release and upgrade, automation, cloud as a way to operate","agile, automation, bigquery, cicd pipelines, cloud as a way to operate, cloud performance monitoring, data factory enablement team, data factory platform, data fusion, data lake, dataflow, git, google cloud platform gcp, infrastructure release and upgrade, jenkins, lean, pubsub, tekton, terraform"
Data Engineer - Scala(U.S. remote),Railroad19,"Lansing, MI",https://www.linkedin.com/jobs/view/data-engineer-scala-u-s-remote-at-railroad19-3782291977,2023-12-17,Michigan,United States,Mid senior,Remote,"Railroad19, Inc.
is hiring
remote
Senior Data Engineers
to be a solid technical resource on a dynamic and growing team of accomplished engineers.
Our ideal candidate is passionate about creating well-architected solutions containing thoroughly tested code. The ability to communicate effectively and develop relationships by empathizing with client goals is a highly valued skill within our company culture.
Core Responsibilities:
Develop new and enhance existing application services
Writing tests to maintain code quality
Understand and adapt to our client's evolving business requirements within the television advertising domain.
Participate in detailed technical design sessions to understand client needs and provide productive feedback
Identify new opportunities, tools, and services to enhance the software platform
Support and troubleshoot issues, identify the root cause, and proactively recommend corrective actions
Skills & Experience:
Scala 2.12 + development experience
Passionate about developing clean and maintainable code with little or no side-effects
Experience building Restful APIs in Scala using Spark 2.4
Strong hands-on experience with AWS running spark jobs on ephemeral EMR clusters on AWS and S3.
Experience with relational and non-relational databases
Willingness to learn new technologies and takes pride in keeping up with the latest technologies and practices within the Scala and Spark development community
Excellent oral and written communication skills
Strong analytical and problem-solving skills
Self-directed and can effectively deliver solutions with little oversight
A bachelor's or master's degree in computer science, computer engineering, or other technical disciplines or equivalent work experience is preferred but not required.
$120,000 - $160,000 a year
Salary is commensurate with experience.
We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender, gender identity or expression, or veteran status. We are proud to be an equal-opportunity workplace.
#Hiringnow
We are actively hiring (Data Engineers)
#remote #Scala #DataEngineering #ApacheSpark
Show more
Show less","Scala 2.12, Spark 2.4, RESTful APIs, AWS, EMR, S3, Relational databases, Nonrelational databases, Apache Kafka, Apache Spark Streaming, Python, Java","scala 212, spark 24, restful apis, aws, emr, s3, relational databases, nonrelational databases, apache kafka, apache spark streaming, python, java","apache kafka, apache spark streaming, aws, emr, java, nonrelational databases, python, relational databases, restful apis, s3, scala 212, spark 24"
Data Engineer IV - Max Digital (Data Engineering),ACV Auctions,"Michigan, United States",https://www.linkedin.com/jobs/view/data-engineer-iv-max-digital-data-engineering-at-acv-auctions-3762875655,2023-12-17,Michigan,United States,Mid senior,Hybrid,"If you are looking for a career at a dynamic company with a people-first mindset and a deep culture of growth and autonomy, ACV is the right place for you! Competitive compensation packages and learning and development opportunities, ACV has what you need to advance to the next level in your career. We will continue to raise the bar every day by investing in our people and technology to help our customers succeed. We hire people who share our passion, bring innovative ideas to the table, and enjoy a collaborative atmosphere.
Who we are:
ACV is a technology company that has revolutionized how dealers buy and sell cars online. We are transforming the automotive industry. ACV Auctions Inc. (ACV), has applied innovation and user-designed, data driven applications and solutions. We are building the most trusted and efficient digital marketplace with data solutions for sourcing, selling and managing used vehicles with transparency and comprehensive insights that were once unimaginable. We are disruptors of the industry and we want you to join us on our journey. ACV’s network of brands includes ACV Auctions, ACV Transportation, ClearCar, MAX Digital and ACV Capital within its Marketplace Products, as well as, True360 and Data Services.
At ACV we focus on the Health, Physical, Financial, Social and Emotional Wellness of our Teammates and to support this we offer:
Multiple medical plans including a high deductible health plan that costs $0 out of your paycheck
Company-sponsored (paid) Short-Term Disability, Long-Term Disability, and Life Insurance
Comprehensive optional benefits such as Dental, Vision, Supplemental Life/AD&D, Legal/ID Protection, and Accident and Critical Illness Insurance
Generous paid time off options, including vacation time, sick days, Company holidays, floating holidays, parental leave, bereavement leave, jury duty leave, voting leave, and other forms of paid leave as required by applicable law or regulation
Employee Stock Purchase Program with additional opportunities to earn stock in the Company
Retirement planning through the Company’s 401(k)
Who we are looking for:
We are seeking a highly skilled Engineer IV in Data Engineering with a strong foundation in computer science and excellent problem-solving skills. You will be responsible for maintaining and extending our database operations, optimizing SQL queries, and designing scalable data services.
What you will do:
Actively and consistently support all efforts to simplify and enhance the customer experience.
Maintain and extend (as required) existing database operations solution for backups, index defragmentation, data retention, etc.
Troubleshoot any SQL Server or ETL stack outages during our operational support window.
Triage any issues with data stack (SSIS, C#, Web APIs).
Support development, integration, and stage SQL Server environments for application development and data science teams.
Ensure that new database development meets company standards for readability, reliability, and performance. Work with internal teams on transactional and analytical schema design.
Collaborate with software and DevOps engineers to design scalable services, plan feature roll-out, and ensure high reliability and performance of our products.
Architect and build entire services including but not limited to; data modeling, storage, message brokers, protocols and interfaces.
Design, build and maintain complex systems that can scale rapidly with little maintenance.
Conduct code reviews, develop high-quality documentation, and build robust test suites.
Own the overall performance of products and services within a defined area of focus.
Be empowered to lead and complete software projects with minimal guidance from managers.
Lead team discussions to define technical requirements on new and current products.
Respond-to and troubleshoot highly complex problems quickly, efficiently, and effectively.
Mentor junior engineers.
Perform additional duties as assigned.
What you will need:
Bachelor's degree in Computer Science, Information Technology, Computer Information Systems, Management Information Systems, or similar
5 years' building & supporting the database-tier of SaaS web applications.
Ability to read, write, speak, and understand English.
Expert understanding of SQL query execution fundamentals and query optimization principles.
Experience maintaining and extending an existing codebase, adapting to pre-existing patterns and tracing the code’s path of execution.
ETL workflow implementation (SSIS, Airflow, C#, Python)
Experience working with Cloud Services (AWS RDS, S3, SQS, SNS)
Experience working with NoSQL data stores (MongoDB)
Experience writing unit and integration testing (DBT, C#)
Expert SQL and data-layer development experience; OLTP schema design.
Experience integrating 3rd-party APIs, implementing authentication & authorization and developing asynchronous data flows.
Nice to Have
OLAP schema design experience.
Experience with Airflow, Snowflake, etc.
Experience with DBT
Our Values
Trust & Transparency | People First | Positive Experiences | Calm Persistence | Never Settling
At ACV, we are committed to an inclusive culture in which every individual is welcomed and empowered to celebrate their true selves. We achieve this by fostering a work environment of acceptance and understanding that is free from discrimination. ACV is committed to being an equal opportunity employer regardless of sex, race, creed, color, religion, marital status, national origin, age, pregnancy, sexual orientation, gender, gender identity, gender expression, genetic information, disability, military status, status as a veteran, or any other protected characteristic. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you have a disability or special need that requires reasonable accommodation, please let us know.
For information on our collection and use of your personal information, please see our Privacy Notice.
Apply Now
Show more
Show less","SQL, ETL, SSIS, Airflow, C#, Python, AWS RDS, S3, SQS, SNS, MongoDB, DBT, OLTP, OLAP, Snowflake, Data modeling, Storage, Message brokers, Protocols, Interfaces, Code reviews, Documentation, Unit testing, Integration testing, 3rdparty APIs, Authentication, Authorization, Data flows, Computer science, Information technology, Computer information systems, Management information systems","sql, etl, ssis, airflow, c, python, aws rds, s3, sqs, sns, mongodb, dbt, oltp, olap, snowflake, data modeling, storage, message brokers, protocols, interfaces, code reviews, documentation, unit testing, integration testing, 3rdparty apis, authentication, authorization, data flows, computer science, information technology, computer information systems, management information systems","3rdparty apis, airflow, authentication, authorization, aws rds, c, code reviews, computer information systems, computer science, data flows, datamodeling, dbt, documentation, etl, information technology, integration testing, interfaces, management information systems, message brokers, mongodb, olap, oltp, protocols, python, s3, snowflake, sns, sql, sqs, ssis, storage, unit testing"
Data Engineer IV - Max Digital (Data Operations),ACV Auctions,"Michigan, United States",https://www.linkedin.com/jobs/view/data-engineer-iv-max-digital-data-operations-at-acv-auctions-3762879092,2023-12-17,Michigan,United States,Mid senior,Hybrid,"If you are looking for a career at a dynamic company with a people-first mindset and a deep culture of growth and autonomy, ACV is the right place for you! Competitive compensation packages and learning and development opportunities, ACV has what you need to advance to the next level in your career. We will continue to raise the bar every day by investing in our people and technology to help our customers succeed. We hire people who share our passion, bring innovative ideas to the table, and enjoy a collaborative atmosphere.
Who we are:
ACV is a technology company that has revolutionized how dealers buy and sell cars online. We are transforming the automotive industry. ACV Auctions Inc. (ACV), has applied innovation and user-designed, data driven applications and solutions. We are building the most trusted and efficient digital marketplace with data solutions for sourcing, selling and managing used vehicles with transparency and comprehensive insights that were once unimaginable. We are disruptors of the industry and we want you to join us on our journey. ACV’s network of brands includes ACV Auctions, ACV Transportation, ClearCar, MAX Digital and ACV Capital within its Marketplace Products, as well as, True360 and Data Services.
At ACV we focus on the Health, Physical, Financial, Social and Emotional Wellness of our Teammates and to support this we offer:
Multiple medical plans including a high deductible health plan that costs $0 out of your paycheck
Company-sponsored (paid) Short-Term Disability, Long-Term Disability, and Life Insurance
Comprehensive optional benefits such as Dental, Vision, Supplemental Life/AD&D, Legal/ID Protection, and Accident and Critical Illness Insurance
Generous paid time off options, including vacation time, sick days, Company holidays, floating holidays, parental leave, bereavement leave, jury duty leave, voting leave, and other forms of paid leave as required by applicable law or regulation
Employee Stock Purchase Program with additional opportunities to earn stock in the Company
Retirement planning through the Company’s 401(k)
Who we are looking for:
We are seeking a highly skilled Engineer IV in Data Engineering with a strong foundation in computer science and excellent problem-solving skills. You will be responsible for maintaining and extending our database operations, optimizing SQL queries, and designing scalable data services.
What you will do:
Actively and consistently support all efforts to simplify and enhance the customer experience.
Maintain and extend (as required) existing database operations solution for backups, index defragmentation, data retention, etc.
Troubleshoot any SQL Server or ETL stack (SSIS, C#, Web APIs) outages during our operational support window.
Leverage monitoring tools to ensure high performance and availability; work with operations and engineering to improve as required.
Leverage DMVs and monitoring tools to ensure system performance; work with data operations and engineering to improve as required.
Ensure existing HADR (availability groups) solution is functional and meets requirements.
Support development, integration, and stage SQL Server environments for application development and data science teams.
Ensure that new database development meets company standards for readability, reliability, and performance. Work with internal teams on transactional and analytical schema design.
Collaborate with software and DevOps engineers to design scalable services, plan feature roll-out, and ensure high reliability and performance of our products.
Conduct code reviews, develop high-quality documentation, and build robust test suites.
Own the overall performance of products and services within a defined area of focus.
Respond-to and troubleshoot highly complex problems quickly, efficiently, and effectively. This may include being part of the emergency after-hours on-call rotation.
Mentor junior data engineers.
Perform additional duties as assigned.
What you will need:
Bachelor's degree in Computer Science, Information Technology, Computer Information Systems, Management Information Systems, or similar
5 years' building & supporting the database-tier of SaaS web applications.
Ability to read, write, speak, and understand English.
Expert in SQL Query optimization
ETL workflow implementation (SSIS, Airflow, C#, Python)
Experience working with Cloud Services (AWS RDS, S3, SQS, SNS)
Experience working with NoSQL data stores (e.g., MongoDB)
Experience developing Windows services in C#
Experience writing unit and integration testing
Expert SQL and data-layer development experience; OLTP schema design.
Experience using and integrating with cloud services, specifically: AWS RDS, S3, SQS, SNS.
Nice to Have
Experience with Airflow
Experience with DBT
Our Values
Trust & Transparency | People First | Positive Experiences | Calm Persistence | Never Settling
At ACV, we are committed to an inclusive culture in which every individual is welcomed and empowered to celebrate their true selves. We achieve this by fostering a work environment of acceptance and understanding that is free from discrimination. ACV is committed to being an equal opportunity employer regardless of sex, race, creed, color, religion, marital status, national origin, age, pregnancy, sexual orientation, gender, gender identity, gender expression, genetic information, disability, military status, status as a veteran, or any other protected characteristic. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you have a disability or special need that requires reasonable accommodation, please let us know.
For information on our collection and use of your personal information, please see our Privacy Notice.
Apply Now
Show more
Show less","Cloud Services, AWS RDS, AWS SQS, AWS SNS, AWS S3, SQL, ETL, SSIS, C#, Python, Airflow, DBT, MongoDB, Hadoop, Linux, Windows, Data Science, DevOps, Unit Testing, Integration Testing","cloud services, aws rds, aws sqs, aws sns, aws s3, sql, etl, ssis, c, python, airflow, dbt, mongodb, hadoop, linux, windows, data science, devops, unit testing, integration testing","airflow, aws rds, aws s3, aws sns, aws sqs, c, cloud services, data science, dbt, devops, etl, hadoop, integration testing, linux, mongodb, python, sql, ssis, unit testing, windows"
Senior Cloud Data Engineer,BDO USA,"Grand Rapids, MI",https://www.linkedin.com/jobs/view/senior-cloud-data-engineer-at-bdo-usa-3765470283,2023-12-17,Michigan,United States,Mid senior,Hybrid,"Job Description
Job Summary:
This position will work with cutting edge technology, deliver high quality solutions across various industries, and oversee team(s) on engagements that range in size and scope. This position will receive continuous career development opportunities, given the size and potential of client engagements. This role will perform hands-on delivery of data analytics projects, contributing to the development and unit testing of solutions.
Job Duties
Designs and implements best in class data ingestion strategies, data warehouse and data mart structures, semantic layers and models, visualizations, streaming processes, API integrations, and automation (RPA) solutions for end-to-end data analytics solutions on primarily, but not limited to, cloud analytics platforms such as Azure and AWS
Listens to client needs to align solution with business requirements and delivery schedule
Creates written functional and technical designs
Participates in project status and stand meetings, and assists with providing aggregated project status for project and program managers
Assists with SLA compliance of solutions, and performs performance tuning and optimization efforts of end-to-end solutions
Writes code using multiple languages and correctly applies frameworks, architectural patterns, and software development principles
Delivers high-performance, scalable, repeatable, and secure deliverables with broad impact (high throughput and low latency)
Assists with implementation of data governance programs and best practices
Performs the cleaning and transforming of data from source systems into analytics models
Implements models to support data visualizations and integrations
Assists with implementing DevOps, DataOps and MLOps methodologies on projects
Writes custom integration logic in applicable programming languages
Assists project managers with work breakdown structure creation, project estimation, resource staffing, workload planning and adjustments throughout the project lifecycle
Assists clients with licensing, security, and cost estimation of solutions
Performs code reviews to ensure adherence to standards
Works directly with clients and team members to establish secure data analytics platforms and infrastructure
Contributes to successful deployments of developed solutions and integration of DevOps tools
Maintains a broad and current understanding of data analytics and business intelligence strategies, cloud platforms, methodologies, and tools
Builds client relationships during project execution, effectively becoming a trusted advisor of the client
Participates in support activities for existing software solutions
Other duties as assigned
Supervisory Responsibilities
Supervises the day-to-day workload of Associates on assigned engagements to ensure that timelines and deliverables are met, and reviews work product
Education
Qualifications, Knowledge, Skills and Abilities:
High School Diploma or GED equivalent, required
Bachelor’s degree, preferred; focus in Information Systems, Data Science or Computer Science, preferred
Experience
Five (5) or more years of experience within Data Analytics, Business Intelligence, Artificial Intelligence, or Application Development, required
One (1) or more years of experience technically leading development projects, preferred
One (1) or more years of consulting experience or implementation of cloud-based data analytics solutions, preferred
Software
Strong SQL skills including Data Definition Language (DDL), Data Manipulation Language (DML), views, functions, stored procedures, or performance tuning, required
Experience with Data Warehousing, Data Modeling, Semantic Model Definition or Star Schema Construction, required
Hands on delivery experience of end-to-end cloud data analytics solutions within Azure or AWS, preferred
Experience with one (1) or more of the following computer languages, preferred:
C#
Python
Java
Scala
Experience with tabular modeling within Microsoft Fabric, Power BI, or Azure Analysis Services, preferred
Experience with Git and DevOps deployment technologies, preferred
Experience with Linux, preferred
Experience with one (1) or more of the following, preferred:
Data Lake Medallion Architecture
Batch and/or streaming data ingestion into a data lake
AI Algorithms/Machine Learning
Automation tools such as UiPath, Alteryx, etc.
Computer Vision based AI technologies
Other Knowledge, Skills & Abilities
Ability to work with a high degree of professionalism and autonomy
Excellent verbal and written communication skills
Solid organizational skills, especially the ability to meet project deadlines with a focus on details
Ability to successfully multi-task while working independently or within a group environment
Ability to work in a deadline-driven environment, and handle multiple projects simultaneously
Ability to interact effectively with people at all organizational levels of the Firm
Ability to effectively interact with a team of professionals and delegating work assignments, as needed
Ability to build and maintain strong relationships with internal and client personnel
Ability to encourage a team environment on engagements, and contribute to the professional development of assigned personnel
Keywords:
Data Analytics, Business Intelligence, BI, Synapse, IoT, Machine Learning, Data Lake, Stream, Cube, Microsoft, SQL Server, Tableau, .Net, C#, Qlik, Power BI, Machine Learning, Azure Data Factory, RedShift, UiPath, Cloud, RPA, AWS, Redshift, Kinesis, QuickSight, SageMaker, S3, Databricks, AWS Lake Formation, Snowflake, Python, Qlik, Athena, Data Pipeline, Glue, Star Schema, Data Modeling, SQL, SSIS, SSAS, SSRS, PySpark, Microsoft Fabric, dbt, Linux, Terraform, Bicep, Data Ops, Purview, Git, Delta, Pandas, Spark SQL
Individual salaries that are offered to a candidate are determined after consideration of numerous factors including but not limited to the candidate’s qualifications, experience, skills, and geography.
California Range: $111,000 - $152,000
Colorado Range: $111,000 - $152,000
New York City/ Valhalla Range: $111,000 - $152,000
Washington Range: $111,000 - $152,000
About Us
BDO delivers assurance, tax, digital technology solutions and financial advisory services to clients throughout the country and around the globe. We offer numerous industry-specific practices, world-class resources, and an unparalleled commitment to meeting our clients’ needs. We currently serve more than 400 publicly traded domestic and international clients.
Unparalleled partner-involvement
Deep industry knowledge and participation
Geographic coverage across the U.S.
Cohesive global network
Focused capabilities across disciplines
BDO brings world-class resources and exceptional service to each and every one of our clients. BDO USA is a member of BDO International, the world’s fifth largest accounting network.
BDO offers a competitive Total Rewards package that encompass so much more than – “traditional benefits”. Our wide range of rewards and our employees’ ability to customize rewards to their individual needs are two of the reasons why BDO has been honored with so many workplace awards, including 100 Best Companies for Working Parents, Working Mother 100 Best Companies, Top Entry Level Employer, 2022 National Best & Brightest Companies to Work For and more.
Some Examples Of Our Total Rewards Offerings Include
Competitive pay and eligibility for an annual performance bonus.
A 401k plan plus an employer match
Comprehensive, medical, dental, vision, FSA, and prescription insurance from day one
Competitive Paid Time Off with daily accrual from day one of employment, plus paid holidays
Paid Parental Leave
Adoption Assistance
Firm paid life insurance
Wellness programs
Additional offerings include BDO Flex, Group Legal insurance, Pet insurance and Long-Term Care Insurance
Above offerings may be subject to eligibility requirements.
Click here to find out more!
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability or protected veteran status.
""BDO USA, P.A. is an EO employer M/F/Veteran/Disability""
Show more
Show less","Data Analytics, Business Intelligence, Data Warehousing, Data Modeling, Semantic Model Definition, Star Schema Construction, Cloud Data Analytics, Azure, AWS, SQL, DDL, DML, Views, Functions, Stored Procedures, Performance Tuning, Git, DevOps, Linux, Data Lake Medallion Architecture, Batch Data Ingestion, Streaming Data Ingestion, AI Algorithms, Machine Learning, Automation Tools, Computer Vision, .Net, C#, Python, Java, Scala, Tabular Modeling, Microsoft Fabric, Power BI, Azure Analysis Services, UiPath, Alteryx, Professionalism, Autonomy, Verbal Communication, Written Communication, Organizational Skills, MultiTasking, Teamwork, Relationship Building, Delegation, Problem Solving, Adaptability, Critical Thinking, Data Visualization, Data Integration, Data Governance, DataOps, MLOps, DevOps","data analytics, business intelligence, data warehousing, data modeling, semantic model definition, star schema construction, cloud data analytics, azure, aws, sql, ddl, dml, views, functions, stored procedures, performance tuning, git, devops, linux, data lake medallion architecture, batch data ingestion, streaming data ingestion, ai algorithms, machine learning, automation tools, computer vision, net, c, python, java, scala, tabular modeling, microsoft fabric, power bi, azure analysis services, uipath, alteryx, professionalism, autonomy, verbal communication, written communication, organizational skills, multitasking, teamwork, relationship building, delegation, problem solving, adaptability, critical thinking, data visualization, data integration, data governance, dataops, mlops, devops","adaptability, ai algorithms, alteryx, automation tools, autonomy, aws, azure, azure analysis services, batch data ingestion, business intelligence, c, cloud data analytics, computer vision, critical thinking, data governance, data integration, data lake medallion architecture, dataanalytics, datamodeling, dataops, datawarehouse, ddl, delegation, devops, dml, functions, git, java, linux, machine learning, microsoft fabric, mlops, multitasking, net, organizational skills, performance tuning, powerbi, problem solving, professionalism, python, relationship building, scala, semantic model definition, sql, star schema construction, stored procedures, streaming data ingestion, tabular modeling, teamwork, uipath, verbal communication, views, visualization, written communication"
Data Center Sr. Analyst,Ascension,"Glendale, WI",https://www.linkedin.com/jobs/view/data-center-sr-analyst-at-ascension-3778872472,2023-12-17,Grafton,United States,Mid senior,Onsite,"Details
Department: Infrastructure
Schedule: Full time, 8 hour day shift, Monday - Friday
Location: Glendale, WI
Benefits
Paid time off (PTO)
Various health insurance options & wellness plans
Retirement benefits including employer match plans
Long-term & short-term disability
Employee assistance programs (EAP)
Parental leave & adoption assistance
Tuition reimbursement
Ways to give back to your community
Please note, benefits and benefits eligibility can vary by position, exclusions may apply for some roles (for example: PRN, Short-Term Option, etc.). Actual compensation offer will vary based upon role, education, experience, location, and qualifications. Connect with your Talent Advisor for additional specifics.
Responsibilities
Work in the Data Center to bring servers and switches online
Ensure Power redundancy and proper cable installation
Maintain hardware onsite including, servers, storage, switches and routers, large UPS and HVAC units, and generators
Install Vertiv 1500 UPS’s in Network Closest throughout the Hospitals
Demonstrate troubleshooting skills through resolution
Maintain inventory of all hardware (Open DCIM)
Rack, Server, Switch installation
Respond to user problems by listening, clarifying, and responding effectively.
Troubleshoot most application problems independently.
Possess and apply working knowledge of at least one technology environment to address and resolve problems.
Analyze a chain of events and apply technical knowledge following established procedures and/or detailed specifications.
Willing to travel when needed
Requirements
Education:
High School diploma equivalency with 2 years of cumulative experience OR Associate's degree/Bachelor's degree OR 4 years of applicable cumulative job specific experience required.
Additional Preferences
No additional preferences.
#ascensiontechnologies
Why Join Our Team
When you join Ascension, you join a team of over 150,000 individuals across the country committed to a Mission of serving others and providing compassionate, personalized care to all. Our inclusive culture, continuing education programs, career coaches and benefit offerings are just a few of the resources and tools that team members can use to create a rewarding career path. In fact, Ascension spent nearly $46 million in tuition assistance alone to support associate growth and development. If you are looking for a career where you can grow and make a difference in your community, we invite you to join our team today.
Equal Employment Opportunity Employer
Ascension will provide equal employment opportunities (EEO) to all associates and applicants for employment regardless of race, color, religion, national origin, citizenship, gender, sexual orientation, gender identification or expression, age, disability, marital status, amnesty, genetic information, carrier status or any other legally protected status or status as a covered veteran in accordance with applicable federal, state and local laws.
For further information, view the EEO Know Your Rights (English) poster or EEO Know Your Rights (Spanish) poster.
As a military friendly organization, Ascension promotes career flexibility and offers many benefits to help support the well-being of our military families, spouses, veterans and reservists. Our associates are empowered to apply their military experience and unique perspective to their civilian career with Ascension.
Pay Non-Discrimination Notice
Please note that Ascension will make an offer of employment only to individuals who have applied for a position using our official application. Be on alert for possible fraudulent offers of employment. Ascension will not solicit money or banking information from applicants.
E-Verify Statement
This employer participates in the Electronic Employment Verification Program. Please click the E-Verify link below for more information.
E-Verify
Show more
Show less","Vertiv 1500 UPS, Open DCIM, HVAC units, Network Closest","vertiv 1500 ups, open dcim, hvac units, network closest","hvac units, network closest, open dcim, vertiv 1500 ups"
Part-Time Data Analyst (Entry Level),Staffingandrecruiting,"Shawinigan, Quebec, Canada",https://ca.linkedin.com/jobs/view/part-time-data-analyst-entry-level-at-staffingandrecruiting-3729288891,2023-12-17,Trois-Rivières, Canada,Mid senior,Remote,"Minimum 1 year of work experience - fully remote position. Freshers are also encouraged to apply.
About us: The Future of AI is Patterned We are a stealth-mode technology startup that is revolutionizing the way AI is used. Our platform uses pattern recognition to train AI models that are more accurate, efficient, and robust than ever before.
We are backed by top investors and we are hiring for almost everything! If you are passionate about AI and want to be a part of something big, then we want to hear from you.
Make a positive impact on the world. Be a part of a fast-growing startup. If you are interested in learning more, please visit our website.
We Are Looking For People Who Are
Passionate about AI.
Excellent problem solvers.
Team players.
Driven to succeed.
Requirements
Role Responsibilities:
Work in close collaboration with the Business Intelligence Lead, Federal Data Lead, and other Program teams
Develop, maintain, and improve BI tools, build and enhance standard operating procedures (SOPs)
Manage various data sets and active Google workbooks with adjacent contract teams, monitor and analyze financial health information at the project and program levels
Communicate with client leadership to assess data needs and emerging requirements
Work with large data sets, workbooks, and spreadsheets to manipulate and manage program-level information using macros, queries, scripts, etc.
Gather requirements and lead the development of long-term data management tools, processes, and solutions based on organizational needs.
Be comfortable working with collaboration tools such as; Google Suite, Microsoft Office
Providing general support to the client including, but not limited to, analysis, data calls, financial management, risk management, audits, and project management-related tasks.
Qualifications
Bachelor's Degree in business, business intelligence, data or information management, or similar.
Proficient in Google Scripts
Minimum 1 year of data or information management and/or data analysis experience.
Experience using Microsoft Excel and Google Sheets (macros, imports, query functions).
Experience with developing Google App Script is a plus.
Experience using SQL Developer is a plus.
Excellent written and verbal communication skills.
Willing to work in an administratively manual environment while working towards automation of processes in the future.
Clearable (able to pass both a criminal background check and credit check).
Highly motivated, self-learner, and technically inquisitive
Benefits
Special Benefits you will love:
Flexible vacation paid unlimited holidays and paid sick days
401(k) with up to 2% employer match
Health, vision, and dental insurance
Why Patterned Learning AI?
Patterned Learning AI is made up of incredibly bright, mission-driven coworkers who are passionate about using technology to solve real-world problems---and we're growing quickly. In order to continue building an engaging and dynamic organization, we're committed to giving everyone the support they need to do great work.
We believe diverse perspectives and backgrounds are critical to building great technology, and our goal is to cultivate an environment where people feel equally valued and respected. Patterned Learning AI is proud to be an equal opportunity workplace, and we welcome applicants from all backgrounds regardless of race, color, ancestry, religion, gender identity or expression, sexual orientation, marital status, age, citizenship, socioeconomic status, disability, or veteran status.
Powered by Webbtree
Show more
Show less","Google Sheets, Microsoft Office, Google Scripts, SQL Developer, Data Analysis, Data Management, Business Intelligence, Microsoft Excel, App Script, Google Suite, Macros, Queries, Scripts","google sheets, microsoft office, google scripts, sql developer, data analysis, data management, business intelligence, microsoft excel, app script, google suite, macros, queries, scripts","app script, business intelligence, data management, dataanalytics, google scripts, google sheets, google suite, macros, microsoft excel, microsoft office, queries, scripts, sql developer"
Data Insights Manager - Remote | WFH,Get It Recruit - Information Technology,"Irvine, CA",https://www.linkedin.com/jobs/view/data-insights-manager-remote-wfh-at-get-it-recruit-information-technology-3777379102,2023-12-17,Garden Grove,United States,Mid senior,Remote,"Our Background: Established in 2011, we are a dynamic data and analytics technology company committed to making consumer insights accessible to businesses of all sizes. Recognized as one of the ""top 26 L.A. tech companies to watch"" in 2023 by Built In, we have developed a cutting-edge SaaS ecosystem that empowers businesses with comprehensive shopper behavior and opinion data, unveiling unprecedented insights into consumer behavior.
Our Offering: Leveraging the nation's most downloaded, highest-rated, and exclusively Apple-approved app and web data collection and survey app, Surveys On The Go, we have seamlessly integrated market research and data science through our platform, Studio. Studio is the exclusive hub for real-time app, web, and foot traffic data combined with validated consumer surveys, providing unparalleled insights to capture the attention of the modern consumer.
Our Mission: We aim to eliminate data blind spots in market research while respecting consumers' privacy rights, enabling marketers to make informed decisions about products, advertising strategies, and competitive positioning. Trusted by numerous leading brands, including Google, Microsoft, Samsung, Walmart, Disney, Spotify, and Lowes.
Our Values: Guided by principles of Simplicity, Humility, Quality, Consistency, and Innovation.
Position: Manager, Data Insights
Company: MFour Mobile Research
Location: Fully remote work opportunity (Company HQ in CA, position open anywhere in the US)
Job Summary: MFour Mobile Research is seeking a Manager, Data Insights to lead our dedicated team of data analysts, BI developers, and data scientists. The role involves mentoring team members, overseeing product deliverables, recruiting top talent, and cultivating new skill sets. The Manager will identify meaningful trends and anomalies, providing valuable insights for key business decisions. Collaborating closely with product and customer service teams, the Manager will influence product strategy and customer service processes based on data analysis. Additionally, the role involves partnering with data engineering for data needs, conducting deep-dive analysis, and leading efforts for consumer segmentation and analysis.
Requirements
Bachelor's degree in Computer Science, Information Systems, or related quantitative field
3 years' experience in a similar role or as a data analyst, systems analyst, systems engineer, or in a closely related field
Experience with tools such as Excel, SQL Service, R, Python, Power BI, and/or Tableau
Salary: $130,152 per year
If you are passionate about data insights, ready to shape the future of market research, and thrive in a collaborative and innovative environment, we invite you to join our dynamic team. Apply now to be part of a company at the forefront of transforming consumer insights!
Employment Type: Full-Time
Show more
Show less","Data Analysis, Data Analytics, BI Development, Data Engineering, Market Research, Data Science, App Data Collection, Web Data Collection, Survey App, Product Strategy, Customer Service, Consumer Segmentation, Computer Science, Information Systems, Quantitative Analysis, Microsoft Excel, SQL, R Programming, Python, Power BI, Tableau","data analysis, data analytics, bi development, data engineering, market research, data science, app data collection, web data collection, survey app, product strategy, customer service, consumer segmentation, computer science, information systems, quantitative analysis, microsoft excel, sql, r programming, python, power bi, tableau","app data collection, bi development, computer science, consumer segmentation, customer service, data engineering, data science, dataanalytics, information systems, market research, microsoft excel, powerbi, product strategy, python, quantitative analysis, r programming, sql, survey app, tableau, web data collection"
BI Data Analyst [Hybrid],SmartIPlace,"Atlanta, GA",https://www.linkedin.com/jobs/view/bi-data-analyst-hybrid-at-smartiplace-3712850513,2023-12-17,Smyrna,United States,Associate,Onsite,"Title: BI Data Analyst [Hybrid]
Location: Atlanta , GA
Experience: 10+ years
Visa: USC, GC, EAD-GC ( Family Sponsor)
Skills
BI,
MS Azure
data modeling,
API,
SQL Server,
Azure Pipeline
Kubernetes
Mongo DB
Show more
Show less","BI, MS Azure, Data Modeling, API, SQL Server, Azure Pipeline, Kubernetes, MongoDB","bi, ms azure, data modeling, api, sql server, azure pipeline, kubernetes, mongodb","api, azure pipeline, bi, datamodeling, kubernetes, mongodb, ms azure, sql server"
Data Science and Analyst,Agile Staffing Groups,"Atlanta, GA",https://www.linkedin.com/jobs/view/data-science-and-analyst-at-agile-staffing-groups-3716364426,2023-12-17,Smyrna,United States,Associate,Onsite,"Or direct client, a leading In-home Senior Living assessments, curated products, and custom home modification services company is looking for a Senior Data Analyst with Data Science in Marietta, GA.
Below Are The Details
Job title: Sr. Data Analyst with Data Science
Location: Marietta, GA (hybrid)
Company: a leading In-home Senior Living assessments, curated products, and custom home modification services company
Essential Job Responsibilities
Analytical support across the supply chain, product development, and warehouse functions.
Research and evaluate areas of opportunity for cost reduction and margin expansion.
Develop decision support models to drive margin, business growth, and market assortment.
Execute market research and impact analysis for new/existing product assortment opportunities.
Investigate supply chain pain points and develop and implement productivity solutions.
Identify, design, and implement supply chain focused process improvements.
Develop and maintain executive KPI and performance dashboard.
Support development and execution of a supply chain replenishment methodology and stock level reorder points.
Source, analyze and visualize data using tools that effectively mine insights and communicate findings to rapidly inform business decisions.
Monitoring supply chain and product performance metrics to understand the impact on the business and develop programs that act on these opportunities.
Required Skills
Exceptional data analysis and problem-solving skills.
Advance proficiency with analytics, reporting, and data visualization tools that structure large data sets to find usable information (Advanced Excel, Power BI, Python, Tableau, Jedox).
Ability to execute market research and provide viable options with clear impact and risk analysis.
Ability to build scalable solutions to analyze large sets of date and to communicate and visualize that data in a simple, actionable way.
Ability to be flexible and execute in ambiguous environment with a positive ‘can-do’ attitude.
Advanced proficiency in Microsoft Office Suite, specifically Excel and PowerPoint
Demonstrated ability to work collaboratively with cross functional teams both within and outside the organization.
Ability to create presentations and reports based on recommendations and findings.
Exceptional verbal, presentation, and written communication skills.
Ability to persuade, inform, and influence others based on findings.
Excellent attention to detail with strong time management and organizational skills.
Preferred Qualification
Working knowledge of scripting/statistical programming language (SQL, VBA, R, SAS) desired.
Sixth sense for “meaningful data” with the ability to find creative, pragmatic solutions to data- oriented problems.
Prior experience managing sales-service based programs or in the home remodeling Industry a plus.
Candidate should be located in Atlanta Market, 15-20% travel will be required.
Candidate Experience
Bachelor’s degree in Statistics/Analytics, Engineering, or Supply Chain preferred.
Minimum of 3 years of proven supply chain or data science/analytics experience
Advanced Excel and presentation skills are required.
Show more
Show less","Data Analysis, Problem Solving, Analytics, Reporting, Data Visualization, Advanced Excel, Power BI, Python, Tableau, Jedox, Market Research, Scalable Solutions, SQL, VBA, R, SAS, Statistical Programming, Microsoft Office Suite, PowerPoint, Presentations, Reports, Communication, Attention to Detail, Time Management, Organizational Skills","data analysis, problem solving, analytics, reporting, data visualization, advanced excel, power bi, python, tableau, jedox, market research, scalable solutions, sql, vba, r, sas, statistical programming, microsoft office suite, powerpoint, presentations, reports, communication, attention to detail, time management, organizational skills","advanced excel, analytics, attention to detail, communication, dataanalytics, jedox, market research, microsoft office suite, organizational skills, powerbi, powerpoint, presentations, problem solving, python, r, reporting, reports, sas, scalable solutions, sql, statistical programming, tableau, time management, vba, visualization"
Data Conversion Analyst (Workday),Parin Technologies,"Atlanta, GA",https://www.linkedin.com/jobs/view/data-conversion-analyst-workday-at-parin-technologies-3770709729,2023-12-17,Smyrna,United States,Associate,Onsite,"Title: Data Conversion Analyst (Workday)
Location: Atlanta, GA - Hybrid
Duration: 18 months
Job Details
Under general supervision, maintains and troubleshoots computers, servers, peripherals, and/or applications. Implements consistent procedures and standards for general system maintenance in consultation with the Information Systems staff.
The client provides accounting and financial reporting expertise and is statutorily responsible for financial reporting. They also provides enterprise oversight and management of the Enterprise Resource Planning (ERP) system, accounting policy development, financial business process improvement recommendations, and management of the payroll and accounting shared service centers.
Position Description
Under general supervision, the Data Conversion Analyst will work closely with a group of talented Business Analysts and Technical Specialists with the objective of extracting and transforming data from various ERP and related systems into Workday.
This is a time-limited, at-will employment position assigned to a project with an approximate 3-year life cycle.
Primary Duties & Responsibilities
Develop/test/execute data conversion extractions, transformations, and processes within an established
conversion framework in a timely and accurate manner.
Develop and maintain data conversion documentation.
Write and maintain automation scripts as needed.
Verify data integrity and identify data cleanliness issues.
Reconcile converted data to ensure accuracy.
Communicate effectively with management.
Develop good working relationships with departmental staff in order to troubleshoot technical and functional issues that may arise during the implementation process.
Skills And Competencies
Solid grasp of relational database structures and strong use of SQL
Familiarity with a variety of database types and interfaces (Microsoft Access, Oracle, various text formats,
ODBC).
Solid understanding and usage of Microsoft Excel
Strong troubleshooting skills and analytical thinking to resolve issues.
Positive, proactive, take-charge attitude and attention to detail.
Strong time management skills with the ability to meet deadlines for multiple, concurrent projects.
Ability to quickly gain working knowledge of product, interpret requirements, and recommend solutions.
Excellent verbal and written communication skills.
Minimum Qualifications
Bachelor's degree in computer science or a related field from an accredited college or university AND One year of job-related experience.
Preferred Qualifications
Preference may be given to applicants who, in addition to meeting the Minimum Qualifications, possess the following:
Experience with PeopleSoft, Oracle, Taleo, Workday or other SaaS and On-Premises ERP systems.
Experience in Human Resource, Financial, or Procurement domains
Can work in a fast paced, fast-growth, high-energy environment and take care of multiple high priority activities concurrently.
Ability to balance multiple projects and initiatives simultaneously.
Standout colleague who can collaborate and communicate optimally with all partners, i.e. developers, technical operations, and customers.
Possess excellent verbal and written communication skills.
Passion for data accuracy and completeness.
Show more
Show less","Workday, SQL, Relational database structures, Microsoft Access, Oracle, Microsoft Excel, ODBC, PeopleSoft, Taleo, Oracle, SaaS, OnPremises ERP systems, Human Resource, Financial, Procurement, ERP","workday, sql, relational database structures, microsoft access, oracle, microsoft excel, odbc, peoplesoft, taleo, oracle, saas, onpremises erp systems, human resource, financial, procurement, erp","erp, financial, human resource, microsoft access, microsoft excel, odbc, onpremises erp systems, oracle, peoplesoft, procurement, relational database structures, saas, sql, taleo, workday"
Data Analyst,Kennesaw State University,"Kennesaw, GA",https://www.linkedin.com/jobs/view/data-analyst-at-kennesaw-state-university-3783679133,2023-12-17,Smyrna,United States,Associate,Onsite,"Financial Aid Analyst
Please apply online at:
https://careers.hprod.onehcm.usg.edu/psc/careers/CAREERS/HRMS/c/HRS_HRAM_FL.HRS_CG_SEARCH_FL.GBL?Page=HRS_APP_JBPST_FL&Action=U&FOCUS=Applicant&SiteId=43000&JobOpeningId=266465&PostingSeq=1
About Us
Are you ready to join a community leading in innovative teaching and learning? Kennesaw State University is one of the 50 largest public institutions in the country. With our growing enrollment and global reach, we are enjoying increased prominence among Georgia's colleges. We offer more than 150 undergraduate, graduate, and doctoral degrees to over 43,000 students. Our entrepreneurial spirit, high-impact research, and Division I athletics also draw students from throughout the region and from 92 countries across the globe. Our university's vibrant culture, career opportunities, and rich benefits make us an employer of choice. We are searching for talented people to join our mission. Come to KSU and find your passion.
Location
Our Kennesaw campus is located at 1000 Chastain Road NW, Kennesaw, GA 30144.
Job Summary
Provides data analysis, management, and reporting for Office Student Financial Aid (OSFA). Identify requirements, collect data, and interpret results for Federal and State funded programs. Leads projects and initiate and implement process improvements. Assists in developing and administering policies and procedures, extensive scope and impact of decisions both internal and external contacts, and mission of the department. Ensures compliance with relevant regulatory requirements.
Responsibilities
KEY RESPONSIBILITIES:
1.Leads and oversees projects that implement process improvement
2.Collects, analyzes and provides accurate data and reports to support decision making and functional activities
3.Assesses and analyzes underlying data elements to identify causes of potential problems and proposes solutions
4.Responsible for the execution and management of individual programs and/or processes
5.Reviews and audits completed work to ensure accuracy and compliance¿ facilitates training as needed
6.Trouble-shoots and maintains quality assurance processes within assigned financial aid area
7.Monitors communications from students to review for possible system and/or training issues
8.Ensures compliance with all federal, state, NCAA, USG and Kennesaw State rules and regulations
9.Contributes and participates in the strategic planning process and future direction of the
department/division
10.Works collaboratively with campus-wide departments and outside entities regarding the implementation of policies and procedures
11.Serves as a point of contact to resolve questions or issues
12.Maintains a master calendar for all OSFA major activity
13.Establishes strategies for effective delivery of program awards
Required Qualifications
Educational Requirements
Bachelor's Degree from an accredited institution of higher education in Communications, Finance, Statistics, Business or related field
Required Experience
Three (3) years related experience in financial aid, post-secondary education, or business
Preferred Qualifications
Additional Preferred Qualifications
National Association of Student Financial Aid Administrators (NASFAA) Credentials
Preferred Educational Qualifications
Master's degree from an accredited institution of higher education in related field
Preferred Experience
Previous experience with Banner (Ellucian)
Proposed Salary
The budgeted salary range is $60,920 to $69,500. Offers are based on relevant experience.
Comprehensive benefits to include Medical, Dental, Vision, 13 paid holidays, vacation, generous retirement plans, tuition waiver, wellness options, and much more! Visit hr.kennesaw.edu/benefits.
Knowledge, Skills, & Abilities
ABILITIES
Able to handle multiple tasks or projects at one time meeting assigned deadlines
Able to understand, research, and resolve complex issues related to various federal and state systems, including but not limited to an understanding of Central Processing System, Electronic Data Exchange, National Student Loan Data System, Common Origination and Disbursement, Surfer
Able to plan, coordinate and complete large scale, detail-oriented projects and events
Able to work in partnership with, staff, faculty, administrators, federal/state entities, and servicers to ensure student success
Able to maintain compliant records of all transactions between the University and outside reporting entities
Able to lead compliance efforts involving federal, state, and institutional financial aid
KNOWLEDGE
Knowledge of financial aid at the federal, state and institutional levels and stays abreast of changes to state and federal rules/regulations
Knowledge of state program rules and regulations for Financial Aid in the State of Georgia
Knowledge of Title IV rules and regulations, as they pertain to Federal Student Financial Aid
Knowledge of and abides by the regulations of the Family Education Rights and Privacy Act (FERPA)
SKILLS
Excellent interpersonal, initiative, teamwork, problem solving, independent judgment, organization, communication (verbal and written), time management, project management and presentation skills
Proficient with computer applications and programs associated with the position (i.e., Microsoft Office suite, Ellucian Banner Financial Aid Module, USG GA Modifications to Banner, Common Origination and Disbursement, Central Processing System, Surfer, and the National Student Loan Data System)
Strong attention to detail and follow up skills
Strong customer service skills and phone and e-mail etiquette
Equal Employment Opportunity
Kennesaw State University is an Equal Employment Opportunity Employer.
Show more
Show less","Data Analysis, Reporting, Microsoft Office suite, Ellucian Banner Financial Aid Module, USG GA Modifications to Banner, Common Origination and Disbursement, Central Processing System, Surfer, National Student Loan Data System, FERPA, Title IV rules and regulations, Financial Aid regulations, Customer service skills, Phone and email etiquette, Project management, Presentation skills, Interpersonal skills, Initiative, Teamwork, Problem solving, Independent judgment, Organization, Communication, Time management","data analysis, reporting, microsoft office suite, ellucian banner financial aid module, usg ga modifications to banner, common origination and disbursement, central processing system, surfer, national student loan data system, ferpa, title iv rules and regulations, financial aid regulations, customer service skills, phone and email etiquette, project management, presentation skills, interpersonal skills, initiative, teamwork, problem solving, independent judgment, organization, communication, time management","central processing system, common origination and disbursement, communication, customer service skills, dataanalytics, ellucian banner financial aid module, ferpa, financial aid regulations, independent judgment, initiative, interpersonal skills, microsoft office suite, national student loan data system, organization, phone and email etiquette, presentation skills, problem solving, project management, reporting, surfer, teamwork, time management, title iv rules and regulations, usg ga modifications to banner"
"Senior Software Engineer, Orders Data Platform",Square,"Atlanta, GA",https://www.linkedin.com/jobs/view/senior-software-engineer-orders-data-platform-at-square-3784938288,2023-12-17,Smyrna,United States,Associate,Remote,"Company Description
Since we opened our doors in 2009, the world of commerce has evolved immensely, and so has Square. After enabling anyone to take payments and never miss a sale, we saw sellers stymied by disparate, outmoded products and tools that wouldn’t work together.
So we expanded into software and started building integrated, omnichannel solutions – to help sellers sell online, manage inventory, offer buy now, pay later functionality through Afterpay, book appointments, engage loyal buyers, and hire and pay staff. Across it all, we’ve embedded financial services tools at the point of sale, so merchants can access a business loan and manage their cash flow in one place. Afterpay furthers our goal to provide omnichannel tools that unlock meaningful value and growth, enabling sellers to capture the next generation shopper, increase order sizes, and compete at a larger scale.
Today, we are a partner to sellers of all sizes – large, enterprise-scale businesses with complex operations, sellers just starting, as well as merchants who began selling with Square and have grown larger over time. As our sellers grow, so do our solutions. There is a massive opportunity in front of us. We’re building a significant, meaningful, and lasting business, and we are helping sellers worldwide do the same.
Job Description
We are the Orders Data Platform team, a team whose mission is to help merchants of all sizes gain insights from their sales data through a variety of interfaces. We sit at the center of critical domains and data flows, and we’re building a multi-layered platform to achieve our goals with Square-wide impact. We’re looking for a senior engineer who can help us build new platform features to dramatically improve user-facing search and reporting experiences at Square.
We provide a unified view of Orders/Orders-adjacent data at Square, and power a variety of interfaces for our teams and third-party developers to access that data. You’ll work with teams to understand requirements, and ensure the platform we’re building works across several distinct use cases.
To power that view of Orders, we’re also building a general-purpose Elasticsearch- and GraphQL-based search and reporting platform that can operate at Square scale. This general-purpose platform already powers search and reporting for a variety of teams at Square in production, and we’re continuing to evolve it for our collective use cases. We build in the open, and look forward to open-sourcing this platform in 2024 for even wider impact.
You will:
Work with Product and partners across Square to identify platform requirements, and work within the engineering team to develop the corresponding features
Provide high-quality hands-on contributions across multiple code bases
Identify technical and architectural end states for the project, and influence/evolve the code case in those directions
Play a key role in choosing technical investments for the team
Qualifications
You Have:
5+ years of software development experience
Familiarity with architecting/implementing Java-/Kotlin-based backend services
Strong product intuition and interest, with platform-building experience
Strategic leadership experience on medium/large-scale software projects
Interest and experience in mentoring other engineers
Even better:
Experience with real-time data streaming platforms such as Kafka and Kinesis
Experience and familiarity with Kotlin, GraphQL, Elasticsearch, and AWS technologies
Proficiency in large-scale Ruby projects, or a history and interest in quickly learning new technologies and stacks
History of contributions to open-source projects
Familiarity with the payments-processing domain
Technologies we use within Orders Data Platform:
Java, Kotlin, Ruby
GraphQL
Elasticsearch, DynamoDb
Terraform, AWS Lambda, SQS, Cloudwatch
Additional Information
Block takes a market-based approach to pay, and pay may vary depending on your location. U.S. locations are categorized into one of four zones based on a cost of labor index for that geographic area. The successful candidate’s starting pay will be determined based on job-related skills, experience, qualifications, work location, and market conditions. These ranges may be modified in the future.
Zone A: USD $167,300 - USD $204,500
Zone B: USD $158,900 - USD $194,300
Zone C: USD $150,600 - USD $184,000
Zone D: USD $142,200 - USD $173,800
To find a location’s zone designation, please refer to this resource. If a location of interest is not listed, please speak with a recruiter for additional information.
Full-time employee benefits include the following:
Healthcare coverage (Medical, Vision and Dental insurance)
Health Savings Account and Flexible Spending Account
Retirement Plans including company match
Employee Stock Purchase Program
Wellness programs, including access to mental health, 1:1 financial planners, and a monthly wellness allowance
Paid parental and caregiving leave
Paid time off (including 12 paid holidays)
Paid sick leave (1 hour per 26 hours worked (max 80 hours per calendar year to the extent legally permissible) for non-exempt employees and covered by our Flexible Time Off policy for exempt employees)
Learning and Development resources
Paid Life insurance, AD&D, and disability benefits
These benefits are further detailed in Block's policies. This role is also eligible to participate in Block's equity plan subject to the terms of the applicable plans and policies, and may be eligible for a sign-on bonus. Sales roles may be eligible to participate in a commission plan subject to the terms of the applicable plans and policies. Pay and benefits are subject to change at any time, consistent with the terms of any applicable compensation or benefit plans.
US and Canada EEOC Statement
We’re working to build a more inclusive economy where our customers have equal access to opportunity, and we strive to live by these same values in building our workplace. Block is a proud equal opportunity employer. We work hard to evaluate all employees and job applicants consistently, without regard to race, color, religion, gender, national origin, age, disability, pregnancy, gender expression or identity, sexual orientation, citizenship, or any other legally protected class.
We believe in being fair, and are committed to an inclusive interview experience, including providing reasonable accommodations to disabled applicants throughout the recruitment process. We encourage applicants to share any needed accommodations with their recruiter, who will treat these requests as confidentially as possible.
Want to learn more about what we’re doing to build a workplace that is fair and square? Check out our
I+D page
.
Additionally, we consider qualified applicants with criminal histories for employment on our team, and always assess candidates on an individualized basis.
We’ve noticed a rise in recruiting impersonations across the industry, where individuals are sending fake job offer emails. Contact from any of our recruiters or employees will always come from an email address ending with @block.xyz, @squareup.com, @tidal.com, or @afterpay.com, @clearpay.co.uk.
Block, Inc. (NYSE: SQ) is a global technology company with a focus on financial services. Made up of Square, Cash App, Spiral, TIDAL, and TBD, we build tools to help more people access the economy. Square helps sellers run and grow their businesses with its integrated ecosystem of commerce solutions, business software, and banking services. With Cash App, anyone can easily send, spend, or invest their money in stocks or Bitcoin. Spiral (formerly Square Crypto) builds and funds free, open-source Bitcoin projects. Artists use TIDAL to help them succeed as entrepreneurs and connect more deeply with fans. TBD is building an open developer platform to make it easier to access Bitcoin and other blockchain technologies without having to go through an institution.
Show more
Show less","Java, Kotlin, Ruby, GraphQL, Elasticsearch, DynamoDB, Terraform, AWS Lambda, SQS, CloudWatch, Kafka, Kinesis","java, kotlin, ruby, graphql, elasticsearch, dynamodb, terraform, aws lambda, sqs, cloudwatch, kafka, kinesis","aws lambda, cloudwatch, dynamodb, elasticsearch, graphql, java, kafka, kinesis, kotlin, ruby, sqs, terraform"
AI Data Scientist,Greenlight,Atlanta Metropolitan Area,https://www.linkedin.com/jobs/view/ai-data-scientist-at-greenlight-3772642354,2023-12-17,Smyrna,United States,Associate,Remote,"Greenlight is the leading family fintech company on a mission to help parents raise financially smart kids. We proudly serve more than 6 million parents and kids with our award-winning banking app for families. With Greenlight, parents can automate allowance, manage chores, set flexible spend controls, and invest for their family’s future. Kids and teens learn to earn, save, spend wisely, and invest.
At Greenlight, we believe every child should have the opportunity to become financially healthy and happy. It’s no small task, and that’s why we leap out of bed every morning to come to work. Because creating a better, brighter future for the next generation depends on it.
We are looking for a
Data Scientist
to join our Data Science and Insights team at Greenlight. In this role you will be responsible for building AI/ML models that support both internal and external business use cases. This role will be working on applying Vector Embeddings and Large Language Models to solve Greenlight business challenges. As we look to build on this nascent and fast-growing technology you will have a unique opportunity to enable us to apply Generative AI in an ethical and exciting way to fulfill our mission of helping parents teach kids about the world of finance.
The Data Science and Insights team is focused on delivering cross-functional and deep insights to drive better business decisions and better customer experiences. This role will work closely with the ML Engineering team as well on the deployment and ongoing tuning of the models.
This role reports to the Director of Data Science and Insights.
Your day-to-day:
Working on the full lifecycle of AI model building from problem definition, data exploration, modeling, and deployment
Applying internal data sources layered with open data sources to customize Generative AI experiences for internal and external use cases
Building models that adequately balance solving the business problem while controlling operational costs
Applying novel vector embedding techniques and Large Language models
Work with ML Engineering team to ensure models deploy to production and monitor ongoing performance
Pull, aggregate, and clean data from a variety of data sources to be used in modeling efforts
Partner with product teams to understand model use cases and design experiments to measure the effectiveness of models built and find improvement opportunities
What you’ll bring to the team:
Expertise in Data Science and Machine Learning
Prior experience in Natural Language Processing techniques is also required
Prior experience with building customer facing data science models is strongly preferred
Deep knowledge and understanding of Large Language Models and Vector Embeddings as well as knowledge on how to apply techniques including Resource Augmented Generation and Fine-tuning
Strong experience in languages such as Python, Spark
Strong experience in data analysis and data retrieval
Excellent SQL skills
Experience with common Data Science models and packages
Experience working on models at scale and balancing performance with computational resource constraints
A passion for curiosity and continuous learning. Expectation is to maintain ongoing knowledge of the latest developments in the space
Strong organizational skills, able to juggle multiple initiatives
Must be a team player, willing to help and be flexible
Not sure this one’s for you? Don’t count yourself out. Show us what you’ve got and we’ll reach out if there’s a great fit.
Work perks at Greenlight:
Medical, dental, vision, and HSA match
Paid life insurance, AD&D, and disability benefits
Traditional 401k with company match and Roth IRA
Unlimited PTO
Paid company holidays and pop-up bonus holidays
Wellness & professional development stipends
Mental health resources
1:1 financial planners
Fertility healthcare
100% paid parental and caregiving leave, plus cleaning service and meals during your leave
Flexible WFH, both remote and in-office opportunities
Fully stocked kitchen, catered lunches, and occasional in-office happy hours
Employee resource groups
Who we are:
It takes a special team to aim for a never-been-done-before mission like ours. We’re looking for people who love working together because they know it makes us stronger, people who look to others and ask, “How can I help?” and then “How can we make this even better?” If you’re ready to roll up your sleeves and help parents raise a financially smart generation, apply to join our team.
Greenlight is an equal opportunity employer and will not discriminate against any employee or applicant based on age, race, color, national origin, gender, gender identity or expression, sexual orientation, religion, physical or mental disability, medical condition (including pregnancy, childbirth, or a medical condition related to pregnancy or childbirth), genetic information, marital status, veteran status, or any other characteristic protected by federal, state or local law.
Show more
Show less","Data Science, Machine Learning, Natural Language Processing, Large Language Models, Vector Embeddings, Python, Spark, SQL, Data Analysis, Data Retrieval, Generative AI, Resource Augmented Generation, Finetuning","data science, machine learning, natural language processing, large language models, vector embeddings, python, spark, sql, data analysis, data retrieval, generative ai, resource augmented generation, finetuning","data retrieval, data science, dataanalytics, finetuning, generative ai, large language models, machine learning, natural language processing, python, resource augmented generation, spark, sql, vector embeddings"
"Data Scientist, gt.school (Remote) - $60,000/year USD",Crossover,"Atlanta, GA",https://www.linkedin.com/jobs/view/data-scientist-gt-school-remote-%2460-000-year-usd-at-crossover-3783187096,2023-12-17,Smyrna,United States,Associate,Remote,"Crossover is the world's #1 source of full-time remote jobs. Our clients offer top-tier pay for top-tier talent. We're recruiting this role for our client, gt.school. Have you got what it takes?
Are you ready to revolutionize education with the power of cutting-edge technology? We're seeking strategic problem solvers who thrive on developing and implementing innovative solutions and are eager to apply the power of generative AI to supercharge their work. Your consistent high performance and dedication will drive our mission to revolutionize education.
At gt.school, we're on a mission to accelerate the pace of learning by 5-10X, harnessing the incredible potential of technology, algorithms, and generative AI. We are creating a platform where students can discover, explore, and learn at an unprecedented rate. You'll play a pivotal role in bringing this vision to life. You'll have the opportunity to build new tools and operational processes, as well as streamline existing workflows for maximum efficiency.
But that's not all! At gt.school, we believe in your growth and development. You'll receive regular coaching and training from a team of global tech experts and operations professionals, ensuring your continuous improvement. We value collaboration and provide a supportive environment where you can meet with team members to work together on exciting projects.
If you're eager for a role that combines research, strategy, product development, and data science in the ever-evolving realm of education and technology, seize the moment and apply below. Let's embark on this exhilarating journey together!
What You Will Be Doing
Engineering generative AI prompts in Python/JavaScript/JSON
Analyze AI outputs and iterate on prompts to reach desired outcomes
Leverage generative AI and technology to develop new educational tools, iterate on current products, and finetune AI training models
Research topics to support the product engineering team
What You Won’t Be Doing
Collecting data or performing administrative tasks - we have a junior operations analyst team to do this type of work.
Working on unimportant projects - you'll be tasked with projects that are mission-critical
Data Scientist Key Responsibilities
Create technical functionalities that revolutionize the way education is delivered.
Basic Requirements
Excellent written and verbal English communication skills
Proficient in Python, R, JavaScript, or another OOP language
2+ years of work experience working in a technical field (science, technology, programming, financial services, data science, etc.)
Consistently able to work 40 hours per week, Monday through Friday, between 6AM and 6PM CST
Must be a proactive communicator who can effectively manage multiple priorities and stakeholders simultaneously
About Gt.school
GT School is an EdTech Startup that leverages technology, AI, and subject matter experts to cultivate a new way of learning. Our unique approach leverages 50+ years of learning science, cutting-edge data analytics and high-performance coaching. In doing so, we can help students learn more, learn faster, and learn better - and have fun while doing it. We are a remote-first company that hires globally via Crossover.
There is so much to cover for this exciting role, and space here is limited. Hit the Apply button if you found this interesting and want to learn more. We look forward to meeting you!
Working with Crossover
This is a full-time (40 hours per week), long-term position. The position is immediately available and requires entering into an independent contractor agreement with Crossover. The compensation level for this role is $30 USD/hour, which equates to $60,000 USD/year assuming 40 hours per week and 50 weeks per year. The payment period is weekly. Consult www.crossover.com/help-and-faqs for more details on this topic.
What to expect next:
You will receive an email with a link to start your self-paced, online job application.
Our hiring platform will guide you through a series of online “screening” assessments to check for basic job fit, job-related skills, and finally a few real-world job-specific assignments.
Important!
If you do not receive an email from us:
First, emails may take up to 15 minutes to send, refresh and check again.
Second, check your spam and junk folders for an email from Crossover.com, mark as “Not Spam” since you will receive other emails as well.
Third, we will send to whatever email account you indicated on the Apply form - by default, that is the email address you use as your LinkedIn username and it might be different than the one you have already checked.
If all else fails, just reset your password by visiting https://www.crossover.com/auth/password-recovery if you already applied using LinkedIn EasyApply.
Crossover Job Code: LJ-4880-US-Atlanta-DataScientist.008
Show more
Show less","Python, JavaScript, JSON, OOP, Data Science, Generative AI, Machine Learning, Algorithms, Data Analytics, Natural Language Processing","python, javascript, json, oop, data science, generative ai, machine learning, algorithms, data analytics, natural language processing","algorithms, data science, dataanalytics, generative ai, javascript, json, machine learning, natural language processing, oop, python"
"Senior Data Scientist, Risk",Square,"Atlanta, GA",https://www.linkedin.com/jobs/view/senior-data-scientist-risk-at-square-3775848431,2023-12-17,Smyrna,United States,Associate,Remote,"Company Description
Since we opened our doors in 2009, the world of commerce has evolved immensely, and so has Square. After enabling anyone to take payments and never miss a sale, we saw sellers stymied by disparate, outmoded products and tools that wouldn’t work together.
So we expanded into software and started building integrated, omnichannel solutions – to help sellers sell online, manage inventory, offer buy now, pay later functionality through Afterpay, book appointments, engage loyal buyers, and hire and pay staff. Across it all, we’ve embedded financial services tools at the point of sale, so merchants can access a business loan and manage their cash flow in one place. Afterpay furthers our goal to provide omnichannel tools that unlock meaningful value and growth, enabling sellers to capture the next generation shopper, increase order sizes, and compete at a larger scale.
Today, we are a partner to sellers of all sizes – large, enterprise-scale businesses with complex operations, sellers just starting, as well as merchants who began selling with Square and have grown larger over time. As our sellers grow, so do our solutions. There is a massive opportunity in front of us. We’re building a significant, meaningful, and lasting business, and we are helping sellers worldwide do the same.
Job Description
We are looking for a Data Scientist to join our Risk Machine Learning & Decision Science team. You'll build processes to root out high-risk activity across the Square platform of products. You will also manage top level Risk KPIs, establish core operational metrics, and make improvements through machine learning solutions or analysis.
The Risk Data Scientist will use analytical skills to identify and prevent the payment risks. You will lead experimentations to promote Risk effectiveness throughout Square. You will partner with product, engineering, operations, policies and sales to influence Squares global Risk road map and processes. You will have a chance to own the key metrics, develop data pipelines, ETL. You will also make direct impact to our key success metrics via building machine learning solutions, risk detection rule development and deployment.
You Will:
Diagnose problems and develop compelling, data-driven recommendations
Develop and maintain multiple data pipelines and ETLs
Partner with Product, Engineering, and operation teams to design solutions to business problems, influence product roadmaps, and solution new products/processes
Have a chance to use machine learning tools to develop data-driven solutions
Promote creative risk solutions through third-party evaluation and integration with a focus on improving the seller experience
Develop executive presentations for Squares leadership
Qualifications
A BS/BA in Statistics, Mathematics, Operations Research, Engineering, Computer Science, Economics, or a related quantitative/technical field
7+ years of relevant experience (or masters and 4+ years)
Experience with SQL, Python and Looker
Experience with machine learning model development and deployment
Experience driving data-driven solutions and project-managing their implementation
Experience answering unstructured questions and managing projects and tasks to a conclusion
A passion for Square's mission
Experience risk, trust and safety, payments, or spam prevention
Additional Information
Block takes a market-based approach to pay, and pay may vary depending on your location. U.S. locations are categorized into one of four zones based on a cost of labor index for that geographic area. The successful candidate’s starting pay will be determined based on job-related skills, experience, qualifications, work location, and market conditions. These ranges may be modified in the future.
Zone A: USD $152,100 - USD $185,900
Zone B: USD $144,500 - USD $176,700
Zone C: USD $136,900 - USD $167,300
Zone D: USD $129,300 - USD $158,100
To find a location’s zone designation, please refer to this resource. If a location of interest is not listed, please speak with a recruiter for additional information.
Full-time employee benefits include the following:
Healthcare coverage (Medical, Vision and Dental insurance)
Health Savings Account and Flexible Spending Account
Retirement Plans including company match
Employee Stock Purchase Program
Wellness programs, including access to mental health, 1:1 financial planners, and a monthly wellness allowance
Paid parental and caregiving leave
Paid time off (including 12 paid holidays)
Paid sick leave (1 hour per 26 hours worked (max 80 hours per calendar year to the extent legally permissible) for non-exempt employees and covered by our Flexible Time Off policy for exempt employees)
Learning and Development resources
Paid Life insurance, AD&D, and disability benefits
These benefits are further detailed in Block's policies. This role is also eligible to participate in Block's equity plan subject to the terms of the applicable plans and policies, and may be eligible for a sign-on bonus. Sales roles may be eligible to participate in a commission plan subject to the terms of the applicable plans and policies. Pay and benefits are subject to change at any time, consistent with the terms of any applicable compensation or benefit plans.
US and Canada EEOC Statement
We’re working to build a more inclusive economy where our customers have equal access to opportunity, and we strive to live by these same values in building our workplace. Block is a proud equal opportunity employer. We work hard to evaluate all employees and job applicants consistently, without regard to race, color, religion, gender, national origin, age, disability, pregnancy, gender expression or identity, sexual orientation, citizenship, or any other legally protected class.
We believe in being fair, and are committed to an inclusive interview experience, including providing reasonable accommodations to disabled applicants throughout the recruitment process. We encourage applicants to share any needed accommodations with their recruiter, who will treat these requests as confidentially as possible.
Want to learn more about what we’re doing to build a workplace that is fair and square? Check out our
I+D page
.
Additionally, we consider qualified applicants with criminal histories for employment on our team, and always assess candidates on an individualized basis.
Block, Inc. (NYSE: SQ) is a global technology company with a focus on financial services. Made up of Square, Cash App, Spiral, TIDAL, and TBD, we build tools to help more people access the economy. Square helps sellers run and grow their businesses with its integrated ecosystem of commerce solutions, business software, and banking services. With Cash App, anyone can easily send, spend, or invest their money in stocks or Bitcoin. Spiral (formerly Square Crypto) builds and funds free, open-source Bitcoin projects. Artists use TIDAL to help them succeed as entrepreneurs and connect more deeply with fans. TBD is building an open developer platform to make it easier to access Bitcoin and other blockchain technologies without having to go through an institution.
Show more
Show less","Data Science, Risk Management, Machine Learning, SQL, Python, Looker, Data Pipelines, ETL, Statistical Analysis, Datadriven Solutions, Project Management, Risk Detection, Rule Development, Executive Presentations, Statistics, Mathematics, Operations Research, Engineering, Computer Science, Economics","data science, risk management, machine learning, sql, python, looker, data pipelines, etl, statistical analysis, datadriven solutions, project management, risk detection, rule development, executive presentations, statistics, mathematics, operations research, engineering, computer science, economics","computer science, data science, datadriven solutions, datapipeline, economics, engineering, etl, executive presentations, looker, machine learning, mathematics, operations research, project management, python, risk detection, risk management, rule development, sql, statistical analysis, statistics"
"Healthcare Data Analyst or Senior Healthcare Data Analyst, Analytics Hub",ECG Management Consultants,"Atlanta, GA",https://www.linkedin.com/jobs/view/healthcare-data-analyst-or-senior-healthcare-data-analyst-analytics-hub-at-ecg-management-consultants-3785928309,2023-12-17,Smyrna,United States,Associate,Hybrid,"Overview
ECG is a national management consulting firm working exclusively in the healthcare industry. At ECG, our primary emphasis is on quality—in our people as well as our services—and we’re seeking others who appreciate our high standards of excellence.
What’s in It for You: Consult with Purpose
At ECG, you can have a bigger impact than you ever imagined. The work you do will help health systems deliver care more effectively and efficiently—and that’s just the beginning. We’re looking for innovators, problem-solvers, and self-starters to collaborate across our five divisions, take on challenging projects, and find new ways to improve patient care. ECG is committed to ensuring a friendly work environment that rewards high performance and welcomes, values, and supports all people.
Join Our Analytics Hub
Our growing Analytics Hub works with our consulting teams to deliver new insights, streamlined analysis, and world-class intelligence to the healthcare organizations we serve as clients. Combined with our consulting teams, you will be helping physician and executive leadership make informed decisions that can alter the course of their enterprises in areas such as physician alignment, mergers and acquisitions, service line planning, provider compensation, financial performance, transforming care models, patient access, managed care portfolio optimization, digital health expansion, and more. ECG uses a Microsoft Azure and Microsoft 365 environment, with Microsoft Power BI as ECG’s current analytics visualization platform. The goal of the position is to further capitalize on the existing ECG infrastructure, as well as implement new features and technologies. The ideal candidate is open to new challenges, exceptional at multitasking, and proficient at implementing new solutions.
Our detailed plans incorporate qualitative findings with data-driven strategic, operational, and financial considerations that enable organizations to pursue realistic change. Our consultants bridge the gap between strategic thinking and operational implementation with sensible action plans and tactical recommendations. This approach allows us to stretch our clients’ thinking while ensuring that initiatives with the broadest and deepest impact are prioritized and implementable. Our wide-ranging engagements often focus on helping our clients:
Position themselves for value-based care delivery.
Align hospitals and physician organizations.
Strengthen financial performance.
Enhance clinical programs.
Develop strategic partnerships and/or mergers.
Your Opportunity with ECG: Data Analyst or Senior Data Analyst
As a data analyst or senior data analyst, you will support the firm’s client delivery and business development efforts, working with consultants and senior leaders to manage large data sets, aggregate internal and external data, develop scalable models and other analyses, and support data visualization and presentation sets. In addition, the analyst will work directly with our internal support teams (IT, business development, and L&D) and associated vendors for data management and IT solutions and will assist with a range of innovative analytic opportunities. Here, no two days or projects are alike, which means you’ll have a lot to learn and plenty of support to help you succeed.
Your Responsibilities May Include:
Gathering and synthesizing data from various sources (e.g., national surveys, proprietary research, client interviews, industries, markets)
Preparing analyses related to hospital inpatient databases and provider claims data (Stratasan)
Building flexible, dynamic, and scalable financial and operational models to forecast trends, preparing scenarios for consulting efforts related to delivery and business development (e.g., strategic planning, operational improvement, market assessments)
Enhancing existing models and capabilities built using SQL, primarily, with some use of Python or R in certain models or scenarios
Performing complex analyses on big data, including cleaning, preparing, and interpreting
Identifying and executing analyses in response to consultant business intelligence inquiries
Building relational databases using a Microsoft SQL Server or Fabric
Communicating market insights to pursuit teams, and supporting business development efforts
Configuring, maintaining, and building flexible reports, queries, and visuals within the Power BI environment and the production and development environments of related databases
Researching, testing, and implementing solutions in Power BI
Partnering with various consulting units (strategy, academic health, provider financial services, performance transformation, etc.) to gather requirements related to data analytics
Developing, configuring, and maintaining interfaces to exchange data among Power BI, SQL databases, and other cloud-based applications
Creating and maintaining analytical support documentation and operating procedures
Developing new and innovative methodologies and approaches
Contributing to internal education and learning opportunities for consultants and operations members for the Analytics Hub and analytics in general, and supporting education about ECG’s use cases related to analytics in work
Assisting the director of data analytics in developing analytics and data strategy to support data analytics capabilities at the firm
Our Expectations of You
A bachelor’s degree in either computer science, computer engineering, mathematics, statistics, health information management, health administration, business, or a related degree that relies heavily on critical thinking, logic, and math
Prior data analytics work experience using databases, SQL, and Power BI in a healthcare or enterprise environment (five+ years of experience for senior analyst level; one to three years for analyst)
Experience using Python, R, Fabric, and Tableau a plus
Proficiency in Microsoft Word, Excel, and PowerPoint
Comfort with manipulating and synthesizing large data sets
Strong written and verbal communication skills
Excellent organizational skills
Job Locations
St. Louis and Washington, DC, offices are preferred. Other offices may include Atlanta, Boston, Chicago, Minneapolis, San Diego, or Seattle. Able to work a hybrid schedule with days in the office and remote.
Schedule
Full time/exempt
What You Can Expect Of Us
To reward our driven, innovative, and passionate employees, we’ve built a company culture that’s centered on performance. We offer an attractive compensation package, challenging work, and an entrepreneurial environment where you can take ownership of your career—and get out as much as you put in.
About ECG
ECG is a strategic consulting firm leading healthcare forward using knowledge and expertise built over the course of five decades to help clients see clearly where the industry is going and navigate toward success. We work as trusted, professional partners with hospitals, health systems, medical groups, and academic medical centers across the country. We thrive on delivering smart counsel and pragmatic solutions to the critical challenges facing healthcare providers. Client success is our primary objective. ECG’s national presence includes offices in Atlanta, Boston, Chicago, Dallas, Minneapolis, San Diego, Seattle, St. Louis, and Washington, DC.
Apply now and make an impact for years to come.
To begin the recruitment process, please submit your resume via our career site at https://careers.ecgmc.com.
ECG provides equal employment opportunities to all employees and applicants for employment without regard to sex, race, color, religion, national origin, citizenship, ancestry, age, disability, pregnancy, medical condition (cancer and genetic characteristics), genetic information, gender, gender identity or expression, sexual orientation, marital status, military or veteran status, or any other legally protected characteristic. People of color are encouraged to apply. We participate in E-Verify as part of our onboarding process. Having the permanent legal right to work in the United States is a condition of employment. ECG is not currently able to provide assistance to candidates requiring sponsorship or a visa.
Residents of the states of California or Washington may receive salary information for this job through this link or by contacting the recruiter directly at schavez@ecgmc.com.
Show more
Show less","Microsoft Azure, Microsoft 365, Microsoft Power BI, SQL, Python, R, Fabric, Tableau, Microsoft Word, Microsoft Excel, Microsoft PowerPoint, Strategic planning, Operational improvement, Market assessments, Big data, Data visualization, Data presentation, Data management, IT solutions, Data analytics, Data strategy, Data science, Data mining, Data modeling, Data warehousing, Business intelligence, Financial analysis, Operational analysis, Market analysis, Forecasting, Scenario planning, Risk assessment, Statistical analysis, Econometric analysis, Market research, Survey research, Competitive analysis, Industry analysis, Healthcare informatics, Health information management, Health administration, Business administration, Computer science, Computer engineering, Mathematics, Statistics","microsoft azure, microsoft 365, microsoft power bi, sql, python, r, fabric, tableau, microsoft word, microsoft excel, microsoft powerpoint, strategic planning, operational improvement, market assessments, big data, data visualization, data presentation, data management, it solutions, data analytics, data strategy, data science, data mining, data modeling, data warehousing, business intelligence, financial analysis, operational analysis, market analysis, forecasting, scenario planning, risk assessment, statistical analysis, econometric analysis, market research, survey research, competitive analysis, industry analysis, healthcare informatics, health information management, health administration, business administration, computer science, computer engineering, mathematics, statistics","big data, business administration, business intelligence, competitive analysis, computer engineering, computer science, data management, data mining, data presentation, data science, data strategy, dataanalytics, datamodeling, datawarehouse, econometric analysis, fabric, financial analysis, forecasting, health administration, health information management, healthcare informatics, industry analysis, it solutions, market analysis, market assessments, market research, mathematics, microsoft 365, microsoft azure, microsoft excel, microsoft power bi, microsoft powerpoint, microsoft word, operational analysis, operational improvement, python, r, risk assessment, scenario planning, sql, statistical analysis, statistics, strategic planning, survey research, tableau, visualization"
BI Data Analyst-locals|| LOCAL MUST,Steneral Consulting,"Atlanta, GA",https://www.linkedin.com/jobs/view/bi-data-analyst-locals-local-must-at-steneral-consulting-3712848363,2023-12-17,Smyrna,United States,Associate,Hybrid,"BI Data Analyst
[Atlanta, GA,
Hybrid onsite onr day a week
Atlanta area candidates only
6 mo
They are looking for 2 Data Analysts with Strong BI, data modeling, API, SQL Server, Azure Pipeline and MS Analysis Services experience in an Azure environment. Secondary skills are Kubernetes and Mongo/DB. These would be C2P - must go in office one day a week (preferably Wednesday) at Glenridge and 285 (old First Data building).
Must have 9/10 comm skills.
Show more
Show less","BI, Data Modeling, API, SQL Server, Azure Pipeline, MS Analysis Services, Kubernetes, MongoDB, Communication","bi, data modeling, api, sql server, azure pipeline, ms analysis services, kubernetes, mongodb, communication","api, azure pipeline, bi, communication, datamodeling, kubernetes, mongodb, ms analysis services, sql server"
Data Engineer,Expedite Technology Solutions LLC,"Atlanta, GA",https://www.linkedin.com/jobs/view/data-engineer-at-expedite-technology-solutions-llc-3768032943,2023-12-17,Smyrna,United States,Mid senior,Onsite,"We are seeking a highly motivated, forward thinking Lead Data Engineer to support the enterprise Collaboration and Support Center Manufacturing excellence group and the GP Decision Analytics Group. The role is focused on developing and automating data analytics solutions used across 150+ facilities within multiple divisions. GP CSC and the Decision Analytics Group functions as a Center of Excellence for all things related to manufacturing and commercial transformation for all of Georgia Pacific. This groups create sustainable value and competitive advantage by leveraging analytics, information technology, and actionable insights across the enterprise while focusing on futuristic possibilities of analytics. This role will have the opportunity to leverage the latest Big Data, Cloud and Analytics technologies and partner with our operations, engineering and data science community to expand our modeling and decision-making capabilities focused on asset health, asset optimization and process optimization.
Job Responsibilities
Hands-on lead for data engineering in the area of Quality and Operations for the Enterprise data and analytics team focused on managing the Data Lake and helping the business develop, deploy and manage predictive and prescriptive models to create business value through optimization of manufacturing facilities.
Develop critical data pipelines and data quality jobs in the AWS environment working with Lambda, Glue, Python, SQL and noSQL databases.
Enhance and optimize exiting data quality processes including automated testing and alerting on critical data assets.
Assist Data Science teams with preparing, cleansing, and delivering analytical datasets for machine learning models.
Monitor SAS and Python based models in production from a model quality perspective.
Participate in various engineering projects building data product tools utilizing batch and streaming data.
On-call support rotation, troubleshooting and improving existing Operations processes including logging, alerting, and monitoring.
Manage own learning and contribute to technical skill building of the team.
Develop deep technical expertise in the data movement patterns, practices and tools.
Demonstrate technical, team, and solution leadership through strong communication skills to recommend actionable, data-driven solutions
Collaborate with team members, business stakeholders and data SMEs to elicit requirements and to develop a technical design and then implement a solution.
Basic Qualifications
Bachelor's degree in Engineering (preferably Analytics, MIS or Computer Science). Master's degrees preferred.
Minimum 5 years of Data Engineering experience and 2-3 years working with AWS serverless technologies like AWS Lambda, AWS Glue, Kinesis, DynamoDB and Redshift.
At least 2 years of Python development experience. At least 2 years experience with using relational databases and SQL.
Data Concepts knowledge (ETL, near-/real-time streaming, data structures, data modeling, metadata, and workflow management).
Experience in Big Data projects utilizing EMR, Kinesis, Spark.
Proficiency in databases, SQL, and data warehousing concepts.
A passion and fearlessness for learning new technologies and putting them in practice.
Ability to thrive in a dynamic team environment.
Excellent written and verbal communications skills.
Ability to draw technical diagrams and present to the team.
Preferred Qualifications
Preferred Qualifications:
AWS certifications
Manufacturing IT systems knowledge (OSI PI)
Markup Languages (JSON, XML, YAML)
Code Management Tools (Git/GitHub, Bit Bucket, SVN, TFS)
SAS tools and scripting experience.
Experience with DevOps processes and tools like Terraform.
Ability to pull together complex and disparate data sources, warehouse those data sources and architect a foundation to produce BI and analytical content, while operating in a fluid, rapidly changing data environment.
Show more
Show less","Data Engineering, AWS, Lambda, Glue, Python, SQL, noSQL, SAS, Machine Learning, Data Science, Data Quality, Data Pipelines, Data Modeling, Data Warehousing, Big Data, EMR, Kinesis, Spark, JSON, XML, YAML, Git, GitHub, BitBucket, SVN, TFS, Terraform, ETL, Data Structures, Metadata, Workflow Management, DevOps","data engineering, aws, lambda, glue, python, sql, nosql, sas, machine learning, data science, data quality, data pipelines, data modeling, data warehousing, big data, emr, kinesis, spark, json, xml, yaml, git, github, bitbucket, svn, tfs, terraform, etl, data structures, metadata, workflow management, devops","aws, big data, bitbucket, data engineering, data quality, data science, data structures, datamodeling, datapipeline, datawarehouse, devops, emr, etl, git, github, glue, json, kinesis, lambda, machine learning, metadata, nosql, python, sas, spark, sql, svn, terraform, tfs, workflow management, xml, yaml"
Sr. ETL/Data Engineer Developer,TechTammina LLC,"Alpharetta, GA",https://www.linkedin.com/jobs/view/sr-etl-data-engineer-developer-at-techtammina-llc-3715676213,2023-12-17,Smyrna,United States,Mid senior,Onsite,"Role: Sr. ETL/Data Engineer Developer
Rate: Market
Local: Alpharetta, GA (Onsite hybrid )
Local candidates only / must have Batch Analytics, Multiple Full Lifecycle and Data Analytics
Our client is looking for a Sr. Developer to join their dynamic team. The ideal candidate will have previous experience as an ETL Developer or a Data Engineer.
Our client has a project in flight and is changing to a different stack. They are looking for someone with a strong background in Data Analytics and/or ETL Development as they move their applications to the cloud.
Experience
Python
Apache Airflow
Cloud
Multiple Full Lifecycle
Batch Analytics
Implementation Experience
Sincerely,
Nikhil Kanchi
Sr Technical Recruiter
Tech Tammina LLC |
4460 Brookfield Corporate Dr. Suite N, Chantilly, VA-20151
Phone:
571-685-2150
|
703-349-1053
E-mail: nikhil.kanchi@tammina.com
Show more
Show less","ETL, Data Engineering, Python, Apache Airflow, Cloud Computing, Batch Analytics, Full Lifecycle Implementation","etl, data engineering, python, apache airflow, cloud computing, batch analytics, full lifecycle implementation","apache airflow, batch analytics, cloud computing, data engineering, etl, full lifecycle implementation, python"
Senior Data Engineer (On-Site),PrismHR,"Duluth, GA",https://www.linkedin.com/jobs/view/senior-data-engineer-on-site-at-prismhr-3768117515,2023-12-17,Smyrna,United States,Mid senior,Onsite,"Do you have a passion for building data architectures that enable smooth and seamless product experiences? Are you an all-around data enthusiast with a knack for ETL? We're hiring Data Engineers to help build and optimize the foundational architecture of our product's data.
We’ve built a strong data engineering team to date, but have a lot of work ahead of us, including:
Migrating from relational databases to a streaming and big data architecture, including a complete overhaul of our data feeds
Defining streaming event data feeds required for real-time analytics and reporting
Leveling up our platform, including enhancing our automation, test coverage, observability, alerting, and performance
As a Senior Data Engineer, you will work with the development team to construct a data streaming platform and data warehouse that serves as the data foundations for our product.
Help us scale our business to meet the needs of our growing customer base and develop new products on our platform. You'll be a critical part of our growing company, working on a cross-functional team to implement best practices in technology, architecture, and process. You’ll have the chance to work in an open and collaborative environment, receive hands-on mentorship and have ample opportunities to grow and accelerate your career!
Responsibilities
Build our next generation data warehouse
Build our event stream platform
Translate user requirements for reporting and analysis into actionable deliverables
Enhance automation, operation, and expansion of real-time and batch data environment
Manage numerous projects in an ever-changing work environment
Extract, transform, and load complex data into the data warehouse using cutting-edge technologies
Build processes for topnotch security, performance, reliability, and accuracy
Provide mentorship and collaborate with fellow team members
Requirements
Bachelor’s or Master’s degree in Computer Science, Information Systems, Operations Research, or related field required
5+ years of experience building data pipelines
5+ years of experience building data frameworks for unit testing, data lineage tracking, and automation
Fluency in Scala is required
Working knowledge of Apache Spark
Familiarity with streaming technologies (e.g., Kafka, Kinesis, Flink)
Nice To Have
Experience with Machine Learning
Familiarity with Looker a plus
Knowledge of additional server-side programming languages (e.g. Golang, C#, Ruby)
PrismHR is a fast-paced SaaS company which provides customers with a cloud-based payroll process software application. PrismHR also provides professional services including system implementation consulting, custom configurations, and training. Lastly, via the Company’s Marketplace platform customers and end users access other human resources and employee benefits applications from PrismHR’s Marketplace Partners.
Diversity, Equity And Inclusion Program/Affirmative Action Plan
We have transformed our company into an inclusive environment where individuals are valued for their talents and empowered to reach their fullest potential. At PrismHR, we strive to continually lead with our values and beliefs that enable our employees to develop their potential, bring their full self to work, and engage in a world of inclusion.
Ensuring an inclusive environment for our employees is an integral part of the PrismHR culture. We aren't just checking a box, we are truly committed to creating a workplace that celebrates the diversity of our employees and fosters a sense of belonging for everyone. This is essential to our success. We are dedicated to building a diverse, inclusive, and authentic workplace, so if you’re excited about our roles but your past experience doesn’t align perfectly with every qualification in the job description, we encourage you to apply anyway. You may be just the right candidate for these open roles or other open roles. We particularly encourage applicants from traditionally under-represented groups as we seek to increase the diversity of our workforce and provide fair opportunities for all.
As a proud Equal Opportunity and Affirmative Action Employer, PrismHR encourages talent from all backgrounds to join our team. Employment decisions are based on an individual’s qualifications as they relate to the job under consideration. The Company’s policy prohibits unlawful discrimination based on sex (which includes pregnancy, childbirth, breastfeeding, or related medical conditions, the actual sex of the individual, or the gender identity or gender expression), race, color, religion, including religious dress practices and religious grooming practices, sexual orientation, national origin, ancestry, citizenship, marital status, familial status, age, physical disability, mental disability, medical condition, genetic information, protected veteran or military status, or any other consideration made unlawful by federal, state or local laws, ordinances, or regulations.
The Company is committed to complying with all applicable laws providing equal employment opportunities. This commitment applies to all persons involved in the operations of the Company and prohibits unlawful discrimination by any employee of the Company, including supervisors and co-workers.
Privacy Policy: For information about how we collect and use your personal information, please see our privacy statement available at https://www.prismhr.com/about/privacy-policy.
PrismHR provides reasonable accommodation for qualified individuals with disabilities and disabled veterans in job application procedures. If you have any difficulty using our online system and you need a reasonable accommodation due to a disability, you may use the following alternative email address to contact us about your interest in employment at PrismHR: taglobal@prismhr.com. Please indicate in the subject line of your email that you are requesting accommodation. Only candidates being considered for a position who require an accommodation will receive a follow-up response.
Show more
Show less","Data Engineering, ETL, Data Warehousing, Data Streaming, Realtime Analytics, Automation, Test Coverage, Observability, Alerting, Performance, Scala, Apache Spark, Kafka, Kinesis, Flink, Looker, Golang, C#, Ruby, Machine Learning","data engineering, etl, data warehousing, data streaming, realtime analytics, automation, test coverage, observability, alerting, performance, scala, apache spark, kafka, kinesis, flink, looker, golang, c, ruby, machine learning","alerting, apache spark, automation, c, data engineering, data streaming, datawarehouse, etl, flink, golang, kafka, kinesis, looker, machine learning, observability, performance, realtime analytics, ruby, scala, test coverage"
Data Engineer - Senior,Republic National Distributing Company,"Atlanta, GA",https://www.linkedin.com/jobs/view/data-engineer-senior-at-republic-national-distributing-company-3734515889,2023-12-17,Smyrna,United States,Mid senior,Onsite,"Republic National Distributing Company (RNDC) is a family-owned business with roots extending before Prohibition that has evolved into one of the nation's largest wine and spirits wholesalers. Our success is grounded in our core values of Family, Service, Accountability, Honesty, and Professionalism. We offer a vibrant, inclusive culture and workplace experience for individuals who want a career that makes them feel accomplished and engaged. RNDC values the health and well-being of our associates, inside and outside the office, offering dynamic health and wellness benefits that supply exceptional care and value. RNDC is geared toward growing our footprint and our people. Join our team of energetic professionals who believe in many happy hours and are experts in our craft.
**Summary**
The Senior Data Engineer manages and organises RNDC's enterprise data. They will translate requirements and designs into functional data pipelines while ensuring the continued quality and completeness of the information. Senior Data Engineers will combine raw information from different sources to create consistent and machine-readable datasets that are easy to analyze and support company initiatives. They will support other Data Engineers and Data Analysts on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They will also implement methods to improve data reliability and quality, improve data visibility and reduce effort through automation.
**In this role, you will**
Contribute on a team of data engineers through design, demand delivery, code reviews, release management, implementation, presentations, and meetings.
Mentor fellow data engineers and contribute to ongoing process improvements for the team
Evaluate business needs and objectives and align architecture/designs with business requirements
Build the data pipelines required for the optimal extraction, transformation, integration and loading of raw data from a wide variety of data sources
Assemble large, complex data sets and model our data in a way that meets functional / non-functional business requirements
Create data tools for analytics team members that assist them in generating innovative industry insights that provide our business a competitive advantage
Implement data tagging mechanisms and metadata management so data is accurately classified and visible to the organization
Build processes to help identify and improve data quality, consistency and effectiveness
Ensure our data is managed in a way that it conforms to all information privacy and protection policies
Use agile software development processes to iteratively make improvements to our data management systems
**What you bring to RNDC**
Bachelor's/Tech School degree in Computer Science, Information Systems, Engineering or equivalent and/or commensurate years of real-world experience in software engineering.
4+ years of relevant experience in data management
3+ years in data engineering with detailed knowledge of data warehouse technical architectures, infrastructure components, ETL/ ELT
Experience with performance analysis and optimization
Experience in data acquisition, transformation and storage design using design principles, patterns and best practices
**What's in it for you**
401(k) with company matching
Medical, dental and vision benefits\*
Generous paid time off program – work your way up to 5 weeks of PTO a year with the ability to carryover unused PTO
Paid volunteer time
Paid parental leave
Paid caregiver leave
Fertility benefits
Paid training
Company paid life insurance, short-term disability, and company-paid holidays
Associate resource groups, and diversity, equity, and inclusion programs available for all associates
\*Participation in these programs are subject to applicable wait periods and all plan and program terms and eligibility
COVID-19 Considerations
We follow CDC Guidelines and have a fun and safe environment for our teams.
**Bonus if you bring**
Data engineering certification is a plus
Previous experience in the Wine and Spirits industry
Republic National Distributing Company and National Distributing Company are Equal Opportunity/Affirmative Action employers. It is our policy not to discriminate against any Employee or Applicant. All qualified applicants will receive consideration for employment without regard to race, religion, color, national origin, sex, age, status as a protected veteran, among other things, or status as a qualified individual with disability. This policy of nondiscrimination in employment includes but is not limited to: recruitment, hiring, placement, promotion, transfer, employment advertising or solicitations, compensation, layoff or termination of employment.
RNDC is committed to providing reasonable accommodation to people with disabilities throughout the job application and interview process, to the point of undue hardship. If you require an accommodation during the application or interview process, please click here.
Show more
Show less","Data Engineering, Software Engineering, Agile Software Development, Data Warehouse, ETL/ELT, Performance Analysis, Data Acquisition, Data Transformation, Data Storage, Data Warehousing, SQL, R, Python, Hadoop, Spark, Pig, Hive, NoSQL, Data Lakes, Informatica, Talend, Tableau, Power BI","data engineering, software engineering, agile software development, data warehouse, etlelt, performance analysis, data acquisition, data transformation, data storage, data warehousing, sql, r, python, hadoop, spark, pig, hive, nosql, data lakes, informatica, talend, tableau, power bi","agile software development, data acquisition, data engineering, data lakes, data storage, data transformation, datawarehouse, etlelt, hadoop, hive, informatica, nosql, performance analysis, pig, powerbi, python, r, software engineering, spark, sql, tableau, talend"
"Data Engineer (InRiver) || Onsite 5 days a week in Kennesaw, GA",Steneral Consulting,"Kennesaw, GA",https://www.linkedin.com/jobs/view/data-engineer-inriver-onsite%C2%A05-days-a-week-in-kennesaw-ga-at-steneral-consulting-3693333352,2023-12-17,Smyrna,United States,Mid senior,Onsite,"Onsite 5 days a week in Kennesaw, GA
Candidates need to take a video screen with prime vendor prior to end client submittal
Must be local
What We Like About This Opportunity
Business Objectives & Key Project Details
Client is standing up a PIM platform InRiver. Both the Data Engineer and the Functional SME will be responsible for:
data flows, end-to-end processes that they need to capture.
Much of the data that needs to be consolidated and upload to PIM. Python, Access or other tools could be used to consolidate the data
The Data Engineer will need to work both a high level understanding the technical workflows of the 10-15 products as well as tactical vs data entry
Marine Marketing, US and Canada does not have a centralized catalog of products so from a technical standpoint, the Data Engineer will need to work with the BU's, eCommerce team to ensure that the data will be organized properly and read to be passed over to the Systems Integrator that will upload into InRiver or with a brand folder
Integration with print module
Both customers in Kennesaw and Canada are marketing folks. They lack functional and technical resource. Val-tek is the system integrator and doing the integration work. They need help with data collection, process flows. Outcome is to hand them a document so they can build the rules in the system. Mallory and Niteen will assist the resources.
One functional person that has had experience with the business produce use cases and document process flows
Technical resource needs data manipulation and excel macros experience with large data sets.
Have to build an excel upload to then push it into the InRiver system
Working with several thousand types of products with their data
A key measurement is reviewing missing data to ensure it is properly coded before it is uploaded into InRiver.
Each BU work with agency to create their own product line and work with dealers. Therefore, this technical resource must be able to look at data from all BU's to determine a common naming convention
loading the current 2023 data and 2024 data. Not looking at older data
Must be aware of working with French language. Not expecting them to work with French or localized content, but know that some context will be in French
Will need to request additional software to have on consultant laptop if they need python or other
Digital asset renaming and image renaming before it gets to the brand folder digital asset management (DAM). Must ensure the right formula to pull the information from the DAM to the PIM
Experience supporting 3rd party software in eCommerce or Sales & Marketing application areas (business / functional support analyst)
Experience with Access database to consolidate excel file content
Experience Technical writing 'end-user' operations guide and/or technical manual with guidance
Team player and can work with minimal supervision (preferred on-site)
Cleanse and upload data via Excel
Client's Job Description
Plan to an onboarding, meet and great. Come up with assignments with what they are working with. Walk them through the specifics for the ongoing meeting. Self starter and manage up.
Month of September, create test files for DAM. DAM naming convention correct, familiar with process flows, SI can build the workflows.
Excel and ppt for documentation
Will create a technical document
User guide how do you maintain the system from an end user perspective. Example how to set up a user.
System manual all the API's they will build. Internal systems, brand folders, permissions, rules they have written, configuration they have done and turn that to ISD. Hiring a third party vendor to manage the system.
Show more
Show less","InRiver, Python, Access, Excel, Macros, Data Manipulation, Data Consolidation, Data Upload, Technical Writing, EndUser Guide, PIM, Excel Upload, French Language, Digital Asset Management (DAM), ThirdParty Software, eCommerce, Sales & Marketing, API, System Integration, Onboarding, Meet and Greet, Assignments, Self Starter, Manage Up, User Guide, System Manual, Configuration, ISD, Third Party Vendor","inriver, python, access, excel, macros, data manipulation, data consolidation, data upload, technical writing, enduser guide, pim, excel upload, french language, digital asset management dam, thirdparty software, ecommerce, sales marketing, api, system integration, onboarding, meet and greet, assignments, self starter, manage up, user guide, system manual, configuration, isd, third party vendor","access, api, assignments, configuration, data consolidation, data manipulation, data upload, digital asset management dam, ecommerce, enduser guide, excel, excel upload, french language, inriver, isd, macros, manage up, meet and greet, onboarding, pim, python, sales marketing, self starter, system integration, system manual, technical writing, third party vendor, thirdparty software, user guide"
"Data Engineer/Lead - Dallas, NY/NJ, Chicago, IL, Atlanta, GA (Onsite for now but could be hybrid too)",Spanco Solutions,"Atlanta, GA",https://www.linkedin.com/jobs/view/data-engineer-lead-dallas-ny-nj-chicago-il-atlanta-ga-onsite-for-now-but-could-be-hybrid-too-at-spanco-solutions-3764508755,2023-12-17,Smyrna,United States,Mid senior,Onsite,"Data Engineer/Lead
12 Months contact
Locations: Dallas, NY/NJ, Chicago, IL, Atlanta, GA (Onsite for now but could be hybrid too)
Job Description
Must have: Python, AWS, Oracle and Snowflake
Experience writing Oracle PLSQL and performing data analysis
Exposure to Continuous Integration & Continuous Delivery (CI/CD) practices specifically in the data engineering space.
Exposure to test automation frameworks that support all types of testing (unit, component, integration, system etc.) executed through CI/CD pipelines.
Experiences in some of the following technologies continuous integration/delivery frameworks: GIT, Cloudbees Jenkins, uDeploy and/or Pivotal Concourse
You will operate as senior engineer on a squad
Bringing a passion for engineering and best engineering practices
Mentoring junior engineers
Collaborating with engineers across other products to share ideas and bring to bear solutions
You can write high performing and well written PLSQL with proficiency, quality, and passion
You understand middleware Java technologies and an aptitude to contribute as a Full Stack developer
You can use test-driven approach to developing software and can build or contribute to automated testing frameworks that can detect any anomalies in functionality, performance, and integration.
You have experiences in some of the following technologies continuous integration/delivery frameworks such as GIT, Cloudbees Jenkins, uDeploy and/or Pivotal Concourse.
You can clearly document design options and decisions, and present these in an organized manner to technical as well as non-technical audiences
You are a strong contributor with a collaborative work style in building, designing, and reviewing code
Show more
Show less","Python, AWS, Oracle, Snowflake, Oracle PLSQL, Data Analysis, Continuous Integration, Continuous Delivery, Test Automation Frameworks, Unit Testing, Component Testing, Integration Testing, System Testing, CI/CD Pipelines, GIT, Cloudbees Jenkins, uDeploy, Pivotal Concourse, Java, Full Stack Development, TestDriven Development, Automated Testing Frameworks, Middleware Java Technologies","python, aws, oracle, snowflake, oracle plsql, data analysis, continuous integration, continuous delivery, test automation frameworks, unit testing, component testing, integration testing, system testing, cicd pipelines, git, cloudbees jenkins, udeploy, pivotal concourse, java, full stack development, testdriven development, automated testing frameworks, middleware java technologies","automated testing frameworks, aws, cicd pipelines, cloudbees jenkins, component testing, continuous delivery, continuous integration, dataanalytics, full stack development, git, integration testing, java, middleware java technologies, oracle, oracle plsql, pivotal concourse, python, snowflake, system testing, test automation frameworks, testdriven development, udeploy, unit testing"
Sr AWS Data Engineer,Experient Group,"Atlanta, GA",https://www.linkedin.com/jobs/view/sr-aws-data-engineer-at-experient-group-3635153180,2023-12-17,Smyrna,United States,Mid senior,Onsite,"Work With Us
At Experient Group, we value community, collaboration and people who are willingto roll up their sleeves to get the job done. While functional and technical skills are critical, we place a priority on hiring people who match our values. Our philosophy is simple: we attract and hire talented people, then provide them with a supportive community, career opportunities and guidance from our experienced leadership so they can thrive. In short, we strive to serve our people better than anyone else.
We are seeking an
Sr. AWS Data Engineer
to join our development team. This individual will be responsible for being a lead for the team, and hands-on building, enhancing, and supporting multiple high-volume data pipelines used by our client to process business critical order transaction data.
What you’ll do
Building ETL / ELT and data streaming pipelines using Apache Airflow, Spark, Flink or similar technologies
Programming big data processing pipelines using Python on AWS lambdas or other pipelining technologies
Follow modern agile development standards from requirements to delivery
Maintain and support multiple databases, Data / Delta Lake, and the processing pipelines.
Mentor and guide more junior engineers
What you will bring
7+ years of experience using one or more of the following:
Apache Airflow
AWS Glue
/ PySpark
AWS
S3
based
Data Lakes
RDS
Redshift
AWS DMS
AWS Athena
Strong SQL Experience with Database Design and maintenance
Experience with programming languages such as Java or Python
Solid leadership experience
Strong analytical and problem-solving skills
Excellent verbal and written communication skills
Passion for learning and a passion for solving business problems with technology.
A strong record of academic accomplishment and a 4-year college degree with a major in Computer Science or related field
Strong team player, self-motivated, and willing to learn
Preferred
Experience using databases such as Postgres and MySQL
Experience using
Graph databases
Experience using NoSQL databases such as AWS DynamoDB
Experience using AWS' SQS and SNS services
Basic level experience with Active Directory or other LDAP based directory
About Us
Experient Group
is an Atlanta-based business + IT consultancy that offers flexible and innovative solutions tailored to our clients' business environment, culture and budget. We don't assume anything - we set out to continually prove ourselves by going above and beyond in every client engagement. We work collaboratively to understand our clients' business and provide guidance that enables them to achieve results.
Experient Group is an Equal Employment Opportunity (EEO) employer and welcomes all qualified applicants. Applicants will receive fair and impartial consideration without regard to race, sex, religion, national origin, age, sexual orientation, disability, veteran status, genetic data, or other legally protected status
Show more
Show less","AWS Glue, Spark, Flink, Python, Apache Airflow, SQL, Java, AWS, S3, Data Lakes, RDS, Redshift, DMS, Athena, Postgres, MySQL, Graph databases, NoSQL databases, AWS DynamoDB, SQS, SNS, Active Directory, LDAP","aws glue, spark, flink, python, apache airflow, sql, java, aws, s3, data lakes, rds, redshift, dms, athena, postgres, mysql, graph databases, nosql databases, aws dynamodb, sqs, sns, active directory, ldap","active directory, apache airflow, athena, aws, aws dynamodb, aws glue, data lakes, dms, flink, graph databases, java, ldap, mysql, nosql databases, postgres, python, rds, redshift, s3, sns, spark, sql, sqs"
Database Engineer,"Cybertec, Inc","Alpharetta, GA",https://www.linkedin.com/jobs/view/database-engineer-at-cybertec-inc-3768045657,2023-12-17,Smyrna,United States,Mid senior,Onsite,"Brokerage
Alpharetta, Georgia (2-3 days onsite per week DAY 1)
$65/hour on C2C
USC/GC preferred
Hybrid 2-3 days a week onsite
General Database Engineer Duties and Responsibilities
Provide guidance and support to Application Developers
Assist with schema design, code review, SQL query tuning
Install, tune, implement and upgrade DBMS installations
Write and deploy SQL patches
Upgrade and improve application schema and data upgrades
Process work accurately and in a timely manner
Always paying attention to the detail
Urgency
Proactively and regularly make recommendations for system improvements
Collate, prepare and present statistical information for internal and external use
Platforms we manage, Postgres, Oracle, Oracle ASM, SAP Sybase, Sybase RepServer, SAP Hana, Redis, Cassandra, Azure, AWS RDS, Python, Linux, KSH, Veritas Clustering
Write and Maintain Database Programs
Database engineers write new database programs and maintain existing programs to ensure they can handle the flow of traffic and the amount of data being stored in the database.
Design New Databases
Database engineers help design new databases based on company needs, data storage needs, and the number of users accessing the database.
Monitor Databases and Programs
Database engineers continuously monitor databases and related systems to ensure high functionality.
Write Programs and Scripts
Database engineers write new support programs and scripts for databases to deploy, operate, increase data storage capacity and fix existing database problems.
Troubleshooting
Database engineers troubleshoot platform specific performance, checking for potential problems that affect database functionality and related database programs.
Debugging
Database engineers look through coding language to debug existing database scripts and programs.
Review Database Reports
Database engineers review daily, weekly, and monthly database reports, including user reports and systems information, to spot problems and ensure that all databases and support systems are working at peak levels.
Requirements
Expertise in Installation, Upgrade & Patching of different components in Oracle, Sybase and experience with the other platforms we manage listed in the Duties and Responsibilities.
Experience with Replication including setup, troubleshooting issues and failovers
Monitor all systems (work processes, users, system logs, short dumps, locks, developer traces, system traces, disk space, etc.).
Ensure production uptime service levels are maintained and made available per requirements that include backup, recovery, refresh, performance tuning, and security
Provide data cleansing services, verify data integrity, and implement access controls
Provide routine maintenance including problem defect analysis and resolution, space, storage, object management, physical database layout, rollback segment management, and temporary space management
Experience with Schema creation, deployment and maintenance
Experience with Setting up and configuration of Auditing, audit trails
Carry out recovery of databases in an event of log full, system crash Troubleshoot issues related to the installation for our supported platforms
Should have knowledge of Disaster Recovery setup and maintenance
Automation using shell-script, python, go, etc. Performance & Tuning Related Expertise
Assist Developers and business analysts with their queries and tune them, if required
Experience with Performance analysis, monitoring of SQL queries and platform specific tuning.
Show more
Show less","Database Engineering, SQL, Python, Linux, KSH, Oracle, Oracle ASM, SAP Sybase, Sybase RepServer, SAP Hana, Redis, Cassandra, Azure, AWS RDS, Postgres, Automation, Shell Script, Go, Performance Tuning, SQL Tuning, Replication, Failover, Troubleshooting, Debugging, Monitoring, Data Cleansing, Data Integrity, Access Controls, Auditing, Disaster Recovery, Schema Creation, Schema Deployment, Schema Maintenance","database engineering, sql, python, linux, ksh, oracle, oracle asm, sap sybase, sybase repserver, sap hana, redis, cassandra, azure, aws rds, postgres, automation, shell script, go, performance tuning, sql tuning, replication, failover, troubleshooting, debugging, monitoring, data cleansing, data integrity, access controls, auditing, disaster recovery, schema creation, schema deployment, schema maintenance","access controls, auditing, automation, aws rds, azure, cassandra, data integrity, database engineering, datacleaning, debugging, disaster recovery, failover, go, ksh, linux, monitoring, oracle, oracle asm, performance tuning, postgres, python, redis, replication, sap hana, sap sybase, schema creation, schema deployment, schema maintenance, shell script, sql, sql tuning, sybase repserver, troubleshooting"
Sr Data Engineer,Pacer Staffing LLC,"Atlanta, GA",https://www.linkedin.com/jobs/view/sr-data-engineer-at-pacer-staffing-llc-3782817292,2023-12-17,Smyrna,United States,Mid senior,Onsite,"Description
Job Description- Data Catalog Tool Dev Engineer
PAY RANGE: XX80-XX90 (will be determined by manager after interviews)
BILL RANGE: XX116 to XX130.50 (will be update on WO)
SHIFT HOURS: 8:30am to 5pm
Interviews Are Required
PLEASE DO NOT SUBMIT ANY CANDIDATES UNTIL SPOTLIGHT CALL IS COMPLETED.
Summary
We are seeking a skilled and passionate Catalog Tool Dev Engineer to join our team and play a crucial role in developing and deploying custom data connectors and management scripts. As a Catalog Tool Dev Engineer, you will be responsible for designing, coding, and testing custom data connectors that seamlessly integrate external data sources into our cataloging platform. Additionally, you will develop and implement management scripts to automate tasks, enhance performance, and optimize our cataloging infrastructure.
Essential Duties And Responsibilities
Design and develop custom connectors using Alation's OCF Java SDK to import metadata into data catalog platform
Collaborate with data engineers and data architects to understand data source requirements and specifications
Architect and provide solutions for custom integration requirements
Automate tasks and optimize cataloging processes through the development of catalog management scripts
Troubleshoot and resolve connectivity issues, ensuring seamless data ingestion and synchronization
Adhere to coding standards and best practices to ensure code quality, maintainability, and scalability
Provide technical support and guidance to catalog users and integrated applications teams
Stay up to date on emerging technologies and trends in data connectivity and catalog management
Qualifications
6 to 10 years of experience in software development, data engineering, or a related field
Strong understanding of data connectivity principles and protocols, including REST APIs, ODBC, JDBC, and web scraping techniques
Proficient in programming languages such as Groovy, SQL, Python, Java, or C#, with experience in web development frameworks like Django or Spring
Proficiency in using version control systems, such as Git
Familiarity with data cataloging platforms and technologies, including Alation, Data Lakehouse platforms, and semantic search engines
Excellent problem-solving and analytical skills, with the ability to identify and resolve complex data connectivity challenges
Effective communication and collaboration skills, with the ability to work independently and as part of a cross-functional team
Additional Details
Have you already identified a candidate? : N/A Loaded Worker
Pre-Identified Candidate Name : (No Value)
Pre-Identified Candidate Contact Info : (No Value)
Pre-Identified Candidate Notes : (No Value)
Will this position require driving? : N/A Loaded Worker
Is the purpose of this Job Posting to replace/backfill a current contingent worker? : N/A Loaded Worker
Assignment Type : Temp to Hire
Is this a Union utilized position? : N/A Loaded Worker
Show more
Show less","Software Development, Data Engineering, Alation's OCF Java SDK, REST APIs, ODBC, JDBC, Web scraping, Groovy, SQL, Python, Java, C#, Django, Spring, Git, Data Catalog Platforms, Data Lakehouse Platforms, Semantic Search Engines, ProblemSolving, Analytical Skills, Communication, Collaboration","software development, data engineering, alations ocf java sdk, rest apis, odbc, jdbc, web scraping, groovy, sql, python, java, c, django, spring, git, data catalog platforms, data lakehouse platforms, semantic search engines, problemsolving, analytical skills, communication, collaboration","alations ocf java sdk, analytical skills, c, collaboration, communication, data catalog platforms, data engineering, data lakehouse platforms, django, git, groovy, java, jdbc, odbc, problemsolving, python, rest apis, semantic search engines, software development, spring, sql, web scraping"
Sr Data Engineer,Compunnel Inc.,"Atlanta, GA",https://www.linkedin.com/jobs/view/sr-data-engineer-at-compunnel-inc-3786802551,2023-12-17,Smyrna,United States,Mid senior,Onsite,"Description
Responsibilities:
Design and develop custom connectors using Alation's OCF Java SDK to import metadata into data catalog platform
Collaborate with data engineers and data architects to understand data source requirements and specifications
Architect and provide solutions for custom integration requirements
Automate tasks and optimize cataloging processes through the development of catalog management scripts
Troubleshoot and resolve connectivity issues, ensuring seamless data ingestion and synchronization
Adhere to coding standards and best practices to ensure code quality, maintainability, and scalability
Provide technical support and guidance to catalog users and integrated applications teams
Stay up to date on emerging technologies and trends in data connectivity and catalog management
Qualifications
6 to 10 years of experience in software development, data engineering, or a related field
Strong understanding of data connectivity principles and protocols, including REST APIs, ODBC, JDBC, and web scraping techniques
Proficient in programming languages such as Groovy, SQL, Python, Java, or C#, with experience in web development frameworks like Django or Spring
Proficiency in using version control systems, such as Git
Familiarity with data cataloging platforms and technologies, including Alation, Data Lakehouse platforms, and semantic search engines
Excellent problem-solving and analytical skills, with the ability to identify and resolve complex data connectivity challenges
Effective communication and collaboration skills, with the ability to work independently and as part of a cross-functional team
Education:
Bachelors Degree
Show more
Show less","Java, Alation's OCF Java SDK, Data Lakehouse platforms, Git, Django, Spring, SQL, Python, Java, C#, Groovy, ODBC, JDBC, REST APIs, Web scraping, Data cataloging, Data connectivity","java, alations ocf java sdk, data lakehouse platforms, git, django, spring, sql, python, java, c, groovy, odbc, jdbc, rest apis, web scraping, data cataloging, data connectivity","alations ocf java sdk, c, data cataloging, data connectivity, data lakehouse platforms, django, git, groovy, java, jdbc, odbc, python, rest apis, spring, sql, web scraping"
Data Engineer - Senior,Republic National Distributing Company,"Atlanta, GA",https://www.linkedin.com/jobs/view/data-engineer-senior-at-republic-national-distributing-company-3733464917,2023-12-17,Smyrna,United States,Mid senior,Onsite,"Republic National Distributing Company (RNDC) is a family-owned business with roots extending before Prohibition that has evolved into one of the nation's largest wine and spirits wholesalers. Our success is grounded in our core values of Family, Service, Accountability, Honesty, and Professionalism. We offer a vibrant, inclusive culture and workplace experience for individuals who want a career that makes them feel accomplished and engaged. RNDC values the health and well-being of our associates, inside and outside the office, offering dynamic health and wellness benefits that supply exceptional care and value. RNDC is geared toward growing our footprint and our people. Join our team of energetic professionals who believe in many happy hours and are experts in our craft.
Summary
The Senior Data Engineer manages and organises RNDC's enterprise data. They will translate requirements and designs into functional data pipelines while ensuring the continued quality and completeness of the information. Senior Data Engineers will combine raw information from different sources to create consistent and machine-readable datasets that are easy to analyze and support company initiatives. They will support other Data Engineers and Data Analysts on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They will also implement methods to improve data reliability and quality, improve data visibility and reduce effort through automation.
In this role, you will
Contribute on a team of data engineers through design, demand delivery, code reviews, release management, implementation, presentations, and meetings.
Mentor fellow data engineers and contribute to ongoing process improvements for the team
Evaluate business needs and objectives and align architecture/designs with business requirements
Build the data pipelines required for the optimal extraction, transformation, integration and loading of raw data from a wide variety of data sources
Assemble large, complex data sets and model our data in a way that meets functional / non-functional business requirements
Create data tools for analytics team members that assist them in generating innovative industry insights that provide our business a competitive advantage
Implement data tagging mechanisms and metadata management so data is accurately classified and visible to the organization
Build processes to help identify and improve data quality, consistency and effectiveness
Ensure our data is managed in a way that it conforms to all information privacy and protection policies
Use agile software development processes to iteratively make improvements to our data management systems
What You Bring To RNDC
Bachelor's/Tech School degree in Computer Science, Information Systems, Engineering or equivalent and/or commensurate years of real-world experience in software engineering.
4+ years of relevant experience in data management
3+ years in data engineering with detailed knowledge of data warehouse technical architectures, infrastructure components, ETL/ ELT
Experience with performance analysis and optimization
Experience in data acquisition, transformation and storage design using design principles, patterns and best practices
What's In It For You
401(k) with company matching
Medical, dental and vision benefits*
Generous paid time off program – work your way up to 5 weeks of PTO a year with the ability to carryover unused PTO
Paid volunteer time
Paid parental leave
Paid caregiver leave
Fertility benefits
Paid training
Company paid life insurance, short-term disability, and company-paid holidays
Associate resource groups, and diversity, equity, and inclusion programs available for all associates
Participation in these programs are subject to applicable wait periods and all plan and program terms and eligibility
COVID-19 Considerations
We follow CDC Guidelines and have a fun and safe environment for our teams.
Bonus if you bring
Data engineering certification is a plus
Previous experience in the Wine and Spirits industry
Republic National Distributing Company and National Distributing Company are Equal Opportunity/Affirmative Action employers. It is our policy not to discriminate against any Employee or Applicant. All qualified applicants will receive consideration for employment without regard to race, religion, color, national origin, sex, age, status as a protected veteran, among other things, or status as a qualified individual with disability. This policy of nondiscrimination in employment includes but is not limited to: recruitment, hiring, placement, promotion, transfer, employment advertising or solicitations, compensation, layoff or termination of employment.
RNDC is committed to providing reasonable accommodation to people with disabilities throughout the job application and interview process, to the point of undue hardship. If you require an accommodation during the application or interview process, please click here.
Show more
Show less","Data Engineering, Data Management, Data Warehousing, ETL, ELT, Performance Analysis, Optimization, Data Acquisition, Data Transformation, Data Storage, Design Principles, Design Patterns, Best Practices, Data Tagging, Metadata Management, Data Quality, Data Consistency, Data Effectiveness, Information Privacy, Information Protection, Agile Software Development, Data Analytics, Data Visualization, Business Intelligence, Data Mining, Machine Learning, Artificial Intelligence, Cloud Computing, Big Data, Hadoop, Spark, Python, Java, SQL, NoSQL, Tableau, Power BI, Looker","data engineering, data management, data warehousing, etl, elt, performance analysis, optimization, data acquisition, data transformation, data storage, design principles, design patterns, best practices, data tagging, metadata management, data quality, data consistency, data effectiveness, information privacy, information protection, agile software development, data analytics, data visualization, business intelligence, data mining, machine learning, artificial intelligence, cloud computing, big data, hadoop, spark, python, java, sql, nosql, tableau, power bi, looker","agile software development, artificial intelligence, best practices, big data, business intelligence, cloud computing, data acquisition, data consistency, data effectiveness, data engineering, data management, data mining, data quality, data storage, data tagging, data transformation, dataanalytics, datawarehouse, design patterns, design principles, elt, etl, hadoop, information privacy, information protection, java, looker, machine learning, metadata management, nosql, optimization, performance analysis, powerbi, python, spark, sql, tableau, visualization"
Sr. Data Engineer,aquesst,"Brookhaven, GA",https://www.linkedin.com/jobs/view/sr-data-engineer-at-aquesst-3784872248,2023-12-17,Smyrna,United States,Mid senior,Onsite,"Senior Data Engineer
Permanent Opportunity in Atlanta, GA
Overview
We are looking for a Sr. Data Engineer to execute our Data Strategy roadmap. This role will be part of a new team accountable for building our critical data ecosystem. You will bring together several diverse data payloads from external partners and customers into specialized data stores. You will work in close collaboration with data science, platform engineering, and product teams to build ETL pipelines, MDM solutions, and access control mechanisms to enable effective governance of the data for product delivery. This is a unique opportunity to be the first in a new department at a rapidly scaling startup.
Responsibilities
Build and maintain scalable ETL pipelines effectively transform data from disparate data sources into a canonical data store
Design data architecture and partner with our infrastructure team to execute the build out of the data foundation
Operate current data ingest services while building the next generation of ETL pipelines
Define a common data vocabulary across the organization
Develop audit reports to provide insights into overall system health and resiliency
Contribute to the technology roadmap
Requirements
Bachelor’s degree in Information Technology, Computer Science, or related field
5+ years as a Data Engineer working on large scale ETL systems
AWS experience developing Integrations and analytical services
Demonstrated proficiency in Python and SQL Spark/PySpark
Experience on distributed system & No SQL databases
Experience with one or more commercial or open-source technologies like Databricks, Informatica, Talend, Airflow, Kafka is essential
Experience collaborating with data scientists and data analyst on projects related to forecasting, dashboards, and reports
Passionate about mentoring other team members and foster a culture of open feedback and communication
Able to communicate effectively with a technical and non-technical audience
Experience building web-based reporting solutions with data visualization technologies like Power BI, Tableau etc. is a plus
Able to contribute to prototypes and architecture by contributing code and designs
Experience working with Product Management in an Agile environment and break down complex features into manageable user stories
Demonstrated track record building data solutions based on a data lake is a plus
AWS, Azure, or GCP cloud certification
Databricks certification
Prior experience with object-oriented programming languages
Experience on search engines (Elasticsearch/Algolia)
Compensation
$160,000 to $180,000, depending on experience
10% bonus
Must be local and eligible without sponsorship.
To apply, please email your resume to gracie@aquesst.com .
Show more
Show less","Data Engineer, ETL pipelines, MDM solutions, SQL, Spark/PySpark, Python, Data architecture, Distributed Systems, NoSQL databases, Databricks, Kafka, Data analysis, Forecasting, Dashboards, Reporting, Power BI, Tableau, Agile, Data lake, Objectoriented programming, Cloud certification (AWS Azure GCP), Databricks certification, Elasticsearch, Algolia","data engineer, etl pipelines, mdm solutions, sql, sparkpyspark, python, data architecture, distributed systems, nosql databases, databricks, kafka, data analysis, forecasting, dashboards, reporting, power bi, tableau, agile, data lake, objectoriented programming, cloud certification aws azure gcp, databricks certification, elasticsearch, algolia","agile, algolia, cloud certification aws azure gcp, dashboard, data architecture, data lake, dataanalytics, databricks, databricks certification, dataengineering, distributed systems, elasticsearch, etl pipelines, forecasting, kafka, mdm solutions, nosql databases, objectoriented programming, powerbi, python, reporting, sparkpyspark, sql, tableau"
Senior Data Engineer,Salesloft,"Atlanta, GA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-salesloft-3758968945,2023-12-17,Smyrna,United States,Mid senior,Onsite,"Salesloft, Inc. has a job opening for the position of Senior Data Engineer based out of its location in Atlanta, GA.
Job Duties:
Liaise between the Data Science and Product Delivery teams to architect and deliver end-to-end data
pipelines. Develop data flows, services, and applications in various system areas including machine learning, natural
language processing, data analytics and third party data integrations. Participate in design sessions for data services
and applications. Collaborate with data scientists and machine learning engineers to train and deploy models in a
production environment. Performs heads down development, test code, review peer code, and ship it to production.
Maintain and debug services and applications in a production environment. Work closely with our Product
Management and our Customer Success teams. May work remotely from any location within the United States.
Requirements:
Master’s degree in data analytics, computer science, or related technical field; plus 2 years of
experience in the job offered, as a Data Engineer, or in a related position involving hands-on software experience in
a high volume distributed systems environment. Requires demonstrated experience in: Working with teams of 4+
data engineers; 2 years’ experience developing applications with Python; 2 years developing data pipelines
leveraging Apache Spark; 2 years developing ETL pipelines in a large data environment; Coding and design with
exposure to micro-service-architecture (MSA); Object-oriented methodologies, UML design techniques, event
driven architectures, and other design patterns; Agile methodologies such as Scrum and Kanban, or comparable;
Ability to troubleshoot, performance tune, solve problems and write unit tests; Ability to communicate effectively
(both oral and written) with experience producing technical and design documentation; SQL and experience
working with relational databases; Analysis and working with unstructured datasets; and Machine learning or
artificial intelligence or data sciences frameworks.
It is Salesloft’s intent to pay all Lofters competitive wages and salaries that are motivational, fair and equitable. The goal of Salesloft’s compensation program is to be transparent, attract potential employees, meet the needs of all current employees, and encourage employees to stay with our organization.
Actual compensation packages are based on several factors that are unique to each candidate, including but not limited to skill set, depth of experience, certifications, and specific work location.
The salary offered for this job opportunity is $145,000. Transparency Disclosure for Informational Purposes: the overall base salary range for this position at Salesloft in the United States is $99,000 to $170,000.
The total compensation package for this position may also include performance bonus, stock, benefits and/or other applicable incentive compensation plans.
Salesloft embraces diversity and invites applications from people of all walks of life. We are proud to be an Equal Opportunity Employer and provide equal employment opportunities to all employees and applicants without regard to race, color, religion, sex, age, national origin, disability, veteran status, pregnancy, sexual orientation, or any other characteristic protected by law.
Apply: https://salesloft.com/careers/
Show more
Show less","Data Analytics, Computer Science, Apache Spark, ETL Pipelines, Microservice Architecture, Objectoriented Programming, UML Design, Eventdriven Architecture, Agile Methodologies, Scrum, Kanban, Unit Testing, SQL, Relational Databases, Unstructured Datasets, Machine Learning, Artificial Intelligence, Data Science Frameworks, Python","data analytics, computer science, apache spark, etl pipelines, microservice architecture, objectoriented programming, uml design, eventdriven architecture, agile methodologies, scrum, kanban, unit testing, sql, relational databases, unstructured datasets, machine learning, artificial intelligence, data science frameworks, python","agile methodologies, apache spark, artificial intelligence, computer science, data science frameworks, dataanalytics, etl pipelines, eventdriven architecture, kanban, machine learning, microservice architecture, objectoriented programming, python, relational databases, scrum, sql, uml design, unit testing, unstructured datasets"
Senior Data Engineer,Professional Diversity Network,"Atlanta, GA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-professional-diversity-network-3785205031,2023-12-17,Smyrna,United States,Mid senior,Onsite,"Job Title
Senior Data Engineer
Overview
Overview
Interested in building the next generation platform that handles petabytes of data and powers data insight creation using cutting edge data science technology? Excited about using Big Data, AI/Machine Learning and Data Analytics to deliver actionable insights? We are a dynamic team of top-notch engineers who are committed to make a dent in the FinTech universe.
The Marketing Services (MS) Technology team is looking for a Senior Data Engineer who is passionate about data and analytics, highly motivated and can take technical ownership of data solutions with an emphasis on engineering efficiency and on-time delivery.
The ideal candidate is exposed to the fast-paced world of Big Data technology and has experience in building data solutions using new and emerging technologies while maintaining stability of the platform. The person will get a chance to work with diverse datasets and be on the cutting edge of transforming the way Mastercard captures, processes, stores and visualizes data.
In This Role, You Will
Develop batch and streaming solutions using various Big Data technologies
Be responsible for assessing technologies and approaches for ingestion, transformation and storage
Develop software utilizing open source technologies to interface with distributed and relational data sources
Work closely with team members from across Mastercard to identify functional and system requirements
All About You
The ideal candidate for this position should:
Have extensive experience in data engineering with proficiency in Spark, Python, PySpark, Hadoop and Kafka.
Have experience working with real-time or near real-time data ingestion
Have strong programming knowledge in Python, Java, or Scala.
Proven track record of delivering software & data engineering projects and willingness to lead the data engineering team as a subject matter expert.
Have excellent oral and written communication
Be familiar with Agile/Scrum methodologies
Have demonstrated ability to adapt to new technologies and learn quickly
Have a BS/BA degree in Computer Science, Information Systems or related field
99cc0d88-dfca-4b54-98da-c46107bc928e
Show more
Show less","Spark, Python, PySpark, Hadoop, Kafka, Realtime data ingestion, Java, Scala, Agile/Scrum","spark, python, pyspark, hadoop, kafka, realtime data ingestion, java, scala, agilescrum","agilescrum, hadoop, java, kafka, python, realtime data ingestion, scala, spark"
Senior Data Engineer,Miratech,"Atlanta, GA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-miratech-3620303898,2023-12-17,Smyrna,United States,Mid senior,Onsite,"Company Description
What We Do
Miratech helps visionaries to change the world. We are a global IT services and consulting company that brings together global enterprise innovation and start-up innovation. Today we support digital transformation for the largest enterprises on the planet.
By partnering with both large and small players, we stay at the leading edge of technology, remain nimble even as a global leader, and create technology that helps our clients further enhance their business. Our culture of Relentless Performance enables over 99% of Miratech’s engagements to succeed by meeting or exceeding scope, schedule, and/or budget objectives since our inception in 1989.
Job Description
We are searching for an accountable, multitalented Data Engineer to join our team, who will help us work on solutions and implement technologies.
Qualifications
6+ years experience in a Data Engineering role
4+ years of experience in Java and Python Development
Proficient in Java Collections, Spring Framework and Multi-threading
Solid understanding of cloud database architectures, schema development, data modeling – Snowflake.
Experience in developing database-driven web services/applications against SQL databases such as Oracle or MySQL
Good understanding of Git and agile practices.
Knowledge of Object-Oriented Programming
Strong debugging skills and troubleshooting experience in a complex technical setup
Detail-focused, with experience in reviewing technical documentation, diagrams, and plans in order to help meet and/or define requirements.
Able to communicate technical information verbally in a clear manner to both technical and non-technical stakeholders
Background in the Fintech industry preferred
Additional Desired Skills
BE/B.Tech in Computer Science or related technical field, or equivalent experience
We offer
Strong career opportunities for professionals
A variety of international projects and mobility across projects
Professional development support and professional certification opportunities
Competitive compensation, advanced bonus systems
Flexible working schedule with a remote possibility
Corporate, social and cultural events
Show more
Show less","Data Engineering, Java, Python, Java Collections, Spring Framework, Multithreading, Cloud database architectures, Schema development, Data modeling, Snowflake, Databasedriven web services/applications, SQL databases, Oracle, MySQL, Git, Agile practices, ObjectOriented Programming, Debugging, Troubleshooting, Technical documentation, Diagrams, Plans, Requirements, Technical communication, Fintech","data engineering, java, python, java collections, spring framework, multithreading, cloud database architectures, schema development, data modeling, snowflake, databasedriven web servicesapplications, sql databases, oracle, mysql, git, agile practices, objectoriented programming, debugging, troubleshooting, technical documentation, diagrams, plans, requirements, technical communication, fintech","agile practices, cloud database architectures, data engineering, databasedriven web servicesapplications, datamodeling, debugging, diagrams, fintech, git, java, java collections, multithreading, mysql, objectoriented programming, oracle, plans, python, requirements, schema development, snowflake, spring framework, sql databases, technical communication, technical documentation, troubleshooting"
Senior Database Developer,Aperia,"Alpharetta, GA",https://www.linkedin.com/jobs/view/senior-database-developer-at-aperia-3657165841,2023-12-17,Smyrna,United States,Mid senior,Onsite,"Summary
Aperia is a fast-paced, technology company started to improve efficiencies in compliance, risk, customer service operations in leading financial institutions. With our headquarters in Dallas and offices in Atlanta, Washington DC, and Vietnam, we're a global organization on a path to change the industries we participate. A career at Aperia promises great challenges, opportunity, culture and the environment to forge your own path ahead. Join us and discover what a great place Aperia is! Aperia offers a competitive salary package and a great work environment with excellent people. As a Senior Database Developer, you will be primarily responsible for analyzing different data streams, engineer data exports and imports, build ETL/ELT streams, support various data needs and end to end support for all data needs.
Responsibilities
Data requirement analysis, scope and documentation
Expertise in ETL and writing stored procedures
Experience in data migration
Proven experience in Banking domain
Should have extensive hands-on various data preparation techniques and with multi-technology database architecture
Strong analytical and problem-solving abilities
Excellent written and verbal communication, team building, and relationship building skills
Strong attention to detail and ability to spot inconsistencies in the data and work with teams to educate the same
Ability to work in a global collaborative team environment
Qualifications
Bachelor's degree in Computer Science
6+ years of related technical experience
Experience in the Financial or Electronic Payment Processing industry is a plus
Data mart experience is a plus
Eligibility Requirements
Must be willing to submit to a background investigation and drug test as part of the selection process.
Job Type
Full-time
Schedule
Monday to Friday
Benefits
Health insurance
Health savings account
Dental insuranc
Vision insurance
401(k) matching
Life insurance
Paid time off
Parental leave
Disability insurance
Childcare assistance
Education reimbursement
Fitness membership
Volunteer time off
This job description is not intended to be all-inclusive. An employee may also perform other reasonable related business duties as assigned by their immediate supervisor or management.
Principals only. Recruiters please don't contact this job poster. DO NOT contact us with unsolicited services or offers.
Show more
Show less","Data Analysis, ETL / ELT, Stored Procedures, Data Migration, Banking Domain, Data Preparation, Multitechnology Database Architecture, SQL, Business Intelligence, Data Warehousing, Data Quality, Communication, Teamwork, Problem solving, Bachelor's in Computer Science, 6+ years of experience","data analysis, etl elt, stored procedures, data migration, banking domain, data preparation, multitechnology database architecture, sql, business intelligence, data warehousing, data quality, communication, teamwork, problem solving, bachelors in computer science, 6 years of experience","6 years of experience, bachelors in computer science, banking domain, business intelligence, communication, data migration, data preparation, data quality, dataanalytics, datawarehouse, etl elt, multitechnology database architecture, problem solving, sql, stored procedures, teamwork"
"Senior Cloud Data Engineer, Fullstack",Presidio,"Atlanta, GA",https://www.linkedin.com/jobs/view/senior-cloud-data-engineer-fullstack-at-presidio-3715509255,2023-12-17,Smyrna,United States,Mid senior,Onsite,"SEIZE THE OPPORTUNITY TO BE A PART OF SOMETHING GREAT!
Presidio is on the leading edge of a technology-driven movement to transform the way business is done, for our customers and our customers' customers. Joining Presidio means immersing yourself in a culture of self-starters, collaborators and innovators who make real, lasting change in the marketplace via cutting-edge technology and business solutions. At Presidio, we know that it’s our people that make the connections happen.
WHY YOU SHOULD JOIN US?
You will set your career on track for outstanding achievement with a company that knows no limits. Presidio is a leading a global digital services and solutions provider focused on Digital Infrastructure, Business Analytics, Cloud, Security & Emerging solutions.
THE ROLE: Fullstack Data Engineer
Job Summary
As a
Data Engineer,
you will be responsible for expanding and optimizing data and data pipeline architecture, as well as optimizing data flow and collection for cross-functional teams.
Travel Requirements
This is a fully remote position
This role may require up to 5% Travel.
Job Responsibilities
Create and maintain optimal data pipeline architecture,
Assemble large, complex data sets that meet functional / non-functional business requirements.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Required Skills
Demonstrated work experience with handling streaming/timeseries data using AWS Kinesis/AWS managed Kafka
Experience with other dashboarding solutions like Tableau, Qlik and/or similar programs is a plus.
Strong work experience on AWS Data Analytics stack - EMR, Redshift, Kinesis/managed Kafka, Glue
Additional Desired Skills
Experience on Azure and GCP data stack is definitely a plus
Strong communication to work with technical and non-technical stakeholders
Education And Experience
Bachelor's degree or equivalent experience and/or military experience
6+ years of advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
3+ years of work experience in developing data pipelines running on Spark
About Presidio
Presidio is committed to Diversity, Equity, and Inclusion at the highest levels and has strengthened its drive to build and drive systemic DEI change process across all levels of the organization.
Cultivating a culture of inclusion where the expression of all our differences are valued, celebrated, and add to our collective achievements.
Presidio is a global digital services and solutions provider accelerating business transformation through secured technology modernization. Highly skilled teams of engineers and solutions architects with deep expertise across cloud, security, networking and modern data center infrastructure help customers acquire, deploy and operate technology that delivers impactful business outcomes. Presidio is a trusted strategic advisor with a flexible full life cycle model of professional, managed, and support and staffing services to help execute, secure, operationalize and maintain technology solutions. We serve as an extension of our clients' IT teams, providing deep expertise and letting them focus on their core business. Presidio operates in 40+ US offices and offices in Ireland, London, Singapore, and India.
For more information visit: http://presidio.com
Presidio is an Equal Opportunity / Affirmative Action Employer / VEVRAA Federal Contractor. All qualified candidates will receive consideration for this position regardless of race, color, creed, religion, national origin, age, sex, citizenship, ethnicity, veteran status, marital status, disability, sexual orientation, gender identification or any other characteristic protected by applicable federal, state and local statutes, regulations and ordinances.
To read more about discrimination protections under Federal Law, please visit: https://www.dol.gov/ofccp/regs/compliance/posters/pdf/OFCCP_EEO_Supplement_Final_JRF_QA_508c.pdf
If you have any difficulty using our online system and need an accommodation in the job application process due to a disability, please send an email to recruitment@presidio.com for assistance.
Presidio is a VEVRAA Federal Contractor requesting priority referrals of protected veterans for its openings. State Employment Services, please provide priority referrals to recruitment@presidio.com.
Recruitment Agencies Please Note
Agencies/3 Parties may not solicit to any employee of Presidio. Any candidate information received from any Agency/3 Party will be considered a gift and property of Presidio, unless the Agency/3 Party is an Authorized Vendor of Presidio with an up-to-date Presidio Contract in hand signed by Presidio Talent Acquisition. No payment will be made to any Agency/3 Party who is not an Authorized Vendor, nor has specific approval in writing from Presidio Talent Acquisition to engage in recruitment efforts for Presidio.
Show more
Show less","AWS Kinesis, AWS Kafka, Tableau, Qlik, AWS EMR, AWS Redshift, AWS Glue, Apache Spark, SQL, Data Pipelines, AWS Data Analytics stack","aws kinesis, aws kafka, tableau, qlik, aws emr, aws redshift, aws glue, apache spark, sql, data pipelines, aws data analytics stack","apache spark, aws data analytics stack, aws emr, aws glue, aws kafka, aws kinesis, aws redshift, datapipeline, qlik, sql, tableau"
Senior Software Engineer - Data Platforms Integrations (Remote),The Home Depot,"Atlanta, GA",https://www.linkedin.com/jobs/view/senior-software-engineer-data-platforms-integrations-remote-at-the-home-depot-3765523048,2023-12-17,Smyrna,United States,Mid senior,Onsite,"Position Purpose
The Sr. Software Engineer is responsible for enterprise integrations including transformation and messaging, integration with our Data Lake, and Master Data Management initiatives. As a Sr. Software Engineer, you will be part of a dynamic team with engineers of all experience levels who help each other build and grow technical and leadership skills while creating, deploying, and supporting production applications. In addition, Sr. Software Engineers may be involved in product and tool selection, configuration, security, resilience, performance tuning and production monitoring as well as mentoring less experienced engineers.
Key Responsibilities
50% Delivery and Execution - Develops, tests, deploys, and maintains software, with a clear understanding of the value the software is to provide; Takes on new opportunities and tough challenges with a sense of urgency, high energy and enthusiasm; Consistently achieves results, even under tough circumstances; Develops test suites (functional, destructive, etc) to enable success, rapid deployment of code to production; Takes a broad view when approaching issues; using a global lens
20% Learns and Grows - Learns through successful and failed experiment when tackling new problems; Actively seeks ways to grow and be challenged using both formal and informal development channels
20% Plans and Aligns - Collaborates with other team members in agile processes; Creates new and better ways for the organization to be successful; Works the Product Team to ensure user stories are valuable, developer ready, easy to understand and testable; Delivers multi-mode communications that convey a clear understanding of the unique needs of different audiences; Adapts approach and demeanor in real time to match the shifting demands of different situations; Relates openly and comfortably with diverse groups of people
10% Supports and Enables - Helps grow junior engineers by providing guidance on modern software development frameworks, and leading technical discussions
Direct Manager/Direct Reports
This position typically reports to Software Engineer Manager or Sr. Manager
This position has 0 Direct Reports
Travel Requirements
No travel required.
Physical Requirements
Most of the time is spent sitting in a comfortable position and there is frequent opportunity to move about. On rare occasions there may be a need to move or lift light articles.
Working Conditions
Located in a comfortable indoor area. Any unpleasant conditions would be infrequent and not objectionable.
Minimum Qualifications
Must be eighteen years of age or older.
Must be legally permitted to work in the United States.
Preferred Qualifications
2-4 years of relevant work experience
Experience with IBM Transformation Extender (Mercator), IBM DataStage, IBM Cloud Pak, IBM Product Master, IBM MQ, Red Hat AMQ, Diyotta (Purple Cube)
Experience with GCP (Google Cloud Platform), Google Big Query, Google CloudSQL, Google Pub-Sub
Experience with DB2, MS SQL Server, Oracle, MySQL, MongoDB, PostgreSQL
Experience with Microsoft Azure, AWS (Amazon Web Services)
Knowledge of Python, Shell Scripting, JavaScript (NodeJS, React, Angular), Java
Experience with Tableau, Looker, Google Analytics
Knowledge of Agile methodology, Jira, Service-Now
Experience with Big Data, Master Data Management (MDM), Machine Learning / Artificial Intelligence (ML/AI/GenAI)
Experience with Enterprise Application Integration (EAI), Data Lake, Data Fabric, Data Mesh, Lake House
Experience with Enterprise Data Warehouse (EDW), ETL (Extract, Transform, Load)
Minimum Education
The knowledge, skills and abilities typically acquired through the completion of a bachelor's degree program or equivalent degree in a field of study related to the job.
Preferred Education
No additional education
Minimum Years Of Work Experience
3
Preferred Years Of Work Experience
No additional years of experience
Minimum Leadership Experience
None
Preferred Leadership Experience
None
Certifications
None
Competencies
Global Perspective
Manages Ambiguity
Nimble Learning
Self-Development
Collaborates
Cultivates Innovation
Situational Adaptability
Communicates Effectively
Drives Results
Interpersonal Savvy
Show more
Show less","Software Development, Agile, Python, Shell Scripting, JavaScript, Java, Tableau, Looker, Google Analytics, Jira, ServiceNow, Big Data, Master Data Management, Machine Learning, Artificial Intelligence, Enterprise Application Integration, Data Lake, Data Fabric, Data Mesh, Lake House, Enterprise Data Warehouse, ETL","software development, agile, python, shell scripting, javascript, java, tableau, looker, google analytics, jira, servicenow, big data, master data management, machine learning, artificial intelligence, enterprise application integration, data lake, data fabric, data mesh, lake house, enterprise data warehouse, etl","agile, artificial intelligence, big data, data fabric, data lake, data mesh, enterprise application integration, enterprise data warehouse, etl, google analytics, java, javascript, jira, lake house, looker, machine learning, master data management, python, servicenow, shell scripting, software development, tableau"
Senior Data Engineer,Lever Middleware Test Company 2,"Atlanta, GA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-lever-middleware-test-company-2-3787337315,2023-12-17,Smyrna,United States,Mid senior,Remote,"a Bit About Us
Lever, an Employ brand, has a vision of connecting human potential to meaningful work. Hiring is the most strategic challenge that every company faces, and Lever is one of the most recognized brands in talent acquisition software. As the innovation leader in our space, we are searching for great people to join us and push further.
With an overall gender ratio of 50:50 women and men, Lever is also fiercely committed to building a team culture that celebrates diversity and inclusion. We have been certified as a “Great Place to Work”, along with “Best Company Outlook 2022,” and “Forbes Best Startup Employers 2022,” and the list keeps growing! Our people are Lever’s biggest competitive advantage. For more information on our team culture, visit https://inside.lever.co/.
THE CHALLENGE
Lever is on a mission to provide the best analytics feature set in the world of Talent software. We’ve made good progress in the last 2 years, but we still have room to improve.
This is where you come in. You will be the technical lead for the engineering team that is evolving Lever’s analytics features. You will help Lever’s customers recruit more efficiently and more predictably by providing them with a reliable and accurate data set to analyze.
This position will focus on maintaining and improving our data pipelines while consulting with partners in product management and product design on building user facing analytics features. As part of that collaboration, you will be encouraged to use your expertise and conduct technical research to help inform the direction we take with our analytics products.
THE OPPORTUNITY
One of Lever’s top priorities is to make our analytics features a reason that people buy and stay with Lever. As the lead engineer on the Analytics team you will have the opportunity to have a big impact on the success of Lever’s product and Lever’s business. You’ll work collaboratively with partners on the analytics team to drive the implementation of features, and be a champion of best practices for writing well-tested, well-organized code.
In addition to working on a high impact product area, as part of your daily work you will have the opportunity to build expertise in multiple Business Intelligence tools. You will be able to exercise and strengthen your automation and optimization skills. You will have the opportunity to learn about state of the art data infrastructure systems like Kafka and Snowflake, and you will be able to leverage observability tools like Datadog and Kibana to observe our systems.
You will also have the opportunity to lead the development of user facing analytics features, depending on your preferences and prior experience.
THE TEAM
Lever’s reporting engineering team combines ETL engineers, product engineers, data analysts, designers, and product managers to build, define, and design analytics products for customers. You will join the team as the lead software engineer building and maintaining advanced analytics products for Lever’s customers.
You will collaborate closely with our brilliant data analysts who create dashboards in the product, and scrappy Talent Analytics Consultants provide custom analysis services for customers. You will use your data and engineering knowledge to help guide the product and design team that strives to build analytics features to connect human potential to meaningful work.
THE SKILL SET
6+ years of software engineering experience
Experience with data pipelines and both the common pitfalls and best practices for maintaining them is a plus
Professional experience with relational databases that speak SQL (Postgres, MySql, MSSql, Oracle, Snowflake, etc)
An understanding of Business Intelligence tools, like Tableau or Looker, and the problems they solve
Knowledge of Looker is a plus, a desire to learn Looker is a must
A desire to teach others with the communication skills needed to effectively share knowledge
A healthy appetite for being a technical mentor for other engineers
This role includes a lot of technical freedom, so the ability to research technical solutions beyond those that already exist in our code base is a plus
Empathy for users of the Lever product and internal users that work at Lever
WITHIN 1 MONTH, YOU'LL
Complete Ramp Camp, our onboarding program designed to get you up to speed on our business, vision, and team.
Learn about the technology behind Lever’s analytics features (Lever Talent Intelligence, Data Warehouse Sync, and Visual Insights)
Participate in the reporting team’s ceremonies and get to know your teammates
Meet internal users of Business Intelligence tools
Deploy your first code change
WITHIN 3 MONTHS, YOU'LL
Learn about Lever’s product and how the analytics feature fit into the full picture
Build familiarity with Lever’s application data model in Mongo, how it aligns with data in SQL, and how the our data pipelines perform the translation
Understand the existing observability tools and monitoring we have in place for our data pipelines
Participate in Lever’s analytics product development process and ship your first feature to customers
Improve tools that reporting engineers use to maintain data systems
WITHIN 6 MONTHS, YOU'LL
Find and fill gaps in observability and automated testing for reporting features
Engage in your first on-call rotation for the product to help diagnose and resolve production problems and customer issues
Collaborate with other product engineering teams to incorporate data generated by new features into our analytics products
Identify and fix large areas of improvement in our data pipelines and schema management tool set
Contribute new ideas to the long-term roadmap of the reporting team
WITHIN 12 MONTHS, YOU'LL
Help the data analysts improve performance of pre-built Looker dashboards with optimizations and pre-calculated aggregations
Improve the experience of data analysts by providing them with better data environments to use for development
Automate continuous data quality and end-to-end service testing of data pipelines
Incorporate custom access control rules into the data available to end users of Lever
Is this role not an exact fit?
Sign up to stay in touch, we'll let you know when we have new positions on the team.
The Lever Story
Lever, an Employ brand, is a leading Talent Acquisition Suite that makes it easy for talent teams to reach their hiring goals and to connect companies with top talent. Lever is the only platform that provides all talent acquisition leaders with complete ATS and robust CRM capabilities in one product, LeverTRM. The LeverTRM features allow leaders to scale and grow their people pipeline, build authentic and long-lasting relationships, and source the right people to hire. Lever Analytics provides customized reports with data visualization, see offers completed and interview feedback, and more, to inform strategic decisions between hiring managers and executives alike.
Our platform also enables companies to hire with inclusivity in mind, helping eliminate any hiring bias. Lever supports the hiring needs of over 5,000 companies around the globe including the teams at Netflix, Spotify, Atlassian, KPMG, and Nielsen. For more information, visit https://www.lever.co.
California residents applying for positions at Lever can see our privacy policy here.
#BI-REMOTE
Show more
Show less","Python, Java, SQL, Looker, Tableau, Postgres, MySQL, MSSQL, Oracle, Snowflake, MongoDB, Kafka, Datadog, Kibana","python, java, sql, looker, tableau, postgres, mysql, mssql, oracle, snowflake, mongodb, kafka, datadog, kibana","datadog, java, kafka, kibana, looker, mongodb, mssql, mysql, oracle, postgres, python, snowflake, sql, tableau"
Senior Staff Data Engineer,"Nav Technologies, Inc.","Atlanta, GA",https://www.linkedin.com/jobs/view/senior-staff-data-engineer-at-nav-technologies-inc-3775332165,2023-12-17,Smyrna,United States,Mid senior,Remote,"Nav is democratizing small business financing. In other words, we give small business owners access and control. Yes, this challenges the norm, but it means working with curious, purpose driven, dedicated, and inquisitively smart people who push themselves, our company and the community to the next level (and every level after that). We are the people behind the tech. And when it's good, we look for better. We don't over think the value we bring nor spend time trying to revamp mantras. We also do not come up with some crafty way to tell you who we are and what we offer. We are Nav! Here, you'll gain a wealth of experience, learn the tricks of the trade, and work with winners. All companies say people are connected to their mission but in our case our mission and our people are one – it is a way of being not just a cause you are committed to. And since 2013, Nav has holistically and organically developed its own ideology because Nav can only be Nav.
We are seeking an innovative and passionate Data Engineer to join our team. In this role you will be responsible for building the data and reporting infrastructure to power our systems, working with our team of engineers, product managers and designers; helping us create a better experience for the millions of Nav Small business users. You'll also work with our business intelligence and data science teams to improve the data platform, ensuring that the entire company is able to make better data-driven decisions.
YOU WILL:
Building cool stuff by maintaining and fulfilling a technical data engineering roadmap and vision.
Creating data and reporting infrastructure by building and optimizing production-grade data pipelines through the use of continuous integration.
Exploring data sets for insight and understanding that will help drive great product changes and priority.
Communicating results and findings to company audiences of varying technical ability in a clear, engaging manner. Breaking down complex technical concepts into digestible tid-bits.
Supporting self-service data pipeline management (ETL) and self-directed learning process.
Addressing competing explanations in result validity, and use formal reasoning approaches to avoid bias and miscommunication.
Collaboration with Engineering and Data Analysts to identify and design ways to solve critical business problems and ensure funnel optimization.
Ability to design, develop, and maintain scalable, reusable code.
WHO YOU ARE:
You are a driven data engineer with experience working with big data technologies such as Docker, ECS, S3, Redshift, Kafka, and RDS.
You have experience with ETL/ELT and data warehousing using tools such as dbt, Azure Data Factory, Matillion and/or Fivetran
You love building data-driven products and have expertise in one or more programming languages (ideally Python).
You have strong SQL experience, with expert level skill in Postgres, Snowflake and /or AWS Redshift.
You're an Apache Airflow practitioner with experience managing cross-DAG dependencies in Airflow.
You know how to implement, advocate, and teach agile practices, and adapt them to your people and circumstances, while working in accordance with data ethics & privacy standards.
You can consistently evolve data models & data schema based on business and engineering needs.
You've developed and extended design patterns, processes, standards, frameworks and reusable components for use in various data engineering functions and areas.
Ability to implement systems for tracking data quality and consistency.
You're comfortable working directly with all internal data consumers and are eager to share your work through calibrations and organizational product demos.
You are committed to quality! And you're someone who's willing to tackle technical debt and constantly evolving our processes to help nav grow.
We'd love it if you have a high level understanding of credit scores, ecommerce, and B2B financing.
Inclusion at Nav:
At Nav, we celebrate what makes our employees unique because the businesses we serve are progressively diverse and distinctly original. Navericks are diverse, side hustlers, immigrants, veterans, queer, and we push generational boundaries. We are college dropouts, PhDs, special needs parents, allies, pet owners and community leaders. Navericks are human. We are committed to upholding a safe, supportive environment where everyone matters. We are committed to making a better future for all of us. We have created a workplace where people of all backgrounds can express their identities authentically. To put it simply, we want you to be proud to be you.
Our Compensation Philosophy is simple but powerful:
At Nav, we are transparent about our total rewards, including pay, across all levels and roles. We believe great, enduring relationships are grounded in trust and transparency. Compensation shouldn't be a distraction, and employees should understand how pay and career advancement decisions are made. Providing equal pay for equal work is table stakes for being a great place to work. Gender and ethnic inequity should only be something that our children read about in history books. We believe providing Navricks with company ownership, competitive pay, and a range of meaningful benefits is the start of creating a culture where people want to give the best they've got — not because they're simply making money, but because they've fallen in love with our vision, mission, values, and team.
During the interview process, your recruiter will be explaining how our rubrics work across all of our total rewards ( base, equity, bonus, perks, and benefit) offerings . The base salary for this role is targeted between $160K - $170K per year. Final offer amount is determined by your proficiencies within this level.
Our impact on you:
Competitive Pay. Company Ownership. Unlimited Vacation. Benefits Day One. 6 Weeks Paid Parental Leave. Work From Anywhere (yes we were distributed before it was cool). Flexible Work Arrangements. Free Telehealth and Telemental Health For All Employees. Employee Networking and Events. Community Network Groups (women's, PRIDE, culture). Meaningful Perks and Rewards. Learning and Development Opportunities. Pet Insurance.
A Naverick's DNA:
We look at the future and say ""why not""; we see possibilities where others see problems or routines. We show the way ahead and are committed to achieving ambitious goals.
We practice straight talk and listen generously to each other with empathy. We value different opinions and point of views. We ensure that we connect outside as well as inside to learn from others and inspire each other.
We hold ourselves accountable for delivering results. We choose to not to be a victim of circumstance. We make decisions & take responsibility so that we can act & support each other, rather than adopting defensive, and ""finger pointing"" behaviors.
As leaders we motivate & engage our teams to undertake beyond what they originally thought possible, by developing our teams & creating the conditions for people to grow and empower themselves through enabling & coaching.
If you are based in California, we encourage you to read our privacy notice for California residents linked here.
Show more
Show less","Data Engineering, SQL, Python, Postgres, Snowflake, AWS Redshift, Apache Airflow, Docker, ECS, S3, Redshift, Kafka, RDS, dbt, Azure Data Factory, Matillion, Fivetran, Credit Scores, Ecommerce, B2B Financing","data engineering, sql, python, postgres, snowflake, aws redshift, apache airflow, docker, ecs, s3, redshift, kafka, rds, dbt, azure data factory, matillion, fivetran, credit scores, ecommerce, b2b financing","apache airflow, aws redshift, azure data factory, b2b financing, credit scores, data engineering, dbt, docker, ecommerce, ecs, fivetran, kafka, matillion, postgres, python, rds, redshift, s3, snowflake, sql"
"Lead Data Engineer (AWS, Azure, GCP)",CapTech,"Atlanta, GA",https://www.linkedin.com/jobs/view/lead-data-engineer-aws-azure-gcp-at-captech-3751643389,2023-12-17,Smyrna,United States,Mid senior,Remote,"Company Description
CapTech is an award-winning consulting firm that collaborates with clients to achieve what’s possible through the power of technology. At CapTech, we’re passionate about the work we do and the results we achieve for our clients. From the outset, our founders shared a collective passion to create a consultancy centered on strong relationships that would stand the test of time. Today we work alongside clients that include Fortune 100 companies, mid-sized enterprises, and government agencies, a list that spans across the country.
Job Description
CapTech Data Engineering consultants enable clients to build and maintain advanced data systems that bring together data from disparate sources in order to enable decision-makers. We build pipelines and prepare data for use by data scientists, data analysts, and other data systems. We love solving problems and providing creative solutions for our clients. Cloud Data Engineers leverage the client’s cloud infrastructure to deliver this value today and to scale for the future. We enjoy a collaborative environment and have many opportunities to learn from and share knowledge with other developers, architects, and our clients.
The Value You Deliver (or What You’ll Do)
Be trusted advisor to customers with best practices, methodologies, and technologies to implement data engineering solutions.
Design, implement, and maintain modern data pipelines to deliver optimal solutions utilizing appropriate cloud technologies.
Partner with product owners and business SMEs to analyze customer requirements and provide a supportable and sustainable engineered solution.
Provide technical leadership and collaborate within and across teams to ensure that the overall technical solution is aligned with the customer needs.
Stay current with the latest cloud technologies, patterns, and methodologies; share knowledge by clearly articulating results and ideas to key stakeholders.
Qualifications
Experience building/operating highly available distributed systems of data extraction, ingestion, and processing large data sets
5+ years of experience delivering data engineering solutions on cloud platform
5+ years of experience implementing modern designs using at least one cloud-based solution/platform (AWS, Azure, GCP)
Advanced level proficiency with at least one ETL / Data Orchestration technology (Azure Data Factory, SSIS, Informatica, Alteryx, Ab Initio, Pentaho, Talend, Matillion)
Experience cloud-based data warehousing and data lake solutions like Snowflake, Redshift, Databricks
5+ years of experience with SQL or NoSQL database (PostgreSQL, MySQL, SQL server, Oracle, Aurora, Presto, BigQuery)
Expertise with SQL, database design/structure and data structure (star, snowflake schemas, de/normalized designs)
5+ years of experience with at least one programming language (Python, Java, R, C / C# / C++, Shell)
Familiarity with one or more DevOps tools (git, Jenkins, CI/CD, Jira)
Fundamental understanding of big data, open source, and data streaming concepts
Ability to think strategically and provide recommendations utilizing traditional and modern architectural components based on business needs
Experience providing technical leadership and mentoring other engineers in data engineering space
Cloud certification on any platform a plus
Additional Information
We want everyone at CapTech to be able to envision a lasting and rewarding career here, which is why we offer a variety of career paths based on your skills and passions. You decide where and how you want to develop, and we help get you there with customizable career progression and a comprehensive benefits package to support you along the way. Alongside our suite of traditional benefits encompassing generous PTO, health coverage, disability insurance, paid family leave and more, we’ve launched extended benefits to help meet our employees’ needs.
CapFlex – Employee-first mentality that supports a remote and hybrid workforce and empowers daily flexibility while servicing our clients
Learning & Development – Programs offering certification and tuition support, digital on-demand learning courses, mentorship, and skill development paths
Modern Health –A mental health and well-being platform that provides 1:1 care, group support sessions, and self-serve resources to support employees and their families through life’s ups and downs
Carrot Fertility –Inclusive fertility and family-forming coverage for all paths to parenthood – including adoption, surrogacy, fertility treatments, pregnancy, and more – and opportunities for employer-sponsored funds to help pay for care
Fringe –A company paid stipend program for personalized lifestyle benefits, allowing employees to choose benefits that matter most to them – ranging from vendors like Netflix, Spotify, and GrubHub to services like student loan repayment, travel, fitness, and more
Employee Resource Groups – Employee-led committees that embrace and incorporate diversity and inclusion into our day-to-day operations
Philanthropic Partnerships – Opportunities to engage in partnerships and pro-bono projects that support our communities.
401(k) Matching – Generous matching and no vesting period to help you continue to build financial wellness
CapTech is an equal opportunity employer committed to fostering a culture of equality, inclusion and fairness — each foundational to our core values. We strive to create a diverse environment where each employee is encouraged to bring their unique ideas, backgrounds and experiences to the workplace. For more information about our Diversity, Inclusion and Belonging efforts, click HERE. As part of this commitment, CapTech will ensure that persons with disabilities are provided reasonable accommodations. If reasonable accommodation is needed to participate in the job application or interview process, to perform essential job functions, and/or to receive other benefits and privileges of employment, please contact Laura Massa directly via email lmassa@captechconsulting.com.
At this time, CapTech cannot transfer nor sponsor a work visa for this position. Applicants must be authorized to work directly for any employer in the United States without visa sponsorship.
Show more
Show less","Data Engineering, Data Pipelines, Cloud Technologies, AWS, Azure, GCP, ETL, Data Orchestration, Informatica, Matillion, Talend, Snowflake, Redshift, Databricks, SQL, NoSQL, PostgreSQL, MySQL, SQL Server, Oracle, Aurora, Presto, BigQuery, Python, Java, R, C, C++, Shell, Git, Jenkins, CI/CD, Jira, DevOps, Big Data, Open Source, Data Streaming, Technical Leadership, Mentoring, Architecture, Business Needs","data engineering, data pipelines, cloud technologies, aws, azure, gcp, etl, data orchestration, informatica, matillion, talend, snowflake, redshift, databricks, sql, nosql, postgresql, mysql, sql server, oracle, aurora, presto, bigquery, python, java, r, c, c, shell, git, jenkins, cicd, jira, devops, big data, open source, data streaming, technical leadership, mentoring, architecture, business needs","architecture, aurora, aws, azure, big data, bigquery, business needs, c, cicd, cloud technologies, data engineering, data orchestration, data streaming, databricks, datapipeline, devops, etl, gcp, git, informatica, java, jenkins, jira, matillion, mentoring, mysql, nosql, open source, oracle, postgresql, presto, python, r, redshift, shell, snowflake, sql, sql server, talend, technical leadership"
Sr. Data Analyst- Operations,Reserv,"Atlanta, GA",https://www.linkedin.com/jobs/view/sr-data-analyst-operations-at-reserv-3770863602,2023-12-17,Smyrna,United States,Mid senior,Remote,"About Reserv
Reserv is an insurtech creating and incubating cutting-edge AI and automation technology to bring efficiency and simplicity to claims. Founded by insurtech veterans with deep experience in SaaS and digital claims, Reserv is venture-backed by Bain Capital and Altai Ventures and began operations in May 2022. We are focused on automating highly manual tasks to tackle long-standing problems in claims and set a new standard for TPAs, insurance technology providers, and adjusters alike.
We have ambitious (but attainable!) goals and need adjusters who can work in an evolving environment. If building a leading TPA and the prospect of tackling the long-standing challenges of the claims role sounds exciting, we can’t wait to meet you.
About The Role
For this role, we are seeking data analysts with strong Python and SQL skills, a curiosity for how claims work and how our systems work them, the ability to interface with customers from technical and non technical backgrounds and the ability to both run independently and collaborate with our engineering and operations orgs alike. If you are ready to mold the vision for Reserv’s technology, join us on the journey.
What We Need
Work with our data stack: Python (Pandas to NLTK), Postgres, DBT, Looker etc
Help translate the core lifecycle of a claim into structured data and partner with the product ops team to incorporate low drag data collection into the claim lifecycle
Build and utilize tooling that monitors the above lifecycle and validate insights gleaned with our claims teams
Coalesce quantitative and qualitative insights about the operational funnel to drive the engineering and ML roadmaps as well as the operational staffing model
Communicate ideas and specifications through detailed, precise writing
Influence the culture, values, and processes of a growing engineering team
Enjoy and have fun being creative and thinking outside of the box. You will need to draw on analogous situations you’ve worked on before, but we are not traversing well-traveled roads. “Where we’re going, we don’t need roads!”
Requirements
6-8 years of experience as an individual contributor with experience using modern analytics and programming languages with an emphasis on statistical analysis
A proven ability to interface with external customers and drive consensus with both technical and non technical audiences alike
Have a strong demonstrated toolkit to build robust designs of experiment
Have a proven track record of shipping high quality data analysis at a high cadence
A passion for building intuitive technology and a thirst for learning
Able to think through customer requirements and prioritize what is a must-have, nice-to-have, or irrelevant
Have a strong technical skillset but also the willingness and ability to take over tasks outside your core competency
Benefits
Generous health-insurance package with nationwide coverage, vision, & dental
401(k) retirement plan with employer matching
Competitive PTO policy – we want our employees fresh, healthy, happy, and energized!
Generous family leave policy
Work from anywhere to facilitate your work life balance paired with frequent, regular corporate retreats to build team cohesion, reinforce culture, and have fun
Apple laptop, large second monitor, and other quality-of-life equipment you may want. Technology is something that should make your life easier, not harder!
At Reserv, we value diversity and believe that a variety of perspectives leads to innovation and success. We are actively seeking candidates who will bring unique perspectives and experiences to our team. We welcome applicants from all backgrounds and encourage those from underrepresented groups to apply. If you believe you are a good fit for this role, we would love to hear from you!
Show more
Show less","Python, SQL, Pandas, NLTK, Postgres, DBT, Looker, Statistical analysis, Data analysis, Design of experiments, Customer interface, Communication, Writing, Culture building, Creativity, Thinking outside the box, Statistical analysis, Software design, Data analysis, Technology development, Customer requirements, Prioritization, Teamwork, Collaboration, Leadership, Problemsolving, Innovation, Diversity, Inclusion","python, sql, pandas, nltk, postgres, dbt, looker, statistical analysis, data analysis, design of experiments, customer interface, communication, writing, culture building, creativity, thinking outside the box, statistical analysis, software design, data analysis, technology development, customer requirements, prioritization, teamwork, collaboration, leadership, problemsolving, innovation, diversity, inclusion","collaboration, communication, creativity, culture building, customer interface, customer requirements, dataanalytics, dbt, design of experiments, diversity, inclusion, innovation, leadership, looker, nltk, pandas, postgres, prioritization, problemsolving, python, software design, sql, statistical analysis, teamwork, technology development, thinking outside the box, writing"
"Senior/Staff Software Engineer, Data",EvenUp,"Atlanta, GA",https://www.linkedin.com/jobs/view/senior-staff-software-engineer-data-at-evenup-3782663482,2023-12-17,Smyrna,United States,Mid senior,Remote,"EvenUp is a venture-backed generative AI startup that ensures injury victims are awarded the full value of their claims, expanding the $100B+ in awards granted to injury victims every year.
Every year, the legal system has made it difficult for millions of ordinary people to seek justice, especially for folks without means or who come from underrepresented backgrounds
. Our vision is to help these injury victims get the justice they deserve, irrespective of their income, demographics, or the quality of their legal representation.
EvenUp operates across all types of injury cases, from police brutality and child abuse to California wildfires and motor vehicle accidents. Our ML-driven software empowers attorneys to accurately assess the value of these cases by doing a core part of their workflow (legal drafting), enabling them to secure larger settlements in record time. As EvenUp evaluates more cases, our proprietary data grows, enhancing the precision of our predictions and delivering more value to both attorneys and victims alike.
As one of the fastest growing startups ($0 to $10M in ARR in <2 years), we raised $65M in investment from some of the best investors in the world (Bessemer, Bain Capital, Signalfire, DCM, NFX, Tribe Capital), seasoned tech executives (i.e. founder of Quora, SVP at Google, former CPO at Uber), and public figures that care about our social mission (Nas, Jared Leto, Byron Jones). Our team comes from top tech, legal, and investing backgrounds including Waymo, Google, Amazon, Uber, Quora, Blizzard, Norton Rose, Warburg Pincus, Bain, and McKinsey.
The role:
We’re looking to bring on board
Senior/Staff Software Engineers
focused on our
Data Pipelines
as we’ve experienced unprecedented growth and need to build & scale out our data pipelines and infrastructure. We’re looking for strong team members to help architect and drive forward the vision of our ideal data infrastructure at EvenUp. We will need to 10x our pipeline processing throughput over the next 12 months. We’ll need to rethink and rebuild how we extract, process and model our ingestion to enable our organization with precise and actionable data.
What you'll do:
Build fault tolerant data pipelines to process diverse datasets at EvenUp
Design and develop modularized services to increase the capabilities and scope of our data infrastructure
Collaborate with our DS team to Integrate ML models into our production workflows and simplify ML deployment and observability
Implement event driven, low latency systems to empower our stakeholders with accurate and reliable data
Analyze and solve key performance bottlenecks, scaling challenges, and high availability issues.
Mentor and coach junior team members
Help grow our engineering team and define a “data first” mentality across our organization.
What we are seeking:
8+ years of industry experience designing and building distributed data systems
Previous experience architecting and scaling event driven architectures
Strong understanding and practical experience with data pipeline tooling and storage systems such as Dagster, DBT, BigQuery, Elasticsearch
The ability to communicate cross-functionally with various stakeholders to derive requirements and architect scalable solutions
Have several years of industry experience building high-quality software, shipping production-ready code and infrastructure
You enjoy owning a project from start to finish and love to drive a project across the finish line.
Interest in making the world a fairer place (we don’t get paid unless we’re helping injured victims and/or their attorneys)
Nice to haves:
Fluency in Python, SQL and GraphQL
Previous experience integrating ML models and LLMs into data services
Domain expertise in legal technology, medical records, and working with unstructured data
Benefits & Perks:
We seek to empower all of our team members to fulfill our mission of making the world a more just place, regardless of our team’s function, geography, or experience level. To that end, we offer:
Fully remote setup - work from wherever you feel is best (Plus a stipend to upgrade your home office!)
Flexible working hours to match your style
Offsites - get to meet your coworkers on a fully-expensed trip every 6-12 months!
Choice of great medical, dental, and vision insurance plan options
Flexible paid time off
A variety of virtual team events such as game nights & happy hours
EvenUp is an equal-opportunity employer. We are committed to diversity and inclusion in our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.
Show more
Show less","Data Pipelines, Data Infrastructure, Distributed Data Systems, Event Driven Architectures, Data Pipeline Tooling, Storage Systems, Dagster, DBT, BigQuery, Elasticsearch, Python, SQL, GraphQL, Machine Learning Models, Large Language Models, Data Services, Legal Technology, Medical Records, Unstructured Data","data pipelines, data infrastructure, distributed data systems, event driven architectures, data pipeline tooling, storage systems, dagster, dbt, bigquery, elasticsearch, python, sql, graphql, machine learning models, large language models, data services, legal technology, medical records, unstructured data","bigquery, dagster, data infrastructure, data pipeline tooling, data services, datapipeline, dbt, distributed data systems, elasticsearch, event driven architectures, graphql, large language models, legal technology, machine learning models, medical records, python, sql, storage systems, unstructured data"
"Cloud Database Engineer (GCP) - SRE [Remote, US only]",Digimarc,"Atlanta, GA",https://www.linkedin.com/jobs/view/cloud-database-engineer-gcp-sre-remote-us-only-at-digimarc-3782261561,2023-12-17,Smyrna,United States,Mid senior,Remote,"THE CHALLENGE…
As a Cloud Database Engineer at Digimarc, you will be responsible for designing, implementing, and maintaining our Google Cloud Platform (GCP) database infrastructure. You will work closely with our development and operations teams to ensure that our database systems are optimized for performance, security, and scalability.
WHAT YOU WILL DO ...
Design, develop, deploy, and maintain highly available data systems in cloud-based ecosystems.
Collaborate with team leadership and software developers to ensure data systems are scalable, performant, and secure.
Provide technical guidance with respect to data systems to other SRE members and software developers.
Help build health team relationships that drive a culture of innovation and creativity.
WHAT WE ARE LOOKING FOR ...
Expert in designing, deploying, and managing GCP-based database systems, including Google Cloud SQL, BigQuery, and other relevant services.
Expert in optimizing database performance, ensuring high availability, and planning for disaster recovery.
Expertise using Terraform to manage infrastructure within GCP.
Expertise in troubleshooting and resolving database-related issues, ensuring minimal downtime.
Expertise in Implementing and enforcing database security policies, access controls, and encryption.
Experience with Snowflake a plus.
Participation in on-call rotation to provide 24/7 support for database-related incidents.
Great documentation skills and habits for database configurations, procedures, and best practices.
Personal drive to stay up to date with the latest GCP database technologies and trends.
Passion in participating, creating, and sustaining a diverse and inclusive culture where individuals and teams can do their best work and unlock their potential
Alignment with the core Digimarc values: collaborative, curious, and courageous
Show more
Show less","Google Cloud Platform (GCP), Google Cloud SQL, BigQuery, Terraform, Snowflake, SQL, Database architecture, Data modeling, Data warehousing, Big data, Data security, Encryption, Disaster recovery, Performance optimization, Scalability, High availability, Troubleshooting, Database administration","google cloud platform gcp, google cloud sql, bigquery, terraform, snowflake, sql, database architecture, data modeling, data warehousing, big data, data security, encryption, disaster recovery, performance optimization, scalability, high availability, troubleshooting, database administration","big data, bigquery, data security, database administration, database architecture, datamodeling, datawarehouse, disaster recovery, encryption, google cloud platform gcp, google cloud sql, high availability, performance optimization, scalability, snowflake, sql, terraform, troubleshooting"
Senior Data Engineer,GSquared Group,Atlanta Metropolitan Area,https://www.linkedin.com/jobs/view/senior-data-engineer-at-gsquared-group-3756011059,2023-12-17,Smyrna,United States,Mid senior,Remote,"Senior Data Engineer
Atlanta, Georgia
Contract Opportunity ( Must already live in the Atlanta area, onsite occasionally for fun activities but mostly remote with occasional onsite).
We have a great opportunity for a Data Engineer with experience using AWA Cloud technologies. This is a long-term contract opportunity in the Atlanta area!
As the Data Engineer, you will build and manage a data pipeline for an AWS cloud-based data platform. If you have strong technical, analytical, programming and critical thinking skills as well as data transformation and data modeling experience – we want to talk to you!
How will you make an impact?
Build and manage the data pipeline (ingest, transformation, distribution, quality rules, data storage) for a cloud-based data platform
Design, develop and maintain automated data solutions
Develop new and existing data processing
Support and troubleshoot the data environment
Document technical artifacts for developed solutions
Qualifications & Experience
4+ years of data engineering experience
Spark and PySpark required
Databricks expeirence required
Python or Java experience preferred
Show more
Show less","Data Engineering, AWS Cloud, Data Pipeline, Data Transformation, Data Modeling, Spark, PySpark, Databricks, Python, Java","data engineering, aws cloud, data pipeline, data transformation, data modeling, spark, pyspark, databricks, python, java","aws cloud, data engineering, data pipeline, data transformation, databricks, datamodeling, java, python, spark"
Senior Data Engineer,Brooksource,Atlanta Metropolitan Area,https://www.linkedin.com/jobs/view/senior-data-engineer-at-brooksource-3775451995,2023-12-17,Smyrna,United States,Mid senior,Remote,"Staff Systems Engineer (Data Engineer)
Remote (EST Time Zone)
Contract (Potential for Conversion)
Brooksource is looking for a Staff Systems Engineer to join their Fortune 30 Retail client in Atlanta. As a Systems Engineer, you will be responsible for crafting, developing, and deploying database solutions throughout an entire Delivery Selling and Fullfillment system. Specifically, you will be supporting our Client's Delivery Exceptions applications.
Our ideal candidate has experience developing and supporting a wide range of software systems, is self-sufficient, a quick learner, and a strong individual contributor with strong quantitative, analytical, strategic thinking and problem solving skills.
Qualifications:
5+ years of experience as a Systems Engineer or related role
Minimum of 1 year of experience in a Team Lead or related role
3+ years of experience developing in Python and Java
5+ years of experience using SQL
Experience writing SQL queries
Additional technologies with exposure to: GCP or another relevant cloud platform
Computer Science or equivalent degree
Responsibilities:
Design, develop, test, and maintain software and database systems which support the platform
Design solutions and technical specifications including automated frameworks.
Ability to understand data and optimize queries
Create new features within SQL
Assist development team during the development phase to resolve data issues and understand data constraints
Follow the established processes, policies, standards, and procedures to assure compliance with corporate and regulatory policies and standards
Work with business users and vendors to ensure understanding of business process and proper implementation in software
Ensure software follows best practices from coding, testing and performance perspective
ABOUT BROOKSOURCE
Brooksource is an IT Services Company, specializing in the recruitment and placement of all levels of IT professionals. We offer competitive compensation, paid holidays, 401k, health benefits, flexible work schedules and just about anything a top tier candidate would demand. Our diverse client base covers all industries and provides us the opportunity to place you, the candidate, in positions that span the entire IT spectrum. Our personal touch and the ability to customize the placement to your interests, is what sets us apart. Our goal is to continue providing the tools and opportunity to help our people grow, while building lasting relationships with our consultants to stay in tune with their career aspirations; thus providing our clients with the best possible candidates from a personal and technical perspective. We believe this creates the basis for a lasting partnership between our business and employees, thus an excellent place to work.
We are an equal opportunity employer and value diversity at our company. We do not discriminate based on race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.
Show more
Show less","Java, Python, SQL, GCP, SQL Queries, Software Development, Database Systems, Automated Frameworks, Data Optimization, Coding, Testing, Performance, Computer Science","java, python, sql, gcp, sql queries, software development, database systems, automated frameworks, data optimization, coding, testing, performance, computer science","automated frameworks, coding, computer science, data optimization, database systems, gcp, java, performance, python, software development, sql, sql queries, testing"
Big Data Developer,Aptonet Inc,"Atlanta, GA",https://www.linkedin.com/jobs/view/big-data-developer-at-aptonet-inc-3769503531,2023-12-17,Smyrna,United States,Mid senior,Remote,"Job Title: Big Data Engineer (No C2C)
Location: Atlanta 30308 _ (100% Remote) on EST Schedule
Duration: 12 month extendable Contract
Job Summary
As a Big Data Engineer, you will be responsible for designing, building, and optimizing big data pipelines. You will work closely with data scientists and other stakeholders to support the data needs of the organization.
Responsibilities
Design and implement high-velocity, high-volume data streaming solutions using Apache Kafka and Spark Streaming.
Develop real-time data processing and streaming techniques using Spark Structured Streaming and Kafka.
Troubleshoot and optimize Spark applications for peak performance.
Work closely with Python and/or Scala for data processing tasks (PySpark/Scala-Spark).
Utilize Databricks for cloud-based big data solutions.
Build, test, and optimize big data ingestion pipelines, architectures, and datasets.
Deploy data platforms on Azure or AWS and manage serverless technologies like S3, Kinesis/MSK, Lambda, and Glue.
Operate messaging platforms like Kafka, Amazon MSK, TIBCO EMS, or IBM MQ Series for asynchronous data communication.
Manage Databricks Notebooks, work with Delta Lake using both Python and Spark SQL, and manage Delta Live Tables and the Unity Catalog.
Ingest data from various formats including JSON, XML, and CSV.
Work with NoSQL databases like HBASE and/or Cassandra.
Perform shell scripting and other tasks on Unix/Linux platforms.
Work with other database solutions like Kudu/Impala or Delta Lake.
Qualifications - must have
Must have hands-on experience with high-velocity, high-volume stream processing: Apache Kafka and Spark Streaming.
Experience with real-time data processing and streaming techniques using Spark structured streaming and Kafka.
Deep knowledge of troubleshooting and tuning Spark applications.
Must have hands-on experience with Python and/or Scala (PySpark/Scala-Spark).
Must have experience with Databricks.
Must have hands-on experience building, testing, and optimizing big data ingestion pipelines, architectures, and datasets.
Experience in successfully building and deploying a new data platform on Azure/AWS.
Experience in Azure/AWS Serverless technologies, like S3, Kinesis/MSK, Lambda, and Glue.
Strong knowledge of Messaging Platforms like Kafka, Amazon MSK & TIBCO EMS or IBM MQ Series.
Experience with Databricks UI, Managing Databricks Notebooks, Delta Lake with Python, Delta Lake with Spark SQL, Delta Live Tables, Unity Catalog.
Experience with data ingestion of different file formats like JSON, XML, CSV.
Experience with NoSQL databases, including HBASE and/or Cassandra.
Knowledge of Unix/Linux platform and shell scripting is a must.
Experience with database solutions like Kudu/Impala, or Delta Lake.
Show more
Show less","Apache Kafka, Spark Streaming, Spark Structured Streaming, Python, Scala, PySpark, ScalaSpark, Databricks, Azure, AWS, S3, Kinesis/MSK, Lambda, Glue, Kafka, Amazon MSK, TIBCO EMS, IBM MQ Series, Delta Lake, Unity Catalog, JSON, XML, CSV, HBASE, Cassandra, Unix/Linux, Kudu/Impala","apache kafka, spark streaming, spark structured streaming, python, scala, pyspark, scalaspark, databricks, azure, aws, s3, kinesismsk, lambda, glue, kafka, amazon msk, tibco ems, ibm mq series, delta lake, unity catalog, json, xml, csv, hbase, cassandra, unixlinux, kuduimpala","amazon msk, apache kafka, aws, azure, cassandra, csv, databricks, delta lake, glue, hbase, ibm mq series, json, kafka, kinesismsk, kuduimpala, lambda, python, s3, scala, scalaspark, spark, spark streaming, spark structured streaming, tibco ems, unity catalog, unixlinux, xml"
Azure Databricks Engineer,"Stellar Consulting Solutions, LLC","Atlanta, GA",https://www.linkedin.com/jobs/view/azure-databricks-engineer-at-stellar-consulting-solutions-llc-3782243963,2023-12-17,Smyrna,United States,Mid senior,Remote,"Title: Azure Databricks Engineer III
Location: Remote
Duration: Contract to hire
JOB PURPOSE
The Data Engineer III builds robust & scalable code to transform and deliver data for reporting & analytics. This role will work closely with other
data engineers and architects
on design and ingestion/delivery pipelines, but the primary focus is building complex business logic in
Scala
and/or
Python
to aggregate and transform data sets and load to a curated zone in
Azure Cloud/ADLS Gen2
. This position will have a high concentration on Databricks development but not limited to.
ESSENTIAL DUTIES AND RESPONSIBILITIES
Demonstrate senior-level knowledge of
Azure Databricks
to develop & deploy solutions for reporting and analytics
Create
real time streaming pipelines using tools such as Streamsets and Qlik
Work with internal customers/partners to gather requirements, build, and deploy robust models. Develop code to aggregate and transform code to
ADLS Gen2 curated zone
.
Develop data management frameworks to properly tag and manage curated data assets, promoting reuse and avoiding a ‘data swamp’
Reverse-engineer & document legacy
ETL code
to inform Databricks development in Azure Cloud. Document new transformation logic in Databricks for maintainability.
Provide ongoing support of Databricks code. Diagnose and resolve issues.
Provide strategic guidance, contribute to evaluation of new tools/platforms/products.
PERFORMANCE METRICS
Loading and transforming large sets of structured, semi structured and unstructured data.
Building data pipelines using Databricks and scheduling Databricks jobs.
Data processing/transformation using various technologies such as spark and cloud Services
JOB REQUIREMENTS
Bachelor's degree in computer science or other appropriate discipline
5-7 years of experience in data strategies, data analysis, database architecture and management, development or administration or Associates degree in related field and 2+ years of relevant data processing, systems analysis/design, and 2-3 years of experience developing/maintaining database objects in one of the database systems or a combination of education and experience from which comparable knowledge and skills are acquired.
Strong skills in Azure Databricks
,
SQL, Spark, Python and/or Scala, Data analysis
Experience in building ETL / data warehouse transformation processes
Hands on experience designing and delivering solutions using the Azure Data Analytics platform (Cortana Intelligence Platform) including Azure Storage, Azure SQL Data Warehouse, Azure Data Lake, Azure Cosmos DB, Azure Stream Analytics
Experience with
Apache Kafka and event-based data
Proficient in a source code control system such as GIT
Experience with
data warehousing architecture
KNOWLEDGE, SKILLS, AND ABILITIES
Knowledge and experience with the definition and design of logical data models and data structures including data and model normalization
Knowledge of the different types of data models (conceptual through physical) and their usages and business value
Knowledge of the data requirements and design challenges associated with delivering data for finance management, finance reporting as well corporate performance needs.
Well rounded knowledge of multiple ETL,ELT tools (SSIS, Databricks)
Ability to translate business requirements into ETL technical requirements
Knowledge of streaming real time data end to end
5+ years of strong hands-on skills and deep knowledge of SQL, Stored Procedure, and advanced SQL in Data Warehousing environment
3+ years of experience in Data Warehousing including Data Cleansing, Optimization, High Volume (Terabytes) and Real-time ETL Process Design & Development
Experience with Data Transformation (Filtering, Cleaning, Aggregation)
Knowledge of Cloud Architecture
Knowledge of unstructured data
Show more
Show less","Azure Databricks, Scala, Python, Azure Cloud/ADLS Gen2, Streamsets, Qlik, ADLS Gen2 curated zone, ETL, Spark, SQL, Apache Kafka, GIT, SSIS, Stored Procedure, Data Cleansing, Optimization, High Volume (Terabytes), Realtime ETL Process Design & Development, Data Transformation (Filtering Cleaning Aggregation), Cloud Architecture, Unstructured data","azure databricks, scala, python, azure cloudadls gen2, streamsets, qlik, adls gen2 curated zone, etl, spark, sql, apache kafka, git, ssis, stored procedure, data cleansing, optimization, high volume terabytes, realtime etl process design development, data transformation filtering cleaning aggregation, cloud architecture, unstructured data","adls gen2 curated zone, apache kafka, azure cloudadls gen2, azure databricks, cloud architecture, data transformation filtering cleaning aggregation, datacleaning, etl, git, high volume terabytes, optimization, python, qlik, realtime etl process design development, scala, spark, sql, ssis, stored procedure, streamsets, unstructured data"
Sr. Cloud Database Engineer,Stellar Professionals,"Atlanta, GA",https://www.linkedin.com/jobs/view/sr-cloud-database-engineer-at-stellar-professionals-3736639485,2023-12-17,Smyrna,United States,Mid senior,Hybrid,"Skills Required
Design and develop data models, data warehouses, data marts
Cloud-based enterprise data warehouse
Snowflake
WhereScape Data warehouse automation
Data Vault 2 methodology
Develop, implement, and automate testing scripts using Validatar
Developing ETL logic
Developing in Wherescape RED and 3D
QA Testing
High proficiency in SQL, R, Python
Experience in the public healthcare industry
Show more
Show less","Data Modeling, Data Warehousing, Data Marts, Cloudbased Data Warehouse, Snowflake, WhereScape Data Warehouse Automation, Data Vault 2 Methodology, ETL Logic, Wherescape RED and 3D, QA Testing, SQL, R, Python, Public Healthcare Industry","data modeling, data warehousing, data marts, cloudbased data warehouse, snowflake, wherescape data warehouse automation, data vault 2 methodology, etl logic, wherescape red and 3d, qa testing, sql, r, python, public healthcare industry","cloudbased data warehouse, data marts, data vault 2 methodology, datamodeling, datawarehouse, etl logic, public healthcare industry, python, qa testing, r, snowflake, sql, wherescape data warehouse automation, wherescape red and 3d"
"Senior Data Engineer -- Long Term Contact -- Atlanta, GA (3 Days Onsite 2 Days Remote)",Lorven Technologies Inc.,"Atlanta, GA",https://www.linkedin.com/jobs/view/senior-data-engineer-long-term-contact-atlanta-ga-3-days-onsite-2-days-remote-at-lorven-technologies-inc-3770686929,2023-12-17,Smyrna,United States,Mid senior,Hybrid,"Position: Senior Data Engineer
Location: Atlanta, GA (3 Days Onsite 2 Days Remote)
Duration: Long Term Contact
Primary Responsibilities
Designing, developing, and maintaining end-to-end master data pipelines that efficiently collect, process, transform, and load data from diverse sources into our data ecosystem.
Collaborating with cross-functional teams to understand business requirements and translate them into effective data architecture solutions.
Ensuring data quality, integrity, and security throughout the data lifecycle by implementing best practices and standards.
Optimizing data pipelines for performance, scalability, and reliability, and identifying opportunities for automation and process improvement.
Utilizing your expertise in SQL, Unix/Shell scripting, Python, and data processing frameworks to create and manage data transformations and integrations.
Utilizing concepts like Data warehouse, Data Lake house, Data Mess to share enterprise data strategies.
Working closely with DevOps and Engineering teams to implement continuous integration and continuous deployment (CI/CD) pipelines for data-related processes.
Staying current with industry trends and emerging technologies in data architecture and applying that knowledge to drive innovation within the organization.
Bachelor’s degree in computer science, Information Technology, or a related field (Master's degree preferred).
A minimum of 5 years of experience in data architecture, with a proven track record of designing and deploying master data pipelines.
Proficiency in SQL for data manipulation and retrieval from relational databases.
Strong scripting skills in Unix/Shell and Python for automating data processes and transformations.
Experience with data processing frameworks such as Apache Spark, Apache Flink, or similar technologies.
Familiarity with CI/CD tools and practices for automating deployment and monitoring of data pipelines.
Excellent problem-solving skills and the ability to optimize data workflows for performance and efficiency.
Solid understanding of data modeling, data warehousing concepts, and ETL processes.
Strong communication skills to collaborate effectively with cross-functional teams and articulate complex technical concepts to non-technical stakeholders.
Knowledge, Skills, And Abilities
In-depth experience of designing and implementing information solutions.
Experience in architecture practice, tools, and methodologies.
Demonstrate the ability to work well with others and exhibit leadership.
Have a track record of remaining unbiased toward specific technologies or vendors.
Be an excellent communicator and collaborator, engaging with multiple technical and business stakeholders and leaders.
Be able to translate the information architecture contribution to business outcomes into simple briefings for use by various data-and-analytics-related roles.
Organizationally savvy, with a good understanding of the enterprise's political climate and how to navigate, influence and persuade political waters.
Ability to communicate, influence and persuade peers and leadership.
Ability to understand the long-term (""big picture"") and short-term perspectives of situations.
Ability to quickly comprehend the functions and capabilities of new technologies.
Displays intellectual curiosity and integrity.
Education & Experience
7+ years to 15 years A minimum of 12+ years of experience in IT, majority in information system design.
3+ years to 7 years A minimum of 5 years as a Data Architect with progressively increasing responsibility.
Bachelor's Degree Computer and Information Science bachelor’s in computer science Required.
Master's Degree Computer and Information Science MS preferred.
Note
THESE EMAILS ARE GENERATED BY KEYWORD AND I APOLOGIZE IF THESE SKILLS SETS DO NOT MATCH YOUR EXPERTISE, OR IF THE LOCATION IS OUT OF RANGE.
We do have other opportunities available. If you are interested, please send me your latest resume.
If you are not currently seeking employment, or if you would prefer, I contact you at some later date, please indicate your date of availability so that I may honor your request.
Show more
Show less","Data Pipelines, Data Architecture, Data Quality, Data Security, Data Warehousing, Data Lakehouse, Data Governance, SQL, Unix/Shell, Python, Apache Spark, Apache Flink, CI/CD, ETL, Data Modeling","data pipelines, data architecture, data quality, data security, data warehousing, data lakehouse, data governance, sql, unixshell, python, apache spark, apache flink, cicd, etl, data modeling","apache flink, apache spark, cicd, data architecture, data governance, data lakehouse, data quality, data security, datamodeling, datapipeline, datawarehouse, etl, python, sql, unixshell"
Senior Data Engineer,SiriusXM,"Atlanta, GA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-siriusxm-3732586345,2023-12-17,Smyrna,United States,Mid senior,Hybrid,"Who We Are
SiriusXM and its brands (Pandora, SXM Media, AdsWizz, Simplecast, and SiriusXM Connected Vehicle Services) are leading a new era of audio entertainment and services by delivering the most compelling subscription and ad-supported audio entertainment experience for listeners -- in the car, at home, and anywhere on the go with connected devices. Our vision is to shape the future of audio, where everyone can be effortlessly connected to the voices, stories and music they love wherever they are.
This is the place where a diverse group of emerging talent and legends alike come to share authentic and purposeful songs, stories, sounds and insights through some of the best programming and technology in the world. Our critically-acclaimed, industry-leading audio entertainment encompasses music, sports, comedy, news, talk, live events, and podcasting. No matter their individual role, each of our employees plays a vital part in bringing SiriusXM’s vision to life every day.
SiriusXM
SiriusXM is the leading audio entertainment company in North America, and the premier programmer and platform for subscription and digital advertising-supported audio products. SiriusXM’s platforms collectively reach approximately 150 million listeners, the largest digital audio audience across paid and free tiers in North America, and deliver music, sports, talk, news, comedy, entertainment and podcasts. Pandora, a subsidiary of SiriusXM, is the largest ad-supported audio entertainment streaming service in the U.S. SiriusXM's subsidiaries Simplecast and AdsWizz make it a leader in podcast hosting, production, distribution, analytics and monetization. The Company’s advertising sales organization, which operates as SXM Media, leverages its scale, cross-platform sales organization and ad tech capabilities to deliver results for audio creators and advertisers. SiriusXM, through Sirius XM Canada Holdings, Inc., also offers satellite radio and audio entertainment in Canada. In addition to its audio entertainment businesses, SiriusXM offers connected vehicle services to automakers.
How You’ll Make An Impact
In this role you’ll join a team who designs, builds, and maintains data products that enable critical decision making across the company through experimentation. Experimentation is a key driver of the product development process, and this role will put you at the center of the platform that solves this at scale.
What You’ll Do
Build flexible cloud-based data pipeline frameworks to produce statistical analyses of user outcomes
Build and improve workflow orchestration tooling to support efficient data pipelines. E.g. airflow plugins, systems integration, deployments
Build tooling to support ML workflows for statistical analysis
Build monitoring dashboards for stakeholders to better understand the health, performance, and cost of the platform
Write documentation to encourage adoption of platform tools and support users in their use
Strengthen corporate best practices around data engineering software development processes
Collaborate closely with cross-functional teams to launch new analyses in a way that’s reliable and scalable
What You’ll Need
5+ years’ experience developing data ETL pipelines and data tools in Scala and/or Python
BS/MS or above in Computer Science or relevant experience
Experience with data warehouse technologies: MapReduce, HDFS, Hive, Tez, Spark, Sqoop
Experience with streaming technologies - Kafka, Kafka Connect, KStreams, KSQL, Beam, Flink, Spark
Experience developing SQL applications of significant complexity
Experience with cloud computing - Google Cloud Platform, Amazon Web Services
Experience developing for Linux-based deployment platforms, developing scalable server-side software for deployment
Experience developing service-oriented architectures/orchestration
Experience with API design/development – RPC, REST, JSON
Experience with unit and integration testing frameworks
Experience with CI/CD, build and deployment technologies such as Jenkins
Experience with Data Visualization or Data Notebook tools (i.e Zeppelin, Tableau, etc.)
Experience working in a Cloud Environment (AWS, GCP, etc.)
Experience developing and deploying machine learning algorithms
Experience developing with additional languages - R, Scala
Experience with workflow tools – Airflow/Composer/Luigi
Experience with data serialization system - Avro, Protobuf
Interpersonal skills and ability to interact and work with staff at all levels.
Excellent written and verbal communication skills.
Ability to work independently and in a team environment.
Ability to project professionalism over the phone and in person.
Ability to handle multiple tasks in a fast-paced environment.
Commitment to “internal client” and customer service principles.
Willingness to take initiative and to follow through on projects.
Creative writing ability.
Excellent time management skills, with the ability to prioritize and multi-task, and work under shifting deadlines in a fast-paced environment.
Must have legal right to work in the U.S
At SiriusXM, we carefully consider a wide range of factors when determining compensation, including your background and experience. These considerations can cause your compensation to vary. We expect the base salary for this position to be in the range of $126,000 to $145,800 and will depend on your skills, qualifications, and experience. Additionally, this role might be eligible for discretionary short-term and long-term incentives. We encourage all interested candidates to apply.
Our goal at SiriusXM is to provide and maintain a work environment that fosters mutual respect, professionalism and cooperation. SiriusXM is an equal opportunity employer that does not discriminate on the basis of actual or perceived race, creed, color, religion, national origin, ancestry, alienage or citizenship status, age, disability or handicap, sex, gender identity, marital status, familial status, veteran status, sexual orientation or any other characteristic protected by applicable federal, state or local laws.
The requirements and duties described above may be modified or waived by the Company in its sole discretion without notice.
R-2023-08-81
Show more
Show less","Data ETL pipelines, Scala, Python, SQL applications, Hadoop Distributed File System (HDFS), MapReduce, Hive, Apache Tez, Apache Spark, Sqoop, Kafka, Kafka Connect, KStreams, KSQL, Beam, Flink, Google Cloud Platform (GCP), Amazon Web Services (AWS), Linuxbased deployment platforms, Serviceoriented architectures, API design/development, RPC, REST, JSON, Unit and integration testing frameworks, Jenkins, Zeppelin, Tableau, Cloud Environment (AWS GCP), Machine learning algorithms, R, Workflow tools (Airflow/Composer/Luigi), Data serialization system, Avro, Protobuf","data etl pipelines, scala, python, sql applications, hadoop distributed file system hdfs, mapreduce, hive, apache tez, apache spark, sqoop, kafka, kafka connect, kstreams, ksql, beam, flink, google cloud platform gcp, amazon web services aws, linuxbased deployment platforms, serviceoriented architectures, api designdevelopment, rpc, rest, json, unit and integration testing frameworks, jenkins, zeppelin, tableau, cloud environment aws gcp, machine learning algorithms, r, workflow tools airflowcomposerluigi, data serialization system, avro, protobuf","amazon web services aws, apache spark, apache tez, api designdevelopment, avro, beam, cloud environment aws gcp, data etl pipelines, data serialization system, flink, google cloud platform gcp, hadoop distributed file system hdfs, hive, jenkins, json, kafka, kafka connect, ksql, kstreams, linuxbased deployment platforms, machine learning algorithms, mapreduce, protobuf, python, r, rest, rpc, scala, serviceoriented architectures, sql applications, sqoop, tableau, unit and integration testing frameworks, workflow tools airflowcomposerluigi, zeppelin"
Lead Data Engineer,GSquared Group,"Atlanta, GA",https://www.linkedin.com/jobs/view/lead-data-engineer-at-gsquared-group-3749346264,2023-12-17,Smyrna,United States,Mid senior,Hybrid,"Lead Data Engineer
Atlanta, Georgia – 100% Remote
Long Term Contract
The Opportunity:
GSquared Group is currently partnering with one of our key clients, a Fortune 500 organization based in Atlanta, to identify a Lead Data Engineer to join their growing team on a long-term contract basis. In this role, you will be responsible for helping in the company's continued efforts to migrate legacy data from their current OnPrem databases to a brand-new AWS cloud environment. As the team Lead, you will need to highly organized and able to pay attention to all of the details across this large initiative, and be responsible for building data pipelines using Spark, Databricks, and AWS.
What are we looking for?
5+ years in data engineering
2+ years as a team lead working on large-scale projects
Enterprise-level experience
Strong attention to detail
Technologies Include:
Spark/PySpark, Databricks, AWS
About GSquared Group:
Shouldn’t your recruiting partner put as much effort and value into your career as you do? With GSquared Group, we take the time to truly understand where you would like to take your career and what is important to you.
GSquared Group is a woman-owned boutique technology services company in the Atlanta area. Founded in 2010, we are a premier provider of IT talent search, management consulting, and software development services. We support a diverse client base that spans all industries and includes Fortune 100 to mid-market companies. We offer direct hire placement, contract, and contract-to-hire positions. We are proud to be known by our community for putting relationships at the core of everything we do.
GSquared Benefits
:
Competitive & Comprehensive Healthcare Package (available only for W2 hourly consultants)
Simple IRA with company match (available only for W2 hourly consultants)
Professional development & networking opportunities
A family-friendly environment
Nice bonuses for referrals
A culture that supports you and your career
Hear what others are saying on Glassdoor: https://www.glassdoor.com/Reviews/GSquared-Group-Reviews-E651488.htm?filter.iso3Language=eng
Show more
Show less","Data Migration, AWS, Spark, Databricks, Data Pipelines, PySpark, Enterprise Data Architecture","data migration, aws, spark, databricks, data pipelines, pyspark, enterprise data architecture","aws, data migration, databricks, datapipeline, enterprise data architecture, spark"
"Data Engineer, Consultant",Thought Logic Consulting,"Atlanta, GA",https://www.linkedin.com/jobs/view/data-engineer-consultant-at-thought-logic-consulting-3779289910,2023-12-17,Smyrna,United States,Mid senior,Hybrid,"Better Focus. Better Results. For Everyone.
Thought Logic is a true business consultancy that prides itself on forming trusted, enjoyable working relationships with our clients and fitting seamlessly into their corporate cultures. We work with the most highly respected and innovative companies in the world by solving and advising them on their most complex and critical business problems. We hire successful experienced consultants who are seeking a place to call home. You won’t be a number or just a resource, but part of a collaborative and growing organization. If you are interested in work-life balance, being home during the week with your family and friends, but still working with a reputable firm made up of smart and experienced professionals, come join us!
What You Need
Successful candidates should demonstrate leadership and flexibility, as well as the ability to collaborate effectively with executive-level clients, project team members, and support staff. Candidates should possess strong core consulting skills and be able to apply critical problem-solving methodology and project management rigor to drive the formulation of strategy and execution of initiatives for our clients.
What You Will Do
Design, develop, and maintain robust, scalable, and efficient data pipelines to support the collection, processing, and storage of structured and unstructured data from various sources.
Collaborate with cross-functional teams, including data analysts and consultants, to understand business requirements and translate them into technical data solutions.
Develop data integration processes to combine data from different sources, ensuring data accuracy, consistency, and quality.
Implement and manage ETL (Extract, Transform, Load) processes to transform raw data into usable formats for analysis and reporting.
Design and optimize data models and databases for performance and scalability, considering both current and future needs.
Work with cloud-based platforms (such as AWS, Azure, or GCP) to deploy and manage data processing and storage solutions.
Monitor and troubleshoot data pipelines to identify and address performance bottlenecks, data quality issues, and other technical challenges.
Collaborate with the technology team to implement data security, access controls, and compliance with relevant data privacy regulations.
Stay current with industry trends and emerging technologies to continuously improve data engineering practices within the firm.
Building ETL / ELT and data streaming pipelines using Apache Airflow, Spark, EMR, Glue, or similar technologies
Programming big data processing pipelines using Python on AWS Lambdas or other pipelining technologies
Qualifications
Bachelors or advanced degree in Computer Science, Data Engineering, Information Technology, or a related field.
Proven experience as a Data Engineer, preferably in a consulting or professional services environment.
Hands-on experience with programming languages such as Python, Java, or Scala.
Proficiency in SQL and experience with relational and NoSQL databases.
Familiarity with cloud platforms and services (e.g., AWS, Azure, GCP) for data storage and processing.
Knowledge of data modeling principles and practices for optimal performance and scalability.
Experience with data visualization tools and techniques is a plus.
Strong problem-solving skills and the ability to work effectively in a collaborative team environment.
Excellent communication skills to convey complex technical concepts to non-technical stakeholders
Nice To Have
Experience in AWS Analytics such as Athena, EMR, Glue, Redshift, Kinesis
Experience in AWS Compute such as EC2, Lambda, Beanstalk, Batch or ECS
Experience in AWS Management and Governance suite of products such as CloudTrail, CloudWatch, Step-functions
Experience using AWS Application Integration Services such as: Simple Notification Service (SNS), Simple Queue Service (SQS), Step Functions
Strong proficiency in data integration, ETL processes, and data warehousing concepts.
Experience with enterprise data lakes, data warehouses, data marts, and big data
Thought Logic is an equal opportunity employer. Thought Logic prohibits discrimination and harassment of any type and affords equal employment opportunities to employees and applicants without regard to race, color, religion, sex, sexual orientation, gender identity or expression, pregnancy, age, national origin, disability status, genetic information, protected veteran status, or any other characteristic protected by law. Thought Logic conforms to the spirit as well as to the letter of all applicable laws and regulations.
Show more
Show less","Leadership, Flexibility, Collaboration, ProblemSolving, Project Management, Python, Java, Scala, SQL, Relational Databases, NoSQL Databases, AWS, Azure, GCP, Data Modeling, Cloud Platforms, Data Visualization, Athena, EMR, Glue, Redshift, Kinesis, EC2, Lambda, Beanstalk, Batch, ECS, CloudTrail, CloudWatch, StepFunctions, SNS, SQS, ETL, Data Warehousing, Data Lakes, Data Marts, Big Data","leadership, flexibility, collaboration, problemsolving, project management, python, java, scala, sql, relational databases, nosql databases, aws, azure, gcp, data modeling, cloud platforms, data visualization, athena, emr, glue, redshift, kinesis, ec2, lambda, beanstalk, batch, ecs, cloudtrail, cloudwatch, stepfunctions, sns, sqs, etl, data warehousing, data lakes, data marts, big data","athena, aws, azure, batch, beanstalk, big data, cloud platforms, cloudtrail, cloudwatch, collaboration, data lakes, data marts, datamodeling, datawarehouse, ec2, ecs, emr, etl, flexibility, gcp, glue, java, kinesis, lambda, leadership, nosql databases, problemsolving, project management, python, redshift, relational databases, scala, sns, sql, sqs, stepfunctions, visualization"
"Senior Data Engineer, Talent Analytics",RVO Health,"Atlanta, GA",https://www.linkedin.com/jobs/view/senior-data-engineer-talent-analytics-at-rvo-health-3766411237,2023-12-17,Smyrna,United States,Mid senior,Hybrid,"AT A GLANCE
RVO Health is looking to grow our Talent Analytics team by adding a Senior Data Engineer. In this role, you’ll be challenged to help shape our Talent Analytics strategy while working on high-priority data efforts – all in line with our broader mission of attracting diverse talent and giving them an experience that will bring out their very best. You will be responsible for scoping, executing, and delivering technical projects to stakeholders across the Human Capital organization, and producing data engineering & analytical solutions that connect them to the data they need.
Where You'll Be
To prioritize togetherness, culture, and accountability, RVO Health operates on a hybrid in-office work schedule. We expect employees to work from our Atlanta office Tuesday, Wednesday and Thursday each week. You are welcome to work remotely Mondays and Fridays if you wish.
Atlantic Station
271 17th Street, NW Atlanta, GA 30363
What You’ll Do
Develop/maintain data pipelines from various data sources (ADP WFN, Greenhouse Recruiting/Onboarding, CultureAmp, Docebo, etc) to a target data warehouse using batch data load strategies utilizing cutting edge cloud technologies.
Conduct hands-on, advanced data engineering & analytics using multiple data sources originating from different applications and systems.
Collaborate with the data science team to identify new opportunities for deep analytics within the Human Capital organization.
Provide input into strategies as they drive the team forward with delivery of business value and technical acumen.
Execute on proof of concepts, where appropriate, to help improve our technical processes.
Documenting database designs that include data models, metadata, ETL specifications and process flows for business data project integrations.
What We’re Looking For
5+ years of Data Engineering experience
3+ years of writing SQL experience against complex databases for data extraction using AWS Athena (Presto), Databricks Delta Lake along with Data Modeling & Data warehousing experience.
3+ years of experience working on Spark (RDDs / Data Frames / Dataset API) using Scala/Python to build and maintain complex ETL pipelines and experience data processing using Parquet and Avro
3+ years of Python coding experience, familiar with utilizing packages such as pandas, boto3, requests, json, csv, os
3+ years of experience working on AWS services including Glue, Athena, Lambda, S3, SNS, SQS, Cloud formation, Step Functions, Serverless architecture.
Experience with GitHub, Code check-in, versioning, Git commands
Introduce and drive adoption of CI/CD framework within the team and build/deploy CI/CD Pipelines using Terraform or AWS Cloud Formation
Experience with visualization tools such as Tableau, Looker or PowerBI to build dynamic/scalable dashboards and reports.
Strong analytical and interpersonal skills
Knowledge or experience within Talent/People analytics is a plus
Enthusiastic, highly motivated and ability to learn quickly.
Able to work through ambiguity in a fast-paced, dynamically changing business environment.
Ability to manage multiple tasks at the same time with minimal supervision.
Pursuant to various state Fair Pay Acts, below is a summary of compensation elements for this role at the company. The following benefits are provided by RVO Health, subject to eligibility requirements.
Starting Salary: $100,000 - $170,000
Note actual salary is based on geographic location, qualifications and experience
Access to a Free Udemy for Business subscription—thousands of hours of learning content on hundreds of different subjects at your fingertips
Health Insurance Coverage (medical, dental, and vision)
Life Insurance
Short and Long-Term Disability Insurance
Flexible Spending Accounts
Paid Time Off
Holiday Pay
401(k) with match
Employee Assistance Program
Paid Parental Bonding Benefit Program
This position may occasionally require travel for training and other work-related duties.
Who We Are
Founded in 2022, RVO Health is a new healthcare platform of digital media brands, services and technologies focused on building relationships with people throughout their health & wellness journey. We meet people where they are in their personal health journeys and connect them with both the information and the care they need. RVO Health was created by joining teams from both Red Ventures and UnitedHealth Group’s Optum Health. Together we’re focused on delivering on our vision of a stronger and healthier world.
RVO Health is comprised of Healthline Media (Healthline, Medical News Today, Psych Central, Greatist and Bezzy), Healthgrades, FindCare and PlateJoy; Optum Perks, Optum Store and the virtual coaching platforms Real Appeal, Wellness Coaching, and QuitForLife.
We offer competitive salaries and a comprehensive benefits program for full-time employees, including medical, dental and vision coverage, paid time off, life insurance, disability coverage, employee assistance program, 401(k) plan and a paid parental leave program.
RVO Health is an equal opportunity employer that does not discriminate against any employee or applicant because of race, creed, color, religion, gender, sexual orientation, gender identity/expression, national origin, disability, age, genetic information, veteran status, marital status, pregnancy or any other basis protected by law. Employment at RVO Health is based solely on a person's merit and qualifications.
We are committed to providing equal employment opportunities to qualified individuals with disabilities. This includes providing reasonable accommodation where appropriate. Should you require a reasonable accommodation to apply or participate in the job application or interview process, please contact accommodations@rvohealth.com.
RVO Health Privacy Policy: https://rvohealth.com/legal/privacy
Show more
Show less","Data Engineering, Data Warehousing, Data Modelling, Data Analytics, SQL, AWS Athena (Presto), Databricks Delta Lake, Apache Spark (RDDs / Data Frames / Dataset API), Scala, Python, Pandas, Boto3, Requests, JSON, CSV, OS, AWS Glue, AWS Lambda, AWS S3, AWS SNS, AWS SQS, AWS CloudFormation, AWS Step Functions, Serverless Architecture, GitHub, CI/CD, Terraform, Tableau, Looker, PowerBI, Talent Analytics, People Analytics","data engineering, data warehousing, data modelling, data analytics, sql, aws athena presto, databricks delta lake, apache spark rdds data frames dataset api, scala, python, pandas, boto3, requests, json, csv, os, aws glue, aws lambda, aws s3, aws sns, aws sqs, aws cloudformation, aws step functions, serverless architecture, github, cicd, terraform, tableau, looker, powerbi, talent analytics, people analytics","apache spark rdds data frames dataset api, aws athena presto, aws cloudformation, aws glue, aws lambda, aws s3, aws sns, aws sqs, aws step functions, boto3, cicd, csv, data engineering, data modelling, dataanalytics, databricks delta lake, datawarehouse, github, json, looker, os, pandas, people analytics, powerbi, python, requests, scala, serverless architecture, sql, tableau, talent analytics, terraform"
Senior Data Engineer,PamTen Inc,"Atlanta, GA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-pamten-inc-3767587585,2023-12-17,Smyrna,United States,Mid senior,Hybrid,"Job Description
As a Data Architect, you will play a pivotal role in shaping our data infrastructure and ensuring that data flows seamlessly through our organization. Your primary responsibilities will include:
Designing, developing, and maintaining end-to-end master data pipelines that efficiently collect, process, transform, and load data from diverse sources into our data ecosystem.
Collaborating with cross-functional teams to understand business requirements and translate them into effective data architecture solutions.
Ensuring data quality, integrity, and security throughout the data lifecycle by implementing best practices and standards.
Optimizing data pipelines for performance, scalability, and reliability, and identifying opportunities for automation and process improvement.
Utilizing your expertise in SQL, Unix/Shell scripting, Python, and data processing frameworks to create and manage data transformations and integrations.
Utilizing concepts like Data warehouse, Data Lake house, Data Mess to share enterprise data strategies.
Working closely with DevOps and Engineering teams to implement continuous integration and continuous deployment (CI/CD) pipelines for data-related processes.
Staying current with industry trends and emerging technologies in data architecture and applying that knowledge to drive innovation within the organization.
Bachelor's degree in computer science, Information Technology, or a related field (Master's degree preferred).
A minimum of 5 years of experience in data architecture, with a proven track record of designing and deploying master data pipelines.
Proficiency in SQL for data manipulation and retrieval from relational databases.
Strong scripting skills in Unix/Shell and Python for automating data processes and transformations.
Experience with data processing frameworks such as Apache Spark, Apache Flink, or similar technologies.
Familiarity with CI/CD tools and practices for automating deployment and monitoring of data pipelines.
Excellent problem-solving skills and the ability to optimize data workflows for performance and efficiency.
Solid understanding of data modeling, data warehousing concepts, and ETL processes.
Strong communication skills to collaborate effectively with cross-functional teams and articulate complex technical concepts to non-technical stakeholders.
Knowledge, Skills, And Abilities
In-depth experience of designing and implementing information solutions.
Experience in architecture practice, tools, and methodologies.
Demonstrate the ability to work well with others and exhibit leadership.
Have a track record of remaining unbiased toward specific technologies or vendors.
Be an excellent communicator and collaborator, engaging with multiple technical and business stakeholders and leaders.
Be able to translate the information architecture contribution to business outcomes into simple briefings for use by various data-and-analytics-related roles.
Organizationally savvy, with a good understanding of the enterprise's political climate and how to navigate, influence and persuade political waters.
Ability to communicate, influence and persuade peers and leadership.
Ability to understand the long-term ( ""big picture "") and short-term perspectives of situations.
Ability to quickly comprehend the functions and capabilities of new technologies.
Displays intellectual curiosity and integrity.
Education & Experience
7+ years to 15 years A minimum of 12+ years of experience in IT, majority in information system design.
3+ years to 7 years A minimum of 5 years as a Data Architect with progressively increasing responsibility.
Bachelor's Degree Computer and Information Science Bachelor's in computer science Required.
Master's Degree Computer and Information Science MS preferred.
Show more
Show less","Data architecture, Data pipelines, Master data management, Data quality & governance, Data modeling, Data warehousing, Data lakehouse, Data mesh, ETL processes, SQL, Unix/Shell scripting, Python, Apache Spark, Apache Flink, Data processing frameworks, CI/CD tools & practices, DevOps, Problemsolving, Communication skills, Presentation skills, Leadership skills, Collaboration skills, Business acumen, Political savvy, Ability to learn new technologies, Intellectual curiosity","data architecture, data pipelines, master data management, data quality governance, data modeling, data warehousing, data lakehouse, data mesh, etl processes, sql, unixshell scripting, python, apache spark, apache flink, data processing frameworks, cicd tools practices, devops, problemsolving, communication skills, presentation skills, leadership skills, collaboration skills, business acumen, political savvy, ability to learn new technologies, intellectual curiosity","ability to learn new technologies, apache flink, apache spark, business acumen, cicd tools practices, collaboration skills, communication skills, data architecture, data lakehouse, data mesh, data processing frameworks, data quality governance, datamodeling, datapipeline, datawarehouse, devops, etl, intellectual curiosity, leadership skills, master data management, political savvy, presentation skills, problemsolving, python, sql, unixshell scripting"
Staff Data Engineer,RVO Health,"Atlanta, GA",https://www.linkedin.com/jobs/view/staff-data-engineer-at-rvo-health-3787305369,2023-12-17,Smyrna,United States,Mid senior,Hybrid,"AT A GLANCE
RVO Health is looking for a talented Staff Data Engineer to join our team! As a Staff Data Engineer at RVO Health, you will have the chance to build technology that drives real improvements to consumer health outcomes and has the potential to have widespread impact across the healthcare industry. You will design, develop, test, and maintain big data pipelines for ingestion, segmentation, and reporting to drive our vision!
What You'll Do
Provide technology ownership for data solutions for projects that the team has been tasked with.
Work with a cross functional team of business analysts, architects, engineers, data analysts and data scientists to formulate both business and technical requirements.
Design and build data pipelines from various data sources to a target data warehouse using batch data load strategies utilizing cutting edge cloud technologies.
Conceptualizing and generating infrastructure that allows data to be accessed and analyzed effectively.
Documenting database designs that include data models, metadata, ETL specifications and process flows for business data project integrations.
Perform periodic code reviews and test plans to ensure data quality and integrity.
Provide input into strategies as they drive the team forward with delivery of business value and technical acumen.
Execute on proof of concepts, where appropriate, to help improve our technical processes.
Conduct Technical assessments and mentor junior team members
What We're Looking For
7-10 years of Data Engineering experience
4+ years of experience working on Spark (RDDs / Data Frames / Dataset API) using Scala/Python to build and maintain complex ETL pipelines.
4+ years of experience in the big data space
Experience in translating business requirements into technical data solutions on a large scale.
2+ years of experience working on AWS (Kinesis / Kafka / S3 / RedShift) or Azure.
Able to research and troubleshoot potential issues presented by stakeholders within the data ecosystem.
Experience with GitHub and CI/CD processes
Experience with Compute technologies like EMR and Databricks
Experience working job orchestration (eg., Airflow / AWS Step Function)
Experience with Data Modeling, Data warehousing
Working with Kubernetes is a plus
Strong analytical and interpersonal skills.
Enthusiastic, highly motivated and ability to learn quickly.
Able to work through ambiguity in a fast-paced, dynamically changing business environment.
Ability to manage multiple tasks at the same time with minimal supervision.
Pursuant to various state Fair Pay Acts, below is a summary of compensation elements for this role at the company. The following benefits are provided by RVO Health, subject to eligibility requirements.
Starting Salary: $150,000 - $200,000
Note actual salary is based on geographic location, qualifications and experience
Access to a Free Udemy for Business subscription—thousands of hours of learning content on hundreds of different subjects at your fingertips
Health Insurance Coverage (medical, dental, and vision)
Life Insurance
Short and Long-Term Disability Insurance
Flexible Spending Accounts
Paid Time Off
Holiday Pay
401(k) with match
Employee Assistance Program
Paid Parental Bonding Benefit Program
Who We Are
Founded in 2022, RVO Health is a new healthcare platform of digital media brands, services and technologies focused on building relationships with people throughout their health & wellness journey. We meet people where they are in their personal health journeys and connect them with both the information and the care they need. RVO Health was created by joining teams from both Red Ventures and UnitedHealth Group’s Optum Health. Together we’re focused on delivering on our vision of a stronger and healthier world.
RVO Health is comprised of Healthline Media (Healthline, Medical News Today, Psych Central, Greatist and Bezzy), Healthgrades, FindCare and PlateJoy; Optum Perks, Optum Store and the virtual coaching platforms Real Appeal, Wellness Coaching, and QuitForLife.
We offer competitive salaries and a comprehensive benefits program for full-time employees, including medical, dental and vision coverage, paid time off, life insurance, disability coverage, employee assistance program, 401(k) plan and a paid parental leave program.
RVO Health is an equal opportunity employer that does not discriminate against any employee or applicant because of race, creed, color, religion, gender, sexual orientation, gender identity/expression, national origin, disability, age, genetic information, veteran status, marital status, pregnancy or any other basis protected by law. Employment at RVO Health is based solely on a person's merit and qualifications.
We are committed to providing equal employment opportunities to qualified individuals with disabilities. This includes providing reasonable accommodation where appropriate. Should you require a reasonable accommodation to apply or participate in the job application or interview process, please contact accommodations@rvohealth.com.
RVO Health Privacy Policy: https://rvohealth.com/legal/privacy
Show more
Show less","Data Engineering, Spark (RDDs Data Frames Dataset API), Scala, Python, AWS (Kinesis Kafka S3 RedShift), Azure, GitHub, CI/CD, Airflow, AWS Step Function, Kubernetes, Data Modeling, Data Warehousing, EMR, Databricks","data engineering, spark rdds data frames dataset api, scala, python, aws kinesis kafka s3 redshift, azure, github, cicd, airflow, aws step function, kubernetes, data modeling, data warehousing, emr, databricks","airflow, aws kinesis kafka s3 redshift, aws step function, azure, cicd, data engineering, databricks, datamodeling, datawarehouse, emr, github, kubernetes, python, scala, spark rdds data frames dataset api"
Sr. Data Engineer,Furniture.com,"Atlanta, GA",https://www.linkedin.com/jobs/view/sr-data-engineer-at-furniture-com-3745120051,2023-12-17,Smyrna,United States,Mid senior,Hybrid,"We are a high growth startup backed by one of America’s top furniture retailers. We are looking for a Sr. Data Engineer to execute our Data Strategy roadmap. This role will be part of a new team accountable for building our critical data ecosystem. You will bring together several diverse data payloads from external partners and customers into specialized data stores. You will work in close collaboration with data science, platform engineering, and product teams to build ETL pipelines, MDM solutions, and access control mechanisms to enable effective governance of the data for product delivery. This is a unique opportunity to be the first in a new department at a rapidly scaling startup.
What you will do:
Build and maintain scalable ETL pipelines effectively transform data from disparate data sources into a canonical data store
Design data architecture and partner with our infrastructure team to execute the build out of the data foundation
Operate current data ingest services while building the next generation of ETL pipelines
Define a common data vocabulary across the organization
Develop audit reports to provide insights into overall system health and resiliency
Contribute to the technology roadmap
What we’re looking for:
Bachelor’s degree in Information Technology, Computer Science, or related field
5+ years as a Data Engineer working on large scale ETL systems
AWS experience developing Integrations and analytical services
Demonstrated proficiency in Python and SQL
Experience with one or more commercial or open-source technologies like Databricks, Informatica, Talend, Airflow, Kafka is essential
Experience collaborating with data scientists and data analyst on projects related to forecasting, dashboards, and reports
Passionate about mentoring other team members and foster a culture of open feedback and communication
Able to communicate effectively with a technical and non-technical audience
Experience building web-based reporting solutions with data visualization technologies like Power BI, Tableau etc. is a plus
Able to contribute to prototypes and architecture by contributing code and designs
Experience working with Product Management in an Agile environment and break down complex features into manageable user stories
Demonstrated track record building data solutions based on a data lake is a plus
The company is a high growth consumer tech start up that is addressing fundamental challenges in the
$200B US furniture space. The company is a B2B and B2C digital aggregator with a simple mission: To
connect furniture customers with the products they desire. For consumers, we make it easy to find the
right furniture by enhancing search and streamlining their end-to-end customer experience. For retail
partners, we deliver a digital platform allowing them to expand their reach with a high-intent furniture
audience. The site experience and discovery tools will differentiate us. The site design and content will
drive brand recognition and loyalty allowing us to capitalize on this market.
We are backed by a top-5 furniture retailer in the US. The business is unique in that it will operate with the agility and innovation of a startup, yet with the robust resources and industry knowledge
set of an established brand.
The company is an equal opportunity employer. We do not discriminate in hiring or employment against any individual based on race, color, gender, national origin, ancestry, religion, physical or mental disability, age, veteran status, sexual orientation, gender identity or expression, marital status, pregnancy, citizenship, or any other factor protected by anti-discrimination laws
Applicants must be authorized to work in the U.S. This role is hybrid work model and candidates must be able to commute to the NE Atlanta/Brookhaven area.
Show more
Show less","Data Engineering, ETL Pipelines, Data Architecture, AWS, Python, SQL, Databricks, Informatica, Talend, Airflow, Kafka, Power BI, Tableau, Data Visualization, User Stories, Data Lake, Agile Methodology","data engineering, etl pipelines, data architecture, aws, python, sql, databricks, informatica, talend, airflow, kafka, power bi, tableau, data visualization, user stories, data lake, agile methodology","agile methodology, airflow, aws, data architecture, data engineering, data lake, databricks, etl pipelines, informatica, kafka, powerbi, python, sql, tableau, talend, user stories, visualization"
Senior Database Engineer,Mastercard,"Atlanta, GA",https://www.linkedin.com/jobs/view/senior-database-engineer-at-mastercard-3785147024,2023-12-17,Smyrna,United States,Mid senior,Hybrid,"Our Purpose
We work to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments and businesses realize their greatest potential. Our decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. We cultivate a culture of inclusion for all employees that respects their individual strengths, views, and experiences. We believe that our differences enable us to be a better team – one that makes better decisions, drives innovation and delivers better business results.
Title And Summary
Senior Database Engineer
The database engineering team is looking for a Senior Database Engineer to help set direction for MasterCard’s use and best practices for our application database solutions. You will work with application teams to implement sound RDMS data solutions that follow industry best practices and methods.
You will also be responsible for migrating existing databases to AWS. You will design, automate, and manage databases as a service leveraging deep experience in Open-Source database technologies like MySQL, Aurora, and other Cloud Native database technologies.
Role
Optimize and maintain legacy systems in MySQL & MongoDB
Implement differing approaches to data management & administration in AWS
Partner with dev teams to create DevSecOps constructed databases in AWS
Develop plans to migrate legacy on-premises databases into AWS data stores
Engineer various database services across multiple AWS services and regions
Be THE escalation point for solving database usage issues and malfunction
All About You
Deep experience with database engineering
Experience sharding, clustering, HA, scaling, monitoring, deploying, tuning, etc.
Experience being the escalation point for any and all DBE/DBA issues
Sr. experience in cloud databases in production, development, and QA environments
Work Conditions
Rotational pager for Production Support
Tech Stack
AWS, Aurora MySQL, MongoDB
In the US, Mastercard is an inclusive Equal Employment Opportunity employer that considers applicants without regard to gender, gender identity, sexual orientation, race, ethnicity, disabled or veteran status, or any other characteristic protected by law. If you require accommodations or assistance to complete the online application process, please contact reasonable_accommodation@mastercard.com and identify the type of accommodation or assistance you are requesting. Do not include any medical or health information in this email. The Reasonable Accommodations team will respond to your email promptly.
Corporate Security Responsibility
All activities involving access to Mastercard assets, information, and networks comes with an inherent risk to the organization and, therefore, it is expected that every person working for, or on behalf of, Mastercard is responsible for information security and must:
Abide by Mastercard’s security policies and practices;
Ensure the confidentiality and integrity of the information being accessed;
Report any suspected information security violation or breach, and
Complete all periodic mandatory security trainings in accordance with Mastercard’s guidelines.
In line with Mastercard’s total compensation philosophy and assuming that the job will be performed in the US, the successful candidate will be offered a competitive base salary based on location, experience and other qualifications for the role and may be eligible for an annual bonus or commissions depending on the role. Mastercard benefits for full time (and certain part time) employees generally include: insurance (including medical, prescription drug, dental, vision, disability, life insurance), flexible spending account and health savings account, paid leaves (including 16 weeks new parent leave, up to 20 paid days bereavement leave), 10 annual paid sick days, 10 or more annual paid vacation days based on level, 5 personal days, 10 annual paid U.S. observed holidays, 401k with a best-in-class company match, deferred compensation for eligible roles, fitness reimbursement or on-site fitness facilities, eligibility for tuition reimbursement, gender-inclusive benefits and many more.
Show more
Show less","MySQL, AWS, Aurora MySQL, MongoDB, Data Modeling, Data Migration, Database Administration, Cloud Computing, SQL, Database Design, NoSQL, DevOps, Data Warehousing, Data Analytics, Sharding, Clustering, HA, Scaling, Monitoring, Tuning, Database Engineering","mysql, aws, aurora mysql, mongodb, data modeling, data migration, database administration, cloud computing, sql, database design, nosql, devops, data warehousing, data analytics, sharding, clustering, ha, scaling, monitoring, tuning, database engineering","aurora mysql, aws, cloud computing, clustering, data migration, dataanalytics, database administration, database design, database engineering, datamodeling, datawarehouse, devops, ha, mongodb, monitoring, mysql, nosql, scaling, sharding, sql, tuning"
Data Engineer - DataStage (1687129),Tier4 Group,"Atlanta, GA",https://www.linkedin.com/jobs/view/data-engineer-datastage-1687129-at-tier4-group-3785726377,2023-12-17,Smyrna,United States,Mid senior,Hybrid,"Data Engineer / Developer (DataStage)
2 days a week onsite in Atlanta, GA 30309
Overview:
We are seeking a Data Engineer with DataStage expertise to design, develop, automate, and support complex applications to extract, transform, and load data; Implement and automate complex processes; Plan and conduct ETL unit and development tests; Monitor results and take corrective actions; Translate data access, transformation, and movement requirements into functional requirements and mapping designs. Use SQL to design database schemas, optimize data retrieval, and implement ETL processes. Investigate, analyze, and resolve complex technical problems related to system functions, environment, and procedures; and Manage service levels, standards, and configurations; Participate in Agile teams to meet goals; Development and support of Kafka integrations, including topics, producers, consumers and streaming applications; Design, develop and maintain Enterprise micro services – security, logging, common APIs and enforcing coding standards.
Required Experience:
5 years of software development and data warehouse experience working extensively with ETL process utilizing IBM Infosphere DataStage/Quality Stage
Automate data workflows such as data ingestion, aggregation, and ETL processing
Experience with large datasets, including structured and unstructured data
Any knowledge of Kafka and messaging
Produce thorough technical design documents, test plans and documented results
Develop API Integrations, Microservices, and build apps using Java and Java based frameworks like Spring Boot in an Agile environment
Strong Skills on SQL, PL/SQL Programming, Query Optimization
Expert with Unix shell scripting
Data Management Strategies (ETL Frameworks, Data Modeling concepts)
Strong understanding of relational databases, oracle functions, procedures, triggers and packages
Show more
Show less","DataStage, Kafka, Microservices, Spring Boot, Java, SQL, PL/SQL, UNIX, ETL, Data modeling, Relational databases, Oracle functions, Procedures, Triggers, Packages, Data warehouse","datastage, kafka, microservices, spring boot, java, sql, plsql, unix, etl, data modeling, relational databases, oracle functions, procedures, triggers, packages, data warehouse","datamodeling, datastage, datawarehouse, etl, java, kafka, microservices, oracle functions, packages, plsql, procedures, relational databases, spring boot, sql, triggers, unix"
Senior Data Engineer,SiriusXM,"Atlanta, GA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-siriusxm-3696002285,2023-12-17,Smyrna,United States,Mid senior,Hybrid,"Who We Are
SiriusXM and its brands (Pandora, SXM Media, AdsWizz, Simplecast, and SiriusXM Connected Vehicle Services) are leading a new era of audio entertainment and services by delivering the most compelling subscription and ad-supported audio entertainment experience for listeners -- in the car, at home, and anywhere on the go with connected devices. Our vision is to shape the future of audio, where everyone can be effortlessly connected to the voices, stories and music they love wherever they are.
This is the place where a diverse group of emerging talent and legends alike come to share authentic and purposeful songs, stories, sounds and insights through some of the best programming and technology in the world. Our critically-acclaimed, industry-leading audio entertainment encompasses music, sports, comedy, news, talk, live events, and podcasting. No matter their individual role, each of our employees plays a vital part in bringing SiriusXM’s vision to life every day.
SiriusXM
SiriusXM is the leading audio entertainment company in North America, and the premier programmer and platform for subscription and digital advertising-supported audio products. SiriusXM’s platforms collectively reach approximately 150 million listeners, the largest digital audio audience across paid and free tiers in North America, and deliver music, sports, talk, news, comedy, entertainment and podcasts. Pandora, a subsidiary of SiriusXM, is the largest ad-supported audio entertainment streaming service in the U.S. SiriusXM's subsidiaries Simplecast and AdsWizz make it a leader in podcast hosting, production, distribution, analytics and monetization. The Company’s advertising sales organization, which operates as SXM Media, leverages its scale, cross-platform sales organization and ad tech capabilities to deliver results for audio creators and advertisers. SiriusXM, through Sirius XM Canada Holdings, Inc., also offers satellite radio and audio entertainment in Canada. In addition to its audio entertainment businesses, SiriusXM offers connected vehicle services to automakers.
How You’ll Make An Impact
In this role you’ll join a team who designs, builds, and maintains data products that enable critical decision making across the company through experimentation. Experimentation is a key driver of the product development process, and this role will put you at the center of the platform that solves this at scale.
What You’ll Do
Build flexible cloud-based data pipeline frameworks to produce statistical analyses of user outcomes
Build and improve workflow orchestration tooling to support efficient data pipelines. E.g. airflow plugins, systems integration, deployments
Build tooling to support ML workflows for statistical analysis
Build monitoring dashboards for stakeholders to better understand the health, performance, and cost of the platform
Write documentation to encourage adoption of platform tools and support users in their use
Strengthen corporate best practices around data engineering software development processes
Collaborate closely with cross-functional teams to launch new analyses in a way that’s reliable and scalable
What You’ll Need
5+ years’ experience developing data ETL pipelines and data tools in Scala and/or Python
BS/MS or above in Computer Science or relevant experience
Experience with data warehouse technologies: MapReduce, HDFS, Hive, Tez, Spark, Sqoop
Experience with streaming technologies - Kafka, Kafka Connect, KStreams, KSQL, Beam, Flink, Spark
Experience developing SQL applications of significant complexity
Experience with cloud computing - Google Cloud Platform, Amazon Web Services
Experience developing for Linux-based deployment platforms, developing scalable server-side software for deployment
Experience developing service-oriented architectures/orchestration
Experience with API design/development – RPC, REST, JSON
Experience with unit and integration testing frameworks
Experience with CI/CD, build and deployment technologies such as Jenkins
Experience with Data Visualization or Data Notebook tools (i.e Zeppelin, Tableau, etc.)
Experience working in a Cloud Environment (AWS, GCP, etc.)
Experience developing and deploying machine learning algorithms
Experience developing with additional languages - R, Scala
Experience with workflow tools – Airflow/Composer/Luigi
Experience with data serialization system - Avro, Protobuf
Interpersonal skills and ability to interact and work with staff at all levels.
Excellent written and verbal communication skills.
Ability to work independently and in a team environment.
Ability to project professionalism over the phone and in person.
Ability to handle multiple tasks in a fast-paced environment.
Commitment to “internal client” and customer service principles.
Willingness to take initiative and to follow through on projects.
Creative writing ability.
Excellent time management skills, with the ability to prioritize and multi-task, and work under shifting deadlines in a fast-paced environment.
Must have legal right to work in the U.S
At SiriusXM, we carefully consider a wide range of factors when determining compensation, including your background and experience. These considerations can cause your compensation to vary. We expect the base salary for this position to be in the range of $126,000 to $145,800 and will depend on your skills, qualifications, and experience. Additionally, this role might be eligible for discretionary short-term and long-term incentives. We encourage all interested candidates to apply.
Our goal at SiriusXM is to provide and maintain a work environment that fosters mutual respect, professionalism and cooperation. SiriusXM is an equal opportunity employer that does not discriminate on the basis of actual or perceived race, creed, color, religion, national origin, ancestry, alienage or citizenship status, age, disability or handicap, sex, gender identity, marital status, familial status, veteran status, sexual orientation or any other characteristic protected by applicable federal, state or local laws.
The requirements and duties described above may be modified or waived by the Company in its sole discretion without notice.
R-2023-08-81
Show more
Show less","Python, Scala, MapReduce, HDFS, Hive, Tez, Spark, Sqoop, Kafka, Kafka Connect, KStreams, KSQL, Beam, Flink, SQL, Google Cloud Platform, Amazon Web Services, Linux, API, RPC, REST, JSON, Unit testing, Integration testing, Jenkins, Data Visualization, Data Notebook, Zeppelin, Tableau, AWS, GCP, Avro, Protobuf, Airflow, Composer, Luigi","python, scala, mapreduce, hdfs, hive, tez, spark, sqoop, kafka, kafka connect, kstreams, ksql, beam, flink, sql, google cloud platform, amazon web services, linux, api, rpc, rest, json, unit testing, integration testing, jenkins, data visualization, data notebook, zeppelin, tableau, aws, gcp, avro, protobuf, airflow, composer, luigi","airflow, amazon web services, api, avro, aws, beam, composer, data notebook, flink, gcp, google cloud platform, hdfs, hive, integration testing, jenkins, json, kafka, kafka connect, ksql, kstreams, linux, luigi, mapreduce, protobuf, python, rest, rpc, scala, spark, sql, sqoop, tableau, tez, unit testing, visualization, zeppelin"
Senior Data Engineer,Zenith,"Atlanta, GA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-zenith-3781741330,2023-12-17,Smyrna,United States,Mid senior,Hybrid,"About Zenith
Zenith is the ROI Agency. Our more than 6,000 specialists across 95 markets offer unparalleled capabilities in Media, Data, Technology, Commerce and Content. We put effectiveness at the heart of our work to solve complex challenges, drive successful business outcomes, and grow our clients’ businesses. Zenith is part of Publicis Media, the media arm of Publicis Groupe.
Responsibilities
Manage the complete API development process from conception to deployment.
Write clean, efficient, and reusable code.
Maintain and upgrade the APIs following deployment.
Write application and database code based on business requirements or user stories, architectural requirements, and existing code.
Modify and improve data engineering processes to handle ever larger, more complex, and more types of data sources and pipelines.
Estimate and plan development work, track and report on task progress, and deliver work on schedule.
Write ad-hoc queries based on schema knowledge for various application requirements
Contribute to operational support documentation.
Participate in weekly on-call rotation.
Qualifications
Bachelor's degree in information technology, computer science, or related field.
At least two years' experience as a Python or Node.js developer.
Advanced SQL coding 4+ years, tuning and query optimization, especially within cloud-based data warehouses MS Azure and Amazon Redshift.
In-depth knowledge of libraries, frameworks, and tech stacks
Architecting code for efficienices and technical limitations
Ability to use Asynchronous Programming to improve productivity
Attention to detail, especially in identifying and fixing errors
Knowledge of design patterns
Good understanding of Agile development methodologies
Professional or academic experience working with large data sets for conducting quantitative analysis.
Self-motivated and a self-starter with strong ability to multitask projects/tasks effectively.
Digital media experience, particularly experience working in ad tech industry with a data-centric role plus.
Additional Information
All your information will be kept confidential according to EEO guidelines.
Veterans Encouraged to Apply
Compensation Range: $81,500 - $100,000 annually. This is the pay range the Company believes it will pay for this position at the time of this posting. Consistent with applicable law, compensation will be determined based on the skills, qualifications, and experience of the applicant along with the requirements of the position, and the Company reserves the right to modify this pay range at any time. For this role, the Company will offer medical coverage, dental, vision, disability, 401k, and paid time off.
Show more
Show less","Python, Node.js, SQL, Amazon Redshift, Microsoft Azure, Asynchronous Programming, Agile methodologies, Data analysis, Data engineering, Data warehouses","python, nodejs, sql, amazon redshift, microsoft azure, asynchronous programming, agile methodologies, data analysis, data engineering, data warehouses","agile methodologies, amazon redshift, asynchronous programming, data engineering, data warehouses, dataanalytics, microsoft azure, nodejs, python, sql"
Senior Data Engineer,SiriusXM,"Atlanta, GA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-siriusxm-3770197668,2023-12-17,Smyrna,United States,Mid senior,Hybrid,"Who We Are
SiriusXM and its brands (Pandora, SXM Media, AdsWizz, Simplecast, and SiriusXM Connected Vehicle Services) are leading a new era of audio entertainment and services by delivering the most compelling subscription and ad-supported audio entertainment experience for listeners -- in the car, at home, and anywhere on the go with connected devices. Our vision is to shape the future of audio, where everyone can be effortlessly connected to the voices, stories and music they love wherever they are.
This is the place where a diverse group of emerging talent and legends alike come to share authentic and purposeful songs, stories, sounds and insights through some of the best programming and technology in the world. Our critically-acclaimed, industry-leading audio entertainment encompasses music, sports, comedy, news, talk, live events, and podcasting. No matter their individual role, each of our employees plays a vital part in bringing SiriusXM’s vision to life every day.
SiriusXM
SiriusXM is the leading audio entertainment company in North America, and the premier programmer and platform for subscription and digital advertising-supported audio products. SiriusXM’s platforms collectively reach approximately 150 million listeners, the largest digital audio audience across paid and free tiers in North America, and deliver music, sports, talk, news, comedy, entertainment and podcasts. Pandora, a subsidiary of SiriusXM, is the largest ad-supported audio entertainment streaming service in the U.S. SiriusXM's subsidiaries Simplecast and AdsWizz make it a leader in podcast hosting, production, distribution, analytics and monetization. The Company’s advertising sales organization, which operates as SXM Media, leverages its scale, cross-platform sales organization and ad tech capabilities to deliver results for audio creators and advertisers. SiriusXM, through Sirius XM Canada Holdings, Inc., also offers satellite radio and audio entertainment in Canada. In addition to its audio entertainment businesses, SiriusXM offers connected vehicle services to automakers.
How You’ll Make An Impact
In this role you’ll join a team who designs, builds, and maintains data products that enable critical decision making across the company through experimentation. Experimentation is a key driver of the product development process, and this role will put you at the center of the platform that solves this at scale.
What You’ll Do
Build flexible cloud-based data pipeline frameworks to produce statistical analyses of user outcomes
Build and improve workflow orchestration tooling to support efficient data pipelines. E.g. airflow plugins, systems integration, deployments
Build tooling to support ML workflows for statistical analysis
Build monitoring dashboards for stakeholders to better understand the health, performance, and cost of the platform
Write documentation to encourage adoption of platform tools and support users in their use
Strengthen corporate best practices around data engineering software development processes
Collaborate closely with cross-functional teams to launch new analyses in a way that’s reliable and scalable
What You’ll Need
5+ years’ experience developing data ETL pipelines and data tools in Scala and/or Python
BS/MS or above in Computer Science or relevant experience
Experience with data warehouse technologies: MapReduce, HDFS, Hive, Tez, Spark, Sqoop
Experience with streaming technologies - Kafka, Kafka Connect, KStreams, KSQL, Beam, Flink, Spark
Experience developing SQL applications of significant complexity
Experience with cloud computing - Google Cloud Platform, Amazon Web Services
Experience developing for Linux-based deployment platforms, developing scalable server-side software for deployment
Experience developing service-oriented architectures/orchestration
Experience with API design/development – RPC, REST, JSON
Experience with unit and integration testing frameworks
Experience with CI/CD, build and deployment technologies such as Jenkins
Experience with Data Visualization or Data Notebook tools (i.e Zeppelin, Tableau, etc.)
Experience working in a Cloud Environment (AWS, GCP, etc.)
Experience developing and deploying machine learning algorithms
Experience developing with additional languages - R, Scala
Experience with workflow tools – Airflow/Composer/Luigi
Experience with data serialization system - Avro, Protobuf
Interpersonal skills and ability to interact and work with staff at all levels.
Excellent written and verbal communication skills.
Ability to work independently and in a team environment.
Ability to project professionalism over the phone and in person.
Ability to handle multiple tasks in a fast-paced environment.
Commitment to “internal client” and customer service principles.
Willingness to take initiative and to follow through on projects.
Creative writing ability.
Excellent time management skills, with the ability to prioritize and multi-task, and work under shifting deadlines in a fast-paced environment.
Must have legal right to work in the U.S
At SiriusXM, we carefully consider a wide range of factors when determining compensation, including your background and experience. These considerations can cause your compensation to vary. We expect the base salary for this position to be in the range of $126,000 to $145,800 and will depend on your skills, qualifications, and experience. Additionally, this role might be eligible for discretionary short-term and long-term incentives. We encourage all interested candidates to apply.
Our goal at SiriusXM is to provide and maintain a work environment that fosters mutual respect, professionalism and cooperation. SiriusXM is an equal opportunity employer that does not discriminate on the basis of actual or perceived race, creed, color, religion, national origin, ancestry, alienage or citizenship status, age, disability or handicap, sex, gender identity, marital status, familial status, veteran status, sexual orientation or any other characteristic protected by applicable federal, state or local laws.
The requirements and duties described above may be modified or waived by the Company in its sole discretion without notice.
R-2023-08-81
Show more
Show less","Python, Scala, R, SQL, MapReduce, HDFS, Spark, Sqoop, Kafka, Hive, Tez, KStreams, Beam, Zeppelin, Tableau, Airflow, Composer, Jenkins, Avro, Protobuf, AWS, GCP","python, scala, r, sql, mapreduce, hdfs, spark, sqoop, kafka, hive, tez, kstreams, beam, zeppelin, tableau, airflow, composer, jenkins, avro, protobuf, aws, gcp","airflow, avro, aws, beam, composer, gcp, hdfs, hive, jenkins, kafka, kstreams, mapreduce, protobuf, python, r, scala, spark, sql, sqoop, tableau, tez, zeppelin"
Data Analyst- Level 1,Stratfield Consulting,"Atlanta, GA",https://www.linkedin.com/jobs/view/data-analyst-level-1-at-stratfield-consulting-3785508752,2023-12-17,Smyrna,United States,Mid senior,Hybrid,"Data Analyst
Our client, a purpose-driven company that provides a cloud-based software solution (SaaS) to improve the administrative efficiency and regulatory compliance for government entities, is seeking a Data Analyst to join their growing team.
This is a full-time, permanent position located in the Atlanta corporate headquarters (Buckhead) offering 100% remote workplace with a rare/occasional onsite requirement as needed.
Responsibilities
Contributes as a technical resource, helping to maintain databases, pipelines and other data infrastructure.
Participates in responding to tenant request, supporting the client services, and implementation teams.
Gathers data and conduct analyses in support of tenant request.
Developing tools as needed to support database consistency and quality across databases.
Collaborating with developers to ensure best practices are adhered and followed.
Collaborates with team members as well as operates individually to execute all phases of complex data migration projects.
Participating in tenant data discovery and analysis calls.
Troubleshoots and resolves operational environment issues after data migrations.
Qualification
BS/MS degree or equivalent in Computer Science, Data/Analytics, Information Systems, or related field
Strong analytical and problem-solving skills.
Analytic and critical mindset demonstrated by asking second and third order questions developed through exploratory data analysis.
Required Technical Skills
TSQL
SQL / Relational DB
MS Office: Excel
Powershell
Nice To Have
Azure cloud experience
Linux
Apache NiFi
About Stratfield Consulting
We started Stratfield Consulting with the belief that companies are looking for a more reliable consulting, staffing, and recruiting firm to deliver an expanding list of projects. Our objective is to be the most trusted firm for our clients. We are very proud of the people that we invite into our firm. Our people are:
Experienced consultants - Smart problem solvers.
Easy to work with - Friendly, modest, and humble.
Earnest - We care enough to do the extra work to ensure project success.
Engaged - 100% committed to project success. We believe that our clients come first.
Show more
Show less","TSQL, SQL, Relational Databases, MS Office: Excel, Powershell, Azure Cloud, Linux, Apache NiFi","tsql, sql, relational databases, ms office excel, powershell, azure cloud, linux, apache nifi","apache nifi, azure cloud, linux, ms office excel, powershell, relational databases, sql, tsql"
Sr Data Analyst,The Home Depot,"Atlanta, GA",https://www.linkedin.com/jobs/view/sr-data-analyst-at-the-home-depot-3764796620,2023-12-17,Smyrna,United States,Mid senior,Hybrid,"Position Purpose
A Sr Data Analyst Supply Chain leverages proven technical abilities to synthesize complex analytical tasks into easily understood data-driven stories. Responsible for working collaboratively among other talented analysts to apply established and create new analytical processes on diverse datasets to deduce insights and solve real-world business problems. Also ensures that all reporting and analytical responsibilities are completed competently in a timely manner, continually seeks out opportunities to learn new and hone existing technical skills (e.g. writing SQL/code, statistics, machine learning, etc.) and operates under minimal supervision and mentorship of more experienced managers and data scientists.
Key Responsibilities
30% Leverages data analytics tools to create new dashboards, reports, and any additional ad-hoc requests
20% Executes existing reporting and analytical responsibilities
20% Ensures the quality of work output by displaying a keen attention to detail
20% Presents findings in easily understood ways, focuses on how the data analytics fits into the bigger picture
10% Develops additional technical competencies and subject matter expertise within core functional group
Direct Manager/Direct Reports
This position reports to Manager or Sr Manager
This position has no Direct Reports
Travel Requirements
Typically requires overnight travel less than 10% of the time.
Physical Requirements
Most of the time is spent sitting in a comfortable position and there is frequent opportunity to move about. On rare occasions there may be a need to move or lift light articles.
Working Conditions
Located in a comfortable indoor area. Any unpleasant conditions would be infrequent and not objectionable.
Minimum Qualifications
Must be eighteen years of age or older.
Must be legally permitted to work in the United States.
Preferred Qualifications
2-4 years work experience in data mining, statistical analysis, auditing, and/or forecasting.
An intermediary understanding of SQL or working directly with MS Access joining relational tables together and/or some knowledge of one other scripting/compiled/statistical programming language (e.g. VBA, Python, R, SAS, .NET, C++, Java, etc.)
Experience building reports/analyses with analytical tools (e.g. Excel, JMP, SAS, Mathematica, SPSS, Tableau, etc.)
Prior direct experience in analyzing the relevant subject matter (e.g. Supply Chain, Merchandising, Operations, etc.)
B.S. in Computer Science, Math, Engineering, Finance, or related quantitative field.
Work experience with MS Excel, MS Access, SQL Server, Teradata, Oracle, or comparable database systems.
Minimum Education
The knowledge, skills and abilities typically acquired through the completion of a bachelor's degree program or equivalent degree in a field of study related to the job.
Preferred Education
No additional education
Minimum Years Of Work Experience
2
Preferred Years Of Work Experience
No additional years of experience
Minimum Leadership Experience
None
Preferred Leadership Experience
None
Certifications
None
Competencies
Critical thinking skills to identify the strengths and weaknesses of alternative solutions; ability to understand and foresee implications of new information for current and future problems solving.
An unquenchable intellectual curiosity for getting at the underlying story being told within the data.
Strong written and verbal communications skills. Ability to persuade, inform, and influence others based on findings. A track record of taking complex results and communicating them in an easily understood way.
Superior interpersonal skills and ability to collaborate actively and work in a team environment.
Ability to quickly learn and adapt to new technologies, tools, and techniques.
Show more
Show less","SQL, Statistics, Machine Learning, Data Analytics, Data Mining, Statistical Analysis, Auditing, Forecasting, VBA, Python, R, SAS, .NET, C++, Java, Excel, JMP, Tableau, Teradata, Oracle, MS Excel, MS Access, SQL Server, Leadership, Critical Thinking, Data Analysis, Data Presentation, Interpersonal Skills, Communication Skills, Problem Solving, SQL Programming, Machine Learning","sql, statistics, machine learning, data analytics, data mining, statistical analysis, auditing, forecasting, vba, python, r, sas, net, c, java, excel, jmp, tableau, teradata, oracle, ms excel, ms access, sql server, leadership, critical thinking, data analysis, data presentation, interpersonal skills, communication skills, problem solving, sql programming, machine learning","auditing, c, communication skills, critical thinking, data mining, data presentation, dataanalytics, excel, forecasting, interpersonal skills, java, jmp, leadership, machine learning, ms access, ms excel, net, oracle, problem solving, python, r, sas, sql, sql server, statistical analysis, statistics, tableau, teradata, vba"
Healthcare Data Analyst,Myers and Stauffer LC,"Atlanta, GA",https://www.linkedin.com/jobs/view/healthcare-data-analyst-at-myers-and-stauffer-lc-3778606397,2023-12-17,Smyrna,United States,Mid senior,Hybrid,"Exempt/Non-Exempt
Exempt
Scheduled Hours Per Week
40
Myers and Stauffer LC is a certified public accounting and health care reimbursement consulting firm, specializing in audit, accounting, data management and consulting services to government-sponsored health care programs (primarily state Medicaid agencies, and the federal Center for Medicare & Medicaid Services). We have 45+ years of experience assisting our government clients with complex health care reimbursement and provider compliance issues, operate 20 offices and have over 900 associates nationwide.
At Myers and Stauffer, you will have a career that is rewarding while also supporting our state and federal government healthcare clients that focus on those in need. We are committed to providing our employees with professional growth and development opportunities, a diverse, dynamic, challenging work environment, and a strong and visionary leadership team.
What We Offer
Health, Dental, and Vision insurance along with other competitive employee benefits
Vacation time, sick time, paid holidays, and two floating holidays
Paid Parental Leave and available support resources
401K with company matching for eligible employees
Tuition reimbursement, referral bonuses, paid volunteer community service time, mentor program, and a variety of other employee programs and perks
A combination of technical and leadership development training at each career milestone
Up to six counseling sessions per year for eligible employees through our Employee Assistance Program
The Healthcare Data Analyst will play a crucial role in our multi-disciplinary team, utilizing data analytics to identify potential fraud, waste, or abuse of Medicaid or other healthcare system funds. This position requires proficiency in SQL, as the analyst will be responsible for reviewing and analyzing provider claims to determine if overpayments or underpayments have occurred in accordance with applicable policies. Additionally, the analyst will perform reimbursement calculations to quantify mispayments and prepare necessary documentation and reports.
Essential Functions
Independently conduct complex analyses and audits related to Medicaid or other reimbursement-related engagements, with a focus on identifying fraud, waste, abuse, or misuse of funds.
Develop and maintain a comprehensive understanding of Medicaid and/or other payer statutes, regulations, provider billing manuals, and healthcare reimbursement policies.
Conduct research to determine the standards for billing and coding compliance with Medicaid or other government program regulations.
Draft and issue notification letters to healthcare providers regarding identified overpayments or underpayments, ensuring clear and concise communication.
Interpret and analyze healthcare data, including medical, financial, or other records, to identify inconsistencies, anomalies, abnormal billing patterns, and indicators of suspected fraud, waste, or abuse (e.g., services not rendered, up-coding, un-bundling, etc.).
Prepare comprehensive reports summarizing findings and recommendations for corrective actions.
Present findings to clients, effectively communicating complex data analysis in a clear and concise manner.
Collaborate effectively with team members, providing mentorship, training, and assistance as needed.
Maintain the security and confidentiality of all protected health information encountered during the performance of duties.
Participate in client meetings and conference calls as required.
Handle phone calls and emails from providers, addressing any questions or concerns.
Evaluate business processes to identify opportunities for improvement.
Perform additional responsibilities as assigned.
Requirements
Bachelor's degree in Health Information Administration, Health Informatics, Healthcare Management, Information Systems, Data Science, or a related field. A postgraduate degree in a related field is preferred.
Preferred certifications: CFE, AHFI, CHDA, CPMA, RHIT, or RHIA.
1-2 years of experience in healthcare data analysis or a related field is preferred.
SQL proficiency is required, preferably with experience in working with large healthcare datasets.
Familiarity with statistical analysis techniques and data mining methodologies is preferred.
Proficient use of Microsoft Office software (Excel, Word, PowerPoint, etc.).
Organized and detail-oriented, with the ability to think independently and problem solve effectively.
Demonstrated ability to communicate verbally and in writing with all levels of an organization, both internally and externally.
Effective in multitasking, prioritization of assignments, and management of multiple deadlines.
Ability to work in a team environment.
Willingness to travel based on client and business needs (0-10% of the time).
Must be able to travel based on client and business needs
The Team
Myers and Stauffer takes pride in the welcoming and collaborative culture we have throughout our offices. For this position, the employee will report to one of our Managers or Senior Managers on the BPI engagement team.
Work Style
Our general business hours are Monday through Friday 8am-5pm, but can vary based on business needs. Dependent on performance, our in-office associates are eligible for a hybrid work schedule after their initial 90-day training period. As a company, we are always willing to discuss potential flexibility that an employee may need to better suit their work-life wellbeing.
Typical Interview Process
Phone Screen with a Recruiter
In person or remote interview with the hiring manager & team
Hiring decision and job offer
We understand that changing or learning a new industry can discourage strong candidates from applying. Myers and Stauffer is committed to investing in your career development and providing the necessary resources to help you succeed. Please do not hesitate to apply, as you may be the right fit for this position or another position we have open.
REASONABLE ACCOMMODATION
If you are a qualified individual with a disability you may request reasonable accommodation if you are unable or limited in your ability to use or access this site as a result of your disability. You can request a reasonable accommodation by calling 844-558-1414 (toll free) or send an email to accom@cbiz.com .
EQUAL OPPORTUNITY EMPLOYER
CBIZ is an affirmative action-equal opportunity employer and reviews applications for employment without regard to the applicant’s race, color, religion, national origin, ancestry, age, gender, gender identity, marital status, military status, veteran status, sexual orientation, disability, or medical condition or any other reason prohibited by law. If you would like more information about your EEO rights as an applicant under the law, please visit these following pages EEO is the Law and EEO is the Law Supplement .
PAY TRANSPARENCY PROTECTION NOTIFICATION
Notice to Applicants and Associates of Affirmative Action Program
Show more
Show less","SQL, Microsoft Office, Data analysis, Data mining, Healthcare data, Healthcare reimbursement, Medicaid, Fraud detection, Waste detection, Abuse detection, Statistical analysis","sql, microsoft office, data analysis, data mining, healthcare data, healthcare reimbursement, medicaid, fraud detection, waste detection, abuse detection, statistical analysis","abuse detection, data mining, dataanalytics, fraud detection, healthcare data, healthcare reimbursement, medicaid, microsoft office, sql, statistical analysis, waste detection"
Senior Data Science Advisor - Clinical Pharmacy Analytics - Hybrid,The Cigna Group,"Bloomfield, CT",https://www.linkedin.com/jobs/view/senior-data-science-advisor-clinical-pharmacy-analytics-hybrid-at-the-cigna-group-3787345756,2023-12-17,Berlin,United States,Mid senior,Hybrid,"Overview
A career within Forsyth Health’s Data & Analytics team will provide you with the opportunity to help Pharma/Life Science organizations uncover patient and market insights. At Forsyth Health, we focus on a collection of data management, business intelligence and advanced analytics capabilities to support various functions within these organizations to meet their business needs around market access and patient support programs.
How You'll Make a Difference
The Sr. Data Scientist role is a key role to the enterprise and will be supporting a highly complex and growing area within the health care data and analytics services space. As a strong individual contributor, the role will lead client engagements to define, develop and communicate insights critical to Commercial, Market Access, HEOR and Evidence Generation functions at Pharma/Life Science. Responsibilities include leading Outcomes Research studies, Advanced Analytics, ML model development and general analytic support for all stakeholders. This role will work closely with the internal Sales and Technology teams. This person will need to be able to understand the needs within the Commercial Pharma Analytics space and translate those into actionable insights.
Role Summary
The Data Science Advisor- Pharmacy Analytics position is an opportunity for an analytics professional to provide leadership on complex analytics projects and initiatives. This role will work with an innovative team on setting and executing the vision for how advanced embedded analytics can lead Cigna to achieving our growth goals. This role will work collaboratively with our business stakeholders to provide partnership in analytics, developing analytics solutions, leveraging data science and technologic capabilities and embedding analytics driven processes.
The job responsibilities include, but are not limited to the following:
Lead analyses related to Healthcare Resource Utilization, Total Healthcare Cost and Clinical Outcomes with a focus on Specialty Medications/Rare diseases, Channel Management, Utilization Management, and specific Therapeutic area research (root cause analyses, Health Outcome Studies, Opportunity analysis including descriptive and multivariate statistics to identify patterns in the data.)
Develop new reports, models and analytic solutions with innovative ways to present data internally and externally in order to support Forsyth Health’s Sales & Business stakeholders. This requires combining business knowledge and data acumen along with technical (SAS, SQL) skills to efficiently complete these ad-hoc requests. Query data warehouse(s) using variety of tools available. Extract data and manipulate into reports for client both internal and external. Maintain turn-around times per agreements.
Consultation with Data & Analytics matrix partners to develop best practices and help understand complex issues and requests. Cross-Functional collaboration as needed to create alignment with stakeholders.
Project management and prioritization – Advisor role will support multiple projects and will need to be able to work with Forysth Health Sales’ and Analytics team to manage multiple initiatives at the same time and negotiate timelines/priority with stakeholders.
Manage the analytic solution development efforts in the role of a Product Owner, requiring effective project management, technical and functional documentation, communication and stakeholder alignment.
Explore and visualize the data using advanced tools such as including Tableau, PowerBI, Thoughtspot and/or Looker. (Largely Tool agnostic environment)
Extraction and analysis of large healthcare claims data using state of the art big-data infrastructure leveraging cloud and on premise tools i.e. SAS, Analytic platform (Python), R, Teradata, Hadoop, etc.
Qualifications
BS/MS/PhD in Econometrics, Actuarial Science, Data Science, Health Outcomes, Epidemiology, Statistics, or in any technical field that provides a solid basis for analytics highly desired.
Minimum 6 years of industry experience in solving Health Plan, PBM business or Commercial Pharma Analytic problems through the application of analytic approaches.
Prior experience in Healthcare analytics, Specialty Medication or Specialty Condition analytics or Health Outcomes required
A demonstrated ability to understand and effectively communicate (both verbally and written) analytic and clinical data to a varied audience.
Deep healthcare data (e.g., PBM experience, Provider Networks, Billing, Medical and Pharmacy claims), statistical analysis experience, and an understanding of all the associated clinical, utilization and financial levers.
Experience with statistical software/programming languages such as SQL programming, SAS, R, Python and other tools preferred (Python knowledge not requisite but preferred).
Experience with data visualization tools such as Tableau, Thoughtspot or PowerBI.
A data-driven personality w/ Intellectual curiosity and internal motivation.
If you will be working at home occasionally or permanently, the internet connection must be obtained through a cable broadband or fiber optic internet service provider with speeds of at least 10Mbps download/5Mbps upload.
About The Cigna Group
Doing something meaningful starts with a simple decision, a commitment to changing lives. At The Cigna Group, we’re dedicated to improving the health and vitality of those we serve. Through our divisions Cigna Healthcare and Evernorth Health Services, we are committed to enhancing the lives of our clients, customers and patients. Join us in driving growth and improving lives.
Qualified applicants will be considered without regard to race, color, age, disability, sex, childbirth (including pregnancy) or related medical conditions including but not limited to lactation, sexual orientation, gender identity or expression, veteran or military status, religion, national origin, ancestry, marital or familial status, genetic information, status with regard to public assistance, citizenship status or any other characteristic protected by applicable equal employment opportunity laws.
If you require reasonable accommodation in completing the online application process, please email: SeeYourself@cigna.com for support. Do not email SeeYourself@cigna.com for an update on your application or to provide your resume as you will not receive a response.
The Cigna Group has a tobacco-free policy and reserves the right not to hire tobacco/nicotine users in states where that is legally permissible. Candidates in such states who use tobacco/nicotine will not be considered for employment unless they enter a qualifying smoking cessation program prior to the start of their employment. These states include: Alabama, Alaska, Arizona, Arkansas, Delaware, Florida, Georgia, Hawaii, Idaho, Iowa, Kansas, Maryland, Massachusetts, Michigan, Nebraska, Ohio, Pennsylvania, Texas, Utah, Vermont, and Washington State.
Show more
Show less","Data Analysis, Advanced Analytics, Health Plan Analytics, PBM Analytics, Commercial Pharma Analytics, Outcomes Research, ML Model Development, Statistical Analysis, SAS, SQL, R, Python, Tableau, PowerBI, Thoughtspot, Looker, Teradata, Hadoop, Econometrics, Actuarial Science, Health Outcomes, Epidemiology, Statistics","data analysis, advanced analytics, health plan analytics, pbm analytics, commercial pharma analytics, outcomes research, ml model development, statistical analysis, sas, sql, r, python, tableau, powerbi, thoughtspot, looker, teradata, hadoop, econometrics, actuarial science, health outcomes, epidemiology, statistics","actuarial science, advanced analytics, commercial pharma analytics, dataanalytics, econometrics, epidemiology, hadoop, health outcomes, health plan analytics, looker, ml model development, outcomes research, pbm analytics, powerbi, python, r, sas, sql, statistical analysis, statistics, tableau, teradata, thoughtspot"
Data Engineer - Batch Processing,Intelliswift Software,United States,https://www.linkedin.com/jobs/view/data-engineer-batch-processing-at-intelliswift-software-3554226781,2023-12-17,New Brunswick,United States,Mid senior,Remote,"Title: Data Analytics & Engineering - Data Engineer IV
Location: Remote Location, CA, United States
Description:
The main function of the Data Engineer is to develop, evaluate, test and maintain architectures and data solutions within our organization. The typical Data Engineer executes plans, policies, and practices that control, protect, deliver, and enhance the value of the organization’s data assets.
Job Responsibilities:
• Manage data engineering projects through the full cycle.
• Identify and underline business initiatives from a data engineering perspective
• Design, construct, install, test and maintain highly scalable data management systems.
• Ensure systems meet business requirements and industry practices.
• Design, implement, automate and maintain large scale enterprise data ETL processes.
• Build high-performance algorithms, prototypes, predictive models and proof of concepts.
Skills:
• Ability to work as part of a team, as well as work independently or with minimal direction.
• Excellent written, presentation, and verbal communication skills.
• Collaborate with data architects, modelers and IT team members on project goals.
• Strong PC skills including knowledge of Microsoft SharePoint.
Education/Experience:
• Bachelor's degree in a technical field such as computer science, computer engineering or related field required.
• Process certification, such as, Six Sigma, CBPP, BPM, ISO 20000, ITIL, CMMI
Show more
Show less","Data Engineering, Data Management, ETL, Predictive Modeling, Algorithms, Data Architecture, Data Modeling, Microsoft SharePoint, Six Sigma, CBPP, BPM, ISO 20000, ITIL, CMMI","data engineering, data management, etl, predictive modeling, algorithms, data architecture, data modeling, microsoft sharepoint, six sigma, cbpp, bpm, iso 20000, itil, cmmi","algorithms, bpm, cbpp, cmmi, data architecture, data engineering, data management, datamodeling, etl, iso 20000, itil, microsoft sharepoint, predictive modeling, six sigma"
"Direct Client – Sr. BI Developer (Alteryx, SQL, Data Bricks, Tableau, BI Apps, Incorta, Advance Excel)",The AES Group,United States,https://www.linkedin.com/jobs/view/direct-client-%E2%80%93-sr-bi-developer-alteryx-sql-data-bricks-tableau-bi-apps-incorta-advance-excel-at-the-aes-group-3746262036,2023-12-17,New Brunswick,United States,Mid senior,Remote,"Location:
100% Remote (as per PST Hours; Seattle, WA)
This job contributes to company's success by guiding business decisions through utilizing data analysis and consulting that results in predicting outcomes, understanding complex data relationships, and developing a quantitative return on investment. This position partners on a cross-functional team of data scientists, statisticians, and business analysts supporting strategic initiatives utilizing the latest in computing technologies and operating across customer transactions and data elements to unlock the opportunities to support our global business.
Tops 3 Skills Needed
1
Skill
: SQL
8+ years
2
Skill
: Azure (Data tools)
8+ years
3
Skill
: ATL tools
8+ years
Years Of Experience
8+ years
Required Background/ Skills
Functional Skills: Strong Data analysis skills, know how to consolidate, link and transform data.
Other skills: Knows how to drive and discover with minimal hand holding, get goer, coordinate work with different teams, can do multi-tasking. Curious, know how to work with in a team, able to connect quickly with partners
Technology Requirements
Advance Excel, Alteryx, SQL,
Azure Data pipeline components like Data Bricks, Tableau, BI Apps, Incorta,
Understanding of large ERP system data,
Understanding of Large Organization Application eco system.
Daily Responsibilities
Support / mature data swat type stuff - requests, etc
One - off analysis of / support of sustainability, etc
Requests mature and support our all up data maturation
Nice-to-Haves
Preferably an experience with Sustainability and Energy related data Analysis, Understanding of Data Modeling
Disqualifiers
Need both Data Analyst and data engineering experience
Show more
Show less","SQL, Azure (Data tools), ATL tools, Data analysis, Data consolidation, Data linking, Data transformation, Data discovery, Multitasking, Teamwork, Communication, Advanced Excel, Alteryx, Tableau, BI Apps, Incorta, ERP systems, Application ecosystem, Data maturation, Data swat, Sustainability data analysis, Energy data analysis, Data modeling","sql, azure data tools, atl tools, data analysis, data consolidation, data linking, data transformation, data discovery, multitasking, teamwork, communication, advanced excel, alteryx, tableau, bi apps, incorta, erp systems, application ecosystem, data maturation, data swat, sustainability data analysis, energy data analysis, data modeling","advanced excel, alteryx, application ecosystem, atl tools, azure data tools, bi apps, communication, data consolidation, data discovery, data linking, data maturation, data swat, data transformation, dataanalytics, datamodeling, energy data analysis, erp systems, incorta, multitasking, sql, sustainability data analysis, tableau, teamwork"
Need Sr Data Engineer (Direct Client) - Remote,SPAR Information Systems LLC,United States,https://www.linkedin.com/jobs/view/need-sr-data-engineer-direct-client-remote-at-spar-information-systems-llc-3731654778,2023-12-17,New Brunswick,United States,Mid senior,Remote,"Hi Associate,
Hope you are doing good.
I have urgent requirement kindly let me know if you have any resource available.
Role: Sr Data Engineer
Location: Remote
Duration: 3 Months Contract to hire
Job Description
We are seeking a highly motivated Senior Data Engineer to join our Vendor data product team. As a Senior Data Engineer you will be responsible for creating products using data from both internal and external vendor data sources to help realize goals of attracting customers through more accurate pricing, greater customer retention, and an increase in underwriting profitability. The right candidate will develop well-designed, testable, and efficient data ingestion, data enrichment, and data transformation solutions using best software development practices. You should be an analytical thinker, a self-learner, and comfortable supporting the needs of multiple projects. This role is a part of the Data Movement team of Data, Security & Infrastructure (DSI) in our
Technology Solutions
organization.
In This Role, You Will
Identify, design, and implement internal process improvements, automating manual processes, optimizing data delivery for greater scalability
Build the processes required for optimal extraction, transformation, and loading of data using a variety of languages and technologies such as Scala, Python, Kafka, Azure Data Factory, Fivetran/HVR, dbt, and Databricks
Collaborate with stakeholders including the Product, Data Engineering, and Agile Delivery teams in an agile environment to assist and resolve data-related issues and support data delivery needs
Work with other data platform or data domain teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies
Perform unit tests and conduct reviews with other team members to make sure code is rigorously designed, elegantly coded, and effectively tuned for performance
Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community
Experience & Skills
At least 2 years of experience with designing, developing, implementing, and maintaining solutions for Big Data or data warehouse system
At least 2 Years of experience working in a cloud environment such as Azure, AWS or other private or public cloud
Experience performing root cause analysis on internal and external data and processes to answer business questions and identify opportunities for improvement
Strong analytical skills related to working with unstructured datasets
Good experience with bringing data into a centralized data repository or manipulating the available data to build additional data sets for Analytics and Reporting purposes.
Experienced with maintaining data quality throughout the lifecycle of the data.
Experience with Data Modeling, source to target mapping, automated testing frameworks, CI/CD pipelines and task automation using scripting
Experienced with working in Agile environment and end to end automation
Developing new and enhancing existing data processing (Data Ingest, Data Transformation, Data Store, Data Management, Data Quality) components
Strong working knowledge of SQL and the ability to write, debug and optimize SQL queries and ETL jobs to reduce the execution window or reduce resource utilization
Data Engineering experience focused on batch and real-time data pipelines development, Data processing/data transformation using ETL/ELT tools, SnowPipe, dbt, or Databricks
Experience with Cloud Data Warehouse solutions experience (Snowflake, Azure DW, Redshift or similar technology in other private or public clouds).
Complete software development lifecycle experience including design, documentation, implementation, testing, and deployment
Basic Qualifications
Bachelor's Degree in a computer-related field or equivalent professional experience required
At least 2 years of experience in data engineering using open-source technology stack along with cloud computing (AWS, Microsoft Azure, Google Cloud)
At least 2 years of experience with designing, developing, implementing, and maintaining solutions for data ingestion and transformation projects with dbt, SnowPipe, or DataBricks
At least 2 years of advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with cloud databases
Preferred Qualifications
3+ years of experience with dbt, SnowPipe, or DataBricks
3+ years of experience working on real-time data and streaming applications (Spark Streaming or Kafka)
3+ years of experience working with Cloud Data Warehouse solutions (i.e., Snowflake, Synapse, Redshift)
3+ years of experience with Agile engineering practices
Rahul.B
Team Lead
SPAR Information Systems
Email: rahul@sparinfosys.com
Show more
Show less","Scala, Python, Kafka, Azure Data Factory, Fivetran/HVR, dbt, Databricks, Agile, SQL, Data Engineering, Cloud Computing, AWS, Microsoft Azure, Google Cloud, Data Modeling, Data Warehousing, Data Quality, Data Pipelines, ETL/ELT tools, Snowflake, Synapse, Redshift","scala, python, kafka, azure data factory, fivetranhvr, dbt, databricks, agile, sql, data engineering, cloud computing, aws, microsoft azure, google cloud, data modeling, data warehousing, data quality, data pipelines, etlelt tools, snowflake, synapse, redshift","agile, aws, azure data factory, cloud computing, data engineering, data quality, databricks, datamodeling, datapipeline, datawarehouse, dbt, etlelt tools, fivetranhvr, google cloud, kafka, microsoft azure, python, redshift, scala, snowflake, sql, synapse"
Senior Data Engineer,Hour Consulting,United States,https://www.linkedin.com/jobs/view/senior-data-engineer-at-hour-consulting-3726911630,2023-12-17,New Brunswick,United States,Mid senior,Remote,"Our client is in the Higher Education Technology space (Ed-Tech) and they are committed to supporting students succeed.
They are a SaaS company with supporting students actively using our platform globally.
Understanding students and institutions, as well as their behaviors through data, lies at the foundation of our work to improve student success. Every data point in the systems is important to helping us achieve that goal, so they are looking for people with a strong background in data engineering and analytics to help us design, build, scale, as well as maintain our data pipelines and models. As a Senior Data Science Engineer, you will be working with a variety of internal teams across engineering, product, and business to help solve their data needs. Your work will directly and tangibly impact the success of millions of students across the world.
In terms of the role and responsibilities, you will
Identify the data needs of our engineering, product, and business teams, understand their specific requirements for metrics and analysis, then build efficient, scalable, accurate, and complete data pipelines to enable data-informed decisions across the company
Architect data pipelines and models that power internal analytics for our teams, as well as customer-facing data visualization product features
Drive the collection of new data and the evolution of existing data sources, collaborate with the engineering teams to manage our product instrumentation strategies and data structures
Help the product and engineering teams understand and generalize statistical models from our research efforts, and help build data systems that would allow these models to be used directly in our product to drive student success
Work with Product Management to ensure productive, fast-moving sprints that deliver the maximum value to our customers
Help to continuously improve the team processes of our engineering team
You should
Have at least 5-7 years of experience in a Data Engineering or Data Science role, with a focus on instrumenting data collection, building data pipelines and conducting data-intensive analysis
Have a strong engineering background and are interested in data
Care deeply about the integrity of data, have a good nose for inconsistencies in data, and be able to pinpoint the issue to ensure that the team is not making decisions based on inaccurate or incomplete data
Have extensive experience of a scientific computing language (e.g. Python) and SQL
Have experience building an Amazon Web Services-based system that processes data across multiple data stores and technologies, including MySQL, Redis, Elasticsearch
Know the best practices of how different types of data should be visualized in different contexts
Be comfortable using multiple communication and collaboration tools to work effectively with colleagues across North America and Europe
What will make you stand out
Experience working with Python or Java web applications
Experience working in the higher-ed technology space, particularly in a Data Engineering related role
Experience leading a team
Experience working with a remote or distributed team
What you can expect
A chance to work towards an amazing mission of helping students succeed as a team member of a global tech startup
Remote-friendly work environment: Ability to work from anywhere in the EST in the US
Generous paid vacation time
Continuous learning and growth culture with many opportunities to develop professionally
Health Benefits including health, dental, life , disability insurance and travel coverage
Participation in matching 401K plan
Home Office Set up support with a company laptop, equipment and support to set up your home office
A chance to work with a global collaborative, friendly and diverse team
Show more
Show less","Python, SQL, Data pipelines, Data engineering, Data analysis, Data science, Machine learning, Statistical modeling, Data visualization, Data integrity, Data collection, Data structures, Amazon Web Services, MySQL, Redis, Elasticsearch, Product management, Communication, Collaboration, Remote work","python, sql, data pipelines, data engineering, data analysis, data science, machine learning, statistical modeling, data visualization, data integrity, data collection, data structures, amazon web services, mysql, redis, elasticsearch, product management, communication, collaboration, remote work","amazon web services, collaboration, communication, data collection, data engineering, data integrity, data science, data structures, dataanalytics, datapipeline, elasticsearch, machine learning, mysql, product management, python, redis, remote work, sql, statistical modeling, visualization"
Database Engineer,Fusion Technology LLC,"Clarksburg, WV",https://www.linkedin.com/jobs/view/database-engineer-at-fusion-technology-llc-3787905061,2023-12-17,New Brunswick,United States,Mid senior,Remote,"Database Engineer (Mainframe Z/OS Experience)
Employee dedication contributes to our success, customer satisfaction, and our nation's most outstanding achievements. At Fusion Technology, we recognize that our employees are our greatest attribute. That is why our top priority is to attract, recruit, retain, and continuously motivate quality employees who meet or exceed customer requirements. Our dedicated people make Fusion Technology one of the fastest-growing information technology (IT) companies in West Virginia. As a HUBZone Small Business, we know what it means to work as one team. As such, we encompass all definitions of the word ""team."" So, are you ready to join ours?
Who is Fusion Technology?
Fusion Technology is a performance-driven HUBZone Small Business concern residing in the heart of the beautiful mountainsides of West Virginia, steps away from the Federal Bureau of Investigation's Criminal Justice Information Services Division's Headquarters. Founded in 2007 by an Engineer-by-trade, Fusion Technology dedicates our valuable resources to providing comprehensive IT services and solutions to mission-critical US Government programs and the Intel Community.
Day to day tasks include:
Document all platform service processes and procedures and maintain the artifacts in a document repository
Perform system operation support including server system administration
General troubleshooting of complex engineering issues for hardware, software, and network components
Software installation, automations, and migrations
Supporting backup and recovery tasks
Evaluating new hardware and software, with the goal of improving and increasingly efficient operations
Required Qualifications:
5+ years of overall related experience with an IBM z/OS Mainframe environment
Knowledgeable in z/OS and it’s subsystems
Ability to work well with people from many different disciplines with varying degrees of technical experience
Ability to express complex technical concepts effectively, both verbally and in writing
Ability to obtain and maintain a Secret clearance.
Desired Qualifications:
z/OS Certifications and Training
Knowledge of the following tools: Java, COBOL, CICS TS, Unix, OAM, IBM Apache Server, FDR, DFSMS, ISPF, CICS, Elastic, Ansible
Ability to apply user requirements to the analysis, planning, and design of technology systems.
Ability to communicate problems and solutions both verbally and in writing to the Government Lead.
Ability to work independently and troubleshoot problems with speed and efficiency.
Ability to work closely with others in a team environment
Ability to interface with all customer levels.
Strong analytical and problem-solving skills
The Database Engineering is operating 100%
Remote.
(with occasional travel to main work site for special events)
What matters to you matters to us:
Fusion Technology values its employees and works hard to ensure proper care for them and their families. We desire to compensate employees in a competitive, motivational, fair, and equitable way with other employers in the marketplace. Salary is only one component of employee compensation but an integral part of recruiting and retaining qualified employees. However, at Fusion Technology, we take a comprehensive approach and consider each employee's needs to tailor a compensation plan that provides financial security and peace of mind. Our competitive package includes a best-in-class matching 401K program, comprehensive Cigna healthcare plan, a competitive employer contribution to a health savings account, vision and dental plans, life insurance, short- and long-term disability, and personal leave, in addition to paid certifications and training.
Fusion Technology LLC is an Equal Opportunity Employer. We respect and seek to empower each individual and support the diverse cultures, perspectives, skills, and experiences within our workforce. Qualified applicants will receive consideration for employment without regard to sex, race, ethnicity, age, national origin, citizenship, religion, physical or mental disability, medical condition, genetic information, pregnancy, family structure, marital status, ancestry, domestic partner status, sexual orientation, gender identity or expression, veteran or military status, or any other basis prohibited by law.
Powered by JazzHR
HwGlLMm0fm
Show more
Show less","z/OS, Java, COBOL, CICS TS, Unix, OAM, IBM Apache Server, FDR, DFSMS, ISPF, CICS, Elastic, Ansible, Apache Server","zos, java, cobol, cics ts, unix, oam, ibm apache server, fdr, dfsms, ispf, cics, elastic, ansible, apache server","ansible, apache server, cics, cics ts, cobol, dfsms, elastic, fdr, ibm apache server, ispf, java, oam, unix, zos"
Data Scientist with AL & ML,Saransh Inc,"Los Angeles, CA",https://www.linkedin.com/jobs/view/data-scientist-with-al-ml-at-saransh-inc-3735366115,2023-12-17,Santa Clarita,United States,Mid senior,Onsite,"Experience in LLM and Generative AI
Experience in Document extraction/chat
Strong experience in Python
Experience in building and maintaining open-domain or health care domain-specific ontologies
Understanding of knowledge graphs
Experience working in a cloud-native environment such as AWS
Should have hands on experience with AWS Neptune or Neo4J graph database
Have experience in building graph-based ontology from scratch and working with structured and unstructured data
Experience supporting ML models development on big data infrastructure (on knowledge graph would be a bonus)
Hands on python to build knowledge Graph/ontologies.
Experience with AWS Tex tract, Comprehend Medical (nice to have)
Need candidates who have strong code level experience in Python – Document Extraction
Note: Client would prefer candidates who are 9/10 on Python, LLM, Generative AI and Document Extraction
TENXE project - Knowledge Graph (AI-ML)
Show more
Show less","Natural Language Processing (NLP), Machine Learning (ML), Artificial Intelligence (AI), Python, AWS, Knowledge Graph, Ontologies, AWS Neptune, Neo4J, Graph Database, Document Extraction, AWS Tex tract, AWS Comprehend Medical, Generative AI","natural language processing nlp, machine learning ml, artificial intelligence ai, python, aws, knowledge graph, ontologies, aws neptune, neo4j, graph database, document extraction, aws tex tract, aws comprehend medical, generative ai","artificial intelligence ai, aws, aws comprehend medical, aws neptune, aws tex tract, document extraction, generative ai, graph database, knowledge graph, machine learning ml, natural language processing nlp, neo4j, ontologies, python"
Sr. Data Analyst,Florida Health Care Plans,"Daytona Beach, FL",https://www.linkedin.com/jobs/view/sr-data-analyst-at-florida-health-care-plans-3731006653,2023-12-17,Daytona Beach,United States,Mid senior,Onsite,"This position is Located In Daytona Beach FL
POSITION SUMMARY:
The goal of this position is to support FHCP’s mission by delivering timely data, reporting, and analysis. This position is all about gathering data, cleaning data, and analyzing data to help others make higher quality decisions.
This purpose is supported by the following activities:
Pulling data from databases and interpreting results
Use of analytical methods to answer business questions
Analysis of the structure and quality of data from source systems
Communication with business users to gather requirements
Resolving data quality problems
Sharing of data knowledge with others
Researching, validating, monitoring, and analyzing data
QUALIFICATIONS:
Education, skills and experience:
Bachelor’s Degree or equivalent experience is preferred
5 years of professional reporting or analysis experience preferred, including at least two years at one company
Demonstrated ability to answer key business questions using data analysis and visualization
Excellent oral and written communication, organization, problem solving and diagnostic skills
Proficient in Microsoft Excel
Proficient in SQL
Proficiency with Microsoft Power BI
Show more
Show less","Data Analysis, Data Visualization, Data Reporting, Data Cleansing, Data Quality Control, SQL, Microsoft Excel, Power BI, Business Intelligence, Problemsolving, Communication, Research, Monitoring, Data Validation, Analytical Methods","data analysis, data visualization, data reporting, data cleansing, data quality control, sql, microsoft excel, power bi, business intelligence, problemsolving, communication, research, monitoring, data validation, analytical methods","analytical methods, business intelligence, communication, data quality control, data reporting, data validation, dataanalytics, datacleaning, microsoft excel, monitoring, powerbi, problemsolving, research, sql, visualization"
Data Migration Analyst,Brown & Brown Insurance,"Daytona Beach, FL",https://www.linkedin.com/jobs/view/data-migration-analyst-at-brown-brown-insurance-3730893807,2023-12-17,Daytona Beach,United States,Mid senior,Onsite,"Built on meritocracy, our unique company culture rewards self-starters and those who are committed to doing what is best for our customers.
It's an exciting time to join Brown & Brown! Our business is growing both in North America and internationally which emphasizes the need to build an unparalleled team that promotes future growth. We're excited to continue solidifying that foundation as we are looking for a Data Migration Analyst to join our growing team in Daytona Beach, Florida or Dallas Texas.
As part of Bridge Specialty Group, the Wholesale division of Brown & Brown, the Data Migration Analyst, under the supervision of the Team Leader, is responsible for planning, implementing, and managing the transfer of data between IT systems, software, or applications. This role plays a crucial part in ensuring a seamless and error-free data migration process, meeting stakeholders' requirements while minimizing potential risks and impacts.
What You'll Do
Meet with clients or users to find out their data migration needs.
Create a project plan for the migration process, including mapping risks and potential impacts.
Undertake an audit of existing data.
Cleanse data as necessary to remove the potential for issues with data migration.
Test the system after the migration process.
Find and fix any outstanding faults or issues with migrated data.
Document the processes undertaken, any issues, and fixes that have been deployed.
Develop best practice standards and protocols for data migration.
Able to follow and execute upon standard operating practices and procedures.
Leverages application expertise to troubleshoot, problem solve and offer solutions.
Supports application conversion and implementation projects.
Other duties may be assigned.
Full-time position that will require periodic early morning, evening, and weekend duties.
Up to 20% travel.
What You'll Need
Experience in DevOps, Jira or equivalent development tools.
Foundational knowledge of the insurance industry.
Experience providing support for insurance-related Agency Management Systems and related applications.
Bachelor’s degree or equivalent experience
Proficient with MS Office Suite, particularly Excel.
Strong problem-solving skills for technology-related issues and complex business scenarios.
2-5 years of experience in a similarly related role.
What We Offer
Excellent growth and advancement opportunities
Competitive pay based on experience
Discretionary Time Off (DTO)
Generous benefits package: health, dental, vision, 401(k), etc.
Employee Stock Purchase Plan
Tuition Reimbursement and Student Loan Repayment Assistance
Mental Health Resources
We are an Equal Opportunity Employer. We take pride in the diversity of our team and seek diversity in our applicants.
Show more
Show less","Data Migration, Project Planning, Data Mapping, Data Audit, Data Cleansing, Data Testing, Debugging, Documentation, Standard Operating Procedures, Troubleshooting, Problem Solving, DevOps, Jira, Insurance Industry, Agency Management Systems, Microsoft Office Suite, Excel, Strong ProblemSolving Skills","data migration, project planning, data mapping, data audit, data cleansing, data testing, debugging, documentation, standard operating procedures, troubleshooting, problem solving, devops, jira, insurance industry, agency management systems, microsoft office suite, excel, strong problemsolving skills","agency management systems, data audit, data mapping, data migration, data testing, datacleaning, debugging, devops, documentation, excel, insurance industry, jira, microsoft office suite, problem solving, project planning, standard operating procedures, strong problemsolving skills, troubleshooting"
"Senior Data Scientist, Business Assurance",T-Mobile,"Overland Park, KS",https://www.linkedin.com/jobs/view/senior-data-scientist-business-assurance-at-t-mobile-3768667620,2023-12-17,Kansas City,United States,Mid senior,Hybrid,"Be unstoppable with us!
T-Mobile is synonymous with innovation–and you could be part of the team that disrupted an entire industry! We reinvented customer service, brought real 5G to the nation, and now we’re shaping the future of technology in wireless and beyond. Our work is as exciting as it is rewarding, so consider the career opportunity below as your invitation to grow with us, make big things happen with us, above all, #BEYOU with us. Together, we won’t stop!
Job Overview
As a Senior Data Scientist, you are responsible for owning the application of machine learning techniques and statistical methods to tackle business problems. You will collaborate with a multi-disciplinary team of technical and non-technical business stakeholders on a wide range of challenges. You must demonstrate expertise across the entire machine learning (ML) lifecycle, including problem framing, data collection, exploratory data analysis, model development, deployment, and performance measurement. Paramount to this job is the understanding that a Senior Data Scientist represents technical expertise and leadership within the Data Science team in creating real business value from data.
Job Responsibilities
:
Extract, prepare and model large, complex data sets using a combination of skills, including machine learning theory, mathematics, statistics, and programming.
Deliver timely quality analysis, interpretation, and synthesis of data into effective, concise, and actionable recommendations that enable intelligent decisioning for the company.
Provide senior-level guidance and mentorship to the data science team, including reviewing projects, models, and code for peers and junior team members.
Work with engineering teams to implement and improve machine learning pipelines and production-ready models.
Effectively communicate meaningful information and insights to business leaders using verbal, written, and data visualization skills.
Education:
Bachelor's Degree Quantitative Field (math, statistics, economics, computer science, physics, engineering, etc.) (Required)
Master's/Advanced Degree Quantitative Field (math, statistics, economics, computer science, physics, engineering) (Preferred)
Work Experience
:
4-7 years experience in predictive modeling, data science, and analysis in an ML engineer or data scientist role building and deploying ML models or hands on experience developing deep learning models
4-7 years experience with data scripting languages (e.g., SQL, Python, R)
2-4 years Experience with big data architecture and pipeline, Hadoop, Hive, Spark, Kafka, etc.
4-7 years Experience articulating and translating business questions and using statistical techniques to arrive at an answer using available data
4-7 years Experience in data visualization
4-7 years Experience working with relational database using SQL
2-4 years Experience in the telecom industry
Knowledge, Skills and Abilities
:
Mathematics Calculus, linear algebra, statistics, and probability (Required)
Programming Expertise in Python and SQL (Required)
Machine Learning: Expertise applying machine learning concepts and techniques related to supervised and unsupervised learning (Required)
Communication Strong communication skills, ability to work with multi-functional teams
At least 18 years of age
Legally authorized to work in the United States
Never stop growing!
T-Mobile doesn’t have a corporate ladder–it’s more like a jungle gym of possibilities! We love helping our employees grow in their careers, because it’s that shared drive to aim high that drives our business and our culture forward.
If you’d like to receive more information about careers at T-Mobile, sign up for the T-Mobile Talent Community today! https://www.tmobile.careers/profile/join/
T-Mobile USA, Inc. is an Equal Opportunity Employer. All decisions concerning the employment relationship will be made without regard to age, race, ethnicity, color, religion, creed, sex, sexual orientation, gender identity or expression, national origin, religious affiliation, marital status, citizenship status, veteran status, the presence of any physical or mental disability, or any other status or characteristic protected by federal, state, or local law. Discrimination, retaliation or harassment based upon any of these factors is wholly inconsistent with how we do business and will not be tolerated.
Talent comes in all forms at the Un-carrier. If you are an individual with a disability and need reasonable accommodation at any point in the application or interview process, please let us know by emailing ApplicantAccommodation@t-mobile.com or calling 1-844-873-9500. Please note, this contact channel is not a means to apply for or inquire about a position and we are unable to respond to non-accommodation related requests.
Show more
Show less","Data Science, Machine Learning, Statistics, Programming, Python, SQL, Apache Hadoop, Apache Hive, Apache Spark, Kafka, Relational Databases, Calculus, Linear Algebra, Probability, Communication","data science, machine learning, statistics, programming, python, sql, apache hadoop, apache hive, apache spark, kafka, relational databases, calculus, linear algebra, probability, communication","apache hadoop, apache hive, apache spark, calculus, communication, data science, kafka, linear algebra, machine learning, probability, programming, python, relational databases, sql, statistics"
Senior Data Engineer,NuStaff Recruitment,"North Lincolnshire, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-at-nustaff-recruitment-3770639480,2023-12-17,Kingston upon Hull, United Kingdom,Mid senior,Onsite,"JOB PURPOSE AND SCOPE
The Senior Data Engineer will be accountable for and have lead responsibility for designing, building, and maintaining our data infrastructure, ensuring the efficient and reliable definition, processing, storage, and analysis of large datasets.
The role is integral to the Force being able to create value from all the data it collects by leading the data engineering activities required to improve and combine data from first contact with the public through to crime intelligence and the back-office
They will make a significant contribution to making sure our data, and data shared by others is of the highest quality, available, usable and shareable by rigorous standard setting and quality control
The role requires a highly comprehensive level of knowledge, functional understanding of modern data technologies and platforms and as sole accountable data engineer for the Force will need to excel in the application data engineering tools and methodologies.
They will cultivate strong and effective working relationships with individuals, teams and colleagues, both internal and external to the Force. In particular they will work performance, information and data analysts specialising in crime, justice, vulnerable populations, public safety across the Force, from other Forces and public sector partners to build products and services which deliver insight and value to the Force and its ability to make Lincolnshire a safe place to live, work and visit
Demonstrate the Force values at all times. All staff involved in carrying out functions in this role will do so in accordance with the principles of the Code of Ethics. The aim of the Code of Ethics is to support each member of the policing profession to deliver the highest professional standards in their service to the public.
CORE WORK AREAS
Lead the design and implement of scalable, efficient, and robust data pipelines to extract, transform, and load (ETL) data from various sources into data storage and platform solutions.
Engage and collaborate with cross-functional teams, to understand their data requirements and translate them into technical solutions.
Build and link data sets so that data scientists and analysts can easily derive insights, including accessing sensitive data held in back-end systems, extracting and loading into the data storage environment (warehouse or lake) or platform (fabric or mesh, whichever strategically appropriate) from which they be provisioned.
Write ETL scripts and code to make sure the ETL process performs optimally, leading on extracting data, joining and helping analysts to derive insights
Lead the build and be accountable for the optimising the performance of pipelines and APIs that connect operational systems, data for analytics and business intelligence (BI) systems, troubleshooting and resolving issues as they arise, to ensure the timely delivery of high-quality data.
Provide leadership to the business in automating its manual data flows and reporting processes, enabling their scaling up and repeatable use, working with leaders across the organisation to identify and elicit the benefits of speeding up and easing access to data.
Lead the development and maintain data models, schemas, and structures that optimize data storage and retrieval for analytics and reporting purposes.
Work with the Principal DBA and Principal Solutions Developer to actively optimise code within the Force's systems to ensure processes perform optimally
Identify ways to improve data reliability, efficiency and quality
Engage and work closely with data stakeholders to understand their data needs and provide support and guidance on how to use the data engineering infrastructure effectively.
Ensure data engineers, information analysts and BI developers have integral working practises that product optimal and reusable visualisations and reports
Work across the Force to extract, clean, combine and store data in a way that allows it to be exploited and as an asset to help service delivery and decision making.
Lead major data engineering activities within business-critical live systems, working with services to understand and improve data quality. This may include purging data in accordance with regulation and internal policy.
Work with the Information Management Unit (IMU) colleagues implementing data quality checks, data validation, and data cleansing processes to help ensure the accuracy, consistency, and integrity of the data.
Uphold data governance policies and standards, ensuring compliance with data privacy and security regulations.
Ensure the data is anonymise wherever necessary, working within the principles of GDPR and with suitable data protection and sharing agreements in place.
Lead on documenting data engineering processes, workflows, and technical specifications to facilitate knowledge sharing and ensure reproducibility of analyses.
Engage with professional networks, actively researching trends and advancements in data engineering, evaluating and adopting new tools, techniques, and best practices where these enhance the data engineering infrastructure, its reliability, security, and scalability.
Make significant contributions to data strategy, setting standards and leading the professional practice for data engineering.
Engage with and manage commercial relationships with data engineering partners; overseeing any project work undertaken by partners and suppliers, including virtual and remote management; set direction for suppliers, manage supplier risk and communicate our needs clearly.
Show more
Show less","Data engineering, Data infrastructure, Data storage, Data analysis, ETL, Data modeling, Data visualization, Data reporting, Data quality, Data governance, Data security, GDPR, Cloud computing, Data pipelines, Data lakes, Data warehouses, Big data, Machine learning, Artificial intelligence, Data science, Data mining, Data integration, Data transformation, Data visualization, Python, SQL, Java, C++, NoSQL, Hadoop, Spark, Hive, Pig, Oozie, Airflow, Tableau, Power BI, QlikView","data engineering, data infrastructure, data storage, data analysis, etl, data modeling, data visualization, data reporting, data quality, data governance, data security, gdpr, cloud computing, data pipelines, data lakes, data warehouses, big data, machine learning, artificial intelligence, data science, data mining, data integration, data transformation, data visualization, python, sql, java, c, nosql, hadoop, spark, hive, pig, oozie, airflow, tableau, power bi, qlikview","airflow, artificial intelligence, big data, c, cloud computing, data engineering, data governance, data infrastructure, data integration, data lakes, data mining, data quality, data reporting, data science, data security, data storage, data transformation, data warehouses, dataanalytics, datamodeling, datapipeline, etl, gdpr, hadoop, hive, java, machine learning, nosql, oozie, pig, powerbi, python, qlikview, spark, sql, tableau, visualization"
Data Analyst - Supply Chain Forecasting,Big Red Recruitment,"Warwickshire, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-analyst-supply-chain-forecasting-at-big-red-recruitment-3779811102,2023-12-17,Coventry, United Kingdom,Associate,Hybrid,"Scanning the market for a new Data Analyst position?
This Global Retailer could offer you bags of opportunity to analyse and forecast sales, product and promotional data.
We’re looking for a Data Analyst to join our team at a leading, award-winning,
global retail company
and help skyrocket the business to new heights! We need someone who's not afraid to dive into the details and is eager to contribute as they navigate the exciting world of stock replenishment.
The team are finalising their transformation programme and will launch this in coming months, you will be a part of this pivotal journey to help them shape the future and understand their data better than ever.
You will sit within the Supply Chain Forecasting team, supporting global teams by extracting, analysing and reporting on stock and sales data. You will be analysing consumer trends and buying habits, storing this data on databases, working with wider teams to establish best selling products.
Previous exposure to a data migration project would be beneficial, as the function are transforming the way they work and are currently in the process of rolling out new systems.
We’re looking for -
Strong SQL database experience, both data extraction and coding
Working in Agile methodology (desirable)
Strong Excel knowledge essential
SAP, R Studio and Python experience (desirable)
Data visualisation tools to create dashboards, Tableau, Power BI or alternative
Exposure to a Forecasting/Supply Chain environment, working with Supply Chain tools (desirable)
Retail or FMCG experience (desirable)
*Unfortunately for this role, we are unable to offer Visa Sponsorship at this time*
Location - Warwickshire
Flexible and hybrid working policy, 3 days per week office-based, 2 days working from home.
Monday - Friday working hours
Starting salary £50,000 - £54,000, rising to £58,000
Please submit your CV to be considered and find out more information.
We are an equal opportunity recruitment company. This means we welcome applications from all suitably qualified people regardless of race, sex, disability, religion, sexual orientation or age.
We are particularly invested in Neurodiversity inclusion and offer reasonable adjustments in the interview process. Reasonable adjustments are changes that we can make in the interview process if your disability puts you at a disadvantage compared with others who are not disabled. If you would benefit from a reasonable adjustment in your interview process, please call or email one of our recruiters.
Show more
Show less","Data Analysis, Data Mining, Data Visualization, SQL, Excel, Agile, SAP, R Studio, Python, Tableau, Power BI, Forecasting, Supply Chain Management, Retail, FMCG","data analysis, data mining, data visualization, sql, excel, agile, sap, r studio, python, tableau, power bi, forecasting, supply chain management, retail, fmcg","agile, data mining, dataanalytics, excel, fmcg, forecasting, powerbi, python, r studio, retail, sap, sql, supply chain management, tableau, visualization"
Data Engineer,Searchability,"Birmingham, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-searchability-3785574517,2023-12-17,Coventry, United Kingdom,Mid senior,Onsite,"Data Engineer
Location:
Birmingham
Salary:
£40,000-£50,000
On-Site
About The Company:
We are a leading innovator in the smart home and Internet of Things (IoT) industry. Our mission is to transform traditional homes into intelligent living spaces, enhancing convenience, security, and energy efficiency for homeowners. Established with a vision to make cutting-edge technology accessible to everyone. We have become a key player in the rapidly evolving landscape of connected homes.
Role overview:
Develop, maintain, and optimize data pipelines and ETL processes.
Collaborate with cross-functional teams to understand data requirements and design solutions that meet business needs.
Work with Python, SQL, NoSQL, TypeScript/JavaScript, and Hadoop technologies to build scalable and efficient data solutions.
Ensure data quality and integrity through effective testing and validation processes.
Continuously evaluate and implement new technologies to enhance the efficiency and performance of our data architecture.
Role requirements:
Bachelor's degree in Computer Science, Information Technology, or a related field.
Proven experience as a Data Engineer with expertise in Python, SQL, NoSQL databases, TypeScript/JavaScript, and Hadoop.
Strong understanding of data modeling, database design, and data warehousing concepts.
Experience with big data technologies and distributed computing.
Excellent problem-solving skills and the ability to work independently or collaboratively in a team environment.
Effective communication skills to convey complex technical concepts to non-technical stakeholders.
Show more
Show less","Data engineering, Data pipelines, ETL processes, Python, SQL, NoSQL, TypeScript, JavaScript, Hadoop, Data quality, Data integrity, Testing, Validation, Data architecture, Computer science, Information technology, Data modeling, Database design, Data warehousing, Big data, Distributed computing, Problemsolving, Team work, Communication","data engineering, data pipelines, etl processes, python, sql, nosql, typescript, javascript, hadoop, data quality, data integrity, testing, validation, data architecture, computer science, information technology, data modeling, database design, data warehousing, big data, distributed computing, problemsolving, team work, communication","big data, communication, computer science, data architecture, data engineering, data integrity, data quality, database design, datamodeling, datapipeline, datawarehouse, distributed computing, etl, hadoop, information technology, javascript, nosql, problemsolving, python, sql, team work, testing, typescript, validation"
Senior Data Engineer,Apexon,"Birmingham, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-at-apexon-3781012675,2023-12-17,Coventry, United Kingdom,Mid senior,Onsite,"Description
Position at Saggezza UK
Company Description: Apexon is a digital-first technology services firm backed by Goldman Sachs Asset Management and Everstone Capital. We specialize in accelerating business transformation and delivering human-centric digital experiences. For over 17 years, Apexon has been meeting customers wherever they are in the digital lifecycle and helping them outperform their competition through speed and innovation.
We are currently looking to hire Senior Data Engineers for our growing team.
Responsibilities
In an Agile environment manage end-to-end systems development cycle from requirements analysis to coding, testing, UAT, implementation and maintenance
Develop high level and detailed technical designs, testing strategies, and implementation plans
Work in a dynamic, fast-paced environment that provides exposure to all areas of Finance and Risk
Understand and respond to business needs, facilitating and developing process workflow, data requirements, and specifications required to support implementation
Identify opportunities for cross-divisional collaboration and reuse of common solutions
Provide technical and functional guidance and leadership to junior members on a need basis
Build and maintain key financial metric calculation models and associated infrastructure to support forward-looking business strategies and decisions in a evolving regulatory landscape
Work closely with a wide range of stakeholders globally, including Controllers, Risk, Operations, and various business units
Build strong relationships with business partners and front-line managers
Lead talented software engineers who are passionate about identifying patterns and building common solutions
Sklls And Experience Needed
Bachelor’s or master's degree in computer science, Mathematics, Electrical Engineering or related technical discipline
Experience in software development, including a clear understanding of data structures, algorithms, software design and core programming concepts
Experience preferably in Java, C/C++, Python, competent in traditional (RDBMS) and modern datastores (NoSQL)
Strong programming and problem-solving skills and analytical skills
A clear understanding of data structures, algorithms, software design and core programming concepts
Strong full-stack technical design and development skills and experience
Comfortable with multi-tasking, managing multiple stakeholders and working as part of a team
Excellent communication skills including experience speaking to technical and business audiences and working globally
Interest in finance
Can apply an entrepreneurial approach and passion to problem solving and product development
Preferred Qualifications
Strong programming experience in at least one language
Comfortable with working with multiple languages
Technologies: Scala, Java, Python, Spark, Linux and shell scripting, TDD (JUnit), build tools (Maven/Gradle/Ant)
Experience with continuous delivery and deployment
Proficient at working with large and complex code bases
Alloy, Data Lake
PURE
Being able to describe, connect and visualize data
Model to model mapping
Creating data models
Office / hub location:
Birmingham
We Also Offer
Up to 10% bonus (based on company and personal performance).
An employer pension scheme
25 days holiday + 8 bank holidays, with the option to carry forward or 'cash-in' 5 days each year
Access to YuLife wellness platform, subscription to Meditopia App, premium subscription to Fiit, life coaching & emotional wellbeing sessions, 24 / 7 virtual GP Access, Employee Assistance Programme
Life Insurance & Income protection
Enhanced Maternity Pay & Paternity Pay
Cycle to work scheme
Travel loan scheme
A Tech Scheme which lets you choose from over 5000 tech products at up to a 12% discount
Free unlimited Udemy account for every employee to support their continuous learning and improvement
Support in obtaining relevant certifications
Disclaimer: If you feel that this is a good match for your skillsets, please submit a current word version of your resume along with a cover letter describing your skills, experience and salary expectations. We are an Equal Opportunity Employer (EOE). You can read our job applicant privacy policy here .
Show more
Show less","Java, C/C++, Python, Scala, Spark, Linux, Shell scripting, TDD, JUnit, Maven, Gradle, Ant, Continuous delivery, Continuous deployment, Data structures, Algorithms, Software design, Core programming concepts, Data modeling, Model to model mapping, Data visualization, Data lakes, Alloy, PURE","java, cc, python, scala, spark, linux, shell scripting, tdd, junit, maven, gradle, ant, continuous delivery, continuous deployment, data structures, algorithms, software design, core programming concepts, data modeling, model to model mapping, data visualization, data lakes, alloy, pure","algorithms, alloy, ant, cc, continuous delivery, continuous deployment, core programming concepts, data lakes, data structures, datamodeling, gradle, java, junit, linux, maven, model to model mapping, pure, python, scala, shell scripting, software design, spark, tdd, visualization"
Data Cabling Engineer,Digital Waffle,"Birmingham, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-cabling-engineer-at-digital-waffle-3739542805,2023-12-17,Coventry, United Kingdom,Mid senior,Onsite,"Job Description
Position: Data Cabling Engineer (Temp/Contract/Freelance)
Department: Information Technology / Network Infrastructure
Location: Birmingham
Job Summary: We are seeking a skilled and dedicated Data Cabling Engineer. The successful candidate will be responsible for the installation, testing, and termination of Cat6 and Cat6a branded copper cables within metal trunking and containment to ensure a reliable and efficient network infrastructure. The role involves working closely with the IT team to maintain high-quality data connectivity, support network expansion, and ensure optimal performance.
Responsibilities:
Cabling Installation:
Plan, install, and route Cat6 and Cat6a copper cables according to industry standards and best practices
Mount, secure, and organize cable pathways, including cable trays, conduits, and wall penetrations
Install cable connectors, outlets, and termination panels in compliance with design specifications
Termination and Labeling:
Terminate copper cables onto jacks, connectors, and patch panels with precise attention to detail and adherence to industry standards
Ensure proper labeling of cables, connectors, and outlets for easy identification and maintenance
Maintain accurate documentation of cable layouts, terminations, and labeling
Testing and Troubleshooting:
Conduct comprehensive cable testing using appropriate tools to validate signal quality, continuity, and performance
Troubleshoot and resolve connectivity issues, signal degradation, and other cable-related problems
Utilize testing equipment such as cable testers and certification tools to ensure compliance with required specifications
Quality Assurance:
Ensure all cabling work is performed to high quality standards and follows company policies and guidelines
Perform quality checks on cable installations to confirm compliance with industry standards and design specifications
Network Expansion:
Collaborate with IT team members to support network expansion projects and ensure seamless integration of new cabling
Participate in designing and implementing cable layouts for new network infrastructure components
Health and Safety:
Adhere to safety protocols and guidelines to prevent accidents and maintain a safe working environment
Operate tools and equipment safely and responsibly, following industry best practices
Requirements:
High school diploma or equivalent; additional technical certifications or coursework in cabling and network infrastructure is a plus
Proven experience as a Data Cabling Engineer or similar role, with expertise in Cat6 and Cat6a copper cable installation, termination, and testing
Familiarity with relevant cabling standards, such as TIA/EIA and ISO/IEC
Proficiency in using cable testing and certification tools
Strong understanding of network topologies, protocols, and network equipment
Ability to interpret technical drawings, schematics, and cabling diagrams
Excellent problem-solving skills with a keen attention to detail
Effective communication skills to collaborate with IT team members and project stakeholders
Ability to work independently or as part of a team in various environments, including office spaces, data centers, and remote locations
Strong commitment to safety and adherence to industry regulations
General tools required for this job:-
Cable Stripping and Cutting Tools:
Cable stripper: Used to remove the outer insulation from cables
Cable cutter: Used to cut cables to appropriate lengths
Termination Tools:
RJ45 crimping tool: Used to attach RJ45 connectors to the ends of Ethernet cables
Punch-down tool: Used for terminating cables on patch panels, keystone jacks, and outlets
Testing and Certification Tools:
Cable tester: Used to test cable continuity, signal quality, and detect faults
Cable certifier: Provides detailed analysis and certification of cable performance against industry standards
Tone generator and probe: Used to trace and identify cables within a bundle or behind walls
Labeling and Identification Tools:
Cable labels and markers: Used to label and identify cables, connectors, and outlets
Label printer: Used to create professional labels for cables and equipment
Measuring and Alignment Tools:
Tape measure or ruler: Used to measure cable lengths accurately
Level: Ensures proper alignment of cable pathways and connectors
Cable Management Tools:
Cable ties and Velcro straps: Used for bundling and organizing cables
Cable clips and mounts: Secure cables along walls, ceilings, or other surfaces
Drilling and Mounting Tools:
Power drill and bits: Used to create holes for cable pass-throughs and mounting hardware
Screwdrivers and wall anchors: Required for mounting cable trays, conduits, and outlets
Safety Gear:
Safety glasses: Protects eyes from debris and potential hazards
Work gloves: Provides hand protection during installation and cable handling
Tool Bag or Pouch:
Keeps tools organized and easily accessible during installation tasks
Documentation Tools:
Pen and notepad or mobile device: Used to document cable layouts, terminations, and labeling
Personal Protective Equipment (PPE):
Steel-toed boots: Provides foot protection in potentially hazardous environments
Hard hat: Required in construction or industrial settings
Optional Tools:
Cable fish tape or rods: Used to guide cables through walls, ceilings, or conduits
Cable lubricant: Assists in pulling cables through tight spaces
Cable toner and probe: Helps identify specific cables in a bundle
If you are a motivated Data Cabling Engineer with a passion for maintaining robust network infrastructure and ensuring seamless connectivity, we encourage you to apply. Contact Joe on joe@digitalwaffle.co.uk
Show more
Show less","Data Cabling, Copper Cables, Metal Trunking, Cat6, Cat6a, Network Infrastructure, IT Infrastructure, Cable Installation, Cable Testing, Troubleshooting, Cable Termination, Cable Labeling, Cable Management, Network Expansion, Health and Safety, Data Centers, TIA/EIA, ISO/IEC, Network Topologies, Network Protocols, Network Equipment, Technical Drawings, Schematics, Cabling Diagrams, RJ45 Crimping Tool, PunchDown Tool, Cable Stripper, Cable Cutter, Cable Tester, Cable Certifier, Tone Generator, Probe, Cable Labels, Markers, Label Printer, Tape Measure, Level, Cable Ties, Velcro Straps, Cable Clips, Cable Mounts, Power Drill, Screwdrivers, Wall Anchors, Safety Glasses, Work Gloves, Tool Bag, Pen, Notepad, Mobile Device, SteelToe Boots, Hard Hat, Cable Fish Tape, Cable Rods, Cable Lubricant, Cable Toner","data cabling, copper cables, metal trunking, cat6, cat6a, network infrastructure, it infrastructure, cable installation, cable testing, troubleshooting, cable termination, cable labeling, cable management, network expansion, health and safety, data centers, tiaeia, isoiec, network topologies, network protocols, network equipment, technical drawings, schematics, cabling diagrams, rj45 crimping tool, punchdown tool, cable stripper, cable cutter, cable tester, cable certifier, tone generator, probe, cable labels, markers, label printer, tape measure, level, cable ties, velcro straps, cable clips, cable mounts, power drill, screwdrivers, wall anchors, safety glasses, work gloves, tool bag, pen, notepad, mobile device, steeltoe boots, hard hat, cable fish tape, cable rods, cable lubricant, cable toner","cable certifier, cable clips, cable cutter, cable fish tape, cable installation, cable labeling, cable labels, cable lubricant, cable management, cable mounts, cable rods, cable stripper, cable termination, cable tester, cable testing, cable ties, cable toner, cabling diagrams, cat6, cat6a, copper cables, data cabling, data centers, hard hat, health and safety, isoiec, it infrastructure, label printer, level, markers, metal trunking, mobile device, network equipment, network expansion, network infrastructure, network protocols, network topologies, notepad, pen, power drill, probe, punchdown tool, rj45 crimping tool, safety glasses, schematics, screwdrivers, steeltoe boots, tape measure, technical drawings, tiaeia, tone generator, tool bag, troubleshooting, velcro straps, wall anchors, work gloves"
Data Cabling Engineer,Digital Waffle,"Birmingham, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-cabling-engineer-at-digital-waffle-3748511982,2023-12-17,Coventry, United Kingdom,Mid senior,Onsite,"Job Description
Position: Data Cabling Engineer (Temp/Contract/Freelance)
Department: Information Technology / Network Infrastructure
Location: Birmingham
Job Summary: We are seeking a skilled and dedicated Data Cabling Engineer. The successful candidate will be responsible for the installation, testing, and termination of Cat6 and Cat6a branded copper cables within metal trunking and containment to ensure a reliable and efficient network infrastructure. The role involves working closely with the IT team to maintain high-quality data connectivity, support network expansion, and ensure optimal performance.
Responsibilities:
Cabling Installation:
Plan, install, and route Cat6 and Cat6a copper cables according to industry standards and best practices
Mount, secure, and organize cable pathways, including cable trays, conduits, and wall penetrations
Install cable connectors, outlets, and termination panels in compliance with design specifications
Termination and Labeling:
Terminate copper cables onto jacks, connectors, and patch panels with precise attention to detail and adherence to industry standards
Ensure proper labeling of cables, connectors, and outlets for easy identification and maintenance
Maintain accurate documentation of cable layouts, terminations, and labeling
Testing and Troubleshooting:
Conduct comprehensive cable testing using appropriate tools to validate signal quality, continuity, and performance
Troubleshoot and resolve connectivity issues, signal degradation, and other cable-related problems
Utilize testing equipment such as cable testers and certification tools to ensure compliance with required specifications
Quality Assurance:
Ensure all cabling work is performed to high quality standards and follows company policies and guidelines
Perform quality checks on cable installations to confirm compliance with industry standards and design specifications
Network Expansion:
Collaborate with IT team members to support network expansion projects and ensure seamless integration of new cabling
Participate in designing and implementing cable layouts for new network infrastructure components
Health and Safety:
Adhere to safety protocols and guidelines to prevent accidents and maintain a safe working environment
Operate tools and equipment safely and responsibly, following industry best practices
Requirements:
High school diploma or equivalent; additional technical certifications or coursework in cabling and network infrastructure is a plus
Proven experience as a Data Cabling Engineer or similar role, with expertise in Cat6 and Cat6a copper cable installation, termination, and testing
Familiarity with relevant cabling standards, such as TIA/EIA and ISO/IEC
Proficiency in using cable testing and certification tools
Strong understanding of network topologies, protocols, and network equipment
Ability to interpret technical drawings, schematics, and cabling diagrams
Excellent problem-solving skills with a keen attention to detail
Effective communication skills to collaborate with IT team members and project stakeholders
Ability to work independently or as part of a team in various environments, including office spaces, data centers, and remote locations
Strong commitment to safety and adherence to industry regulations
General tools required for this job:-
Cable Stripping and Cutting Tools:
Cable stripper: Used to remove the outer insulation from cables
Cable cutter: Used to cut cables to appropriate lengths
Termination Tools:
RJ45 crimping tool: Used to attach RJ45 connectors to the ends of Ethernet cables
Punch-down tool: Used for terminating cables on patch panels, keystone jacks, and outlets
Testing and Certification Tools:
Cable tester: Used to test cable continuity, signal quality, and detect faults
Cable certifier: Provides detailed analysis and certification of cable performance against industry standards
Tone generator and probe: Used to trace and identify cables within a bundle or behind walls
Labeling and Identification Tools:
Cable labels and markers: Used to label and identify cables, connectors, and outlets
Label printer: Used to create professional labels for cables and equipment
Measuring and Alignment Tools:
Tape measure or ruler: Used to measure cable lengths accurately
Level: Ensures proper alignment of cable pathways and connectors
Cable Management Tools:
Cable ties and Velcro straps: Used for bundling and organizing cables
Cable clips and mounts: Secure cables along walls, ceilings, or other surfaces
Drilling and Mounting Tools:
Power drill and bits: Used to create holes for cable pass-throughs and mounting hardware
Screwdrivers and wall anchors: Required for mounting cable trays, conduits, and outlets
Safety Gear:
Safety glasses: Protects eyes from debris and potential hazards
Work gloves: Provides hand protection during installation and cable handling
Tool Bag or Pouch:
Keeps tools organized and easily accessible during installation tasks
Documentation Tools:
Pen and notepad or mobile device: Used to document cable layouts, terminations, and labeling
Personal Protective Equipment (PPE):
Steel-toed boots: Provides foot protection in potentially hazardous environments
Hard hat: Required in construction or industrial settings
Optional Tools:
Cable fish tape or rods: Used to guide cables through walls, ceilings, or conduits
Cable lubricant: Assists in pulling cables through tight spaces
Cable toner and probe: Helps identify specific cables in a bundle
If you are a motivated Data Cabling Engineer with a passion for maintaining robust network infrastructure and ensuring seamless connectivity, we encourage you to apply. Contact Joe on joe@digitalwaffle.co.uk
Show more
Show less","Cat6, Cat6a, Copper Cabling, Network Infrastructure, Cable Testing, Cable Installation, Cable Termination, Cable Labeling, Troubleshooting, Quality Assurance, Network Expansion, TIA/EIA, ISO/IEC, RJ45, PunchDown Tool, Cable Stripper, Cable Cutter, Cable Tester, Cable Certifier, Tone Generator, Probe, Cable Labels, Cable Markers, Tape Measure, Level, Cable Ties, Velcro Straps, Cable Clips, Cable Mounts, Power Drill, Drill Bits, Screw Drivers, Wall Anchors, Safety Glasses, Work Gloves, Tool Bag, Pen, Notepad, Mobile Device, Personal Protective Equipment, Steeltoed Boots, Hard Hat, Cable Fish Tape, Cable Rods, Cable Lubricant, Cable Toner","cat6, cat6a, copper cabling, network infrastructure, cable testing, cable installation, cable termination, cable labeling, troubleshooting, quality assurance, network expansion, tiaeia, isoiec, rj45, punchdown tool, cable stripper, cable cutter, cable tester, cable certifier, tone generator, probe, cable labels, cable markers, tape measure, level, cable ties, velcro straps, cable clips, cable mounts, power drill, drill bits, screw drivers, wall anchors, safety glasses, work gloves, tool bag, pen, notepad, mobile device, personal protective equipment, steeltoed boots, hard hat, cable fish tape, cable rods, cable lubricant, cable toner","cable certifier, cable clips, cable cutter, cable fish tape, cable installation, cable labeling, cable labels, cable lubricant, cable markers, cable mounts, cable rods, cable stripper, cable termination, cable tester, cable testing, cable ties, cable toner, cat6, cat6a, copper cabling, drill bits, hard hat, isoiec, level, mobile device, network expansion, network infrastructure, notepad, pen, personal protective equipment, power drill, probe, punchdown tool, quality assurance, rj45, safety glasses, screw drivers, steeltoed boots, tape measure, tiaeia, tone generator, tool bag, troubleshooting, velcro straps, wall anchors, work gloves"
Data Cabling Engineer,Digital Waffle,"Birmingham, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-cabling-engineer-at-digital-waffle-3732196340,2023-12-17,Coventry, United Kingdom,Mid senior,Onsite,"Position:
Data Cabling Engineer (Temp/Contract/Freelance)
Department:
Information Technology / Network Infrastructure
Location:
Birmingham, West Midlands, UK
Job Summary:
We are seeking a skilled and dedicated Data Cabling Engineer to join our Information Technology team. The successful candidate will be responsible for the installation, testing, and termination of Cat6 and Cat6a branded copper cables to ensure a reliable and efficient network infrastructure. The role involves working closely with the IT team to maintain high-quality data connectivity, support network expansion, and ensure optimal performance.
Responsibilities:
Cabling Installation:
Plan, install, and route Cat6 and Cat6a copper cables according to industry standards and best practices
Mount, secure, and organize cable pathways, including cable trays, conduits, and wall penetrations
Install cable connectors, outlets, and termination panels in compliance with design specifications
Termination and Labeling:
Terminate copper cables onto jacks, connectors, and patch panels with precise attention to detail and adherence to industry standards
Ensure proper labeling of cables, connectors, and outlets for easy identification and maintenance
Maintain accurate documentation of cable layouts, terminations, and labeling
Testing and Troubleshooting:
Conduct comprehensive cable testing using appropriate tools to validate signal quality, continuity, and performance
Troubleshoot and resolve connectivity issues, signal degradation, and other cable-related problems
Utilize testing equipment such as cable testers and certification tools to ensure compliance with required specifications
Quality Assurance:
Ensure all cabling work is performed to high quality standards and follows company policies and guidelines
Perform quality checks on cable installations to confirm compliance with industry standards and design specifications
Network Expansion:
Collaborate with IT team members to support network expansion projects and ensure seamless integration of new cabling
Participate in designing and implementing cable layouts for new network infrastructure components
Health and Safety:
Adhere to safety protocols and guidelines to prevent accidents and maintain a safe working environment
Operate tools and equipment safely and responsibly, following industry best practices
Requirements:
High school diploma or equivalent; additional technical certifications or coursework in cabling and network infrastructure is a plus
Proven experience as a Data Cabling Engineer or similar role, with expertise in Cat6 and Cat6a copper cable installation, termination, and testing
Familiarity with relevant cabling standards, such as TIA/EIA and ISO/IEC
Proficiency in using cable testing and certification tools
Strong understanding of network topologies, protocols, and network equipment
Ability to interpret technical drawings, schematics, and cabling diagrams
Excellent problem-solving skills with a keen attention to detail
Effective communication skills to collaborate with IT team members and project stakeholders
Ability to work independently or as part of a team in various environments, including office spaces, data centers, and remote locations
Strong commitment to safety and adherence to industry regulations
General tools required for this job:-
Cable Stripping and Cutting Tools:
Cable stripper: Used to remove the outer insulation from cables
Cable cutter: Used to cut cables to appropriate lengths
Termination Tools:
RJ45 crimping tool: Used to attach RJ45 connectors to the ends of Ethernet cables
Punch-down tool: Used for terminating cables on patch panels, keystone jacks, and outlets
Testing and Certification Tools:
Cable tester: Used to test cable continuity, signal quality, and detect faults
Cable certifier: Provides detailed analysis and certification of cable performance against industry standards
Tone generator and probe: Used to trace and identify cables within a bundle or behind walls
Labeling and Identification Tools:
Cable labels and markers: Used to label and identify cables, connectors, and outlets
Label printer: Used to create professional labels for cables and equipment
Measuring and Alignment Tools:
Tape measure or ruler: Used to measure cable lengths accurately
Level: Ensures proper alignment of cable pathways and connectors
Cable Management Tools:
Cable ties and Velcro straps: Used for bundling and organizing cables
Cable clips and mounts: Secure cables along walls, ceilings, or other surfaces
Drilling and Mounting Tools:
Power drill and bits: Used to create holes for cable pass-throughs and mounting hardware
Screwdrivers and wall anchors: Required for mounting cable trays, conduits, and outlets
Safety Gear:
Safety glasses: Protects eyes from debris and potential hazards
Work gloves: Provides hand protection during installation and cable handling
Tool Bag or Pouch:
Keeps tools organized and easily accessible during installation tasks
Documentation Tools:
Pen and notepad or mobile device: Used to document cable layouts, terminations, and labeling
Personal Protective Equipment (PPE):
Steel-toed boots: Provides foot protection in potentially hazardous environments
Hard hat: Required in construction or industrial settings
Optional Tools:
Cable fish tape or rods: Used to guide cables through walls, ceilings, or conduits
Cable lubricant: Assists in pulling cables through tight spaces
Cable toner and probe: Helps identify specific cables in a bundle
If you are a motivated Data Cabling Engineer with a passion for maintaining robust network infrastructure and ensuring seamless connectivity, we encourage you to apply. Contact Jake on jake@digitalwaffle.co.uk
Show more
Show less","Cat6, Cat6a, TIA/EIA, ISO/IEC, RJ45, Cable Stripper, Cable Cutter, Crimping Tool, PunchDown Tool, Cable Tester, Cable Certifier, Tone Generator and Probe, Cable Labels and Markers, Tape Measure or Ruler, Cable Ties and Velcro Straps, Cable Clips and Mounts, Power Drill and Bits, Screwdrivers and Wall Anchors, Safety Glasses, Work Gloves, Pen and Notepad, Mobile Device, SteelToe Boots, Hard Hat, Cable Fish Tape or Rods, Cable Lubricant, Cable Toner and Probe","cat6, cat6a, tiaeia, isoiec, rj45, cable stripper, cable cutter, crimping tool, punchdown tool, cable tester, cable certifier, tone generator and probe, cable labels and markers, tape measure or ruler, cable ties and velcro straps, cable clips and mounts, power drill and bits, screwdrivers and wall anchors, safety glasses, work gloves, pen and notepad, mobile device, steeltoe boots, hard hat, cable fish tape or rods, cable lubricant, cable toner and probe","cable certifier, cable clips and mounts, cable cutter, cable fish tape or rods, cable labels and markers, cable lubricant, cable stripper, cable tester, cable ties and velcro straps, cable toner and probe, cat6, cat6a, crimping tool, hard hat, isoiec, mobile device, pen and notepad, power drill and bits, punchdown tool, rj45, safety glasses, screwdrivers and wall anchors, steeltoe boots, tape measure or ruler, tiaeia, tone generator and probe, work gloves"
Lead Data Engineer,Apexon,"Birmingham, England, United Kingdom",https://uk.linkedin.com/jobs/view/lead-data-engineer-at-apexon-3780719414,2023-12-17,Coventry, United Kingdom,Mid senior,Onsite,"Description
Position at Saggezza UK
Company Description: Apexon is a digital-first technology services firm backed by Goldman Sachs Asset Management and Everstone Capital. We specialize in accelerating business transformation and delivering human-centric digital experiences. For over 17 years, Apexon has been meeting customers wherever they are in the digital lifecycle and helping them outperform their competition through speed and innovation.
We are currently looking to hire Lead Data Engineers for our growing team.
Responsibilities
In an Agile environment manage end-to-end systems development cycle from requirements analysis to coding, testing, UAT, implementation and maintenance
Develop high level and detailed technical designs, testing strategies, and implementation plans
Work in a dynamic, fast-paced environment that provides exposure to all areas of Finance and Risk
Understand and respond to business needs, facilitating and developing process workflow, data requirements, and specifications required to support implementation
Identify opportunities for cross-divisional collaboration and reuse of common solutions
Provide technical and functional guidance and leadership to junior members on a need basis
Build and maintain key financial metric calculation models and associated infrastructure to support forward-looking business strategies and decisions in a evolving regulatory landscape
Work closely with a wide range of stakeholders globally, including Controllers, Risk, Operations, and various business units
Build strong relationships with business partners and front-line managers
Lead talented software engineers who are passionate about identifying patterns and building common solutions
Sklls And Experience Needed
Bachelor’s or master's degree in computer science, Mathematics, Electrical Engineering or related technical discipline
Experience in software development, including a clear understanding of data structures, algorithms, software design and core programming concepts
Experience preferably in Java, C/C++, Python, competent in traditional (RDBMS) and modern datastores (NoSQL)
Strong programming and problem-solving skills and analytical skills
A clear understanding of data structures, algorithms, software design and core programming concepts
Strong full-stack technical design and development skills and experience
Comfortable with multi-tasking, managing multiple stakeholders and working as part of a team
Excellent communication skills including experience speaking to technical and business audiences and working globally
Interest in finance
Can apply an entrepreneurial approach and passion to problem solving and product development
Preferred Qualifications
Strong programming experience in at least one language
Comfortable with working with multiple languages
Technologies: Scala, Java, Python, Spark, Linux and shell scripting, TDD (JUnit), build tools (Maven/Gradle/Ant)
Experience with continuous delivery and deployment
Proficient at working with large and complex code bases
Alloy, Data Lake
PURE
Being able to describe, connect and visualise data
Model to model mapping
Creating data models
Office / hub location:
Birmingham
Onsite/Remote/Hybrid (if travel required how much?):
Onsite
We Also Offer
Up to 10% bonus (based on company and personal performance).
An employer pension scheme
25 days holiday + 8 bank holidays, with the option to carry forward or 'cash-in' 5 days each year
Access to YuLife wellness platform, subscription to Meditopia App, premium subscription to Fiit, life coaching & emotional wellbeing sessions, 24 / 7 virtual GP Access, Employee Assistance Programme
Life Insurance & Income protection
Enhanced Maternity Pay & Paternity Pay
Cycle to work scheme
Travel loan scheme
A Tech Scheme which lets you choose from over 5000 tech products at up to a 12% discount
Free unlimited Udemy account for every employee to support their continuous learning and improvement
Support in obtaining relevant certifications
Disclaimer: If you feel that this is a good match for your skillsets, please submit a current word version of your resume along with a cover letter describing your skills, experience and salary expectations. We are an Equal Opportunity Employer (EOE). You can read our job applicant privacy policy here .
Show more
Show less","Java, C/C++, Python, Scala, Spark, Linux, Shell scripting, TDD (JUnit), Maven, Gradle, Ant, Data structures, Algorithms, Software design, Core programming concepts, NoSQL, RDBMS, Continuous delivery, Continuous deployment, Large code bases, Complex code bases, Alloy, Data Lake, PURE, Data visualisation, Model mapping, Data modelling","java, cc, python, scala, spark, linux, shell scripting, tdd junit, maven, gradle, ant, data structures, algorithms, software design, core programming concepts, nosql, rdbms, continuous delivery, continuous deployment, large code bases, complex code bases, alloy, data lake, pure, data visualisation, model mapping, data modelling","algorithms, alloy, ant, cc, complex code bases, continuous delivery, continuous deployment, core programming concepts, data lake, data modelling, data structures, data visualisation, gradle, java, large code bases, linux, maven, model mapping, nosql, pure, python, rdbms, scala, shell scripting, software design, spark, tdd junit"
Data Cabling Engineer- Contract,Digital Waffle,"Birmingham, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-cabling-engineer-contract-at-digital-waffle-3765285518,2023-12-17,Coventry, United Kingdom,Mid senior,Onsite,"Position: Data Cabling Engineer (Temp/Contract/Freelance)
Department: Information Technology / Network Infrastructure
Location: Birmingham
Rate: £180-200 p/d
Contract: 6 Months
Job Summary: We are seeking a skilled and dedicated Data Cabling Engineer. The successful candidate will be responsible for the installation, testing, and termination of Cat6 and Cat6a branded copper cables within metal trunking and containment to ensure a reliable and efficient network infrastructure. The role involves working closely with the IT team to maintain high-quality data connectivity, support network expansion, and ensure optimal performance.
Responsibilities:
Cabling Installation:
Plan, install, and route Cat6 and Cat6a copper cables according to industry standards and best practices
Mount, secure, and organize cable pathways, including cable trays, conduits, and wall penetrations
Install cable connectors, outlets, and termination panels in compliance with design specifications
Termination and Labeling:
Terminate copper cables onto jacks, connectors, and patch panels with precise attention to detail and adherence to industry standards
Ensure proper labelling of cables, connectors, and outlets for easy identification and maintenance
Maintain accurate documentation of cable layouts, terminations, and labelling
Testing and Troubleshooting:
Conduct comprehensive cable testing using appropriate tools to validate signal quality, continuity, and performance
Troubleshoot and resolve connectivity issues, signal degradation, and other cable-related problems
Utilize testing equipment such as cable testers and certification tools to ensure compliance with required specifications
Quality Assurance:
Ensure all cabling work is performed to high-quality standards and follows company policies and guidelines
Perform quality checks on cable installations to confirm compliance with industry standards and design specifications
Network Expansion:
Collaborate with IT team members to support network expansion projects and ensure seamless integration of new cabling
Participate in designing and implementing cable layouts for new network infrastructure components
Health and Safety:
Adhere to safety protocols and guidelines to prevent accidents and maintain a safe working environment
Operate tools and equipment safely and responsibly, following industry best practices.
Requirements:
High school diploma or equivalent; additional technical certifications or coursework in cabling and network infrastructure is a plus
Proven experience as a Data Cabling Engineer or similar role, with expertise in Cat6 and Cat6a copper cable installation, termination, and testing
Familiarity with relevant cabling standards, such as TIA/EIA and ISO/IEC
Proficiency in using cable testing and certification tools
Strong understanding of network topologies, protocols, and network equipment
Ability to interpret technical drawings, schematics, and cabling diagrams
Excellent problem-solving skills with a keen attention to detail
Effective communication skills to collaborate with IT team members and project stakeholders
Ability to work independently or as part of a team in various environments, including office spaces, data centers, and remote locations
Strong commitment to safety and adherence to industry regulations.
General tools required for this job:-
Cable Stripping and Cutting Tools:
Cable stripper: Used to remove the outer insulation from cables
Cable cutter: Used to cut cables to appropriate lengths
Termination Tools:
RJ45 crimping tool: Used to attach RJ45 connectors to the ends of Ethernet cables
Punch-down tool: Used for terminating cables on patch panels, keystone jacks, and outlets
Testing and Certification Tools:
Cable tester: Used to test cable continuity, signal quality, and detect faults
Cable certifier: Provides detailed analysis and certification of cable performance against industry standards
Tone generator and probe: Used to trace and identify cables within a bundle or behind walls
Labeling and Identification Tools:
Cable labels and markers: Used to label and identify cables, connectors, and outlets
Label printer: Used to create professional labels for cables and equipment
Measuring and Alignment Tools:
Tape measure or ruler: Used to measure cable lengths accurately
Level: Ensures proper alignment of cable pathways and connectors
Cable Management Tools:
Cable ties and Velcro straps: Used for bundling and organizing cables
Cable clips and mounts: Secure cables along walls, ceilings, or other surfaces
Drilling and Mounting Tools:
Power drill and bits: Used to create holes for cable pass-throughs and mounting hardware
Screwdrivers and wall anchors: Required for mounting cable trays, conduits, and outlets
Safety Gear:
Safety glasses: Protects eyes from debris and potential hazards
Work gloves: Provides hand protection during installation and cable handling
Tool Bag or Pouch:
Keeps tools organized and easily accessible during installation tasks
Documentation Tools:
Pen and notepad or mobile device: Used to document cable layouts, terminations, and labelling
Personal Protective Equipment (PPE):
Steel-toed boots: Provides foot protection in potentially hazardous environments
Hard hat: Required in construction or industrial settings
Optional Tools:
Cable fish tape or rods: Used to guide cables through walls, ceilings, or conduits
Cable lubricant: Assists in pulling cables through tight spaces
Cable toner and probe: Helps identify specific cables in a bundle.
If you are a motivated Data Cabling Engineer with a passion for maintaining robust network infrastructure and ensuring seamless connectivity, we encourage you to apply. Contact Joe on joe@digitalwaffle.co.uk
Show more
Show less","CAT6, CAT6a, TIA/EIA, ISO/IEC, RJ45, Punchdown tool, Cable tester, Cable certifier, Tone generator, Cable labels, Label printer, Cable ties, Velcro straps, Cable clips, Cable mounts, Power drill, Screwdrivers, Safety glasses, Work gloves, Tool bag, Pen, Notepad, Mobile device, Personal Protective Equipment (PPE), Steeltoed boots, Hard hat, Cable fish tape, Cable lubricant, Cable toner","cat6, cat6a, tiaeia, isoiec, rj45, punchdown tool, cable tester, cable certifier, tone generator, cable labels, label printer, cable ties, velcro straps, cable clips, cable mounts, power drill, screwdrivers, safety glasses, work gloves, tool bag, pen, notepad, mobile device, personal protective equipment ppe, steeltoed boots, hard hat, cable fish tape, cable lubricant, cable toner","cable certifier, cable clips, cable fish tape, cable labels, cable lubricant, cable mounts, cable tester, cable ties, cable toner, cat6, cat6a, hard hat, isoiec, label printer, mobile device, notepad, pen, personal protective equipment ppe, power drill, punchdown tool, rj45, safety glasses, screwdrivers, steeltoed boots, tiaeia, tone generator, tool bag, velcro straps, work gloves"
Principal Data Engineer,DiverseJobsMatter,"Birmingham, England, United Kingdom",https://uk.linkedin.com/jobs/view/principal-data-engineer-at-diversejobsmatter-3777679440,2023-12-17,Coventry, United Kingdom,Mid senior,Onsite,"JOB DESCRIPTION
About The Role
We are recruiting for a
Principal Data Engineer
to join our team.
Principal Data Engineers
work in multi-disciplinary teams that build, support & maintain User-Centred digital solutions that offer real value and work for everyone.
Our
Principal Data Engineers
are experts in their field. You will act as a principal consultant providing technical expertise in data engineering practices both internally and to our clients. You will drive the team's technical deliverables, maintain client relationships and be passionate about developing and upskilling others.
Your role in nutshell:
Act as a technical SME within our engineering community to produce and ensure user-centred software is developed based on business requirements and following best practice
Work collaboratively with colleagues to explore, design and deliver solutions to client problems
Maintain client relationships and troubleshoot issues through collaboration
Design and manage software development and deployment pipelines resolving issues and potential bottlenecks before they occur
Continuously reviews and prioritises the delivery teams workload and can anticipate the need for realignment with evolving operational prioritise and complex work-streams and assignments
Be a strong communicator and presenter, presenting prototypes, solutions and progress to internal/external stakeholders in a clear concise manner
Build great relationships with your team and stakeholders, identifying and ensuring that challenges are overcome
Lead in the recruitment of other engineers, and support other consultants in their professional development
Ensure our Data and Engineering communities contribute to our business at a strategic level
Lead on new business opportunities, writing technical bids and proposals
Promote our Data team externally (for example through writing Blogs, Workshops, Seminars or Conferences)
About the Candidate:
Excellent experience delivering in Python and experience with at least one other core language
Excellent experience of SQL and relational databases. NoSQL databases is also desirable.
Solid experience of at least one Cloud provider such as AWS, Azure or GCP.
Preference will be given on candidates with strong Azure Cloud and general Microsoft Technology Stacks (e.g. MS SQL and .Net frameworks)
Strong experience of working with different data formats, e.g., CSV, JSON and XML
Building reliable Data Pipelines
Broader knowledge of IT — e.g., Security and Networking
Working in an Agile Environment
Test-Driven Development and/or Behaviour Driven Development
Continuous Integration and Continuous Deployment (CI/CD)
Experience of operating as a technical leader on complex projects and able to manage stakeholder expectations and influencing decisions
Coaching and guiding others on the best way to achieve an optimal solution
Excellent verbal/written communication skills, able to articulate in both technical/non-technical terms depending on audience
Technical leadership and/or mentoring
Define working procedures and methodologies for the team
Show more
Show less","Data Engineering, Python, SQL, NoSQL, AWS, Azure, GCP, Microsoft SQL, .Net, CSV, JSON, XML, Data Pipelines, Security, Networking, Agile, TestDriven Development, Behaviour Driven Development, Continuous Integration, Continuous Deployment, Stakeholder Management, Coaching, Mentoring, Communication","data engineering, python, sql, nosql, aws, azure, gcp, microsoft sql, net, csv, json, xml, data pipelines, security, networking, agile, testdriven development, behaviour driven development, continuous integration, continuous deployment, stakeholder management, coaching, mentoring, communication","agile, aws, azure, behaviour driven development, coaching, communication, continuous deployment, continuous integration, csv, data engineering, datapipeline, gcp, json, mentoring, microsoft sql, net, networking, nosql, python, security, sql, stakeholder management, testdriven development, xml"
Senior Data Engineer,Nigel Frank International,"Birmingham, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-at-nigel-frank-international-3731264272,2023-12-17,Coventry, United Kingdom,Mid senior,Onsite,"I am working with a market leading manufacturer of specialist medical equipment who are looking for a Senior Data Engineer at the start of a number of exciting data driven projects. You would be joining at the start of a greenfield project where you will be responsible for the data migration to the brand new Azure Data Platform!
The overall goal of the business and ultimately this role is to deliver data to healthcare professionals to aid with rehabilitation. You will join a close-knit team, with talented professionals who specialise in a number of areas including software development and data science. You will become the expert for this team on all things data engineering and will take the lead on the creation, development and maintenance of the Azure data platform.
As part of this role, you will be responsible for some of the following areas.
Design, maintain and optimise on premise database solutions.
Develop and maintain robust ETL pipelines.
Work with other specialists within the team to make technical decisions that will benefit the overall business.
Take the lead of migration of data to the new Azure data platform.
This is a salaried role paying up to £65,000 per annum depending on experience and a company benefits package. This role would be largely a remote opportunity with occasional visits to the office in Stafford. These visits would be as and when needed in line with project requirements.
To be successful in the role you will have.
Strong knowledge of Databricks for data ingestion and transformation.
Experience working with the Azure tech stack including Azure Data Factory, Synapse and Azure Data Lake.
Experience designing and implementing ETL solutions.
Strong coding experience with languages such as Python or C#.
This is just a brief overview of the role. For the full information, simply apply to the role with your CV, and I will call you to discuss further. My client is looking to begin the interview process ASAP, so don't miss out, APPLY now! To do so please email me at a.pinkerton@nigelfrank.com or call me on 0191 3387487.
Nigel Frank International are the go-to recruiter for Power BI and Azure Data Platform roles in the UK offering more opportunities across the country than any other recruitment agency. We're the proud sponsor and supporter of SQLBits, Power Platform World Tour, the London Power BI User Group, Newcastle Power BI User Group and Newcastle Data Platform and Cloud User Group. We are the global leaders in Microsoft recruitment.
Show more
Show less","Data Engineering, Azure Data Platform, Azure Data Factory, Synapse, Azure Data Lake, Databricks, ETL, Python, C#, SQL","data engineering, azure data platform, azure data factory, synapse, azure data lake, databricks, etl, python, c, sql","azure data factory, azure data lake, azure data platform, c, data engineering, databricks, etl, python, sql, synapse"
Data Engineering Consultant,Searchability,"Lichfield, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineering-consultant-at-searchability-3779097849,2023-12-17,Coventry, United Kingdom,Mid senior,Onsite,"Data Engineering Consultant
Location:
Lichfield
Salary:
£40,000-£80,000
On-Site
About The Company:
With our expertise in data analytics, data management, data architecture, data tool etc, we help complex companies to enable data-driven decision making. We deliver projects all over the UK that support every colleague in an organisation, with no need to correct data or for manual intervention.
Role overview:
As a Data Engineer, your primary role will involve providing robust support for BI initiatives and projects by contributing to the design and implementation of our Azure-based Data and Business Analytics Platform. Your responsibilities will extend to creating comprehensive design guides and infrastructure documentation. In addition to your hands-on involvement, you will play a pivotal role in client interactions, conducting data and infrastructure workshops on-site. It will be your responsibility to secure agreement and sign-off from customers.
While actively engaged in the technical aspects of data engineering, you will also assume a leadership role, guiding and supporting team members within the Data Engineering function. Your influence will extend to providing valuable insights and direction to both the senior team and analytical teams
Role requirements:
Strong Azure Data Factory experience
Experience with Python
Databricks experience
Ability to consult face-to-face with customers
Ability to design and implement ETL
Show more
Show less","Azure Data Factory, Python, Databricks, ETL, Data Analytics, Data Management, Data Architecture, Data Tools, Datadriven Decision Making, Business Analytics Platform, Design Guides, Infrastructure Documentation, Leadership, Guidance, Support","azure data factory, python, databricks, etl, data analytics, data management, data architecture, data tools, datadriven decision making, business analytics platform, design guides, infrastructure documentation, leadership, guidance, support","azure data factory, business analytics platform, data architecture, data management, data tools, dataanalytics, databricks, datadriven decision making, design guides, etl, guidance, infrastructure documentation, leadership, python, support"
Senior AWS Data Engineer,Jefferson Frank,"Birmingham, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-aws-data-engineer-at-jefferson-frank-3726882327,2023-12-17,Coventry, United Kingdom,Mid senior,Onsite,"We are a global analytics and digital solutions company that partners with clients to improve business outcomes and unlock growth. Bringing together domain expertise with robust data, powerful analytics, cloud and AI to create agile, scalable solutions and execute complex operations for the world's leading corporations. We were founded on the core values of innovation, collaboration, excellence, integrity and respect creating value from data to ensure faster decision-making and transforming operating models. Key industries including Insurance, Healthcare, Banking and Financial Services, Media, and Retail among others.
Headquartered in New York, our team is over 40,000 strong, with more than 50 offices spanning six continents.
Role Title: Cloud Data Engineer
BU/Segment: Analytics
Location: Northampton or Knutsford
Employment Type: Permanent or FTC
Summary Of The Role
Working with senior stakeholders in the organisational data platforms team and representing that team internally to various lines of business.
As part of your duties, you will be responsible for:
Engaging with various businesses (e.g., payments, cards, consumer banking, retail credit risk, fraud etc.) to analyse their data consumption patterns and chart the individual journeys to AWS (their current state could be varied) - to help deliver a consistent cloud-based data platform
Technically hands-on on data warehousing, ETL and Cloud (AWS) technologies and functional knowledge of use of data for downstream processes
Strong background in retail banking use-cases
Qualifications and experience we consider to be essential for the role:
Functional Engineer or Project Manager with 'hands-on' coding experience to generate business ready 'marts'
You will be skilled to engage with internal teams and take their data and analytics stack to the cloud.
You will ideally have 6 plus years within retail banking.
Skills And Personal Attributes We Would Like To Have
Able to work in a matrix fast paced financial services group.
Hands on 'write code' technical skills.
Taking Data and Analytics stack to the cloud
As part of a leading global analytics and digital solutions company, you can look forward to:
A competitive salary with private healthcare, critical illness life assurance at 4 x your annual salary, income protection insurance, and a rewarding pension.
We are committed to providing our employees with the tools and resources they need to succeed and excel in their careers. We offer a wide range of professional and personal development opportunities. We also support a range of learning initiatives that allow our employees to build on their existing skills and knowledge. From online courses to seminars and workshops, our employees have the opportunity to enhance their skills and stay up to date with the latest trends and technologies.
As an Equal Opportunity Employer, we are committed to diversity. Our company does not discriminate based on race, religion, colour, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, age, or disability status.
We offer a flexible hybrid working model that allows employees to live a balanced, healthy lifestyle while strengthening our culture of collaboration.
Show more
Show less","Cloud Computing, AWS, Data Warehousing, ETL, Retail Banking, Data Analytics, Data Consumption Patterns, Data Journey, Data Stack, Cloud Migration, Software Development, Matrix Management, Coding, Programming","cloud computing, aws, data warehousing, etl, retail banking, data analytics, data consumption patterns, data journey, data stack, cloud migration, software development, matrix management, coding, programming","aws, cloud computing, cloud migration, coding, data consumption patterns, data journey, data stack, dataanalytics, datawarehouse, etl, matrix management, programming, retail banking, software development"
"Senior Cloud Data Engineer - GBP70,000",Nigel Frank International,"Birmingham, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-cloud-data-engineer-gbp70-000-at-nigel-frank-international-3763115856,2023-12-17,Coventry, United Kingdom,Mid senior,Onsite,"Senior Cloud Data Engineer - £70,000
I am working with a market leading manufacturer of specialist medical equipment who are looking for a Senior Cloud Data Engineer at the start of a number of exciting data driven projects. You would be joining at the start of a greenfield project where you will be responsible for the data migration, development and maintenance of the brand new Azure Data Platform!
The overall goal of the business and ultimately this role is to deliver data to healthcare professionals to aid with rehabilitation of patients. You will join a close-knit team, with talented professionals who specialise in a number of areas including software development and data science. You will be the in house expert for the team on all things data engineering and will take the lead on the creation, development and maintenance of the Azure data platform.
As part of this role, you will be responsible for some of the following areas.
Design, maintain and optimize both on premise and cloud based database solutions.
Develop and maintain robust ETL pipelines.
Work with other specialists within the team to make technical decisions that will benefit the overall business.
Take the lead of migration of data to the new Azure data platform.
This is a salaried role paying up to £70,000 per annum depending on experience and a company benefits package. This role would be largely a remote opportunity with occasional visits to the office in Stafford. These visits would be as and when needed in line with project requirements.
To be successful in the role you will have.
Experience working with the Azure tech stack including Azure Data Factory, Synapse and Azure Data Lake.
Experience designing and implementing ETL solutions.
Strong coding experience with languages such as Python or C#.
Experience working as a DBA or completing database administration tasks
This is just a brief overview of the role. For the full information, simply apply to the role with your CV, and I will call you to discuss further. My client is looking to begin the interview process ASAP, so don't miss out, APPLY now! To do so please email me at a.pinkerton@nigelfrank.com or call me on 0191 3387487.
Nigel Frank International are the go-to recruiter for Power BI and Azure Data Platform roles in the UK offering more opportunities across the country than any other recruitment agency. We're the proud sponsor and supporter of SQLBits, Power Platform World Tour, the London Power BI User Group, Newcastle Power BI User Group and Newcastle Data Platform and Cloud User Group. We are the global leaders in Microsoft recruitment.
Show more
Show less","Azure tech stack, Azure Data Factory, Synapse, Azure Data Lake, ETL solutions, Python, C#, DBA, Database administration, SQL, Power BI","azure tech stack, azure data factory, synapse, azure data lake, etl solutions, python, c, dba, database administration, sql, power bi","azure data factory, azure data lake, azure tech stack, c, database administration, dba, etl solutions, powerbi, python, sql, synapse"
Data Analyst - SQL & Power BI - UK-wide,Nigel Frank International,"Birmingham, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-analyst-sql-power-bi-uk-wide-at-nigel-frank-international-3780312517,2023-12-17,Coventry, United Kingdom,Mid senior,Onsite,"A healthcare provider are looking for an Information Analyst with skills in SQL and either SSRS or Power BI to join their established Management Information team, where you'll be responsible for delivering timely and accurate data and reporting solutions for users across the organisation.
This organisation are headquartered in Hertfordshire, although this is a home-based role, so you can be based anywhere in the UK. It's likely you will be asked to travel to their Head Office around twice per year for full IT team meetings, and this will be fully expensed.
We are looking for an individual with strong SQL and T-SQL skills in order to effectively work with the data in their SQL and Snowflake database, as well as experience with either SSRS or Power BI to produce insightful reports and visualisations - currently they're on a journey to migrate most of their SSRS reports over to Power BI, with any new reports being created in Power BI.
This is a really exciting opportunity for an ambitious data professional to progress their career with an organisation who will be committed to your ongoing professional development, with the chance to work towards your Microsoft certifications - you'll have the opportunity to complete your Power BI and Snowflake certifications, and get your ITIL qualifications too.
Requirements:
Strong SQL and T-SQL skills - including complex queries, stored procedures, tables, views etc.
SSRS or Power BI Report builder experience (including DAX)
Excellent communication, problem solving and stakeholder management skills
Benefits:
Salary of £25-32,000, depending upon experience
25 days' annual leave plus bank holidays plus day off for your birthday (increasing by 1 day for every 2 years' service up to 28 days)
Pension with 5% employer contribution and 3% employee contribution
Enhanced maternity package
Death in service (2X salary)
Learning and development opportunities
Please Note: This is a permanent role for UK residents only. This role does not offer Sponsorship. You must have the right to work in the UK with no restrictions. Some of our roles may be subject to successful background checks including a DBS and Credit Check.
Nigel Frank are the go-to recruiter for Power BI and Azure Data Platform roles in the UK, offering more opportunities across the country than any other. We're the proud sponsor and supporter of SQLBits, Power Platform World Tour, the London Power BI User Group, Newcastle Power BI User Group and Newcastle Data Platform and Cloud User Group. To find out more and speak confidentially about your job search or hiring needs, please contact me directly at v.simpson@nigelfrank.com
Show more
Show less","SQL, TSQL, SSRS, Power BI, DAX, Snowflake, Azure Data Platform, ITIL","sql, tsql, ssrs, power bi, dax, snowflake, azure data platform, itil","azure data platform, dax, itil, powerbi, snowflake, sql, ssrs, tsql"
Senior Database Engineer (UK Remote),Turnitin,"Birmingham, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-database-engineer-uk-remote-at-turnitin-3693694966,2023-12-17,Coventry, United Kingdom,Mid senior,Remote,"Company Description
MUST BE UK BASED
100% REMOTE
When you join Turnitin, you'll be welcomed into a company that is a recognized innovator in the global education space. For more than 20 years, Turnitin has partnered with educational institutions to promote honesty, consistency, and fairness across all subject areas and assessment types. Over 16,000 academic institutions, publishers, and corporations use our services: Gradescope by Turnitin, iThenticate, Turnitin Feedback Studio, Turnitin Originality, Turnitin Similarity, ExamSoft, and ProctorExam.
Turnitin has offices in Australia, India, Indonesia, Japan, Korea, Mexico, the Netherlands, the Philippines, Ukraine, the United Kingdom, and the United States. Our diverse community of colleagues are all unified by a shared desire to make a difference in education. Come join us, and let's make change together.
Job Description
We are looking for an experienced Senior Data Engineer who thinks in clever ways to solve data problems of scale and load with elegant solutions.  Our team supports several PostgreSQL/SQLServer clusters that contain thousands of databases.  These data stores are up to 4+TB each with hundreds of millions of rows, as well as many on-premises and cloud data storage systems, such as Aurora, Redis, Memcached, Redshift, CockroachDB, DynamoDB, and Cassandra.  As a senior member of this team, you will work closely with our DevOps and Engineering Teams to help us maintain automation and stability in our data stores.
Key Responsibilities
Protect, tune, migrate, and administer On-premises and Cloud Data Stores.
Participate in a 24x7 on-call rotation.
Always perform in a manner that guarantees the Protection, Availability, and Performance of our Global Data Stores.
Be opinionated enough to speak up when you think we could be doing something better than we're doing it now -- and tactful and empathetic enough to communicate this in a way that brings people along instead of distancing them
Qualifications
Passion for data stores and a high sense of ownership while performing critical duties based on senior-level experience in Security, Disaster Recovery, and High Availability.
Ability to have a strong work ethic in a fast-paced environment with multiple priorities that may occasionally change.
Ability to work independently and perform under pressure.
Good interpersonal skills, friendly, and approachable.
Deep Linux experience.Strong SQL skills.
Expert in PostgreSQL/SQLServer tuning and best practices.
AWS experience, including Terraform.
Automated monitoring and alerting of On-premises and Cloud data technologies, such as Aurora, Redshift, Redis, CockroachDB, and Cassandra.
Kubernetes experience.
Working experience with configuration management tools, preferably Puppet and Terraform.
Additional Information
No agency submissions
Our Mission
is to ensure the integrity of global education and meaningfully improve learning outcomes.
Our Values
underpin everything we do.
Customer Centric -
We realize our mission to ensure integrity and improve learning outcomes by  putting educators and learners at the center of everything we do.
Passion for Learning -
We seek out teammates that are constantly learning and growing and build a workplace which enables them to do so.
I
ntegrity -
We believe integrity is the heartbeat of Turnitin. It shapes our products, the way we treat each other, and how we work with our customers and vendors.
Action & Ownership -
We have a bias toward action and empower teammates to make decisions.
One Team -
We strive to break down silos, collaborate effectively, and celebrate each other’s successes.
Global Mindset -
We respect local cultures and embrace diversity. We think globally and act locally to maximize our impact on education.
Global Benefits
Flexible/hybrid working
Remote First Culture
Health Care Coverage*
Tuition Reimbursement*
Competitive Paid Time Off
4 Self-Care Days per year
National Holidays*
2 Founder Days + Juneteenth Observed
Paid Volunteer Time*
Charitable contribution match*
Monthly Wellness Reimbursement/Home Office Equipment*
Access to Modern Health (mental health platform)
Parental Leave*
Retirement Plan with match/contribution*
* varies by country
Seeing Beyond the Job Ad
At Turnitin, we recognize it’s unrealistic for candidates to fulfill 100% of the criteria in a job ad.  We encourage you to apply if you meet the majority of the requirements because we know that skills evolve over time. If you’re willing to learn and evolve alongside us, join our team!
Turnitin, LLC is committed to the policy that all persons have equal access to its programs, facilities and employment. All qualified applicants will receive consideration for employment without regard to age, race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.
Show more
Show less","PostgreSQL, SQLServer, Linux, SQL, Terraform, Puppet, Kubernetes, Aurora, Redis, CockroachDB, Cassandra, Redshift, AWS","postgresql, sqlserver, linux, sql, terraform, puppet, kubernetes, aurora, redis, cockroachdb, cassandra, redshift, aws","aurora, aws, cassandra, cockroachdb, kubernetes, linux, postgresql, puppet, redis, redshift, sql, sqlserver, terraform"
Data Engineer,RemoteWorker UK,"Coleshill, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-remoteworker-uk-3765623650,2023-12-17,Coventry, United Kingdom,Mid senior,Remote,"Data Engineer
Mercury is not just a company; we’re a transformative force in the world of SaaS for recruitment and staffing firms. Since our founding in 2014, we’ve been at the forefront, demonstrating a commitment to excellence and innovation that has allowed us to create a ground breaking SaaS platform, based on the Microsoft D365 and the Power Platform.
With a global presence in strategic locations, including Coleshill, London, Boston, Chicago, and Sydney, we’ve expanded at an impressive pace, breaking into Australia in 2019 and the US in 2020. We are now a dynamic team of 140 employees, and we’re not stopping there – we’re on track to double in size over the next 24 months.
Mercury is all about our customers, with over 100 worldwide and 8,000+ users experiencing the transformative power of our solutions. We’ve seen remarkable growth in headcount, surging by over 125% in the last two years. As a dedicated SaaS innovator, we are here to simplify and enhance the operations of recruitment and staffing firms.
What you’ll do
* Team up with our professional services experts, led by the dynamic Team Lead Data Migration, to skilfully transform customer data into the cutting-edge Mercury format.
* Take the reins of complete data migration processes, ensuring a flawless and timely integration of customer data into our Mercury solution.
* Craft and curate technical documentation, a crucial pillar supporting seamless project implementation and professional services processes.
* Showcase your prowess by delivering clear and achievable timescale estimates for customer data migrations, adhering to predefined benchmarks of time, cost, and scope.
* Dive into collaboration with project managers and new customers, mastering the art of comprehending and migrating business data into Mercury, guided by meticulous data mapping documentation.
* Be a driving force behind continuous improvement, offering insightful advice on updates to mapping documentation and processes, elevating our service to new heights.
Who you are
* 2-5 years of hands-on experience in commercial data engineering roles.
* Proficient in writing SQL statements and code, with a focus on T-SQL for effective data manipulation.
* Minimum of 2 years of practical experience with ETL processes, showcasing your ability to handle data transfers seamlessly.
* A dedicated data engineer, not a DBA, with a clear understanding of manipulating, storing, and transferring data.
* Proven ability to troubleshoot and solve data engineering challenges within a commercial context.
Desirable
* Experience or eagerness to work with SQL Server Integration Services (SSIS) for enhanced data integration.
* Knowledge or interest in working with Azure Data Factory, demonstrating a grasp of cloud-based data engineering.
* A mindset geared towards ongoing learning and growth within a collaborative team environment.
* Ability to adapt quickly to new technologies and methodologies, ensuring flexibility in handling diverse data engineering tasks.
* Developing analytical thinking skills, contributing to more insightful and strategic data engineering solutions.
What we offer
* Competitive salary
* Great learning and development plan
* Microsoft certifications paid for and supported by Mercury
* Permanent Health Insurance (PHI)​
* Health Cash Plan​
* Critical Illness Cover
* Electric Car Vehicle Scheme​
* 4% Employer Pension Contribution​
* 25 days holiday per year​
* 1 full extra day “free holiday” on your Birthday
* Free parking
Location
At Mercury, we believe in the power of face-to-face collaboration. That’s why we encourage our team members to join us at our Coleshill office on Mondays and Tuesdays. However, we’re all about flexibility, so we support remote work from Wednesday through Friday if that’s your preference
Show more
Show less","SQL, TSQL, ETL, SSIS, Azure Data Factory, Cloudbased data engineering","sql, tsql, etl, ssis, azure data factory, cloudbased data engineering","azure data factory, cloudbased data engineering, etl, sql, ssis, tsql"
Senior Azure Data Engineer - Remote - Up to GBP70k,Nigel Frank International,"Birmingham, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-azure-data-engineer-remote-up-to-gbp70k-at-nigel-frank-international-3731978621,2023-12-17,Coventry, United Kingdom,Mid senior,Remote,"Senior Azure Data Engineer - Remote - £65-70k
Our client is a market-leading Legal researching organisation. They are looking to add an experienced Azure Data Engineer to join a diverse, results-driven, and collaborative team. The purpose of the team is to support the ongoing development of a data platform.
In this role, you will get to work with Azure in modern delta lake architecture. You will be working alongside in-house team experts who will support you in your skills and career development. This is a perfect opportunity for data engineers looking to take their careers to the next level.
Skills & Qualifications
6+ years' experience in Data engineering
Excellent knowledge and hands on experience in Databricks & PySpark is essential
Exzellent understanding in SQL databases
Knowledge of Azure cloud services
Good understanding and strong TSQL AND Python programming
Exposure to Azure data lakes, Azure data factory, HDFS and Azure Synapse
Good knowledge of data governance , data cataloguing, MDM
Culture
Innovative
Friendly
Collaborative
Results-driven
Diverse
Show more
Show less","Azure, Delta Lake, Databricks, PySpark, SQL, Azure Cloud Services, TSQL, Python, Azure Data Lake, Azure Data Factory, HDFS, Azure Synapse, Data Governance, Data Cataloguing, MDM","azure, delta lake, databricks, pyspark, sql, azure cloud services, tsql, python, azure data lake, azure data factory, hdfs, azure synapse, data governance, data cataloguing, mdm","azure, azure cloud services, azure data factory, azure data lake, azure synapse, data cataloguing, data governance, databricks, delta lake, hdfs, mdm, python, spark, sql, tsql"
Senior Azure Data Engineer - Remote - Up to GBP70k,Nigel Frank International,"Birmingham, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-azure-data-engineer-remote-up-to-gbp70k-at-nigel-frank-international-3733889030,2023-12-17,Coventry, United Kingdom,Mid senior,Remote,"Senior Azure Data Engineer - Remote - £65-70k
Our client is a market-leading Legal researching organisation. They are looking to add an experienced Azure Data Engineer to join a diverse, results-driven, and collaborative team. The purpose of the team is to support the ongoing development of a data platform.
In this role, you will get to work with Azure in modern delta lake architecture. You will be working alongside in-house team experts who will support you in your skills and career development. This is a perfect opportunity for data engineers looking to take their careers to the next level.
Skills & Qualifications
6+ years' experience in Data engineering
Excellent knowledge and hands on experience in Databricks & PySpark is essential
Exzellent understanding in SQL databases
Knowledge of Azure cloud services
Good understanding and strong TSQL AND Python programming
Exposure to Azure data lakes, Azure data factory, HDFS and Azure Synapse
Good knowledge of data governance , data cataloguing, MDM
Culture
Innovative
Friendly
Collaborative
Results-driven
Diverse
Show more
Show less","Azure, Delta Lake, Databricks, PySpark, SQL, TSQL, Python, Azure Data Lake, Azure Data Factory, HDFS, Azure Synapse, Data Governance, Data Cataloguing, MDM","azure, delta lake, databricks, pyspark, sql, tsql, python, azure data lake, azure data factory, hdfs, azure synapse, data governance, data cataloguing, mdm","azure, azure data factory, azure data lake, azure synapse, data cataloguing, data governance, databricks, delta lake, hdfs, mdm, python, spark, sql, tsql"
Senior Azure Data Engineer - Remote - Up to GBP70k,Nigel Frank International,"Birmingham, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-azure-data-engineer-remote-up-to-gbp70k-at-nigel-frank-international-3727985012,2023-12-17,Coventry, United Kingdom,Mid senior,Remote,"Senior Azure Data Engineer - Remote - £65-70k
Our client is a market-leading Legal researching organisation. They are looking to add an experienced Azure Data Engineer to join a diverse, results-driven, and collaborative team. The purpose of the team is to support the ongoing development of a data platform.
In this role, you will get to work with Azure in modern delta lake architecture. You will be working alongside in-house team experts who will support you in your skills and career development. This is a perfect opportunity for data engineers looking to take their careers to the next level.
Skills & Qualifications
6+ years' experience in Data engineering
Excellent knowledge and hands on experience in Databricks & PySpark is essential
Exzellent understanding in SQL databases
Knowledge of Azure cloud services
Good understanding and strong TSQL AND Python programming
Exposure to Azure data lakes, Azure data factory, HDFS and Azure Synapse
Good knowledge of data governance , data cataloguing, MDM
Culture
Innovative
Friendly
Collaborative
Results-driven
Diverse
Show more
Show less","Data Engineering, Databricks, PySpark, SQL, Azure Cloud Services, TSQL, Python, Azure Data Lakes, Azure Data Factory, HDFS, Azure Synapse, Data Governance, Data Cataloguing, MDM","data engineering, databricks, pyspark, sql, azure cloud services, tsql, python, azure data lakes, azure data factory, hdfs, azure synapse, data governance, data cataloguing, mdm","azure cloud services, azure data factory, azure data lakes, azure synapse, data cataloguing, data engineering, data governance, databricks, hdfs, mdm, python, spark, sql, tsql"
Junior Data Engineer,Sparta Global,"Birmingham, England, United Kingdom",https://uk.linkedin.com/jobs/view/junior-data-engineer-at-sparta-global-3783936447,2023-12-17,Coventry, United Kingdom,Mid senior,Hybrid,"About Sparta Global
Embark on a transformative career journey with Sparta Global, where diversity, innovation, and passion for technology converge. We welcome individuals from all backgrounds, offering not just jobs, but dynamic careers in the tech industry. You'll work alongside enthusiastic professionals, receiving top-tier training and mentorship to hone your skills for success in both public and private sectors. Our commitment to designing impactful careers and coaching future leaders is evident in our over 10 prestigious awards in Learning & Development and Equality, Diversity & Inclusion. As a Top 20 Employer for Social Mobility and a proud B-Corp certified organisation, we're not just advancing careers; we're fostering a more diverse, equitable, and inclusive tech landscape. Join us in shaping the future of technology - where your growth is our mission, and your success, our pride. Apply now to be part of our award-winning team at Sparta Global.?
About This Role
You'll become versatile in a wide array of tools across topics covering data focused coding, data visualisation, Cloud Services and Big Data.
You'll be designing, building, maintaining, and troubleshooting the data pipelines that enable organizations to store, process, and analyse their data, and ensuring the data is reliable through testing and debugging. Looking for efficiencies and optimising the data pipelines for scalability and performance will be a focus.
You'll be handling and working with large sets of structured and unstructured data and will be responsible for ensuring that the data is organized and available for data scientists and analysts to use.
Working with others is key, you could be working with other engineers, developers, data scientists, analysts and even stakeholders to understand their data needs.
We're not expecting you to have the proficiencies right away - that's where our award-winning Academy comes in. We are the experts in building skills and confidence in a fun and supportive environment that will not only challenge you but also develop your specialist capabilities ready to work on our clients' projects.
What we're looking for.
To be successful for this role you will demonstrate a level of ability in Python or similar. You will be passionate about technology and eager to learn programme development to an advanced level.
We're seeking candidates who can exemplify our values:??
Empathy and Diversity: Integrity, respect, and a commitment to inclusivity.??
Drive: A goal-oriented mindset with pride in exceeding targets.??
Collaboration: A team-focused approach, fostering positive relationships.??
Innovation: Curiosity, creativity, and openness to diverse ideas.??
Flexibility: Adaptability and composure in the face of change.
As a national organisation with clients across the UK, we require flexibility and a willingness to relocate post-remote training. Deployment locations vary and cannot be guaranteed. We encourage applications from diverse backgrounds and experience levels. Eligibility to work in the UK by the start of employment is mandatory.
Why you should apply:?
Our environment is designed to nurture your talents and skills, your hard work and progress are not just appreciated - they're tangibly rewarded. We conduct performance-based reviews every six months, offering you the chance to increase your earning potential twice a year. This regular appraisal system is our way of ensuring that your efforts and achievements are consistently recognised and rewarded.?
We also provide:??
20 days annual leave + bank holidays.??
An extra day off for your birthday.??
Pension.??
Discounted gym membership.??
Eye care.??
Death in service cover.??
Cycle to work scheme.??
Season ticket loan.??
Employee assistance program.??
Yearly budget for personal development.??
Access to alumni and community networks.??
Opportunities to be brand ambassadors.??
Being employed by Sparta Global is an investment in your future that pays dividends along the way. We give you breadth of experience and skills, along with increasing opportunities to develop further and earn more. No two career paths look the same at Sparta.??
Our Recruitment Process:?
Begin your journey via our supportive recruitment process. Apply online and our team will promptly review your application, contacting successful candidates within 48 hours to initiate the next steps. If you pass our initial screening, candidates will proceed to online assessments which vary depending on the opportunity you are applying for. The final stage is a competency interview, here you'll have the opportunity to impress us with your ability to communicate effectively and exhibit behavioural competencies through relevant examples. We're looking for candidates who can demonstrate a collaborative spirit and a growth mindset.??
Your dedicated Talent Team member will be with you every step of the way to support and answer any questions you have. You can also visit our YouTube channel to gain valuable insights and expert advice on virtual interviews, strategies to manage nerves, and tips on nonverbal communication.???
We look forward to receiving your application - good luck!???
Show more
Show less","Python, Programming, Data Pipelines, Data Visualization, Cloud Services, Big Data, Data Analysis, Data Storage, Data Processing, Structured Data, Unstructured Data, Data Organization, Data Accessibility, Data Scientists, Data Analysts, Stakeholders, Collaboration, TeamFocused Approach, Positive Relationships, Adaptability, Composure, Change Management, PerformanceBased Reviews, Earning Potential, Personal Development, Alumni Networks, Community Networks, Brand Ambassadors, Recruitment Process, Online Application, Initial Screening, Online Assessments, Competency Interview, Effective Communication, Behavioural Competencies, Collaborative Spirit, Growth Mindset","python, programming, data pipelines, data visualization, cloud services, big data, data analysis, data storage, data processing, structured data, unstructured data, data organization, data accessibility, data scientists, data analysts, stakeholders, collaboration, teamfocused approach, positive relationships, adaptability, composure, change management, performancebased reviews, earning potential, personal development, alumni networks, community networks, brand ambassadors, recruitment process, online application, initial screening, online assessments, competency interview, effective communication, behavioural competencies, collaborative spirit, growth mindset","adaptability, alumni networks, behavioural competencies, big data, brand ambassadors, change management, cloud services, collaboration, collaborative spirit, community networks, competency interview, composure, data accessibility, data analysts, data organization, data processing, data scientists, data storage, dataanalytics, datapipeline, earning potential, effective communication, growth mindset, initial screening, online application, online assessments, performancebased reviews, personal development, positive relationships, programming, python, recruitment process, stakeholders, structured data, teamfocused approach, unstructured data, visualization"
Data Engineer,BJSS,"Birmingham, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-bjss-2742686756,2023-12-17,Coventry, United Kingdom,Mid senior,Hybrid,"About Us
We’re an award-winning innovative tech consultancy - a team of creative problem solvers. Since 1993 we’ve been finding better, more sustainable ways to solve complex technology problems for some of the world’s leading organisations and delivered solutions that millions of people use every day.
In the last 30 years we won several awards, including a prestigious Queen’s Award for Enterprise in the Innovation category for our Enterprise Agile delivery approach.
Operating from 26 locations across the world, we bring together teams of creative experts with diverse backgrounds and experiences, who enjoy working and learning in our collaborative and open culture and are committed to world-class delivery.
We want to continue to grow our team with people just like you!
We are DataOps advocates and use software engineering best practices to build scalable and re-usable data solutions to help clients use their data to gain insights, drive decisions and deliver business value. Clients don’t engage BJSS to do the straightforward things, they ask us to help on their biggest challenges which means we get to work with a wide range of tools and technologies and there are always new things to learn.
About the Role
BJSS data engineers are specialist software engineers that build, optimise and maintain data applications, systems and services. This role combines the discipline of software engineering with the knowledge and experience of building data solutions in order to deliver business value.
As a BJSS data engineer you’ll help our clients deploy data pipelines and processes in a production-safe manner, using the latest technologies and with a DataOps culture.
You’ll work in a fast moving, agile environment, within multi-disciplinary teams of highly skilled consultants, delivering modern data platforms into large organisations.
You can expect to get involved in variety of projects in the cloud (AWS, Azure, GCP), learning about and using data services such as Databricks, Data Factory, Synapse, Kafka, Redshift, Glue, Athena, BigQuery, S3, Cloud Data Fusion etc.
About You
You're an engineer at heart and enjoy the challenge of building reliable, efficient data applications systems, services and platforms.
You have a good understanding of coding best practices and design patterns and experience with code and data versioning, dependency management, code quality and optimisation, error handling, logging, monitoring, validation and alerting.
You have experience in writing well tested object-oriented Python.
You have experience with using CI/CD tooling to analyse, build, test and deploy your code.
You have a good understanding of design choices for data storage and data processing, with a particular focus on cloud data services.
You have experience in using parallel computing to process large datasets and to optimise computationally intensive tasks.
You have experience in programmatically deploying, scheduling and monitoring components in a workflow.
You have experience in writing complex queries against relational and non-relational data stores.
Some of the Perks
Flexible benefits allowance – you choose how to spend your allowance (additional pension contributions, healthcare, dental and more)
Industry leading health and wellbeing plan - we partner with several wellbeing support functions to cater to each individual's need, including 24/7 GP services, mental health support, and other
Life Assurance (4 x annual salary)
25 days annual leave plus bank holidays
Hybrid working - Our roles are not fully remote as we take pride in the tight knit communities we have created at our local offices. But we offer plenty of flexibility and you can split your time between the office, client site and WFH
Discounts – we have preferred rates from dozens of retail, lifestyle, and utility brands
An industry-leading referral scheme with no limits on the number of referrals
Flexible holiday buy/sell option
Electric vehicle scheme
Training opportunities and incentives – we support professional certifications across engineering and non-engineering roles, including unlimited access to O’Reilly
Giving back – the ability to get involved nationally and regionally with partnerships to get people from diverse backgrounds into tech
You will become part of a squad with people from different areas within the business who will help you grow at BJSS
We have a busy social calendar that you can chose to join– quarterly town halls/squad nights out/weekends away with families included/office get togethers
GymFlex gym membership programme
Please note: any applicants must be able to gain valid SC clearance.
Show more
Show less","Python, DataOps, AWS, Azure, GCP, Databricks, Data Factory, Synapse, Kafka, Redshift, Glue, Athena, BigQuery, S3, Cloud Data Fusion, CI/CD, Relational Databases, NonRelational Databases, Parallel Computing, Workflow Scheduling, Complex Queries, O'Reilly","python, dataops, aws, azure, gcp, databricks, data factory, synapse, kafka, redshift, glue, athena, bigquery, s3, cloud data fusion, cicd, relational databases, nonrelational databases, parallel computing, workflow scheduling, complex queries, oreilly","athena, aws, azure, bigquery, cicd, cloud data fusion, complex queries, data factory, databricks, dataops, gcp, glue, kafka, nonrelational databases, oreilly, parallel computing, python, redshift, relational databases, s3, synapse, workflow scheduling"
Data Engineer,NFU Mutual,"Stratford-upon-Avon, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-nfu-mutual-3766639613,2023-12-17,Coventry, United Kingdom,Mid senior,Hybrid,"An exciting prospect to join a supportive and inclusive Department that offers exceptional career potential
A business area that has seen substantial investment in the people, process and technology to deliver a step change in Underwriting performance and Pricing agility
Salary up to £60,000 depending on experience + up to 17.5% annual bonus
Hybrid homeworking options available
About the role
This Data Consultant role sits as part of a team dedicated to supporting NFU Mutual’s largest strategic programme, to replace the core Policy Administration System.
This long-term programme will require Data Professionals to support product, underwriting and pricing activity. This will involve extracting and manipulating data from existing systems, exploiting new external data sources and acting as a key stakeholder for the design and implementation of NFU Mutual’s future data platforms and reporting capability. As a technical leader, you'll be leading the data engineering and delivery activities whilst managing and supporting a team of Data Analysts in delivering their goals.
You'll work collaboratively across a range of stakeholders, including data suppliers, IT, Business Analysts, Architects and Implementation Partners. Key responsibilities will include;
Building and managing projects to develop the data infrastructure for Pricing & Underwriting
Efficient utilisation of the full ‘SQL stack’ to serve up ‘insight-ready’ data for a broad variety of needs
Manipulating large datasets and preparing complex system data for delivery to colleagues in a simplified manner
Interpreting the business requirements of our colleagues across Pricing and Underwriting
Monitoring and reporting on data/data trends, making recommendations and managing the agreed activity to deliver consistent improvements of data quality and integrity
Implementation of new data sources supplied by external parties or built from existing internal data stores
This role is based in our Tiddington Head Office, just outside Stratford-Upon-Avon. However, we understand how important a positive work-life balance is, so to help you give your best, we offer great facilities when you want to be in an office environment and support to work up to 80% of your hours from home.
About you
You'll be an analytically-minded data engineering professional with experience of managing and engaging technical teams to successfully deliver on objectives. Having led teams before, you'll have experience of defining what best practice looks like and embedding this effectively. It will be beneficial if you also have experience of delivering technical training to your team.
It's important that you're able to demonstrate strong stakeholder management and communication skills along with your ability to think critically, defining solutions and driving these forwards.
Other technical skills we are looking for include;
SQL Stack: SSMS, T-SQL, SSIS, SSAS (multi-dimensional & tabular)
Data quality/profiling knowledge/experience
MS Azure experience would be an advantage for likely future projects
PowerBI (or experience of preparing data sets for consumption by similar reporting & visualisation tools)
Knowledge of General Insurance Pricing practices would be advantageous, however this is not essential. Your technical data and people-focussed skills are what's important, and we'll provide you with all the tools and support you need, including professional qualifications, to build your industry knowledge.
Benefits and rewards
When you join our team, you can expect a supportive culture and an attractive range of rewards and benefits including:
Salary up to £60,000 depending on experience
Annual bonus (up to 17.5% of salary)
Contributory pension scheme, up to 20%, including your 8% contribution
25 days annual leave + bank holidays + buy/sell/save holiday trading scheme
A Family Friendly policy that helps you balance your work and family responsibilities
Access to savings at High Street brands, travel and supermarkets
£20 contribution to a monthly gym membership – subject to T&Cs
Health and wellbeing plan - cashback for dentist, opticians, physio and more
Access to voluntary benefits, including health assessments, private medical insurance and dental insurance
Employee Volunteering - volunteer in the community for one day each year
Unlimited access to Refer a Friend £500 bonus scheme
Life Assurance cover of 4 x salary
Employee discounts of 15% on a range of NFU Mutual insurance policies.
Show more
Show less","SQL, SSMS, TSQL, SSIS, SSAS, Data quality, Data profiling, MS Azure, PowerBI, General Insurance Pricing","sql, ssms, tsql, ssis, ssas, data quality, data profiling, ms azure, powerbi, general insurance pricing","data profiling, data quality, general insurance pricing, ms azure, powerbi, sql, ssas, ssis, ssms, tsql"
Senior Data Engineer,ADLIB Recruitment | B Corp™,"Birmingham, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-at-adlib-recruitment-b-corp%E2%84%A2-3747054392,2023-12-17,Coventry, United Kingdom,Mid senior,Hybrid,"Join a high performing team supporting BI and data science functions.
Work with leading cloud and data technologies to shape the data landscape.
Lots of growth opportunities, with the team set to double in size.
Senior Data Engineer
Industry Leader in Insurance
We’re on the lookout for a seasoned Senior Data Engineer to join a leading figure in the insurance domain.
Be an influential force by sculpting the data infrastructure in an organisation where innovation is the focus. Your contribution will be pivotal in propelling them toward their goal of offering unparalleled solutions, all while harnessing the most advanced technologies in cloud-based and open-source data engineering.
What You’ll Be Doing
In your role as a Senior Data Engineer, your key responsibilities will be to:
Engineer, enhance and safeguard sophisticated data architectures, including databases and vast processing systems.
Construct intricate data streams to bolster analytical capabilities.
Construct efficient data frameworks for the extraction, transformation and loading of data.
Offer crucial data and analytic support to facilitate superior decision-making.
Embrace and drive automation.
With the recent integration of Snowflake, their ambition extends beyond data management; they’re set on harnessing data to innovate, evolve solutions and advance their mission.
What Experience You’ll Need To Apply
A rich background in Data Engineering.
Profound expertise in programming (Python, Java, etc.), SQL/NoSQL databases.
Skilled in data warehousing concepts.
Familiarity with data pipeline and workflow management systems such as Azkaban, Luigi, Airflow.
Experienced in utilizing cloud services.
Exceptional communicative clarity, especially in translating complex technical concepts to a non-technical audience.
What You’ll Get In Return For Your Experience
For your valuable experience, you’ll be rewarded with a base salary of up to £85,000 plus bonus and benefits. They also believe in a flexible work-life balance, offering a largely remote working model (1 day bi-weekly). For your career development, you’ll have access to top-tier learning and training programs, a variety of benefits, and exciting opportunities for growth.
If you value a culture that encourages individual input while achieving collective results, this is the place for you.
What’s next?
Feel like you’re the one? Click ‘Apply’ and let us know why you are the perfect fit for this role.
Show more
Show less","Data Engineering, Python, Java, SQL, NoSQL, Data Warehousing, Data Pipeline, Workflow Management, Azkaban, Luigi, Airflow, Cloud Services, Snowflake","data engineering, python, java, sql, nosql, data warehousing, data pipeline, workflow management, azkaban, luigi, airflow, cloud services, snowflake","airflow, azkaban, cloud services, data engineering, data pipeline, datawarehouse, java, luigi, nosql, python, snowflake, sql, workflow management"
Senior Data Analyst,be:technology,"Birmingham, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-analyst-at-be-technology-3770129296,2023-12-17,Coventry, United Kingdom,Mid senior,Hybrid,"Job Opportunity
: Senior Data Analyst
Location:
Worcester/Hybrid (2x a week in the office)
Salary:
£40-50k
Benefits
Monthly subscription boxes
Book subscription
Vitality health insurance
Pay review targets
Annual inflation based pay rise
Are you a data enthusiast looking for a career-defining opportunity? We're seeking a talented individual to join our client as their inaugural Data Analyst. Be part of the company's rapid expansion and play a pivotal role in shaping their data strategy from the ground up.
Why Join Us:
As our first Data Analyst, you'll not only analyse CRM and Marketing data but also have full autonomy to set the stage for future data-driven successes. With a commitment to internal growth, this role offers a clear path to leadership as they continue to expand and add to the team.
Key Requirements
3+ years’ experience
Power BI
SQL
Advanced Excel
FULL UK DRIVING LICENSE
Python (desirable)
Machine Learning (desirable)
Roles & Responsibilities
Data Collection & Cleaning
– Ensure CRM data is accurate, complete and consistent.
Data Analysis & Reporting
– Analyse CRM data & generate reports that provide insights into customer behaviour, sales performance and other key metrics.
Segmentation
– Segment customers based on demographics, purchase history and other factors to target customers with more relevant marketing campaigns and offers.
Predictive Analysis
– Forecast customer churn, identify potential customers and predict future sales
Insights –
Offer insights on how to improve the CRM strategy, identify new opportunities and make better business decisions to positively impact the company’s strategy.
Ready to shape the data landscape of a growing organisation? Send your CV and apply today to be our client’s first Data Analyst and kickstart your journey toward leadership!
Show more
Show less","Data Analysis, Data Collection, Data Reporting, Data Cleaning, Segmentation, Predictive Analysis, Machine Learning, Python, Power BI, SQL, Advanced Excel, CRM, Marketing Analytics","data analysis, data collection, data reporting, data cleaning, segmentation, predictive analysis, machine learning, python, power bi, sql, advanced excel, crm, marketing analytics","advanced excel, crm, data cleaning, data collection, data reporting, dataanalytics, machine learning, marketing analytics, powerbi, predictive analysis, python, segmentation, sql"
"Senior Data Engineer - Hybrid - Up to GBP65,000",Nigel Frank International,"Birmingham, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-hybrid-up-to-gbp65-000-at-nigel-frank-international-3734508036,2023-12-17,Coventry, United Kingdom,Mid senior,Hybrid,"Senior Data Engineer - Hybrid - Up to £65,000
I'm currently working with a market leading manufacturer of specialist medical equipment who are working with groundbreaking technology to deliver data to health care professionals. They are looking for a Senior Data Engineer to join at the start of an exciting greenfield project where you will be responsible for the data migration to their brand new azure data platform.
In this role you will design, maintain and optimise on premise database solutions. You will be developing and maintaining ETL pipelines. You will also take lead of the migration of data to the new Azure Data Platform. This role will involve working with other specialists within the team to make technical decisions that will benefit the overall business.
This is a salaried position of up to £65,000 depending on experience plus a company benefits package. They pride themselves on being an employee orientated business and as such, they strongly believe in providing a healthy work life balance. This role offers hybrid working with occasional travel to their Stafford office on average of 4 times per month.
I am looking for...
Strong experience with Databricks for data ingestion and transformation
Experience working with the Azure Stack - Data factory, Synapse, Data Lake
Experience designing and implementing ETL solutions
Strong coding experience with coding languages such as Python or C#
This is a unique opportunity for a Senior Data Engineer to work on an exciting greenfield project and be supported in achieving you career goals and aspirations.
If this is of interest then get in touch ASAP. Send across your CV to d.moore1@nigelfrank.com or alternatively, give me a call on 0191 338 7577
Key Skills: Microsoft, Azure, Databricks, Data Factory, Data Lake, SQL, Python, C#, ETL, Engineer, Engineering
Show more
Show less","Azure Stack, Apache Spark, Data Factory, Apache Synapse, Data Lake, MS SQL, Python, C#, ETL, Databricks","azure stack, apache spark, data factory, apache synapse, data lake, ms sql, python, c, etl, databricks","apache spark, apache synapse, azure stack, c, data factory, data lake, databricks, etl, ms sql, python"
Finance Data Analyst,The Consultancy Group (London),"Birmingham, England, United Kingdom",https://uk.linkedin.com/jobs/view/finance-data-analyst-at-the-consultancy-group-london-3782036066,2023-12-17,Coventry, United Kingdom,Mid senior,Hybrid,"The Consultancy Group are partnered with a global hospitality client who is looking for a highly skilled and qualified Finance Data Analyst to join their growing team. This individual will play a crucial role in driving performance improvements through insightful data analysis and actionable recommendations.
Responsibilities:
Utilise your finance expertise to analyse large and complex datasets, extracting meaningful insights that contribute to strategic decision-making.
Collaborate with cross-functional teams to understand business requirements and translate them into effective data analysis strategies.
Develop and maintain dashboards and reports to monitor key financial metrics, providing regular updates to stakeholders.
Conduct in-depth data mining and analysis, identifying trends, patterns, and opportunities for process optimisation and improvement.
Qualifications:
Proven experience as a Finance Data Analyst, demonstrating a track record of delivering tangible improvements in business performance.
Ability to work with large, complex datasets and derive actionable insights and build a BI Suite with the use of Power BI for reporting and Excel
Excellent communication skills with the ability to articulate complex data findings in a clear and concise manner to non-technical stakeholders.
Agile and dynamic mindset, able to adapt quickly to changing priorities and business needs.
Professional qualification in ACA, ACCA, or CIMA
Show more
Show less","Finance Data Analysis, Power BI, Excel, Business Intelligence, Data Mining, Data Analysis, Process Optimization, Business Performance Improvement, Agile Mindset, Financial Metrics, Data Visualization, DataDriven Decision Making, CrossFunctional Collaboration, ACA, ACCA, CIMA","finance data analysis, power bi, excel, business intelligence, data mining, data analysis, process optimization, business performance improvement, agile mindset, financial metrics, data visualization, datadriven decision making, crossfunctional collaboration, aca, acca, cima","aca, acca, agile mindset, business intelligence, business performance improvement, cima, crossfunctional collaboration, data mining, dataanalytics, datadriven decision making, excel, finance data analysis, financial metrics, powerbi, process optimization, visualization"
"Senior Data Engineer - Hybrid - Up to GBP65,000",Nigel Frank International,"Lichfield, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-hybrid-up-to-gbp65-000-at-nigel-frank-international-3734504545,2023-12-17,Coventry, United Kingdom,Mid senior,Hybrid,"Senior Data Engineer - Hybrid - Up to £65,000
I'm currently working with a market leading manufacturer of specialist medical equipment who are working with groundbreaking technology to deliver data to health care professionals. They are looking for a Senior Data Engineer to join at the start of an exciting greenfield project where you will be responsible for the data migration to their brand new azure data platform.
In this role you will design, maintain and optimise on premise database solutions. You will be developing and maintaining ETL pipelines. You will also take lead of the migration of data to the new Azure Data Platform. This role will involve working with other specialists within the team to make technical decisions that will benefit the overall business.
This is a salaried position of up to £65,000 depending on experience plus a company benefits package. They pride themselves on being an employee orientated business and as such, they strongly believe in providing a healthy work life balance. This role offers hybrid working with occasional travel to their Stafford office on average of 4 times per month.
I am looking for...
Strong experience with Databricks for data ingestion and transformation
Experience working with the Azure Stack - Data factory, Synapse, Data Lake
Experience designing and implementing ETL solutions
Strong coding experience with coding languages such as Python or C#
This is a unique opportunity for a Senior Data Engineer to work on an exciting greenfield project and be supported in achieving you career goals and aspirations.
If this is of interest then get in touch ASAP. Send across your CV to d.moore1@nigelfrank.com or alternatively, give me a call on 0191 338 7577
Key Skills: Microsoft, Azure, Databricks, Data Factory, Data Lake, SQL, Python, C#, ETL, Engineer, Engineering
Show more
Show less","Azure Stack, Azure Data Factory, Azure Synapse, Azure Data Lake, SQL, Python, C#, ETL, Data Migration, Data Transformation, Data Ingestion, Databricks","azure stack, azure data factory, azure synapse, azure data lake, sql, python, c, etl, data migration, data transformation, data ingestion, databricks","azure data factory, azure data lake, azure stack, azure synapse, c, data ingestion, data migration, data transformation, databricks, etl, python, sql"
Senior Data Engineer - up to GBP75k,Nigel Frank International,"Birmingham, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-up-to-gbp75k-at-nigel-frank-international-3728581670,2023-12-17,Coventry, United Kingdom,Mid senior,Hybrid,"Are you a Data Engineer with a passion for Databricks?
My client is a leader in the legal industry and they are looking to grow their data team. They offer an amazing opportunity for a Senior Data Engineer, with heavy focus on Databricks. Within this role, you will join an established, small team of data engineers and you will be working alongside the reporting team too.
You will be developing pipelines with Databricks on premises and you will be using Azure Data Factory for orchestration too. Also, you will be supporting internal business apps as they are moving into new bespoke applications which relies on Databricks.
There is great room for progression within this role, and support and training will be provided as well.
Experience required:
Very strong experience with Databricks and SQL
Strong experience with Python / PySpark
Azure Data Factory
Excellent communication skills
This is a permanent position and pays between £60,000 and £70,000, depending on experience and the strength of your skillset.
Benefits
Well-being fund
Hybrid working (1-2x/month in London)
Great pension package
Private medical health insurance
Support and training provided
If this sounds like you, then apply NOW as my client is hiring ASAP and you don't want to miss this opportunity!
To discuss in more detail, please send your CV s.fokas@nigelfrank.com or alternatively, call Spiros Fokas on 191 243 5445
NOTE: This role is for UK Residents only and does not offer sponsorship.
Nigel Frank International is the leading Microsoft Business Intelligence recruitment firm in the UK, advertising more Business Intelligence jobs than any other agency. We deal with both Microsoft Partners & End Users throughout the UK and Europe and we have never had more live requirements jobs for Microsoft Business Intelligence professionals. By specialising solely in placing candidates in the market I have built relationships with key employers in the UK and have an unrivalled understanding of where the best opportunities & Business Intelligence jobs are.
Show more
Show less","Databricks, SQL, Python, PySpark, Azure Data Factory","databricks, sql, python, pyspark, azure data factory","azure data factory, databricks, python, spark, sql"
Data Engineer,Talent,"West Midlands, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-talent-3766019776,2023-12-17,Coventry, United Kingdom,Mid senior,Hybrid,"6 month contract with a critical public service client
Hybrid working, West Midlands base
Outside IR35 rate
Data Engineer
** MUST HAVE active UK Security Clearance **
In the midst of a digital transformation programme, our critical public service client is in need of an experienced Data Engineer to support a major project where they’re building a modern, strategic data platform to facilitate the generation of insight from within the organisation’s data.
Your Responsibility
Knowledge transfer – sharing your knowledge with the incumbent team to ensure they are knowledgeable on what is required to support any solution
Conducting technical assessments of the current environment to support and suggest a successful roadmap for future implementation
Hands on development of Data Pipelines and facilitating access to identified datasets
Supporting the development of the Master/Common data model and the mapping of data sources to this
Hands on development of data storage solutions and facilitating access to this data via the analytical toolsets
Supporting the development of Power BI
** MUST HAVE active UK Security Clearance **
If you have the relevant skills and experience, please apply below and I will endeavour to get back to you.
Show more
Show less","Data Engineering, Data Pipelines, Data Storage Solutions, Power BI, Data Modeling, Dataset Mapping, Technical Assessments, Knowledge Transfer","data engineering, data pipelines, data storage solutions, power bi, data modeling, dataset mapping, technical assessments, knowledge transfer","data engineering, data storage solutions, datamodeling, datapipeline, dataset mapping, knowledge transfer, powerbi, technical assessments"
Data Analyst,Gleeson Recruitment Group,"Birmingham, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-analyst-at-gleeson-recruitment-group-3782026951,2023-12-17,Coventry, United Kingdom,Mid senior,Hybrid,"Data Analyst - (Debts & Collections)
Birmingham based - Hybrid working. £40K
Must have experience with SQL / BI tool (power bi/qlik/tableau or similar)
Must have experience working in a debts team / environment.
Gleeson Recruitment Group are working with a leading Utilities organisation based in Birmingham who are looking for a Data Analyst to join their Debts & Collections team.
Working in a small specialist team but focused on collaborative working with departments across the business, this role recognises the driver to further exploit our data and improve our analysis capability. You will identify opportunities to drive change and process improvements finding solutions to technically challenging issues. You will lead on collating relevant data, logging any defects to help to deliver changes to functionality and processes.
You will need to translate the data, come up with ways on representing it and then build that into visuals.
Must have experience working in a similar environment in IT analytical role as well as understanding customers.
Experience working in Utilities would be a bonus.
Please apply asap for more information.
Show more
Show less","SQL, Power BI, Qlik, Tableau, Utilities, Data Analysis, Data Visualization, IT Analytics","sql, power bi, qlik, tableau, utilities, data analysis, data visualization, it analytics","dataanalytics, it analytics, powerbi, qlik, sql, tableau, utilities, visualization"
Database Engineer,Leidos,"Melbourne, Victoria, Australia",https://au.linkedin.com/jobs/view/database-engineer-at-leidos-3784214005,2023-12-17,Victoria, Australia,Mid senior,Onsite,"Company Description
At Leidos, we do work that really matters inspired by our mission to make the world safer, healthier, and more efficient through technology, engineering, and science. With 25 years of local experience, our over 2000 team members, work together to solve Australia’s toughest challenges in government, defence, intelligence and border protection. We’re robust and ambitious, and we empower our people to do their best work. You’ll feel inspired by what you can achieve and will be supported by an inclusive and flexible culture that genuinely cares for your wellbeing. Together, we can be the difference.
We've got so much to offer at Leidos, here are a just a few of the
Benefits
we provide our team:
12 Extra Days Leave: Life Days are the Leidos way of recognising that we all need some extra time out to take care of life. By working slightly more than the minimum weekly hours (2 hours per week for full timers) you can accrue up to an extra 12 days of leave per year.
Leidos Life Hub provides access to discount offers or cashback rewards with over 400 Australian and International retailers.
When you grow, we grow. Be it through our exciting pipeline line of projects or by partnering with a People Leader who will guide, mentor and support you.
Job Description
Leidos Australia have a permanent full-time or contract opportunity for a Database Engineer (Postgres on REHL) to join our team in a role that offers great flexible work arrangements including remote work. Reporting to the Program Technical Lead and working within our team responsible for the implementation, support and maintenance of a large enterprise environment you will be responsible for supporting the customer platform, tuning system performance, installing system wide software, and supporting database hardware and software.
As a talented member of our multidisciplinary teams, you will rapidly grow and progress with us across a diverse and influential Federal Government and Defence portfolio. This is a rare opportunity to join the team who gets to influence programs today that will redefine the customer’s business tomorrow.
In this role you will perform the following functions as individual assignments or as part of a team:
Perform Level 2 and 3 platform support including management and administration of PostgresSQL database products on RedHat;
Implement and test database upgrades;
Design and implement data migration activities;
Maintain database design and implementation documentation;
Implement organisational objectives for database improvement and compliance with industry standards;
Provide solutions to a variety of technical problems of moderate scope and complexity
Maintaining currency of the environment, where possible.
Develops and configures tools to enable automation of database administration tasks.
Source and analyse relevant subject matter documentation
Contribute to IT Support process and policy development
Providing input into policies, procedures, and SOP’s relating to maintenance and management of the operational database infrastructure. Interact with users and technical subject matter experts to ensure delivery is fit for purpose.
Produce enterprise database designs and make recommendations for upgrades, designs and implementations will be advantage.
Qualifications
About You And What You'll Bring
Coupled with your education and practical experience, you will demonstrate a pro-active approach with the ability to understand the business, identify issues and develop relationships to achieve the company’s objectives. You will also have:
Substantial professional experience working in a role within an engineering or project environment (preferably in a similar industry)
Strong Skills/Experience with PostgreSQL is required.
Knowledge and application of a broad range of skills including software, systems and enterprise architecture, software and hardware engineering, interface protocols
Substantial experience working in a large enterprise environments
Skills
Strong individual contributor and team player in a professional environment
Highly effective in managing multiple, concurrent activities, while understanding and managing priorities, dependencies and risk
Excellent communication (verbal and written) and interpersonal skills
Good initiative in developing and improving systems and procedures
Ability to independently develop and determine approaches to solutions in a practical and creative manner
Strong conceptual, analytical and interpretive skills with high level of attention to details
Strong focus on customer service
Strong planning and organising skills
Experience with MS SQL and other database technologies are highly regarded.
Experience with Ansible or infrastructure as code preferred.
Experience with Liquibase highly regarded.
This role does require the successful applicant to be an Australian Citizen hold and maintain or be willing to undergo a Baseline Security Clearance.
Additional Information
We’ve been keeping Australia safer, healthier and more efficient for 25 years, here’s just three ways how
We’re building the next generation secret end user environment for the Australian Department of Defence across a number of strategic locations around Australia.
We deliver software development and operational support through to the integration of underwater autonomous vehicles and survey ships to meet mission objectives.
We support key networks for the Australian Cyber Security Centre.
Applicants may also need to meet International Traffic in Arms Regulations (ITAR) requirements. In certain circumstances this can place limitations on persons who hold dual nationality, permanent residency or are former nationals of certain countries as per ITAR 126.1.
At Leidos, we embrace diversity and are committed to creating a truly inclusive workplace. We welcome and encourage applications from Aboriginal and Torres Strait Islanders, culturally and linguistically diverse people, people with disabilities, veterans, neurodiverse people, and people of all genders, sexualities and age groups.
Show more
Show less","Postgres, Oracle SQL, Ansible, Liquibase, Software engineering, Systems architecture, Hardware engineering, Interface protocols, Docker, Data migration, Database maintenance, Code automation, Python, Linux, Microsoft SQL Server, Infrastructure as code, Testing","postgres, oracle sql, ansible, liquibase, software engineering, systems architecture, hardware engineering, interface protocols, docker, data migration, database maintenance, code automation, python, linux, microsoft sql server, infrastructure as code, testing","ansible, code automation, data migration, database maintenance, docker, hardware engineering, infrastructure as code, interface protocols, linux, liquibase, microsoft sql server, oracle sql, postgres, python, software engineering, systems architecture, testing"
Staff Data Engineer,SIPE Education,"Melbourne, Victoria, Australia",https://au.linkedin.com/jobs/view/staff-data-engineer-at-sipe-education-3757948278,2023-12-17,Victoria, Australia,Mid senior,Onsite,"The Role
Linktree’s Data Platform team is at the forefront of transforming the way Linktree leverages data, working closely with the insights, analytics engineering, product and marketing teams to become truly data driven.
As a Staff Data Engineer at Linktree, you will be the driving force behind scaling how data is consumed at Linktree. Not only will you be modeling some of our most important data sets to gain insights, such as the data that helps us understand the driving factors behind the success of our product or our revenue, we take self-service one step further. You will also build the platform that can be used by Data Analysts, Marketers, Engineers and anyone else who is interested to model their own data to fit their specific needs and not be dependent on a data team.
This is your opportunity to make an impact at Linktree and push the boundaries of what is possible with data self-service!
Location Expectations:
Hybrid. We're growing our team in LA and the Bay Area, and plan to have offices in both locations. We expect team members to come into their respective office 2x/week.
What You Will Do
Develop and implement the strategy for the Data Platform in close collaboration with other data and engineering leaders.
Continue leading the transformation to make data a first-class citizen in software development at Linktree. This includes providing other teams with the self-service capabilities to ingest, transform, consume and integrate data with other systems for both production and analytics use-cases.
You will be building and improving data-driven systems including, but not limited to, instrumentation at the source, event ingestion, event brokers, data warehouses, pipelines and integration systems used to activate data in third parties, such as an experimentation platform, product analytics and marketing tooling.
Ensure our data platform continues to scale with our rapidly growing user base and ever-evolving product. Linktree’s growth constantly creates new challenges like needing to master data governance and discoverability with many different teams producing data, or hitting the scalability limits of cloud providers.
What We Are Looking For
A platform mindset. Our Data Platform team does not do repetitive data transformation and integration jobs for others. The team builds the infrastructure and tools that allows Linktree to perform these tasks at scale by enabling all teams to perform data-related jobs themselves.
Data as well as a software engineering background. Our data platform spans from the SDKs that our teams use to emit and consume data in front- and back-end applications to the integration with various third-party systems, including everything in between (data ingestion, event brokers, data persistence, etc.). To be successful in your role you’ll need to be proficient in both. Having worked with distributed, event-driven systems is a plus.
Experience operating at scale. You have worked on data systems that power a product that serves hundreds of millions of active users.
Linktree is committed to providing a competitive compensation package. Our cash compensation amount for this role is targeted at $175,000-$225,000 in the San Francisco Bay or Los Angeles area. Final offer amounts are determined by multiple factors including candidate expertise, the scope of role and level, and may vary from the amounts listed above.
P.S. If you don’t tick every box in this ad, please don’t rule yourself out. We take pride in inclusion and hiring incredible human beings with great potential over ticking boxes – so if this role resonates with you, hit that apply button!
Where And How We Work
We are a global and diverse group offering a truly flexible and family friendly work environment. Kids, pets, and the occasional delivery person are all actively encouraged to appear on our Zoom screens. All of us at Linktree work either fully remote or a hybrid ""remote, but in-office sometimes"" approach.
We currently have offices in Melbourne, Sydney and LA, but our team is spread across Australia, United States, and New Zealand. As our team approaches 200 people, our company will be 10x the size we were in 2020.
We offer autonomy and flexibility in how you structure your days and weeks. There will be the need for some collaboration outside of a ""normal"" 9-5 being a global company, but we aim to work asynchronously where possible.
Our Culture And Benefits
Linktree's company culture and values are based around collaboration, diversity, inclusion, and flexibility. Those are all nice words but to give you some more specific examples:
We recognize that our team are individually unique and have designed our benefits with this in mind. Each person has an annual allowance ($7,000 AUD / $4,900 USD) to use on things like (but not limited to) fitness memberships, development courses, childcare, travel, charitable donations, pet insurance, home office set up - the choice is yours!
We provide top-flight medical, dental, vision, disability and life insurance - we cover 100% of your monthly premiums (and 80% for your dependents).
Employee Stock Option Program - we want each and every employee to share in the company’s success as we go further together.
To learn more about our benefits, including our parental leave program, volunteering leave, DE&I initiatives, and more, click here!
Our Story
We're on a mission to empower anyone to curate and grow their digital universe. We created the ""link in bio"" category and are trusted by some of the world's biggest brands and celebrities including TikTok, The UN Environmental Program, The White House, F1, Manchester United, Selena Gomez, Alicia Keys, and Dwayne “The Rock” Johnson. With a flexible work environment and a team spread across multiple time zones, we offer autonomy and flexibility. Join us in empowering people to control their online presence!
At Linktree, we celebrate and support everyone’s perspective and background, and we’re proud to be an equal opportunity workplace. We aim to foster a diverse and inclusive environment where all team members have a sense of belonging, because we believe in
going further together.
Linktree welcomes all people regardless of sex, gender identity, race, ethnicity, disability, pregnancy, age, or other lived experience. If you require accommodations to fully participate in our opportunities, please don't hesitate to reach us at
recruiting@linktr.ee
– your needs are important to us.
Show more
Show less","Data Engineering, Software Engineering, Data Analytics, Data Warehousing, Data Pipelines, Data Science, Data Visualization, Data Governance, Data Discoverability, Cloud Computing, Distributed Systems, EventDriven Systems, Scalability, Performance Tuning, Python, Java, R, SQL, NoSQL, Apache Spark, Kafka, Airflow, Tableau, Power BI","data engineering, software engineering, data analytics, data warehousing, data pipelines, data science, data visualization, data governance, data discoverability, cloud computing, distributed systems, eventdriven systems, scalability, performance tuning, python, java, r, sql, nosql, apache spark, kafka, airflow, tableau, power bi","airflow, apache spark, cloud computing, data discoverability, data engineering, data governance, data science, dataanalytics, datapipeline, datawarehouse, distributed systems, eventdriven systems, java, kafka, nosql, performance tuning, powerbi, python, r, scalability, software engineering, sql, tableau, visualization"
*Data Engineer - Cloud Tech*,Accenture Australia,"Melbourne, Victoria, Australia",https://au.linkedin.com/jobs/view/data-engineer-cloud-tech-at-accenture-australia-3775612147,2023-12-17,Victoria, Australia,Mid senior,Onsite,"Accenture
is a global professional services company with leading capabilities in digital, cloud and security. Find out more about us at accenture.com.
Great humans needed. Combine the best of your ingenuity and Cloud technologies to build a better world and lead clients towards change. Join us and do extraordinary things.
About you:
As a Cloud Data Engineer, you will design and maintain data analytic road maps and data structures that support business and technology objectives. Naturally inquisitive and open to exploration of underlying data, finding valuable insights and work with functional areas to drive identified actions. You enjoy working both autonomously and as part of a team and have the confidence to influence and communicate with stakeholders at all levels, and to work in a fast-paced complex environment with conflicting priorities.
About the role:
You will be working on client projects with teams from across our Modern Data Platform and Applied Intelligence practice alongside our industry, functional and technology SMEs on some of our clients most challenging projects. Reporting to the delivery lead, you will deliver digestible, contemporary and immediate data content to support and drive business decisions. The key focus of the role is to deliver reports, dashboards and custom solutions for various business critical requirements. You will be involved in all aspects of data engineering from delivery planning, estimating and analysis, all the way through to data architecture and pipeline design, delivery and production implementation.
From day one, you will be involved in the design and implementation of complex data solutions ranging from batch to streaming and event-driven architectures, across cloud, on-premise and hybrid client technology landscapes. We are looking for 3+ years of experience in data engineering in a customer or business facing capacity and experience in the following:
Preferred
Ability to understand and articulate requirements to technical and non-technical audiences
Stakeholder management and communication skills, including prioritising, problem solving and interpersonal relationship building
Strong experience in SDLC delivery, including waterfall, hybrid and Agile methodologies. Experience delivering in an agile environment,
Experience of implementing and delivering data solutions and pipelines on cloud platforms such as AWS, Azure and GCP
A strong understanding of data modelling, data structures, databases, and ETL processes
An in-depth understanding of large-scale data sets, including both structured and unstructured data
Strong SQL and/or Python knowledge
Knowledge and experience of delivering CI/CD and DevOps capabilities in a data environment
Desirable
Previous experience with high-scale or distributed RDBMS (Teradata, Netezza, Greenplum, Aster Data, Vertica), including metadata management, SQL development and performance tuning within these database environments.
Experience in addressing and implementing data quality solutions, data reconciliation framework and strong understanding and experience in building data lineage documentation, and Data Catalogues.
Experience with related/complementary open source software platforms and languages (e.g. Java, Linux, Apache, Perl/Python/PHP, Chef)
Qualification:
Computer Science, Software Engineering, or a related field
Join Us
If you are inspired to make a positive difference where you will be helping translate data to insights that enable the businesses to make key decisions as well as using visualisations to tell a compelling story with data to simplify complex topics to be understandable, please submit your resume for us to connect and get to know you.
Salary range for this role: $80K to $140K
Equal Employment Opportunity Statement for Australia:
At Accenture, we recognise that our people are multi-dimensional, and we create a work environment where all people feel like they can bring their authentic selves to work, every day.
Our unwavering commitment to inclusion and diversity unleashes innovation and creates a culture where everyone feels they have equal opportunity. Our range of progressive policies support flexibility in ‘where’, ‘when’ and ‘how’ our people work to ensure that Accenture is an organisation where you can strive for more, achieve great things and maintain the balance and wellbeing you need.
We encourage applications from all people, and we are committed to removing barriers to the recruitment process and employee lifecycle. All employment decisions shall be made without regard to age, disability status, ethnicity, gender, gender identity or expression, religion or sexual orientation and we do not tolerate discrimination. If you require adjustments to the recruitment process or have a preferred communication method, please email exectalent@accenture.com and cite the relevant Job Number, or contact us on +61 2 9005 5000.
To ensure our workplace is inclusive and diverse we are setting bold goals and taking comprehensive action. To achieve these goals, we collect information that allows us to track the effectiveness of our Inclusion and Diversity programs. Learn how Accenture protects your personal data and know your rights in relation to your personal data. Read more about our Privacy Statement.
n/a
Show more
Show less","Data Engineering, Data Analytics, Cloud Computing, AWS, Azure, GCP, Data Structures, Databases, Data Modeling, ETL, SQL, Python, DevOps, CI/CD, Teradata, Netezza, Greenplum, Aster Data, Vertica, SQL, Java, Linux, Apache, Unix, Perl, PHP, Chef","data engineering, data analytics, cloud computing, aws, azure, gcp, data structures, databases, data modeling, etl, sql, python, devops, cicd, teradata, netezza, greenplum, aster data, vertica, sql, java, linux, apache, unix, perl, php, chef","apache, aster data, aws, azure, chef, cicd, cloud computing, data engineering, data structures, dataanalytics, databases, datamodeling, devops, etl, gcp, greenplum, java, linux, netezza, perl, php, python, sql, teradata, unix, vertica"
Staff Data Engineer,Linktree,"Melbourne, Victoria, Australia",https://au.linkedin.com/jobs/view/staff-data-engineer-at-linktree-3731218098,2023-12-17,Victoria, Australia,Mid senior,Onsite,"The Role
Linktree’s Data Platform team is at the forefront of transforming the way Linktree leverages data, working closely with the insights, analytics engineering, product and marketing teams to become truly data driven.
As a Staff Data Engineer at Linktree, you will be the driving force behind scaling how data is consumed at Linktree. Not only will you be modeling some of our most important data sets to gain insights, such as the data that helps us understand the driving factors behind the success of our product or our revenue, we take self-service one step further. You will also build the platform that can be used by Data Analysts, Marketers, Engineers and anyone else who is interested to model their own data to fit their specific needs and not be dependent on a data team.
This is your opportunity to make an impact at Linktree and push the boundaries of what is possible with data self-service!
Location Expectations:
Hybrid. We're growing our team in LA and the Bay Area, and plan to have offices in both locations. We expect team members to come into their respective office 2x/week.
What You Will Do
Develop and implement the strategy for the Data Platform in close collaboration with other data and engineering leaders.
Continue leading the transformation to make data a first-class citizen in software development at Linktree. This includes providing other teams with the self-service capabilities to ingest, transform, consume and integrate data with other systems for both production and analytics use-cases.
You will be building and improving data-driven systems including, but not limited to, instrumentation at the source, event ingestion, event brokers, data warehouses, pipelines and integration systems used to activate data in third parties, such as an experimentation platform, product analytics and marketing tooling.
Ensure our data platform continues to scale with our rapidly growing user base and ever-evolving product. Linktree’s growth constantly creates new challenges like needing to master data governance and discoverability with many different teams producing data, or hitting the scalability limits of cloud providers.
What We Are Looking For
A platform mindset. Our Data Platform team does not do repetitive data transformation and integration jobs for others. The team builds the infrastructure and tools that allows Linktree to perform these tasks at scale by enabling all teams to perform data-related jobs themselves.
Data as well as a software engineering backpment tools, big data concepts, open source concepts, data streaming concepts, devops tools, git, jenkins, cicd, jira, agile","agile, big data concepts, cicd, cloud data engineering, cloudbased data warehousing, data lake solutions, data streaming concepts, data structure, dataanalytics, database design, datapipeline, devops tools, etl data orchestration, git, jenkins, jira, nosql database, open source concepts, programming language, software development tools, sql",,
Sr. Software Engineer - Data,Pax8,"Colorado, United States",https://www.linkedin.com/jobs/view/sr-software-engineer-data-at-pax8-3731012617,2023-12-17,Colorado,United States,Mid senior,Remote,"Pax8 is the leading cloud-based technology marketplace, simplifying the cloud journey for our partners by integrating technology, business intelligence and proactive service to deliver an unparalleled experience. Serving thousands of partners through the indirect sales channel, our mission is to be the world’s favorite place to buy cloud products. We are a fast-growing, dynamic and high-energy startup organization, allowing you to make a meaningful impact on the business. Culture is important to us, and at Pax8, it’s business, and it
IS
personal. We are passionate, creative and humorously offbeat. We work hard, keep it fun, and expect the best.
We Elev8 each other. We Advoc8 for our partners. We Innov8 continuously. We Celebr8 life.
No matter who you are, Pax8 is a place you can call home. We know there’s no such thing as a
“perfect""
candidate, so we don’t look for the right ""
fit
"" – instead, we look for the add. We encourage you to apply for a role at Pax8 even if you don’t meet 100% of the bullet points. We believe in cultivating an environment with a diversity of perspectives, in hopes that we can all thrive in an inclusive environment.
We are only as great as our people. And we have great people all over the world. No matter where you live and work, you’re a part of the Pax8 team. This means embracing hybrid- and remote-work whenever possible.
Position Summary:
Are you a wizard with Data? At Pax8, we are looking to the future and how data will help get us there. You will join our growing Data Science team who are committed to the creation and expansion of clean accessible data to our internal Pax8 partners.
As a Software Engineer - Data, you will work amongst a collaborative team working with data from inception to modeling.This team works with ETL processes in order to pull data from across various data sources and consolidate it into our newly created and centralized Redshift data warehouse. The data you will work with and be creating a central point of truth for will aid several different Pax8 teams stretching across the organization that will be aimed at improving our customers' experience within our marketplace.
For you to successfully perform in this role, you will need to be equipped with experience to run with data, end-to-end, including everything from inception to modeling. Strong Python and SQL skills will be needed to navigate and deliver on the architectural areas and testing. You'll work with a vast amount of data, one crucial area will be within real-time streaming data harnessed by the use of Kafka. Having a solutions mindset will also be a key element to your contribution to our team approach to projects.
Meet the team and hear what they have to say about our Product & Engineering group: Meet the Team
Essential Responsibilities:
Builds pipelines to ingest new data sources
Transforms data to support varied use cases
Includes testing in all aspects of the development process
Mentors junior and mid-level Engineers
Optimizes existing data pipelines and improves existing code quality
Makes updates and improvements to deployment processes
Participates in project planning and architecture discussions
Analyzes potential problems and finds solutions to pressing data issues
Participates in on-call rotation
Ideal Skills, Experience, and Competencies:
At least Four (4) years of relevant data engineering experience
Advanced experience with Python
Expert experience with SQL
Intermediate experience with a JVM language
Exposure to other software development languages
Advanced experience with Apache Spark or other distributed processing engines
Advanced experience with Apache Kafka or other stream processing frameworks
Intermediate experience with Terraform, Docker, Kubernetes, or other similar infrastructure tooling
Advanced experience with cloud data tools such as S3, Glue, and Athena
Intermediate experience with building CI/CD pipelines
Effective problem solving and troubleshooting abilities
Ability to consistently achieve results, even under tough circumstances
Effective technical leadership abilities
Excellent verbal and written communication skills
Experience with innovative application design and implementation
Ability to make sense of complex, high quantity, and sometimes contradictory information to effectively solve problems
Required Education & Certifications:
B.A./B.S. in related field or equivalent work experience
M.S./M.A. in related field or equivalent work experience
Compensation:
Qualified candidates can expect a salary beginning at $140,000 or more depending on experience
#Dice-J #BI-Remote
Note: Compensation is benchmarked on local Denver Metro area market rates. Qualified candidates in other locations can expect a salary package that may be adjusted based off applicable cost of wages in their respective location.
At Pax8 we believe that your Total Rewards should include a benefits package that shows how much we value our greatest assets. All
FTE
Pax8 people enjoy the following benefits:
Non-Commissioned Bonus Plans or Variable Commission
401(k) plan with employer match
Medical, Dental & Vision Insurance
Employee Assistance Program
Employer Paid Short & Long Term Disability, Life and AD&D Insurance
Flexible, Open Vacation
Paid Sick Time Off
Extended Leave for Life events
RTD Eco Pass (For local Colorado Employees)
Career Development Programs
Stock Option Eligibility
Employee-led Resource Groups
Pax8 is an EEOC Employer.
Show more
Show less","Data engineering, Python, SQL, JVM languages, Other software development languages, Apache Spark, Apache Kafka, Stream processing frameworks, Terraform, Docker, Kubernetes, Cloud data tools, CI/CD pipelines, Problem solving, Troubleshooting, Technical leadership, Verbal communication, Written communication, Innovative application design, Information handling, B.A./B.S. degree, M.S./M.A. degree, NonCommissioned Bonus Plans, Variable Commission, 401(k) plan with employer match, Medical Dental & Vision Insurance, Employee Assistance Program, Employer Paid Short & Long Term Disability, Life and AD&D Insurance, Flexible Open Vacation, Paid Sick Time Off, Extended Leave for Life events, RTD Eco Pass, Career Development Programs, Stock Option Eligibility, Employeeled Resource Groups","data engineering, python, sql, jvm languages, other software development languages, apache spark, apache kafka, stream processing frameworks, terraform, docker, kubernetes, cloud data tools, cicd pipelines, problem solving, troubleshooting, technical leadership, verbal communication, written communication, innovative application design, information handling, babs degree, msma degree, noncommissioned bonus plans, variable commission, 401k plan with employer match, medical dental vision insurance, employee assistance program, employer paid short long term disability, life and add insurance, flexible open vacation, paid sick time off, extended leave for life events, rtd eco pass, career development programs, stock option eligibility, employeeled resource groups","401k plan with employer match, apache kafka, apache spark, babs degree, career development programs, cicd pipelines, cloud data tools, data engineering, docker, employee assistance program, employeeled resource groups, employer paid short long term disability, extended leave for life events, flexible open vacation, information handling, innovative application design, jvm languages, kubernetes, life and add insurance, medical dental vision insurance, msma degree, noncommissioned bonus plans, other software development languages, paid sick time off, problem solving, python, rtd eco pass, sql, stock option eligibility, stream processing frameworks, technical leadership, terraform, troubleshooting, variable commission, verbal communication, written communication"
"Senior/Staff Software Engineer, Data",EvenUp,"Denver, CO",https://www.linkedin.com/jobs/view/senior-staff-software-engineer-data-at-evenup-3782661726,2023-12-17,Colorado,United States,Mid senior,Remote,"EvenUp is a venture-backed generative AI startup that ensures injury victims are awarded the full value of their claims, expanding the $100B+ in awards granted to injury victims every year.
Every year, the legal system has made it difficult for millions of ordinary people to seek justice, especially for folks without means or who come from underrepresented backgrounds
. Our vision is to help these injury victims get the justice they deserve, irrespective of their income, demographics, or the quality of their legal representation.
EvenUp operates across all types of injury cases, from police brutality and child abuse to California wildfires and motor vehicle accidents. Our ML-driven software empowers attorneys to accurately assess the value of these cases by doing a core part of their workflow (legal drafting), enabling them to secure larger settlements in record time. As EvenUp evaluates more cases, our proprietary data grows, enhancing the precision of our predictions and delivering more value to both attorneys and victims alike.
As one of the fastest growing startups ($0 to $10M in ARR in <2 years), we raised $65M in investment from some of the best investors in the world (Bessemer, Bain Capital, Signalfire, DCM, NFX, Tribe Capital), seasoned tech executives (i.e. founder of Quora, SVP at Google, former CPO at Uber), and public figures that care about our social mission (Nas, Jared Leto, Byron Jones). Our team comes from top tech, legal, and investing backgrounds including Waymo, Google, Amazon, Uber, Quora, Blizzard, Norton Rose, Warburg Pincus, Bain, and McKinsey.
The role:
We’re looking to bring on board
Senior/Staff Software Engineers
focused on our
Data Pipelines
as we’ve experienced unprecedented growth and need to build & scale out our data pipelines and infrastructure. We’re looking for strong team members to help architect and drive forward the vision of our ideal data infrastructure at EvenUp. We will need to 10x our pipeline processing throughput over the next 12 months. We’ll need to rethink and rebuild how we extract, process and model our ingestion to enable our organization with precise and actionable data.
What you'll do:
Build fault tolerant data pipelines to process diverse datasets at EvenUp
Design and develop modularized services to increase the capabilities and scope of our data infrastructure
Collaborate with our DS team to Integrate ML models into our production workflows and simplify ML deployment and observability
Implement event driven, low latency systems to empower our stakeholders with accurate and reliable data
Analyze and solve key performance bottlenecks, scaling challenges, and high availability issues.
Mentor and coach junior team members
Help grow our engineering team and define a “data first” mentality across our organization.
What we are seeking:
8+ years of industry experience designing and building distributed data systems
Previous experience architecting and scaling event driven architectures
Strong understanding and practical experience with data pipeline tooling and storage systems such as Dagster, DBT, BigQuery, Elasticsearch
The ability to communicate cross-functionally with various stakeholders to derive requirements and architect scalable solutions
Have several years of industry experience building high-quality software, shipping production-ready code and infrastructure
You enjoy owning a project from start to finish and love to drive a project across the finish line.
Interest in making the world a fairer place (we don’t get paid unless we’re helping injured victims and/or their attorneys)
Nice to haves:
Fluency in Python, SQL and GraphQL
Previous experience integrating ML models and LLMs into data services
Domain expertise in legal technology, medical records, and working with unstructured data
Benefits & Perks:
We seek to empower all of our team members to fulfill our mission of making the world a more just place, regardless of our team’s function, geography, or experience level. To that end, we offer:
Fully remote setup - work from wherever you feel is best (Plus a stipend to upgrade your home office!)
Flexible working hours to match your style
Offsites - get to meet your coworkers on a fully-expensed trip every 6-12 months!
Choice of great medical, dental, and vision insurance plan options
Flexible paid time off
A variety of virtual team events such as game nights & happy hours
EvenUp is an equal-opportunity employer. We are committed to diversity and inclusion in our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.
Show more
Show less","Data Pipelines, Data Infrastructure, Data Storage Systems, Data Analysis, Machine Learning Deployment, Distributed Systems, EventDriven Architectures, Dagster, DBT, BigQuery, Elasticsearch, Python, SQL, GraphQL, Machine Learning Models, Large Language Models, Legal Technology, Medical Records, Unstructured Data","data pipelines, data infrastructure, data storage systems, data analysis, machine learning deployment, distributed systems, eventdriven architectures, dagster, dbt, bigquery, elasticsearch, python, sql, graphql, machine learning models, large language models, legal technology, medical records, unstructured data","bigquery, dagster, data infrastructure, data storage systems, dataanalytics, datapipeline, dbt, distributed systems, elasticsearch, eventdriven architectures, graphql, large language models, legal technology, machine learning deployment, machine learning models, medical records, python, sql, unstructured data"
Data Engineer - Scala(U.S. remote),Railroad19,"Centennial, CO",https://www.linkedin.com/jobs/view/data-engineer-scala-u-s-remote-at-railroad19-3782858134,2023-12-17,Colorado,United States,Mid senior,Remote,"Railroad19, Inc.
is hiring
remote
Senior Data Engineers
to be a solid technical resource on a dynamic and growing team of accomplished engineers.
Our ideal candidate is passionate about creating well-architected solutions containing thoroughly tested code. The ability to communicate effectively and develop relationships by empathizing with client goals is a highly valued skill within our company culture.
Core Responsibilities:
Develop new and enhance existing application services
Writing tests to maintain code quality
Understand and adapt to our client's evolving business requirements within the television advertising domain.
Participate in detailed technical design sessions to understand client needs and provide productive feedback
Identify new opportunities, tools, and services to enhance the software platform
Support and troubleshoot issues, identify the root cause, and proactively recommend corrective actions
Skills & Experience:
Scala 2.12 + development experience
Passionate about developing clean and maintainable code with little or no side-effects
Experience building Restful APIs in Scala using Spark 2.4
Strong hands-on experience with AWS running spark jobs on ephemeral EMR clusters on AWS and S3.
Experience with relational and non-relational databases
Willingness to learn new technologies and takes pride in keeping up with the latest technologies and practices within the Scala and Spark development community
Excellent oral and written communication skills
Strong analytical and problem-solving skills
Self-directed and can effectively deliver solutions with little oversight
A bachelor's or master's degree in computer science, computer engineering, or other technical disciplines or equivalent work experience is preferred but not required.
$120,000 - $160,000 a year
Salary is commensurate with experience.
We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender, gender identity or expression, or veteran status. We are proud to be an equal-opportunity workplace.
#Hiringnow
We are actively hiring (Data Engineers)
#remote #Scala #DataEngineering #ApacheSpark
Show more
Show less","Scala 2.12, Spark 2.4, AWS, EMR, S3, Relational databases, Nonrelational databases, Scala and Spark development community, Restful APIs, Unit testing, Technical writing, Problem solving, Computer science, Computer engineering","scala 212, spark 24, aws, emr, s3, relational databases, nonrelational databases, scala and spark development community, restful apis, unit testing, technical writing, problem solving, computer science, computer engineering","aws, computer engineering, computer science, emr, nonrelational databases, problem solving, relational databases, restful apis, s3, scala 212, scala and spark development community, spark 24, technical writing, unit testing"
Data Engineer - Scala(U.S. remote),Railroad19,"Greeley, CO",https://www.linkedin.com/jobs/view/data-engineer-scala-u-s-remote-at-railroad19-3782852892,2023-12-17,Colorado,United States,Mid senior,Remote,"Railroad19, Inc.
is hiring
remote
Senior Data Engineers
to be a solid technical resource on a dynamic and growing team of accomplished engineers.
Our ideal candidate is passionate about creating well-architected solutions containing thoroughly tested code. The ability to communicate effectively and develop relationships by empathizing with client goals is a highly valued skill within our company culture.
Core Responsibilities:
Develop new and enhance existing application services
Writing tests to maintain code quality
Understand and adapt to our client's evolving business requirements within the television advertising domain.
Participate in detailed technical design sessions to understand client needs and provide productive feedback
Identify new opportunities, tools, and services to enhance the software platform
Support and troubleshoot issues, identify the root cause, and proactively recommend corrective actions
Skills & Experience:
Scala 2.12 + development experience
Passionate about developing clean and maintainable code with little or no side-effects
Experience building Restful APIs in Scala using Spark 2.4
Strong hands-on experience with AWS running spark jobs on ephemeral EMR clusters on AWS and S3.
Experience with relational and non-relational databases
Willingness to learn new technologies and takes pride in keeping up with the latest technologies and practices within the Scala and Spark development community
Excellent oral and written communication skills
Strong analytical and problem-solving skills
Self-directed and can effectively deliver solutions with little oversight
A bachelor's or master's degree in computer science, computer engineering, or other technical disciplines or equivalent work experience is preferred but not required.
$120,000 - $160,000 a year
Salary is commensurate with experience.
We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender, gender identity or expression, or veteran status. We are proud to be an equal-opportunity workplace.
#Hiringnow
We are actively hiring (Data Engineers)
#remote #Scala #DataEngineering #ApacheSpark
Show more
Show less","Scala, Spark, AWS, EMR, S3, Relational databases, Nonrelational databases, Restful APIs, Cloud computing, Data engineering, Software development, Communication skills, Analytical skills, Problemsolving skills, Teamwork, Selfdirection","scala, spark, aws, emr, s3, relational databases, nonrelational databases, restful apis, cloud computing, data engineering, software development, communication skills, analytical skills, problemsolving skills, teamwork, selfdirection","analytical skills, aws, cloud computing, communication skills, data engineering, emr, nonrelational databases, problemsolving skills, relational databases, restful apis, s3, scala, selfdirection, software development, spark, teamwork"
Data Engineer - Scala(U.S. remote),Railroad19,"Thornton, CO",https://www.linkedin.com/jobs/view/data-engineer-scala-u-s-remote-at-railroad19-3782855515,2023-12-17,Colorado,United States,Mid senior,Remote,"Railroad19, Inc.
is hiring
remote
Senior Data Engineers
to be a solid technical resource on a dynamic and growing team of accomplished engineers.
Our ideal candidate is passionate about creating well-architected solutions containing thoroughly tested code. The ability to communicate effectively and develop relationships by empathizing with client goals is a highly valued skill within our company culture.
Core Responsibilities:
Develop new and enhance existing application services
Writing tests to maintain code quality
Understand and adapt to our client's evolving business requirements within the television advertising domain.
Participate in detailed technical design sessions to understand client needs and provide productive feedback
Identify new opportunities, tools, and services to enhance the software platform
Support and troubleshoot issues, identify the root cause, and proactively recommend corrective actions
Skills & Experience:
Scala 2.12 + development experience
Passionate about developing clean and maintainable code with little or no side-effects
Experience building Restful APIs in Scala using Spark 2.4
Strong hands-on experience with AWS running spark jobs on ephemeral EMR clusters on AWS and S3.
Experience with relational and non-relational databases
Willingness to learn new technologies and takes pride in keeping up with the latest technologies and practices within the Scala and Spark development community
Excellent oral and written communication skills
Strong analytical and problem-solving skills
Self-directed and can effectively deliver solutions with little oversight
A bachelor's or master's degree in computer science, computer engineering, or other technical disciplines or equivalent work experience is preferred but not required.
$120,000 - $160,000 a year
Salary is commensurate with experience.
We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender, gender identity or expression, or veteran status. We are proud to be an equal-opportunity workplace.
#Hiringnow
We are actively hiring (Data Engineers)
#remote #Scala #DataEngineering #ApacheSpark
Show more
Show less","Scala, Java, Spark, EMR, S3, Restful APIs, AWS, relational databases, nonrelational databases","scala, java, spark, emr, s3, restful apis, aws, relational databases, nonrelational databases","aws, emr, java, nonrelational databases, relational databases, restful apis, s3, scala, spark"
Data Engineer - Scala(U.S. remote),Railroad19,"Aurora, CO",https://www.linkedin.com/jobs/view/data-engineer-scala-u-s-remote-at-railroad19-3782852874,2023-12-17,Colorado,United States,Mid senior,Remote,"Railroad19, Inc.
is hiring
remote
Senior Data Engineers
to be a solid technical resource on a dynamic and growing team of accomplished engineers.
Our ideal candidate is passionate about creating well-architected solutions containing thoroughly tested code. The ability to communicate effectively and develop relationships by empathizing with client goals is a highly valued skill within our company culture.
Core Responsibilities:
Develop new and enhance existing application services
Writing tests to maintain code quality
Understand and adapt to our client's evolving business requirements within the television advertising domain.
Participate in detailed technical design sessions to understand client needs and provide productive feedback
Identify new opportunities, tools, and services to enhance the software platform
Support and troubleshoot issues, identify the root cause, and proactively recommend corrective actions
Skills & Experience:
Scala 2.12 + development experience
Passionate about developing clean and maintainable code with little or no side-effects
Experience building Restful APIs in Scala using Spark 2.4
Strong hands-on experience with AWS running spark jobs on ephemeral EMR clusters on AWS and S3.
Experience with relational and non-relational databases
Willingness to learn new technologies and takes pride in keeping up with the latest technologies and practices within the Scala and Spark development community
Excellent oral and written communication skills
Strong analytical and problem-solving skills
Self-directed and can effectively deliver solutions with little oversight
A bachelor's or master's degree in computer science, computer engineering, or other technical disciplines or equivalent work experience is preferred but not required.
$120,000 - $160,000 a year
Salary is commensurate with experience.
We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender, gender identity or expression, or veteran status. We are proud to be an equal-opportunity workplace.
#Hiringnow
We are actively hiring (Data Engineers)
#remote #Scala #DataEngineering #ApacheSpark
Show more
Show less","Scala 2.12, AWS, S3, Spark 2.4, EMR, Restful APIs, Relational databases, Nonrelational databases, Apache Spark","scala 212, aws, s3, spark 24, emr, restful apis, relational databases, nonrelational databases, apache spark","apache spark, aws, emr, nonrelational databases, relational databases, restful apis, s3, scala 212, spark 24"
Data Engineer - Scala(U.S. remote),Railroad19,"Westminster, CO",https://www.linkedin.com/jobs/view/data-engineer-scala-u-s-remote-at-railroad19-3782856437,2023-12-17,Colorado,United States,Mid senior,Remote,"Railroad19, Inc.
is hiring
remote
Senior Data Engineers
to be a solid technical resource on a dynamic and growing team of accomplished engineers.
Our ideal candidate is passionate about creating well-architected solutions containing thoroughly tested code. The ability to communicate effectively and develop relationships by empathizing with client goals is a highly valued skill within our company culture.
Core Responsibilities:
Develop new and enhance existing application services
Writing tests to maintain code quality
Understand and adapt to our client's evolving business requirements within the television advertising domain.
Participate in detailed technical design sessions to understand client needs and provide productive feedback
Identify new opportunities, tools, and services to enhance the software platform
Support and troubleshoot issues, identify the root cause, and proactively recommend corrective actions
Skills & Experience:
Scala 2.12 + development experience
Passionate about developing clean and maintainable code with little or no side-effects
Experience building Restful APIs in Scala using Spark 2.4
Strong hands-on experience with AWS running spark jobs on ephemeral EMR clusters on AWS and S3.
Experience with relational and non-relational databases
Willingness to learn new technologies and takes pride in keeping up with the latest technologies and practices within the Scala and Spark development community
Excellent oral and written communication skills
Strong analytical and problem-solving skills
Self-directed and can effectively deliver solutions with little oversight
A bachelor's or master's degree in computer science, computer engineering, or other technical disciplines or equivalent work experience is preferred but not required.
$120,000 - $160,000 a year
Salary is commensurate with experience.
We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender, gender identity or expression, or veteran status. We are proud to be an equal-opportunity workplace.
#Hiringnow
We are actively hiring (Data Engineers)
#remote #Scala #DataEngineering #ApacheSpark
Show more
Show less","Scala 2.12, Clean and maintainable code, Restful APIs, Spark 2.4, AWS, EMR clusters, S3, Relational databases, Nonrelational databases, Analytical skills, Problemsolving skills, Selfdirected, Bachelor's or master's degree in computer science computer engineering or other technical disciplines","scala 212, clean and maintainable code, restful apis, spark 24, aws, emr clusters, s3, relational databases, nonrelational databases, analytical skills, problemsolving skills, selfdirected, bachelors or masters degree in computer science computer engineering or other technical disciplines","analytical skills, aws, bachelors or masters degree in computer science computer engineering or other technical disciplines, clean and maintainable code, emr clusters, nonrelational databases, problemsolving skills, relational databases, restful apis, s3, scala 212, selfdirected, spark 24"
"Senior/Staff Software Engineer, Data Pipelines",EvenUp,"Denver, CO",https://www.linkedin.com/jobs/view/senior-staff-software-engineer-data-pipelines-at-evenup-3766512261,2023-12-17,Colorado,United States,Mid senior,Remote,"EvenUp is a venture-backed generative AI startup that ensures injury victims are awarded the full value of their claims, expanding the $100B+ in awards granted to injury victims every year.
Every year, the legal system has made it difficult for millions of ordinary people to seek justice, especially for folks without means or who come from underrepresented backgrounds
. Our vision is to help these injury victims get the justice they deserve, irrespective of their income, demographics, or the quality of their legal representation.
EvenUp operates across all types of injury cases, from police brutality and child abuse to California wildfires and motor vehicle accidents. Our ML-driven software empowers attorneys to accurately assess the value of these cases by doing a core part of their workflow (legal drafting), enabling them to secure larger settlements in record time. As EvenUp evaluates more cases, our proprietary data grows, enhancing the precision of our predictions and delivering more value to both attorneys and victims alike.
As one of the fastest growing startups ($0 to $10M in ARR in <2 years), we raised $65M in investment from some of the best investors in the world (Bessemer, Bain Capital, Signalfire, DCM, NFX, Tribe Capital), seasoned tech executives (i.e. founder of Quora, SVP at Google, former CPO at Uber), and public figures that care about our social mission (Nas, Jared Leto, Byron Jones). Our team comes from top tech, legal, and investing backgrounds including Waymo, Google, Amazon, Uber, Quora, Blizzard, Norton Rose, Warburg Pincus, Bain, and McKinsey.
The role:
We’re looking to bring on board
Senior/Staff Software Engineers
focused on our
Data Pipelines
as we’ve experienced unprecedented growth and need to build & scale out our data pipelines and infrastructure. We’re looking for strong team members to help architect and drive forward the vision of our ideal data infrastructure at EvenUp. We will need to 10x our pipeline processing throughput over the next 12 months. We’ll need to rethink and rebuild how we extract, process and model our ingestion to enable our organization with precise and actionable data.
What you'll do:
Build fault tolerant data pipelines to process diverse datasets at EvenUp
Design and develop modularized services to increase the capabilities and scope of our data infrastructure
Collaborate with our DS team to Integrate ML models into our production workflows and simplify ML deployment and observability
Implement event driven, low latency systems to empower our stakeholders with accurate and reliable data
Analyze and solve key performance bottlenecks, scaling challenges, and high availability issues.
Mentor and coach junior team members
Help grow our engineering team and define a “data first” mentality across our organization.
What we are seeking:
8+ years of industry experience designing and building distributed data systems
Previous experience architecting and scaling event driven architectures
Strong understanding and practical experience with data pipeline tooling and storage systems such as Dagster, DBT, BigQuery, Elasticsearch
The ability to communicate cross-functionally with various stakeholders to derive requirements and architect scalable solutions
Have several years of industry experience building high-quality software, shipping production-ready code and infrastructure
You enjoy owning a project from start to finish and love to drive a project across the finish line.
Interest in making the world a fairer place (we don’t get paid unless we’re helping injured victims and/or their attorneys)
Nice to haves:
Fluency in Python, SQL and GraphQL
Previous experience integrating ML models and LLMs into data services
Domain expertise in legal technology, medical records, and working with unstructured data
Benefits & Perks:
We seek to empower all of our team members to fulfill our mission of making the world a more just place, regardless of our team’s function, geography, or experience level. To that end, we offer:
Fully remote setup - work from wherever you feel is best (Plus a stipend to upgrade your home office!)
Flexible working hours to match your style
Offsites - get to meet your coworkers on a fully-expensed trip every 6-12 months!
Choice of great medical, dental, and vision insurance plan options
Flexible paid time off
A variety of virtual team events such as game nights & happy hours
EvenUp is an equal-opportunity employer. We are committed to diversity and inclusion in our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.
Show more
Show less","Python, SQL, GraphQL, Data Pipelines, ML Integration, Dagster, DBT, BigQuery, Elasticsearch, Distributed Data Systems, EventDriven Architectures, Software Development, Infrastructure Management, CrossFunctional Communication, Problem Solving, Mentoring, Team Building, DataDriven Culture","python, sql, graphql, data pipelines, ml integration, dagster, dbt, bigquery, elasticsearch, distributed data systems, eventdriven architectures, software development, infrastructure management, crossfunctional communication, problem solving, mentoring, team building, datadriven culture","bigquery, crossfunctional communication, dagster, datadriven culture, datapipeline, dbt, distributed data systems, elasticsearch, eventdriven architectures, graphql, infrastructure management, mentoring, ml integration, problem solving, python, software development, sql, team building"
Data Engineer - Scala(U.S. remote),Railroad19,"Lakewood, CO",https://www.linkedin.com/jobs/view/data-engineer-scala-u-s-remote-at-railroad19-3782858112,2023-12-17,Colorado,United States,Mid senior,Remote,"Railroad19, Inc.
is hiring
remote
Senior Data Engineers
to be a solid technical resource on a dynamic and growing team of accomplished engineers.
Our ideal candidate is passionate about creating well-architected solutions containing thoroughly tested code. The ability to communicate effectively and develop relationships by empathizing with client goals is a highly valued skill within our company culture.
Core Responsibilities:
Develop new and enhance existing application services
Writing tests to maintain code quality
Understand and adapt to our client's evolving business requirements within the television advertising domain.
Participate in detailed technical design sessions to understand client needs and provide productive feedback
Identify new opportunities, tools, and services to enhance the software platform
Support and troubleshoot issues, identify the root cause, and proactively recommend corrective actions
Skills & Experience:
Scala 2.12 + development experience
Passionate about developing clean and maintainable code with little or no side-effects
Experience building Restful APIs in Scala using Spark 2.4
Strong hands-on experience with AWS running spark jobs on ephemeral EMR clusters on AWS and S3.
Experience with relational and non-relational databases
Willingness to learn new technologies and takes pride in keeping up with the latest technologies and practices within the Scala and Spark development community
Excellent oral and written communication skills
Strong analytical and problem-solving skills
Self-directed and can effectively deliver solutions with little oversight
A bachelor's or master's degree in computer science, computer engineering, or other technical disciplines or equivalent work experience is preferred but not required.
$120,000 - $160,000 a year
Salary is commensurate with experience.
We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender, gender identity or expression, or veteran status. We are proud to be an equal-opportunity workplace.
#Hiringnow
We are actively hiring (Data Engineers)
#remote #Scala #DataEngineering #ApacheSpark
Show more
Show less","Scala, Spark, AWS, EMR, S3, Restful API, Relational databases, Nonrelational databases","scala, spark, aws, emr, s3, restful api, relational databases, nonrelational databases","aws, emr, nonrelational databases, relational databases, restful api, s3, scala, spark"
Data Engineer - Scala(U.S. remote),Railroad19,"Fort Collins, CO",https://www.linkedin.com/jobs/view/data-engineer-scala-u-s-remote-at-railroad19-3782852875,2023-12-17,Colorado,United States,Mid senior,Remote,"Railroad19, Inc.
is hiring
remote
Senior Data Engineers
to be a solid technical resource on a dynamic and growing team of accomplished engineers.
Our ideal candidate is passionate about creating well-architected solutions containing thoroughly tested code. The ability to communicate effectively and develop relationships by empathizing with client goals is a highly valued skill within our company culture.
Core Responsibilities:
Develop new and enhance existing application services
Writing tests to maintain code quality
Understand and adapt to our client's evolving business requirements within the television advertising domain.
Participate in detailed technical design sessions to understand client needs and provide productive feedback
Identify new opportunities, tools, and services to enhance the software platform
Support and troubleshoot issues, identify the root cause, and proactively recommend corrective actions
Skills & Experience:
Scala 2.12 + development experience
Passionate about developing clean and maintainable code with little or no side-effects
Experience building Restful APIs in Scala using Spark 2.4
Strong hands-on experience with AWS running spark jobs on ephemeral EMR clusters on AWS and S3.
Experience with relational and non-relational databases
Willingness to learn new technologies and takes pride in keeping up with the latest technologies and practices within the Scala and Spark development community
Excellent oral and written communication skills
Strong analytical and problem-solving skills
Self-directed and can effectively deliver solutions with little oversight
A bachelor's or master's degree in computer science, computer engineering, or other technical disciplines or equivalent work experience is preferred but not required.
$120,000 - $160,000 a year
Salary is commensurate with experience.
We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender, gender identity or expression, or veteran status. We are proud to be an equal-opportunity workplace.
#Hiringnow
We are actively hiring (Data Engineers)
#remote #Scala #DataEngineering #ApacheSpark
Show more
Show less","Scala 2.12, Spark 2.4, AWS, EMR, S3, Relational databases, Nonrelational databases, Restful APIs, Problemsolving, Oral communication, Written communication, Analytical","scala 212, spark 24, aws, emr, s3, relational databases, nonrelational databases, restful apis, problemsolving, oral communication, written communication, analytical","analytical, aws, emr, nonrelational databases, oral communication, problemsolving, relational databases, restful apis, s3, scala 212, spark 24, written communication"
Data Engineer - Scala(U.S. remote),Railroad19,"Pueblo, CO",https://www.linkedin.com/jobs/view/data-engineer-scala-u-s-remote-at-railroad19-3782856438,2023-12-17,Colorado,United States,Mid senior,Remote,"Railroad19, Inc.
is hiring
remote
Senior Data Engineers
to be a solid technical resource on a dynamic and growing team of accomplished engineers.
Our ideal candidate is passionate about creating well-architected solutions containing thoroughly tested code. The ability to communicate effectively and develop relationships by empathizing with client goals is a highly valued skill within our company culture.
Core Responsibilities:
Develop new and enhance existing application services
Writing tests to maintain code quality
Understand and adapt to our client's evolving business requirements within the television advertising domain.
Participate in detailed technical design sessions to understand client needs and provide productive feedback
Identify new opportunities, tools, and services to enhance the software platform
Support and troubleshoot issues, identify the root cause, and proactively recommend corrective actions
Skills & Experience:
Scala 2.12 + development experience
Passionate about developing clean and maintainable code with little or no side-effects
Experience building Restful APIs in Scala using Spark 2.4
Strong hands-on experience with AWS running spark jobs on ephemeral EMR clusters on AWS and S3.
Experience with relational and non-relational databases
Willingness to learn new technologies and takes pride in keeping up with the latest technologies and practices within the Scala and Spark development community
Excellent oral and written communication skills
Strong analytical and problem-solving skills
Self-directed and can effectively deliver solutions with little oversight
A bachelor's or master's degree in computer science, computer engineering, or other technical disciplines or equivalent work experience is preferred but not required.
$120,000 - $160,000 a year
Salary is commensurate with experience.
We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender, gender identity or expression, or veteran status. We are proud to be an equal-opportunity workplace.
#Hiringnow
We are actively hiring (Data Engineers)
#remote #Scala #DataEngineering #ApacheSpark
Show more
Show less","Scala, Java, Apache Spark, AWS, EMR, S3, Relational databases, Nonrelational databases, REST APIs, Unit testing, Troubleshooting, Problemsolving, Analytical skills, Communication skills, Software development, Agile development, Data engineering, Machine learning, Big data","scala, java, apache spark, aws, emr, s3, relational databases, nonrelational databases, rest apis, unit testing, troubleshooting, problemsolving, analytical skills, communication skills, software development, agile development, data engineering, machine learning, big data","agile development, analytical skills, apache spark, aws, big data, communication skills, data engineering, emr, java, machine learning, nonrelational databases, problemsolving, relational databases, rest apis, s3, scala, software development, troubleshooting, unit testing"
Staff Cybersecurity Data Platform Engineer,Adobe,"Colorado, United States",https://www.linkedin.com/jobs/view/staff-cybersecurity-data-platform-engineer-at-adobe-3767919834,2023-12-17,Colorado,United States,Mid senior,Remote,"Our Company
Changing the world through digital experiences is what Adobe’s all about. We give everyone—from emerging artists to global brands—everything they need to design and deliver exceptional digital experiences! We’re passionate about empowering people to create beautiful and powerful images, videos, and apps, and transform how companies interact with customers across every screen.
We’re on a mission to hire the very best and are committed to creating exceptional employee experiences where everyone is respected and has access to equal opportunity. We realize that new ideas can come from everywhere in the organization, and we know the next big idea could be yours!
As a Staff Cybersecurity Data Platform Engineer at Adobe, you will be joining a team responsible for enhancing the organization's security data platform. Our focus is on contributing to the organization by emphasizing real-world security and embracing automation, as well as optimizing data flow and consumption of the data to multiple teams within Security.
The ideal candidate will help develop security-focused data platform needs and the scaling of a data lake spanning petabytes of data. The job involves system architecture, design, hands-on development, optimization, setting expectations and SLAs with a focus on reliability, availability, and performance.
They must be self-directed and comfortable supporting the data needs of multiple security teams. The right candidate will be excited by the prospect of optimizing our company’s data platform to support our next generation of security products and data initiatives.
Key Responsibilities:
Collaborate with partners, cybersecurity engineers, and operations teams across the security organization to drive the development of enterprise-scale security solutions.
Work with enterprise architects team to ensure alignment with strategic objectives, and leverage design principles while providing input into program direction.
Translate business needs into technical requirements, capabilities, and formulation of solutions, while identifying risks, dependencies, financial impacts, and the risk profile in the technical solution.
Take a hands-on approach to driving proof-of-concept, design, and implementation activities from an architectural perspective.
Develop and maintain solution architecture documents and other artifacts to guide the planning, design, and implementation of the proposed solution.
Required Skills to be Successful:
7 years of experience in designing and building solutions in Databricks, across multiple clouds, using orchestration, data ingestion, medallion architectures, unity catalog, autoloader jobs, and Delta Lakehouse concepts.
10 years of experience working as either: Software Engineer/Data Engineer: query tuning, performance tuning, troubleshooting, and implementing/debugging Spark/PySpark and/or other big data solutions.
10 years of proficiency in developing or architecting modern distributed cloud architectures using AWS tools and technology.
Design end-to-end robust, scalable, real-time data streaming (Kafka / Flink) and data platform architecture that will support the analytical and reporting needs of the entire organization.
Offer technical expertise by collaborating with analysts and business users to translate diverse and intricate functional specifications into technical designs.
Build data models and improve standard schemas across different data sources and normalize data.
Significant experience in security, encompassing threat management, incident response, and enterprise security.
Strong understanding of Security Operations Center (SOC) operations and security management workflows within large organizations.
Our compensation reflects the cost of labor across several  U.S. geographic markets, and we pay differently based on those defined markets. The U.S. pay range for this position is $146,400 -- $275,000 annually. Pay within this range varies by work location and may also depend on job-related knowledge, skills, and experience. Your recruiter can share more about the specific salary range for the job location during the hiring process.
At Adobe, for sales roles starting salaries are expressed as total target compensation (TTC = base + commission), and short-term incentives are in the form of sales commission plans. Non-sales roles starting salaries are expressed as base salary and short-term incentives are in the form of the Annual Incentive Plan (AIP).
In addition, certain roles may be eligible for long-term incentives in the form of a new hire equity award.
Adobe is proud to be an Equal Employment Opportunity and affirmative action employer. We do not discriminate based on gender, race or color, ethnicity or national origin, age, disability, religion, sexual orientation, gender identity or expression, veteran status, or any other applicable characteristics protected by law. Learn more.
Adobe aims to make Adobe.com accessible to any and all users. If you have a disability or special need that requires accommodation to navigate our website or complete the application process, email accommodations@adobe.com or call (408) 536-3015.
Adobe values a free and open marketplace for all employees and has policies in place to ensure that we do not enter into illegal agreements with other companies to not recruit or hire each other’s employees.
Show more
Show less","Databricks, Apache Spark, PySpark, Kafka, Flink, Data lakehouse, Unity Catalog, Autoloader jobs, Delta Lakehouse, AWS, Security Operations Center (SOC), Threat management, Incident response, Enterprise security","databricks, apache spark, pyspark, kafka, flink, data lakehouse, unity catalog, autoloader jobs, delta lakehouse, aws, security operations center soc, threat management, incident response, enterprise security","apache spark, autoloader jobs, aws, data lakehouse, databricks, delta lakehouse, enterprise security, flink, incident response, kafka, security operations center soc, spark, threat management, unity catalog"
Data Engineer,"IDR, Inc.","Boulder, CO",https://www.linkedin.com/jobs/view/data-engineer-at-idr-inc-3755322870,2023-12-17,Colorado,United States,Mid senior,Hybrid,"IDR is seeking a Data Engineer to join one of our top clients in Thornton, CO. If you are looking for an opportunity to join a large organization and work within an ever-growing team-oriented culture, please apply today! This is a hybrid onsite position, so please only apply if you are local to the Denver metro area!
Required Skills For The Data Engineer
Good knowledge of Rockwell Automation on the PyVision
Brief Knowledge of MES
Good knowledge of manufacturing and programming.
Experience working with NetSuite as an ERP system.
Expert level knowledge in red zone
What’s in it for you?
Competitive compensation package
Full Benefits; Medical, Vision, Dental, and more!
Opportunity to get in with an industry leading organization
Close-knit and team-oriented culture
Why IDR?
25+ Years of Proven Industry Experience in 4 major markets
Employee Stock Ownership Program
Dedicated Engagement Manager who is committed to you and your success
Medical, Dental, Vision, and Life Insurance
ClearlyRated’s Best of Staffing® Client and Talent Award winner 10 years in a row
Compensation Details:
$65-70/hr
Show more
Show less","Python, PyVision, Rockwell Automation, Manufacturing, Programming, NetSuite, ERP, Red zone","python, pyvision, rockwell automation, manufacturing, programming, netsuite, erp, red zone","erp, manufacturing, netsuite, programming, python, pyvision, red zone, rockwell automation"
Mid to Senior Level Data Engineer--Consulting Services--Mainly Remote Work Possible if Somewhat Close to Denver,EMW Staffing Solutions LLC,Denver Metropolitan Area,https://www.linkedin.com/jobs/view/mid-to-senior-level-data-engineer-consulting-services-mainly-remote-work-possible-if-somewhat-close-to-denver-at-emw-staffing-solutions-llc-3728564230,2023-12-17,Colorado,United States,Mid senior,Hybrid,"Thanks for your interest. Please apply for further information.
Term-Full Time
Location--Customer is in Denver area--This is a mainly remote role for those close to the Denver area
$-Salary is open for this position and depends on experience, with base salaries ranging from $85, 000 to $140, 000 per year + potential for bonus.
Vertical--Services.
Open to mid to approaching senior/stand alone resource
Please note that this role is open to remote candidates provided that they lives relatively close to Denver CO to include neighboring states.
Customer needs team members that excel when working directly with clients to meet their goals. They understand the client’s needs and requirements and build a collaborative environment to ensure a successful project delivery.
Data Engineers analyze and develop on-premises and/or cloud data and ETL solutions to solve the client’s challenges. They enjoy the challenges of consulting and thrive to knock the socks off of clients
Please note that this role is vendor agnostic in regards to what ETL tools are used, so having multi vendor experience would be ideal.
Responsibilities:
Hands-on development and serve as technical expert on projects
Develop data solutions leveraging traditional and cloud product offerings from leading vendors
Develop data models to meet client needs
Develop data models to meet client needs, including transactional, third-normal form, dimensional, columnar, distributed and NoSQL
Develop ETL/ELT processes and patterns to efficiently move data
Create data visualizations, dashboards and reports as needed
Data migrations and conversions to the cloud and to cloud data warehouse services
Develop and scope requirements
Travel as needed (currently less than 5%)
Maintain effective communication with team and customers
Qualifications
2+ years designing and developing data analytics solutions
2+ years with databases such as SQL Server, Oracle, MySQL
2+ years data warehouse, dimensional modeling design and architecture
Knowledge of distributed databases such as Redshift, Snowflake, Azure Synapse, and BigQuery
A passion to learn and improve your skills to deliver the best possible solutions to customers
Experience with cloud based data services offered by Azure, AWS and Google
Experience with data visualization tools such as Power BI and Tableau
Previous consulting experience preferred
Degree in computer science, information technology, engineering or business
Show more
Show less","Data Analytics, Databases, SQL Server, Oracle, MySQL, Data Warehouse, Dimensional Modeling, Distributed Databases, Redshift, Snowflake, Azure Synapse, BigQuery, Cloud Data Services, Azure, AWS, Google, Data Visualization Tools, Power BI, Tableau, Consulting, Computer Science, Information Technology, Engineering, Business","data analytics, databases, sql server, oracle, mysql, data warehouse, dimensional modeling, distributed databases, redshift, snowflake, azure synapse, bigquery, cloud data services, azure, aws, google, data visualization tools, power bi, tableau, consulting, computer science, information technology, engineering, business","aws, azure, azure synapse, bigquery, business, cloud data services, computer science, consulting, data visualization tools, dataanalytics, databases, datawarehouse, dimensional modeling, distributed databases, engineering, google, information technology, mysql, oracle, powerbi, redshift, snowflake, sql server, tableau"
Senior Cloud Data Engineer,BDO USA,"Denver, CO",https://www.linkedin.com/jobs/view/senior-cloud-data-engineer-at-bdo-usa-3765471248,2023-12-17,Colorado,United States,Mid senior,Hybrid,"Job Description
Job Summary:
This position will work with cutting edge technology, deliver high quality solutions across various industries, and oversee team(s) on engagements that range in size and scope. This position will receive continuous career development opportunities, given the size and potential of client engagements. This role will perform hands-on delivery of data analytics projects, contributing to the development and unit testing of solutions.
Job Duties
Designs and implements best in class data ingestion strategies, data warehouse and data mart structures, semantic layers and models, visualizations, streaming processes, API integrations, and automation (RPA) solutions for end-to-end data analytics solutions on primarily, but not limited to, cloud analytics platforms such as Azure and AWS
Listens to client needs to align solution with business requirements and delivery schedule
Creates written functional and technical designs
Participates in project status and stand meetings, and assists with providing aggregated project status for project and program managers
Assists with SLA compliance of solutions, and performs performance tuning and optimization efforts of end-to-end solutions
Writes code using multiple languages and correctly applies frameworks, architectural patterns, and software development principles
Delivers high-performance, scalable, repeatable, and secure deliverables with broad impact (high throughput and low latency)
Assists with implementation of data governance programs and best practices
Performs the cleaning and transforming of data from source systems into analytics models
Implements models to support data visualizations and integrations
Assists with implementing DevOps, DataOps and MLOps methodologies on projects
Writes custom integration logic in applicable programming languages
Assists project managers with work breakdown structure creation, project estimation, resource staffing, workload planning and adjustments throughout the project lifecycle
Assists clients with licensing, security, and cost estimation of solutions
Performs code reviews to ensure adherence to standards
Works directly with clients and team members to establish secure data analytics platforms and infrastructure
Contributes to successful deployments of developed solutions and integration of DevOps tools
Maintains a broad and current understanding of data analytics and business intelligence strategies, cloud platforms, methodologies, and tools
Builds client relationships during project execution, effectively becoming a trusted advisor of the client
Participates in support activities for existing software solutions
Other duties as assigned
Supervisory Responsibilities
Supervises the day-to-day workload of Associates on assigned engagements to ensure that timelines and deliverables are met, and reviews work product
Education
Qualifications, Knowledge, Skills and Abilities:
High School Diploma or GED equivalent, required
Bachelor’s degree, preferred; focus in Information Systems, Data Science or Computer Science, preferred
Experience
Five (5) or more years of experience within Data Analytics, Business Intelligence, Artificial Intelligence, or Application Development, required
One (1) or more years of experience technically leading development projects, preferred
One (1) or more years of consulting experience or implementation of cloud-based data analytics solutions, preferred
Software
Strong SQL skills including Data Definition Language (DDL), Data Manipulation Language (DML), views, functions, stored procedures, or performance tuning, required
Experience with Data Warehousing, Data Modeling, Semantic Model Definition or Star Schema Construction, required
Hands on delivery experience of end-to-end cloud data analytics solutions within Azure or AWS, preferred
Experience with one (1) or more of the following computer languages, preferred:
C#
Python
Java
Scala
Experience with tabular modeling within Microsoft Fabric, Power BI, or Azure Analysis Services, preferred
Experience with Git and DevOps deployment technologies, preferred
Experience with Linux, preferred
Experience with one (1) or more of the following, preferred:
Data Lake Medallion Architecture
Batch and/or streaming data ingestion into a data lake
AI Algorithms/Machine Learning
Automation tools such as UiPath, Alteryx, etc.
Computer Vision based AI technologies
Other Knowledge, Skills & Abilities
Ability to work with a high degree of professionalism and autonomy
Excellent verbal and written communication skills
Solid organizational skills, especially the ability to meet project deadlines with a focus on details
Ability to successfully multi-task while working independently or within a group environment
Ability to work in a deadline-driven environment, and handle multiple projects simultaneously
Ability to interact effectively with people at all organizational levels of the Firm
Ability to effectively interact with a team of professionals and delegating work assignments, as needed
Ability to build and maintain strong relationships with internal and client personnel
Ability to encourage a team environment on engagements, and contribute to the professional development of assigned personnel
Keywords:
Data Analytics, Business Intelligence, BI, Synapse, IoT, Machine Learning, Data Lake, Stream, Cube, Microsoft, SQL Server, Tableau, .Net, C#, Qlik, Power BI, Machine Learning, Azure Data Factory, RedShift, UiPath, Cloud, RPA, AWS, Redshift, Kinesis, QuickSight, SageMaker, S3, Databricks, AWS Lake Formation, Snowflake, Python, Qlik, Athena, Data Pipeline, Glue, Star Schema, Data Modeling, SQL, SSIS, SSAS, SSRS, PySpark, Microsoft Fabric, dbt, Linux, Terraform, Bicep, Data Ops, Purview, Git, Delta, Pandas, Spark SQL
Individual salaries that are offered to a candidate are determined after consideration of numerous factors including but not limited to the candidate’s qualifications, experience, skills, and geography.
California Range: $111,000 - $152,000
Colorado Range: $111,000 - $152,000
New York City/ Valhalla Range: $111,000 - $152,000
Washington Range: $111,000 - $152,000
About Us
BDO delivers assurance, tax, digital technology solutions and financial advisory services to clients throughout the country and around the globe. We offer numerous industry-specific practices, world-class resources, and an unparalleled commitment to meeting our clients’ needs. We currently serve more than 400 publicly traded domestic and international clients.
Unparalleled partner-involvement
Deep industry knowledge and participation
Geographic coverage across the U.S.
Cohesive global network
Focused capabilities across disciplines
BDO brings world-class resources and exceptional service to each and every one of our clients. BDO USA is a member of BDO International, the world’s fifth largest accounting network.
BDO offers a competitive Total Rewards package that encompass so much more than – “traditional benefits”. Our wide range of rewards and our employees’ ability to customize rewards to their individual needs are two of the reasons why BDO has been honored with so many workplace awards, including 100 Best Companies for Working Parents, Working Mother 100 Best Companies, Top Entry Level Employer, 2022 National Best & Brightest Companies to Work For and more.
Some Examples Of Our Total Rewards Offerings Include
Competitive pay and eligibility for an annual performance bonus.
A 401k plan plus an employer match
Comprehensive, medical, dental, vision, FSA, and prescription insurance from day one
Competitive Paid Time Off with daily accrual from day one of employment, plus paid holidays
Paid Parental Leave
Adoption Assistance
Firm paid life insurance
Wellness programs
Additional offerings include BDO Flex, Group Legal insurance, Pet insurance and Long-Term Care Insurance
Above offerings may be subject to eligibility requirements.
Click here to find out more!
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability or protected veteran status.
""BDO USA, P.A. is an EO employer M/F/Veteran/Disability""
Show more
Show less","Data Analytics, Business Intelligence, Machine Learning, Artificial Intelligence, Cloud Computing, Data Warehousing, Data Modeling, Semantic Model Definition, Star Schema Construction, SQL, Data Definition Language (DDL), Data Manipulation Language (DML), Views, Functions, Stored Procedures, Performance Tuning, Azure, AWS, C#, Python, Java, Scala, Microsoft Fabric, Power BI, Azure Analysis Services, Git, DevOps, Linux, Data Lake Medallion Architecture, Batch Data Ingestion, Streaming Data Ingestion, AI Algorithms, Automation Tools, Computer Vision","data analytics, business intelligence, machine learning, artificial intelligence, cloud computing, data warehousing, data modeling, semantic model definition, star schema construction, sql, data definition language ddl, data manipulation language dml, views, functions, stored procedures, performance tuning, azure, aws, c, python, java, scala, microsoft fabric, power bi, azure analysis services, git, devops, linux, data lake medallion architecture, batch data ingestion, streaming data ingestion, ai algorithms, automation tools, computer vision","ai algorithms, artificial intelligence, automation tools, aws, azure, azure analysis services, batch data ingestion, business intelligence, c, cloud computing, computer vision, data definition language ddl, data lake medallion architecture, data manipulation language dml, dataanalytics, datamodeling, datawarehouse, devops, functions, git, java, linux, machine learning, microsoft fabric, performance tuning, powerbi, python, scala, semantic model definition, sql, star schema construction, stored procedures, streaming data ingestion, views"
Data Application Developer,Advantis Global,"Denver, CO",https://www.linkedin.com/jobs/view/data-application-developer-at-advantis-global-3520460432,2023-12-17,Colorado,United States,Mid senior,Hybrid,"Overview
Looking for a blend of ETL/ELT experience along with Business Intelligence tools that make this a well rounded candidate that knows how to work with data.
This is perhaps coined as a data engineer or perhaps a “full-stack” data developer as they are able to take data from source systems all the way through to building reports, dashboards and visualizations that leverage existing industry standard software. Solutions may be involved in include build cloud-based serverless data pipelines, automating metadata driven orchestrated data flows, automated data product and service generation, integrations with open source and proprietary SaaS APIs, and semantic feature generation.
Platform Technology
Web Tech Skills: Node, JS Frameworks (e.g. Angular 2+, React, Vue) HTML5, CSS, Responsive Design,
Programming Languages: Javascript, Python, Java,C++/C#
Geospatial Tech Skills:
Team Tools: Git, Jira (or Rally or similar)
Database Tech Skills: SQL, RDBMS, NoSQL, PostgreSQL, mongodb
Cloud Tech Skills: Understands basic cloud security (ssh keys, IAM), understands basic networking (firewalls, routing, ports), experience with database as a service and virtual machine instances
Design Experience: Understand basic data modeling and data structures for building data warehouses and data marts
Required
Skill Requirements
Coding Language: ES6 (Javascript) and/or Python
Database Technology: SQL
Cloud Experience: One of GCP, AWS or Azure
Tools and Software: Power BI, Qlik Sense or Tableau
Preferred
NodeJS libraries: Express, React, D3, jQuery
Coding skills: Unit Testing (mocha), ESLint
Cloud Experience: GCP (GCS, Cloud Functions, DataFlow, Pub/Sub), AWS (EC2, S3, Lambda, SNS/SQS)
Tools and Software: Git, Elastic Search, Kubernetes, Docker, Power BI/Tableau
Software Engineering: Asynchronous, Functional Programming, Object Oriented Programming, Data Modeling, Interaction Diagram, Machine Learning
Job Requirements
5 years min. experience (Real World, not only Bootcamp)
Bachelor’s Degree min. education in Computer Science or Mathematics with Minor in Comp Sci.
Must be able to pass basic NACI, background, drug and reference checks
No Relocation Assistance, must be local.
Travel None to Rare/In-State
Position Scope
Data Engineering (50%)
Use an agile-based, hybrid, project management approach that focuses on the understanding, capture, and documentation of business processes and associated requirements prior-to and during application development activities.
Business Intelligence (50%)
Develop Dashboards, Data Management Tools, Mission Data Management applications
Develop interfaces into existing systems that provide a user friendly and streamlined workflow into managing and maintaining data across disparate systems.
Support other team members in demonstrating integration capabilities and possibilities with application prototypes
Job Goal
First 30 days: Ramp-up on existing software platform, setup access, support lead in current tasks.
First 90 days: Support the project by improving the applications based on team lead’s direction. Involved in PoCs and rapid research.
First 180 days: Can implement code additions/changes on existing development on own volition. Evaluating software and creating assessments.
First Year Job Goal: Fully capable of identifying issues and resolution without depending on other staff. Can make good design decisions with the product and know how to support existing clients technical issues.
Minimum Key Leadership or Management Competencies:
Highly Coachable - Not looking for a know-it-all, but someone who is willing to learn just-in-time, adjust, receive coaching, and roll with the ever changing needs of the product's development life cycle.
Client relationship experience - Shows understanding, enthusiasm, courtesy, tact, and follows the Xentity methodology: Valued Output. Less Sooner. Checkpoint Often. Timely Iterations.
Sets well-defined and realistic personal goals; displays a high level of initiative, effort, and commitment towards completing assignments in a timely manner; works with minimal supervision; is motivated to achieve; demonstrates responsible behavior.
Must be able to rapidly context switch across products we maintain.
Benefits
We emphasize a balance of work and life and target 40-50 hour weeks with ample time to refresh with great paid-time off.
Salary & Bonus Programs - Competitive Salary. Multiple Recognition and Rewards Bonus Programs (Performance Bonus plan reviewed twice annually - total ranging from 2-5% of salary and Business Development Bonus Plan, Employee Referral Bonus Plan, and Company Profit Sharing Plan).
Paid Time off - Self Managed Time Off Including 11 Paid Holidays
Medical Insurance - Coverage for Major Medical and Surgical, Medical Health Care, Dependents’ Health Care with 100% of employees or 80% employee and 50% family. Options to enroll in Dental Insurance, Vision Discount Program, Prescription Discount Program, Group Term Life Insurance, Accidental Death & Dismemberment Insurance, and Professional insurance advisors to guide employees through these benefits as needed.
Solid, managed retirement savings plan including - Multiple 401(k) funds with traditional and Roth options, Company paid fees, Company Match, Third-party Trust Management with personalized retirement portfolio web analysis tools.
About Xentity
We are a fast-growing data consulting and support services firm - focused on large data programs in data types such as geospatial, open, big, and IoT data. Check out www.xentity.com to learn about our focus, services, clients, missions, values as well as our excellent benefits package and career information.
We have high profile clients and projects that our staff really deliver for, as well enjoy knowing they make a large difference.
Xentity is an EEOC (Equal Employment Opportunity Commission) company and SBA certified small disadvantaged business
Show more
Show less","ETL/ELT, Business Intelligence, Cloudbased serverless data pipelines, Automated metadata driven orchestrated data flows, Automated data product and service generation, Open source and proprietary SaaS APIs, Semantic feature generation, Node.js, Javascript frameworks (Angular 2+ React Vue), HTML5, CSS, Responsive design, Javascript, Python, Java, C++/C#, Git, Jira (or Rally or similar), SQL, RDBMS, NoSQL, PostgreSQL, MongoDB, Cloud security (ssh keys IAM), Networking (firewalls routing ports), Database as a service, Virtual machine instances, Data modeling, Data structures, Data warehouses, Data marts, ES6 (Javascript), Unit testing (mocha), ESLint, GCP (GCS Cloud Functions DataFlow Pub/Sub), AWS (EC2 S3 Lambda SNS/SQS), Elastic Search, Kubernetes, Docker, Power BI/Tableau, Software Engineering, Asynchronous programming, Functional programming, Object oriented programming, Data modeling, Interaction diagram, Machine learning","etlelt, business intelligence, cloudbased serverless data pipelines, automated metadata driven orchestrated data flows, automated data product and service generation, open source and proprietary saas apis, semantic feature generation, nodejs, javascript frameworks angular 2 react vue, html5, css, responsive design, javascript, python, java, cc, git, jira or rally or similar, sql, rdbms, nosql, postgresql, mongodb, cloud security ssh keys iam, networking firewalls routing ports, database as a service, virtual machine instances, data modeling, data structures, data warehouses, data marts, es6 javascript, unit testing mocha, eslint, gcp gcs cloud functions dataflow pubsub, aws ec2 s3 lambda snssqs, elastic search, kubernetes, docker, power bitableau, software engineering, asynchronous programming, functional programming, object oriented programming, data modeling, interaction diagram, machine learning","asynchronous programming, automated data product and service generation, automated metadata driven orchestrated data flows, aws ec2 s3 lambda snssqs, business intelligence, cc, cloud security ssh keys iam, cloudbased serverless data pipelines, css, data marts, data structures, data warehouses, database as a service, datamodeling, docker, elastic search, es6 javascript, eslint, etlelt, functional programming, gcp gcs cloud functions dataflow pubsub, git, html5, interaction diagram, java, javascript, javascript frameworks angular 2 react vue, jira or rally or similar, kubernetes, machine learning, mongodb, networking firewalls routing ports, nodejs, nosql, object oriented programming, open source and proprietary saas apis, postgresql, power bitableau, python, rdbms, responsive design, semantic feature generation, software engineering, sql, unit testing mocha, virtual machine instances"
Senior Cloud Data Engineer,BDO USA,"Boulder, CO",https://www.linkedin.com/jobs/view/senior-cloud-data-engineer-at-bdo-usa-3765469449,2023-12-17,Colorado,United States,Mid senior,Hybrid,"Job Description
Job Summary:
This position will work with cutting edge technology, deliver high quality solutions across various industries, and oversee team(s) on engagements that range in size and scope. This position will receive continuous career development opportunities, given the size and potential of client engagements. This role will perform hands-on delivery of data analytics projects, contributing to the development and unit testing of solutions.
Job Duties
Designs and implements best in class data ingestion strategies, data warehouse and data mart structures, semantic layers and models, visualizations, streaming processes, API integrations, and automation (RPA) solutions for end-to-end data analytics solutions on primarily, but not limited to, cloud analytics platforms such as Azure and AWS
Listens to client needs to align solution with business requirements and delivery schedule
Creates written functional and technical designs
Participates in project status and stand meetings, and assists with providing aggregated project status for project and program managers
Assists with SLA compliance of solutions, and performs performance tuning and optimization efforts of end-to-end solutions
Writes code using multiple languages and correctly applies frameworks, architectural patterns, and software development principles
Delivers high-performance, scalable, repeatable, and secure deliverables with broad impact (high throughput and low latency)
Assists with implementation of data governance programs and best practices
Performs the cleaning and transforming of data from source systems into analytics models
Implements models to support data visualizations and integrations
Assists with implementing DevOps, DataOps and MLOps methodologies on projects
Writes custom integration logic in applicable programming languages
Assists project managers with work breakdown structure creation, project estimation, resource staffing, workload planning and adjustments throughout the project lifecycle
Assists clients with licensing, security, and cost estimation of solutions
Performs code reviews to ensure adherence to standards
Works directly with clients and team members to establish secure data analytics platforms and infrastructure
Contributes to successful deployments of developed solutions and integration of DevOps tools
Maintains a broad and current understanding of data analytics and business intelligence strategies, cloud platforms, methodologies, and tools
Builds client relationships during project execution, effectively becoming a trusted advisor of the client
Participates in support activities for existing software solutions
Other duties as assigned
Supervisory Responsibilities
Supervises the day-to-day workload of Associates on assigned engagements to ensure that timelines and deliverables are met, and reviews work product
Education
Qualifications, Knowledge, Skills and Abilities:
High School Diploma or GED equivalent, required
Bachelor’s degree, preferred; focus in Information Systems, Data Science or Computer Science, preferred
Experience
Five (5) or more years of experience within Data Analytics, Business Intelligence, Artificial Intelligence, or Application Development, required
One (1) or more years of experience technically leading development projects, preferred
One (1) or more years of consulting experience or implementation of cloud-based data analytics solutions, preferred
Software
Strong SQL skills including Data Definition Language (DDL), Data Manipulation Language (DML), views, functions, stored procedures, or performance tuning, required
Experience with Data Warehousing, Data Modeling, Semantic Model Definition or Star Schema Construction, required
Hands on delivery experience of end-to-end cloud data analytics solutions within Azure or AWS, preferred
Experience with one (1) or more of the following computer languages, preferred:
C#
Python
Java
Scala
Experience with tabular modeling within Microsoft Fabric, Power BI, or Azure Analysis Services, preferred
Experience with Git and DevOps deployment technologies, preferred
Experience with Linux, preferred
Experience with one (1) or more of the following, preferred:
Data Lake Medallion Architecture
Batch and/or streaming data ingestion into a data lake
AI Algorithms/Machine Learning
Automation tools such as UiPath, Alteryx, etc.
Computer Vision based AI technologies
Other Knowledge, Skills & Abilities
Ability to work with a high degree of professionalism and autonomy
Excellent verbal and written communication skills
Solid organizational skills, especially the ability to meet project deadlines with a focus on details
Ability to successfully multi-task while working independently or within a group environment
Ability to work in a deadline-driven environment, and handle multiple projects simultaneously
Ability to interact effectively with people at all organizational levels of the Firm
Ability to effectively interact with a team of professionals and delegating work assignments, as needed
Ability to build and maintain strong relationships with internal and client personnel
Ability to encourage a team environment on engagements, and contribute to the professional development of assigned personnel
Keywords:
Data Analytics, Business Intelligence, BI, Synapse, IoT, Machine Learning, Data Lake, Stream, Cube, Microsoft, SQL Server, Tableau, .Net, C#, Qlik, Power BI, Machine Learning, Azure Data Factory, RedShift, UiPath, Cloud, RPA, AWS, Redshift, Kinesis, QuickSight, SageMaker, S3, Databricks, AWS Lake Formation, Snowflake, Python, Qlik, Athena, Data Pipeline, Glue, Star Schema, Data Modeling, SQL, SSIS, SSAS, SSRS, PySpark, Microsoft Fabric, dbt, Linux, Terraform, Bicep, Data Ops, Purview, Git, Delta, Pandas, Spark SQL
Individual salaries that are offered to a candidate are determined after consideration of numerous factors including but not limited to the candidate’s qualifications, experience, skills, and geography.
California Range: $111,000 - $152,000
Colorado Range: $111,000 - $152,000
New York City/ Valhalla Range: $111,000 - $152,000
Washington Range: $111,000 - $152,000
About Us
BDO delivers assurance, tax, digital technology solutions and financial advisory services to clients throughout the country and around the globe. We offer numerous industry-specific practices, world-class resources, and an unparalleled commitment to meeting our clients’ needs. We currently serve more than 400 publicly traded domestic and international clients.
Unparalleled partner-involvement
Deep industry knowledge and participation
Geographic coverage across the U.S.
Cohesive global network
Focused capabilities across disciplines
BDO brings world-class resources and exceptional service to each and every one of our clients. BDO USA is a member of BDO International, the world’s fifth largest accounting network.
BDO offers a competitive Total Rewards package that encompass so much more than – “traditional benefits”. Our wide range of rewards and our employees’ ability to customize rewards to their individual needs are two of the reasons why BDO has been honored with so many workplace awards, including 100 Best Companies for Working Parents, Working Mother 100 Best Companies, Top Entry Level Employer, 2022 National Best & Brightest Companies to Work For and more.
Some Examples Of Our Total Rewards Offerings Include
Competitive pay and eligibility for an annual performance bonus.
A 401k plan plus an employer match
Comprehensive, medical, dental, vision, FSA, and prescription insurance from day one
Competitive Paid Time Off with daily accrual from day one of employment, plus paid holidays
Paid Parental Leave
Adoption Assistance
Firm paid life insurance
Wellness programs
Additional offerings include BDO Flex, Group Legal insurance, Pet insurance and Long-Term Care Insurance
Above offerings may be subject to eligibility requirements.
Click here to find out more!
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability or protected veteran status.
""BDO USA, P.A. is an EO employer M/F/Veteran/Disability""
Show more
Show less","Data Analytics, Business Intelligence, Data Warehousing, Data Modeling, Cloud Data Analytics, SQL, Data Definition Language (DDL), Data Manipulation Language (DML), Views, Functions, Stored Procedures, Performance Tuning, Semantic Model Definition, Star Schema Construction, C#, Python, Java, Scala, Tabular Modeling, Microsoft Fabric, Power BI, Azure Analysis Services, Git, DevOps Deployment Technologies, Linux, Data Lake Medallion Architecture, Batch Data Ingestion, Streaming Data Ingestion, Data Lake, AI Algorithms, Machine Learning, Automation Tools, UiPath, Alteryx, Computer Vision, Kubernetes, Terraform, Apache Spark, Apache Airflow, Hadoop, Hive, Kafka, Cassandra, NoSQL, Spark SQL, Pandas, Databricks, Glue, Athena, Star Schema, Data Pipeline, SSIS, SSAS, SSRS, PySpark, Delta, Redshift, QuickSight, Kinesis, SageMaker, S3, Snowflake, Synapse, IoT","data analytics, business intelligence, data warehousing, data modeling, cloud data analytics, sql, data definition language ddl, data manipulation language dml, views, functions, stored procedures, performance tuning, semantic model definition, star schema construction, c, python, java, scala, tabular modeling, microsoft fabric, power bi, azure analysis services, git, devops deployment technologies, linux, data lake medallion architecture, batch data ingestion, streaming data ingestion, data lake, ai algorithms, machine learning, automation tools, uipath, alteryx, computer vision, kubernetes, terraform, apache spark, apache airflow, hadoop, hive, kafka, cassandra, nosql, spark sql, pandas, databricks, glue, athena, star schema, data pipeline, ssis, ssas, ssrs, pyspark, delta, redshift, quicksight, kinesis, sagemaker, s3, snowflake, synapse, iot","ai algorithms, alteryx, apache airflow, apache spark, athena, automation tools, azure analysis services, batch data ingestion, business intelligence, c, cassandra, cloud data analytics, computer vision, data definition language ddl, data lake, data lake medallion architecture, data manipulation language dml, data pipeline, dataanalytics, databricks, datamodeling, datawarehouse, delta, devops deployment technologies, functions, git, glue, hadoop, hive, iot, java, kafka, kinesis, kubernetes, linux, machine learning, microsoft fabric, nosql, pandas, performance tuning, powerbi, python, quicksight, redshift, s3, sagemaker, scala, semantic model definition, snowflake, spark, spark sql, sql, ssas, ssis, ssrs, star schema, star schema construction, stored procedures, streaming data ingestion, synapse, tabular modeling, terraform, uipath, views"
Senior Cloud Data Engineer,BDO USA,"Greeley, CO",https://www.linkedin.com/jobs/view/senior-cloud-data-engineer-at-bdo-usa-3765469448,2023-12-17,Colorado,United States,Mid senior,Hybrid,"Job Description
Job Summary:
This position will work with cutting edge technology, deliver high quality solutions across various industries, and oversee team(s) on engagements that range in size and scope. This position will receive continuous career development opportunities, given the size and potential of client engagements. This role will perform hands-on delivery of data analytics projects, contributing to the development and unit testing of solutions.
Job Duties
Designs and implements best in class data ingestion strategies, data warehouse and data mart structures, semantic layers and models, visualizations, streaming processes, API integrations, and automation (RPA) solutions for end-to-end data analytics solutions on primarily, but not limited to, cloud analytics platforms such as Azure and AWS
Listens to client needs to align solution with business requirements and delivery schedule
Creates written functional and technical designs
Participates in project status and stand meetings, and assists with providing aggregated project status for project and program managers
Assists with SLA compliance of solutions, and performs performance tuning and optimization efforts of end-to-end solutions
Writes code using multiple languages and correctly applies frameworks, architectural patterns, and software development principles
Delivers high-performance, scalable, repeatable, and secure deliverables with broad impact (high throughput and low latency)
Assists with implementation of data governance programs and best practices
Performs the cleaning and transforming of data from source systems into analytics models
Implements models to support data visualizations and integrations
Assists with implementing DevOps, DataOps and MLOps methodologies on projects
Writes custom integration logic in applicable programming languages
Assists project managers with work breakdown structure creation, project estimation, resource staffing, workload planning and adjustments throughout the project lifecycle
Assists clients with licensing, security, and cost estimation of solutions
Performs code reviews to ensure adherence to standards
Works directly with clients and team members to establish secure data analytics platforms and infrastructure
Contributes to successful deployments of developed solutions and integration of DevOps tools
Maintains a broad and current understanding of data analytics and business intelligence strategies, cloud platforms, methodologies, and tools
Builds client relationships during project execution, effectively becoming a trusted advisor of the client
Participates in support activities for existing software solutions
Other duties as assigned
Supervisory Responsibilities
Supervises the day-to-day workload of Associates on assigned engagements to ensure that timelines and deliverables are met, and reviews work product
Education
Qualifications, Knowledge, Skills and Abilities:
High School Diploma or GED equivalent, required
Bachelor’s degree, preferred; focus in Information Systems, Data Science or Computer Science, preferred
Experience
Five (5) or more years of experience within Data Analytics, Business Intelligence, Artificial Intelligence, or Application Development, required
One (1) or more years of experience technically leading development projects, preferred
One (1) or more years of consulting experience or implementation of cloud-based data analytics solutions, preferred
Software
Strong SQL skills including Data Definition Language (DDL), Data Manipulation Language (DML), views, functions, stored procedures, or performance tuning, required
Experience with Data Warehousing, Data Modeling, Semantic Model Definition or Star Schema Construction, required
Hands on delivery experience of end-to-end cloud data analytics solutions within Azure or AWS, preferred
Experience with one (1) or more of the following computer languages, preferred:
C#
Python
Java
Scala
Experience with tabular modeling within Microsoft Fabric, Power BI, or Azure Analysis Services, preferred
Experience with Git and DevOps deployment technologies, preferred
Experience with Linux, preferred
Experience with one (1) or more of the following, preferred:
Data Lake Medallion Architecture
Batch and/or streaming data ingestion into a data lake
AI Algorithms/Machine Learning
Automation tools such as UiPath, Alteryx, etc.
Computer Vision based AI technologies
Other Knowledge, Skills & Abilities
Ability to work with a high degree of professionalism and autonomy
Excellent verbal and written communication skills
Solid organizational skills, especially the ability to meet project deadlines with a focus on details
Ability to successfully multi-task while working independently or within a group environment
Ability to work in a deadline-driven environment, and handle multiple projects simultaneously
Ability to interact effectively with people at all organizational levels of the Firm
Ability to effectively interact with a team of professionals and delegating work assignments, as needed
Ability to build and maintain strong relationships with internal and client personnel
Ability to encourage a team environment on engagements, and contribute to the professional development of assigned personnel
Keywords:
Data Analytics, Business Intelligence, BI, Synapse, IoT, Machine Learning, Data Lake, Stream, Cube, Microsoft, SQL Server, Tableau, .Net, C#, Qlik, Power BI, Machine Learning, Azure Data Factory, RedShift, UiPath, Cloud, RPA, AWS, Redshift, Kinesis, QuickSight, SageMaker, S3, Databricks, AWS Lake Formation, Snowflake, Python, Qlik, Athena, Data Pipeline, Glue, Star Schema, Data Modeling, SQL, SSIS, SSAS, SSRS, PySpark, Microsoft Fabric, dbt, Linux, Terraform, Bicep, Data Ops, Purview, Git, Delta, Pandas, Spark SQL
Individual salaries that are offered to a candidate are determined after consideration of numerous factors including but not limited to the candidate’s qualifications, experience, skills, and geography.
California Range: $111,000 - $152,000
Colorado Range: $111,000 - $152,000
New York City/ Valhalla Range: $111,000 - $152,000
Washington Range: $111,000 - $152,000
About Us
BDO delivers assurance, tax, digital technology solutions and financial advisory services to clients throughout the country and around the globe. We offer numerous industry-specific practices, world-class resources, and an unparalleled commitment to meeting our clients’ needs. We currently serve more than 400 publicly traded domestic and international clients.
Unparalleled partner-involvement
Deep industry knowledge and participation
Geographic coverage across the U.S.
Cohesive global network
Focused capabilities across disciplines
BDO brings world-class resources and exceptional service to each and every one of our clients. BDO USA is a member of BDO International, the world’s fifth largest accounting network.
BDO offers a competitive Total Rewards package that encompass so much more than – “traditional benefits”. Our wide range of rewards and our employees’ ability to customize rewards to their individual needs are two of the reasons why BDO has been honored with so many workplace awards, including 100 Best Companies for Working Parents, Working Mother 100 Best Companies, Top Entry Level Employer, 2022 National Best & Brightest Companies to Work For and more.
Some Examples Of Our Total Rewards Offerings Include
Competitive pay and eligibility for an annual performance bonus.
A 401k plan plus an employer match
Comprehensive, medical, dental, vision, FSA, and prescription insurance from day one
Competitive Paid Time Off with daily accrual from day one of employment, plus paid holidays
Paid Parental Leave
Adoption Assistance
Firm paid life insurance
Wellness programs
Additional offerings include BDO Flex, Group Legal insurance, Pet insurance and Long-Term Care Insurance
Above offerings may be subject to eligibility requirements.
Click here to find out more!
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability or protected veteran status.
""BDO USA, P.A. is an EO employer M/F/Veteran/Disability""
Show more
Show less","Data Analytics, Business Intelligence, Data Warehousing, Data Modeling, Semantic Model Definition, Star Schema Construction, Cloud Data Analytics, SQL, Data Definition Language (DDL), Data Manipulation Language (DML), Views, Functions, Stored Procedures, Performance Tuning, C#, Python, Java, Scala, Tabular Modeling, Microsoft Fabric, Power BI, Azure Analysis Services, Git, DevOps, Linux, Data Lake Medallion Architecture, Batch and/or streaming data ingestion, AI Algorithms, Machine Learning, Automation Tools, UiPath, Alteryx, Computer Vision, AWS, Redshift, Kinesis, QuickSight, SageMaker, S3, Databricks, AWS Lake Formation, Snowflake, Qlik, Athena, Data Pipeline, Glue, SSIS, SSAS, SSRS, PySpark, dbt, Terraform, Bicep, Data Ops, Purview, Delta, Pandas, Spark SQL","data analytics, business intelligence, data warehousing, data modeling, semantic model definition, star schema construction, cloud data analytics, sql, data definition language ddl, data manipulation language dml, views, functions, stored procedures, performance tuning, c, python, java, scala, tabular modeling, microsoft fabric, power bi, azure analysis services, git, devops, linux, data lake medallion architecture, batch andor streaming data ingestion, ai algorithms, machine learning, automation tools, uipath, alteryx, computer vision, aws, redshift, kinesis, quicksight, sagemaker, s3, databricks, aws lake formation, snowflake, qlik, athena, data pipeline, glue, ssis, ssas, ssrs, pyspark, dbt, terraform, bicep, data ops, purview, delta, pandas, spark sql","ai algorithms, alteryx, athena, automation tools, aws, aws lake formation, azure analysis services, batch andor streaming data ingestion, bicep, business intelligence, c, cloud data analytics, computer vision, data definition language ddl, data lake medallion architecture, data manipulation language dml, data ops, data pipeline, dataanalytics, databricks, datamodeling, datawarehouse, dbt, delta, devops, functions, git, glue, java, kinesis, linux, machine learning, microsoft fabric, pandas, performance tuning, powerbi, purview, python, qlik, quicksight, redshift, s3, sagemaker, scala, semantic model definition, snowflake, spark, spark sql, sql, ssas, ssis, ssrs, star schema construction, stored procedures, tabular modeling, terraform, uipath, views"
"Senior Systems Test, Data Analysis",NVIDIA,"New Jersey, United States",https://www.linkedin.com/jobs/view/senior-systems-test-data-analysis-at-nvidia-3759877526,2023-12-17,Bordentown,United States,Mid senior,Remote,"The Automotive Vehicles team is searching for a creative and experienced Systems Engineers to help us bring NVIDIA's autonomous vehicle solution out to the world. You will participate in a focused effort to develop and productize ground-breaking solutions that will redefine the world of transportation and the growing field of self-driving cars. You will work with hardworking and dedicated multi-functional engineering development teams across various vehicle subsystems to integrate their work into our AV SW platform, while achieving or exceeding all meaningful NVIDIA and automotive standards & guidelines. You'll find the work is exciting, fun, and relevant. We have deadlines, customers, and competition.
We are leading the technology revolution, shaping the future of transportation through our commitment to developing self-driving vehicles! As a data analyst/engineer, your primary role will be to construct robust and scalable data infrastructure that will enable us to track, analyze and validate our self-driving technology using billions of data points.
What You Will Be Doing
Design, develop, and maintain data infrastructure that supports large-scale, complex analysis of autonomous driving systems.
Develop and enhance tools that automate the data analysis and KPI reporting process, fostering a culture of data-driven decision making across the company.
Create robust and sharable dashboards and reports using data visualization tools.
Collaborate across engineering, data science, testing, and release teams to identify needs, build and scale analytical solutions to address gaps and reduce risks to the project.
Contribute to improve the overall process for improving quality of products.
Identify, troubleshoot, and resolve system-level issues that affect the performance and reliability of our data infrastructure.
What We Need To See
BS (or equivalent experience), MS, or PhD in Engineering, Computer Science, or related field.
6+ years of data engineering and analysis experience, preferably in the field of simulation, autonomous systems, robotics, artificial intelligence or related areas.
Deep technical proficiency with standard data analytics tools such as Python (Pandas, Jupyter Notebooks, Python-based visualization, etc.) and SQL.
Experience with writing clean and maintainable code in a collaborative environment.
Strong understanding of scalable data modeling, ETL processes, and data visualization techniques.
Experience with distributed computing and cloud platforms.
Excellent problem-solving, analytical skills, and technical troubleshooting abilities.
Strong communication and collaboration skills.
Demonstrated ability to perform in situations with high ambiguity and competing priorities.
Curiosity and a desire to “dive deep” into data to derive insights.
Ways To Stand Out From The Crowd
Experience with designing KPIs and dashboards for complex systems such robotics
Knowledge of AI/ML systems, or a background in working with AI-native systems.
Experience in large-scale data engineering for hardware or software validation of any robotics solution.
The base salary range is 176,000 USD - 333,500 USD. Your base salary will be determined based on your location, experience, and the pay of employees in similar positions.
You will also be eligible for equity and benefits .
NVIDIA accepts applications on an ongoing basis.
NVIDIA is committed to fostering a diverse work environment and proud to be an equal opportunity employer. As we highly value diversity in our current and future employees, we do not discriminate (including in our hiring and promotion practices) on the basis of race, religion, color, national origin, gender, gender expression, sexual orientation, age, marital status, veteran status, disability status or any other characteristic protected by law.#deeplearning
Show more
Show less","Autonomous Vehicles, SW Platform, NVIDIA standards & guidelines, Systems Engineering, Scalable Data Infrastructure, Data Analysis, Data Validation, Datadriven Decision Making, Data Visualization, Data Analytics Tools, Python, Pandas, Jupyter Notebooks, SQL, Clean Code, Scalable Data Modeling, ETL Processes, Distributed Computing, Cloud Platforms, AI/ML Systems, Hardware/Software Validation, Complex Systems, Robotics, KPIs, Dashboards","autonomous vehicles, sw platform, nvidia standards guidelines, systems engineering, scalable data infrastructure, data analysis, data validation, datadriven decision making, data visualization, data analytics tools, python, pandas, jupyter notebooks, sql, clean code, scalable data modeling, etl processes, distributed computing, cloud platforms, aiml systems, hardwaresoftware validation, complex systems, robotics, kpis, dashboards","aiml systems, autonomous vehicles, clean code, cloud platforms, complex systems, dashboard, data analytics tools, data validation, dataanalytics, datadriven decision making, distributed computing, etl, hardwaresoftware validation, jupyter notebooks, kpis, nvidia standards guidelines, pandas, python, robotics, scalable data infrastructure, scalable data modeling, sql, sw platform, systems engineering, visualization"
Senior Data Platform Engineer - Agency Temp,SiriusXM,"Lawrenceville, NJ",https://www.linkedin.com/jobs/view/senior-data-platform-engineer-agency-temp-at-siriusxm-3732066984,2023-12-17,Bordentown,United States,Mid senior,Hybrid,"Who We Are
SiriusXM and its brands (Pandora, SXM Media, AdsWizz, Simplecast, and SiriusXM Connected Vehicle Services) are leading a new era of audio entertainment and services by delivering the most compelling subscription and ad-supported audio entertainment experience for listeners -- in the car, at home, and anywhere on the go with connected devices. Our vision is to shape the future of audio, where everyone can be effortlessly connected to the voices, stories and music they love wherever they are. This is the place where a diverse group of emerging talent and legends alike come to share authentic and purposeful songs, stories, sounds and insights through some of the best programming and technology in the world. Our critically-acclaimed, industry-leading audio entertainment encompasses music, sports, comedy, news, talk, live events, and podcasting. No matter their individual role, each of our employees plays a vital part in bringing SiriusXM’s vision to life every day.
SiriusXM
is the leading audio entertainment company in North America, and the premier programmer and platform for subscription and digital advertising-supported audio products. SiriusXM’s platforms collectively reach approximately 150 million listeners, the largest digital audio audience across paid and free tiers in North America, and deliver music, sports, talk, news, comedy, entertainment and podcasts. Pandora, a subsidiary of SiriusXM, is the largest ad-supported audio entertainment streaming service in the U.S. SiriusXM's subsidiaries Simplecast and AdsWizz make it a leader in podcast hosting, production, distribution, analytics and monetization. The Company’s advertising sales organization, which operates as SXM Media, leverages its scale, cross-platform sales organization and ad tech capabilities to deliver results for audio creators and advertisers. SiriusXM, through Sirius XM Canada Holdings, Inc., also offers satellite radio and audio entertainment in Canada. In addition to its audio entertainment businesses, SiriusXM offers connected vehicle services to automakers.
How You’ll Make An Impact
In this role you will be a member of a team responsible for designing, developing and supporting a data platform which will be used across data organization and other groups.
What You’ll Do
Build cloud-based data platform which supports Datalake, Job Orchestration, ETL template, ETL Compute, integration with third party tools like fivetran, Monte Carlo
Design, code and maintain infrastructure as a code (IaC) using CDK, typescript, CDKTF.
Build and improve workflow orchestration tooling to support efficient data pipelines E.g., airflow plugins, systems integration, deployments.
Strengthen best practices around data platform setup and configuration.
What You’ll Need
BS or MS in Computer Science or related technical field
7+ years’ experience developing infrastructure as Code such as AWS CDK, typescript/or Python.
3+ years of experience working on a cloud platform (ex. GCP, AWS, etc.)
AWS CDK with Typescript as language for CDK development
Working Experience for Job Orchestration tool – Airflow/MWAA
Working Experience/Expertise creating AWS infrastructure for AWS services including but not limited to: S3 Datalake, Kms keys, IAM Role/Policy, MWAA, RDS, Lambda function
Experience/Expertise on Databricks is a plus.
Experience architecting, designing and building infrastructure in AWS.
Experience using GitHub for code PR management.
Working Experience in Linux Operating System
Nice to Have - Knowledge/Expertise on tools viz (Fivetran, Monte-Carlo, Datadog, Tableau/Looker)
Excellent written and verbal communication skills.
Ability to work independently and in a team environment.
Ability to pay attention to details and be organized.
Ability to handle multiple tasks in a fast-paced environment.
Willingness to take initiative and to follow through on projects.
Excellent time management skills, with the ability to prioritize and multi-task, and work under shifting deadlines in a fast-paced environment.
Must have legal right to work in the U.S
Our goal at SiriusXM is to provide and maintain a work environment that fosters mutual respect, professionalism and cooperation. SiriusXM is an equal opportunity employer that does not discriminate on the basis of actual or perceived race, creed, color, religion, national origin, ancestry, alienage or citizenship status, age, disability or handicap, sex, gender identity, marital status, familial status, veteran status, sexual orientation or any other characteristic protected by applicable federal, state or local laws.
The requirements and duties described above may be modified or waived by the Company in its sole discretion without notice.
R-2023-10-27
Show more
Show less","AWS, CDK, Typescript, Airflow, MWAA, S3 Datalake, Kms keys, IAM Role/Policy, RDS, Lambda function, Databricks, GitHub, Linux Operating System, Fivetran, MonteCarlo, Datadog, Tableau/Looker","aws, cdk, typescript, airflow, mwaa, s3 datalake, kms keys, iam rolepolicy, rds, lambda function, databricks, github, linux operating system, fivetran, montecarlo, datadog, tableaulooker","airflow, aws, cdk, databricks, datadog, fivetran, github, iam rolepolicy, kms keys, lambda function, linux operating system, montecarlo, mwaa, rds, s3 datalake, tableaulooker, typescript"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Princeton, NJ",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3759712021,2023-12-17,Bordentown,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto,
San
Francisco or Chicago
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Pandas, R, Python, Java, bash, SQL, Git, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, AWS, GCP, Azure, DynamoDB, Kafka, Storm, SparkStreaming, Machine learning, Data classification, Data retention","pandas, r, python, java, bash, sql, git, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, dynamodb, kafka, storm, sparkstreaming, machine learning, data classification, data retention","airflow, aws, azure, bash, data classification, data retention, docker, dynamodb, gcp, git, helm, java, kafka, kubernetes, machine learning, pandas, python, r, snowflake, spark, sparkstreaming, sql, storm"
Data Engineer,Torch.AI,"Kansas City, KS",https://www.linkedin.com/jobs/view/data-engineer-at-torch-ai-3752887099,2023-12-17,Independence,United States,Associate,Onsite,"Why this Role is Interesting?
There are very few companies in the world with the credentials we have. You’ll have an opportunity to support critically important customer missions and work with paradigm-changing technology.
You’ll be supporting one of the largest programs and customers in the company, offering significant visibility of your impact.
The role is critical in positioning Torch.AI as a leader in data infrastructure AI in the market and helps ensure the company is well-positioning for the next stages of growth.
You’ll have the opportunity to showcase your skills as a seasoned data engineer, while also learning from a team of extremely talented and credentialed individuals.
Torch.AI is in the midst of a pivotal, high-growth period, with several initiatives and investments anticipated to fuel hyper-growth across a diverse customer base.
You’ll play a significant roe in strengthening Torch.AI’s reputation as one of Forbes Best Startup Employers.
A Game-Changing AI Company in the Heart of the Midwest
Torch.AI, the Data Infrastructure AI Pioneers™, are headquartered in Kansas City with offices in Washington, DC. We build AI that makes data easier to use by processing data in-flight and radically evolving analytic and operational capabilities in any IT environment. Our Torch Platform instantly unlocks value from data and provides information needed for humans and machines to be more productive. Partnering with U.S. military forces, our solutions and people support a growing array of national defense capabilities with advanced technology.
At Torch.AI, we’re passionate about building software that solves some of the world’s most challenging problems. We have helped our customers enhance top-secret clearances, stop fraud at massive scales, discover new trends and global events, gain an edge in financial markets, and beyond. We are driven by our mission to unlock human potential and serve our clients in valuable and meaningful ways.
The Role
The U.S. Department of Defense relies on complex data every day to drive decisions, support warfighters, mitigate risks to national security, and ensure our global leadership. Data is being generated at an exponential rate and existing solutions don’t address today’s needs.
This Data Engineer role will work in support of developing and implementing solutions for a major U.S. Department of Defense program. The role will engage with cross-functional teams to discover all existing data types and documentation that are currently in use in multiple data producer environments. The successful candidate will be a creative problem solver, diligent and detail oriented, and will have a sharp focus on value creation and mission importance. They will have a keen eye on understanding customer needs, how they currently leverage Torch.AI software, and will help in driving the future success of the customer program.
What Success Looks Like
Participates in design meetings and consults with other staff to evaluate interface between hardware and software, and operational and performance requirements of overall system.
Develop and perform automated builds, testing, and deployments in support of NiFi development.
Work with the development and services teams to support service artifacts, which could include bug fixes, security improvements, functional extension, performance improvement, refactoring, and/or rewriting.
Experience in building data ingestion workflows/pipeline flows using NiFi, NiFi registry, and other Nifi management tools.
Create Artifact that will primarily be Apache Nifi flows and/or custom processors used in Nifi flows.
Create tables in Trino to deliver data via APIs to package and disseminate data to mission partners.
Build data models and analytics to support mission needs.
Pull or receive command and control files.
Test services artifacts for correctness and/or performance.
What We Value
B.S. degree in related field or equivalent combination of training and experience.
5+ years of proven work experience as a data engineer.
Demonstrable experience with Java, Parquet, Nifi, Kafka.
Experience with ETLs and APIs.
Understanding and Experience with Apache Nifi tool.
Analysis of the product to determine the end points as per the requirements.
Well-versed with docker.
Hands on experience on GIT.
Understanding of security products focusing on IAM.
Knowledge on ETL focusing on mapping.
Good to have experience with NiFi-supported scripting languages (Python) and writing regular expressions.
Experience creating custom NiFi processors.
Full Software Development Life Cycle (SDLC) experience.
Work well within a formal team structure and with minimal supervision.
This position REQUIRES an active TS/SCI. TS/SCI with CI Poly is preferred.
Work Environment & Travel Requirements
This job operates in state-of-the-art, professional office environment.
You will be expected to work out of Torch.AI's Leawood, KS office; limited hybrid/remote days may occasionally be available.
Travel: You will be required to travel to Northern Virginia and/or St. Louis on a periodic basis (10-50%) to work in and attend client meetings in a SCIF environment.
Perks & Compensation
Competitive salary, performance bonus, and benefits package. The salary range for this role is commensurate with experience.
Opportunity to participate in Torch.AI’s employee equity grant program.
Unlimited PTO.
In-Office Catering for Lunch Every Monday.
Access to company suite at the T-Mobile Center, with tickets to all major events and concerts.
Amazing professional growth opportunity at a high growth start-up.
Passionate, smart, and fun people to work with.
Excellent medical, dental, and vision insurance.
Life and disability coverage.
Relocation assistance.
11 paid holidays each year.
Torch.AI is an Equal Opportunity /Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, age, sex, national origin, protected veteran status or status as an individual with a disability.
Show more
Show less","Data Engineer, Java, Parquet, NiFi, Kafka, ETLs, APIs, Apache NiFi, Docker, GIT, IAM, NiFisupported scripting languages, Python, Regular Expressions, SDLC, SQL, Trino","data engineer, java, parquet, nifi, kafka, etls, apis, apache nifi, docker, git, iam, nifisupported scripting languages, python, regular expressions, sdlc, sql, trino","apache nifi, apis, dataengineering, docker, etls, git, iam, java, kafka, nifi, nifisupported scripting languages, parquet, python, regular expressions, sdlc, sql, trino"
Lead Data Engineer,Zelis,"Jackson County, MO",https://www.linkedin.com/jobs/view/lead-data-engineer-at-zelis-3767525941,2023-12-17,Independence,United States,Mid senior,Onsite,"Summary
Build High level technical design both for Streaming and batch processing systems
Design and build reusable components, frameworks and libraries at scale to support analytics data products
Perform POCs on new technology, architecture patterns
Design and implement product features in collaboration with business and Technology stakeholders
Anticipate, identify, and solve issues concerning data management to improve data quality
Clean, prepare and optimize data at scale for ingestion and consumption
Drive the implementation of new data management projects and re-structure of the current data architecture
Implement complex automated workflows and routines using workflow scheduling tools
Build continuous integration, test-driven development and production deployment frameworks
Drive collaborative reviews of design, code, test plans and dataset implementation performed by other data engineers in support of maintaining data engineering standards
Analyze and profile data for the purpose of designing scalable solutions
Troubleshoot complex data issues and perform root cause analysis to proactively resolve product and operational issues
Lead, Mentor and develop offshore Data Engineers in adopting best practices and deliver data products.
Partner closely with product management to understand business requirements, breakdown Epics,
Partner with Engineering Managers to define technology roadmaps, align on design, architecture, and enterprise strategy
Requirements
Minimum of 8+ years experience with the following:
Snowflake (Columnar MPP Cloud data warehouse)
DBT (ETL tool)
Python
Experience designing and implementing Data Warehouse
Preferred Skills
Azure/AWS cloud technology
SQL objects (procedures, triggers, views, functions) in SQL Server. SQL query optimizations
Understanding of T-SQL, indexes, stored procedures, triggers, functions, views, etc.
Design and development of Azure/AWS Data Factory Pipelines preferred.
Design and development of data marts in Snowflake preferred
Working knowledge of Azure/AWS Architecture, Data Lake, Data Factory
Business analysis experience to analyze data to write code and drive solutions
Knowledge of: Git, Azure DevOps, Agile, Jira and Confluence
Independence/ Accountability
Requires minimal daily supervision
Receives detailed instruction on new assignments and determines next steps with guidance
Regularly reviews goals and objectives with supervisor
Demonstrates competence in relevant job responsibilities which allows for increasing level of independence
Ability to manage and prioritize multiple tasks
Ability to work under pressure and meet deadlines
Problem Solving
Makes logical suggestions of likely causes of problems and independently suggests solutions
Excellent organizational skills are required to prioritize responsibilities, thus completing work in a timely fashion
Outstanding ability to multiplex tasks as required
Excellent project management and/or business analysis skills.
Attention to detail and concern for impact is essential
As a leading payments company in healthcare, we guide, price, explain, and pay for care on behalf of insurers and their members. We’re Zelis in our pursuit to align the interests of payers, providers, and consumers to deliver a better financial experience and more affordable, transparent care for all. We partner with more than 700 payers, including the top-5 national health plans, BCBS insurers, regional health plans, TPAs and self-insured employers, over 4 million providers, and 100 million members, enabling the healthcare industry to pay for care, with care. Zelis brings adaptive technology, a deeply ingrained service culture, and a comprehensive navigation through adjudication and payment platform to manage the complete payment process.
Commitment to Diversity, Equity, Inclusion, and Belonging
At Zelis, we champion diversity, equity, inclusion, and belonging in all aspects of our operations. We embrace the power of diversity and create an environment where people can bring their authentic and best selves to work. We know that a sense of belonging is key not only to your success at Zelis, but also to your ability to bring your best each day.
Equal Employment Opportunity
Zelis is proud to be an equal opportunity employer. All applicants will receive consideration for employment without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.
We encourage members of traditionally underrepresented communities to apply, even if you do not believe you 100% fit the qualifications of the position, including women, LGBTQIA people, people of color, and people with disabilities.
Accessibility Support
We are dedicated to ensuring our application process is accessible to all candidates. If you are a qualified individual with a disability or a disabled veteran and require a reasonable accommodation with any part of the application and/or interview process, please email TalentAcquisition@zelis.com
SCAM ALERT: There is an active nationwide employment scam which is now using Zelis to garner personal information or financial scams. This site is secure, and any applications made here are with our legitimate partner. If you’re contacted by a Zelis Recruiter, please ensure whomever is contacting you truly represents Zelis Healthcare. We will never asked for the exchange of any money or credit card details during the recruitment process. Please be aware of any suspicious email activity from people who could be pretending to be recruiters or senior professionals at Zelis.
Show more
Show less","High level technical design, Streaming systems, Batch processing systems, Reusable components, Frameworks, Libraries, POCs, Technology, Architecture patterns, Product features, Data management, Data quality, Data cleaning, Data preparation, Data optimization, Data ingestion, Data consumption, Data architecture, Automated workflows, Routines, Workflow scheduling tools, Continuous integration, Testdriven development, Production deployment frameworks, Collaborative reviews, Design, Code, Test plans, Dataset implementation, Data engineering standards, Data analysis, Profiling, Scalable solutions, Troubleshooting, Root cause analysis, Product issues, Operational issues, Data engineering, Best practices, Product management, Business requirements, Epics, Technology roadmaps, Design alignment, Architecture alignment, Enterprise strategy, Snowflake, DBT, Python, Data Warehouse, Azure, AWS, SQL objects, SQL query optimization, TSQL, Stored procedures, Triggers, Functions, Views, Data Factory Pipelines, Data marts, Azure Architecture, Data Lake, Business analysis, Git, Azure DevOps, Agile, Jira, Confluence","high level technical design, streaming systems, batch processing systems, reusable components, frameworks, libraries, pocs, technology, architecture patterns, product features, data management, data quality, data cleaning, data preparation, data optimization, data ingestion, data consumption, data architecture, automated workflows, routines, workflow scheduling tools, continuous integration, testdriven development, production deployment frameworks, collaborative reviews, design, code, test plans, dataset implementation, data engineering standards, data analysis, profiling, scalable solutions, troubleshooting, root cause analysis, product issues, operational issues, data engineering, best practices, product management, business requirements, epics, technology roadmaps, design alignment, architecture alignment, enterprise strategy, snowflake, dbt, python, data warehouse, azure, aws, sql objects, sql query optimization, tsql, stored procedures, triggers, functions, views, data factory pipelines, data marts, azure architecture, data lake, business analysis, git, azure devops, agile, jira, confluence","agile, architecture alignment, architecture patterns, automated workflows, aws, azure, azure architecture, azure devops, batch processing systems, best practices, business analysis, business requirements, code, collaborative reviews, confluence, continuous integration, data architecture, data cleaning, data consumption, data engineering, data engineering standards, data factory pipelines, data ingestion, data lake, data management, data marts, data optimization, data preparation, data quality, dataanalytics, dataset implementation, datawarehouse, dbt, design, design alignment, enterprise strategy, epics, frameworks, functions, git, high level technical design, jira, libraries, operational issues, pocs, product features, product issues, product management, production deployment frameworks, profiling, python, reusable components, root cause analysis, routines, scalable solutions, snowflake, sql objects, sql query optimization, stored procedures, streaming systems, technology, technology roadmaps, test plans, testdriven development, triggers, troubleshooting, tsql, views, workflow scheduling tools"
Senior Data Engineer,Valorem Reply,"Kansas City, MO",https://www.linkedin.com/jobs/view/senior-data-engineer-at-valorem-reply-3754706155,2023-12-17,Independence,United States,Mid senior,Onsite,"Note
This role requires candidates to be US Citizens.
This role requires individuals to be within driving distance from our office locations or willing to relocate (Atlanta, Chicago, Detroit, Kansas City and Philadelphia).
Valorem Reply is an award-winning digital transformation firm focused on delivering data-driven enterprise, IT modernization, customer experience, product transformation and digital workplace. Through the expertise of their people and power of Microsoft technologies, they provide hyper-scale and agile delivery of unique digital business services, strategic business models and design-led user experiences. Their innovative strategies and solutions securely and rapidly transform the way their clients do business.
The Senior Data Engineer will lead the creation of high-value data-driven solutions, leveraging Valorem Reply's proven implementation methodology and solutions for enterprise projects. They will also contribute to technical pre-sales activities as required. The responsibilities include designing solution architecture, defining requirements, and leading the project delivery team. There will be an opportunity to work with and learn about the latest cloud solutions in an exciting work environment. This position will work collaboratively across all of Valorem Reply's sales, service delivery, and account management organizations to serve Valorem Reply's customers.
This position will represent Valorem Reply's approach to advanced data engineering solutions and, as such, must demonstrate proficiency at the architecture level. It will require an understanding of how advanced analytics are positioned to meet business objectives and how data translates to business and enterprise value. The role will also involve implementing the Data Lakehouse solution through people, processes, and technology. This is a hands-on role, leading, coding, and delivering on the most advanced cloud data analytics platforms available. Projects will span from workshops to full enterprise production end-to-end solutions.
The ideal candidate will have extensive experience with Microsoft/Azure data services and Databricks technology. Proficiency with the Databricks platform and the implementation of enterprise Data Lakehouse solutions will be required. Candidates will be expected to contribute to all stages of the data lifecycle, including data ingestion, data modeling, data profiling, data quality, data transformation, data movement, and data curation. The candidate should be familiar with market challenges in multiple industry verticals and have experience with both traditional and modern technologies across the Microsoft technology stack.
Responsibilities
Leading the development of data-driven solutions using Valorem Reply's methodology and enterprise project solutions.
Designing solution architecture and defining project requirements.
Staying up to date with the latest cloud solutions and technologies.
Collaborating across different teams to provide exceptional service to customers.
Demonstrating expertise in data engineering and understanding how it aligns with business objectives.
Managing the entire data lifecycle, from data ingestion to curation, and proficiency in Microsoft/Azure data services and Databricks technology.
Minimum Requirements
Bachelor's/master’s degree in computer science or equivalent with a focus on Azure data engineering solutions
6+ years of data engineering delivery experience
3+ years of Databricks engineering development experience
2+ years of technical team leadership or technical management experience
Candidates must be US Citizens
Show more
Show less","Data Engineering, DataDriven Solutions, Solution Architecture, Project Requirements, Cloud Solutions, Microsoft Azure, Databricks, Data Lifecycle, Data Ingestion, Data Modeling, Data Profiling, Data Quality, Data Transformation, Data Movement, Data Curation, Data Analytics Platforms, Workshops, Full Enterprise Production, EndtoEnd Solutions, Microsoft Technology Stack, Technical Team Leadership, Technical Management","data engineering, datadriven solutions, solution architecture, project requirements, cloud solutions, microsoft azure, databricks, data lifecycle, data ingestion, data modeling, data profiling, data quality, data transformation, data movement, data curation, data analytics platforms, workshops, full enterprise production, endtoend solutions, microsoft technology stack, technical team leadership, technical management","cloud solutions, data analytics platforms, data curation, data engineering, data ingestion, data lifecycle, data movement, data profiling, data quality, data transformation, databricks, datadriven solutions, datamodeling, endtoend solutions, full enterprise production, microsoft azure, microsoft technology stack, project requirements, solution architecture, technical management, technical team leadership, workshops"
Business Data Analyst,SpecChem,"Kansas City, MO",https://www.linkedin.com/jobs/view/business-data-analyst-at-specchem-3779856434,2023-12-17,Independence,United States,Mid senior,Onsite,"SpecChem, LLC supplies and services the concrete distributor network with quality construction chemicals that provide performance and value to the industry.
We are seeking an intelligent, energetic candidate for a full-time
Business Data Analyst
position in our corporate headquarters located in
Kansas City, MO
.
The ideal candidate for this position has outstanding analytical, problem-solving, and people skills, thinks innovatively, and constantly strives for excellence and improvement.
Responsibilities
Generate, create, and issue reports regarding sales, finance, and operations.
Create dashboards, spreadsheets analysis, and presentations as requested.
Troubleshoot data issues by validating different data sources.
Track and analyze reports to identify areas of improvement in business procedures.
Identify trends and variances in which positive changes can be made.
Serve as liaison between all departments to support desired result.
Build and maintain in-depth knowledge of distributor and manufacturer data files.
Deliver dependable and complete work and verify accuracy to ensure usability by executives, cross-functional teams, and customers.
Assist with project management, including capital expenditures and process improvements.
Proactively troubleshoots inconsistencies, errors, missing data and works quickly to make necessary corrections to provide timely support.
Triage and resolve data issues from identification to resolution - investigate the cause, find solutions and routinely follow-up to ensure consistency.
Other duties as dictated by business needs.
Requirements
Bachelor's degree in Accounting, Finance, or Business
1-3 year's relevant experience processing and organizing data.
Fluency in MS Excel (Pivot Tables, Macros, Data Sorting, etc.) required.
Problem solving, critical-thinking, and organizational skills required.
Highly detailed oriented with an ability to stay organized and manage a large workload.
Ability to work in teams and independently.
Ability to deal with ambiguity and demonstrates agility as business needs change.
Demonstrated ability to learn new analytical tools and software.
Excellent written and verbal communication skills.
Knowledge/Experience with R, Python, SQL, and Power BI a plus.
Knowledge/Experience of SAP a plus.
Knowledge/Experience with manufacturing a plus.
Ability to pass a background check, pre-employment physical exam, and drug screen.
All applicants are considered for all positions without regard to race, religion, color, sex, gender, sexual orientation, military/veteran status, genetic information, marital status, ethnicity, alienage or any other protected classification, in accordance with applicable federal, state, and local laws.
Show more
Show less","MS Excel, Macros, Python, R, SQL, Power BI, SAP","ms excel, macros, python, r, sql, power bi, sap","macros, ms excel, powerbi, python, r, sap, sql"
Junior Data Scientist,WorkForge,"Kansas City, KS",https://www.linkedin.com/jobs/view/junior-data-scientist-at-workforge-3782669231,2023-12-17,Independence,United States,Mid senior,Onsite,"Key Responsibilities:
Predictive Analytics
- Work with managers and developers to build predictive analytics features for our users
- Determine key components that will drive our predictive analytics data engine
- Utilize statistical and machine learning techniques to extract insights from data and build predictive models
- Collaborate with product and other internal teams to generate value for our users
Product Insights
- Manage and optimize processes for data intake, validation, and analysis as well as modeling, visualization and communication
- Ensure that product usage data is structured, organized, and accessible to stakeholders and build out reporting capabilities for accurately tracking key metrics
- Investigate, analyze and develop recommendations that support key business priorities; translating these recommendations into initiatives and executable actions
- Present data insights and deliver presentations with the goal of educating and championing a data-informed culture.
- Analyze data trends and patterns to identify opportunities for improvement and make recommendations to leadership
Leadership
- As the company’s first analytics hire, you’ll be able to flex your own entrepreneurial muscles, with the potential to develop an Analytics team within the organization.
- Champion the value of data insights across the organization, getting buy in from peers
- Advocate for best practices, staying current on industry trends and technology that can help achieve the company’s goals
Required Skills/Abilities:
- Bachelor's or Master's degree in a related field (e.g. Computer Science, Statistics, Applied Mathematics, Data Science)
- 5+ years of experience in data analysis and visualization of large data sets
- Strong experience with statistical and machine learning techniques
- Experience building dashboards in a modern business intelligence tool (e.g. Looker, Tableau, or PowerBI)
- Ability to write, improve, and troubleshoot complex SQL queries
- Analytical background and experience in querying and transforming data to compute metrics and conduct a variety of analyses to inform decisions
- Working knowledge of key SaaS metrics
- Previous experience with Heap, Pendo, Amplitude, Mixpanel or similar product analytics tools preferred
- Must live in the United States and be legally authorized to work in the United States on a full-time basis without requiring future sponsorship for employment visa status.
This job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities required of the employee for this job. Duties, responsibilities and activities may change at any time with or without notice.
It is the policy of WorkForge/BERKS Group/News-Press and Gazette Company to provide equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law.
Show more
Show less","Predictive Analytics, Data Analytics, Statistical Analysis, Machine Learning, Data Visualization, Business Intelligence, Data Warehousing, Data Mining, Data Transformation, Data Interpretation, Reporting, Data Communication, Datainformed Culture, Leadership, Data Insights, SaaS Metrics, SQL, Looker, Tableau, PowerBI, Heap, Pendo, Amplitude, Mixpanel","predictive analytics, data analytics, statistical analysis, machine learning, data visualization, business intelligence, data warehousing, data mining, data transformation, data interpretation, reporting, data communication, datainformed culture, leadership, data insights, saas metrics, sql, looker, tableau, powerbi, heap, pendo, amplitude, mixpanel","amplitude, business intelligence, data communication, data insights, data interpretation, data mining, data transformation, dataanalytics, datainformed culture, datawarehouse, heap, leadership, looker, machine learning, mixpanel, pendo, powerbi, predictive analytics, reporting, saas metrics, sql, statistical analysis, tableau, visualization"
Senior Data Analyst,Steadily,"Overland Park, KS",https://www.linkedin.com/jobs/view/senior-data-analyst-at-steadily-3725020311,2023-12-17,Independence,United States,Mid senior,Onsite,"Steadily is an insurance technology company that specializes in rental property insurance for landlords. We’re Series B, just over 100 people, manage $6B in risk, and are intent on becoming the world’s best and largest insurer of rental properties.
We exist to deliver fast, affordable insurance and to pay claims quickly and fairly. To do that, we hire the best engineers, actuaries, sales, and service people in the country to build the experience that we’d want if we were the client.
Since we only hire the best, if selected, you'll get to work alongside other team members who are as sharp as you are which will just serve to make you even sharper.
What You'll Do
Build and maintain analytical models to drive sales/marketing efficiencies and advise customer retention strategies.
Calculate and analyze sales commissions to ensure business objectives are achieved.
Evaluate and improve sales processes and systems by identifying areas of inefficiency in our sales process and proposing solutions and workflows that would resolve problems, optimize metrics, and improve productivity.
Develop deep knowledge of in-house data sources, and become adept at quickly extracting and analyzing data from these sources.
Drive sales data quality, and data hygiene efforts, through excellent change management and maintenance.
Establish and track revenue KPIs and assist in analyzing performance against these metrics on a recurring and ad hoc basis.
Your background
Experienced: Minimum 3 years of sales/marketing operations experience, preferably in a high volume DTC environment.
Digital: You are an expert in MS Excel and SQL. Experience with Salesforce highly preferred.
Hungry: You want to make the leap into an earlier-stage tech company to rapidly accelerate your growth. You want to roll up your sleeves and hustle – you are not looking for a traditional 9-5 job.
Self-Driven: You are disciplined and are comfortable executing every day with minimal oversight. You know what needs to be done and you do it without being asked. You are comfortable being held accountable to your results.
Compensation
at least $120,000 + equity in the company
Benefits
Equity in the company
Traditional Medical, Dental, Vision, and Life Insurance
Discretionary PTO
401K
Free snacks every day and regular team lunches
Location
Overland Park, KS (Kansas City Metro)
Steadily is unable to offer sponsorship of immigration cases at this time (for example, H-1B or other employment-based immigration cases)
Culture
Steadily boasts a very unique culture that our teammates love. We call it like we see it and we’re nothing if not candid. Check out our culture deck here to learn what we’re all about.
Steadily was awarded Comparably’s Best Company Leadership 2023, Best CEOs for Women 2023, and Best Company for Women 2022
Read What's Being Said About Steadily
Press Room
(News)
| TrustPilot
(Clients)
| Comparably
(Employees)
Show more
Show less","Sales/Marketing Operations, MS Excel, SQL, Salesforce, Data Extraction, Digital Marketing, Revenue KPIs, Analytics, Data Hygiene, Change Management","salesmarketing operations, ms excel, sql, salesforce, data extraction, digital marketing, revenue kpis, analytics, data hygiene, change management","analytics, change management, data extraction, data hygiene, digital marketing, ms excel, revenue kpis, salesforce, salesmarketing operations, sql"
Human Resources Compensation and Data Analyst,CAPTRUST,"Leawood, KS",https://www.linkedin.com/jobs/view/human-resources-compensation-and-data-analyst-at-captrust-3775815712,2023-12-17,Independence,United States,Mid senior,Onsite,"WHO are we looking for?
The
Human Resources Compensation and Data Analyst
will oversee the firm’s compensation function and day-to-day operations of data processing. The analyst will research job requirements and evaluate job positions to ensure the company is competitive in the areas of salaries and be responsible for data production, report generation, and dashboards to ensure the timely and accurate processing of data. Additionally, the analyst will provide leadership for math heavy functional areas such as benefits design and analysis and project coordination for the human resource information system (HRIS).
Responsibilities
Benchmark jobs against survey data and other market intelligence to determine competitive compensation ranges for each position
Prepare and maintain job classifications and salary scales
Prepare and present summary reports of job analysis and compensation analysis information
Gather data from market-based compensation surveys; use spreadsheet and data analysis techniques to assess results and market trends
Evaluate and implement job analysis instruments and materials
Interview and survey employees and managers to gather and document job, organizational, and occupational information including duties, responsibilities, and skills required by each job
Assesses jobs and their respective duties to determine classification as exempt or nonexempt and appropriate salary range
Oversee and manage assigned project and/or department’s data
Coordinate the production of data and ensure timely and accurate processing
Ensure integrity of data being processed is maintained, and field data-related questions
Build PowerBI dashboards for various department initiatives and for executives
Serve as the point person for data-related tasks on an assigned project including data requests for requests for proposals (RFPs)
Participate in the HRIS function in managing the day-to-day activities to support the HR, payroll and other interrelated functions
Primary financial analyst for benefits design
Perform other related duties as assigned
Qualifications
Minimum Qualifications:
Bachelor’s degree in a related field (Mathematics, Finance or Computer Science)
Minimum of two to three years’ experience in compensation and analyzing complex data
Desired Qualifications/Skills
Strong analytical and problem-solving skills
Expert in Microsoft Excel
Detail oriented
Excellent organizational and time management skills
Strong verbal and written communication skills
Ability to prioritize tasks
Proficient in Microsoft Office Suite or related software
Ability to explain technical information in understandable language to nontechnical personnel
WHAT can you expect from your career at CAPTRUST?
Our colleagues, like our clients, tend to stay with CAPTRUST for years. There’s a reason for it; it’s a great culture in which to work and grow. We all work together, each of us motivating those around us with our commitment to high standards. At CAPTRUST, expect a fully stocked break room, fun employee events, and a quality team surrounding you with opportunities for personal growth.
Our Employee Benefits Package shows how much we value our team. Some benefits include:
Employee ownership opportunities
Brick Bonus success sharing program
Comprehensive health coverage + Virgin Pulse wellness platform
401(k) program with a 5% employer match + financial planning for colleagues
WHERE will you be working?
The position can be filled at any of the following office locations:
400 North Tampa St #1800 | Tampa, FL 33602
4200 West 115th Street #210 | Leawood, KS 66211
4201 Congress St #160 | Charlotte, NC 28209
4208 Six Forks Rd #1700 | Raleigh, NC 27609
OR
700 North Saint Mary's St #100 | San Antonio, TX 78205
HOW do we build a world class organization one brick at a time?
We make it a priority to hire those who have a commitment to service, a real interest in other people, and a passion to continuously improve. Simply put: the difference at CAPTRUST is the quality of our people and depth of our bench. If you are ready to make your mark, we want to talk to you.
Are you the next brick?
To get it done the CAPTRUST Way, an individual should exhibit the following characteristics:
Ability to build successful, collaborative, and trusting relationships
Instinctive aptitude for consistently creating accurate, concise, respectful, and easy-to-understand verbal and written communications conveying complex information
A strong sense of urgency about getting work done and solving problems to achieve results that benefit our clients and colleagues, even when faced with challenges
Inherent desire to give back to our communities and enrich the lives of those around us
An other-centered mindset
Integrity through maintaining objectivity
EEO/Diversity Statement
At CAPTRUST, we are committed to building and maintaining a diverse workforce and inclusive work environment where ALL colleagues feel authentically seen, respected, and supported.
CAPTRUST is committed to providing employment opportunities without regard to race, color, age, sex, sexual orientation, familial status, religious creed, national origin, ancestry, medical condition, marital status and registered domestic partner status, citizenship status, military and veteran status, disability, protected medical condition, genetic information, or any other status protected by law. CAPTRUST makes all employment decisions without regard to these protected statuses and does not tolerate harassment or discrimination. #hybrid #mid-senior
Show more
Show less","Data analysis, Microsoft Excel, Microsoft Office Suite, PowerBI, HRIS, Compensation analysis, Job classification, Salary scales, Benefits design, Data processing, Report generation, Dashboards, SQL, Python, R, Statistics, Probability, Linear algebra, Calculus, Machine Learning, Natural Language Processing, Data Structures, Algorithms","data analysis, microsoft excel, microsoft office suite, powerbi, hris, compensation analysis, job classification, salary scales, benefits design, data processing, report generation, dashboards, sql, python, r, statistics, probability, linear algebra, calculus, machine learning, natural language processing, data structures, algorithms","algorithms, benefits design, calculus, compensation analysis, dashboard, data processing, data structures, dataanalytics, hris, job classification, linear algebra, machine learning, microsoft excel, microsoft office suite, natural language processing, powerbi, probability, python, r, report generation, salary scales, sql, statistics"
Azure Data Engineer,TriCom Technical Services,"Kansas City, MO",https://www.linkedin.com/jobs/view/azure-data-engineer-at-tricom-technical-services-3768735555,2023-12-17,Independence,United States,Mid senior,Onsite,"Azure Data Analyst/Engineer
Summary
Our client is seeking a technically well-versed individual to build and enhance PMI systems and applications. This individual must be comfortable communicating and working directly with clients to find optimal solutions.
Requirements
At least 5 years of experience with ETL tools including Azure Data Factory, FME, or similar.
Experience integrating multiple data sources using ETL technology or APIs.
Knowledge of Python libraries including Pandas, NumPy, or C#.NET.
At least 5-years of experience with relational or NoSQL databases (SQL Server is highly preferred).
Knowledge and experience implementing RESTful Web services.
Experience working in an Agile/Scrum environment.
Skill and experience with the following technologies and processes:
ETL;
PMIS, program/project management tools, integration, and data management;
Azure or AWS Cloud;
CI/CD and Docker.
Preferred
At least 5-years of experience with SQL Server.
This is a 6-Month Contract opportunity with our Kansas City, MO client. Employee benefits include Medical/Dental Benefits, Paid time off, Paid Holidays, and 401(k) (with immediately-vested company match) available with TriCom during the contract period. H1-B Visa sponsorship is not available for this position. No third-parties, please.
#onsite
#hybrid
Show more
Show less","Azure Data Factory, FME, ETL, Python, Pandas, NumPy, C#.NET, SQL Server, NoSQL databases, RESTful Web services, Agile, Scrum, PMIS, Program/project management tools, Integration, Data management, Azure Cloud, AWS Cloud, CI/CD, Docker","azure data factory, fme, etl, python, pandas, numpy, cnet, sql server, nosql databases, restful web services, agile, scrum, pmis, programproject management tools, integration, data management, azure cloud, aws cloud, cicd, docker","agile, aws cloud, azure cloud, azure data factory, cicd, cnet, data management, docker, etl, fme, integration, nosql databases, numpy, pandas, pmis, programproject management tools, python, restful web services, scrum, sql server"
Sr Data Engineer,Lockton,"Kansas City, MO",https://www.linkedin.com/jobs/view/sr-data-engineer-at-lockton-3754704943,2023-12-17,Independence,United States,Mid senior,Onsite,"Your Responsibilities
What’s the role and Key responsibilities:
As a Sr. Data Engineer, you will create and maintain optimal data pipeline architecture, Infrastructure automation (IAC) and also will assemble large, complex data sets that meet functional/non-functional business requirements across Data Solutions.
You will also identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability etc.
Implement foundational and advanced analytics methodologies to address key metrics to improve customer experience, network performance, quality of service, profitability of products, and personalized offerings
Delivering technical functionality developed against the Global Lockton Cloud Data Platform's with a Product Mindset.
Use data, analysis, and automation to generate value within the company through advanced data modeling and other advanced analytics principles
Building the infrastructure required for optimal ETL/ELT of data from a wide variety of data sources using Cloud Technologies and OnPrem.
Responsible for helping an organization deliver high data availability and quality throughout the entire data life cycle from ingestion to end products: dashboards, machine learning models, and production datasets.
The Successful Applicant
Technical Knowledge, Key skills and experience required for the successful candidate:
Hands-on experience with Azure services such as Azure Data Factory, Azure Databricks, ADLS-Delta lake, Azure functions, Logic aps, Synapse, Azure Logic Apps, Azure Storage, Azure Search, Purview, Azure SQL server, Scala/Spark/SQL/PYTHON etc.
Azure Database development experience with Infra as a Code, Azure CI/CD, Azure Classic Pipelines, Azure YAML, ARM/Terraforms/Ansible, AKS
Maintenance, Azure ML, Pipeline integrations, Azure PaaS and IaaS, Azure Infra Experience (Spinning VM's, Databases and Cosmos)
Expertise in Data warehousing - data modelling, relational and Dimension modelling, Data quality, data catalog and Data Governance
Design and build highly scalable data pipelines using new generation tools and technologies like Spark, Kafka to induct data from various systems
Experience with data security and data access controls and design across digital transformation/cloud migration programs.
Deep understanding of relational as well as NoSQL data stores, methods and approaches (star and snowflake, dimensional modelling)
Ensure proper requirement traceability and Quality delivery during development and integration testing
A strong background in the Microsoft SQL Server stack (including SSIS, SSRS, Power BI, and SSAS)
Ability to build processes supporting data transformation, data structures, metadata, dependency, and workload management.
Strong analytic skills related to working with structured and unstructured datasets
Should support the Production live application by maintaining 100% Data accuracy and data availability in all 24*7.
Qualifications
Required Experience:
Minimum of 7+ years of increasingly responsible experience in Data Platforms, Data Engineering, Software Engineering at high impactful and changing environments
4+ years of working experience in Azure or cloud platforms as Solution/Data Architect/Data Engineer
4+ years of hands-on experience with one or more cloud technologies such as Azure, AWS, Google.
Experience in collaborating with a high-profile team of Data Engineers, data scientists, Business Analytics and Data Analysts
Knowledge of Software development agile methodologies and worked in Scrum teams
Excellent communication & presentation skills with a track record of engaging with senior business leaders
Experience in Finance/Insurance would be a distinct advantage
Should have worked in a dynamic and fast-changing environment
High affinity with AI-powered insight tools and engines and application of real-time solutions to analytics problems
Experience with GEN AI and LLM's will be added advantage
Exposure to COGS/KTLO, Operations and Command Center activity
Soft Skills Required
Good communication and presentation skills
Highly driven, energetic, flexible, resourceful & ability to multitask
Clarity of thoughts and vision
Ability to ideate and bring solutions to the table
Adherence to timelines, without sacrificing the quality of output
Hands-on and detail-oriented, with a strong ability to coordinate across different Geographies and with different stakeholders
Academics
Graduate degree in Engineering, Computer Science, Management Information Systems, or related
Strong exposure to Databases and data structures of cloud-based technologies
Azure/GCP/AWS Solution Architect certification or Data Engineer certification is plus
Career progression:
Receive ongoing support and funding towards your personal learning and development, including coverage for relevant courses and certifications in data related fields. Lockton prides itself on offering outstanding career progression programs for those who wish to progress in their field of interest, be it in a Technical or Leadership capacity.
Show more
Show less","Azure Data Factory, Azure Databricks, ADLSDelta lake, Azure functions, Logic aps, Synapse, Azure Logic Apps, Azure Storage, Azure Search, Purview, Azure SQL server, Scala/Spark/SQL/PYTHON, Infra as a Code, Azure CI/CD, Azure Classic Pipelines, Azure YAML, ARM/Terraforms/Ansible, Maintenance, Azure ML, Pipeline integrations, Azure PaaS, Azure IaaS, Data warehousing, Data modelling, Relational modelling, Dimension modelling, Data quality, Data catalog, Data Governance, Spark, Kafka, Data security, Data access controls, NoSQL data stores, Star and snowflake, Dimensional modelling, SSIS, SSRS, Power BI, SSAS, Data transformation, Data structures, Metadata, Dependency management, Workload management, Data accuracy, Data availability, Agile methodologies, Scrum teams, AIpowered insight tools, Realtime solutions, GEN AI, LLM's, COGS/KTLO, Operations, Command Center","azure data factory, azure databricks, adlsdelta lake, azure functions, logic aps, synapse, azure logic apps, azure storage, azure search, purview, azure sql server, scalasparksqlpython, infra as a code, azure cicd, azure classic pipelines, azure yaml, armterraformsansible, maintenance, azure ml, pipeline integrations, azure paas, azure iaas, data warehousing, data modelling, relational modelling, dimension modelling, data quality, data catalog, data governance, spark, kafka, data security, data access controls, nosql data stores, star and snowflake, dimensional modelling, ssis, ssrs, power bi, ssas, data transformation, data structures, metadata, dependency management, workload management, data accuracy, data availability, agile methodologies, scrum teams, aipowered insight tools, realtime solutions, gen ai, llms, cogsktlo, operations, command center","adlsdelta lake, agile methodologies, aipowered insight tools, armterraformsansible, azure cicd, azure classic pipelines, azure data factory, azure databricks, azure functions, azure iaas, azure logic apps, azure ml, azure paas, azure search, azure sql server, azure storage, azure yaml, cogsktlo, command center, data access controls, data accuracy, data availability, data catalog, data governance, data modelling, data quality, data security, data structures, data transformation, datawarehouse, dependency management, dimension modelling, dimensional modelling, gen ai, infra as a code, kafka, llms, logic aps, maintenance, metadata, nosql data stores, operations, pipeline integrations, powerbi, purview, realtime solutions, relational modelling, scalasparksqlpython, scrum teams, spark, ssas, ssis, ssrs, star and snowflake, synapse, workload management"
Data Analyst I,Lockton,"Kansas City, MO",https://www.linkedin.com/jobs/view/data-analyst-i-at-lockton-3774281918,2023-12-17,Independence,United States,Mid senior,Onsite,"Your Responsibilities
Description:
Collects data and analyzes client information
Work collaboratively with managers and other teammates to collect data, ensuring reporting is accurate, timely, and of high quality.
Audits data results and reports findings to supervisors
Prepares standard and ad hoc reports
Facilitates process and complete pricing comparisons for request for proposals (RFPs)
Analyzes client data in conjunction with other consultants to recommend plan design changes, programs or formulary changes
Integrates data from multiple data sets into relevant systems for reports publication.
Follows policies, procedures, and/or reports that make the overall practice more efficient and effective
Supports standard data sets monthly within relevant systems
Coordinates new client set-up, including file submission and testing, with consultants and system administrators
Gathers new reporting requirements from business users and coordinate development/deployment with system administrators
Qualifications
Qualifications:
Bachelor's degree required. Major in Actuarial Science, Mathematics, Statistics or Finance.
MS Excel experience required. In addition, experience using SQL and MS Access a plus.
Understands data sources, processes, formulas and output General knowledge of healthcare issues and their associated impacts on plan sponsors and plan members.
Ability to understand data integrity and correctness.
Excellent written and oral communication skills to effectively present information to Associates at all levels of the Lockton organization.
Strong proactive style.
Proven ability to manage multiple projects simultaneously.
Understands industry trends and governmental regulations
Ability to complete continuing education requirements as needed
Ability to attend company, department, and team meetings as required, including industry training sessions
Ability to comply with all company policies and procedures, proactively protecting confidentiality of Client and company information
Ability to efficiently organize work and manage time in order to meet deadlines
Ability to travel by automobile and aircraft
Ability to use office equipment such as a computer, keyboard, calculator, photocopier, and facsimile machine
Ability to work on a computer for a prolonged amount of time
Ability to work outside of normal business hours as needed
Legally able to work in the United States
Show more
Show less","Actuarial Science, Mathematics, Statistics, Finance, MS Excel, SQL, MS Access, Data sources, Data processes, Data formulas, Data output, Healthcare issues, Plan sponsors, Plan members, Data integrity, Data correctness, Communication skills, Proactive style, Project management, Industry trends, Governmental regulations, Continuing education, Company policies, Confidentiality, Time management, Deadlines, Travel, Office equipment, Computer skills, Extended computer use, Nonstandard working hours, United States work authorization","actuarial science, mathematics, statistics, finance, ms excel, sql, ms access, data sources, data processes, data formulas, data output, healthcare issues, plan sponsors, plan members, data integrity, data correctness, communication skills, proactive style, project management, industry trends, governmental regulations, continuing education, company policies, confidentiality, time management, deadlines, travel, office equipment, computer skills, extended computer use, nonstandard working hours, united states work authorization","actuarial science, communication skills, company policies, computer skills, confidentiality, continuing education, data correctness, data formulas, data integrity, data output, data processes, data sources, deadlines, extended computer use, finance, governmental regulations, healthcare issues, industry trends, mathematics, ms access, ms excel, nonstandard working hours, office equipment, plan members, plan sponsors, proactive style, project management, sql, statistics, time management, travel, united states work authorization"
Data Analyst III,Lockton,"Kansas City, MO",https://www.linkedin.com/jobs/view/data-analyst-iii-at-lockton-3720808018,2023-12-17,Independence,United States,Mid senior,Onsite,"Your Responsibilities
Description:
We are looking for a data analyst to be responsible for creating complex analysis of medical, pharmacy, and biometric data using SQL and Tableau . The ideal candidate will be motivated, results-driven, flexible, detail-oriented, and collaborative. Our fast-paced environment has a supportive culture that values autonomy, initiative, and personal accountability.
Job Duties:
Develop and support new analytical tools and reports, primarily using SQL and Tableau.
Gathers new reporting requirements from stakeholders.
Work collaboratively to ensure reporting that is accurate, timely, and of high quality.
Set strategies and take ownership in on-going analytics development.
Generate benchmark data on a requested basis.
Responsible for data consolidation, analysis, and business reporting.
Develops and maintains reports for stakeholders at all levels of the organization.
Provides support to research teams or management by collecting and analyzing data and reporting results based on the needs of end users.
Maintains data integrity and data processing efficiency by working to eliminate redundancy and applying best practice data stewardship techniques.
Stays informed of the ways the organization uses its data.
Recognize data issues and work with data quality team to resolve problems.
Must continuously be learning and staying abreast of technology advances.
Provide exceptional customer service and adhere to the Lockton philosophies.
Qualifications
Requirements:
Bachelor's degree required. Major in Mathematics, Statistics, Finance, Insurance, or Economics preferred.
Advanced knowledge and experience using SQL.
Knowledge of Tableau strongly preferred.
Understand data sources, processes, formulas, and output.
Ability to understand data integrity and correctness.
Previous experience working with claims data, CPT codes, and ICD-10 codes preferred.
Proven ability to manage multiple projects simultaneously.
Aptitude to develop unique, creative, and efficient work products.
Excellent written and oral communication skills to effectively present information to associates at all levels of the Lockton organization.
Assumes ownership for individual projects. Strong proactive style and self-motivated to actively seek solutions to problems without supervision.
Ability to comply with all company policies and procedures, proactively protecting confidentiality of client and company information.
Show more
Show less","SQL, Tableau, Data analysis, Data consolidation, Data reporting, Data stewardship, Data integrity, Data quality, Customer service, Mathematics, Statistics, Finance, Insurance, Economics, Claims data, CPT codes, ICD10 codes, Project management, Communication, Problem solving","sql, tableau, data analysis, data consolidation, data reporting, data stewardship, data integrity, data quality, customer service, mathematics, statistics, finance, insurance, economics, claims data, cpt codes, icd10 codes, project management, communication, problem solving","claims data, communication, cpt codes, customer service, data consolidation, data integrity, data quality, data reporting, data stewardship, dataanalytics, economics, finance, icd10 codes, insurance, mathematics, problem solving, project management, sql, statistics, tableau"
Sr. Data Analyst,Barkley,"Kansas City, MO",https://www.linkedin.com/jobs/view/sr-data-analyst-at-barkley-3784882135,2023-12-17,Independence,United States,Mid senior,Remote,"Our Data Management & Measurement (DMM) team lives at the center of marketing results and business impacts. We assess how our clients can best optimize their marketing efforts to maximize the direct impact on business results.
We are looking for a Sr. Data Analyst, who will work on the front lines of our practice - delivering impactful and inspiring reporting for our internal media teams and our clients. This person will ideally become a trusted expert regarding their clients' marketing performance data and its impact on business results. As a member of the DMM team, you will play an important role in discovering, analyzing and presenting findings and reports related to all media channels. You will be at the heart of uncovering and measuring the connections between marketing results and business objectives.
Responsibilities
Have a passion for data analysis, critical thinking and storytelling
Be able to work with internal media and reporting teams to develop measurement solutions for client campaigns.
Analyze campaign performance and other marketing efforts to assess their impact on client KPIs and make recommendations for improvements.
Participate in the creation of our clients' measurement strategies, including KPI selection, data capture requirements, measurement frameworks and data visualization
Become a trusted expert regarding clients' marketing performance data and will assist team members across departments in accessing and interpreting results.
Have experience with digital tracking, including tagging, mobile and social limitations
Excellent verbal and written communication skills to interpret and present business value to clients.
Work with client internal analytics and IT teams to implement custom tracking parameters
Implement and maintain media performance dashboard reporting across platforms like Excel, Sheets, Data Studio, PowerPoint, Slides, Datorama, Tableau and Power BI
Demonstrate problem-solving ability with emphasis on drawing inferences with data
Personal characteristics:
Intellectual curiosity, problem solving skills and determination
Detail-oriented and thorough
Effective communications verbally and in writing
Ability to listen to stakeholders, process feedback and provide solutions
Strong work ethic and integrity
Qualifications
2+ years of marketing analytics and business intelligence experience with an emphasis on media and marketing measurement
Experience with data visualization tools, especially Power BI, is a plus. This includes both the data visualization component of dashboard development and backend data preparation and aggregation.
Strong working knowledge of spreadsheet platforms (i.e. Excel, Sheets, etc)
Experience with advertising planning, buying and performance metrics
Experience with media reporting platform(s) (Doubleclick, Adwords, etc)
Experience with web analytics platform(s) (Adobe, Google Analytics,etc.)
Data analysis experience including spreadsheet (Excel) and SQL platforms
Relational database programming languages (e.g. SQL) and statistical tools (e.g. SAS, SPSS, Python) is a plus
Effectiveness in managing multiple projects across multiple clients and stakeholders
Experience in analysis, research and presentation creation
Barkley's Commitment to Diversity & Inclusion
We believe being radically diverse and inclusive is the key to becoming one of the world's great creative idea companies. By embracing everything that makes our partners who they are and what makes them unique to the world around them, we create the conditions and capacity to help creative, original thinking thrive.
Barkley is committed to Diversity, Equity, Inclusion and Belonging as part of our corporate strategic goals, supported by a formal DEI+B program, Employee Resource Groups, Director of Diversity leadership and agency commitment to The Brand Lab.
Show more
Show less","Data analysis, KPI, SQL, SAS, SPSS, Python, Excel, Sheets, Data visualization, Dashboard, PowerPoint, Datorama, Tableau, Power BI, Problemsolving, Data preparation, Spreadsheet platforms, Project management, Adobe Analytics, Google Analytics, Media reporting platforms, Marketing analytics, Business intelligence, Digital tracking, Measurement frameworks, Tagging","data analysis, kpi, sql, sas, spss, python, excel, sheets, data visualization, dashboard, powerpoint, datorama, tableau, power bi, problemsolving, data preparation, spreadsheet platforms, project management, adobe analytics, google analytics, media reporting platforms, marketing analytics, business intelligence, digital tracking, measurement frameworks, tagging","adobe analytics, business intelligence, dashboard, data preparation, dataanalytics, datorama, digital tracking, excel, google analytics, kpi, marketing analytics, measurement frameworks, media reporting platforms, powerbi, powerpoint, problemsolving, project management, python, sas, sheets, spreadsheet platforms, spss, sql, tableau, tagging, visualization"
"Data Conversion Developer, Senior Associate",PwC,"Kansas City, MO",https://www.linkedin.com/jobs/view/data-conversion-developer-senior-associate-at-pwc-3749935677,2023-12-17,Independence,United States,Mid senior,Remote,"Specialty/Competency:
Functional & Industry Technologies
Industry/Sector:
Not Applicable
Time Type:
Full time
Travel Requirements:
Up to 80%
A career within Functional and Industry Technologies services will provide you with the opportunity to build secure and new digital experiences for customers, employees, and suppliers. We focus on improving apps or developing new apps for traditional and mobile devices as well as conducting usability testing to find ways to improve our clients’ user experience. Our team helps clients transform their business through enabling technologies across marketing, finance and operations in the functional areas such as Maximo and PowerPlant.
To really stand out and make us fit for the future in a constantly changing world, each and every one of us at PwC needs to be a purpose-led and values-driven leader at every level. To help us achieve this we have the PwC Professional; our global leadership development framework. It gives us a single set of expectations across our lines, geographies and career paths, and provides transparency on the skills we need as individuals to be successful and progress in our careers, now and in the future.
Responsibilities
As a Senior Associate, you'll work as part of a team of problem solvers, helping to solve complex business issues from strategy to execution. PwC Professional skills and responsibilities for this management level include but are not limited to:
Use feedback and reflection to develop self awareness, personal strengths and address development areas.
Delegate to others to provide stretch opportunities, coaching them to deliver results.
Demonstrate critical thinking and the ability to bring order to unstructured problems.
Use a broad range of tools and techniques to extract insights from current industry or sector trends.
Review your work and that of others for quality, accuracy and relevance.
Know how and when to use tools available for a given situation and can explain the reasons for this choice.
Seek and embrace opportunities which give exposure to different situations, environments and perspectives.
Use straightforward communication, in a structured way, when influencing and connecting with others.
Able to read situations and modify behavior to build quality relationships.
Uphold the firm's code of ethics and business conduct.
Basic Qualifications
Minimum Degree Required:
Bachelor Degree
Minimum Years Of Experience
4 years
Preferred Qualifications
Degree Preferred:
Master Degree
Certification(s) Preferred
Azure Data Engineer Associate
Databricks Certified Data Engineer Associate
Preferred Fields Of Study
Computer and Information Science, Computer Engineering, Computer Management, Management Information Systems, Information Technology
Preferred Knowledge/Skills
Demonstrates a thorough level of abilities with, and/or a proven record of success as both an individual contributor and team member, identifying and addressing client needs:
Supports in data analysis techniques to assess source data structures, identify mapping requirements, and define transformation rules for data conversion into Maximo;
Leads Maximo's modules and functionalities related to Asset Management and Work Order Management and IBM Maximo, including its data structures, configuration settings, and integration capabilities;
Identifies relational databases, preferably experience in working with databases commonly used in Maximo, such as IBM DB2, Oracle, Microsoft SQL Server, along with familiarity with Maximo's Integration Framework (MIF) and its capabilities for data integration and conversion;
Supports in designing and implementing data extraction, transformation, and loading processes for Maximo data conversion;
Showcases understanding in SQL and database querying languages to extract and manipulate data from source systems along with understanding ETL tools and methodologies commonly used in Maximo data conversion;
Identifies technologies commonly used with Maximo, including web services (SOAP, RESTful APIs), XML, JSON, and other relevant data exchange formats;
Supports in pipeline architecture and development using one of the tools such as Azure ADF, AWS Glue, SSIS, DataBricks (multiple preferred);
Utilizes data cleansing techniques and methodologies to ensure the integrity and accuracy of converted data in Maximo;
Developes data cleansing functional business rules as per Maximo Business Object (MBO) definitions for source to Maximo conversion requirements;and comprehensive testing plans and executing validation processes to verify the accuracy and integrity of converted data in Maximo;
Customizes Maximo options, such as Automation Scripts, Java Customizations, Database Configuration, or Application Designer, to support data conversion requirements;
Identifies integrations within Maximo with other enterprise systems, such as ERP systems, GIS systems, or asset management systems. Knowledge of integration patterns, data synchronization, and data exchange protocols; and,
Showcases work experience as a Data Engineer, Data Architect or similar role, along with experience in programming languages including Python, PySpark, Scala, SQL.
Learn more about how we work: https://pwc.to/how-we-work
PwC does not intend to hire experienced or entry level job seekers who will need, now or in the future, PwC sponsorship through the H-1B lottery, except as set forth within the following policy: https://pwc.to/H-1B-Lottery-Policy.
All qualified applicants will receive consideration for employment at PwC without regard to race; creed; color; religion; national origin; sex; age; disability; sexual orientation; gender identity or expression; genetic predisposition or carrier status; veteran, marital, or citizenship status; or any other status protected by law. PwC is proud to be an affirmative action and equal opportunity employer.
For positions based in San Francisco, consideration of qualified candidates with arrest and conviction records will be in a manner consistent with the San Francisco Fair Chance Ordinance.
Applications will be accepted until the position is filled or the posting is removed, unless otherwise set forth on the following webpage. Please visit this link for information about anticipated application deadlines: https://pwc.to/us-application-deadlines
For positions in California, Colorado, Hawaii, Nevada, New York State, or Washington State, or for opportunities that will report to a supervisor, office or other work site in New York State, please visit the following link for pay range information: https://pwc.to/payrange-v1-advisoryseniorassociate
Show more
Show less","Maximo, PowerPlant, PwC Professional, Azure Data Engineer Associate, Databricks Certified Data Engineer Associate, Computer and Information Science, Computer Engineering, Computer Management, Management Information Systems, Information Technology, Data analysis, SQL, ETL tools, XML, JSON, Data cleansing techniques, Maximo Business Object (MBO), Python, PySpark, Scala, Java Customizations, Database Configuration, ERP systems, GIS systems, Asset management systems","maximo, powerplant, pwc professional, azure data engineer associate, databricks certified data engineer associate, computer and information science, computer engineering, computer management, management information systems, information technology, data analysis, sql, etl tools, xml, json, data cleansing techniques, maximo business object mbo, python, pyspark, scala, java customizations, database configuration, erp systems, gis systems, asset management systems","asset management systems, azure data engineer associate, computer and information science, computer engineering, computer management, data cleansing techniques, dataanalytics, database configuration, databricks certified data engineer associate, erp systems, etl tools, gis systems, information technology, java customizations, json, management information systems, maximo, maximo business object mbo, powerplant, pwc professional, python, scala, spark, sql, xml"
"Data Conversion Developer, Senior Associate",PwC,"Kansas City, MO",https://www.linkedin.com/jobs/view/data-conversion-developer-senior-associate-at-pwc-3749937532,2023-12-17,Independence,United States,Mid senior,Remote,"Specialty/Competency:
Functional & Industry Technologies
Industry/Sector:
Not Applicable
Time Type:
Full time
Travel Requirements:
Up to 80%
A career within Functional and Industry Technologies services will provide you with the opportunity to build secure and new digital experiences for customers, employees, and suppliers. We focus on improving apps or developing new apps for traditional and mobile devices as well as conducting usability testing to find ways to improve our clients’ user experience. Our team helps clients transform their business through enabling technologies across marketing, finance and operations in the functional areas such as Maximo and PowerPlant.
To really stand out and make us fit for the future in a constantly changing world, each and every one of us at PwC needs to be a purpose-led and values-driven leader at every level. To help us achieve this we have the PwC Professional; our global leadership development framework. It gives us a single set of expectations across our lines, geographies and career paths, and provides transparency on the skills we need as individuals to be successful and progress in our careers, now and in the future.
Responsibilities
As a Senior Associate, you'll work as part of a team of problem solvers, helping to solve complex business issues from strategy to execution. PwC Professional skills and responsibilities for this management level include but are not limited to:
Use feedback and reflection to develop self awareness, personal strengths and address development areas.
Delegate to others to provide stretch opportunities, coaching them to deliver results.
Demonstrate critical thinking and the ability to bring order to unstructured problems.
Use a broad range of tools and techniques to extract insights from current industry or sector trends.
Review your work and that of others for quality, accuracy and relevance.
Know how and when to use tools available for a given situation and can explain the reasons for this choice.
Seek and embrace opportunities which give exposure to different situations, environments and perspectives.
Use straightforward communication, in a structured way, when influencing and connecting with others.
Able to read situations and modify behavior to build quality relationships.
Uphold the firm's code of ethics and business conduct.
Basic Qualifications
Minimum Degree Required:
Bachelor Degree
Minimum Years Of Experience
4 years
Preferred Qualifications
Degree Preferred:
Master Degree
Certification(s) Preferred
Azure Data Engineer Associate
Databricks Certified Data Engineer Associate
Preferred Fields Of Study
Computer and Information Science, Computer Engineering, Computer Management, Management Information Systems, Information Technology
Preferred Knowledge/Skills
Demonstrates a thorough level of abilities with, and/or a proven record of success as both an individual contributor and team member, identifying and addressing client needs:
Supports in data analysis techniques to assess source data structures, identify mapping requirements, and define transformation rules for data conversion into Maximo;
Leads Maximo's modules and functionalities related to Asset Management and Work Order Management and IBM Maximo, including its data structures, configuration settings, and integration capabilities;
Identifies relational databases, preferably experience in working with databases commonly used in Maximo, such as IBM DB2, Oracle, Microsoft SQL Server, along with familiarity with Maximo's Integration Framework (MIF) and its capabilities for data integration and conversion;
Supports in designing and implementing data extraction, transformation, and loading processes for Maximo data conversion;
Showcases understanding in SQL and database querying languages to extract and manipulate data from source systems along with understanding ETL tools and methodologies commonly used in Maximo data conversion;
Identifies technologies commonly used with Maximo, including web services (SOAP, RESTful APIs), XML, JSON, and other relevant data exchange formats;
Supports in pipeline architecture and development using one of the tools such as Azure ADF, AWS Glue, SSIS, DataBricks (multiple preferred);
Utilizes data cleansing techniques and methodologies to ensure the integrity and accuracy of converted data in Maximo;
Developes data cleansing functional business rules as per Maximo Business Object (MBO) definitions for source to Maximo conversion requirements;and comprehensive testing plans and executing validation processes to verify the accuracy and integrity of converted data in Maximo;
Customizes Maximo options, such as Automation Scripts, Java Customizations, Database Configuration, or Application Designer, to support data conversion requirements;
Identifies integrations within Maximo with other enterprise systems, such as ERP systems, GIS systems, or asset management systems. Knowledge of integration patterns, data synchronization, and data exchange protocols; and,
Showcases work experience as a Data Engineer, Data Architect or similar role, along with experience in programming languages including Python, PySpark, Scala, SQL.
Learn more about how we work: https://pwc.to/how-we-work
PwC does not intend to hire experienced or entry level job seekers who will need, now or in the future, PwC sponsorship through the H-1B lottery, except as set forth within the following policy: https://pwc.to/H-1B-Lottery-Policy.
All qualified applicants will receive consideration for employment at PwC without regard to race; creed; color; religion; national origin; sex; age; disability; sexual orientation; gender identity or expression; genetic predisposition or carrier status; veteran, marital, or citizenship status; or any other status protected by law. PwC is proud to be an affirmative action and equal opportunity employer.
For positions based in San Francisco, consideration of qualified candidates with arrest and conviction records will be in a manner consistent with the San Francisco Fair Chance Ordinance.
Applications will be accepted until the position is filled or the posting is removed, unless otherwise set forth on the following webpage. Please visit this link for information about anticipated application deadlines: https://pwc.to/us-application-deadlines
For positions in California, Colorado, Hawaii, Nevada, New York State, or Washington State, or for opportunities that will report to a supervisor, office or other work site in New York State, please visit the following link for pay range information: https://pwc.to/payrange-v1-advisoryseniorassociate
Show more
Show less","IBM Maximo, Azure Data Engineer Associate, PowerPlant, SQL, Oracle, AWS Glue, Python, PySpark, Scala, XML, JSON, ERP, GIS, Salesforce, Data Analysis, ETL, Data Conversion, Data Integration, Data Extraction, Data Transformation, Database querying, Database Configuration, Application Designer, Data Cleansing, Java, Maximo Business Object (MBO), SOAP, RESTful APIs","ibm maximo, azure data engineer associate, powerplant, sql, oracle, aws glue, python, pyspark, scala, xml, json, erp, gis, salesforce, data analysis, etl, data conversion, data integration, data extraction, data transformation, database querying, database configuration, application designer, data cleansing, java, maximo business object mbo, soap, restful apis","application designer, aws glue, azure data engineer associate, data conversion, data extraction, data integration, data transformation, dataanalytics, database configuration, database querying, datacleaning, erp, etl, gis, ibm maximo, java, json, maximo business object mbo, oracle, powerplant, python, restful apis, salesforce, scala, soap, spark, sql, xml"
"Data Conversion Developer, Senior Associate",PwC,"Kansas City, MO",https://www.linkedin.com/jobs/view/data-conversion-developer-senior-associate-at-pwc-3749936650,2023-12-17,Independence,United States,Mid senior,Remote,"Specialty/Competency:
Functional & Industry Technologies
Industry/Sector:
Not Applicable
Time Type:
Full time
Travel Requirements:
Up to 80%
A career within Functional and Industry Technologies services will provide you with the opportunity to build secure and new digital experiences for customers, employees, and suppliers. We focus on improving apps or developing new apps for traditional and mobile devices as well as conducting usability testing to find ways to improve our clients’ user experience. Our team helps clients transform their business through enabling technologies across marketing, finance and operations in the functional areas such as Maximo and PowerPlant.
To really stand out and make us fit for the future in a constantly changing world, each and every one of us at PwC needs to be a purpose-led and values-driven leader at every level. To help us achieve this we have the PwC Professional; our global leadership development framework. It gives us a single set of expectations across our lines, geographies and career paths, and provides transparency on the skills we need as individuals to be successful and progress in our careers, now and in the future.
Responsibilities
As a Senior Associate, you'll work as part of a team of problem solvers, helping to solve complex business issues from strategy to execution. PwC Professional skills and responsibilities for this management level include but are not limited to:
Use feedback and reflection to develop self awareness, personal strengths and address development areas.
Delegate to others to provide stretch opportunities, coaching them to deliver results.
Demonstrate critical thinking and the ability to bring order to unstructured problems.
Use a broad range of tools and techniques to extract insights from current industry or sector trends.
Review your work and that of others for quality, accuracy and relevance.
Know how and when to use tools available for a given situation and can explain the reasons for this choice.
Seek and embrace opportunities which give exposure to different situations, environments and perspectives.
Use straightforward communication, in a structured way, when influencing and connecting with others.
Able to read situations and modify behavior to build quality relationships.
Uphold the firm's code of ethics and business conduct.
Basic Qualifications
Minimum Degree Required:
Bachelor Degree
Minimum Years Of Experience
4 years
Preferred Qualifications
Degree Preferred:
Master Degree
Certification(s) Preferred
Azure Data Engineer Associate
Databricks Certified Data Engineer Associate
Preferred Fields Of Study
Computer and Information Science, Computer Engineering, Computer Management, Management Information Systems, Information Technology
Preferred Knowledge/Skills
Demonstrates a thorough level of abilities with, and/or a proven record of success as both an individual contributor and team member, identifying and addressing client needs:
Supports in data analysis techniques to assess source data structures, identify mapping requirements, and define transformation rules for data conversion into Maximo;
Leads Maximo's modules and functionalities related to Asset Management and Work Order Management and IBM Maximo, including its data structures, configuration settings, and integration capabilities;
Identifies relational databases, preferably experience in working with databases commonly used in Maximo, such as IBM DB2, Oracle, Microsoft SQL Server, along with familiarity with Maximo's Integration Framework (MIF) and its capabilities for data integration and conversion;
Supports in designing and implementing data extraction, transformation, and loading processes for Maximo data conversion;
Showcases understanding in SQL and database querying languages to extract and manipulate data from source systems along with understanding ETL tools and methodologies commonly used in Maximo data conversion;
Identifies technologies commonly used with Maximo, including web services (SOAP, RESTful APIs), XML, JSON, and other relevant data exchange formats;
Supports in pipeline architecture and development using one of the tools such as Azure ADF, AWS Glue, SSIS, DataBricks (multiple preferred);
Utilizes data cleansing techniques and methodologies to ensure the integrity and accuracy of converted data in Maximo;
Developes data cleansing functional business rules as per Maximo Business Object (MBO) definitions for source to Maximo conversion requirements;and comprehensive testing plans and executing validation processes to verify the accuracy and integrity of converted data in Maximo;
Customizes Maximo options, such as Automation Scripts, Java Customizations, Database Configuration, or Application Designer, to support data conversion requirements;
Identifies integrations within Maximo with other enterprise systems, such as ERP systems, GIS systems, or asset management systems. Knowledge of integration patterns, data synchronization, and data exchange protocols; and,
Showcases work experience as a Data Engineer, Data Architect or similar role, along with experience in programming languages including Python, PySpark, Scala, SQL.
Learn more about how we work: https://pwc.to/how-we-work
PwC does not intend to hire experienced or entry level job seekers who will need, now or in the future, PwC sponsorship through the H-1B lottery, except as set forth within the following policy: https://pwc.to/H-1B-Lottery-Policy.
All qualified applicants will receive consideration for employment at PwC without regard to race; creed; color; religion; national origin; sex; age; disability; sexual orientation; gender identity or expression; genetic predisposition or carrier status; veteran, marital, or citizenship status; or any other status protected by law. PwC is proud to be an affirmative action and equal opportunity employer.
For positions based in San Francisco, consideration of qualified candidates with arrest and conviction records will be in a manner consistent with the San Francisco Fair Chance Ordinance.
Applications will be accepted until the position is filled or the posting is removed, unless otherwise set forth on the following webpage. Please visit this link for information about anticipated application deadlines: https://pwc.to/us-application-deadlines
For positions in California, Colorado, Hawaii, Nevada, New York State, or Washington State, or for opportunities that will report to a supervisor, office or other work site in New York State, please visit the following link for pay range information: https://pwc.to/payrange-v1-advisoryseniorassociate
Show more
Show less","Azure Data Engineer Associate, Databricks Certified Data Engineer Associate, Maximo, PowerPlant, Python, PySpark, Scala, SQL, Azure ADF, AWS Glue, SSIS, DataBricks, SOAP, RESTful APIs, XML, JSON, Automation Scripts, Java Customizations, Database Configuration, Application Designer, ERP, GIS, Data Engineer, Data Architect","azure data engineer associate, databricks certified data engineer associate, maximo, powerplant, python, pyspark, scala, sql, azure adf, aws glue, ssis, databricks, soap, restful apis, xml, json, automation scripts, java customizations, database configuration, application designer, erp, gis, data engineer, data architect","application designer, automation scripts, aws glue, azure adf, azure data engineer associate, data architect, database configuration, databricks, databricks certified data engineer associate, dataengineering, erp, gis, java customizations, json, maximo, powerplant, python, restful apis, scala, soap, spark, sql, ssis, xml"
Big Data Engineer #: 19-02049,HireTalent - Diversity Staffing & Recruiting Firm,"Fairport, NY",https://www.linkedin.com/jobs/view/big-data-engineer-%23-19-02049-at-hiretalent-diversity-staffing-recruiting-firm-3764244503,2023-12-17,Williamson,United States,Mid senior,Onsite,"Company
Our Client is a custom software development firm driving unique solutions that meet the needs of our customers. This means web, mobile, design, back-end, QA, strategy - the entire solution. The core of this - our talented team leading tomorrow's creations. Want to love what you do using new technology in a relaxing environment?
Position
You've earned a bachelor's degree in Computer Science, Information Systems, Statistics, or other technical field
5&plus; years of experience in big data engineering, building real time data pipelines
Experience in a large scale, high-volume real-time production environment
Advanced SQL knowledge working with many database types.
Experience building and optimizing 'big data' pipelines, architectures and data sets.
Strong analytic skills related to working with unstructured datasets.
Deep experience with a variety of traditional, streaming, and big data tools including:
&plus; Hadoop, Hive
&plus; Spark, Kafka
&plus; SQL databases, including MySQL, Postgres.
&plus; AWS cloud services: EC2, EMR, RDS, Redshift
&plus; Stream-processing systems: Storm, Spark-Streaming, etc.
&plus; Languages: Python, Java, Scala
&plus; We use: mysqldb, snowflake, RDS, Kafka.
Experience working with Client teams is a huge plus point.
Show more
Show less","Big Data Engineering, Data Pipelines, RealTime Data Processing, SQL, Hadoop, Hive, Spark, Kafka, MySQL, PostgreSQL, AWS Cloud Services, EC2, EMR, RDS, Redshift, Storm, SparkStreaming, Python, Java, Scala, mysqldb, Snowflake","big data engineering, data pipelines, realtime data processing, sql, hadoop, hive, spark, kafka, mysql, postgresql, aws cloud services, ec2, emr, rds, redshift, storm, sparkstreaming, python, java, scala, mysqldb, snowflake","aws cloud services, big data engineering, datapipeline, ec2, emr, hadoop, hive, java, kafka, mysql, mysqldb, postgresql, python, rds, realtime data processing, redshift, scala, snowflake, spark, sparkstreaming, sql, storm"
Staff Data Engineer,Recruiting from Scratch,"Rochester, NY",https://www.linkedin.com/jobs/view/staff-data-engineer-at-recruiting-from-scratch-3744399009,2023-12-17,Williamson,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our client is a dating app.
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, deliver analysis to further improve engagement in existing features, and empower our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Solve technical problems of the highest scope and complexity
Exert significant influence on the company’s analytical long-range goals and data architecture
Define and extend our internal standards for style, maintenance, and best practices for a high-scale data platform
Provide mentorship for all on your team to help them grow in their technical responsibilities
Propose ideas to improve the scale, performance, and capabilities of the Data Platform
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We'll Love About You
7+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field
7+ years experience using Python
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention
Location
: This role will be located in office twice a week minimum in Palo Alto, San Francisco or Chicago.
Salary Range: $180,000 - 250,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data engineering, Business intelligence, Data science, Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, SQL, TDD, Pair programming, Continuous integration, Automated testing, Deployment, Kafka, Storm, Sparkstreaming, Dimensional data modeling, Schema design, Data warehouses, ETL, Data management tools, Data classification, Data retention","data engineering, business intelligence, data science, python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, tdd, pair programming, continuous integration, automated testing, deployment, kafka, storm, sparkstreaming, dimensional data modeling, schema design, data warehouses, etl, data management tools, data classification, data retention","airflow, automated testing, business intelligence, continuous integration, data classification, data engineering, data management tools, data retention, data science, data warehouses, deployment, dimensional data modeling, docker, etl, helm, kafka, kubernetes, pair programming, python, schema design, snowflake, spark, sparkstreaming, sql, storm, tdd"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Rochester, NY",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3744393521,2023-12-17,Williamson,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
They are a profitable dating application.
Location: This is a hybrid role based in their Chicago office and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location
: This role is twice a week in a hybrid role minimum.
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Apache Kafka, Apache Hadoop, Apache Spark, Apache Airflow, Snowflake, Python, Kubernetes, Docker, Helm, SQL, Data Science, Data Engineering, Data Analysis, Business Intelligence, ETL, DevOps, Continuous Integration, TestDriven Development (TDD), Pair Programming, Agile, Cloud Computing, DevOps, Data Governance, Data Classification, Data Retention","apache kafka, apache hadoop, apache spark, apache airflow, snowflake, python, kubernetes, docker, helm, sql, data science, data engineering, data analysis, business intelligence, etl, devops, continuous integration, testdriven development tdd, pair programming, agile, cloud computing, devops, data governance, data classification, data retention","agile, apache airflow, apache hadoop, apache kafka, apache spark, business intelligence, cloud computing, continuous integration, data classification, data engineering, data governance, data retention, data science, dataanalytics, devops, docker, etl, helm, kubernetes, pair programming, python, snowflake, sql, testdriven development tdd"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Rochester, NY",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3748833037,2023-12-17,Williamson,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our next evolution is underway as a newly public company looking to expand and continue to build meaningful experiences for our users. From social issues to original content, we’re blazing innovative paths with impact for our community, all while leveraging the latest tech stacks and striving for engineering excellence. At the heart of our work in this new chapter is a shared set of core values: openness and exploration, a bias for action, and strong support of the LGBTQ community.
With a track record of strong financial performance and plans for continued headcount growth, we’re looking to build a team of talented, passionate, and open-minded people who believe in our mission, align with our values, and are excited to work at the intersection of innovative technology and social impact. Come be a part of this exciting journey with us.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location: Palo Alto, San Francisco or Chicago
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Realtime Streaming Technologies, TDD, Automation, Continuous Delivery, Data Products, Data Governance, Security, Scalability, ETL Processes, Data Scientists, SREs, Product Managers, Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, SQL, Agile Engineering Practices, Pair Programming, Continuous Integration, Automated Testing, Deployment, StreamProcessing Systems, Kafka, Storm, SparkStreaming, Dimensional Data Modeling, Schema Design, Data Warehouses","data engineering, realtime streaming technologies, tdd, automation, continuous delivery, data products, data governance, security, scalability, etl processes, data scientists, sres, product managers, python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, agile engineering practices, pair programming, continuous integration, automated testing, deployment, streamprocessing systems, kafka, storm, sparkstreaming, dimensional data modeling, schema design, data warehouses","agile engineering practices, airflow, automated testing, automation, continuous delivery, continuous integration, data engineering, data governance, data products, data scientists, data warehouses, deployment, dimensional data modeling, docker, etl, helm, kafka, kubernetes, pair programming, product managers, python, realtime streaming technologies, scalability, schema design, security, snowflake, spark, sparkstreaming, sql, sres, storm, streamprocessing systems, tdd"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Rochester, NY",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3759708637,2023-12-17,Williamson,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto,
San
Francisco or Chicago
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Applied ML, Pandas, R, Data visualization, Data mining, Data cleaning, Data modeling, Data pipelines, Airflow, Kubeflow, NLP (Natural Language Processing), Large language models, Python, Java, Bash, SQL, Git, Snowflake, Kubernetes, Docker, Helm, Spark, Pyspark, AWS, GCP, Azure, DynamoDB, ETL, Kafka, Storm, Spark Streaming, Microservices, Machine learning, Data management tools, Data classification, Data retention","applied ml, pandas, r, data visualization, data mining, data cleaning, data modeling, data pipelines, airflow, kubeflow, nlp natural language processing, large language models, python, java, bash, sql, git, snowflake, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, dynamodb, etl, kafka, storm, spark streaming, microservices, machine learning, data management tools, data classification, data retention","airflow, applied ml, aws, azure, bash, data classification, data cleaning, data management tools, data mining, data retention, datamodeling, datapipeline, docker, dynamodb, etl, gcp, git, helm, java, kafka, kubeflow, kubernetes, large language models, machine learning, microservices, nlp natural language processing, pandas, python, r, snowflake, spark, spark streaming, sql, storm, visualization"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Rochester, NY",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3773087721,2023-12-17,Williamson,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto
or
San
Francisco
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data engineering, Machine learning, Python, Java, SQL, Git, Bash, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, AWS, GCP, Azure, DynamoDB, Kafka, Storm, SparkStreaming, ETL, ML Data OPs, Pandas, R, NLP, Large language models, Microservices","data engineering, machine learning, python, java, sql, git, bash, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, dynamodb, kafka, storm, sparkstreaming, etl, ml data ops, pandas, r, nlp, large language models, microservices","airflow, aws, azure, bash, data engineering, docker, dynamodb, etl, gcp, git, helm, java, kafka, kubernetes, large language models, machine learning, microservices, ml data ops, nlp, pandas, python, r, snowflake, spark, sparkstreaming, sql, storm"
"Data Analyst - SQL, Power BI - Sales/Marketing Team - Onsite",Addison Group,Greater Chicago Area,https://www.linkedin.com/jobs/view/data-analyst-sql-power-bi-sales-marketing-team-onsite-at-addison-group-3776636198,2023-12-17,Calumet City,United States,Mid senior,Onsite,"Title: Data Analyst - SQL, Power BI - Sales/Marketing Team - Onsite
Location: Northern Suburbs Chicago - Onsite five days weekly - No relocation
Salary: $130-150K + Bonus
No sponsorship
Musts: SQL, Power BI, Ability to work independently and speak to internal stakeholders. Work onsite in the Chicagoland area (Buffalo Grove).
This role will report directly to the VP of Marketing. This role requires curiosity for leveraging data to streamline business intelligence, sales enablement, and data accessibility. Effective collaboration in a team environment with the VP and other divisions as necessary to achieve data-driven solutions that transform and strengthen its data utility and accessibility for empowered decision-making. To foster a data intelligence practice that systemizes solutions through leading and lagging feedback loops. This job requires to be on-site 5 days a week.
Data Strategy
Advise service-level agreement (SLA) vendor partners on CRM database management, needs, and strategic guidance, which empower and position the CRM platform and sales enablement for success.
Create and execute a comprehensive, linear process of early opportunity identification, opportunity predictability to the financial forecasting model, which identifies inputs and outputs, and business financial impact, which informs business health.
Develop, maintain, and utilize the firm's data, data integrations, and data science tools to help produce business insights and support project initiatives.
Remain current in emerging automation, artificial intelligence, and machine learning trends that can streamline systems and address business challenges.
Dashboard Management:
Develop, manage, and hand off specific ad-hoc, pipeline reports, and dashboards.
Design business intelligence dashboards with user experience (UX) best practices that synthesize and visualize data for an executive-level, decision-orientated audience.
Leverage disparate data sources such as CRM platform, Workday, etc., through API's and SQL for activity dashboards.
Data Research:
Mining pre-stressed, watch-list-worthy opportunity-target companies through headlines, financial, and secondary data providers. Leveraging various data providers: CapitalIQ, Pitchbook, SugarCRM, Ibis World, Statista, Credit Cloud, and ZoomInfo.
Cross-reference pre-stressed pre-opportunities with CRM platform business development activity, point of contact, date, and connections.
Identify, troubleshoot, and address data discrepancies and uphold the highest integrity and ethics.
Pull ad-hoc queries on industry data and financials.
Collaboration:
Be a role player to indirect reports, junior analysts, or support staff to maintain existing dashboards, reports, and aspiring data scientists.
Develop and maintain strong relationships with key clients at all levels within the firm and with key vendors.
Collaborating with and serving various operating companies.
Additional Duties And Responsibilities As Required.
Characteristics:
Strong work ethic and interpersonal skills, emphasizing teamwork, collaboration, initiative, and integrity.
Detail-oriented and capable of working through large amounts of data efficiently.
Hands-on, resourceful, and proactive professional, able to work autonomously and operate as part of a strong, collaborative team.
Solid interpersonal communication skills and ability to work with employees at all levels of the organization.
Strong research abilities, ability to source company, contact data, and hunt down key information.
Excellent problem-solving skills.
Highly organized and detail-oriented individual who thrives in dynamic and fast-paced working environments.
Ability to prioritize and manage multiple projects/processes at one time.
A strong team player comfortable working in matrix environments.
Strong verbal and written communication skills.
Skills/Qualifications:
Master’s in a highly quantitative field (e.g., mathematics, statistics, computer science, or related fields) preferred.
Experience leading large data science projects, passionate about solving business challenges through data science.
Experience leading directly/indirectly other data scientists or aspiring data scientists.
Demonstrated research ability a plus.
Understanding of data partners such as PitchBook, Capital IQ, Credit Cloud, Bloomberg terminal, ZoomInfo, etc., is a plus.
Experience in a CRM (SugarCRM preferred), such as Salesforce, HubSpot, Microsoft, etc.
Ability to quickly pick up new data languages, emerging tools, and technology technologies.
Proficiency in PowerBI, SQL, MySQL is required, and related programming languages are a plus.
Experience in predictive modeling, data science, and analysis in both batch and streaming settings.
Experience in discrepancies detection, supervised/unsupervised learning, time-series data, and natural language processing.
Proficient with data visualization and translating complex problems into actionable insights.
Ability to maintain high confidentiality of personally identifiable information.
Proficient in Microsoft Excel, PowerPoint, Word, and Outlook.
Show more
Show less","SQL, Power BI, MySQL, Data Analysis, Data Mining, Data Visualization, Business Intelligence, Data Warehousing, Dashboard Development, Data Integration, Data Modeling, Data Science, Predictive Modeling, Machine Learning, Cloud Computing, Data Security, Data Privacy, Data Ethics, Data Governance, Data Quality, Data Pipelines, Data Lakes, Data Warehouses, Data Curation, Data Exploration, Data Storytelling, Python, R, SAS, Scala, Hadoop, Spark, Tableau, Qlik, Power BI, Google Analytics, Customer Relationship Management (CRM), Sales, Marketing, Finance, Accounting, Operations, Supply Chain Management, Risk Management, Project Management, Business Intelligence, Reporting, Budgeting, Forecasting, ProblemSolving, Analytical Thinking, Communication, Teamwork, Collaboration, Leadership, Management","sql, power bi, mysql, data analysis, data mining, data visualization, business intelligence, data warehousing, dashboard development, data integration, data modeling, data science, predictive modeling, machine learning, cloud computing, data security, data privacy, data ethics, data governance, data quality, data pipelines, data lakes, data warehouses, data curation, data exploration, data storytelling, python, r, sas, scala, hadoop, spark, tableau, qlik, power bi, google analytics, customer relationship management crm, sales, marketing, finance, accounting, operations, supply chain management, risk management, project management, business intelligence, reporting, budgeting, forecasting, problemsolving, analytical thinking, communication, teamwork, collaboration, leadership, management","accounting, analytical thinking, budgeting, business intelligence, cloud computing, collaboration, communication, customer relationship management crm, dashboard development, data curation, data ethics, data exploration, data governance, data integration, data lakes, data mining, data privacy, data quality, data science, data security, data storytelling, data warehouses, dataanalytics, datamodeling, datapipeline, datawarehouse, finance, forecasting, google analytics, hadoop, leadership, machine learning, management, marketing, mysql, operations, powerbi, predictive modeling, problemsolving, project management, python, qlik, r, reporting, risk management, sales, sas, scala, spark, sql, supply chain management, tableau, teamwork, visualization"
Senior Data Engineer,StoneX Group Inc.,"Chicago, IL",https://www.linkedin.com/jobs/view/senior-data-engineer-at-stonex-group-inc-3752008137,2023-12-17,Calumet City,United States,Mid senior,Hybrid,"Overview
The Data Platform Team looks to raise the level and productivity of data engineering and data science by building, scaling, and supporting our big data infrastructure with an emphasis on simple and efficient solutions on top of complex distributed data stores. As a contributing senior data engineer, you will assist in architecting, designing, and implementing components within our cloud data platform expanding our data assets while continuously improving the architecture and processes around our daily operations.
Responsibilities
Create cloud and big data technical design recommendations for developing and integrating new software and system technologies – from the physical layer through to the virtual layer – per written specifications; test, evaluate, engineer, implement and support said technologies.
Review, influence and contribute to new and evolving design, architecture, standards, and methods for operating and contributing to services within our big data ecosystem.
Add to our existing business and data models. Reviews existing designs and processes to highlight more efficient ways to complete existing workload more effectively through industry perspectives.
Drive technical innovation and efficiency in infrastructure operations through automation by assisting in improvements to continuous integration, continuous deployment and
Create cloud and big data technical design recommendations for developing and integrating new software and system technologies – from the physical layer through to the virtual layer – per written specifications; test, evaluate, engineer, implement and support those technologies
Collaborates with technical teams and utilizes system expertise to deliver technical solutions, continuously learning and evolving big data skillsets.
Monitors and evaluates overall strategic data infrastructure; tracks system efficiency and reliability; identifies and recommends efficiency improvements and mitigates operational vulnerabilities. Respond to and resolve emergent service problems. Design solutions using automation and self-repair rather than relying on alarming and human intervention
Qualifications
Pursuing a Bachelor’s degree or relevant work experience in Computer Science, Mathematics, Electrical Engineering or related technical discipline.
5-7 years experience developing software in a professional environment (preferably financial services but not required)
Exposure to Docker/Containers, microservices, distributed systems architecture, Kubernetes, and cloud computing preferably Azure.
Comfortable with core programming concepts and techniques (e.g. concurrency, memory management)
ETL tooling like Airflow and Databricks.
Experience in supporting API Gateways and building and consuming REST APIs along with other distribution technologies.
Familiarity with Financial Systems architecture/ecosystems, Real Time Market Data messaging and FIX Protocol a huge plus.
Foundational knowledge of data structures, algorithms, and designing for performance.
Competent in one of the following programming languages: Java, C# or Python (preferred) and willingness to learn and adopt new languages as necessary
Experience in database technology like MSSQL and one of key value and document databases like MongoDb, Redis, Dynamo Db, Casandra.
Monitoring/Observability concepts and tooling: APM, Distributed Tracing, Grafana, Splunk, Prometheus.
Excellent communications skills and the ability to work with subject matter expert to extract critical business concepts.
Ability to work and potentially lead in an Agile methodology environment.
Show more
Show less","Cloud Computing, Azure, Docker, Containers, Microservices, Kubernetes, Distributed Systems, Concurrency, Memory Management, ETL, Airflow, Databricks, API Gateways, REST APIs, Java, C#, Python, MSSQL, MongoDB, Redis, DynamoDB, Cassandra, APM, Distributed Tracing, Grafana, Splunk, Prometheus","cloud computing, azure, docker, containers, microservices, kubernetes, distributed systems, concurrency, memory management, etl, airflow, databricks, api gateways, rest apis, java, c, python, mssql, mongodb, redis, dynamodb, cassandra, apm, distributed tracing, grafana, splunk, prometheus","airflow, api gateways, apm, azure, c, cassandra, cloud computing, concurrency, containers, databricks, distributed systems, distributed tracing, docker, dynamodb, etl, grafana, java, kubernetes, memory management, microservices, mongodb, mssql, prometheus, python, redis, rest apis, splunk"
Senior Data Engineer,Motion Recruitment,"Chicago, IL",https://www.linkedin.com/jobs/view/senior-data-engineer-at-motion-recruitment-3759645327,2023-12-17,Calumet City,United States,Mid senior,Hybrid,"Lead Data Engineer We are seeking a talented and experienced full-time Lead Data Engineer to join our Analytics team for a hybrid position in either Chicago, IL or Madison, WI. As a Lead Data Engineer, you will play a pivotal role in shaping our data infrastructure and analytics capabilities, enabling data-driven decision-making across the organization. You will lead a team of two data engineers and work closely with data scientists, analysts, and other stakeholders to design, build, and maintain data pipelines, databases, and data models to support analytics and reporting solutions, including performance management, data science, and personalization.
Our company is a resort chain known for its indoor water parks, providing a year-round destination for family-friendly vacations. In addition to the water park, locations typically offer various dining options, entertainment, and comfortable accommodations for families.
As a part of the team, you are given discounted stays at any of our resort locations! In addition to being remote on Monday and Friday, you will also be able to take 6 weeks of full remote work! Required Skills & Experience
Bachelor’s or Master’s Degree in Technology, Computer Science, or similar technical field
5+ years experience in data engineering technologies; SQL, ETL tools (Matillion and dbt), Snowflake, Redshift, BigQuery
Python and/or R
Experience with cloud platforms; AWS and Azure
Desired Skills & Experience
Familiarity with data visualization tools; Tableau is preferred
2+ years in a leadership role
What You Will Be Doing Tech Breakdown
40% Matillion
35% Snowflake
25% dbt
Daily Responsibilities
40% Hands On
30% Management Duties
30% Team Collaboration
The Offer
Bonus eligible
You Will Receive The Following Benefits
Medical Insurance
Dental Benefits
Vision Benefits
Paid Time Off (PTO)
401(k) {including match- if applicable}
Applicants must be currently authorized to work in the US on a full-time basis now and in the future.
Posted By:
Mia Quinn
Show more
Show less","Data Engineering, SQL, Matillion, dbt, Snowflake, Redshift, BigQuery, Python, R, AWS, Azure, Tableau, Data Visualization, Leadership","data engineering, sql, matillion, dbt, snowflake, redshift, bigquery, python, r, aws, azure, tableau, data visualization, leadership","aws, azure, bigquery, data engineering, dbt, leadership, matillion, python, r, redshift, snowflake, sql, tableau, visualization"
Senior RCM Data Engineer - Metrics & Reporting,TAG - The Aspen Group,"Chicago, IL",https://www.linkedin.com/jobs/view/senior-rcm-data-engineer-metrics-reporting-at-tag-the-aspen-group-3781190219,2023-12-17,Calumet City,United States,Mid senior,Hybrid,"The Aspen Group (TAG) is one of the largest and most trusted retail healthcare business support organizations in the U.S. and has supported over 20,000 healthcare professionals and team members at more than 1,300 health and wellness offices across 48 states in four distinct categories: dental care, urgent care, medical aesthetics, and animal health. Working in partnership with independent practice owners and clinicians, the team is united by a single purpose: to prove that healthcare can be better and smarter for everyone. TAG provides a comprehensive suite of centralized business support services that power the impact of five consumer-facing businesses: Aspen Dental, ClearChoice Dental Implant Centers, WellNow Urgent Care, Chapter Aesthetic Studio, and AZPetVet. Each brand has access to a deep community of experts, tools and resources to grow their practices, and an unwavering commitment to delivering high-quality consumer healthcare experiences at scale.
As a part of the data engineering team, you will join one of our teams focused on a core business domain – clinical operation that generates revenue and claim. You will work alongside developers modernizing our legacy systems by developing new software and services using cloud-native GCP technologies. Our stack uses MS-SQL, SSIS, GCP Big Query, and Python, using advanced SQL and NoSQL data layers on Kubernetes operating as an active-active multi-region solution.
Responsibilities
Partner with software developers on designing and implementing the new modern cloud architecture on the Google Cloud platform.
Working with data analytics and other data engineers over designing, developing, and deploying data pipelines with established data governance guidelines and best practices.
Help drive data analytics and modeling and build data pipelines and dashboards.
Troubleshoot critical issues in a production environment when needed.
Contribute innovative ideas to improve software practices and processes for our platform.
Contribute and help manage the development and software rollout process.
Work independently and make sound technical decisions with and without guidance.
Experience Requirements
Bachelor's Degree or higher in CS, CIS, Data, Analytics, or a related field
With 5+ years of hands-on experience
Strong technical expertise, people skills, and excellent communication skills
Experience building data pipelines on cloud-based solutions with modern Python.
Experienced with SQL, ideally SQL Server or PostgreSQL data solutions.
Experience with dealing with the security and compliance side of software development.
Experienced with transactional DB, immutable DB, and data warehouse
Experienced with building and maintaining metrics, alarms, and monitoring services.
Data engineering, data analysis, data pipeline, and metrics creation skills
If you are an applicant residing in California, please view our privacy policy here:
https://careers.aspendental.com/us/en/tag-privacy-policy-for-california-employees
Show more
Show less","Cloud Computing, Data Engineering, Data Pipelines, Data Analytics, Data Modeling, Data Warehousing, Troubleshooting, SQL, Python, GCP, Kubernetes, ActiveActive MultiRegion Solutions, Data Governance, Best Practices, Metrics, Alarms, Monitoring, Security, Compliance, Software Development, Software Rollout","cloud computing, data engineering, data pipelines, data analytics, data modeling, data warehousing, troubleshooting, sql, python, gcp, kubernetes, activeactive multiregion solutions, data governance, best practices, metrics, alarms, monitoring, security, compliance, software development, software rollout","activeactive multiregion solutions, alarms, best practices, cloud computing, compliance, data engineering, data governance, dataanalytics, datamodeling, datapipeline, datawarehouse, gcp, kubernetes, metrics, monitoring, python, security, software development, software rollout, sql, troubleshooting"
Senior Data Engineer (Azure),iManage,"Chicago, IL",https://www.linkedin.com/jobs/view/senior-data-engineer-azure-at-imanage-3766798322,2023-12-17,Calumet City,United States,Mid senior,Hybrid,"We offer a flexible working policy that supports the health and well-being of our iManage employees. As an organization, we value collaborating and learning from our peers in person, while providing the necessary flexibility for our employees to have a meaningful work-life balance. Please reach out to learn more.
Being a Senior Data Engineer at iManage Means
You are excited about data and believe in the democratization of data to support data driven decision-making. You will partner with our Information Technology team to implement, support, and extend our Enterprise Data Lake hosted on Azure and built using Azure Synapse. You will gather requirements from iManage business units and craft solutions which provide access to critical business data. You will develop data models and data pipelines for our Enterprise Data Lake, and provide integration with BI platforms and tools such as Totango and Power BI. You are passionate about lakehouse architecture and have experience using Delta Lake and bronze, silver, and gold data lake design.
Here is what one of our leaders, Senior Director of Enterprise Data and Integration Engineering (Jacqueline Toepfer), has to say about the role:
“As a Senior Data Engineer on our team, you will get the opportunity to showcase your expertise and make a real difference across the organization. You will be part of a truly collaborative team that is passionate about delivering quality solutions. You will be the in-house expert in the data models of multiple, disparate enterprise SaaS systems and utilize your wealth of knowledge to provide recommendations and solutions for consolidation, transformation, and integration of the disparate data sources.”
iM Responsible For
Modeling, managing, and reporting of data stored in Azure Data Lake.
Gathering data requirements from various business units and translating these requirements into data models.
Using Python, PySpark, and system specific APIs to extract, transform, store and analyze data from a variety of systems.
Data modeling, defining data pipelines, and integrations necessary to present data in BI platforms such as Totango, or BI tools like Power BI.
Identifying and modeling all current disparate data sources and the data flows between these data sources.
Analyzing current repositories and proposing changes to data repositories and data flows to better support company objectives for the measurement of user experience and customer success.
Understanding the business needs of data integration and governance from disparate systems to drive the enhancement of the enterprise data lake.
Applying best practices to ensure the security and privacy of the data repositories.
Ensuring data repositories meet company standards for storage of PII.
iM Qualified Because I Have
A Bachelor’s degree or higher in Computer Science or related field.
2-3 years of experience working with data in a business setting.
Proficiency in data extraction and transformation utilizing Spark, Python and REST APIs.
Proficiency in data reporting and data integration utilizing Transact-SQL, SQL Views, REST APIs, or other BI Tools.
Experience designing data pipelines with a cloud-native mindset using Azure or AWS.
Experience ingesting data from SaaS solutions and other services via API or other related technologies.
Experience with Azure Synapse or adjacent technologies like Databricks.
A passion to work collaboratively within a team using an agile framework.
Commitment to understanding data requirements and delivering high quality, scalable solutions that meet those requirements.
High attention to detail.
A creative mindset with a desire to explore new technologies and create innovative solutions.
Bonus Points If I Have
Familiarity with delta lake or lakehouse technologies.
Familiarity with Microsoft Azure Event Hub or a similar event streaming solution.
A background with relational databases and data warehouse design using star schemas.
Experience with cloud-based data models for business solutions like Salesforce, Zendesk, and NetSuite.
Don't meet every qualification listed above? Studies show that women and people of color are less likely to apply to jobs unless they meet all qualifications. At iManage, we are committed to building a diverse and inclusive environment and encourage everyone to show up as their full authentic selves. We welcome those that come with a growth mindset and a hunger for learning; so, if you are excited about this role but your past experience doesn't align perfectly with every qualification, we encourage you to apply anyways!
iM Getting To
Join a supportive, experienced team with an inclusive, encouraging, and vibrant culture.
Have flexible work hours that allow me to balance my ‘me time’ with my work commitments.
Collaborate in a modern open plan workspace, with a gaming area, free snacks, drinks and regular social events.
Focus on impactful work, solving complex, real challenges utilizing the latest technologies and protocols.
Own my career path with our internal development framework. Ask us more about this!
Learn new skills and earn certifications with access to unlimited courses in LinkedIn Learning.
Join an innovative, industry leading SaaS company that is continuing to grow & scale!
iManage Is Supporting Me By
Creating an inclusive environment where I can help shape the culture not just by fitting in, but by adding to it.
Providing a market competitive salary that is applied through a consistent process, equitable for all our employees, and regularly reviewed based on industry data.
Rewarding me with an annual performance-based bonus.
Offering comprehensive Health/Vision/Dental/Life Insurance, and a 401k Retirement Savings Plan with a company match up to 4%.
Giving access to HealthJoy, a healthcare concierge service, to help me maximize my health benefits.
Granting enhanced leave for expecting parents; 20 weeks 100% paid for primary leave, and 10 weeks 100% paid for secondary leave.
Providing me with a flexible time off policy to take the time off that I need. Be it for vacation, volunteering, celebrating holidays, spending time with family, or simply taking time to recharge and reset.
Caring for my mental health and well-being with multiple company wellness days and free access to the Healthy Minds app for mindfulness, meditation and more.
About iManage
iManage is dedicated to Making Knowledge WorkTM. Over one million professionals across 65+ countries rely on our intelligent, cloud-enabled, secure knowledge work platform to uncover and activate the knowledge that exists inside their business content and communications.
We are continuously innovating to solve the most complex professional challenges and enable better business outcomes; Our work is not always easy but it is ambitious and rewarding.
So we’re looking for people who love a challenge. People who are happiest when they’re solving problems and collaborating with the industry’s best and brightest. That’s the iManage way. It’s how we do things that might appear impossible. How we develop our employees’ strengths and unlock their potential. How we find meaning in everything we do.
Whoever you are, whatever you do, however you work. Make it mean something at iManage.
iManage provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.
This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.
Learn more at: www.imanage.com
Please see our privacy statement for more information on how we handle your personal data: https://imanage.com/privacy-policy/
Show more
Show less","Azure, Azure Synapse, TransactSQL, SQL Views, REST APIs, Python, PySpark, Data modeling, Data pipelines, BI platforms, Totango, Power BI, Spark, AWS, SaaS, Event streaming, Microsoft Azure Event Hub, Agile, Star schemas, Salesforce, Zendesk, NetSuite, Delta Lake, Lakehouse","azure, azure synapse, transactsql, sql views, rest apis, python, pyspark, data modeling, data pipelines, bi platforms, totango, power bi, spark, aws, saas, event streaming, microsoft azure event hub, agile, star schemas, salesforce, zendesk, netsuite, delta lake, lakehouse","agile, aws, azure, azure synapse, bi platforms, datamodeling, datapipeline, delta lake, event streaming, lakehouse, microsoft azure event hub, netsuite, powerbi, python, rest apis, saas, salesforce, spark, sql views, star schemas, totango, transactsql, zendesk"
Lead Data Engineer,Jarvis Consulting Group,"Ontario, CA",https://www.linkedin.com/jobs/view/lead-data-engineer-at-jarvis-consulting-group-3772374932,2023-12-17,Corona,United States,Mid senior,Onsite,"Jarvis Consulting Group
Location:
Hybrid, downtown Toronto (2x/week)
Start Date:
ASAP
We have an exciting opportunity for the right person – do you have Big Data experience, Spark ETL, Scala, Databricks, SQL development and want to be part of a new team? We are looking for you. You will be working with one of the best new consulting companies and have an integral role in their new data consulting practice.
Work you will do
Understand Big Data and Cloud environments from a performance and capacity management standpoint, using the tools needed to onboard, monitor and move data into these environments
Collaborate as the Big Data and Cloud Technical lead with multiple technology teams to ensure that the appropriate associated applications, integrations, infrastructure, and security architecture are designed to meet evolving business requirements, meet standards for reliability, scalability, performance, and availability
Be the lead for internal and external products and projects. Helping to build world class big data software that solves real problems.
Responsible for the successful implementation of technical solutions for projects, supporting highly complex business applications with complex integration needs across multiple technology disciplines
Formulate and define project scope, and objectives based on a thorough understanding of the technical requirements of the projects
Use sound Agile development practices (code reviews, unit testing, etc.) to develop and deliver quality code and data products
Provide day-to-day support and technical expertise to both technical and non-technical teams
Help to develop junior talent and transform them into experts
Work with other engineers to brainstorm solutions to problems and support bank objectives
Who We Are Looking For
Build out scalable and reliable ETL pipelines and processes to ingest data from a large number and variety of data sources.
4+ years Spark Development experience using Scala
Knowledge of Hadoop and Cloud Big Data ecosystem (Spark, Hive, HDFS, Azure/AWS/GCP, etc)
Experience with streaming technologies such as Kafka and Spark Streaming
Experience with Agile development (JIRA / Confluence)
Experience with version control systems, such as Git
Knowledge of dev tools like IDE’s, Maven, SBT
Knowledge of orchestration tools like Airflow, Autosys, Oozie and Cron
Experience with containerization (Docker) and orchestration (Kubernetes) is a plus
Experience with developing and deploying applications to the cloud environment
Who You Are
Works well both individually and as part of a team
Proven ability to work creatively in a problem-solving environment
Strong communication skills
Able to work closely with technical and non-technical team in a collaborative environment
University degree in relevant STEM discipline (Computer Sciences, Electrical/Computer/Software Engineering and Mathematics)
Show more
Show less","Big Data, Spark ETL, Scala, Databricks, SQL, Hadoop, Hive, HDFS, Kafka, Spark Streaming, Agile development, JIRA, Confluence, Git, Maven, SBT, Airflow, Autosys, Oozie, Cron, Docker, Kubernetes, ETL pipelines, Cloud Big Data ecosystem, Azure, AWS, GCP, Cloud environment, STEM","big data, spark etl, scala, databricks, sql, hadoop, hive, hdfs, kafka, spark streaming, agile development, jira, confluence, git, maven, sbt, airflow, autosys, oozie, cron, docker, kubernetes, etl pipelines, cloud big data ecosystem, azure, aws, gcp, cloud environment, stem","agile development, airflow, autosys, aws, azure, big data, cloud big data ecosystem, cloud environment, confluence, cron, databricks, docker, etl pipelines, gcp, git, hadoop, hdfs, hive, jira, kafka, kubernetes, maven, oozie, sbt, scala, spark etl, spark streaming, sql, stem"
Homes.com - Senior Data Engineer,Homes.com,"Irvine, CA",https://www.linkedin.com/jobs/view/homes-com-senior-data-engineer-at-homes-com-3785824063,2023-12-17,Corona,United States,Mid senior,Onsite,"Overview
Job Description
CoStar Group (NASDAQ: CSGP) is a leading global provider of commercial and residential real estate information, analytics, and online marketplaces. Included in the S&P 500 Index and the NASDAQ 100, CoStar Group is on a mission to digitize the world’s real estate, empowering all people to discover properties, insights and connections that improve their businesses and lives.
Homes.com is already one of the fastest growing real estate portals in the industry, we are driven to be #1. Just ask Brad Bellflower, Chief Change Officer at Apartments.com. After its acquisition in 2014, Apartments.com quickly turned into the most popular place to find a place. Proven success at the highest level – and we’re doing it again with the new Homes.com. Homes.com is a CoStar Group company with 20+ years' experience in leading and growing digital marketplaces. We pride ourselves on continually improving, innovating, and setting the standard for property search and marketing experiences. With Homes.com we’re building a brand on the cusp of defining the industry.
We’re looking for big thinkers, brave leaders, and creative advertising wizards ready to influence a new age of homebuying within a tried-and-true, award-winning company.
Learn more about Homes.com.
CoStar delivers real-time, verified real estate data that helps clients confidently spot great opportunities and make smart choices ahead of competitors. By combining the power of CoStar’s independent research organization – the industry’s largest – with global data delivery, software, and application solutions, clients can act on opportunities with confidence.
We are hiring a talented Data Engineer to build cloud-based data pipelines for machine learning, data processing with Apache Spark, and database development.
Responsibilities
Designing, building, testing and deploying scalable, reusable and maintainable applications that handle substantial amounts of data.
Taking full ownership of your work from development and testing to eventual deployment and support in production.
Collaborating with other engineers, product owners, designers, and leadership.
Becoming a trusted team member in matters of technical architecture, design and code.
Advocating for evolution and improvement - both technical and non-technical - within our teams. Includes new tech, tools and best practices.
Gaining a deep understanding of the CoStar business, including the Analytic products.
Basic Qualifications
Bachelor’s Degree required from an accredited, not for profit university or college, with degree preferably in Computer Science, Data Science, or related field. MSc or PhD is a plus.
A track record of commitment to prior employers
A demonstrable record of accomplishment of building and launching successful products that use terabytes of data
8+ years of data pipeline engineering experience, and/or deep database engineering experience
Ability to analyze technical requirements and design new architectures, data models and ETL strategies
Hands-on experience with cloud-based relational and non-relational databases
Deliver work products that meet specifications, are free of defects, with excellent performance
Define Architecture and Development best practices
Facilitate discussions among developers to exchange ideas and introduce innovative technologies
Evaluate emerging technologies to find opportunities, trends and best practices that can be used to strengthen CoStar’s technology platform and development practices
Willingness to take an active role in mentoring other developers
Preferred Skills
Performance tuning of database queries (SQL Server or PostgreSQL), database design, monitoring, and analysis
Experience with No-SQL databases (e.g. DynamoDB)
Experience with data pipeline tools (e.g. Glue, Step Functions, Lambda)
Experience using Confluent Kafka
Knowledge and/or experience working with Apache Spark/Databricks
Monitoring & dashboard metric management (e.g. CloudWatch, Kibana)
What’s In It For You
When you join CoStar Group, you’ll experience a collaborative and innovative culture working alongside the best and brightest to empower our people and customers to succeed.
We offer you generous compensation and performance-based incentives. CoStar Group also invests in your professional and academic growth with internal training, tuition reimbursement, and an inter-office exchange program.
Our Benefits Package Includes (but Is Not Limited To)
Comprehensive healthcare coverage: Medical / Vision / Dental / Prescription Drug
Life, legal, and supplementary insurance
Virtual and in person mental health counseling services for individuals and family
Commuter and parking benefits
401(K) retirement plan with matching contributions
Employee stock purchase plan
Paid time off
Tuition reimbursement
On-site fitness center and/or reimbursed fitness center membership costs (location dependent), with yoga studio, Pelotons, personal training, group exercise classes
Access to CoStar Group’s Diversity, Equity, & Inclusion Employee Resource Groups
Complimentary gourmet coffee, tea, hot chocolate, fresh fruit, and other healthy snacks
We welcome all qualified candidates who are currently eligible to work full-time in the United States to apply. However, please note that CoStar Group is not able to provide visa sponsorship for this position.
This position offers a base salary range of $132,800.00 - 230,000.00, based on relevant skills and experience and includes a generous benefits plan.
CoStar Group is an Equal Employment Opportunity Employer; we maintain a drug-free workplace and perform pre-employment substance abuse testing
Show more
Show less","Data Engineering, Cloud Computing, Apache Spark, ETL, SQL Server, PostgreSQL, DynamoDB, Glue, Step Functions, Lambda, Confluent Kafka, Kibana, CloudWatch, NoSQL, Database Design, Performance Tuning","data engineering, cloud computing, apache spark, etl, sql server, postgresql, dynamodb, glue, step functions, lambda, confluent kafka, kibana, cloudwatch, nosql, database design, performance tuning","apache spark, cloud computing, cloudwatch, confluent kafka, data engineering, database design, dynamodb, etl, glue, kibana, lambda, nosql, performance tuning, postgresql, sql server, step functions"
Sr. Data Integration Developer,MemorialCare,"Fountain Valley, CA",https://www.linkedin.com/jobs/view/sr-data-integration-developer-at-memorialcare-3775458294,2023-12-17,Corona,United States,Mid senior,Onsite,"Title:
Sr. Data Integration Developer
Location:
Fountain Valley, CA / Remote
Department:
IS Healthcare Intelligence
Status:
Full Time
Shift:
Days
Pay Range*: $117, 852.80 - $170,851.20
At MemorialCare Health System, we believe in providing extraordinary healthcare to our communities and an exceptional working environment for our employees. Memorial Care stands for excellence in Healthcare. Across our family of medical centers, we support each one of our bright, talented employees in reaching the highest levels of professional development, contribution, collaboration and accountability. Whatever your role and whatever expertise you bring, we are dedicated to helping you achieve your full potential in an environment of respect, innovation and teamwork.
Position Summary
Members of the MemorialCare Data Integration job family work side by side with our end users to deliver analytics solutions to the enterprise through data analysis, data profiling, extraction and transformations, data modeling, and leveraging the appropriate Business Intelligence tools. Members from this job family are accountable for growing, as well as enhancing and maintaining the enterprise data warehouse (EDW), used for reporting and data analysis. The EDW is considered a core component of business intelligence acting as a central repository for source data. In support of the EDW, team members develop automated processes for integrating data from one or more disparate sources. They analyze and perform data profiling to ensure data quality and may perform data cleansing if required prior to delivering data into the EDW to be used for reporting. They build reusable data models and data marts to support data governance and consistency in reporting. They analyze and provide resolutions to all EDW related data issues and coordinate with developers or subject matter experts (SME) to validate all requirements. Data Integration members design and develop reliable, accurate, and robust processes to share and exchange data with internal customers or external vendor solutions on a near real time and/or batch basis. They demonstrate integrity, ethics and professional conduct at all times.
The Sr. Data Integration Developer is considered a subject matter expert involved with all phases of delivering analytics integration solutions from technical requirements gathering, design, development, Quality Assurance, implementation and post implementation problem solving. They independently provide an advanced level of development expertise with their focus geared more towards complex and overall large projects. They can effectively diagnose and resolve complex problems quickly and act as a mentor to the Data Integration Developers.
Essential Functions And Responsibilities Of The Job
Participates and manages multiple data integration projects. Develops project plans, project roadmaps, and strategizes the project execution from inception to completion. Develops, maintains, and demonstrates in-depth business knowledge in assigned areas, and expertise in contemporary use of automation to improve business performance.
Conducts data sourcing by analyzing applications, analyzing, defining and validating data, identifying the relationship among data objects, and performing basic data modeling activities. Uses Structured Query Language (SQL) against diverse data sources.
Develops a robust and extensible data acquisition and integration solution for complex systems to meet the requirements of the business.
Provides technical and administrative support of complex, enterprise-wide analytics systems to ensure data integrity and proper system functioning. Ensures data presented to customers is connected to the enterprise data dictionary, standardized terminologies, and provides insight into data lineage and transformations after initial creation.
Creates new data structures using data modeling software and design principles to support more robust analysis capabilities. Develops solutions that are high performance, rapid response time, and as close to real-time as possible.
Provides guidance and reviews work of lower level staff on development projects. Provides coaching as necessary.
Mentors lower level staff and customers on analytics integration principles and best practices.
Develops a robust and extensible data acquisition and integration solution for systems to meet the functional and non-functional requirements of the business.
Works to analyze and understand source and target data systems for mapping purposes. Defines and captures any additional metadata associated with data handling processes.
Develops and maintains data extraction scripts using MemorialCare’ s enterprise information management (EIM) toolset from data sources, populating data warehouse and metadata repository data models in supporting existing and new applications.
Optimizes query and database performance by applying best practices in database, data model, and query tuning techniques.
Works closely with customers and technical team members, initiating and maintaining positive and proactive relationships and working collaboratively to deliver data integration solutions that are high performance, rapid response time, meets deadlines, and as close to real-time as possible.
Adheres to and promotes best practices and standard work for all recurring activities
Placement in the pay range is based on multiple factors including, but not limited to, relevant years of experience and qualifications. In addition to base pay, there may be additional compensation available for this role, including but not limited to, shift differentials, extra shift incentives, and bonus opportunities. Health and wellness is our passion at MemorialCare—that includes taking good care of employees and their dependents. We offer high quality health insurance plan options, so you can select the best choice for your family. And there’s more...Check out our MemorialCare Benefits for more information about our Benefits and Rewards.
Show more
Show less","SQL, Structured Query Language, Enterprise data warehouse (EDW), Data warehouse, Data integration, Data modeling, Data cleansing, Data profiling, Data governance, Data consistency, Data quality, Data analysis, Data transformation, Data dictionary, Data lineage, Data marts, Data acquisition, Data extraction, Data migration, Data mapping, Data modeling software, Data visualization, Business intelligence tools, Automation, Business performance, Data integrity, Data security, Analytics integration, Best practices","sql, structured query language, enterprise data warehouse edw, data warehouse, data integration, data modeling, data cleansing, data profiling, data governance, data consistency, data quality, data analysis, data transformation, data dictionary, data lineage, data marts, data acquisition, data extraction, data migration, data mapping, data modeling software, data visualization, business intelligence tools, automation, business performance, data integrity, data security, analytics integration, best practices","analytics integration, automation, best practices, business intelligence tools, business performance, data acquisition, data consistency, data dictionary, data extraction, data governance, data integration, data integrity, data lineage, data mapping, data marts, data migration, data modeling software, data profiling, data quality, data security, data transformation, dataanalytics, datacleaning, datamodeling, datawarehouse, enterprise data warehouse edw, sql, visualization"
Data Analyst I,Stewart Title,"Irvine, CA",https://www.linkedin.com/jobs/view/data-analyst-i-at-stewart-title-3775315412,2023-12-17,Corona,United States,Mid senior,Onsite,"Are you looking to help transform a company and an industry? As a Stewart employee, you’ll be joining a company that is committed to helping you own, develop, and nurture your career while growing our company. We invest in your career journey because we understand that as you grow so does our company. You will be part of a diverse and equitable work environment that reflects the customers we serve. You’ll be empowered to use your unique experiences, passion and skills to help our company constantly evolve and improve. Together, we can achieve our vision of becoming the premier title services company.
More information can be found at https://www.stewart.com, subscribe to the Stewart blog at https://blog.stewart.com or follow Stewart on Twitter®
@stewarttitleco
.
Job Description
Job Summary
The Data Analyst I provides ongoing project support for new and existing Data Solutions clients. This position supports both internal and external business needs while managing the overall project lifecycle to ensure business standards and client expectations are met. The candidate must have prior SQL and Python experience; and is able to extract, transform, load, and analyze data from clients, data providers and internal sources. The project lifecycle will include the following: develop project plans / client criteria, order management, file transfers, intermediate processing, external file transmissions, and pre to post project reporting. Additionally, the Data Analyst I will complete tasks related to production support including day-to-day operations, product business requirements, technical requirements, documenting procedures, client requirements, client implementation, and day to day production support tasks. This position is for Informative Research, a subsidiary of Stewart Title.
Job Responsibilities
Day to day production support operations.
Communicate with clients to support projects pre- and post-campaign.
Monitor progress, accuracy, risk, and completion of all assigned campaigns.
Communicate status with team members, end users and clients within client expectations.
Quality control – verify the accuracy of completed projects.
Collects, analyzes, and interprets data to support the reporting needs of the business
Provides data-based trends, recommendations and resolutions to the organization
Performs a wide range of support functions to assist in departmental processes
Follows clearly defined procedures to complete daily tasks and responsibilities
Uses existing procedures to solve straightforward problems without the consistent need to exercise discretion
Uses basic communication skills to address internal and/or external clients and/or team members
Individual contributor working under direct supervision with little autonomy
Performs all other duties as assigned by management
Education
High school diploma required; Bachelor’s preferred
Experience
Typically requires 0-2 years of related work experience
Equal Employment Opportunity Employer
Stewart is committed to ensuring that its online application process provides an equal employment opportunity to all job seekers, including individuals with disabilities. If you have a disability and need assistance or an accommodation in the application process, please contact us by email at careers@stewart.com.
Pay Range & Benefits
The base salary range provided is consistent with similar roles at the Company. The base salary range is not an absolute, but a guide, and actual offers will be based on the individual candidate’s knowledge, skills, education, experience, location, market conditions, and other compensation components. Depending upon all of the preceding considerations, the base salary may be lower or higher than the stated range.
Stewart offers eligible employees a competitive benefits package that includes, but is not limited to a variety of health and wellness insurance options and programs, paid time off, 401K with company match, employee stock purchase program, and employee discounts
Show more
Show less","SQL, Python, Data analysis, Data mining, Data reporting, Data visualization, Communication, Problem solving, Teamwork, Attention to detail, Accuracy, Discretion, High school diploma, Bachelor's degree (preferred), 02 years of related work experience","sql, python, data analysis, data mining, data reporting, data visualization, communication, problem solving, teamwork, attention to detail, accuracy, discretion, high school diploma, bachelors degree preferred, 02 years of related work experience","02 years of related work experience, accuracy, attention to detail, bachelors degree preferred, communication, data mining, data reporting, dataanalytics, discretion, high school diploma, problem solving, python, sql, teamwork, visualization"
"Staff Engineer, Test Data and Instrumentation",Rivian,"Irvine, CA",https://www.linkedin.com/jobs/view/staff-engineer-test-data-and-instrumentation-at-rivian-3628359506,2023-12-17,Corona,United States,Mid senior,Onsite,"About Rivian
Rivian is on a mission to keep the world adventurous forever. This goes for the emissions-free Electric Adventure Vehicles we build, and the curious, courageous souls we seek to attract.
As a company, we constantly challenge what’s possible, never simply accepting what has always been done. We reframe old problems, seek new solutions and operate comfortably in areas that are unknown. Our backgrounds are diverse, but our team shares a love of the outdoors and a desire to protect it for future generations.
Responsibilities
Assist in developing engineering group data logging and storage strategies
Identify acquisition hardware needs across multiple development programs and phases
Integrate high volume cloud data with bespoke and specialized engineering acquisition systems (fleet durability, road load data capture, etc.)
Research and implement emerging technologies to improve and optimize company-wide practices
Develop, specify, procure, use and train others in use of, and maintain data acquisition sensors, systems, practices and databases for all aspects of vehicle measurement (thermal, strain, NVH, and digital networks among others)
Leading Vehicle level design validation (DVP) testing instrumentation efforts
Liaison across multiple engineering departments
Qualifications
7+ years in a similar role
Bachelors Degree or higher required
Mechanical/Mechatronics or similar background desired
Embedded Controls knowledge and cloud / big data system management preferred
Detailed experience working hands-on with in-vehicle data acquisition systems including experiment design, installation, use, and data analysis
Direct experience working with CAN, LIN, and Ethernet vehicle data networks, including debugging and reverse engineering
Racing/professional high performance instrumentation background preferred
Proving grounds and on-road / on-track experience a plus
Willing to travel up to of 25% of the time
Eye for detail with a focus on quality
Proven experience with at least 2 major vehicle systems (EG, Electric Powertrain, Chassis / Suspension, HVAC, Braking, etc.)
Pay Disclosure
Salary range for California Based Applicants: $150,000-$173,000 (actual compensation will be determined based on experience, location, and other factors permitted by law).
Benefits Summary: Rivian provides robust medical/Rx, dental and vision insurance packages for full-time employees, their spouse or domestic partner, and children up to age 26. Coverage is effective on the first day of employment, and Rivian covers most of the premiums.
Equal Opportunity
Rivian is an equal opportunity employer and complies with all applicable federal, state, and local fair employment practices laws. All qualified applicants will receive consideration for employment without regard to race, color, religion, national origin, ancestry, sex, sexual orientation, gender, gender expression, gender identity, genetic information or characteristics, physical or mental disability, marital/domestic partner status, age, military/veteran status, medical condition, or any other characteristic protected by law.
Rivian is committed to ensuring that our hiring process is accessible for persons with disabilities. If you have a disability or limitation, such as those covered by the Americans with Disabilities Act, that requires accommodations to assist you in the search and application process, please email us at candidateaccommodations@rivian.com .
Candidate Data Privacy
Rivian may collect, use and disclose your personal information or personal data (within the meaning of the applicable data protection laws) when you apply for employment and/or participate in our recruitment processes (“Candidate Personal Data”). This data includes contact, demographic, communications, educational, professional, employment, social media/website, network/device, recruiting system usage/interaction, security and preference information. Rivian may use your Candidate Personal Data for the purposes of (i) tracking interactions with our recruiting system; (ii) carrying out, analyzing and improving our application and recruitment process, including assessing you and your application and conducting employment, background and reference checks; (iii) establishing an employment relationship or entering into an employment contract with you; (iv) complying with our legal, regulatory and corporate governance obligations; (v) recordkeeping; (vi) ensuring network and information security and preventing fraud; and (vii) as otherwise required or permitted by applicable law.
Rivian may share your Candidate Personal Data with (i) internal personnel who have a need to know such information in order to perform their duties, including individuals on our People Team, Finance, Legal, and the team(s) with the position(s) for which you are applying; (ii) Rivian affiliates; and (iii) Rivian’s service providers, including providers of background checks, staffing services, and cloud services.
Rivian may transfer or store internationally your Candidate Personal Data, including to or in the United States, Canada, the United Kingdom, and the European Union and in the cloud, and this data may be subject to the laws and accessible to the courts, law enforcement and national security authorities of such jurisdictions.
Please note that we are currently not accepting applications from third party application services.
Show more
Show less","Data logging, Data storage, Data acquisition, Cloud computing, Big data, CAN, LIN, Ethernet, Vehicle data networks, Instrumentation, Mechanical engineering, Mechatronics, Embedded controls, Racing, High performance instrumentation, Proving grounds, Onroad testing, Ontrack testing, Quality assurance, Vehicle systems","data logging, data storage, data acquisition, cloud computing, big data, can, lin, ethernet, vehicle data networks, instrumentation, mechanical engineering, mechatronics, embedded controls, racing, high performance instrumentation, proving grounds, onroad testing, ontrack testing, quality assurance, vehicle systems","big data, can, cloud computing, data acquisition, data logging, data storage, embedded controls, ethernet, high performance instrumentation, instrumentation, lin, mechanical engineering, mechatronics, onroad testing, ontrack testing, proving grounds, quality assurance, racing, vehicle data networks, vehicle systems"
"SR. Scala Engineer, Database Engineering",Experfy,"Riverside, CA",https://www.linkedin.com/jobs/view/sr-scala-engineer-database-engineering-at-experfy-3590297984,2023-12-17,Corona,United States,Mid senior,Remote,"As a Sr. Software Engineer for our Data Platform Engineering team you will join skilled Scala engineers and core database developers responsible for developing hosted cloud analytics infrastructure (Apache Spark-based), distributed SQL
processing frameworks, proprietary data science platforms, and core database optimization. This team is responsible for building the automated, intelligent, and highly performant query planner and execution engines, RPC calls between data
warehouse clusters, shared secondary cold storage, etc. This includes building new SQL features and customer-facing functionality, developing novel query optimization techniques for industry-leading performance, and building a database
system that's highly parallel, efficient and fault-tolerant. This is a vital role reporting to exec leadership and senior engineering leadership.
Requirements
Responsibilities:
Writing Scala code with tools like Apache Spark + Apache Arrow to build a hosted, multi-cluster data warehouse for Web3
Developing database optimizers, query planners, query and data routing mechanisms, cluster-to-cluster communication, and workload management techniques
Scaling up from proof of concept to ""cluster scale"" (and eventually hundreds of clusters with hundreds of terabytes each), in terms of both infrastructure/architecture and problem structure
Codifying best practices for future reuse in the form of accessible, reusable patterns, templates, and code bases to facilitate meta data capturing and management
Managing a team of software engineers writing new code to build a bigger, better, faster, more optimized HTAP database (using Apache Spark, Apache Arrow and a wealth of other open source data tools)
Interacting with exec team and senior engineering leadership to define, prioritize, and ensure smooth deployments with other operational components
Highly engaged with industry trends within analytics domain from a data acquisition processing, engineering, management perspective
Understand data and analytics use cases across Web3 / blockchains
Skills & Qualifications
Bachelor's degree in computer science or related technical field. Masters or PhD a plus
6+ years experience engineering software and data platforms / enterprise-scale data warehouses, preferably with knowledge of open source Apache stack (especially Apache Spark, Apache Arrow, and others)
3+ years experience with Scala and Apache Spark
A track record of recruiting and leading technical teams in a demanding talent market
Rock solid engineering fundamentals; query planning, optimizing and distributed data warehouse systems experience is preferred but not required
Nice to have: Knowledge of blockchain indexing, web3 compute paradigms, Proofs and consensus mechanisms... is a strong plus but not required
Experience with rapid development cycles in a web-based environment
Strong scripting and test automation knowledge
Nice to have: Passionate about Web3, blockchain, decentralization, and a base understanding of how data/analytics plays into this
Show more
Show less","Apache Spark, Apache Arrow, Scala, SQL, Data science platforms, Query planner, Execution engines, RPC calls, Shared secondary cold storage, Code optimization techniques, HTAP database, Data acquisition, Data processing, Data engineering, Data management, Web3, Blockchain, Proofs, Consensus mechanisms, Scripting, Test automation","apache spark, apache arrow, scala, sql, data science platforms, query planner, execution engines, rpc calls, shared secondary cold storage, code optimization techniques, htap database, data acquisition, data processing, data engineering, data management, web3, blockchain, proofs, consensus mechanisms, scripting, test automation","apache arrow, apache spark, blockchain, code optimization techniques, consensus mechanisms, data acquisition, data engineering, data management, data processing, data science platforms, execution engines, htap database, proofs, query planner, rpc calls, scala, scripting, shared secondary cold storage, sql, test automation, web3"
Principal Data Analyst - Remote | WFH,Get It Recruit - Information Technology,"Tustin, CA",https://www.linkedin.com/jobs/view/principal-data-analyst-remote-wfh-at-get-it-recruit-information-technology-3783316696,2023-12-17,Corona,United States,Mid senior,Remote,"Are you passionate about unraveling the insights hidden within data? We are seeking a skilled and experienced Data Platform Analyst to join our dynamic team. In this role, you will play a crucial part in conducting in-depth data analysis, contributing to data model design, and ensuring accurate data mapping.
Responsibilities
Conduct thorough data analysis to extract valuable insights.
Collaborate with cross-functional teams to design effective data models.
Ensure precise data mapping to support the organization's goals.
Qualifications
Bachelor's degree in Computer Science or a related field (Electronics, IT).
Foreign degree equivalents are accepted.
Minimum of 5 years of progressive experience in healthcare data platforms.
Additional Information
Full-time position, 40 hours per week.
Location: Orange, CA, with the option for remote work.
Why Join Us
Opportunity to work in a dynamic and collaborative environment.
Remote work option available, providing flexibility.
Contribute to meaningful projects in the healthcare sector.
If you are a data enthusiast with a background in data analysis and a passion for healthcare, we invite you to apply and be part of our innovative team. We look forward to welcoming dedicated professionals who share our commitment to excellence.
Employment Type: Full-Time
Show more
Show less","Data Analysis, Data Modeling, Data Mapping, Healthcare Data Platforms, Data Extraction, CrossFunctional Collaboration, Business Intelligence, SQL, Python, Tableau, Power BI","data analysis, data modeling, data mapping, healthcare data platforms, data extraction, crossfunctional collaboration, business intelligence, sql, python, tableau, power bi","business intelligence, crossfunctional collaboration, data extraction, data mapping, dataanalytics, datamodeling, healthcare data platforms, powerbi, python, sql, tableau"
(Remote) Principal Data Quality Engineer,First American,"Santa Ana, CA",https://www.linkedin.com/jobs/view/remote-principal-data-quality-engineer-at-first-american-3785856791,2023-12-17,Corona,United States,Mid senior,Remote,"Who We Are
Join a team that puts its People First! Since 1889, First American (NYSE: FAF) has held an unwavering belief in its people. They are passionate about what they do, and we are equally passionate about fostering an environment where all feel welcome, supported, and empowered to be innovative and reach their full potential. Our inclusive, people-first culture has earned our company numerous accolades, including being named to the Fortune 100 Best Companies to Work For® list for eight consecutive years. We have also earned awards as a best place to work for women, diversity and LGBTQ+ employees, and have been included on more than 50 regional best places to work lists. First American will always strive to be a great place to work, for all. For more information, please visit www.careers.firstam.com.
What We Do
Responsible for working with the business to develop the strategic vision, product planning and product roadmap, and managing user acceptance.. Supports the projects team execution of the vision through the SDLC. The Product Manager is expected to work with customers to develop and prioritize high level product requirements, define the product vision and partner with the business analysts to ensure customer satisfaction and goals are met. The Product Manager family would be the next career path for a Business Analyst or Project Manager.
We’re looking for an experienced Senior Product Manager to help drive the future of our enterprise data platform. As a leader in the Enterprise Data Delivery team, you’ll focus on building a single, reliable, trusted source of data for consumers, which reaches all aspects of our business. The ideal candidate should be obsessed about product, data, and consumer experience. Be part of a new future as we expand data use and literacy across First American.
Success in this role means:
You look forward to interacting with customers and internal stakeholders to formulate strategies and shape product direction
You are comfortable creating requirement and feature details based on customer interviews, surveys, and metrics analysis
You enjoy evangelizing your products including authoring blogs, technical tutorials, webinars, and more
You are adept at working with engineering teams to translate customer needs into workable technology solutions
You empathize with individuals while also look at the broader picture and bridge the two
You don’t mind being hands-on, able to use the product as users do and putting yourself in their shoes
You are a leader and team player comfortable with uncertainty, change, and figuring things out as they come
Responsibilities:
Manage an 18-month product strategy, including the objectives, initiatives, and roadmap
Collaborate with leadership and customers to formulate a longer-term strategy
Creates and executes the product roadmap
Facilitate collaboration to build empathy, understand customer needs, and validate hypotheses
Writes user stories and acceptance criteria
Ensures the right work is brought into the team backlog (Rejects or prioritizes work as needed)
Create context with user story mapping and user journey mapping
Breaks down efforts into smaller deliverables that create incremental value
Articulates functional details to developers based on customer requirements
Communicates product delivery status and progress
Release and feature definition, including marketing new releases or launches
Frequently seek feedback from stakeholders and customers
Helps validate and accept development builds and provides feedback to the development team on usability
Demonstrates technical product expertise from a functional perspective
Participates in Agile ceremonies
Drive customer feedback loops throughout the product development process
Qualification:
BA/BS required. Technical degree preferred such as computer science or engineering.
2+ years in business intelligence, ETL, data engineering, data analysis, or data warehousing
Strong knowledge in SQL with the ability to write complex queries
Agile product management training
3-5 years of product management experience
Knowledge or experience with Snowflake or similar products a plus
Pay Range: $71,500 - $148,500 Annually
This hiring range is a reasonable estimate of the base pay range for this position at the time of posting. Pay is based on a number of factors which may include job-related knowledge, skills, experience, business requirements and geographic location
.
What We Offer
By choice, we don’t simply accept individuality – we embrace it, we support it, and we thrive on it! Our People First Culture celebrates diversity, equity and inclusion not simply because it’s the right thing to do, but also because it’s the key to our success. We are proud to foster an authentic and inclusive workplace For All. You are free and encouraged to bring your entire, unique self to work. First American is an equal opportunity employer in every sense of the term.
Based on eligibility, First American offers a comprehensive benefits package including medical, dental, vision, 401k, PTO/paid sick leave and other great benefits like an employee stock purchase plan.
Show more
Show less","Product management, User experience, Agile, Cloud computing, Data warehousing, Business intelligence, Data engineering, Data analysis, SQL, Snowflake","product management, user experience, agile, cloud computing, data warehousing, business intelligence, data engineering, data analysis, sql, snowflake","agile, business intelligence, cloud computing, data engineering, dataanalytics, datawarehouse, product management, snowflake, sql, user experience"
Lead Data Engineer / Gaming Space / Hybrid / Los Angeles,Motion Recruitment,"Irvine, CA",https://www.linkedin.com/jobs/view/lead-data-engineer-gaming-space-hybrid-los-angeles-at-motion-recruitment-3784913588,2023-12-17,Corona,United States,Mid senior,Hybrid,"Our client is in the gaming space looking for a Senior Data Engineer with 6-10 years of experience to lead a team across the Data landscape. This person ideally has experience in the gaming industry and has experience across Data Engineering, Analytics, and machine learning. Located near Santa Monica this role will be mostly onsite.
Basic Qualifications (Required Skills & Experience)
6-10 years of experience
Data Engineering experience
Data Analytics experience
Machine Learning experience
Python, R, SQL, AWS, Tableau, PowerBI
Other Qualifications & Desired Competencies
Onsite in Santa Monica / El segundo
Bonus on top of base salary / equity / relocation package
You Will Receive The Following Benefits
Medical Insurance
Dental Benefits
Vision Benefits
401(k) matching
Posted By:
Casey Ryan
Show more
Show less","Data Engineering, Data Analytics, Machine Learning, Python, R, SQL, AWS, Tableau, PowerBI","data engineering, data analytics, machine learning, python, r, sql, aws, tableau, powerbi","aws, data engineering, dataanalytics, machine learning, powerbi, python, r, sql, tableau"
Data Analyst/Business Intelligence Manager,Essex Property Trust,"Irvine, CA",https://www.linkedin.com/jobs/view/data-analyst-business-intelligence-manager-at-essex-property-trust-3785803526,2023-12-17,Corona,United States,Mid senior,Hybrid,"The Data Platform team is a tight group of technologists and data scientists whose mission is to spread insight, information, and data driven solutions throughout the organization. We have complex data pipelines, large macroeconomic and market-based data sets, as well as our proprietary in-house data to develop with.
The Data Analyst / Business Intelligence Manger is responsible for designing, developing, implementing, and supporting complex business intelligence and data solutions utilized to support the business analytical and reporting needs. Collaborate with data scientists, business analysts, and operational stakeholders to define data requirements, build reports and dashboards, and provide technical expertise in managing and querying our datasets.
The Data Analyst / Business Intelligence Manger responsible for, but not limited to, the following:
Developing and creating clear, compelling, story-driven data visualizations by leveraging proprietary datasets.
Managing, updating, and re-imagining existing BI dashboards, data pipelines, and data structures in our data warehouse.
Supporting operational data metrics, models, and calculations used to provide insight to various department heads and senior leaders.
Preparing and delivering a formal Monthly and Quarterly Operations Earnings package to the Senior Leadership to prepare for public earnings calls.
Partnering with stakeholders to gather requirements and understand business needs to support the creation of data pipelines, exports, connections, and visualizations.
Advising and providing SME support on data collection, data modeling, schema design, and data analysis.
Automating the generation, management, and archiving of KPIs, macro-economic and industry specific business metrics, and ancillary metadata
Aiding in the transformation and adoption of modern, cloud-based data practices, pipelines, and techniques.
Becoming an expert in understanding the nuances of different, operational data points.
Communicating to the broader, corporate community recent developments, deliveries, and insights.
QUALIFICATION REQUIREMENTS:
Bachelor’s degree in computer science, engineering, hard science, economics or relevant field or equivalent demonstrable experience
6+ years of experience developing across the full Microsoft Power Suite: Excel, Power BI, Power Query, Power Pivot, including PowerBI Premium Capabilities – Data Marts, Dataflows, Azure ML
Must have technical leadership experience in an existing BI role
Must be detail oriented, enjoy problem solving, and analytics.
Experience developing and delivering complex reports, dashboards, and visualizations in Power BI or Tableau
Experience with PowerBI Administration - Access Control, Data Gateway setup, Tenant Configuration etc.
Knowledge of SQL queries, SQL Server Analysis Services (SSAS) and SQL Server Integration Services (SSIS), or open-source equivalent (e.g. Snowflake, Airflow)
Knowledge of industry best practices in information/data management techniques and platforms (Data Warehousing Design, ETL Process, Data Modeling)
Expert coding skills in DAX and M. Strong experience with a scripting language like Python or R.
Excellent interpersonal, communication, listening, writing, and presentation skills
Obsession with data visualizations, data pipelines, data integrity, and general cloud-based technology.
Ability to work on a team to solve complex problems.
Familiarity with Data Security practices and controls.
Prior multi-family real estate experience a plus.
Good meme selection and desire to have fun.
Please note that this job position entails in-person office requirements for a minimum of 3 days per week: Mondays, Tuesdays, and Wednesdays, located at Essex's corporate offices in Irvine, Woodland Hills, San Mateo, and Bellevue.
All full-time regular associates are offered competitive salaries, experience career growth, and are eligible for benefit packages that include medical, dental, vision, paid parental leave, 401k employer match, excellence rewards, wellness programs, and much more. With our Sunday property operations office closures, 10 paid holidays, and 15 PTO days, work/life balance is a priority! Additionally, most positions are eligible for a housing discount of 20%.
Essex provides great communities in which to live, work and invest. We are a purpose-driven company, and we pride ourselves on promoting an internal culture of growth and opportunity by engaging, enabling, and empowering our teams. Working at Essex is not a destination. It is a journey where you can confidently build your career.
The salary range for this position is $113,000.00 - $170,000.00 per year. New hires generally start between $113,000.00 - $141,000.00 per year. The final salary offer will be determined after reviewing relevant factors, including but not limited to skill sets; relevant experience; internal equity; and other business and organizational needs.
This role is also eligible to participate in Essex’s discretionary Annual Bonus program that is commensurate with the level of the position.
Show more
Show less","Data pipelines, Data visualization, Data warehousing, SQL, DAX, M, Python, R, Tableau, Power BI, SQL Server Analysis Services (SSAS), SQL Server Integration Services (SSIS), Snowflake, Airflow, Data modeling, Data security, Data integrity","data pipelines, data visualization, data warehousing, sql, dax, m, python, r, tableau, power bi, sql server analysis services ssas, sql server integration services ssis, snowflake, airflow, data modeling, data security, data integrity","airflow, data integrity, data security, datamodeling, datapipeline, datawarehouse, dax, m, powerbi, python, r, snowflake, sql, sql server analysis services ssas, sql server integration services ssis, tableau, visualization"
Data Scientist / Santa Monica Hybrid,Motion Recruitment,"Orange, CA",https://www.linkedin.com/jobs/view/data-scientist-santa-monica-hybrid-at-motion-recruitment-3785897058,2023-12-17,Corona,United States,Mid senior,Hybrid,"We have an exciting opportunity available for a Data Scientist in the field of influencer marketing and analytics. This is a full-time position with a fast-growing and profitable startup company. This company provides an influencer relationship management platform and full-service agency for clients ranging from world-class brands to fast-growing startups.
As a Data Scientist at this company, you'll have the chance to work with a talented team and enhance the vast dataset of social network profiles and content. We prioritize collaboration and rapid iterations, utilizing agile methodologies and modern tools like Github, Slack, Jira, and Invision. You will contribute to the development of data analytics models for various social media platforms and leverage machine learning algorithms to enhance data around influencers and content. We're looking for a candidate who has a strong background in data analytics, advanced SQL knowledge, and experience with scripting languages like Python. This is an opportunity to work on cutting-edge technologies and make a significant impact in the influencer marketing industry.
Required Skills & Experience
MS or Ph.D. in Computer Science, Statistics, Data Analytics, Information Systems, or related quantitative field.
3-5+ years of experience in a Data Analyst/Data Scientist role.
Advanced SQL knowledge and experience with relational databases.
Strong analytic skills for manipulating and extracting insights from large datasets.
2+ years of experience with scripting languages like Python (or Ruby).
Desired Skills & Experience
Knowledge of other languages such as Java, R, Perl, PHP, and shell scripts.
Proven ability to translate insights into business recommendations.
Bonus: Experience with machine learning.
Bonus: Knowledge of Ruby on Rails.
Bonus: Experience with filesystems, server architectures, and distributed systems.
Bonus: Experience with AWS.
The Offer
Medical, Dental, and Vision Insurance
401k matching
Annual Bonus
Paid Time Off
Paid Sick Time
Applicants must be currently authorized to work in the US on a full-time basis now and in the future.
Posted By:
Cassi Benson
Show more
Show less","Data Analytics, SQL, Python, R, Java, Ruby, Perl, PHP, Shell Scripts, Machine Learning, Ruby on Rails, Filesystems, Server Architectures, Distributed Systems, AWS, Agile Methodologies, Github, Slack, Jira, Invision","data analytics, sql, python, r, java, ruby, perl, php, shell scripts, machine learning, ruby on rails, filesystems, server architectures, distributed systems, aws, agile methodologies, github, slack, jira, invision","agile methodologies, aws, dataanalytics, distributed systems, filesystems, github, invision, java, jira, machine learning, perl, php, python, r, ruby, ruby on rails, server architectures, shell scripts, slack, sql"
CLINICAL DATA ANALYST-HSOM,University of Alabama at Birmingham,"Birmingham, AL",https://www.linkedin.com/jobs/view/clinical-data-analyst-hsom-at-university-of-alabama-at-birmingham-3777401863,2023-12-17,Birmingham,United States,Mid senior,Onsite,"Clinical Data Analyst-HSOM
The University of Alabama at Birmingham
This position will work with study investigators to finalize analytic plans and prepare analytic datasets. Assist in data cleaning, management, and quality checks. Contribute to the selection of appropriate statistical methods and study design. Conduct statistical analyses and preparation of data for presentation and dissemination. Participate in data interpretation, manuscript preparation, and grant writing. May require the use of various software packages.
Key Duties & Responsibilities
Obtains information for form completion by review of patient charts and clinical information available through the electronic medical record, Social Security Death Index, and follow up with referring physicians.
Interprets lab, radiologic data to determine disease status as time of transplant and response to transplant at specified time points following therapy in accordance with Center for International.
Blood and Marrow Transplant Research (CIBMTR) criteria.
Collects, analyzes, and records relevant patient health information from source documentation onto the Transplant Essential Data (TED) forms required for submission to CIBMTR for every patient receiving a transplant.
Collaborates with peers to conduct internal audits of the accuracy of data being submitted to CIBMTR against source documentation and performs form corrections as errors are identified.
Participates in audits and/or site visits by external organizations.
Communicates errors in documentation to appropriate physician or advanced practice provider for corrections.
Attends weekly clinical case conferences with physicians, advanced practice providers, and pretransplant coordinators to discuss each patient preparing for transplant, and address any discrepancies in physician documentation, missing laboratory or test results, and final determination of disease status at time of transplant.
Attends other meetings as deemed necessary.
Obtains relevant source documentation such as external laboratory reports, progress notes, radiology studies, or other relevant data from outside facilities for referrals pre-transplant and for follow up data post-transplant.
Communicates with patients and/or family members to obtain relevant health information when that information is not available through internal sources or from the referring physician.
Audits data entered into the internal RedCAP database for accuracy.
Perform other duties as assigned
Salary Range
$ 39,215 - $63,725
Education
Bachelor's degree in statistical analysis, epidemiology, health information, biological/health sciences or a related field
And one (1) year of experience in data statistical analysis or research required.
Work experience may NOT substitute for education requirement.
Primary Location
University
Job Category
Clerical & Administrative
Organization
311852800 Surgery - Transplantation
Employee Status
Regular
Shift
Day/1st Shift
Work Arrangement (final schedule to be determined by the department/hiring manager)
Onsite
Show more
Show less","Statistical analysis, Epidemiology, Data management, Statistical methods, Data cleaning, Data interpretation, Manuscript preparation, Grant writing, Data presentation, Data dissemination, Clinical information systems, Electronic medical records, Social Security Death Index, CIBMTR criteria, Transplant Essential Data (TED) forms, RedCAP database","statistical analysis, epidemiology, data management, statistical methods, data cleaning, data interpretation, manuscript preparation, grant writing, data presentation, data dissemination, clinical information systems, electronic medical records, social security death index, cibmtr criteria, transplant essential data ted forms, redcap database","cibmtr criteria, clinical information systems, data cleaning, data dissemination, data interpretation, data management, data presentation, electronic medical records, epidemiology, grant writing, manuscript preparation, redcap database, social security death index, statistical analysis, statistical methods, transplant essential data ted forms"
DATA BASE ANALYST III-RESEARCH STUDY,University of Alabama at Birmingham,"Birmingham, AL",https://www.linkedin.com/jobs/view/data-base-analyst-iii-research-study-at-university-of-alabama-at-birmingham-3771598962,2023-12-17,Birmingham,United States,Mid senior,Onsite,"Data Base Analyst III
University of Alabama at Birmingham
The University of Alabama at Birmingham is seeking a Data Base Analyst III. This position will work with the CARDIA (Coronary Artery Risk Development in Young Adults) study. Priorities are understanding data management best practices including a strong understanding of relational databases, strong experience in data analytics, good project management skills, experience with quality control, account and file management, and growth mindset (willingness to keep learning).
Duties and Responsibilities
Receive, process, and manage data electronically from the core CARDIA study, CARDIA ancillary studies, and various laboratories.
Produce various reports, including recruitment reports, for the core CARDIA study and ancillary studies.
Produce lists of samples to be pulled and shipped from the CARDIA repository for approved ancillary studies.
Maintain the inventory database of CARDIA biospecimens.
Program various study questionnaires in REDCap.
Work with CARDIA IT staff, Quality Control Coordinator, and biostatistics staff, as needed.
Provides technical support to programming staff in the area of database design, programming, and file-related problem determination.
Maintains integrity of database through backup and restores procedures as well as assists with system efficiency by monitoring file placement, reorganization
Knowledge, Skills, Abilities & Work Characteristics
Highly organized with strong attention to detail.
Effective communicator with research staff, participants, and non-academic research partners. Accuracy and attention to detail.
Adaptability to a fast-paced, changing work environment.
Critical thinking and problem-solving abilities.
Dependability.
Multitasking and time management skills.
Organizational skills and the ability to prioritize assignments.
Salary Range: $67,070 - $108,990
Education
Bachelor's degree in Computer Science, Engineering, Math or a related field
And five (5) years of related experience required.
Work experience may substitute for education requirement.
A list of experience and education substitutions can be found on the UAB Compensation website, if job allows for such substitutions.
Primary Location
University
Job Category
Scientific Research
Organization
311401800 Med - Preventive Medicine
Employee Status
Regular
Shift
Day/1st Shift
Work Arrangement (final schedule to be determined by the department/hiring manager)
Remote/Hybrid Eligible
Show more
Show less","Data Analytics, Database Management, Relational Databases, REDCap, Quality Control, Project Management, Account Management, File Management, Programming, System Administration, Backup and Restore, File Placement, Reorganization, Communication, Multitasking, Time Management, Organizational Skills, Prioritization, Attention to Detail, Critical Thinking, Problem Solving, Adaptability, Dependability","data analytics, database management, relational databases, redcap, quality control, project management, account management, file management, programming, system administration, backup and restore, file placement, reorganization, communication, multitasking, time management, organizational skills, prioritization, attention to detail, critical thinking, problem solving, adaptability, dependability","account management, adaptability, attention to detail, backup and restore, communication, critical thinking, dataanalytics, database management, dependability, file management, file placement, multitasking, organizational skills, prioritization, problem solving, programming, project management, quality control, redcap, relational databases, reorganization, system administration, time management"
Azure Data Engineer Lead,Adastra,"Markham, Ontario, Canada",https://ca.linkedin.com/jobs/view/azure-data-engineer-lead-at-adastra-3786596218,2023-12-17,North York, Canada,Mid senior,Hybrid,"Overview
Adastra is currently seeking an Azure Data Engineer Lead to join our team. We are in search of a senior- or lead-level consultant skilled in both solutions design and hands-on development.
Location:
Toronto, Hybrid only (2 days/wk onsite)
Status:
Contract, with potential to convert to FTE later on
Role Overview
As Azure Data Engineer Lead, you will handle end-to-end solutions, architecture design, and documentation for large scale data implementations (related to big data and data warehousing). This is a technical leadership role that requires the ability to take initiative and the ability to drive the migration from SQL Server to Azure. Majority of the migration is SQL and SSIS code.
Qualifications, Skills, & Experience
Senior/Lead Azure Data Engineer (7+ years) possessing skills in solutions design and hands-on development
Solid expertise (5+ years) in Azure / Databricks
Previous experience working in a technical leadership role
Knowledge and work experience handling end-to-end solutions, architecture design, and documentation for large scale data implementations (big data, data warehouse)
Hands-on skills in:
Databricks
Azure Data Factory (ADF)
Hadoop
SQL Server (SSIS)
Azure Synapse
Python / Spark / PySpark / JSON
Working experience with data modeling, relational modeling, and dimensional modeling
Microsoft Azure Certified: Data Engineer
Understanding of migrations, especially as it relates to SQL/SSIS code
Excellent communication skills
Strong leadership skills
Familiarity with Rahona ETL framework is an asset
About Adastra
For more than 20 years, Adastra Corporation has been hyper-focused on data and the infrastructures to support it. We are an international consulting company working work with businesses and IT Leaders to extract value through data and analytic solutions in the domains of Cloud, Data Management, Analytics, AI and ML, Governance, Hyper Automation, and Enterprise Application Development. Our team of experts, combined with our worldclass data management platform, helps provide trusted insights, establish frameworks, enforce best practices, and streamline processes for around-the-clock global coverage. Adastra has a proven track record of helping organizations of all sizes – from SMEs to Fortune 1000s – achieve their data transformation goals, whether that be accelerating innovation, improving operational excellence, or creating unforgettable customer experiences.
What We Offer
Opportunity for advancement and career progression
Competitive compensation package
Comprehensive benefits plan
Successful referral program
The opportunity to work with one of Canada’s 50 Best Managed Companies
Satisfaction of working for a reputable company
A flexible, dynamic, and diverse workplace
Equal Opportunity Employer
In our commitment to promote fair and equitable treatment of all employees and applicants, Adastra Corporation provides equal employment opportunities for all individuals regardless of age, sex, disability, race, ethnic origin, citizenship, creed, sexual orientation, marital status, or any other ground as described in the Ontario Human Rights Code. In addition, accommodation will be provided during the hiring process. Adastra Corporation’s implementation and support of employment initiatives, encourage diversified labour force participation and equal access to opportunities based on merit and performance.
There has been an increased instance of fraudulent job offers coming from people posing as Adastra HR employees. Please note that Adastra will never request fees as part of our recruiting process and any emails sent to you that are not from ‘@adastragrp.com’ or ‘@talent.icims.com’ are fraudulent. All employment offers are sent via DocuSign and if you receive an employment offer from a suspicious email and not via DocuSign, it is fraudulent.
Contact careers@adastragrp.com to inquire about jobs at Adastra, to report a suspicious request for money or personal information from external websites or suspicious employment offers.
Show more
Show less","Azure, Databricks, Azure Data Factory (ADF), Hadoop, SQL Server (SSIS), Azure Synapse, Python, Spark, PySpark, JSON, Data modeling, Relational modeling, Dimensional modeling, Microsoft Azure Certified: Data Engineer, SQL, SSIS, Rahona ETL framework, Cloud, Data Management, Analytics, AI, ML, Governance, Hyper Automation, Enterprise Application Development","azure, databricks, azure data factory adf, hadoop, sql server ssis, azure synapse, python, spark, pyspark, json, data modeling, relational modeling, dimensional modeling, microsoft azure certified data engineer, sql, ssis, rahona etl framework, cloud, data management, analytics, ai, ml, governance, hyper automation, enterprise application development","ai, analytics, azure, azure data factory adf, azure synapse, cloud, data management, databricks, datamodeling, dimensional modeling, enterprise application development, governance, hadoop, hyper automation, json, microsoft azure certified data engineer, ml, python, rahona etl framework, relational modeling, spark, sql, sql server ssis, ssis"
Data Ingestion Developer,Zortech Solutions,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/data-ingestion-developer-at-zortech-solutions-3667474714,2023-12-17,North York, Canada,Mid senior,Hybrid,"Title: Data Ingestion Developer
Location: Toronto, ON
Data Eng Developers to help execute on data ingestion projects. Someone who has strong Ingestion project knowledge in Spark, kafka techs.
Need Software developers who have Hadoop, Spark and Data Ingestion experience.
Need someone to process data for existing requirements
Need Someone to work with existing team to pick up additional work
updates from NMS and update to Hadoop big data system - kafka spark access - NMS systems - Data engineers
Need more experienced people 5+ year's experience
Show more
Show less","Spark, Kafka, Hadoop, Data Ingestion, NMS, Data Engineering","spark, kafka, hadoop, data ingestion, nms, data engineering","data engineering, data ingestion, hadoop, kafka, nms, spark"
Data Ingestion Developer,Zortech Solutions,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/data-ingestion-developer-at-zortech-solutions-3734013714,2023-12-17,North York, Canada,Mid senior,Hybrid,"Title: Data Ingestion Developers
Location: Toronto, ON
Permanent/Contract
Data Eng Developers to help execute on data ingestion projects. Someone who has strong Ingestion project knowledge in Spark, kafka techs.
Need Software developers who have Hadoop, Spark and Data Ingestion experience.
Need someone to process data for existing requirements
Need Someone to work with existing team to pick up additional work
updates from NMS and update to Hadoop big data system - kafka spark access - NMS systems - Data engineers
Need more experienced people 5+ year's experience
Show more
Show less","Spark, Kafka, Hadoop, Data Ingestion, NMS, Big Data","spark, kafka, hadoop, data ingestion, nms, big data","big data, data ingestion, hadoop, kafka, nms, spark"
Data Integration (ETL) Developer,People Corporation,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/data-integration-etl-developer-at-people-corporation-3750635612,2023-12-17,North York, Canada,Mid senior,Hybrid,"Job Description
We are seeking a Data Integration (ETL) Developer to join our team in Winnipeg or Toronto. You will be responsible for designing data storage systems. You must be an expert at taking a big-picture view of a company’s data situation to come up with comprehensive data storage solutions. You must be client-delivery focused and committed to either independently or working along side various teams to deliver quality to our clients in a timely manner.
The Data Integration (ETL) Developer will:
Assist with the selection and installation of an enterprise ETL tool
Establish and support standards for system architecture, code quality and collaborative team development
Defining new data collection and analysis processes
Working alongside teams within the business or the management team to establish business needs
Perform impact analysis for interface/system changes affecting the applications and data environment
Interpret data, analyze results using statistical techniques and provide ongoing reports
Acquire data from primary or secondary data sources and maintain databases/data systems
Identify, analyze, and interpret trends or patterns in complex data sets
Filter and “clean” data by reviewing reports, printouts, and performance indicators to locate and correct code problems
Develop and perform detailed unit, quality assurance and regression tests to validate the readiness of internal developed code for production
Monitor/ensure acceptable levels of system performance, integrity and security
To be successful as a Data Integration (ETL) Developer with People Corporation, you will need:
Bachelor’s Degree in Computer Science or equivalent combination of education and experience.
Minimum 3 years’ experience with ETL tools, concepts and processes such as data migration, database conversion, data mining or data analysis.
Senior to expert level knowledge of ETL processes/design as well as data modeling, both relational and dimensional.
Senior to expert level knowledge in supporting ETL, production data operations (File processing, data distribution etc.,) including debugging, addressing production issues, performance tuning and performing root cause analysis.
Strong MS SQL Server development skills with experience writing SQL scripts, dynamic SQL, complex stored procedures and functions.
Ability to use SQL development tools such as SQL Navigator and Toad as well as maintain code in source code control systems.
Knowledge of proper database normalization, indexing, transaction protection and locking is essential.
Experience with database design/modeling tools such as Erwin.
All-star candidates will have:
Extensive experience in ETL design/development with ETL tools such as Informatica, SSIS, Talend, etc. Experience with selecting these types of tools is also an asset.
Experience in windows batch scripting and scheduling jobs using job scheduling tools.
Thorough understanding of the software development life cycle and experience in working with geographically distributed teams.
Must have a teamwork focused attitude and be skilled at building relationships within IT organizations and across business functions.
Strong technical documentation skills.
Strong time management and organizational skills to enable productivity in a fast-paced, dynamic development environment.
Strong written and verbal communication skills.
What's in it for you:
Learn by working alongside our experts
Extended health care and dental benefits
A retirement savings plan with company contributions
A suite of Health & Wellness offerings
Mental Health programs and support for you and your family
Assistance for the completion of industry designations
Competitive compensation
At People Corporation we are committed to helping businesses succeed. We are a national provider of benefits, retirement, wealth, wellness, and human resource solutions. Our experts and solutions serve over 20,000 clients representing nearly 3 million Canadians. We offer customized solutions designed to fit the unique needs of businesses and their employees, members and stakeholders.
Providing an inclusive, accessible environment, where all employees and clients feel valued, respected and supported is something we're committed to. We are dedicated to building a workforce that reflects the diversity of the communities in which we live, and to creating an environment where every employee has the opportunity to reach their potential. If you require an accommodation or an alternative format of any posting please reach out to careers@peoplecorporation.com
Show more
Show less","ETL, Data Integration, Data Storage, Data Migration, Data Mining, Data Analysis, Data Modeling, Relational Database, Dimensional Database, SQL, SQL Scripts, Dynamic SQL, Stored Procedures, Functions, SQL Development Tools, Toad, Source Code Control, Database Normalization, Indexing, Transaction Protection, Locking, Erwin, Informatica, SSIS, Talend, Windows Batch Scripting, Job Scheduling, Software Development Life Cycle, Teamwork, Relationship Building, Technical Documentation, Time Management, Organizational Skills, Communication Skills","etl, data integration, data storage, data migration, data mining, data analysis, data modeling, relational database, dimensional database, sql, sql scripts, dynamic sql, stored procedures, functions, sql development tools, toad, source code control, database normalization, indexing, transaction protection, locking, erwin, informatica, ssis, talend, windows batch scripting, job scheduling, software development life cycle, teamwork, relationship building, technical documentation, time management, organizational skills, communication skills","communication skills, data integration, data migration, data mining, data storage, dataanalytics, database normalization, datamodeling, dimensional database, dynamic sql, erwin, etl, functions, indexing, informatica, job scheduling, locking, organizational skills, relational database, relationship building, software development life cycle, source code control, sql, sql development tools, sql scripts, ssis, stored procedures, talend, teamwork, technical documentation, time management, toad, transaction protection, windows batch scripting"
Business Data Analyst (1020103),The Judge Group,"Los Angeles, CA",https://www.linkedin.com/jobs/view/business-data-analyst-1020103-at-the-judge-group-3774890355,2023-12-17,West Covina,United States,Mid senior,Onsite,"Location:
Los Angeles, CA
Salary:
$70,000.00 USD Annually - $103,000.00 USD Annually
Description:
Our client is currently seeking a Business Data Analyst
Hybrid in Los Angeles, CA (1x per week)
Direct Hire/Perm
This Job Will Have The Following Responsibilities
Using the information that the business data analyst uncovers to identify problems and propose solutions.
To identify new sources of data and develop methods to improve data mining, analysis, and reporting.
To present the findings in reports (in table, chart, or graph format) to help the management team in the decision-making process.
To perform routine analysis tasks to support day-to-day business functioning and decision making.
Ensuring that business data and reporting needs are met
Developing and monitoring data quality metrics
Identify, analyze, and interpret trends or patterns in complex data sets
Work with management to prioritize business and information needs
Locate and define new process improvement opportunities
Qualifications & Requirements:
Bachelor Science in Computer Science, Information Systems, Computer Engineering, or related field
Contact:
ckim01@judge.com
This job and many more are available through The Judge Group. Find us on the web at www.judge.com
Show more
Show less","Business Data Analyst, Data Mining, Data Analysis, Data Reporting, Data Quality Metrics, Data Visualization, Data Interpretation, Trend Analysis, Process Improvement, Computer Science, Information Systems, Computer Engineering","business data analyst, data mining, data analysis, data reporting, data quality metrics, data visualization, data interpretation, trend analysis, process improvement, computer science, information systems, computer engineering","business data analyst, computer engineering, computer science, data interpretation, data mining, data quality metrics, data reporting, dataanalytics, information systems, process improvement, trend analysis, visualization"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Los Angeles, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3744387773,2023-12-17,West Covina,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
They are a profitable dating application.
Location: This is a hybrid role based in their Chicago office and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location
: This role is twice a week in a hybrid role minimum.
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, SQL, TDD, Data Warehouses, Kafka, Storm, SparkStreaming, ETL, Data Compliance, Data Management, Data Classification, Data Retention","python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, tdd, data warehouses, kafka, storm, sparkstreaming, etl, data compliance, data management, data classification, data retention","airflow, data classification, data compliance, data management, data retention, data warehouses, docker, etl, helm, kafka, kubernetes, python, snowflake, spark, sparkstreaming, sql, storm, tdd"
Data Engineer,UNCOMN,"Scott AFB, IL",https://www.linkedin.com/jobs/view/data-engineer-at-uncomn-3776795942,2023-12-17,Illinois,United States,Associate,Onsite,"Here at
UNCOMN
, our mission is to empower systems thinkers to create elegant solutions to complex problems – to improve the systems that improve our communities. Our team members apply their natural curiosity and grit to discover elegant solutions for our clients’ most complex organizational, logistics, process, data, and technical challenges, with the overall goal of building great businesses that contribute to great communities.
We’re an award-winning firm, one of the country’s fastest-growing and—more importantly—a consistent ‘Top Workplace’ as evaluated by our own employees. We are a values-driven organization (see the Core Values section of our website) and we’re looking for new Uncommon Geniuses to join our growing team, so if you’re a person who likes to solve problems, fix things, build things, tweak things, or otherwise show creative flair, you might just be an ""
UNCOMN
Genius"" and we encourage you to check out the specifics of this position below!
UNCOMN
is seeking a
Data Engineer
to:
Analyze raw source data to determine contents and meaning and apply to client requirements
Design and build processes for loading and transforming data including pipelines, warehouses, and platforms
Create and present analytical dashboards to executive-level decision-makers
Turn complex data sets into useful information
Work closely with the customer to understand the business problem and need, then dig into their data to find the information to solve it
Provide the customer with a deep understanding of their data, what it means, and how it can be used
Work unsupervised and participate in team meetings, design reviews, and peer reviews
Apply exceptional communication skills and the ability to communicate appropriately at all levels of the organization
Apply expertise to multiple complex work assignments
Perform assignments that may be broad in nature, requiring originality and innovation in accomplishing tasks
Operate with appreciable latitude in developing methodology and presenting solutions to problems
Perform all functional duties independently
Requirements
3+ years of experience in a professional work environment
Experience with Python or R
Experience using Databricks, or another similar tool, to perform data transformations or modeling
Experience with SQL
Bachelor’s Degree from an accredited university in a related field
Ability to navigate a complex matrix environment and manage competing priorities
Active (top) secret clearance, granted by the US Government
Preferred
Experience with BI Dashboard tools, including Qlik Sense, Tableau, or Power BI
Experience with Qlik dashboarding, development functions, and mashups
Experience with using Cloud Native Services
Ability to comprehend stakeholder needs effectively, communicate development plans, and track progress milestones
Superb organizational and time management skills
Excellent critical thinking skills for assessing numbers, trends, and data to reach new conclusions based on findings
Possession of excellent quantitative skills, including statistical analysis, process design, and data management
Qlik Data Architect, Qlik Data Analyst, or QlikView Developer Certification
Why UNCOMN?
Flexible PTO effective on day 1*
7 Paid Holidays & up to 3 Floating Holidays*
Eligible for Health Benefits on day 1*
401K Safe Harbor Match Program*
Training and Education Assistance*
*Must be a full-time employee
Don’t meet every single requirement? We’re dedicated to building an uncommon, inclusive, and authentic workplace, so if you’re excited about this role but your experience doesn’t align perfectly with every qualification in the job description, we encourage you to apply anyways. You may be just the right candidate for this or other roles.
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, or national origin.
Show more
Show less","Python, R, Databricks, SQL, Qlik Sense, Tableau, Power BI, Qlik dashboarding, Cloud Native Services, Statistical analysis, Process design, Data management, Qlik Data Architect, Qlik Data Analyst, QlikView Developer","python, r, databricks, sql, qlik sense, tableau, power bi, qlik dashboarding, cloud native services, statistical analysis, process design, data management, qlik data architect, qlik data analyst, qlikview developer","cloud native services, data management, databricks, powerbi, process design, python, qlik dashboarding, qlik data analyst, qlik data architect, qlik sense, qlikview developer, r, sql, statistical analysis, tableau"
Data Analyst,Illinois State Board of Education,"Springfield, Illinois Metropolitan Area",https://www.linkedin.com/jobs/view/data-analyst-at-illinois-state-board-of-education-3730699543,2023-12-17,Illinois,United States,Associate,Onsite,"Reporting to the Supervisor, supports department work stream(s) including but not limited to data collections, data reporting, ad hoc data and analysis requests, and data visualizations. Supports various operational projects (budget, staff training, invoice management, vendor contract management, department travel, etc).
Duties and Responsibilities:
Supports the data collection and data and/or analysis request department leads. May include supporting process management and compiling data from databases.
Supports the data reporting department leads. May include supporting process management and writing reports.
Supports department management in various operational projects (budget, staff training, invoice management, vendor contract management, department travel, etc.).
Performs other duties as assigned.
Required Qualifications:
Bachelor’s degree, or, Associate's degree in quantitative field such as Computer Science, Mathematics, Data Science, Measurement, Statistics, Economics, Public Policy, Public Administration, Psychology, Sociology, Business, or Education (with substantial coursework exhibiting quantitative content). (COPY OF TRANSCRIPTS REQUIRED AT TIME OF APPLICATION)
Experience with descriptive statistics.
Proficiency in excel and powerpoint.
Superior writing skills and strong visual communication skills.
Excellent critical thinking, problem solving, and organizational skills.
Ability to deliver assignments and projects on-time and complete.
Ability to respond to communication in a timely and complete manner.
Creativity and ability to work collaboratively.
Commitment to public service and equity in public schools.
Preferred Qualifications:
﻿Experience writing quantitative reports and developing presentations describing the results of data analysis.
Experience is SAS/R/other statistical software.
Ability to identify issues with data completeness and quality.
Knowledge of any of the following areas: early childhood education, special education, English language learning, teacher and leader effectiveness, assessment, accountability, demographics, post-secondary readiness, gifted and talented or school performance.
ISBE provides equal employment opportunities to all applicants without regard to race, color, religion, sex, national origin, age, disability, or genetics. We actively foster a culture of inclusion and encourage individuals of all backgrounds to apply.
Illinois has an urgent and collective responsibility to achieve educational equity by ensuring that all policies, programs, and practices affirm the strengths that each and every child brings within their diverse backgrounds and life experiences, and by delivering the comprehensive supports, programs, and educational opportunities they need to succeed.
Show more
Show less","Data collection, Statistical reporting, Excel, Powerpoint, SAS, R, Data analysis, Data visualization, Quantitative skills, Descriptive statistics, Writing skills, Visual communication skills, Critical thinking, Problem solving, Organizational skills, Time management, Communication skills, Creativity, Collaboration, Public service, School performance, Early childhood education, Special education, Teacher effectiveness, Assessment, Accountability, Demographics, Postsecondary readiness","data collection, statistical reporting, excel, powerpoint, sas, r, data analysis, data visualization, quantitative skills, descriptive statistics, writing skills, visual communication skills, critical thinking, problem solving, organizational skills, time management, communication skills, creativity, collaboration, public service, school performance, early childhood education, special education, teacher effectiveness, assessment, accountability, demographics, postsecondary readiness","accountability, assessment, collaboration, communication skills, creativity, critical thinking, data collection, dataanalytics, demographics, descriptive statistics, early childhood education, excel, organizational skills, postsecondary readiness, powerpoint, problem solving, public service, quantitative skills, r, sas, school performance, special education, statistical reporting, teacher effectiveness, time management, visual communication skills, visualization, writing skills"
Master Data Analyst,Alpha Consulting Corp.,"North Chicago, IL",https://www.linkedin.com/jobs/view/master-data-analyst-at-alpha-consulting-corp-3780460258,2023-12-17,Illinois,United States,Associate,Hybrid,"MASTER DATA ANALYST
NORTH CHICAGO, IL
Hybrid – 2-3 days a week.
The pay ranges between $53.57 to $54.74 per hour.
Project Description:
The position is responsible for providing data analysis and stewardship for US commercial customer master data management.
The focus of the job is to maintain the affiliation master and the health care organization accounts, responding to user's inquiries or change requests.
Supports systems and process development activities.
Works with stakeholders to enhance affiliation management processes.
Utilizes analytical and critical thinking skills to effectively resolve issues. Shares best practices with peers and management.
Effectively prioritize and adjust projects and tasks to meet changing business needs, dynamic working environment and customer's expectations.
Seeks management's direction when priorities conflict with objectives.
Proactively identifies, leverages, and shares best practices and lessons learned with peers to enhance process/business success.
Clear and concise written and verbal communications to customers, peers, and management.
Identifies and recommends ways to improve use of tools.
Mostly Individual but will be pulled into group meetings as needed.
Will also need to setup time with others to discuss issues or data anomalies.
Required Skills:
BS Preferred.
Working knowledge of customer Master Data Management (MDM).
Familiar with Reltio MDM platform, Veeva or Saleforce.com.
Demonstrated expertise of using data analytics tools, such as Excel, SQL, Snowflake, Power BI tools to create queries and macros.
Candidates should have (MDM) Master Data Management, Data Steward or Data Research, Data Scientist on their resumes.
Key words of MDM - Master Data Management, Research, IQVIA, Reltio, IBM Informatica, SQL, Analysis, Data Sources, etc.
Demonstrate expertise in utilizing and maintaining data and technical tools within the Customer Master Data Management function.
Demonstrate understanding of life science syndicated data, such as IQVIA Xponent, HCOS, AMA etc.
Ability to analyze large amount of data, draw conclusions and provide actionable information in appropriate format.
Ability to analyze large amounts of data, draw conclusions and provide actionable information in appropriate format.
Clear and concise written and verbal communications to customers, peers, and management.
Investigative thinking to determine a sound decision.
Knowledge with familiar systems or databases.
Ability to write code using other coding tools.
Information technology, experience with databases, understanding data sources and crosswalks.
Familiarity with life science industry data, such as IQVIA, DEA, AMA, etc. is a plus.
This 12+ month position starts ASAP.
ALPHA'S REQUIREMENT #23-01730
MUST BE ELIGIBLE TO WORK IN THE U.S. AS AN HOURLY W2 EMPLOYEE
Show more
Show less","MDM, Data Steward, Data Research, Data Scientist, Data Analytics, Reltio, Veeva, Salesforce, SQL, Snowflake, Power BI, Xponent, HCOS, AMA, DEA","mdm, data steward, data research, data scientist, data analytics, reltio, veeva, salesforce, sql, snowflake, power bi, xponent, hcos, ama, dea","ama, data research, data scientist, data steward, dataanalytics, dea, hcos, mdm, powerbi, reltio, salesforce, snowflake, sql, veeva, xponent"
Master Data Analyst,Alpha Consulting Corp.,"Lake Forest, IL",https://www.linkedin.com/jobs/view/master-data-analyst-at-alpha-consulting-corp-3779830400,2023-12-17,Illinois,United States,Associate,Hybrid,"MASTER DATA ANALYST
METTAWA, IL
Hybrid work schedule. 2-3 days in office.
The pay ranges between $51 - $53.57 per hour.
Project Description:
The position is responsible for providing data analysis and stewardship for US commercial customer master data management.
The focus of the job is to maintain the affiliation master and the health care organization accounts, responding to user's inquiries or change requests.
Supports systems and process development activities. Works with stakeholders to enhance affiliation management processes.
Required Skills:
5+ Years experience - BS Preferred.
Demonstrate expertise in utilizing and maintaining data and technical tools within the Customer Master Data Management function.
Familiar with Reltio MDM platform, Veeva or Saleforce.com.
Identifies and recommends ways to improve use of tools.
Demonstrate expertise in using MicroSoft Office applications, SQL, Snowflake, Power BI tools to create queries and macros.
Demonstrate understanding of life science syndicated data, such as IQVIA Xponent, HCOS, AMA etc.
Ability to analyze large amounts of data, draw conclusions and provide actionable information in appropriate format.
Utilizes analytical and critical thinking skills to effectively resolve issues. Shares best practices with peers and management.
Effectively prioritize and adjust projects and tasks to meet changing business needs, dynamic working environment and customer's expectations. Seeks management's direction when priorities conflict with objectives.
Proactively identifies, leverages, and shares best practices and lessons learned with peers to enhance process/business success.
Clear and concise written and verbal communications to customers, peers, and management.
Investigative thinking to determine a sound decision.
Familiarity with life science industry data, such as IQVIA, DEA, AMA, etc. is a plus.
This 12+ month position starts ASAP.
ALPHA'S REQUIREMENT #23-01922
MUST BE ELIGIBLE TO WORK IN THE U.S. AS AN HOURLY W2 EMPLOYEE
Show more
Show less","Data analysis, Data stewardship, Customer master data management, Reltio MDM platform, Veeva, Saleforce.com, MicroSoft Office, SQL, Snowflake, Power BI, IQVIA Xponent, HCOS, AMA, Analytical thinking, Critical thinking, Best practices sharing, Prioritization, Project management, Task management, Time management, Conflict resolution, Verbal communication, Written communication, Investigative thinking, Life science industry data","data analysis, data stewardship, customer master data management, reltio mdm platform, veeva, saleforcecom, microsoft office, sql, snowflake, power bi, iqvia xponent, hcos, ama, analytical thinking, critical thinking, best practices sharing, prioritization, project management, task management, time management, conflict resolution, verbal communication, written communication, investigative thinking, life science industry data","ama, analytical thinking, best practices sharing, conflict resolution, critical thinking, customer master data management, data stewardship, dataanalytics, hcos, investigative thinking, iqvia xponent, life science industry data, microsoft office, powerbi, prioritization, project management, reltio mdm platform, saleforcecom, snowflake, sql, task management, time management, veeva, verbal communication, written communication"
Senior Data Engineer,LTIMindtree,"Chicago, IL",https://www.linkedin.com/jobs/view/senior-data-engineer-at-ltimindtree-3779265414,2023-12-17,Illinois,United States,Mid senior,Onsite,"About Us:
LTIMindtree
is a global technology consulting and digital solutions company that enables enterprises across industries to reimagine business models, accelerate innovation, and maximize growth by harnessing digital technologies. As a digital transformation partner to more than 700+ clients, LTIMindtree brings extensive domain and technology expertise to help drive superior competitive differentiation, customer experiences, and business outcomes in a converging world. Powered by nearly 90,000 talented and entrepreneurial professionals across more than 30 countries, LTIMindtree — a Larsen & Toubro Group company — combines the industry-acclaimed strengths of erstwhile Larsen and Toubro Infotech and Mindtree in solving the most complex business challenges and delivering transformation at scale. For more information, please visit www.ltimindtree.com.
Job Title: Data Engineer
Work Location
Chicago, IL
Job Description:
Experience 13 + years
Extensive experience in design and development of Databases, SQL Server
,
stored procedures, Indexes, Views, and Triggers.
Database concepts, ability to write complex stored procedures, functions, triggers etc. with
close view on database performance.
Strong SQL
and performance improvement background.
Good with designing table structure considering performance in mind.
Good understanding of building data pipelines.
Good understanding of Snowflake and associated workflows.
Azure
data basics
Benefits and Perks:
Comprehensive Medical Plan Covering Medical, Dental, Vision
Short Term and Long-Term Disability Coverage
401(k) Plan with Company match
Life Insurance
Vacation Time, Sick Leave, Paid Holidays
Paid Paternity and Maternity Leave
The range displayed on each job posting reflects the minimum and maximum salary target for the position across all US locations. Within the range, individual pay is determined by work location and job level and additional factors including job-related skills, experience, and relevant education or training. Depending on the position offered, other forms of compensation may be provided as part of overall compensation like an annual performance-based bonus, sales incentive pay and other forms of bonus or variable compensation.
Disclaimer:
The compensation and benefits information provided herein is accurate as of the date of this posting.
LTIMindtree
is an equal opportunity employer that is committed to diversity in the workplace. Our employment decisions are made without regard to race, color, creed, religion, sex (including pregnancy, childbirth or related medical conditions), gender identity or expression, national origin, ancestry, age, family-care status, veteran status, marital status, civil union status, domestic partnership status, military service, handicap or disability or history of handicap or disability, genetic information, atypical hereditary cellular or blood trait, union affiliation, affectional or sexual orientation or preference, or any other characteristic protected by applicable federal, state, or local law, except where such considerations are bona fide occupational qualifications permitted by law.
Safe return to office
:
In order to comply with LTIMindtree’ s company COVID-19 vaccine mandate, candidates must be able to provide proof of full vaccination against COVID-19 before or by the date of hire. Alternatively, one may submit a request for reasonable accommodation from LTIMindtree’s COVID-19 vaccination mandate for approval, in accordance with applicable state and federal law, by the date of hire. Any request is subject to review through LTIMindtree’s applicable processes.
Show more
Show less","Data Engineering, Databases, SQL Server, Stored Procedures, Indexes, Views, Triggers, Data Pipelines, Snowflake, Azure","data engineering, databases, sql server, stored procedures, indexes, views, triggers, data pipelines, snowflake, azure","azure, data engineering, databases, datapipeline, indexes, snowflake, sql server, stored procedures, triggers, views"
Azure Data Engineer,Tredence Inc.,"Chicago, IL",https://www.linkedin.com/jobs/view/azure-data-engineer-at-tredence-inc-3782060634,2023-12-17,Illinois,United States,Mid senior,Onsite,"About Tredence­: -
Tredence focuses on last mile delivery of insights into actions by uniting its strengths in business analytics, data science, and software engineering. The largest companies across industries are engaging with Tredence and deploying its prediction and optimization solutions at scale –empowering end users to improve decision making. Headquartered in the San Francisco Bay Area, the company serves clients in the US, Canada, Europe, and SE Asia. Learn more at www.tredence.com
Location - Cincinnati OR Chicago OR Atlanta
Required Skills/Qualifications:
6+ years of relevant experience
Bachelor's and/or master’s degree in computer science or equivalent experience.
Strong communication, analytical and problem-solving skills with a high attention to detail.
Desired Experience:
At least two years of experience building and leading highly complex, technical engineering teams.
Strong hands-on experience in Data bricks and Snowflake
Implement scalable and sustainable data engineering solutions using tools such as Data bricks, Azure, Apache Spark, and Python. The data pipelines must be created, maintained, and optimized as workloads move from development to production for specific use cases.
Experience managing distributed teams preferred.
Comfortable working with ambiguity and multiple stakeholders.
Comfortable working cross functionality with product management and directly with customers; ability to deeply understand product and customer personas.
Expertise on Azure Cloud platform
Good SQL knowledge
Knowledge on orchestrating workloads on cloud
Ability to set and lead the technical vision while balancing business drivers
Strong experience with PySpark, Python programming
Proficiency with APIs, containerization and orchestration is a plus
Experience handling large and complex sets of data from various sources and databases
Solid grasp of database engineering and design principles
Familiarity with CI/CD methods desired
Good to have Teradata Experience (not Mandatory)
Qualifications:
Bachelor's and/or master’s degree
About you:
You are self-motivated, collaborative, eager to learn, and hands on
You love trying out new apps, and find yourself coming up with ideas to improve them
You stay ahead with all the latest trends and technologies
You are particular about following industry best practices and have high standards regarding quality
Why join Tredence?
There is a reason we are one of the fastest growing private companies in the country! You will have the opportunity to work with some of the smartest, friendliest, hardest working people in the data analytics space. You will work with the latest technologies and interface directly with the key decision stakeholders at our clients, some of the largest and most innovative businesses in the world. We offer a 401k match; full medical, dental and vision benefits, a fun team atmosphere and a work life balance. Our people are our greatest asset and we value every one of them. Come see why we’re so successful in one of the most competitive and fastest growing industries in the world.
Show more
Show less","Data engineering, Data analytics, Data science, Software engineering, Business analytics, Azure Cloud platform, SQL, Data pipelines, Machine learning, PySpark, Python, Apache Spark, Teradata, APIs, Containerization, Orchestration, CI/CD, DataBricks, Snowflake","data engineering, data analytics, data science, software engineering, business analytics, azure cloud platform, sql, data pipelines, machine learning, pyspark, python, apache spark, teradata, apis, containerization, orchestration, cicd, databricks, snowflake","apache spark, apis, azure cloud platform, business analytics, cicd, containerization, data engineering, data science, dataanalytics, databricks, datapipeline, machine learning, orchestration, python, snowflake, software engineering, spark, sql, teradata"
Sr Data Engineer,Mars,"Chicago, IL",https://www.linkedin.com/jobs/view/sr-data-engineer-at-mars-3781902600,2023-12-17,Illinois,United States,Mid senior,Onsite,"Job Description:
What are we looking for?
Master's or Bachelor's in an analytical field, Computer Science, IT, or equivalent experience
8+ years of work experience in big data or large scale data projects
Expertise in best practices data design, data governance, metadata management, etc.
Hands-on experience wth cloud-based data platforms (e.g, Azure, AWS) and their underlying solutions, as well as traditional enterprise RDBMS, SQL, and data orchestration.
Expertise in Snowflake is required.
Excellent communication and organization skills, with comfortability working in a cross-matrixed organization
What will be your key responsibilities?
Responsible for ingesting, processing, and distributing the data used by Mars’s data-driven business systems in Snowflake, Snowpark & Azure ML
Understanding of data warehousing, data cleaning, data pipelines and other analytical techniques required for data usage.
Maintain and optimize the performance, reliability, and scalability of the platform.
Help gather requirements and ensure seamless integration of the Snowflake with various data sources, marketing tools, and third-party platforms.
Monitor and troubleshoot data quality issues, ensuring data accuracy and completeness.
Work closely with the core team to create, implement, and refine customer segmentation and work with data scientists on propensity models (where applicable), and other data-driven insights.
Maintain data governance and privacy best practices, ensuring compliance with relevant regulations and industry standards.
Provide technical support to Mars Wrigley brands, addressing platform-related questions, concerns, and issues.
What can you expect from Mars?
Work with over 130,000 diverse and talented Associates, all guided by the Five Principles.
Join a purpose driven company, where we’re striving to build the world we want tomorrow, today.
Best-in-class learning and development support from day one, including access to our in-house Mars University.
An industry competitive salary and benefits package, including company bonus.
Mars is an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability status, protected veteran status, or any other characteristic protected by law. If you need assistance or an accommodation during the application process because of a disability, it is available upon request. The company is pleased to provide such assistance, and no applicant will be penalized as a result of such a request.
Show more
Show less","Data Design, Data Governance, Metadata Management, Cloudbased Data Platforms, Azure, AWS, SQL, Data Orchestration, Snowflake, Data Warehousing, Data Cleaning, Data Pipelines, Data Analytics, Data Usage, Data Quality, Data Accuracy, Data Completeness, Customer Segmentation, Propensity Modeling, Datadriven Insights, Data Governance, Data Privacy, Technical Support, Five Principles, Inhouse Mars University","data design, data governance, metadata management, cloudbased data platforms, azure, aws, sql, data orchestration, snowflake, data warehousing, data cleaning, data pipelines, data analytics, data usage, data quality, data accuracy, data completeness, customer segmentation, propensity modeling, datadriven insights, data governance, data privacy, technical support, five principles, inhouse mars university","aws, azure, cloudbased data platforms, customer segmentation, data accuracy, data cleaning, data completeness, data design, data governance, data orchestration, data privacy, data quality, data usage, dataanalytics, datadriven insights, datapipeline, datawarehouse, five principles, inhouse mars university, metadata management, propensity modeling, snowflake, sql, technical support"
Senior Data Engineer,Adobe,"Chicago, IL",https://www.linkedin.com/jobs/view/senior-data-engineer-at-adobe-3768487033,2023-12-17,Illinois,United States,Mid senior,Onsite,"Our Company
Changing the world through digital experiences is what Adobe’s all about. We give everyone—from emerging artists to global brands—everything they need to design and deliver exceptional digital experiences! We’re passionate about empowering people to create beautiful and powerful images, videos, and apps, and transform how companies interact with customers across every screen.
We’re on a mission to hire the very best and are committed to creating exceptional employee experiences where everyone is respected and has access to equal opportunity. We realize that new ideas can come from everywhere in the organization, and we know the next big idea could be yours!
Our Company
At Adobe, we’re changing the world. How? We give people the tools to bring their ideas to life and create content that makes life more fun and work more significant. We give businesses and organizations the power to truly engage their customers.
You will be immersed in an exceptional work environment that is recognized throughout the world by numerous “best places to work” awards. Every day, we get to work with some of the most creative, analytical, authentic and inspiring people in the world.
The challenge
The Data Engineer will be responsible for helping build the data infrastructure for the $2B+ Digital Media for Enterprise business, including Adobe’s flagship Creative Cloud and Document Cloud solutions. Working with the Senior Manager of Data Engineering, the Data Engineer will be responsible for ingesting data from a variety of internal and external sources into a centralized data lake and creating application and analytics use case specific data structures. Additionally, the Data Engineer will partner with other members of the Business Intelligence team to create dashboard and analytic solutions that provide critical insights to the Digital Media Enterprise Sales and Post-Sales teams.
What you'll do:
Architect, design and develop fault tolerant data infrastructure and data reporting solutions
Collaborate across engineering teams in product and IT
Work with variety of distributed platforms and systems, with deep understanding of databases, data in storage and data in motion
Create efficient data ingestion processes with low latency systems and data warehouses
Create and maintain solutions, frameworks and data pipelines which involve both batch and streams
Create or improve engineering efficiency, automate manual steps, improve monitoring and alerting solutions which are already in place
Be able to work creatively and analytically in a problem-solving, fast-paced agile development environment
Perform detailed analysis of business problems and technical environments and use this in designing the solution
What You Need To Succeed
10+ years of professional software and data engineering experience on customer-facing products
Strong experience with data transformation & ELT on large data sets
Strong enterprise development experience using Linux, Bash, SQL, Python
Strong experience in API, REST services & custom integrations
Ability to multi-task simultaneous different projects
BS or MS in Computer Science or comparable experience
Special consideration given for:
Experience using Hadoop or Map/Reduce solutions & other Big Data technologies to process large volumes of data
Knowledge in Data Science, ML/AI application life-cycle
Experience in full stack development & integrations on Linux/Unix operating systems
Experience using Amazon cloud technologies to build scalable and robust solutions
Experience with reporting frameworks, especially Power BI
Adobe is an equal opportunity employer. We welcome and encourage diversity in the workplace regardless of race, gender, sexual orientation, gender identity, disability, or veteran status.
Our compensation reflects the cost of labor across several  U.S. geographic markets, and we pay differently based on those defined markets. The U.S. pay range for this position is $154,000 -- $278,800 annually. Pay within this range varies by work location and may also depend on job-related knowledge, skills, and experience. Your recruiter can share more about the specific salary range for the job location during the hiring process.
At Adobe, for sales roles starting salaries are expressed as total target compensation (TTC = base + commission), and short-term incentives are in the form of sales commission plans. Non-sales roles starting salaries are expressed as base salary and short-term incentives are in the form of the Annual Incentive Plan (AIP).
In addition, certain roles may be eligible for long-term incentives in the form of a new hire equity award.
Adobe is proud to be an Equal Employment Opportunity and affirmative action employer. We do not discriminate based on gender, race or color, ethnicity or national origin, age, disability, religion, sexual orientation, gender identity or expression, veteran status, or any other applicable characteristics protected by law. Learn more.
Adobe aims to make Adobe.com accessible to any and all users. If you have a disability or special need that requires accommodation to navigate our website or complete the application process, email accommodations@adobe.com or call (408) 536-3015.
Adobe values a free and open marketplace for all employees and has policies in place to ensure that we do not enter into illegal agreements with other companies to not recruit or hire each other’s employees.
Show more
Show less","Data Engineering, Data Infrastructure, Cloud Computing, Data Lake, Data Analytics, Data Science, Machine Learning, Artificial Intelligence, Linux, Bash, SQL, Python, API, REST, Hadoop, MapReduce, Big Data, Full Stack Development, Unix, Amazon Web Services, Power BI","data engineering, data infrastructure, cloud computing, data lake, data analytics, data science, machine learning, artificial intelligence, linux, bash, sql, python, api, rest, hadoop, mapreduce, big data, full stack development, unix, amazon web services, power bi","amazon web services, api, artificial intelligence, bash, big data, cloud computing, data engineering, data infrastructure, data lake, data science, dataanalytics, full stack development, hadoop, linux, machine learning, mapreduce, powerbi, python, rest, sql, unix"
Sr. Data Engineer,Molex,"Lisle, IL",https://www.linkedin.com/jobs/view/sr-data-engineer-at-molex-3747349684,2023-12-17,Illinois,United States,Mid senior,Onsite,"Are you looking to make a connection to your career? Come to Molex where we create connections for life. We use innovation, engineering excellence, commitment to quality and reliability and superior customer experience to improve communication, elevate experiences, and engage and inspire people everywhere.
Joining our Enterprise Data Platform team means becoming a key player in a dynamic and innovative group of professionals dedicated to unlocking the power of data. Here, you'll find a diverse and collaborative environment where your ideas and expertise will shape the future of our data-driven organization. We pride ourselves on fostering a culture of continuous learning, creativity, and teamwork, and we're looking for individuals who are eager to contribute their skills and passion to our shared mission.
Our Senior Data Engineer will be a technical team member responsible for the overall contributions to developing and maintaining our data quality. This will require using our data pipeline tools and performing data modeling to optimize our data.
What You Will Do
Collaborate closely with business partners and data scientists to align data engineering efforts with their specific needs and objectives, ensuring that data solutions contribute effectively to the organization's overall goals.
Develop, maintain, and optimize data pipelines using tools such as Snowflake, Talend, AWS Glue, and Lambda.
Design and implement data modeling and ETL/ELT processes to ensure data quality, consistency, and availability.
Optimize and fine-tune data pipelines for performance and efficiency.
Manage and maintain data warehouses and ensure data security and compliance with company policies and relevant regulations.
Perform data transformations, aggregations, and data cleansing to support analytics and reporting needs.
Implement data integration strategies for streaming and batch data sources.
Automate and orchestrate data workflows to improve efficiency and reduce manual intervention.
Document data engineering processes, pipelines, and architecture for knowledge sharing and compliance.
Stay up-to-date with industry best practices, emerging technologies, and trends in data engineering.
Who You Are (Basic Qualifications)
Bachelor's degree in Computer Science, Information Technology, or a related field
Proven experience as a Data Engineer or similar role
Experience with data engineering principles and best practices
Proficiency in SQL and data modeling
Experience with data warehouse technologies such as Snowflake
Experience with programming skills in languages like Python, Java, or Scala
Experience with cloud computing platforms, particularly AWS
Experience with data integration concepts and tools like AWS Glue and Lambda
Experience with DevOps practices and tools, such as continuous integration (CI) and continuous deployment (CD) pipelines, version control systems (e.g., Git).
Experience in using Agile project management and collaboration tools (e.g., Jira, Confluence) to track progress and manage work in an Agile environment
This position is not eligible for visa sponsorship
What Will Put You Ahead
Certification in data engineering or related AWS certifications.
Knowledge of data orchestration and workflow management tools (e.g., Apache Airflow).
Experience with data streaming technologies (e.g., Kafka).
Knowledge of data governance and compliance best practices.
Ability to mentor junior team members and lead data engineering projects.
Strong project management and organizational skills.
Ability to collaborate closely with data scientists to deploy and operationalize machine learning models.
At Koch companies, we are entrepreneurs. This means we openly challenge the status quo, find new ways to create value and get rewarded for our individual contributions. Any compensation range provided for a role is an estimate determined by available market data. The actual amount may be higher or lower than the range provided considering each candidate's knowledge, skills, abilities, and geographic location. If you have questions, please speak to your recruiter about the flexibility and detail of our compensation philosophy.
Hiring Philosophy
All Koch companies value diversity of thought, perspectives, aptitudes, experiences, and backgrounds. We are Military Ready and Second Chance employers. Learn more about our hiring philosophy here.
Who We Are
As a Koch company, Molex is a leading supplier of connectors and interconnect components, driving innovation in electronics and supporting industries from automotive to health care and consumer to data communications. The thousands of innovators who work for Molex have made us a global electronics leader. Our experienced people, groundbreaking products and leading-edge technologies help us deliver a wider array of solutions to more markets than ever before.
At Koch, employees are empowered to do what they do best to make life better. Learn how our business philosophy helps employees unleash their potential while creating value for themselves and the company.
Our Benefits
Our goal is for each employee, and their families, to live fulfilling and healthy lives. We provide essential resources and support to build and maintain physical, financial, and emotional strength - focusing on overall wellbeing so you can focus on what matters most. Our benefits plan includes - medical, dental, vision, flexible spending and health savings accounts, life insurance, ADD, disability, retirement, paid vacation/time off, educational assistance, and may also include infertility assistance, paid parental leave and adoption assistance. Specific eligibility criteria is set by the applicable Summary Plan Description, policy or guideline and benefits may vary by geographic region. If you have questions on what benefits apply to you, please speak to your recruiter.
Equal Opportunities
Equal Opportunity Employer, including disability and protected veteran status. Except where prohibited by state law, all offers of employment are conditioned upon successfully passing a drug test. This employer uses E-Verify. Please visit the following website for additional information: http://www.kochcareers.com/doc/Everify.pdf
Show more
Show less","Data Engineering, Data Quality, Data Pipeline Tools, Data Modeling, Snowflake, Talend, AWS Glue, Lambda, ETL, ELT, AWS, AWS Glue, Lambda, DevOps, Continuous Integration, Continuous Deployment, Git, Jira, Confluence, SQL, Python, Java, Scala, Apache Airflow, Kafka, Machine Learning, Agile","data engineering, data quality, data pipeline tools, data modeling, snowflake, talend, aws glue, lambda, etl, elt, aws, aws glue, lambda, devops, continuous integration, continuous deployment, git, jira, confluence, sql, python, java, scala, apache airflow, kafka, machine learning, agile","agile, apache airflow, aws, aws glue, confluence, continuous deployment, continuous integration, data engineering, data pipeline tools, data quality, datamodeling, devops, elt, etl, git, java, jira, kafka, lambda, machine learning, python, scala, snowflake, sql, talend"
Senior Data Technical Analyst,McDonald's,"Chicago, IL",https://www.linkedin.com/jobs/view/senior-data-technical-analyst-at-mcdonald-s-3770180163,2023-12-17,Illinois,United States,Mid senior,Onsite,"McDonald’s evolving Accelerating the Arches growth strategy puts our customers and people first, and uses our competitive advantages to strengthen our brand. We are recognized on lists like
Fortune’s Most Admired Companies
and
Fast Company’s Most Innovative Companies.
Doubling Down on the 4Ds (Delivery, Digital, Drive Thru, and Development)
Our growth pillars emphasize the meaningful role technology plays as the leading, global omni-channel restaurant brand. Technology enables the organization through digital technology, and improving the customer, crew and employee experience each and every day.
Global Technology forging the way
Leading the digitization of our business is the Technology organization made up of intrapreneurs who build industry defining tech using the latest innovations and platforms, like AI and edge computing to deliver on the next set of cutting-edge opportunities for the business. At McDonald’s you get to solve technology innovation challenges at an incredible scale, and work across global teams who are always eager for a challenge. This provides access to exciting career paths for technologists. It’s bonus points when you get to see your family and friends use the tech you build at their favorite McD restaurant.
Job Description
We are seeking a highly motivated individual to be our Manager, Sr Data Technical Analyst for the data integrations component of the marketer customer data platform (mCDP). The rollout and maintenance of mCDP is a critical initiative for the marketing technology space at McDonald’s. It is the central location for all customer data used for activation and engagement, containing things like profile information, transactions, mobile app sessions, and more.
This individual will be responsible for collaborating with business and market partners on new enhancements for key market initiatives and help identify any impact to mCDP. They will work with the mCDP development squads, participate in agile ceremonies, and will work with various product and business teams to remove any blockers that are preventing development teams from making progress. The Manager, Sr Data Technical Analyst requires having in depth-knowledge of your area and will assist as a Subject Matter Expert.
Responsibilities
Collaborate with cross-functional teams to understand data needs for the markets and global initiatives and objectives
Assess current data infrastructure and identify opportunities for automation and integration improvements
Design, develop, and implement data solutions to automate marketing processes, ensuring accuracy and efficiency in data handling
Maintain key documentation like data contracts and data workflow diagrams with upstream source systems
Work with campaign team to understand any new data requirements or gaps
Stay updated on industry trends, emerging technologies, and best practices in marketing data automation
Coordinate across data product teams to oversee the delivery of new data development efforts into new markets
Participate in Solutioning sessions and workshops to ensure that the data teams are represented and considered in all architectural designs
Qualifications
4+ years working with data and analytics products in digital marketing or related industries
Strong leadership, communication, and writing skills
Experience working with customer data platforms and / or engagement tools (i.e. mParticle, Braze, SalesForce Marketing Cloud)
Ability to work collaboratively with others and navigate complex decision making
Working knowledge of cloud-based data systems
Skilled at defining and prioritizing product opportunities, building feedback loops, and creating product metrics
Experience working with the Agile methodology
Experience working with a large managed services partner
Ability to interact with customers at a variety of levels and across functions
Additional Information
All your information will be kept confidential according to EEO guidelines.
Show more
Show less","Data Analytics, Digital Marketing, Automation, Integration, Data Platforms, Data Contracts, Data Pipelines, Agile Methodology, Customer Data Platforms, Cloud Computing, mParticle, Braze, SalesForce Marketing Cloud, SQL, Python, Java, R","data analytics, digital marketing, automation, integration, data platforms, data contracts, data pipelines, agile methodology, customer data platforms, cloud computing, mparticle, braze, salesforce marketing cloud, sql, python, java, r","agile methodology, automation, braze, cloud computing, customer data platforms, data contracts, data platforms, dataanalytics, datapipeline, digital marketing, integration, java, mparticle, python, r, salesforce marketing cloud, sql"
Senior Data Engineer,Kin Insurance,"Chicago, IL",https://www.linkedin.com/jobs/view/senior-data-engineer-at-kin-insurance-3764351110,2023-12-17,Illinois,United States,Mid senior,Onsite,"The world has changed. Why hasn’t insurance?
Kin’s mission is to reimagine home insurance
For Every New Normal.
While other insurers struggle to handle a fast-changing world, Kin is built for the future and is prepared to meet its challenges head on while helping our customers do the same.
Kin is proud to be one of BuiltIn Chicago’s 2021 and 2022 Best Mid Sized Companies to work for, and Forbes 2021 Best Startup Employers in North America. Simply put, our people are what make us great, and we need forward-thinking, inspired game-changers like you to join us in our mission.
So, what’s the role?
Data is central to Kin’s operations and success. As a senior data engineer, you will be part of a data management team that supports and enables our product, operations, analytics, and data science teams, amongst others. As we scale, you will be integral in how we manage, structure, and store our data, as well as develop new solutions related to data architecture and ETL pipelines.
A day in the life could include:
Creating, designing, and maintaining ETL pipelines
Working with data science and BI teams to create data sets to be used in various projects
Taking on more advanced design and architectural decisions in the lifecycle of our data processing
Participating in recurring scrum events
Collaborating with cross-functional team members
Providing subject matter expertise and support
I’ve got the skills… but do I have the necessary ones?
5+ years of data engineering and/or dba experience
Experience with the entire ETL pipeline: Data Integration tools, Databases, Big Data Platforms, and cloud based data platforms.
Exposure to and support of data visualization tools, such as Looker, Tableau and Microstrategy.
Experience in building from the ground up a modern next generation data warehouse platform.
Advanced proficiency with SQL or PL/SQL optimization and development.
Experience working with AWS-based Data Platforms is preferred.
Deep understanding of data architecture as it relates to business goals and objectives
Strong scripting skills in one or more of the following: Bash, Python, and/or Ruby
Oh, and don’t worry, we’ve got you covered!
Medical, Dental, Vision, Disability and Life Insurance
Flexible PTO policy
Remote work
Generous equity package
401K with company match
Parental leave
Continuing education and professional development
The excitement of joining a high-growth Insurtech company and seeing your work make an impact
About Kin
In an industry that hasn't budged in more than 100 years, our technology transforms the user experience, cuts inefficiencies that waste billions of consumer dollars, and customizes coverage homeowners want. We believe insurance was always meant to be a digital product – we’re making that a reality.
Our approach to the industry makes us unique, and the people at Kin help us excel. We’re a team of problem solvers, collaborators, builders, and dreamers who are passionate about creating positive change in the lives of our customers and in our industry. Kin is more than just our name – it’s how we treat each other. That’s one of the many reasons we’ve been recognized as a great place to work by Built In, Forbes, and Fast Company.
EEOC Statement
Kin is proud to be an Equal Employment Opportunity and Affirmative Action Employer. We don't just accept difference – we honor it, nurture it, and celebrate it. We don’t discriminate based on race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.
Kin encourages applications from all backgrounds, communities and industries, and are committed to having a team that is made up of diverse skills, experiences and abilities.
Remote
Show more
Show less","Data engineering, Data management, ETL pipelines, Data science, BI, AWS, Data architecture, SQL, PL/SQL, Bash, Python, Ruby, Looker, Tableau, Microstrategy","data engineering, data management, etl pipelines, data science, bi, aws, data architecture, sql, plsql, bash, python, ruby, looker, tableau, microstrategy","aws, bash, bi, data architecture, data engineering, data management, data science, etl pipelines, looker, microstrategy, plsql, python, ruby, sql, tableau"
Quantitative Data Engineer [73096],Onward Search,"Illinois, United States",https://www.linkedin.com/jobs/view/quantitative-data-engineer-73096-at-onward-search-3762670926,2023-12-17,Illinois,United States,Mid senior,Remote,"THIS JOB IS NOT OPEN FOR C2C
Onward Search is seeking a Quantitative Data Engineer for one of our clients on a contract basis working remotely in Central Time.
Job Summary:
The Quantitative Data Engineer plays a pivotal role in developing and maintaining quantitative products and datasets with a significant impact on the investment management industry. Collaborating with stakeholders in technology, research, and operations, the role involves building a robust data platform, ensuring high data integrity for alpha factors, risk models, and quantitative investing software. Responsibilities include developing on-premises and cloud-based data processes, engineering data models, collaborating with the quantitative research team, and actively contributing to innovation initiatives within Market Intelligence. The role offers the opportunity to build technology workflows, work with proprietary datasets, and gain exposure to quantitative research and strategy development.
Qualifications:
Bachelor's/Master's degree in Computer Science, Information Systems, or a related field
Proficiency in analytical, data, and programming skills, including Python, SQL, NoSQL, and JavaScript
Over 3 years of experience working with large datasets, employing ETL techniques, and architecting them for optimal performance; familiarity with alternative unstructured data is advantageous
1+ year of experience with cloud computing services, with a preference for AWS
Ability to design infrastructure and data products, particularly for Quant/Data Scientists, is a valuable asset
Demonstrated effectiveness in agile environments, collaborating with diverse stakeholders on complex research and development projects
Genuine interest in investment strategies, equities, and fixed income, with a plus for experience in the asset management industry
Strong verbal and written communication skills, coupled with a collaborative team-oriented approach
Show more
Show less","Python, SQL, NoSQL, JavaScript, ETL, AWS, Cloud computing, Agile, Quantitative research, Data architecture, Data engineering, Data modeling, Data integrity, Risk models, Alpha factors, Investment management, Asset management, Investment strategies, Equities, Fixed income","python, sql, nosql, javascript, etl, aws, cloud computing, agile, quantitative research, data architecture, data engineering, data modeling, data integrity, risk models, alpha factors, investment management, asset management, investment strategies, equities, fixed income","agile, alpha factors, asset management, aws, cloud computing, data architecture, data engineering, data integrity, datamodeling, equities, etl, fixed income, investment management, investment strategies, javascript, nosql, python, quantitative research, risk models, sql"
Sr. Data Engineer - Remote,Chamberlain Group,"Illinois, United States",https://www.linkedin.com/jobs/view/sr-data-engineer-remote-at-chamberlain-group-3699222172,2023-12-17,Illinois,United States,Mid senior,Remote,"If you are a current Chamberlain Group employee, please click here to apply through your Workday account.
Chamberlain Group is a global leader in access solutions with top brands, such as LiftMaster and Chamberlain, found in millions of homes, businesses, and communities worldwide.
As a leader in the Smart Home industry, we boast one of the largest IoT install bases, with innovative products consisting of cameras, locks, card readers, garage door openers, gates and more, all powered by our myQ digital ecosystem.
This role is responsible for providing technical expertise and leadership to design and deliver end-to-end data engineering solutions to support advanced analytics capabilities and drive innovation and decision-making
across Chamberlain.
Essential Duties And Responsibilities
Build and maintain real-time and batch data pipelines across the advanced analytics platform.
Design, develop and orchestrate highly robust and scalable ETL pipelines.
Design and implement Dimensional and NoSQL data modelling as per the business requirements.
Develop highly optimal codebase and perform Spark optimizations for Big Data use cases.
Design, develop and deploy optimal monitoring and testing strategy for the data products.
Collaborate with stakeholders and advanced analytics business partners to understand business needs and translate requirements into scalable data engineering solutions.
Collaborate with data scientists to prepare data for model development and production.
Collaborate with data visualization and reporting application developers to ensure the sustainability of production applications and reports.
Collaborate with data architects on the enhancement of Chamberlain’s enterprise data architecture and platforms.
Provide leadership to third-party contractors.
Comply with health and safety guidelines and rules.
Protect CGI’s reputation by keeping information confidential.
Maintain professional and technical knowledge by attending educational workshops, professional publications, establishing personal networks, and participating in professional societies.
Minimum Qualifications
Education/Certifications:
Bachelor’s degree in computer science or related quantitative field of study
Experience:
4+ years of professional experience
Knowledge, Skills, and Abilities:
Natural sense of urgency, teamwork, and collaboration reflected in daily work ethic.
Proficient in Spark or Databricks, Cloud Data Engineering Services preferably Azure, Streaming frameworks like Event Hubs or Kafka.
Proficient in Microsoft Office.
Familiarity with modern Machine Learning Operationalization techniques.
Agile methodologies.
Familiarity with Data visualization tools, such as Qlik or Power BI.
Preferred Qualifications
Education/Certifications:
Master’s degree in computer science or related quantitative field of study
Experience:
4+ years of professional experience
2+ years of professional experience delivering engineering for advanced analytics or data science solutions
Knowledge, Skills, and Abilities:
Agile methodologies
Experience with IoT Data Architecture.
Machine Learning Operationalization (MLOps) proficiency.
REST API design and development.
Proficiency with streaming design patterns.
The pay range for this position is $103,300.00 to $177.475.00; base pay offered may vary depending on a number of factors including, but not limited to, the position offered, location, education, training, and/or experience. In addition to base pay, also offered is a comprehensive benefits package and 401k contribution (all benefits are subject to eligibility requirements).
This position is eligible for participation in a short-term incentive plan subject to the terms of the applicable plans and policies.
Chamberlain Group wants all of its employees to succeed and encourages people of all backgrounds to apply. We’re proud to be an Equal Opportunity Employer, and you’ll be considered for this role regardless of race, color, religion, sex, national origin, age, sexual orientation, ancestry; marital, disabled or veteran status. We’re committed to fostering an environment where people of all lived experiences feel welcome.
Persons with disabilities who anticipate needing accommodations for any part of the application process may contact, in confidence Recruiting@Chamberlain.com.
NOTE: Staffing agencies, headhunters, recruiters, and/or placement agencies, please do not contact our hiring managers directly.
Show more
Show less","Spark, Databricks, Cloud Data Engineering Services, Azure, Event Hubs, Kafka, Microsoft Office, Machine Learning Operationalization, Agile methodologies, Data visualization tools, Qlik, Power BI, REST API, streaming design patterns, IoT Data Architecture, MLOps","spark, databricks, cloud data engineering services, azure, event hubs, kafka, microsoft office, machine learning operationalization, agile methodologies, data visualization tools, qlik, power bi, rest api, streaming design patterns, iot data architecture, mlops","agile methodologies, azure, cloud data engineering services, data visualization tools, databricks, event hubs, iot data architecture, kafka, machine learning operationalization, microsoft office, mlops, powerbi, qlik, rest api, spark, streaming design patterns"
Senior Data Engineer,MGIC,"Illinois, United States",https://www.linkedin.com/jobs/view/senior-data-engineer-at-mgic-3735204554,2023-12-17,Illinois,United States,Mid senior,Remote,"Why work at MGIC?
Are you someone who wants to play a critical role in our company’s success? Do you enjoy solving puzzles and finding a better way to get things done? Are you someone who likes to Take The Lead and make an impact? If so, then imagine yourself at MGIC. At MGIC we are a team of dedicated professionals on a fearless mission. A team that fosters a culture of career development and continuous learning opportunities to help you rise to new heights. We are passionate about providing outstanding customer service and making a difference in our community. #WeAreMGIC
Preferred location: Milwaukee based – hybrid (3 days office, 2 days remote)
Other locations: Madison, Minnesota, Illinois, and Michigan (occasional travel into the office to support team engagements).
How will you make an impact?
We are currently looking for a Senior Data Engineer to deliver high quality, maintainable and scalable data solutions. The Senior Data Engineer will partner with solution architects and other data engineers to develop our enterprise analytics data platform through new and updated data pipelines leveraging shared components and aligning to standards and best practices. This role is focused on collecting data from internal and external sources and transforming it into usable information for the business including data scientists and data analysts.
Deliver high quality data assets to be used by the business to transform business processes and to enable leaders to make data-driven decisions
Continuously improve data solutions to increase quality, speed of delivery and trust of data engineering team’s deliverables to enable business outcomes
Reduce total cost of ownership of solutions by developing shared components and implementing best practices and coding standards
Do you have what it takes?
Ability to translate data engineering designs into working code
Data analysis and data engineering pipeline experience including design, development, and support
Experience with AWS services or cloud data offerings including S3, Lambda, EMR, Dynamo DB, Glue, Snowflake and/or other data technologies.
Experience with coding in Python, PySpark, and Terraform
Experience with DevOps practices including Continuous Integration, Continuous Delivery, and Infrastructure as Code to deliver end-to-end automation of data delivery.
Ability to train and mentor junior data engineers.
Experience with Agile engineering practices including the scrum framework.
Enjoy these benefits from day one:
Competitive Salary & pay-for-performance bonus
Financial Benefits (401k with company match, profit sharing, HSA, wellness rewards program)
On-site Fitness Center and classes (corporate office)
Paid-time off and paid company holidays
Business casual dress
For additional information about MGIC and to apply, please visit our website at www.mgic.com/careers.
Show more
Show less","Python, PySpark, Terraform, AWS, S3, Lambda, EMR, Dynamo DB, Glue, Snowflake, Agile, Scrum, DevOps, Continuous Integration, Continuous Delivery, Infrastructure as Code, Data Analysis, Data Engineering, Data Pipeline","python, pyspark, terraform, aws, s3, lambda, emr, dynamo db, glue, snowflake, agile, scrum, devops, continuous integration, continuous delivery, infrastructure as code, data analysis, data engineering, data pipeline","agile, aws, continuous delivery, continuous integration, data engineering, data pipeline, dataanalytics, devops, dynamo db, emr, glue, infrastructure as code, lambda, python, s3, scrum, snowflake, spark, terraform"
Senior Database Engineer- Cassandra,Grubhub,"Chicago, IL",https://www.linkedin.com/jobs/view/senior-database-engineer-cassandra-at-grubhub-3760315361,2023-12-17,Illinois,United States,Mid senior,Hybrid,"We are looking for a Senior Site Reliability Engineer to join our Database Engineering organization. At Grubhub, the Database Engineering organization owns the top-level reliability, observability, and availability of the Datastore platforms, including but not limited to Cassandra, ElasticSearch and Kafka. This team contributes to projects, services, designs, and processes with the aim to steward good architecture and provide tools and services to enable software engineering teams to measure and meet reliability agreements.
The Impact You Will Make
Manage large critical Cassandra and Elasticsearch clusters supporting millions of transactions per day
Build systems to automate all build and maintenance tasks using Ansible and python
Develop self-service tools to allow engineers to manage and provision resources with GrubHub best practices
Monitor cluster availability, read/ write latencies, and other important performance metrics to proactively identify SLO misses and help mitigate issues
Evaluate new technologies and software versions. Test and develop roadmaps
Tune Cassandra and ES databases for optimizing throughput and read /write latencies
24X7 on-call rotation support with rest of team for rapid incident response
Implement DR strategies, including backups and recovery techniques with minimal downtime.
Work with other engineers to manage our data persistence integration and performance with the GrubHub platform.
Monitor and scale Elasticsearch/Cassandra clusters to handle growth in traffic
What You Bring To The Table
Experience managing large Cassandra clusters in production
Experience managing, working or developing large Elasticsearch clusters in highly available 24x7 production environments
Experience automating the maintenance of infrastructure using Python and Ansible or similar tools.
Experience managing automated cloud infrastructures on AWS or other major cloud providers.
Experience developing backend applications in Python or Java
Experience working with Docker is a plus
Ability to quickly learn new concepts and technologies and adapt to changing needs
About Our Tech
Most of our internal tooling is written in Python.
Most of our microservices are written in Java
Observability tools we use: Datadog, Splunk, Lightstep.
Our primary persistence store is Cassandra
We operate in 3 Amazon regions (hot+hot+hot)
We primarily rely on AWS and its services: EC2, S3, SNS/SQS, ElastiCache, Lambda, etc.
​
And Of Course, Perks!
Flexible PTO.
Grubhub employees enjoy a generous amount of time to recharge.
Health and Wellness.
Excellent medical, dental and vision benefits, 401k matching, employee network groups and paid parental leave are just a few of our programs to support your overall well-being.
Compensation.
You'll receive a great compensation package with eligibility for generous incentives, bonuses, commission, or RSUs (role-specific).
Free Meals.
Our employees get a weekly Grubhub credit to enjoy and support local restaurants.
Social Impact.
We believe in giving back through programs like the Grubhub Community Relief Fund, and provide our employees opportunities to support causes that are important to them.
Grubhub is an equal opportunity employer. We welcome diversity and encourage a workplace that is just as diverse as the customers we serve. We evaluate qualified applicants without regard to race, color, religion, age, sex, sexual orientation, gender identity, national origin, disability, veteran status, and other legally protected characteristics. If you're applying for a job in the U.S. and need an accommodation for any part of the employment process, please send an email to TalentAcquisition@grubhub.com and let us know the nature of your request and contact information.
CA Privacy Notice: If you are a resident of the State of California and would like a copy of our CA privacy notice, please email privacy@grubhub.com.
Show more
Show less","Site Reliability Engineering, Cassandra, Elasticsearch, Kafka, Python, AWS, Java, Datadog, Splunk, Lightstep, EC2, S3, SNS/SQS, ElastiCache, Lambda","site reliability engineering, cassandra, elasticsearch, kafka, python, aws, java, datadog, splunk, lightstep, ec2, s3, snssqs, elasticache, lambda","aws, cassandra, datadog, ec2, elasticache, elasticsearch, java, kafka, lambda, lightstep, python, s3, site reliability engineering, snssqs, splunk"
Senior Data Engineer,Cash App,"Sydney, New South Wales, Australia",https://au.linkedin.com/jobs/view/senior-data-engineer-at-cash-app-3764033203,2023-12-17,Parramatta, Australia,Associate,Onsite,"Company Description
It all started with an idea at Block in 2013. Initially built to take the pain out of peer-to-peer payments, Cash App has gone from a simple product with a single purpose to a dynamic ecosystem, developing unique financial products, including Afterpay/Clearpay, to provide a better way to send, spend, invest, borrow and save to our millions of monthly active customers. We want to redefine the world’s relationship with money to make it more relatable, instantly available, and universally accessible.
Today, Cash App has thousands of employees working globally across office and remote locations, with a culture geared toward innovation, collaboration and impact. We’ve been a distributed team since day one, and many of our roles can be done remotely from the countries where Cash App operates. No matter the location, we tailor our experience to ensure our employees are creative, productive, and happy.
Job Description
The BI Team at Cash App enables our teams to make impactful business decisions. Our BI Engineers handle everything from data architecture and modeling to data pipeline tooling and dashboarding. As a Senior BI Engineer at Cash App, you will report to the BI Manager and work with Analysts, Data Scientists, Product Managers, and Software Engineers to lay the foundation for analyzing our large, unique dataset. We are an extremely data-driven team - from understanding our customers, managing and operating our business, to informing product development. You will build, curate, document, and manage key datasets and ETLs to increase the impact of the entire team. You will develop strong partnership with our stakeholders by understanding their priorities and pain points and deliver value via our data, reporting, and tools.
Key Responsibilities
Create brand new and optimize existing data models in the Commerce domain at Cash App.
Build strong partnerships with Product Data Scientist, Product Manager, and Software Engineer teams by understanding their priorities and pain points and deliver values from our data, reporting, and interactive data products.
Build reusable data engineering framework to improve the data quality, reliability, and scalability of our ETLs using a combination of tools like dbt, prefect, airflow, databricks, spark, etc
Mentor and coach team members to adopt data modeling best practice, improve data engineering solutions, and adopt developing, testing, and release best practice
Collaborate with external teams including data infrastructure, tooling, and raw data provision teams to keep updated on data engineering framework, tooling, and engineering excellence.
Enable stakeholders from less technical teams to self service data pipelines using UI-based task orchestration tools
Qualifications
You have:
Background/knowledge in Computer Science, Applied Math, Engineering, Stats, Physics, or something comparable
7+ years of industry experience building complex, scalable ETLs for a variety of different business and product use cases.
3+ years of experience in ETL orchestration and workflow management tools like Airflow, Prefect
Experience working with Snowflake, Databricks, Redshift, PostgreSQL and/or other DBMS platforms
Experience using CI/CD tools in data engineering pipelines. Experience using dbt in building and enriching data models. Experience in building interactive data products to automate internal business processes will be a plus
An interest in advancing Cash App's vision of building products for economic empowerment - this should be something that legitimately excites you
Comfortable working in fast paced environment, are a self starter and self organizing
Technologies we use and teach:
SQL, Spark
Airflow, Prefect, Looker and Tableau
Python / Java
Additional Information
We’re working to build a more inclusive economy where our customers have equal access to opportunity, and we strive to live by these same values in building our workplace. Block is a proud equal opportunity employer. We work hard to evaluate all employees and job applicants consistently, based solely on the core competencies required of the role at hand, and without regard to any legally protected class.
We believe in being fair, and are committed to an inclusive interview experience, including providing reasonable accommodations to disabled applicants throughout the recruitment process. We encourage applicants to share any needed accommodations with their recruiter, who will treat these requests as confidentially as possible.
Want to learn more about what we’re doing to build a workplace that is fair and square? Check out our
I+D page
.
Block, Inc. (NYSE: SQ) is a global technology company with a focus on financial services. Made up of Square, Cash App, Spiral, TIDAL, and TBD, we build tools to help more people access the economy. Square helps sellers run and grow their businesses with its integrated ecosystem of commerce solutions, business software, and banking services. With Cash App, anyone can easily send, spend, or invest their money in stocks or Bitcoin. Spiral (formerly Square Crypto) builds and funds free, open-source Bitcoin projects. Artists use TIDAL to help them succeed as entrepreneurs and connect more deeply with fans. TBD is building an open developer platform to make it easier to access Bitcoin and other blockchain technologies without having to go through an institution.
Show more
Show less","Data Architecture, Data Modeling, Data Pipeline Tooling, Dashboarding, ETL, Data Engineering, dbt, Prefect, Airflow, Databricks, Spark, Snowflake, Redshift, PostgreSQL, CI/CD, Python, Java, SQL","data architecture, data modeling, data pipeline tooling, dashboarding, etl, data engineering, dbt, prefect, airflow, databricks, spark, snowflake, redshift, postgresql, cicd, python, java, sql","airflow, cicd, dashboard, data architecture, data engineering, data pipeline tooling, databricks, datamodeling, dbt, etl, java, postgresql, prefect, python, redshift, snowflake, spark, sql"
Data Engineer,Cartology - Part of the Woolworths Group,"Sydney, New South Wales, Australia",https://au.linkedin.com/jobs/view/data-engineer-at-cartology-part-of-the-woolworths-group-3764750580,2023-12-17,Parramatta, Australia,Mid senior,Hybrid,"Join Australia’s pioneering retail media organisation where there is ample opportunity to develop and grow
Contribute to new initiatives, in an inclusive culture of growth & innovation
Work as part of a collaborative, fast-growing data engineering team
Cartology is the Woolworths Group’s new stand-alone retail media business. Customer behaviour is changing. They have more choices than ever before – both in the way they shop, and how they consume media. With all this choice, it’s making it harder for marketers to reach audiences at scale and grab their attention. Cartology delivers data-led marketing solutions, putting insights and customers at the heart of everything we do.
By combining our rich customer knowledge with our diverse media channels we can make sure products are being shown to the right customers, at the right place, at exactly the right time. Our point of difference is that we are part of Australia’s largest retailer allowing us to provide unparalleled career opportunities.
We are currently in a scale-up phase with significant further growth ahead, Cartology leverages the best of the Woolworths group expertise while creating its own distinct offering and culture.
What You’ll Do
Cartology is seeking a Data Engineer to join our growing team of data experts to build market-leading data products and services. Reporting to the Data Engineering Manager, you will be vital in designing, building, and maintaining our data products, pipelines, and infrastructure.
This role will work on a number of new initiatives, working on projects from end to end, and play a key role in setting up best practices within Data Engineering at Cartology.
Design and Deliver Data Infrastructure and solutions in GCP, including data models, ETL pipelines, and warehousing using various tools e.g. dbt, Bigquery, Looker, Dataflow, Fivetran, Cloud Composer etc.
Ensure innovation, continuous improvement, maintainability, supportability and performance of the shared data platform.
Implement and maintain best practices for data governance, security, quality, and integrity for all data assets.
Develop and maintain documentation for data engineering processes, workflows, and solutions.
Support data analysis and reporting activities through data tooling, data access patterns, data visualisation, query, and report optimisation
What You’ll Bring
As Cartology continues to grow and develop our data capabilities, we are looking for someone who is a keen learner and curious about new technological innovations and experimentation.
We are looking for an experienced Data Engineer, with proven ability to own and drive solution delivery and demonstrated ability to work both independently and collaboratively with teams to deliver projects and business goals.
Advanced SQL and data modelling skills with demonstrated success in designing and building complex data infrastructure and pipelines
Experience building meaningful dashboards in a modern BI tool (e.g. Looker, Tableau, PowerBI etc)
Proven experience building ETL pipelines and modelling data for both BI and Data Product consumption.
Technical understanding of modern cloud based data warehousing e.g BigQuery, Snowflake etc.
Experience working with large volumes of data and transforming it for optimal business use.
What You’ll Experience
Our Team Members are at the heart of everything we do and we’re always looking for ways to support your career journey and reward great work:
A truly unique opportunity to be involved in creating, shaping and leading the new retail media industry.
Team discounts across our range of Woolworths Group brands you know and love and a robust rewards program that celebrates and incentivises purpose-driven work.
Feel supported and empowered by your leaders
A range of programs to help you prioritise and manage your wellbeing, including 24/7 access to the Sonder app.
A progressive and competitive leave policy that gives you more space for what matters to you.
Endless possibilities with Woolworths Group
We’re a proud part of the Woolworths Group – 200,000+ bright minds, passionate hearts and unique perspectives working together connected by a shared Purpose – ‘to create better experiences together for a better tomorrow.’
Here you’ll be supported and empowered to explore new ideas, make brave commitments and innovate better ways to meet the food and everyday needs of more than 24 million customers every week. Backed by a culture of genuine care, a flexible approach to work and the support to grow your career and make a meaningful impact, the possibilities for what we can achieve together are endless.
Everyone belongs at Woolworths Group
Diversity, equity, inclusion, and belonging are key to realising our purpose of better together for a better tomorrow. We recognise the value our team’s diversity brings to our business, customers, and communities and that teams with diverse experiences and backgrounds enrich our group and are better able to innovate and solve problems. As one of the largest employers in Australia and New Zealand, we aim to create a truly inclusive workplace where everyone feels that they belong, can be their best selves, and reach their full potential.
We encourage all candidates to apply; please let us know in your application if we can support you with any adjustments in the hiring process.
You can learn more about working with us on LinkedIn or via www.wowcareers.com.au.  #work180. #Hybrid
Our Talent Acquisition Team and Hiring Leaders kindly request no unsolicited resumes or approaches from Recruitment Agencies. Woolworths Group is not responsible for any fees related to unsolicited resumes.
Show more
Show less","Data Engineering, SQL, Data Modeling, BigQuery, Looker, Dataflow, Fivetran, Cloud Composer, ETL Pipelines, Cloudbased Data Warehousing, Snowflake, BI Tools, Tableau, PowerBI, Data Governance, Data Security, Data Quality, Data Integrity, Data Visualization, Query Optimization, Report Optimization, GCP","data engineering, sql, data modeling, bigquery, looker, dataflow, fivetran, cloud composer, etl pipelines, cloudbased data warehousing, snowflake, bi tools, tableau, powerbi, data governance, data security, data quality, data integrity, data visualization, query optimization, report optimization, gcp","bi tools, bigquery, cloud composer, cloudbased data warehousing, data engineering, data governance, data integrity, data quality, data security, dataflow, datamodeling, etl pipelines, fivetran, gcp, looker, powerbi, query optimization, report optimization, snowflake, sql, tableau, visualization"
Lead Data Engineer,ING Australia,"Sydney, New South Wales, Australia",https://au.linkedin.com/jobs/view/lead-data-engineer-at-ing-australia-3781761565,2023-12-17,Parramatta, Australia,Mid senior,Hybrid,"Position Summary
The role is responsible for ensuring that the Extract, Transform and Load (ETL) programs and processes are kept up-to-date, and that new and modified programs are delivered as needed. The Lead Data Engineer is also responsible for designing and enhancing the data models used in the Data warehouse environment. This provides the organisation with a best practice analytical platform to make effective, fact-based decisions.
The Lead Data Engineer will also be responsible for coaching and mentoring less experienced team members to build their capability. Work collaboratively with product owners and stakeholder relationship management skills will be key to drive the right outcomes.
In future, as a lead data engineer you will be working on the implementation of a modern tech stack in Azure for data analytics, aggregation and storage of banking data, multiple data solutions will be required.
What are you going to do? You will be…
Collaborate with the Business to fully understand business requirements.
Provide support and guidance during the data transformation and deliver a road-map to help integrate unique business requirements
Design and develop ETL processes using Informatica to ING’s standards.
Analyse source systems to identify appropriate sources of data.
Identify and resolve data quality issues.
Identify opportunities to value-add to source system data.
Implement and enhance automation processes.
Design solutions that are in keeping with ING’s data model.
Design and document Data models for Data Marts & Data Feeds
Improve system performance by maintaining and tuning the DWH databases.
Use components of the Project Life Cycle to enhance quality in areas of Documentation, Standards, Formal and informal code reviews – as either reviewer or author.
Communicate data gaps (missing data or poor quality data) and potential solutions to business users.
Metadata documentation & facilitation.
Promoting ‘Best Practise’ ethos.
Essential
Technical Skills
7+ years’ experience in Data warehousing or related function.
5+ years’ experience as a Data Integration Expert.
3+ End to End BI Project Delivery experience.
Quick-Thinking, ready to hit the ground running.
Experience creating data integration solutions.
Strong knowledge in Informatica Power Center and related products.
Experience in Logical and Physical Data Modelling, including dimensional, relational, and flat-file data structures.
Good relational database experience (preferably fluent in MS SQL Server).
Experience in Agile and Waterfall methodology.
Experience in Kimball modelling techniques.
Desirable
Data lake storage, ADLS
Azure Services - Sql for ODS, Key Vault, Delta Lake
Synapse Dataflows and Pipeline for ETL's
Azure Functions preferably on Python/PySpark
Logging- Log analytics, Log monitoring
PowerBI for reporting
Azure devops with Terraforms
Data bricks for analytics
Logic apps
About Us
At ING, we want to make life simpler and more worthwhile – for everyone who banks with us, for the people who work with us, and the community at large, too.
When you come to work at ING, you’re joining a team where individuality isn’t just accepted, it’s encouraged. We’ve built a culture that’s fun, friendly and supportive – it’s the kind of place where you can be yourself and make the most of whatever you have to offer.
We give people the freedom to take risks, think differently, take ownership of their work, and make great things happen. We’re here to help you get ahead. And with our global network, there’s plenty of scope to take your career in new directions, perhaps even ones you’ve never considered.
People of all ages, sexual orientations, cultures and backgrounds are welcome to apply – likewise if you’re an indigenous Australian, or you’re living with a disability, or you have family or caring responsibilities.
Sound like the kind of place you’d feel at home? We’d love to hear from you.
(One last thing, ING operates a direct talent sourcing model. So no agency introductions, please.)
Show more
Show less","Data warehousing, Data integration, ETL (Extract Transform Load), Data modeling, Informatica Power Center, SQL Server, Kimball modeling, Azure Data Lake Storage (ADLS), Azure Services, Synapse Dataflows, Synapse Pipelines, Azure Functions, Python, PySpark, Log analytics, Log monitoring, PowerBI, Azure DevOps, Terraforms, Data bricks, Logic apps","data warehousing, data integration, etl extract transform load, data modeling, informatica power center, sql server, kimball modeling, azure data lake storage adls, azure services, synapse dataflows, synapse pipelines, azure functions, python, pyspark, log analytics, log monitoring, powerbi, azure devops, terraforms, data bricks, logic apps","azure data lake storage adls, azure devops, azure functions, azure services, data bricks, data integration, datamodeling, datawarehouse, etl extract transform load, informatica power center, kimball modeling, log analytics, log monitoring, logic apps, powerbi, python, spark, sql server, synapse dataflows, synapse pipelines, terraforms"
Senior / Lead Data engineer,Clarrow,"Sydney, New South Wales, Australia",https://au.linkedin.com/jobs/view/senior-lead-data-engineer-at-clarrow-3785576063,2023-12-17,Parramatta, Australia,Mid senior,Hybrid,"Are you a hands on and highly skilled Data Engineer? Join a high profile data transformation programme and play a key role in a global team.
We are searching for multiple Senior / Lead
data engineers
to join our global client to help develop innovative applications and analysis for a migration project in their commodities global markets team.
In This Role You Will
build complex data models and work with the business to understand data requirements. you will deliver insights from a wide range of data sources/databases as well as create reusable tools in Python/JavaScript.
a major part of the role will be documenting your findings using Confluence, Collibra and Alation. This will all be done in an Agile environment.
What You Will Be Skilled In
Strong data modelling experience with a background in Banking and Finance industry
Good SQL experience
Python experience
Experience with Collibra and Alation
Experience with Big Data querying tools like Hive, Spark, Presto
Knowledge of NoSQL databases
Managed data pipelining with tools like Apache Oozie or Airflow
Knowledge of programming languages like Java, C++ or Scala in context of Big data technologies
Prior working experience with AWS - any of EC2, S3, EBS, ELB, RDS, Dynamo DB, EMR, Apache Parquet
API integration
Knowledge of real time integration using Collibra Kafka, Spark streaming or other technologies.
The key cultural attributes you will have are excellent communication skills both written and verbal and be a team player with the ability to forecast workloads and priorities for effective efficiencies.
If you tick these boxes and want to know more please reach out to
Steve Bielby
on
0419588115
or email your CV to
steve@clarrow.com.au.
Show more
Show less","Data Engineering, Data Modeling, Python, JavaScript, Confluence, Collibra, Alation, Agile, SQL, Big Data, Hive, Spark, Presto, NoSQL, Apache Oozie, Airflow, Java, C++, Scala, AWS, EC2, S3, EBS, ELB, RDS, Dynamo DB, EMR, Apache Parquet, API integration, Collibra Kafka, Spark streaming","data engineering, data modeling, python, javascript, confluence, collibra, alation, agile, sql, big data, hive, spark, presto, nosql, apache oozie, airflow, java, c, scala, aws, ec2, s3, ebs, elb, rds, dynamo db, emr, apache parquet, api integration, collibra kafka, spark streaming","agile, airflow, alation, apache oozie, apache parquet, api integration, aws, big data, c, collibra, collibra kafka, confluence, data engineering, datamodeling, dynamo db, ebs, ec2, elb, emr, hive, java, javascript, nosql, presto, python, rds, s3, scala, spark, spark streaming, sql"
Senior Database Engineer,ING Australia,"Sydney, New South Wales, Australia",https://au.linkedin.com/jobs/view/senior-database-engineer-at-ing-australia-3781795251,2023-12-17,Parramatta, Australia,Mid senior,Hybrid,"At ING Australia, we do things differently. By challenging ourselves and collaborating on a global scale, we offer a forward-thinking approach to banking. Join us and achieve your full potential while making a difference to the communities we're part of.
Great opportunity for an experienced “
Senior Database Engineer
” to join our Database Engineering Team. Ideal candidate for this role will have broad knowledge of system engineering/automation concepts, practices and be able to perform work related to analysis, design, programming, implementation, maintenance and enhancements in a SQL Server database platform.
What you’ll do
…
Responsible for end to end infra component and support.
Set the standard for effective documentation and communication of knowledge, system and process.
Champion effective adoption of Devops methodology, practice and attitudes / mindsets
Build collaborative relationships with key stakeholders to achieve business outcomes
Role model Agile leadership behaviours Champion innovative use of people, process & technology to solve hidden problems.
What We’re Looking For….
Strong expertise with Always ON/availability groups.
Proficient in building and supporting a SQL Server (2012+)
Experience in performance and query tuning.
Proficient with TSQL.
Good PowerShell skills for automation.
What’s In It For You..
Discounted ING Health Insurance
An additional Rest Day to support your wellbeing
An IMPACT day to volunteer on an approved sustainability activity
About Us
At ING, we want to make life simpler and more worthwhile – for everyone who banks with us, for the people who work with us, and the community at large, too.
When you come to work at ING, you’re joining a team where individuality isn’t just accepted, it’s encouraged. We’ve built a culture that’s fun, friendly and supportive – it’s the kind of place where you can be yourself and make the most of whatever you have to offer.
We give people the freedom to take risks, think differently, take ownership of their work, and make great things happen. We’re here to help you get ahead. And with our global network, there’s plenty of scope to take your career in new directions, perhaps even ones you’ve never considered.
We are all about celebrating success and as a result we are proud to be a WGEA Employer of Choice for Gender Equality and a certified Family Inclusive workplace.
Sound like the kind of place you’d feel at home? We’d love to hear from you
Show more
Show less","System Engineering, Automation, Analysis, Design, Programming, Implementation, Maintenance, Enhancements, SQL Server, Always On / Availability Groups, Performance Tuning, Query Tuning, TSQL, PowerShell, DevOps, Agile Leadership, People Skills, Process Improvement, Technology Adoption, Hidden Problem Solving, Risk Taking, Ownership, Global Network, Gender Equality, Family Inclusive Workplace","system engineering, automation, analysis, design, programming, implementation, maintenance, enhancements, sql server, always on availability groups, performance tuning, query tuning, tsql, powershell, devops, agile leadership, people skills, process improvement, technology adoption, hidden problem solving, risk taking, ownership, global network, gender equality, family inclusive workplace","agile leadership, always on availability groups, analysis, automation, design, devops, enhancements, family inclusive workplace, gender equality, global network, hidden problem solving, implementation, maintenance, ownership, people skills, performance tuning, powershell, process improvement, programming, query tuning, risk taking, sql server, system engineering, technology adoption, tsql"
Data Archival Engineer,"The Dignify Solutions, LLC","Bloomfield, CT",https://www.linkedin.com/jobs/view/data-archival-engineer-at-the-dignify-solutions-llc-3768009550,2023-12-17,Marion,United States,Associate,Onsite,"Roles & Responsibilities
Five plus years of hands-on experience with IBM Optim data archival tool
Good understanding of database Life Cycle management
Good command of SQL for multiple relational database environments (e.g., Oracle, DB2 LUW, DB2 Mainframe, Postgres, MS SQL Server,Teradata etc.)
End to end Optim data archive expertise( like Create Access Definition , Archive Request, Extract Request, Restore Request, DB alias configuration on multiple sources
Extensive knowledge on Optim Connect
Generate Reports for archive requests and delete requests for audit purpose
Experience on Unstructured data archival
Automation skills with Python, shell script, Power Shell
Knowledge to apply ifix for Optim when needed
Knowlede on upgrading Optim version
Manage and maintain Data Archival processes
Monitor and tune data archival performance
Create storage profiles
Create Legal holds based on the data retention policies
Mentor team members to develop talent on the data archival
Provide insight to team leadership regarding best practices for data archival
Level 3 support for oncall support
Setup security standards, guidelines
Experience with cloud implementation with Data Archival
Experience with database level encryption
Experience/Education
Bachelor's degree in Computer Science or related discipline
Understanding of builds and management of Relational database technologies
Experience with database performance, scaling, and tuning concepts
Strong collaboration skills and experience working across multiple teams both within a division and enterprise-wide
Strong interpersonal, problem solving, facilitation, and prioritization skills
Strong technical acumen, able to ensure that technical solutions, strategies, practices, and governance deliver business and customer value
Development experience with one or more modern scripting/automation languages (Ansible, Python, etc.) - preferred
Energy, integrity, and strong work ethic
Show more
Show less","IBM Optim, Data Archival, Database life cycle management, SQL, Oracle, DB2 LUW, DB2 Mainframe, Postgres, Microsoft SQL Server, Teradata, Optim Connect, Python, Shell script, Power Shell, ifix, Storage profiles, Legal holds, Encryption, Computer Science, Relational database technologies, Database performance, DBMS, Collaboration, Interpersonal skills, Problem solving, Facilitation, Prioritization, Technical acumen, Governance, Automation, Ansible","ibm optim, data archival, database life cycle management, sql, oracle, db2 luw, db2 mainframe, postgres, microsoft sql server, teradata, optim connect, python, shell script, power shell, ifix, storage profiles, legal holds, encryption, computer science, relational database technologies, database performance, dbms, collaboration, interpersonal skills, problem solving, facilitation, prioritization, technical acumen, governance, automation, ansible","ansible, automation, collaboration, computer science, data archival, database life cycle management, database performance, db2 luw, db2 mainframe, dbms, encryption, facilitation, governance, ibm optim, ifix, interpersonal skills, legal holds, microsoft sql server, optim connect, oracle, postgres, power shell, prioritization, problem solving, python, relational database technologies, shell script, sql, storage profiles, technical acumen, teradata"
Future Opportunity- Data Engineering Consultant,Avanade,"Hartford, CT",https://www.linkedin.com/jobs/view/future-opportunity-data-engineering-consultant-at-avanade-3781777907,2023-12-17,Marion,United States,Associate,Onsite,"Job Description
RECRUITING FOR THE FUTURE
This posting is for a future job opportunity.
To support the needs of our clients, Avanade is actively recruiting and interviewing for the role outlined below, as we continue to build on another year of growth in our business. Now, with more than 60,000 employees around the globe, we are positioning our organization to effectively prepare for the future. What does this mean for you? We encourage you to apply and interview for this role without the need to decide now. This allows you to connect with leaders and hiring managers at a pace that works for you and when roles become available, you will be the first know.
Data Engineering Consultant
Our talented Data and AI Practice is made up of globally recognized experts- and there's room for more analytical and ambitious data professionals. If you're passionate about helping clients make better data-driven decisions to tackle their most complex business issues. let's talk. Take your skills to a new level and launch a career where you can can truly do what matters.
Come join us
As a Data Engineer within our Data and AI practice, you’ll design and set up projects that bring together information from a variety of sources, to enable analysis and decision-making. You make sure that data is accessible and easy to use, so that it can be used for routine and ad-hoc analysis.
Together we do what matters.
What You'll Do
Use your knowledge to plan and deliver data warehouses and storage
Take part in crafting and running bespoke data services for individual projects
Stay up to date with business best practice in using and retrieving data
Design, develop, adapt, and maintain data warehouse architecture and relational databases that support data mining
Customize storage and extraction, metadata, and information repositories
Build and use effective metrics and monitoring processes
Help to develop business intelligence tools
Craft and maintain report forms and formats, information dashboards, data generators and canned reports, as well as other information portals and resources
Skills And Experiences
Strong knowledge of Python, Spark, and T-SQL
Microsoft Fabric/Synapse, Purview and Azure Databricks, PowerBI
Database, storage, collection and aggregation models, techniques, and technologies – and how to apply them in business
Ability to use technology to aim business problems using one or more Microsoft Analytics services for building data pipelines, data streams, and system integration
About You
Characteristics that can spell success for this role:
Advanced knowledge in entity and relationship extraction from unstructured data
Understanding of database table indexing, with emphasis on clustered column store tables
Proficient in data handling, understanding, analysis and interpretation of results
Experience with SQL technologies: Databricks (Spark), OR Azure Synapse
Data security: Must be able to demonstrate an understanding of data security at rest and in transit
Scale: Deliver performance at scale, and accurately anticipate system enhancement needs
Data integrity: Data manipulation, error identification and handling, and modeling
Enjoy your career
Some of the best things about working at Avanade
People-first culture that supports innovation and encourages people to move forward
Turn your ideas to human impact by cultivating a team that will help clients unlock what’s next
Opportunities to innovate in the Microsoft platform while charting your own career path
On-the-job growth through hackathons, innovation contests, and other expressions of our passion for innovating with purpose
Real-time access to technical and skilled resources globally
Find out more about some of our benefits here .
A great place to work
As you bring your skills and abilities to Avanade, you’ll get distinctive experiences, limitless learning, and ambitious growth in return. As we continue to build our diverse and inclusive culture, we become even more innovative and creative, helping us better serve our clients and our communities. You’ll join a community of smart, supportive collaborators to lift, mentor, and guide you, but to also lean on your expertise . You get a company purpose-built for business-critical, leading-edge technology solutions, committed to improving the way humans work, interact, and live. It’s all here, so take a closer look!
We work hard to provide an inclusive, diverse culture with a deep sense of belonging for all our employees. Visit our Inclusion & Diversity page.
Create a future for our people that focuses on
Expanding your thinking
Experimenting courageously
Learning and pivoting
Inspire greatness in our people by
Empowering every voice
Encouraging boldness
Celebrating progress
Accelerate the impact of our people by
Amazing the client
Prioritizing what matters
Acting as one
Learn more
To learn more about the types of projects our Data & AI team works on check out these case studies:
thyssenkrupp Materials Services uses data to help strike a delicate operational balance
What matters to SSE Renewables is innovating for a sustainable future
Hachette UK enables employees with machine learning-driven contract searches
Marston Holdings identifies significant cloud savings
Interested in knowing what’s going on inside Avanade? Check out our blogs:
Avanade Insights – exchange ideas that drive tomorrow’s innovation
Inside Avanade – explore what life is like working at Avanade
Compensation for roles at Avanade varies depending on a wide array of factors including but not limited to the specific office location, role, skill set and level of experience. As required by local law, Avanade provides a reasonable range of compensation for roles that may be hired in New York as set forth below and information on benefits offered is here.
Range of Starting Pay for role:
New York: $121,500 – $ 143,000
About Avanade
Avanade is the leading provider of innovative digital, cloud and advisory services, industry solutions and design-led experiences across the Microsoft ecosystem. Every day, our 59,000 professionals in 26 countries make a genuine human impact for our clients, their employees and their customers.
We have been recognized as Microsoft’s Global SI Partner of the Year more than any other company. With the most Microsoft certifications (60,000+) and 18 (out of 18) Gold-level Microsoft competencies, we are uniquely positioned to help businesses grow and solve their toughest challenges.
We are a people first company, committed to providing an inclusive workplace where employees feel comfortable being their authentic selves. As a responsible business, we are building a sustainable world and helping people from underrepresented communities fulfil their potential.
Majority owned by Accenture, Avanade was founded in 2000 by Accenture LLP and Microsoft Corporation. Learn more at www.avanade.com .
Show more
Show less","Python, Spark, TSQL, Microsoft Fabric/Synapse, Purview, Azure Databricks, PowerBI, Database, Storage, Collection, Aggregation, Data pipelines, Data streams, System integration, Entity extraction, Relationship extraction, Data security, Data integrity, SQL, Data warehouses, Data mining","python, spark, tsql, microsoft fabricsynapse, purview, azure databricks, powerbi, database, storage, collection, aggregation, data pipelines, data streams, system integration, entity extraction, relationship extraction, data security, data integrity, sql, data warehouses, data mining","aggregation, azure databricks, collection, data integrity, data mining, data security, data streams, data warehouses, database, datapipeline, entity extraction, microsoft fabricsynapse, powerbi, purview, python, relationship extraction, spark, sql, storage, system integration, tsql"
"Associate Director, Data Engineering Services","Alexion Pharmaceuticals, Inc.","New Haven, CT",https://www.linkedin.com/jobs/view/associate-director-data-engineering-services-at-alexion-pharmaceuticals-inc-3732679744,2023-12-17,Marion,United States,Associate,Onsite,"At
Alexion, AstraZeneca Rare Disease
, we are innovators, leaders, and collaborators. At the heart of everything we do is an intense desire to pursue the toughest challenges and truly change the world for people living with rare and devastating diseases.
This is what you will do:
Associate Director, Data Engineering Services, reports to the Head of IT Data, Insights & Analytics (DI&A) and is a critical role in Alexion IT Rare Disease Unit (RDU) organization. The focus of this role is to take a data product-driven approach to deliver large Data & AI Programs in RDU.
Alexion is looking for a sharp-minded, assertive, and energetic DI&A leader who will be responsible for establishing the strategy, functional plans, and delivery of programs to meet the short- and long-term goals and metrics of the organization. Should be able to engineer data solutions to meet specific business demands, enabling value from information and aligning with the overall DI&A team to ensure end-to-end delivery.
We are looking for adaptable candidates with a broad set of technology skills to be able to lead robust data solutions that leverage disparate datasets, data lake repositories, engineering workflows, visualization, and Data Science toolkits. The candidate will have to oversee cross-functional and global demands and work with internal and vendor delivery organizations to ensure supply is aligned to demand and the right delivery approach/model has been followed.
The candidate needs to be client-oriented, capable of independently developing a sophisticated data foundation, data engineering & integration solutions, and skilled at presenting solutions to broad audiences within IT and business groups.
The right candidate for this position should have a proven track record for an international Pharma/Biopharma company. They should have superior program delivery and team management capabilities, strong technical skills, and the ability to manage vendor relationships. Since Alexion RDU is looking to build the next generation of patient-centric data products, we are looking for a qualified individual whose passion is understanding how to best utilize healthcare data and platforms to drive business innovation.
You will be responsible for:
Delivering data engineering services, ingesting data in the Alexion Data Platform, creating data hubs to integrate and distribute data in the form of products, supporting integration to digital applications and generation of insights through AI/ML
Gathering and understanding requirements via interactions with cross-functional business users, aligning on technical design with Information Architects, and converting them into technical solutions
Work with delivery teams to test and quality assess new DI&A solutions, to ensure they are fit for release: Code Assurance, Unit and System Integration Testing, Data testing, Release Management, and support of UAT processes
Evangelize adoption of Agile best practices, standard DevOps toolsets, and ensuring we have modern, controlled and regulated agile software development practices and a great engineering culture
Promote and embed CI/CD to improve the software development process and transform the IT system estate into an ecosystem that delivers great user experiences and excellent IT service to the business
Manage the 3rd Party delivery teams to drive Data Engineering operational excellence with regular performance metrics, health, scorecards and analysis reports
Work with the Data Management team to align platform and delivery capabilities towards the development of FAIR Data Products to support AI/ML solutions
Provide consultancy advice, as a trusted partner, to both the global and regional stakeholders within the business and other technology teams – including AZ Enterprise Architecture - from strategic partners / vendors for on-going and new implementation projects and ensure they are aligned with organization goals
You will need to have:
BS with 7 plus years of industry experience leading, designing, and developing data lake and data integration architectures, using FAIR data sets and prior experience in managing end-to-end engagements in BI using Qlik Sense and Data Science languages like R and Python
Knowledge of Cloud platforms like AWS suite (EMR, EC2, S3, Athena, Lambda), modern data storage layer like Snowflake, dbt, FiveTran, SnapLogic, and cloud data management applications like Immuta, Collibra and Reltio/ Veeva Network
Excellent SQL, Python, Jave programming skills with hands on experience with an orchestration capability (e.g Airflow)
Familiarity with semantic web technologies, ontology modeling, and graph algorithms
A desire to stay up to date with cutting edge technologies in the field and streaming data processing and help further a culture of innovation and excellence within the team.
Experience working in software engineering CI/CD processes, with experience in the design and implementation of applications build, release, deployment, containerization and configuration control management.
Excellent Data Modelling skills (RDBMS concepts, Normalization, dimensional modelling, star/snowflake schema etc.)
Experience working with Git, JIRA, ServiceNow and Automated Testing tools (Jenkins, Bamboo, JIRA, Confluence, CI CD (Continuous Integration /Continuous Deployment) tools.
Knowledge of Privacy and healthcare interoperability standards like GDPR, HIPAA and HL7, FHIR.
Familiarity with Git, Data-Ops, and Continuous Integration /Continuous Deployment pipelines and experience in Agile project execution.
Experience leading a globally distributed team of Data Engineers through all phases of the development life cycles, including concept definition, product design, development, implementation, and testing
Must be analytic, creative, and self-motivated and bring a problem-solving capability focused on continuous improvement.
Excellent communication & documentation skills, eager to learn and develop new tech skills as required.
The duties of this role are generally conducted in an office environment. As is typical of an office-based role, employees must be able, with or without an accommodation to use a computer; engage in communications via phone, video, and electronic messaging; engage in problem solving and non-linear thought, analysis, and dialogue; collaborate with others; maintain general availability during standard business hours
We would prefer for you to have:
M.Sc. in a relevant field (such as mathematics, computer science, engineering) with 5+ years of industry experience
Understanding and prior experience developing and maintaining physical data models, schemas, and ontologies for knowledge graphs, ensuring they accurately represent complex concepts and relationships, and optimized for performance
Experience with graph database systems (Neo4j, Amazon Neptune, Virtuoso) and related query languages (Cypher, SPARQL)
Prior experience with Natural Language Processing (NLP) and common machine learning models
Domain knowledge (processes & data): Pharma Compliance, Regulatory Reporting etc.
Prior experience working on digital health initiatives.
When we put unexpected teams in the same room, we unleash bold thinking with the power to inspire life-changing medicines. In-person working gives us the platform we need to connect, work at pace and challenge perceptions. That’s why we work, on average, a
minimum of three days per week from the office
. But that doesn’t mean we’re not flexible. We balance the expectation of being in the office while respecting individual flexibility. Join us in our unique and ambitious world.
AstraZeneca requires all US employees to be fully vaccinated for COVID-19 but will consider requests for reasonable accommodations as required by applicable law.
AstraZeneca embraces diversity and equality of opportunity. We are committed to building an inclusive and diverse team representing all backgrounds, with as wide a range of perspectives as possible, and harnessing industry-leading skills.
We believe that the more inclusive we are, the better our work will be. We welcome and consider applications to join our team from all qualified candidates, regardless of their characteristics. We comply with all applicable laws and regulations on non-discrimination in employment (and recruitment), as well as work authorization and employment eligibility verification requirements. #HYBRID
Show more
Show less","Data Engineering, Data Integration, Agile Methodology, Dev Ops, Cloud Computing (AWS Snowflake), Business Intelligence (Qlik Sense), Data Science (R Python), SQL, Java, Airflow, Git, Jira, Jenkins, Bamboo, Confluence, GDPR, HIPAA, HL7, FHIR, Data Modelling, Normalization, Dimensional Modelling, Star/Snowflake Schema, DataOps, Continuous Integration, Continuous Deployment, Neo4j, Amazon Neptune, Virtuoso, Cypher, SPARQL, Natural Language Processing, Machine Learning, Pharma Compliance, Regulatory Reporting, Digital Health","data engineering, data integration, agile methodology, dev ops, cloud computing aws snowflake, business intelligence qlik sense, data science r python, sql, java, airflow, git, jira, jenkins, bamboo, confluence, gdpr, hipaa, hl7, fhir, data modelling, normalization, dimensional modelling, starsnowflake schema, dataops, continuous integration, continuous deployment, neo4j, amazon neptune, virtuoso, cypher, sparql, natural language processing, machine learning, pharma compliance, regulatory reporting, digital health","agile methodology, airflow, amazon neptune, bamboo, business intelligence qlik sense, cloud computing aws snowflake, confluence, continuous deployment, continuous integration, cypher, data engineering, data integration, data modelling, data science r python, dataops, dev ops, digital health, dimensional modelling, fhir, gdpr, git, hipaa, hl7, java, jenkins, jira, machine learning, natural language processing, neo4j, normalization, pharma compliance, regulatory reporting, sparql, sql, starsnowflake schema, virtuoso"
Data Engineer,Jobs for Humanity,"Hartford, CT",https://www.linkedin.com/jobs/view/data-engineer-at-jobs-for-humanity-3786355184,2023-12-17,Marion,United States,Mid senior,Onsite,"Company Description
Jobs for Humanity is partnering with MassMutual to build an inclusive and just employment ecosystem. Therefore, we prioritize individuals coming from the following communities: Refugee, Neurodivergent, Single Parent, Blind or Low Vision, Deaf or Hard of Hearing, Black, Hispanic, Asian, Military Veterans, the Elderly, the LGBTQ, and Justice Impacted individuals. This position is open to candidates who reside in and have the legal right to work in the country where the job is located.
Company Name: MassMutual
Job Description
At MassMutual, we believe in helping everyone achieve financial freedom. We are looking for passionate individuals to join our Data Management and Engineering team as data engineers.
What We Are Looking For
We want a data engineer who is enthusiastic about working with data. You should be able to create data projects from scratch and collaborate with business partners to meet their requirements. Learning new technologies and working in the cloud should excite you. Additionally, you should be a team player and a good communicator.
Job Responsibilities
Design, build, and maintain complex data jobs that provide value to the business
Translate business requirements into technical specifications
Collect and organize data from various sources
Clean and enhance data quality
Provide support and technical leadership to the offshore team.
MassMutual’s data platform
Create tools to improve the delivery of projects
Collaborate with other developers and provide mentorship
Evaluate and recommend tools, technologies, and processes
Work in an Agile development environment and make incremental improvements
Basic Qualifications
Bachelor's degree in computer science, engineering, or a related field
5+ years of experience in Data warehousing and analytics
Proficiency in SQL programming and query optimization
Understanding of ETL/ELT methodologies and tools
Troubleshooting and root cause analysis skills
Strong communication, problem-solving, and organizational skills
Ability to work independently and lead small teams
Preferred Qualifications
Master's degree in computer science, engineering, or a related field
Experience working in a cloud environment (e.g., AWS)
Hands-on experience with Python
Knowledge of data processing technologies like Apache Spark or Kafka
Familiarity with orchestration and scheduling tools (e.g., Apache Airflow)
Experience in data reporting and cataloging tools (e.g., Microstrategy, Tableau, Looker, Alation)
Note: Veterans are welcome to apply, regardless of their discharge status. We offer equal employment opportunities to all individuals, including minorities, females, LGBTQIA+, individuals with disabilities, and protected veterans. If you need any accommodations during the application process, please contact us and let us know how we can assist you.
Show more
Show less","Data engineering, Data warehousing, SQL, ETL/ELT, Apache Spark, Kafka, Python, AWS, Apache Airflow, Microstrategy, Tableau, Looker, Alation","data engineering, data warehousing, sql, etlelt, apache spark, kafka, python, aws, apache airflow, microstrategy, tableau, looker, alation","alation, apache airflow, apache spark, aws, data engineering, datawarehouse, etlelt, kafka, looker, microstrategy, python, sql, tableau"
Associate Data Engineer,The Hartford,"Hartford, CT",https://www.linkedin.com/jobs/view/associate-data-engineer-at-the-hartford-3768141290,2023-12-17,Marion,United States,Mid senior,Onsite,"Assoc Data Engineer - GE08BE
We’re determined to make a difference and are proud to be an insurance company that goes well beyond coverages and policies. Working here means having every opportunity to achieve your goals – and to help others accomplish theirs, too. Join our team as we help shape the future.
Group Benefits – Enterprise Data Services is undergoing a transformation to Cloud, and we are looking for an enthusiastic Data Engineer to join our team. As a Data Engineer you will work on implementing complex data projects focusing on collecting, parsing, managing, analyzing, and visualizing large sets of data to turn information into insights using multiple platforms. You will develop and implement Proof of Concepts (PoCs), generic frameworks, and real time pipelines, using SQL, Amazon Web Services (AWS), ETL tools, Snowflake, and Python. Introduce automation into software application development using Continuous Integration/Continuous Delivery (CI/CD) methodologies.
Responsibilities:
Design, develop, and maintain data pipelines for extraction, transformation and loading processes using ETL tools, R/Python in Cloud and Oracle environments to support data analytics and reporting efforts.
Integrate and aggregate complex data from multiple data sources and platforms to enhance customer access to information and promote a data-driven culture.
Incorporate core data management competencies including data governance, data security and data quality.
Collaborate with Business Intelligence team and Data Engineering lead to establish a strategic end-to-end solution and to ensure data assets meet reporting and analytics requirements.
Develop & deploy analytical solutions leveraging machine learning technologies.
Develop and support the migration activities of the reporting assets in Oracle assets to Snowflake.
Collaborate with the Enterprise Data teams to provide user acceptance testing, maintain data quality, and advance the technical toolset.
Develop and maintain data analytics tools and frameworks.
Stay up to date with emerging data technologies and industry best practices.
Demonstrate a strong willingness to explore and leverage new tools and technologies as per project needs.
Qualifications:
Must be authorized to work in the U.S.
Bachelor’s degree in computer science, Data Engineering, or a related field
Minimum 2 years of experience as a Data Engineer, with a strong track record in quantitative, analytical and data manipulation skills
Experience with ETL tools (Informatica, Talend)
Unit, interface and end user testing concepts and tooling (functional & non-functional)
Confident, self-starter capable of independently driving multiple concurrent projects to completion.
Knowledge on any Cloud tech stack
Advanced knowledge of SQL as it pertains to data & analytics and reporting.
Experience with any scripting or programing language – Python, JavaScript etc.
Experience with Test automation & DevOps tools
Knowledge of Agile Scrum/SAFE methodology
Effectively use collaboration tools like Rally, Jira etc.
Nice to Have:
JavaScript/React/Angular experience a huge plus.
Experience with any of the reporting tools –Tableau, Business Objects, Micro-Strategy
Insurance & Financial services domain knowledge
Knowledge of AWS cloud technology – Code Build, Code Pipeline, Containers
Compensation
The listed annualized base pay range is primarily based on analysis of similar positions in the external market. Actual base pay could vary and may be above or below the listed range based on factors including but not limited to performance, proficiency and demonstration of competencies required for the role. The base pay is just one component of The Hartford’s total compensation package for employees. Other rewards may include short-term or annual bonuses, long-term incentives, and on-the-spot recognition. The annualized base pay range for this role is:
$70,640 - $105,960
Equal Opportunity Employer/Females/Minorities/Veterans/Disability/Sexual Orientation/Gender Identity or Expression/Religion/Age
About Us | Culture & Employee Insights | Diversity, Equity and Inclusion | Benefits
Show more
Show less","Cloud, Python, Snowflake, SQL, ETL tools, Amazon Web Services (AWS), Continuous Integration/Continuous Delivery (CI/CD), Data pipelines, Machine learning, Oracle, JavaScript, Rally, Jira, Tableau, Business Objects, MicroStrategy, AWS cloud technology, Code Build, Code Pipeline, Containers","cloud, python, snowflake, sql, etl tools, amazon web services aws, continuous integrationcontinuous delivery cicd, data pipelines, machine learning, oracle, javascript, rally, jira, tableau, business objects, microstrategy, aws cloud technology, code build, code pipeline, containers","amazon web services aws, aws cloud technology, business objects, cloud, code build, code pipeline, containers, continuous integrationcontinuous delivery cicd, datapipeline, etl tools, javascript, jira, machine learning, microstrategy, oracle, python, rally, snowflake, sql, tableau"
Data Software Engineer,Jobs for Humanity,"Hartford, CT",https://www.linkedin.com/jobs/view/data-software-engineer-at-jobs-for-humanity-3786355154,2023-12-17,Marion,United States,Mid senior,Onsite,"Company Description
Jobs for Humanity is partnering with MassMutual to build an inclusive and just employment ecosystem. Therefore, we prioritize individuals coming from the following communities: Refugee, Neurodivergent, Single Parent, Blind or Low Vision, Deaf or Hard of Hearing, Black, Hispanic, Asian, Military Veterans, the Elderly, the LGBTQ, and Justice Impacted individuals. This position is open to candidates who reside in and have the legal right to work in the country where the job is located.
Company Name: MassMutual
Job Description
Our Enterprise Data Analytics Platform (EDAP) is designed to help our customers explore and understand data. Our customers, both internal and external, rely on us to provide accurate, real-time solutions to their growing data needs. From storing and processing data to analyzing and serving it, our platform is essential for their analytics needs. We use a variety of open source and industry recognized technologies such as Apache Spark, Apache Storm, Amazon Web Services, and Apache Kafka.
We are looking for a skilled Software Engineer to join our team. You will work closely with other engineers and product management to build reliable and scalable systems that address real-world data challenges. This role focuses on building and optimizing distributed systems.
Responsibilities:
Develop and optimize cloud computing infrastructure
Design and implement distributed data-focused analytics solutions for high-volume and complex data
Write code, conduct testing and code reviews, and work on API specifications and automation
Build scalable and robust services related to our data platform
Implement new features and optimize existing ones for maximum performance
Identify opportunities to adopt innovative technologies and continuously improve service availability
Troubleshoot problems in data flow, transformation, and other stages of the data processing pipelines
Collaborate with other teams to prevent and resolve technical issues
Work in an Agile development environment and prioritize features
Basic Qualifications:
Bachelor's degree
5 years of relevant experience
Strong curiosity and ability to get things done
Strong knowledge of algorithms, data structures, and coding
Experience with software engineering best practices
Programming experience in Java or Scala
Strong in SQL
Experience with distributed technologies such as relational stores, columnar or NoSQL stores, distributed processing engines, and distributed queues
Experience working with large amounts of data
Ability to work with data scientists and analysts to develop solutions
Experience with AWS or similar cloud platforms
Authorized to work in the United States
Preferred Qualifications:
Bachelor's degree in Computer Science
8 years of relevant development experience
Knowledge of open source and industry standard data processing, storage, and serving technologies
Ability to troubleshoot and solve problems
Experience with data warehousing
Experience with Git or other source control systems
Ability to grasp and execute development based on design requirements
Ability to drive change through consensus
Experience in the Financial/Insurance industry
MassMutual is an equal opportunity employer. We welcome applicants of all backgrounds and identities. Veterans are also encouraged to apply. If you require any accommodations during the application process, please contact us to let us know how we can assist you.
Show more
Show less","Apache Spark, Apache Storm, Amazon Web Services, Apache Kafka, Cloud Computing, SQL, NoSQL, Distributed Systems, Java, Scala, AWS, Git, Source Control, Data Warehousing, Data Science, Data Analysis, Agile Development","apache spark, apache storm, amazon web services, apache kafka, cloud computing, sql, nosql, distributed systems, java, scala, aws, git, source control, data warehousing, data science, data analysis, agile development","agile development, amazon web services, apache kafka, apache spark, apache storm, aws, cloud computing, data science, dataanalytics, datawarehouse, distributed systems, git, java, nosql, scala, source control, sql"
Quantitative Data Engineer,Jobs for Humanity,"Hartford, CT",https://www.linkedin.com/jobs/view/quantitative-data-engineer-at-jobs-for-humanity-3786352552,2023-12-17,Marion,United States,Mid senior,Onsite,"Company Description
Jobs for Humanity is partnering with MassMutual to build an inclusive and just employment ecosystem. Therefore, we prioritize individuals coming from the following communities: Refugee, Neurodivergent, Single Parent, Blind or Low Vision, Deaf or Hard of Hearing, Black, Hispanic, Asian, Military Veterans, the Elderly, the LGBTQ, and Justice Impacted individuals. This position is open to candidates who reside in and have the legal right to work in the country where the job is located.
Company Name: MassMutual
Job Description
Job Title:
Quantitative Data Engineer
Team:
Quantitative Investment Platforms and Developer Operations Team
Employment:
Full-time
Location:
Boston, MA, Springfield, MA or New York, NY
The Opportunity
Are you passionate about technology and problem-solving? Join our team as a Data Engineer and work in a dynamic and collaborative environment. We believe that both technical skills and emotional intelligence are essential for success. You will have the opportunity to work on exciting projects, utilizing your expertise in Data Engineering, workflow orchestration, CI/CD pipelines, and cloud platforms.
The Team
Join the Quantitative Investment & Developer Operations team within MassMutual’s Investment Management organization. This team consists of skilled professionals who provide collaborative solutions to portfolio managers, focusing on resilience, accountability, agility, and continuous improvement. We foster a culture that encourages work-life balance and values cross-functional collaboration.
The Impact
We are seeking a Quantitative Data Engineer who is passionate about data and automating workflows. In this role, you will collaborate with business partners, translate requirements into technical specifications, and deliver high-quality solutions. Your responsibilities will include designing and maintaining ETL jobs, ingesting data from various sources, ensuring data quality control, and contributing to the development of our data platform. You will also have the opportunity to mentor other developers, evaluate technologies, and deploy workflow automation.
The Minimum Qualifications
Bachelor’s degree in Computer Science, Finance, Business, or a related field
2+ years of experience in IT and/or the finance industry
Proficiency in Python and understanding of ETL methodologies
Experience in cloud environments, such as AWS
Knowledge of relational databases (SQL Server/PostgreSQL) and NoSQL databases (Mongo)
Experience with Git and code review/deployment
Experience working in Agile/SCRUM environments
The Ideal Qualifications
Master’s degree in computer science, engineering, or a related field
4+ years of experience in IT, quantitative, investment, or finance industry
Experience with troubleshooting and root cause analysis
Knowledge of orchestration and scheduling tools
Experience with data reporting tools or Python libraries
Entrepreneurial mindset and ability to work in a rapid and iterative development environment
Demonstrated organization, problem-solving, and analytical skills
Excellent communication skills, both written and verbal
Data-driven mindset and ability to use quantitative tools
Ability to adapt to changing priorities and strong work ethic
Curiosity towards emerging digital and technology trends
Proven collaboration and influencing skills
What to Expect as Part of MassMutual and the Team
Regular meetings with the Quantitative and ETX project teams
One-on-one meetings with your manager
Mentorship opportunities
Networking opportunities through Business Resource Groups
Access to learning content on Degreed and other platforms
Ethics and integrity valued with industry-leading pay and benefits
MassMutual is an Equal Employment Opportunity Employer
We welcome and encourage individuals of all backgrounds to apply. Veterans are also welcome to apply, regardless of discharge status. If you require any accommodations during the application process, please contact us to discuss your specific needs.
Show more
Show less","Python, ETL, AWS, SQL, PostgreSQL, Mongo, Git, Agile, Scrum, Data Engineering, Workflow Orchestration, CI/CD Pipelines, Cloud Platforms","python, etl, aws, sql, postgresql, mongo, git, agile, scrum, data engineering, workflow orchestration, cicd pipelines, cloud platforms","agile, aws, cicd pipelines, cloud platforms, data engineering, etl, git, mongo, postgresql, python, scrum, sql, workflow orchestration"
Senior Database & Web Data Services Developer,Golden Compass,"New Haven, CT",https://www.linkedin.com/jobs/view/senior-database-web-data-services-developer-at-golden-compass-3779083248,2023-12-17,Marion,United States,Mid senior,Onsite,"Job Description
Job Description:
The Senior Database and Web Data Services developer is a hands-on implementer who uses ETL (Talend) and Java based technologies to code and configure utility applications, generic integration patterns, and identity solutions with guidance from a tech lead or software architect. Design, Code and unit-test functionality in ETL technologies. Develop generic implementations of integration patterns, write “glue code” critical to the customization and connectivity of core infrastructure applications (email, directory, calendar, etc.). Support and expand the process to integrate identity and access information from data stores and applications using direct and custom connections. Create and document functional/technical designs and test plans for proposed solutions. Adhere to and contribute to development and integration standards, best practices, and related deliverables. Provide Level 3 support for authentication solutions. Skills and Abilities Analyze, design, program, test, and implement ETL solutions using Talend Creating and maintaining complex SQL queries, stored procedures, functions, triggers, cursors, index, ad hoc queries, etc. Experience with Data Modeling and multiple data source types(SQL Server, XML files, Oracle and etc.) POJO (Plain Old Java Object) development skills sufficient to write and debug java exit routines for data manipulation Web Services / SOA Gateway configuration experience (CA SOA Gateway or similar) Excellent communication, interpersonal, and collaboration skills. Familiarity with Enterprise Integration Patterns Strong desire to work with team to adopt Agile best practices including time boxed sprints, frequent refactoring, continuous integration and test driven development. Proven ability to design and develop enterprise applications. Proven experience mentoring others. Excellent analytical and problem solving skills. Must be a fast learner with a strong desire and motivation to learn about the higher education environment. Familiarity with UML artifacts (class, sequence diagrams, etc.) Experienced in Agile Development Methodology. Must be highly adaptable to perform in a fast paced environment. Our ideal candidate will be passionate about newer technologies and is driven to continuously evolve and improve our development process through leveraging these technologies.
Show more
Show less","Java, Talend, ETL, Data Modeling, SQL, Stored Procedures, Functions, Triggers, Cursors, XML, POJO, Web Services, SOA Gateway, Agile, UML, Continuous Integration, Test Driven Development, Enterprise Applications","java, talend, etl, data modeling, sql, stored procedures, functions, triggers, cursors, xml, pojo, web services, soa gateway, agile, uml, continuous integration, test driven development, enterprise applications","agile, continuous integration, cursors, datamodeling, enterprise applications, etl, functions, java, pojo, soa gateway, sql, stored procedures, talend, test driven development, triggers, uml, web services, xml"
Sr. Data Engineer - Hybrid,The Hartford,"Hartford, CT",https://www.linkedin.com/jobs/view/sr-data-engineer-hybrid-at-the-hartford-3746299361,2023-12-17,Marion,United States,Mid senior,Onsite,"Sr Data Engineer - GE07BE
We’re determined to make a difference and are proud to be an insurance company that goes well beyond coverages and policies. Working here means having every opportunity to achieve your goals – and to help others accomplish theirs, too. Join our team as we help shape the future.
The
Small Commercial Analytics Office
is seeking a highly motivated
Senior Data
with a strong background in data analytics ,support the evolution of data strategy and proven ability to explore and embrace new technologies to join our dynamic team.
Job Summary:
As a Senior Data Engineer, you will be a key player in our data analytics journey, responsible for building and maintaining data pipelines, facilitating data-driven insights, and driving innovation within our organization. You will collaborate with cross-functional teams to design, develop, and optimize data infrastructure.
This role will have a Hybrid work arrangement, with the expectation of working in an office (Hartford, CT or Charlotte, NC) 3 days a week (Tuesday through Thursday).
Key Responsibilities:
Design, develop, and maintain data pipelines to support data analytics and reporting efforts.
Integrate and aggregate complex data from multiple data sources and platforms to enhance customer access to information and promote a data-driven culture
Develop & deploy analytical solutions leveraging machine learning technologies
Collaborate with the Enterprise Data teams to provide user acceptance testing, maintain data quality and advance the technical toolset.
Partner with Business Intelligence and Data Engineering leads to establish a strategic end-to-end solution
Develop and maintain data analytics tools and frameworks.
Stay up-to-date with emerging data technologies and industry best practices.
Demonstrate a strong willingness to explore and leverage new tools and technologies as per project needs.
Mentor junior data engineers and contribute to team growth.
Qualifications:
Bachelor's or Master's degree in Computer Science, Data Engineering, or a related field.
Minimum 5 years of experience as a Data Engineer, with a strong track record in quantitative, analytical and data manipulation skills
Strong technical experience in Oracle PL/SQL, Snowflake, Python, and R.
Familiar with ETL tools (Informatica, Talend)
Familiarity with data warehousing solutions (e.g., Oracle, Snowflake).
Knowledge of data integration tools and technologies.
Strong problem-solving skills and attention to detail.
Excellent communication and teamwork skills.
Experience in Commercial Lines is a plus
Able to independently drive multiple concurrent projects to completion
Compensation
The listed annualized base pay range is primarily based on analysis of similar positions in the external market. Actual base pay could vary and may be above or below the listed range based on factors including but not limited to performance, proficiency and demonstration of competencies required for the role. The base pay is just one component of The Hartford’s total compensation package for employees. Other rewards may include short-term or annual bonuses, long-term incentives, and on-the-spot recognition. The annualized base pay range for this role is:
$110,560 - $165,840
Equal Opportunity Employer/Females/Minorities/Veterans/Disability/Sexual Orientation/Gender Identity or Expression/Religion/Age
About Us | Culture & Employee Insights | Diversity, Equity and Inclusion | Benefits
Show more
Show less","Data Analytics, Data Strategy, Data Pipelines, Machine Learning, Oracle PL/SQL, Snowflake, Python, R, Informatica, Talend, Oracle, Data Warehousing, Data Integration, Data Manipulation, ProblemSolving, Communication, Teamwork, Commercial Lines","data analytics, data strategy, data pipelines, machine learning, oracle plsql, snowflake, python, r, informatica, talend, oracle, data warehousing, data integration, data manipulation, problemsolving, communication, teamwork, commercial lines","commercial lines, communication, data integration, data manipulation, data strategy, dataanalytics, datapipeline, datawarehouse, informatica, machine learning, oracle, oracle plsql, problemsolving, python, r, snowflake, talend, teamwork"
Lead Data Engineer,CVS Health,"Hartford, CT",https://www.linkedin.com/jobs/view/lead-data-engineer-at-cvs-health-3739964575,2023-12-17,Marion,United States,Mid senior,Onsite,"Bring your heart to CVS Health. Every one of us at CVS Health shares a single, clear purpose: Bringing our heart to every moment of your health. This purpose guides our commitment to deliver enhanced human-centric health care for a rapidly changing world. Anchored in our brand — with heart at its center — our purpose sends a personal message that how we deliver our services is just as important as what we deliver.
Our Heart At Work Behaviors™ support this purpose. We want everyone who works at CVS Health to feel empowered by the role they play in transforming our culture and accelerating our ability to innovate and deliver solutions to make health care more personal, convenient and affordable.
Position Summary
Join our fast-paced, innovative, and collaborative environment focused on providing an AIOps platform that enhances the intelligence of the CVS Health infrastructure. Work closely with subject matter experts and colleagues to build and scale out machine learning and AI solutions that will detect, predict, and recommend solutions to correct issues before system impact and enhance the efficiency, reliability and performance of CVS Health’s IT operations. As a lead data engineer, you will be responsible for designing, developing and maintaining the data pipelines, databases and systems required for efficient data processing, storage and retrieval. You will collaborate with data scientists, developers and other stakeholders in an agile team environment.
Key Responsibilities include:
Design, implement and manage data pipelines for extracting, transforming and loading data from various sources into data lakes for processing, analytics, and correlation.
Create and maintain data models ensuring data quality, and efficiency. Design solutions that can scale with growing data volumes and evolving business needs
Develop and automate processes to clean, transform and prepare data for analytics, ensuring data accuracy and consistency
Integrate data from disparate sources, both structured and unstructured to provide a unified view of key infrastructure platform and application data
Utilize big data technologies such as Kafka to process and analyze large volumes of data efficiently
Implement data security measures to protect sensitive information and ensure compliance with data and privacy regulation
Create/maintain documentation for data processes, data flows and system configurations
Monitor and optimize data pipelines and systems for performance, scalability and cost-effectiveness
Required Qualifications
7+years programming experience in languages such as Python, Java, SQL
5+ years experience with ETL tools and database management (relational, non-relational)
Proficiency in data modeling techniques and tools to design efficient scalable data structures
Skills in data quality assessment, data cleansing and data validation
Team Player: Mentor, share knowledge, and work with others to make the team successful. Provide guidance to others ensuring successful execution of projects.
Communication: Exceptional verbal, written, organizational, presentation, and communication skills.
Creativity: Ability to take written and verbal requirements and come up with other innovative ideas.
Attention to detail: Systematically and accurately research future solutions and current problems.
Strong work ethic: The innate drive to do work extremely well.
Passion: A drive to deliver better products and services than expected to customers.
Preferred Qualifications
Knowledge of big data technologies and cloud platforms
Experience with technologies like PySpark, Databricks, Azure Synapse, Delta Lake
Education
Bachelor’s degree in Computer Science, Information Technology or related filed, or equivalent working experience
Pay Range
The typical pay range for this role is:
$120,750.00 - $240,000.00
This pay range represents the base hourly rate or base annual full-time salary for all positions in the job grade within which this position falls. The actual base salary offer will depend on a variety of factors including experience, education, geography and other relevant factors. This position is eligible for a CVS Health bonus, commission or short-term incentive program in addition to the base pay range listed above. This position also includes an award target in the company’s equity award program.
In addition to your compensation, enjoy the rewards of an organization that puts our heart into caring for our colleagues and our communities. The Company offers a full range of medical, dental, and vision benefits. Eligible employees may enroll in the Company’s 401(k) retirement savings plan, and an Employee Stock Purchase Plan is also available for eligible employees. The Company provides a fully-paid term life insurance plan to eligible employees, and short-term and long term disability benefits. CVS Health also offers numerous well-being programs, education assistance, free development courses, a CVS store discount, and discount programs with participating partners. As for time off, Company employees enjoy Paid Time Off (“PTO”) or vacation pay, as well as paid holidays throughout the calendar year. Number of paid holidays, sick time and other time off are provided consistent with relevant state law and Company policies.
For more detailed information on available benefits, please visitjobs.CVSHealth.com/benefits
CVS Health requires certain colleagues to be fully vaccinated against COVID-19 (including any booster shots if required), where allowable under the law, unless they are approved for a reasonable accommodation based on disability, medical condition, religious belief, or other legally recognized reasons that prevents them from being vaccinated.
You are required to have received at least one COVID-19 shot prior to your first day of employment and to provide proof of your vaccination status or apply for a reasonable accommodation within the first 10 days of your employment. Please note that in some states and roles, you may be required to provide proof of full vaccination or an approved reasonable accommodation before you can begin to actively work.
CVS Health is committed to recruiting, hiring, developing, advancing, and retaining individuals with disabilities. As such, we strive to provide equal access to the benefits and privileges of employment, including the provision of a reasonable accommodation to perform essential job functions. CVS Health can provide a request for a reasonable accommodation, including a qualified interpreter, written information in other formats, translation or other services throughColleagueRelations@CVSHealth.comIf you have a speech or hearing disability, please call 7-1-1 to utilize Telecommunications Relay Services (TRS). We will make every effort to respond to your request within 48 business hours and do everything we can to work towards a solution.
Show more
Show less","Python, Java, SQL, ETL tools, Data modeling, Data quality assessment, Data cleansing, Data validation, Big data technologies, Cloud platforms, PySpark, Databricks, Azure Synapse, Delta Lake, Data pipelines, Data lakes, Analytics, Correlation, Data models, Scalable data structures, Data security","python, java, sql, etl tools, data modeling, data quality assessment, data cleansing, data validation, big data technologies, cloud platforms, pyspark, databricks, azure synapse, delta lake, data pipelines, data lakes, analytics, correlation, data models, scalable data structures, data security","analytics, azure synapse, big data technologies, cloud platforms, correlation, data lakes, data models, data quality assessment, data security, data validation, databricks, datacleaning, datamodeling, datapipeline, delta lake, etl tools, java, python, scalable data structures, spark, sql"
MDM Data Engineer,Jobs for Humanity,"Hartford, CT",https://www.linkedin.com/jobs/view/mdm-data-engineer-at-jobs-for-humanity-3786352598,2023-12-17,Marion,United States,Mid senior,Onsite,"Company Description
Jobs for Humanity is partnering with MassMutual to build an inclusive and just employment ecosystem. Therefore, we prioritize individuals coming from the following communities: Refugee, Neurodivergent, Single Parent, Blind or Low Vision, Deaf or Hard of Hearing, Black, Hispanic, Asian, Military Veterans, the Elderly, the LGBTQ, and Justice Impacted individuals. This position is open to candidates who reside in and have the legal right to work in the country where the job is located.
Company Name: MassMutual
Job Description
Job Title: Data Engineer - Informatica Power Center and MDM
At MassMutual, our mission is to help people secure their future and protect the ones they love. We believe in an inclusive and diverse workplace where everyone can thrive, regardless of their age, background, or abilities. We are currently seeking a Data Engineer with expertise in Informatica Power Center and MDM to join our team.
Objectives of the Role:
Design, build, and measure complex Informatica Power Center and MDM processes to master data for different Master Data domains within MassMutual.
Work on a range of projects including batch pipelines, data modeling, and Master data solutions, collaborating with project teams to implement robust data collection and processing pipelines that meet specific business needs.
Daily and Monthly Responsibilities:
Design, construct, and evaluate intricate ELT/ETL tasks to assimilate data from diverse sources into a high-quality, reliable data repository.
Develop Master Data Management (MDM) solutions for various master data domains within MassMutual.
Provide comprehensive support for MDM solutions, including data ingestion, master record creation, outbound data extracts, and API integration.
Implement data modeling policies, procedures, processes, and standards, while actively contributing feedback.
Assist in recording and documenting essential technical data, system flows, database design, and other relevant system information.
Establish data quality benchmarks and develop tools/processes to ensure data accuracy.
Conduct data profiling and perform detailed analysis of source system data.
Collaborate with various departments to understand emerging data patterns.
Convert high-level business needs into precise technical specifications.
Actively engage in the development and rollout of MDM Solutions and other Information Management initiatives at MassMutual.
Basic Qualifications:
Bachelor’s degree in computer science or engineering.
5+ years of experience with Informatica Power Center.
5+ years of experience with data analytics, data modeling, and database design.
3+ years of coding and scripting (Python, Java, Scala) and design experience.
3+ years of experience with Informatica MDM platform in the customer/party subject area.
Experience with ELT methodologies and tools.
Expertise in tuning and troubleshooting SQL.
Strong data integrity, analytical, and multitasking skills.
Experience with Oracle database.
Experience with AWS.
Knowledge of basic UNIX commands and shell scripts.
Experience with 3rd party job schedulers like Maestro.
Experience with RESTful APIs.
Experience with near real-time mastering via SIF.
Experience with data profiling tools.
Willingness to provide Level 2 support for batch processing cycles outside business hours.
Excellent communication, problem-solving, organizational, and analytical skills.
Able to work independently.
Authorized to work in the USA with or without sponsorship.
Preferred Qualifications:
Master’s degree in computer science or engineering.
Familiarity with agile project delivery processes.
Knowledge of SQL and its use in data access and analysis.
Ability to manage diverse projects impacting multiple roles and processes.
Able to troubleshoot problem areas and identify data gaps and issues.
Ability to adapt to a fast-changing environment.
Experience with Python.
Experience with Kafka.
Basic knowledge of database technologies (Vertica, Redshift, etc.).
Experience designing and implementing automated ETL processes.
MassMutual is an Equal Employment Opportunity employer, striving to create an inclusive and diverse workplace. We welcome applicants from all backgrounds, including minorities, females, individuals with disabilities, veterans, and those from LGBTQIA+ communities. Veterans are encouraged to apply, regardless of their discharge status.
If you require any accommodations to complete the application process, please contact us and let us know the specific assistance you need.
Show more
Show less","Informatica Power Center, Informatica MDM, Hadoop, Python, Java, Scala, Cassandra, SQL, Oracle, AWS, Redshift, Vertica, SIF, UNIX, shell scripts, Maestro, RESTful APIs, ETL, ELT, Data modeling, Data profiling, Agile, Kafka","informatica power center, informatica mdm, hadoop, python, java, scala, cassandra, sql, oracle, aws, redshift, vertica, sif, unix, shell scripts, maestro, restful apis, etl, elt, data modeling, data profiling, agile, kafka","agile, aws, cassandra, data profiling, datamodeling, elt, etl, hadoop, informatica mdm, informatica power center, java, kafka, maestro, oracle, python, redshift, restful apis, scala, shell scripts, sif, sql, unix, vertica"
Senior Lead Data Engineer,Jobs for Humanity,"Hartford, CT",https://www.linkedin.com/jobs/view/senior-lead-data-engineer-at-jobs-for-humanity-3768297411,2023-12-17,Marion,United States,Mid senior,Onsite,"Company Description
Jobs for Humanity is partnering with Capital One to build an inclusive and just employment ecosystem. Therefore, we prioritize individuals coming from the following communities: Refugee, Neurodivergent, Single Parent, Blind or Low Vision, Deaf or Hard of Hearing, Black, Hispanic, Asian, Military Veterans, the Elderly, the LGBTQ, and Justice Impacted individuals. This position is open to candidates who reside in and have the legal right to work in the country where the job is located.
Company Name: Capital One
Job Description
Senior Lead Data Engineer - Job Opportunity Are you passionate about technology and solving complex business problems? Do you thrive in a collaborative and inclusive work environment? At Capital One, we are a diverse and innovative group of individuals who are dedicated to making a positive impact. We are currently looking for Data Engineers who are excited about using data and emerging technologies to drive transformation. As a Senior Lead Data Engineer at Capital One, you will play a crucial role in shaping the future of our organization. Team Info: In our Finance Tech team, we are working towards creating a Finance Ecosystem that connects different capabilities using a common design pattern. We are looking for a Senior Lead Data Engineer to join us in delivering exciting new data engineering capabilities across multiple teams. What You’ll Do: - Collaborate with Agile teams to design, develop, test, implement, and support technical solutions using full-stack development tools and technologies. - Lead a team of developers with expertise in machine learning, distributed microservices, and full stack systems. - Utilize programming languages like Java, Scala, Python, and open-source databases, as well as cloud-based data warehousing services. - Stay up-to-date with the latest tech trends, experiment with new technologies, and actively participate in technology communities to share knowledge with other engineers. - Collaborate with digital product managers to deliver cloud-based solutions that empower millions of Americans in achieving financial stability. - Conduct unit tests and code reviews to ensure high code quality, performance, and scalability. Basic Qualifications: - Bachelor's Degree. - At least 8 years of experience in application development, excluding internship experience. - At least 2 years of experience in big data technologies. - At least 1 year of experience with cloud computing platforms (AWS, Microsoft Azure, Google Cloud). Preferred Qualifications: - Master's Degree. - 9+ years of experience in application development with languages like Python, SQL, Scala, or Java. - 4+ years of experience with public cloud platforms (AWS, Microsoft Azure, Google Cloud). - 5+ years of experience with distributed data and computing tools (MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, MySQL). - 4+ years of experience working on real-time data and streaming applications. - 4+ years of experience with NoSQL databases (MongoDB, Cassandra). - 4+ years of experience with data warehousing (Redshift, Snowflake). - 4+ years of experience with UNIX or Linux, including basic commands and shell scripting. - 2+ years of experience with Agile engineering practices. At Capital One, we are committed to inclusivity and diversity in the workplace. We provide comprehensive benefits that support your overall well-being. Our website offers more information on our benefits package. Eligibility varies based on employment status. To apply, please visit our website and apply directly. We do not accept applications through agencies. If you require any accommodations during the application process, please contact Capital One Recruiting at 1-800-304-9102 or email RecruitingAccommodation@capitalone.com. All information provided will be kept confidential and used only for the purpose of providing necessary accommodations. For technical support or questions about the recruiting process, please email Careers@capitalone.com. Note: Capital One Financial encompasses multiple entities. Positions in Canada are for Capital One Canada, positions in the UK are for Capital One Europe, and positions in the Philippines are for Capital One Philippines Service Corp. (COPSSC).
Show more
Show less","Data engineering, Fullstack development, Machine learning, Microservices, Agile, Python, Java, Scala, SQL, AWS, Azure, Hadoop, Spark, MongoDB, Cassandra, Redshift, Snowflake, UNIX, Linux, Unit testing, Code review","data engineering, fullstack development, machine learning, microservices, agile, python, java, scala, sql, aws, azure, hadoop, spark, mongodb, cassandra, redshift, snowflake, unix, linux, unit testing, code review","agile, aws, azure, cassandra, code review, data engineering, fullstack development, hadoop, java, linux, machine learning, microservices, mongodb, python, redshift, scala, snowflake, spark, sql, unit testing, unix"
Data Engineer - Mastery,Jobs for Humanity,"Hartford, CT",https://www.linkedin.com/jobs/view/data-engineer-mastery-at-jobs-for-humanity-3786352565,2023-12-17,Marion,United States,Mid senior,Onsite,"Company Description
Jobs for Humanity is partnering with MassMutual to build an inclusive and just employment ecosystem. Therefore, we prioritize individuals coming from the following communities: Refugee, Neurodivergent, Single Parent, Blind or Low Vision, Deaf or Hard of Hearing, Black, Hispanic, Asian, Military Veterans, the Elderly, the LGBTQ, and Justice Impacted individuals. This position is open to candidates who reside in and have the legal right to work in the country where the job is located.
Company Name: MassMutual
Job Description
At MassMutual, our mission is to help people protect their families, support their communities, and assist one another. We believe in inspiring individuals to Live Mutual, to collaborate and uplift each other. We are committed to creating a diverse and inclusive workforce that values and respects people from all backgrounds. We are currently seeking a Data Engineer who is passionate about data and enjoys building data projects. In this role, you will work closely with business partners to understand requirements and deliver robust solutions that meet the highest standards. You will also have the opportunity to learn new technologies and work in the cloud. A strong team player and communicator, you will collaborate with other developers and provide mentorship. Key Responsibilities: - Design, build, and maintain complex ELT jobs that deliver business value - Translate high-level business requirements into technical specifications - Ingest data from different sources into the data lake and data warehouse - Cleanse and enrich data, applying data quality controls - Provide insights and guidance to shape the future development of MassMutual's data platform - Develop reusable tools to streamline the delivery of new projects - Collaborate closely with other developers and provide mentorship - Evaluate and recommend tools, technologies, processes, and reference architectures - Work in an Agile development environment, attending daily stand-up meetings and delivering incremental improvements Basic Qualifications: - Bachelor's degree in computer science, engineering, or related field - 8+ years of experience in data analytics and warehousing - Deep knowledge of SQL and query optimization - Good understanding of ELT methodologies and tools - Experience with troubleshooting and root cause analysis - Strong communication, problem-solving, organizational, and analytical skills - Ability to work independently and provide leadership to small development teams Preferred Qualifications: - Master's degree in computer science, engineering, or related field - Experience working in a cloud environment (e.g., AWS) - Hands-on experience developing with Python - Experience with data processing technologies such as Apache Spark or Kafka - Knowledge of orchestration and scheduling tools (e.g., Apache Airflow) - Experience with data reporting and data cataloging tools (e.g., Microstrategy, Tableau, Looker, Alation) MassMutual is an equal opportunity employer committed to fostering an inclusive and diverse workforce. We welcome applications from all individuals, including minorities, females, individuals with disabilities, veterans, and members of the LGBTQIA+ community. If you require accommodations during the application process, please contact us and let us know how we can assist you.
Show more
Show less","Apache Spark, SQL, Apache Airflow, ETL, Data analytics, Data warehousing, AWS, Python, ELT methodologies, Troubleshooting, Root cause analysis, Data quality controls, Cloud computing, Apache Kafka, Microstrategy, Tableau, Looker, Alation","apache spark, sql, apache airflow, etl, data analytics, data warehousing, aws, python, elt methodologies, troubleshooting, root cause analysis, data quality controls, cloud computing, apache kafka, microstrategy, tableau, looker, alation","alation, apache airflow, apache kafka, apache spark, aws, cloud computing, data quality controls, dataanalytics, datawarehouse, elt methodologies, etl, looker, microstrategy, python, root cause analysis, sql, tableau, troubleshooting"
Sr. Data Engineer- Customer Analytics (Hybrid),Selective Insurance,"Glastonbury, CT",https://www.linkedin.com/jobs/view/sr-data-engineer-customer-analytics-hybrid-at-selective-insurance-3747072801,2023-12-17,Marion,United States,Mid senior,Onsite,"At Selective, we don't just insure uniquely, we employ uniqueness.
Our Business
Selective is a midsized U.S. domestic property and casualty insurance company with a history of strong, consistent financial performance for nearly 100 years. Selective's unique position as both a leading insurance group and an employer of choice is recognized in a wide variety of awards and honors, including listing in Forbes Best Midsize Employers in 2023 and certification as a Great Place to Work® in 2023.
Working at Selective
At Selective, we don't just insure uniquely – we employ uniqueness. Employees are empowered and encouraged to
Be Uniquely You
by being their true, unique selves and contributing their diverse talents, experiences, and perspectives to our shared success. Together, we are a high-performing team working to serve our customers responsibly by helping to mitigate loss, keep them safe, and restore their lives and businesses after an insured loss occurs. Employees receive comprehensive total rewards packages - including competitive compensation and performance awards, health benefits, and retirement savings - and professional development opportunities and flexible schedules to support their health, wealth, and well-being. Join our team and help make a difference.
Overview
Selective Insurance is seeking an energetic and collaborative Sr. Data Engineer to work on data and analytics projects supporting the Customer Analytics team within the Information Management group. This group is responsible for technology support of all Data Engineering, Analytics and Reporting for the Marketing, Customer Experience and Contact Center business areas. This includes Data Engineering services, Enterprise reporting support and ML Ops Engineering operations for these groups. The candidate must be hands on with very good technical skills and a proven track record of project delivery.
Responsibilities
Hands on development and support of new or existing data applications.
Work closely with business and analysts to understand data and business process and make recommendations to clients as requested on best practices or long-term solutions to resolve current issues and also for future system design
Works closely with Application and Enterprise Architects to create/review low level implementation designs, understand high level data flow designs developed by data architects.
Provide technical guidance to the team for implementing complex data solutions.
Provide support in the design, development, code reviews, test deploy and documentation of data engineering and data integration Applications.
Maintain detailed documentation to support downstream integrations
Provide support for production issues
Performs activities of a scrum master
Identify technology trends and explore opportunities for use within the organization
Qualifications
Five to seven years of experience in Data Warehousing, Data integration or Data Engineering projects
Ability to effectively work well with people in other departments and/or outside of the enterprise.
Proficient in SQL.
Experience working within Azure ecosystem
Experience in Informatica Powercenter, IICS, Cognos, Netezza Performance servers
Experienced in any of these analytical platforms - PowerBI, AzureML, Databricks or Synapse
Experience using Python or Scala.
Experience in Azure DevOps and Github is preferred
P&C Insurance experience is preferred
Possesses excellent communication skills.
Bachelor’s degree in computer science or related engineering field preferred.
Salary range: $90,600 - $149,600. The actual base salary is based on geographic location, and the range is representative of salaries for this role throughout Selective's footprint. Additional considerations include the candidate's qualifications and experience.
Selective is an Equal Employment Opportunity employer. That means we respect and value every individual’s unique opinions, beliefs, abilities, and perspectives. We are committed to promoting a welcoming culture that celebrates diverse talent, individual identity, different points of view and experiences – and empowers employees to contribute new ideas that support our continued and growing success. Building a highly engaged team is one of our core strategic imperatives, which we believe is enhanced by diversity, equity, and inclusion. We expect and encourage all employees and all of our business partners to embrace, practice, and monitor the attitudes, values, and goals of acceptance; address biases; and foster diversity of viewpoints and opinions.
Selective maintains a drug-free workplace
.
Show more
Show less","Data Engineering, Data Warehousing, Data Integration, SQL, Azure Ecosystem, Informatica Powercenter, IICS, Cognos, Netezza Performance servers, PowerBI, AzureML, Databricks, Synapse, Python, Scala, Azure DevOps, Github, P&C Insurance, Scrum master","data engineering, data warehousing, data integration, sql, azure ecosystem, informatica powercenter, iics, cognos, netezza performance servers, powerbi, azureml, databricks, synapse, python, scala, azure devops, github, pc insurance, scrum master","azure devops, azure ecosystem, azureml, cognos, data engineering, data integration, databricks, datawarehouse, github, iics, informatica powercenter, netezza performance servers, pc insurance, powerbi, python, scala, scrum master, sql, synapse"
Senior Data Engineer,CVS Health,"Hartford, CT",https://www.linkedin.com/jobs/view/senior-data-engineer-at-cvs-health-3777366845,2023-12-17,Marion,United States,Mid senior,Onsite,"Bring your heart to CVS Health. Every one of us at CVS Health shares a single, clear purpose: Bringing our heart to every moment of your health. This purpose guides our commitment to deliver enhanced human-centric health care for a rapidly changing world. Anchored in our brand — with heart at its center — our purpose sends a personal message that how we deliver our services is just as important as what we deliver.
Our Heart At Work Behaviors™ support this purpose. We want everyone who works at CVS Health to feel empowered by the role they play in transforming our culture and accelerating our ability to innovate and deliver solutions to make health care more personal, convenient and affordable.
Position Summary
Supports the creation and operation of Big Data AI/machine learning solutions, using complex healthcare data to produce actionable insights
Leads and participates in the design, build and management of large-scale data structures and pipelines and efficient Extract/Load/Transform (ETL) workflows.
Uses strong data analysis skills to profile and validate source datasets to recommend solutions.
Collaborates with data science team to define and build datasets for training of analytic data models.
Leverages data management experience to design large scale data structures that support efficient consumption of analytic results.
Uses strong programming skills in Python, Java or any of the major languages to develop efficient ETL to collect and standardize data to generate insights and addresses reporting needs.
Develops software to deliver both batch and real time analytics
Required Qualifications
5+ years of relevant work experience in Data Engineering/ETL Development
5+ years of SQL experience with large datasets
2+ years’ experience with Spark, Python, or Java to build robust data pipelines
Experience leveraging Cloud Technologies (GCP Preferred, AWS, Azure) for ETL and data analysis
5 or more years of progressively complex related experience.
Preferred Qualifications
Strong SQL and data analysis experience; data exploration, profiling, and validation.
Strong collaboration and communication skills within and across teams.
Uses knowledge in Hadoop architecture, HDFS commands and experience designing& optimizing queries to build data pipelines.
Experience working with Healthcare or Health insurance data
Understanding of data science methods and statistics
Education
Bachelor's degree or equivalent work experience in Computer Science,
Engineering, Machine Learning, or related discipline.
Master’s degree or PhD preferred.
Pay Range
The typical pay range for this role is:
$94,500.00 - $196,000.00
This pay range represents the base hourly rate or base annual full-time salary for all positions in the job grade within which this position falls. The actual base salary offer will depend on a variety of factors including experience, education, geography and other relevant factors. This position is eligible for a CVS Health bonus, commission or short-term incentive program in addition to the base pay range listed above.
In addition to your compensation, enjoy the rewards of an organization that puts our heart into caring for our colleagues and our communities. The Company offers a full range of medical, dental, and vision benefits. Eligible employees may enroll in the Company’s 401(k) retirement savings plan, and an Employee Stock Purchase Plan is also available for eligible employees. The Company provides a fully-paid term life insurance plan to eligible employees, and short-term and long term disability benefits. CVS Health also offers numerous well-being programs, education assistance, free development courses, a CVS store discount, and discount programs with participating partners. As for time off, Company employees enjoy Paid Time Off (“PTO”) or vacation pay, as well as paid holidays throughout the calendar year. Number of paid holidays, sick time and other time off are provided consistent with relevant state law and Companypolicies.
For more detailed information on available benefits, please visitjobs.CVSHealth.com/benefits
CVS Health requires certain colleagues to be fully vaccinated against COVID-19 (including any booster shots if required), where allowable under the law, unless they are approved for a reasonable accommodation based on disability, medical condition, religious belief, or other legally recognized reasons that prevents them from being vaccinated.
You are required to have received at least one COVID-19 shot prior to your first day of employment and to provide proof of your vaccination status or apply for a reasonable accommodation within the first 10 days of your employment. Please note that in some states and roles, you may be required to provide proof of full vaccination or an approved reasonable accommodation before you can begin to actively work.
CVS Health is committed to recruiting, hiring, developing, advancing, and retaining individuals with disabilities. As such, we strive to provide equal access to the benefits and privileges of employment, including the provision of a reasonable accommodation to perform essential job functions. CVS Health can provide a request for a reasonable accommodation, including a qualified interpreter, written information in other formats, translation or other services throughColleagueRelations@CVSHealth.comIf you have a speech or hearing disability, please call 7-1-1 to utilize Telecommunications Relay Services (TRS). We will make every effort to respond to your request within 48 business hours and do everything we can to work towards a solution.
Show more
Show less","Big Data, AI, Machine Learning, Data Engineering, ETL, SQL, Spark, Python, Java, Cloud Technologies, Hadoop, HDFS, Data Science, Statistics, Data Analysis, Data Exploration, Data Profiling, Data Validation","big data, ai, machine learning, data engineering, etl, sql, spark, python, java, cloud technologies, hadoop, hdfs, data science, statistics, data analysis, data exploration, data profiling, data validation","ai, big data, cloud technologies, data engineering, data exploration, data profiling, data science, data validation, dataanalytics, etl, hadoop, hdfs, java, machine learning, python, spark, sql, statistics"
"Software Data Engineer, Java",Jobs for Humanity,"Hartford, CT",https://www.linkedin.com/jobs/view/software-data-engineer-java-at-jobs-for-humanity-3786351836,2023-12-17,Marion,United States,Mid senior,Onsite,"Company Description
Jobs for Humanity is partnering with MassMutual to build an inclusive and just employment ecosystem. Therefore, we prioritize individuals coming from the following communities: Refugee, Neurodivergent, Single Parent, Blind or Low Vision, Deaf or Hard of Hearing, Black, Hispanic, Asian, Military Veterans, the Elderly, the LGBTQ, and Justice Impacted individuals. This position is open to candidates who reside in and have the legal right to work in the country where the job is located.
Company Name: MassMutual
Job Description
Job Description
At MassMutual, we have been committed to helping people protect their families, support their communities, and help one another since 1851. We want to inspire people to Live Mutual. Our goal is to create a diverse and inclusive work environment where all voices are heard, and everyone can thrive. We invite you to join us in making a positive impact on millions of Americans as we support and rely on each other.
Together, we are stronger.
What you'll do in this role
Design, develop, and deliver complex systems using technologies like Python, Java, AWS serverless, Apache Spark, Apache Kafka, and REST
Collaborate closely with data engineers and data scientists to understand and refine requirements
Write clean, efficient code and conduct code reviews
Debug and troubleshoot code and data pipelines
Evaluate and recommend tools, technologies, and processes for improvement
Collaborate with other developers and provide mentorship as needed
Work in an Agile development environment and deliver incremental improvements
Basic Qualifications
4+ years of development experience in Java
Strong understanding of algorithms, design patterns, and writing efficient code
Good knowledge of data processing tools like Spark, Kafka, and SQL, as well as relational and analytics databases
Experience with source control and CI/CD tools
Proficiency in writing unit, integration, and load tests
Excellent communication, problem-solving, and organizational skills
Able to work independently and provide leadership when necessary
Bachelor’s degree or equivalent work experience
Preferred Qualifications
Experience with cloud platforms like AWS and serverless architectures
2+ years of experience with big data and/or streaming technologies like Spark, Kafka, and Flink
MassMutual is an equal opportunity employer. We welcome applications from individuals of all backgrounds, including minorities, females, individuals with disabilities, LGBTQIA+ individuals, veterans, and refugees. Note: Veterans are welcome to apply, regardless of their discharge status.
If you need accommodations to complete the application process, please contact us and let us know how we can assist you.
Show more
Show less","Python, Java, AWS Serverless, Apache Spark, Apache Kafka, REST, Data engineering, Data science, Agile development, Algorithms, Design patterns, Unit testing, Integration testing, Load testing, Communication, Problemsolving, Organizational skills, Leadership, Cloud platforms, Big data, Streaming technologies","python, java, aws serverless, apache spark, apache kafka, rest, data engineering, data science, agile development, algorithms, design patterns, unit testing, integration testing, load testing, communication, problemsolving, organizational skills, leadership, cloud platforms, big data, streaming technologies","agile development, algorithms, apache kafka, apache spark, aws serverless, big data, cloud platforms, communication, data engineering, data science, design patterns, integration testing, java, leadership, load testing, organizational skills, problemsolving, python, rest, streaming technologies, unit testing"
Senior Data Movement Engineer,Nesco Resource,"New Haven, CT",https://www.linkedin.com/jobs/view/senior-data-movement-engineer-at-nesco-resource-3783918195,2023-12-17,Marion,United States,Mid senior,Onsite,"We are currently seeking a Senior Data Engineer who can assist us with modernizing our current ETL environment, as well as begin building new ETL/ELT workflows and data pipelines as we move towards a cloud environment. The Data Movement Engineer will support our data architects, data analysts, and business intelligence developers on various data initiatives. They must be self-directed and comfortable supporting the data requirements of multiple teams and systems. The ideal candidate will possess experience in the data management domain, and be willing to contribute their technical skills and experience towards the development of elegant and efficient data processes.
Core Responsibilities
Complete full life cycle of ETL/ELT development to address business needs or resolve issues including design, mappings, data transformations, scheduling and testing.
Translate data movement requirements into technical ETL design.
Develop ETL/ELT workflows and mappings to extract, transform, and load data into target environments.
Develop data extraction and transmissions to external vendors.
Develop test plans and perform unit testing.
Create supporting documentation for new ETL processes.
Work closely with data analysts to gain understanding of business processes and corporate data.
Determine impacts to data warehouse structures and information flows due to changes in business or technical requirements.
Contribute to architectural decisions to support business processes.
Provide production support for data solutions.
Complete root cause analysis and contribute to remediation planning and implementation.
Perform data quality analysis, report data quality issues and propose solutions for data quality management.
Learn and expand upon internal controls and participate in customer support.
Perform design and code reviews in support of organizational standards.
Mentor less experienced team members.
Prepare effort estimation including researching and estimating costs of software development, unit testing. May provide estimates for upgrades of vendor packages upgrades and integration with existing systems.
On Call and/or after hours work required.
Required
Minimum of 8 years of relevant experience in data warehousing, business intelligence tools, and the analysis of data
Minimum of 5 years of SQL query development, preferably across multiple database management platforms, and working with dimensional and star schema data warehouse implementations
Proficiency in ETL/ELT concepts and tools (Informatica IICS, Talend, Boomi and PowerCenter preferred)
Working knowledge of data warehousing concepts, structures and ETL best practices
Experience using query tools (e.g.. AQT, MS Query)
Ability to problem solve using analytical thinking skills
Must work well independently - must be inquisitive and seek answers to complex questions without being prompted
Strong organizational and time management skills
Strong communication skills including verbal and written to communicate effectively with clients and management
Strong project management skills to ensure that projects get done on time and within budget
Effectively participates in teams and moves the team toward completion of goals
Ability to resolve conflicts
Nesco Resource provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state, or local laws.
Show more
Show less","Data Warehousing, ETL/ELT, Data Management, Data Extraction, Data Transformation, Data Loading, SQL, Informatica IICS, Talend, Boomi, PowerCenter, Dimensional Modeling, Star Schema, Query Tools, Analytical Thinking, Organizational Skills, Time Management, Communication Skills, Project Management Skills, Team Player, Conflict Resolution","data warehousing, etlelt, data management, data extraction, data transformation, data loading, sql, informatica iics, talend, boomi, powercenter, dimensional modeling, star schema, query tools, analytical thinking, organizational skills, time management, communication skills, project management skills, team player, conflict resolution","analytical thinking, boomi, communication skills, conflict resolution, data extraction, data loading, data management, data transformation, datawarehouse, dimensional modeling, etlelt, informatica iics, organizational skills, powercenter, project management skills, query tools, sql, star schema, talend, team player, time management"
Business Data Analyst,Lorven Technologies Inc.,"Hartford, CT",https://www.linkedin.com/jobs/view/business-data-analyst-at-lorven-technologies-inc-3692179905,2023-12-17,Marion,United States,Mid senior,Onsite,"8+ Years of Experience in Business Data Analysis/ Data Analysis with 3+ years of experience in insurance
Preferred, Good expereince in Data Governance and Data Quality work
Should be able to write DQ rules
Good working experience in Lineage and Metadata management.
Effectively communicate functional requirements to development team.
Serve as liaison between development staff and user community during development, testing and implementation stages as well as for on-going modifications;
Develop significant application and related business requirements expertise.
Show more
Show less","Data Analysis, Data Governance, Data Quality, DQ rules, Lineage management, Metadata management, Business requirements analysis, Liaison between development and user community, Application development","data analysis, data governance, data quality, dq rules, lineage management, metadata management, business requirements analysis, liaison between development and user community, application development","application development, business requirements analysis, data governance, data quality, dataanalytics, dq rules, liaison between development and user community, lineage management, metadata management"
Data Engineer - Scala(U.S. remote),Railroad19,"Waterbury, CT",https://www.linkedin.com/jobs/view/data-engineer-scala-u-s-remote-at-railroad19-3782879876,2023-12-17,Marion,United States,Mid senior,Remote,"Railroad19, Inc.
is hiring
remote
Senior Data Engineers
to be a solid technical resource on a dynamic and growing team of accomplished engineers.
Our ideal candidate is passionate about creating well-architected solutions containing thoroughly tested code. The ability to communicate effectively and develop relationships by empathizing with client goals is a highly valued skill within our company culture.
Core Responsibilities:
Develop new and enhance existing application services
Writing tests to maintain code quality
Understand and adapt to our client's evolving business requirements within the television advertising domain.
Participate in detailed technical design sessions to understand client needs and provide productive feedback
Identify new opportunities, tools, and services to enhance the software platform
Support and troubleshoot issues, identify the root cause, and proactively recommend corrective actions
Skills & Experience:
Scala 2.12 + development experience
Passionate about developing clean and maintainable code with little or no side-effects
Experience building Restful APIs in Scala using Spark 2.4
Strong hands-on experience with AWS running spark jobs on ephemeral EMR clusters on AWS and S3.
Experience with relational and non-relational databases
Willingness to learn new technologies and takes pride in keeping up with the latest technologies and practices within the Scala and Spark development community
Excellent oral and written communication skills
Strong analytical and problem-solving skills
Self-directed and can effectively deliver solutions with little oversight
A bachelor's or master's degree in computer science, computer engineering, or other technical disciplines or equivalent work experience is preferred but not required.
$120,000 - $160,000 a year
Salary is commensurate with experience.
We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender, gender identity or expression, or veteran status. We are proud to be an equal-opportunity workplace.
#Hiringnow
We are actively hiring (Data Engineers)
#remote #Scala #DataEngineering #ApacheSpark
Show more
Show less","Scala, Spark, AWS, EMR, S3, Relational databases, Nonrelational databases, Restful APIs, AWS, EMR, Hadoop, Data Engineering, Apache Spark, Restful APIs","scala, spark, aws, emr, s3, relational databases, nonrelational databases, restful apis, aws, emr, hadoop, data engineering, apache spark, restful apis","apache spark, aws, data engineering, emr, hadoop, nonrelational databases, relational databases, restful apis, s3, scala, spark"
Data Engineer - Scala(U.S. remote),Railroad19,"New Haven, CT",https://www.linkedin.com/jobs/view/data-engineer-scala-u-s-remote-at-railroad19-3782883181,2023-12-17,Marion,United States,Mid senior,Remote,"Railroad19, Inc.
is hiring
remote
Senior Data Engineers
to be a solid technical resource on a dynamic and growing team of accomplished engineers.
Our ideal candidate is passionate about creating well-architected solutions containing thoroughly tested code. The ability to communicate effectively and develop relationships by empathizing with client goals is a highly valued skill within our company culture.
Core Responsibilities:
Develop new and enhance existing application services
Writing tests to maintain code quality
Understand and adapt to our client's evolving business requirements within the television advertising domain.
Participate in detailed technical design sessions to understand client needs and provide productive feedback
Identify new opportunities, tools, and services to enhance the software platform
Support and troubleshoot issues, identify the root cause, and proactively recommend corrective actions
Skills & Experience:
Scala 2.12 + development experience
Passionate about developing clean and maintainable code with little or no side-effects
Experience building Restful APIs in Scala using Spark 2.4
Strong hands-on experience with AWS running spark jobs on ephemeral EMR clusters on AWS and S3.
Experience with relational and non-relational databases
Willingness to learn new technologies and takes pride in keeping up with the latest technologies and practices within the Scala and Spark development community
Excellent oral and written communication skills
Strong analytical and problem-solving skills
Self-directed and can effectively deliver solutions with little oversight
A bachelor's or master's degree in computer science, computer engineering, or other technical disciplines or equivalent work experience is preferred but not required.
$120,000 - $160,000 a year
Salary is commensurate with experience.
We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender, gender identity or expression, or veteran status. We are proud to be an equal-opportunity workplace.
#Hiringnow
We are actively hiring (Data Engineers)
#remote #Scala #DataEngineering #ApacheSpark
Show more
Show less","Scala, Spark, AWS, EMR, S3, Restful APIs, SQL, NoSQL","scala, spark, aws, emr, s3, restful apis, sql, nosql","aws, emr, nosql, restful apis, s3, scala, spark, sql"
Data Engineer - Scala(U.S. remote),Railroad19,"Hartford, CT",https://www.linkedin.com/jobs/view/data-engineer-scala-u-s-remote-at-railroad19-3782882531,2023-12-17,Marion,United States,Mid senior,Remote,"Railroad19, Inc.
is hiring
remote
Senior Data Engineers
to be a solid technical resource on a dynamic and growing team of accomplished engineers.
Our ideal candidate is passionate about creating well-architected solutions containing thoroughly tested code. The ability to communicate effectively and develop relationships by empathizing with client goals is a highly valued skill within our company culture.
Core Responsibilities:
Develop new and enhance existing application services
Writing tests to maintain code quality
Understand and adapt to our client's evolving business requirements within the television advertising domain.
Participate in detailed technical design sessions to understand client needs and provide productive feedback
Identify new opportunities, tools, and services to enhance the software platform
Support and troubleshoot issues, identify the root cause, and proactively recommend corrective actions
Skills & Experience:
Scala 2.12 + development experience
Passionate about developing clean and maintainable code with little or no side-effects
Experience building Restful APIs in Scala using Spark 2.4
Strong hands-on experience with AWS running spark jobs on ephemeral EMR clusters on AWS and S3.
Experience with relational and non-relational databases
Willingness to learn new technologies and takes pride in keeping up with the latest technologies and practices within the Scala and Spark development community
Excellent oral and written communication skills
Strong analytical and problem-solving skills
Self-directed and can effectively deliver solutions with little oversight
A bachelor's or master's degree in computer science, computer engineering, or other technical disciplines or equivalent work experience is preferred but not required.
$120,000 - $160,000 a year
Salary is commensurate with experience.
We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender, gender identity or expression, or veteran status. We are proud to be an equal-opportunity workplace.
#Hiringnow
We are actively hiring (Data Engineers)
#remote #Scala #DataEngineering #ApacheSpark
Show more
Show less","Scala 2.12, Spark 2.4, AWS, EMR, S3, Restful API development, Relational databases, Nonrelational databases","scala 212, spark 24, aws, emr, s3, restful api development, relational databases, nonrelational databases","aws, emr, nonrelational databases, relational databases, restful api development, s3, scala 212, spark 24"
Senior Data Movement Engineer,Dice,"New Haven, CT",https://www.linkedin.com/jobs/view/senior-data-movement-engineer-at-dice-3787359838,2023-12-17,Marion,United States,Mid senior,Remote,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, GSS Infotech, is seeking the following. Apply via Dice today!
Senior Data Movement Engineer
New Haven, CT (Hybrid)
Key Job Responsibilities.
Experience in Data Engineering in AWS/Azure Cloud
Experience with API data integration
Experience with Talend or other data movement tools Informatica, Azure Data Factory
Snowflake or other relational database experience such as Oracle or SQL Server
CI/CD Experience either using Git or Azure Dev Ops
Team Lead responsibility Looking for the resource to become the overall Data Engineering lead
If the candidate is in CT, they have to come in for meetings. But if they are not it would be fully remote
Strong data engineer, have solid cloud experience
Ideally they would like the resource to have talend exp
MUST have API experience
They would lead data engineering space
They need to have good understanding of the best practices of data engineering
Need to have exp in snowflake preferred
Relational database exp needed – SQL
Needs someone who is motivated self-starter
Do peer reviews, oversee developers – they have 3 developers on the team and also they would be working with SI partner as well
Need someone who would be a leader and be a mentor
POWER BI would be PLUS
STRONG SQL, has development in API exp, solid cloud exp, need to understand data warehousing well, talend would be a plus
It would be a 30-60 days c2hire if they like the individual
Ideally like to have someone in east coast business timing
They are not looking for order takers
2 rounds of interview
Core Responsibilities
Complete full life cycle of ETL/ELT development to address business needs or resolve issues including design, mappings, data transformations, scheduling and testing.
Translate data movement requirements into technical ETL design.
Develop ETL/ELT workflows and mappings to extract, transform, and load data into target environments.
Develop data extraction and transmissions to external vendors.
Develop test plans and perform unit testing.
Create supporting documentation for new ETL processes.
Work closely with data analysts to gain understanding of business processes and corporate data.
Determine impacts to data warehouse structures and information flows due to changes in business or technical requirements.
Contribute to architectural decisions to support business processes.
Provide production support for data solutions.
Complete root cause analysis and contribute to remediation planning and implementation.
Perform data quality analysis, report data quality issues and propose solutions for data quality management.
Learn and expand upon internal controls and participate in customer support.
Perform design and code reviews in support of organizational standards.
Mentor less experienced team members.
Prepare effort estimation including researching and estimating costs of software development, unit testing. May provide estimates for upgrades of vendor packages upgrades and integration with existing systems.
On Call and/or after hours work required.
Essential Competencies
Professionalism/Personal Accountability, Collaboration and Teamwork, Communication, Flexible and Adapts to Change, Service to Customers and Clients
Skill Qualifications
Required:
Minimum of 8 years of relevant experience in data warehousing, business intelligence tools, and the analysis of data
Minimum of 5 years of SQL query development, preferably across multiple database management platforms, and working with dimensional and star schema data warehouse implementations
Proficiency in ETL/ELT concepts and tools (Informatica IICS, Talend, Boomi and PowerCenter preferred)
Working knowledge of data warehousing concepts, structures and ETL best practices
Experience using query tools (e.g.. AQT, MS Query)
Ability to problem solve using analytical thinking skills
Must work well independently - must be inquisitive and seek answers to complex questions without being prompted
Strong organizational and time management skills
Strong communication skills including verbal and written to communicate effectively with clients and management
Strong project management skills to ensure that projects get done on time and within budget
Effectively participates in teams and moves the team toward completion of goals
Ability to resolve conflicts
Preferred:
Some experience with cloud database technologies preferred
Some experience with data visualization tools (e.g. Tableau) desirable
Education And Experience Qualifications
Required:
BA or BS in Computer Science, Information Systems or related field.
10+ years’ experience
Show more
Show less","AWS, Azure, API data integration, Talend, Informatica, Azure Data Factory, Snowflake, Oracle, SQL Server, CI/CD, Git, Azure DevOps, Data warehousing, ETL/ELT, Data transformations, Unit testing, Data quality analysis, Data quality management, Internal controls, Code reviews, Effort estimation, SQL query development, Dimensional modeling, Star schema, AQT, MS Query, Analytical thinking, Organizational skills, Time management, Communication skills, Project management, Conflict resolution, Tableau","aws, azure, api data integration, talend, informatica, azure data factory, snowflake, oracle, sql server, cicd, git, azure devops, data warehousing, etlelt, data transformations, unit testing, data quality analysis, data quality management, internal controls, code reviews, effort estimation, sql query development, dimensional modeling, star schema, aqt, ms query, analytical thinking, organizational skills, time management, communication skills, project management, conflict resolution, tableau","analytical thinking, api data integration, aqt, aws, azure, azure data factory, azure devops, cicd, code reviews, communication skills, conflict resolution, data quality analysis, data quality management, data transformations, datawarehouse, dimensional modeling, effort estimation, etlelt, git, informatica, internal controls, ms query, oracle, organizational skills, project management, snowflake, sql query development, sql server, star schema, tableau, talend, time management, unit testing"
Sr. Cloud Database Engineer,Steneral Consulting,"Hartford, CT",https://www.linkedin.com/jobs/view/sr-cloud-database-engineer-at-steneral-consulting-3760208527,2023-12-17,Marion,United States,Mid senior,Hybrid,"Sr. Cloud Database Engineer (AWS + SQL Server database infrastructure deployment to AWS EC2 and RDS)
HYBRID ONSITE - 7 locations
Hybrid (Tues., Wed, Thurs. in office) - Hartford, CT, San Antonio, TX, Lake Mary, FL, Phoenix, AZ, Aurora, IL, New York City, NY and Danbury, CT
Local candidates to any of the above location
Must have SQL SERVER DBA Infrastructure
Full Job Description
Senior Cloud Database Engineer
Core Stack:
Windows/Powershell/Ansible/SQL Server
Job Overview:
We are seeking a highly skilled and experienced Senior Cloud Database Engineer to join our team. The preferred candidate for this position will have an extensive history working with the full stack of products offered by Microsoft with a core focus on SQL Server database infrastructure deployment experience and a proven ability to develop with Powershell and WinRM to automate infrastructure deployments and configurations. A history of experience using configuration management tools such as Ansible in conjunction with Powershell to manage Windows servers is highly desirable for this position.
The Senior Cloud Database Engineer will work closely with a team of database product specialists and help enhance our product portfolio of database products on the AWS platform with a core focus on SQL Server products deployed on EC2 and RDS. The ability to work on other platforms such as Linux and with other database technologies such as PostgreSQL, Oracle and NoSQL is desirable but not required.
Qualifications
6+ years working in an Enterprise environment that uses Microsoft Active Directory services to manage users and groups, and have a strong understanding of security best practices using Windows Auth and Kerberos.
3+ years of experience working on cloud platforms such as AWS or Azure, with preference on AWS.
6+ years of highly technical experience of Microsoft SQL Server, Availability Groups and Failover Clusters.
5+ years of development/automation experience using Powershell and Powershell remoting via WinRM.
2+ years of experience using Ansible to configure Windows systems using WinRM (Chef, Puppet experience will also be considered)
2+ years of experience with Terraform.
2+ years of experience using 3rd Party database backup tools such as Rubrik, Idera, Redgate, etc. and a firm understanding of native backup strategies.
Familiarity with programming languages such as TSQL, .Net, Python, GO.
Strong understanding of backup and recovery, security configuration, license management, reporting, integration, monitoring, audit, and performance troubleshooting skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience working in an Agile development environment is a plus.
Experience with other database platforms such as Oracle, PostgreSQL, etc is a plus.
Show more
Show less","SQL Server, Windows, Powershell, Ansible, AWS, EC2, RDS, Terraform, Rubrik, Idera, Redgate, TSQL, .Net, Python, Go, Linux, PostgreSQL, Oracle, NoSQL, Active Directory, Kerberos, Windows Auth, WinRM, Chef, Puppet, Agile","sql server, windows, powershell, ansible, aws, ec2, rds, terraform, rubrik, idera, redgate, tsql, net, python, go, linux, postgresql, oracle, nosql, active directory, kerberos, windows auth, winrm, chef, puppet, agile","active directory, agile, ansible, aws, chef, ec2, go, idera, kerberos, linux, net, nosql, oracle, postgresql, powershell, puppet, python, rds, redgate, rubrik, sql server, terraform, tsql, windows, windows auth, winrm"
Data Engineer IV - Max Digital (Data Operations),ACV Auctions,"Connecticut, United States",https://www.linkedin.com/jobs/view/data-engineer-iv-max-digital-data-operations-at-acv-auctions-3762879093,2023-12-17,Marion,United States,Mid senior,Hybrid,"If you are looking for a career at a dynamic company with a people-first mindset and a deep culture of growth and autonomy, ACV is the right place for you! Competitive compensation packages and learning and development opportunities, ACV has what you need to advance to the next level in your career. We will continue to raise the bar every day by investing in our people and technology to help our customers succeed. We hire people who share our passion, bring innovative ideas to the table, and enjoy a collaborative atmosphere.
Who we are:
ACV is a technology company that has revolutionized how dealers buy and sell cars online. We are transforming the automotive industry. ACV Auctions Inc. (ACV), has applied innovation and user-designed, data driven applications and solutions. We are building the most trusted and efficient digital marketplace with data solutions for sourcing, selling and managing used vehicles with transparency and comprehensive insights that were once unimaginable. We are disruptors of the industry and we want you to join us on our journey. ACV’s network of brands includes ACV Auctions, ACV Transportation, ClearCar, MAX Digital and ACV Capital within its Marketplace Products, as well as, True360 and Data Services.
At ACV we focus on the Health, Physical, Financial, Social and Emotional Wellness of our Teammates and to support this we offer:
Multiple medical plans including a high deductible health plan that costs $0 out of your paycheck
Company-sponsored (paid) Short-Term Disability, Long-Term Disability, and Life Insurance
Comprehensive optional benefits such as Dental, Vision, Supplemental Life/AD&D, Legal/ID Protection, and Accident and Critical Illness Insurance
Generous paid time off options, including vacation time, sick days, Company holidays, floating holidays, parental leave, bereavement leave, jury duty leave, voting leave, and other forms of paid leave as required by applicable law or regulation
Employee Stock Purchase Program with additional opportunities to earn stock in the Company
Retirement planning through the Company’s 401(k)
Who we are looking for:
We are seeking a highly skilled Engineer IV in Data Engineering with a strong foundation in computer science and excellent problem-solving skills. You will be responsible for maintaining and extending our database operations, optimizing SQL queries, and designing scalable data services.
What you will do:
Actively and consistently support all efforts to simplify and enhance the customer experience.
Maintain and extend (as required) existing database operations solution for backups, index defragmentation, data retention, etc.
Troubleshoot any SQL Server or ETL stack (SSIS, C#, Web APIs) outages during our operational support window.
Leverage monitoring tools to ensure high performance and availability; work with operations and engineering to improve as required.
Leverage DMVs and monitoring tools to ensure system performance; work with data operations and engineering to improve as required.
Ensure existing HADR (availability groups) solution is functional and meets requirements.
Support development, integration, and stage SQL Server environments for application development and data science teams.
Ensure that new database development meets company standards for readability, reliability, and performance. Work with internal teams on transactional and analytical schema design.
Collaborate with software and DevOps engineers to design scalable services, plan feature roll-out, and ensure high reliability and performance of our products.
Conduct code reviews, develop high-quality documentation, and build robust test suites.
Own the overall performance of products and services within a defined area of focus.
Respond-to and troubleshoot highly complex problems quickly, efficiently, and effectively. This may include being part of the emergency after-hours on-call rotation.
Mentor junior data engineers.
Perform additional duties as assigned.
What you will need:
Bachelor's degree in Computer Science, Information Technology, Computer Information Systems, Management Information Systems, or similar
5 years' building & supporting the database-tier of SaaS web applications.
Ability to read, write, speak, and understand English.
Expert in SQL Query optimization
ETL workflow implementation (SSIS, Airflow, C#, Python)
Experience working with Cloud Services (AWS RDS, S3, SQS, SNS)
Experience working with NoSQL data stores (e.g., MongoDB)
Experience developing Windows services in C#
Experience writing unit and integration testing
Expert SQL and data-layer development experience; OLTP schema design.
Experience using and integrating with cloud services, specifically: AWS RDS, S3, SQS, SNS.
Nice to Have
Experience with Airflow
Experience with DBT
Our Values
Trust & Transparency | People First | Positive Experiences | Calm Persistence | Never Settling
At ACV, we are committed to an inclusive culture in which every individual is welcomed and empowered to celebrate their true selves. We achieve this by fostering a work environment of acceptance and understanding that is free from discrimination. ACV is committed to being an equal opportunity employer regardless of sex, race, creed, color, religion, marital status, national origin, age, pregnancy, sexual orientation, gender, gender identity, gender expression, genetic information, disability, military status, status as a veteran, or any other protected characteristic. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you have a disability or special need that requires reasonable accommodation, please let us know.
For information on our collection and use of your personal information, please see our Privacy Notice.
Apply Now
Show more
Show less","SQL, ETL, SSIS, C#, Python, AWS, RDS, S3, SQS, SNS, MongoDB, Windows services, Unit and integration testing, OLTP, Data layer development, Airflow, DBT","sql, etl, ssis, c, python, aws, rds, s3, sqs, sns, mongodb, windows services, unit and integration testing, oltp, data layer development, airflow, dbt","airflow, aws, c, data layer development, dbt, etl, mongodb, oltp, python, rds, s3, sns, sql, sqs, ssis, unit and integration testing, windows services"
"Manager, Data Engineer",Arch Capital Services LLC,"Hartford, CT",https://www.linkedin.com/jobs/view/manager-data-engineer-at-arch-capital-services-llc-3737171030,2023-12-17,Marion,United States,Mid senior,Hybrid,"With a company culture rooted in collaboration, expertise and innovation, we aim to promote progress and inspire our clients, employees, investors and communities to achieve their greatest potential. Our work is the catalyst that helps others achieve their goals. In short, We Enable Possibility℠.
Job Summary
Strategic Analytics is a growing team at Arch that has established itself as a driving force in how the business is run. This is achieved through the implementation of real-time predictive analytic solutions that move the company forward. Our track record is 100% adoption of our tools and services. Data plays a critical role in our mission. We create best-in-class data solutions from internal and external sources by leveraging a diverse set of cloud technologies like Snowflake, the Azure tech stack, Databricks and Python.
As Manager of Strategic Analytics Services, you will implement complex data pipelines that put analytics at the heart of decision-making. You will work closely with leaders across the company on high-profile analytics projects that drive business strategies. As a key member of the data engineering team, you will extend our capacity to deliver and push the team forward.
Responsibilities
Work with strategic partners to solve business problems by directing junior employees to develop best-in-class data solutions
Manage data engineering throughout the project lifecycle
Build strong partnerships with peers across the organization to support data-related goals
Proactively design intuitive data structures by anticipating analytic needs
Explore new technologies and data sources with curiosity and creativity
Leverage technology to automate data ingestion and link external and internal data
Provide the appropriate documentation of sources and technical solutions
Desired Skills
Demonstrated experience building data structures to support analytics/research/actuarial functions in an insurance company setting
Experience with cloud technologies like Snowflake and Databricks
Familiarity with the MLOps framework
Ability to operate independently – managing tasks and engaging people across the team
Exceptional collaboration and relationship building skills
Comfortable handling ambiguous concepts and breaking down complex problems into manageable pieces
Strong data manipulation skills for analytics
Resilient problem solving and critical thinking skills
Thorough understanding of data warehousing concepts and design
Ability to communicate effectively to different target audiences
Flexibility – able to meet changing requirements and priorities
Required Skills
4+ years of data engineering experience
2+ years of project management experience
Education
College degree in Computer Science, Engineering, Statistics, Mathematics, Actuarial Science, Data Analytics, or equivalent
For individuals assigned or hired to work in California, Colorado, Jersey City, NJ; New York State; and/or Washington State, the base salary range is listed below. This range is as of the time of posting. Position is incentive eligible.
$148,614 - $201,066
Total individual compensation (base salary, short & long-term incentives) offered will take into account a number of factors including but not limited to geographic location, scope & responsibilities of the role, qualifications, talent availability & specialization as well as business needs. The above range may be modified in the future
Click here to learn more on available benefits
Do you like solving complex business problems, working with talented colleagues and have an innovative mindset? Arch may be a great fit for you. If this job isn’t the right fit but you’re interested in working for Arch, create a job alert! Simply create an account and opt in to receive emails when we have job openings that meet your criteria. Join our talent community to share your preferences directly with Arch’s Talent Acquisition team.
Show more
Show less","Data Engineering, Data Warehousing, Data Analytics, Machine Learning Operations (MLOps), Snowflake, Databricks, Python, Azure, Cloud Technologies, SQL, Data Manipulation, Data Structures, Critical Thinking, Problem Solving, Communication, Teamwork, Project Management","data engineering, data warehousing, data analytics, machine learning operations mlops, snowflake, databricks, python, azure, cloud technologies, sql, data manipulation, data structures, critical thinking, problem solving, communication, teamwork, project management","azure, cloud technologies, communication, critical thinking, data engineering, data manipulation, data structures, dataanalytics, databricks, datawarehouse, machine learning operations mlops, problem solving, project management, python, snowflake, sql, teamwork"
Senior Staff AI Data Engineer,Recruiting from Scratch,"New Haven, CT",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3759707753,2023-12-17,Marion,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto,
San
Francisco or Chicago
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data engineering, ML data pipelines, ML models, Applied ML solutions, Data mining, Data cleaning, Data normalization, Data modeling, Statistical analysis, Data visualization, Pandas, R, Data platforms, Data frameworks, Big data technologies, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, Public cloud environment, AWS, GCP, Azure, Relational databases, SQL, NoSQL databases, DynamoDB, ETL pipelines, Conversational AI APIs, Recommender systems, Distributed systems, Microservices, Kafka, Storm, SparkStreaming, Applied machine learning, Data management tools, Data classification, Data retention, Complex data projects","data engineering, ml data pipelines, ml models, applied ml solutions, data mining, data cleaning, data normalization, data modeling, statistical analysis, data visualization, pandas, r, data platforms, data frameworks, big data technologies, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, public cloud environment, aws, gcp, azure, relational databases, sql, nosql databases, dynamodb, etl pipelines, conversational ai apis, recommender systems, distributed systems, microservices, kafka, storm, sparkstreaming, applied machine learning, data management tools, data classification, data retention, complex data projects","airflow, applied machine learning, applied ml solutions, aws, azure, big data technologies, complex data projects, conversational ai apis, data classification, data cleaning, data engineering, data frameworks, data management tools, data mining, data normalization, data platforms, data retention, datamodeling, distributed systems, docker, dynamodb, etl pipelines, gcp, helm, kafka, kubernetes, microservices, ml data pipelines, ml models, nosql databases, pandas, public cloud environment, r, recommender systems, relational databases, snowflake, spark, sparkstreaming, sql, statistical analysis, storm, visualization"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Hartford, CT",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3773087718,2023-12-17,Marion,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto
or
San
Francisco
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Machine Learning, Data Engineering, Data Pipelines, Data Mining, Data Cleaning, Data Normalization, Data Modeling, Pandas, R, Airflow, KubeFlow, Python, Java, Bash, SQL, Git, Snowflake, Kubernetes, Docker, Helm, Spark, PySpark, AWS, GCP, Azure, DynamoDB, ETL, Kafka, Storm, SparkStreaming, Microservices","machine learning, data engineering, data pipelines, data mining, data cleaning, data normalization, data modeling, pandas, r, airflow, kubeflow, python, java, bash, sql, git, snowflake, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, dynamodb, etl, kafka, storm, sparkstreaming, microservices","airflow, aws, azure, bash, data cleaning, data engineering, data mining, data normalization, datamodeling, datapipeline, docker, dynamodb, etl, gcp, git, helm, java, kafka, kubeflow, kubernetes, machine learning, microservices, pandas, python, r, snowflake, spark, sparkstreaming, sql, storm"
Senior Data Analyst,LIMRA,"Windsor, CT",https://www.linkedin.com/jobs/view/senior-data-analyst-at-limra-3681907556,2023-12-17,Marion,United States,Mid senior,Hybrid,"LIMRA is seeking a highly skilled and motivated Senior Data Analyst to join our dynamic Data Analytics and Business Intelligence team. As a Senior Data Analyst, you will play a crucial role in helping our member companies gain valuable insights into important business issues. Your expertise will contribute to the development and implementation of strategic initiatives aimed at making data more accessible and useful for our members. This is an exciting opportunity for someone who enjoys collaborating with talented individuals and working on diverse projects.
Responsibilities May Include
Data Analysis and Interpretation:
Collect, organize, and analyze large datasets related to the insurance and financial services industry, including market trends, customer behavior, and industry performance.
Utilize statistical methods and data visualization techniques to derive meaningful insights and present them in a clear and concise manner.
Collaborate with cross-functional teams to develop predictive models and forecasting tools to support strategic initiatives.
Team Collaboration and Project Management:
Collaborate with cross-functional teams, including IT, marketing, research, and member relations, to support the successful execution of key projects and initiatives.
Actively participate in team discussions and contribute insights, suggestions, and innovative ideas to improve data processes and deliverables.
Effectively manage multiple projects concurrently, ensuring timely completion and high-quality deliverables.
Mentor and provide guidance to junior team members, fostering their professional growth and development.
Data Strategy and Development:
Contribute to the development of data-driven strategies that enhance the accessibility and utility of information for our member companies.
Assist in the design and implementation of data collection methods, ensuring data integrity, quality, and security.
Work closely with the IT team to optimize data storage, retrieval, and analysis processes, utilizing industry best practices and technologies.
Collaborate with stakeholders to identify data needs, define key performance indicators (KPIs), and create standardized reporting templates.
Qualifications
Bachelor's degree in a relevant field (e.g., Data Science, Statistics, Mathematics, Economics) is required.
Proven experience as a Data Analyst, preferably in the insurance or financial services industry.
Strong analytical and problem-solving skills with the ability to work with complex datasets
Proficiency in data manipulation and analysis tools, such as SAS, SQL, Python, R, or similar programming languages.
Experience with data visualization tools, such as Tableau, Power BI, or similar platforms, to effectively communicate findings.
Knowledge of predictive modeling techniques and familiarity with machine learning algorithms is highly desirable.
Excellent written and verbal communication skills, with the ability to present complex data in a clear and understandable manner.
Strong project management skills, with the ability to prioritize tasks, manage deadlines, and deliver high-quality work.
A collaborative mindset with the ability to work effectively in cross-functional teams.
We participate in the e-verify process for I-9 compliance.
Show more
Show less","Data Analysis, Data Interpretation, Statistical Methods, Data Visualization, Predictive Modeling, Forecasting, CrossFunctional Collaboration, Project Management, Mentoring, Data Strategy, Data Development, Data Collection, Data Integrity, Data Security, Data Storage, Data Retrieval, Industry Best Practices, Data Needs, Key Performance Indicators (KPIs), Standardized Reporting, Data Science, Statistics, Mathematics, Economics, SAS, SQL, Python, R, Tableau, Power BI, Machine Learning, Communication, Project Management, Teamwork","data analysis, data interpretation, statistical methods, data visualization, predictive modeling, forecasting, crossfunctional collaboration, project management, mentoring, data strategy, data development, data collection, data integrity, data security, data storage, data retrieval, industry best practices, data needs, key performance indicators kpis, standardized reporting, data science, statistics, mathematics, economics, sas, sql, python, r, tableau, power bi, machine learning, communication, project management, teamwork","communication, crossfunctional collaboration, data collection, data development, data integrity, data interpretation, data needs, data retrieval, data science, data security, data storage, data strategy, dataanalytics, economics, forecasting, industry best practices, key performance indicators kpis, machine learning, mathematics, mentoring, powerbi, predictive modeling, project management, python, r, sas, sql, standardized reporting, statistical methods, statistics, tableau, teamwork, visualization"
Supply Chain Data Analyst-Purchasing,"Stanley Black & Decker, Inc.","Valley City, OH",https://www.linkedin.com/jobs/view/supply-chain-data-analyst-purchasing-at-stanley-black-decker-inc-3778338887,2023-12-17,Cuyahoga Falls,United States,Associate,Onsite,"Come Build The World With Us
It takes great people to achieve greatness. People with a sense of purpose and integrity. People with a relentless pursuit of excellence. People who care about making things better For Those Who Make The World™. Sound like you? Join our top-notch team of nearly 60,000 professionals globally who are making their mark on some of the world’s most beloved brands, including DEWALT®, CRAFTSMAN®, CUB CADET®, STANLEY® and BLACK+DECKER®
The Job:
We’re the World’s largest tool company. We’re industry visionaries. We’re solving problems and advancing the manufacturing trade through innovative technology and our Industry 4.0 Initiative. We are committed to ensuring our state-of-the-art “smart factory” products and services provide greater quality to our customers & greater environmental and social value to our planet. We are unique in that we have a rich and storied history dating back to 1843, but that hasn't stopped us from evolving into a vibrant, diverse, global growth company
.
The Person:
You’re a curious problem solver who has the ability to bring big ideas to life. You’re creative and scrappy but can navigate a large organization with ease. You’re agile and adaptable but work with precision to ensure we deliver top-notch customer experiences, always. You also have:
Bachelor’s degree in Business/related field or equivalent experience.
3-5 years professional, relevant experience.
A team-focused mindset and positive attitude in the face of complicated and unfamiliar circumstances.
Proficiency in Microsoft Office, with emphasis on Excel and PowerPoint.
Advanced Excel skills required for this role.
ERP/MRP experience, Power BI, and Smartsheet experience a plus.
Strong written and verbal communication skills.
Excellent organizational skills with a compliant mindset and strong follow through.
Ability to work remotely with minimal supervision and effectively in a team environment.
Understanding of fundamental concepts, practices, and procedures in Purchasing, Supply Management, and Information Management.
Already have, or willingness to attain, a Lean Six Sigma Certification desired.
Visa Sponsorship is not provided, must be US Citizen, Green Card Holder/Permanent Resident.
What You’ll Do
This Data Analyst will be part of the Business Process and Supply Management team within the Outdoor Purchasing organization located in Valley City, OH. This individual will enjoy working in a fast-paced environment where the ability to multi-task is essential. Specifically, you’ll:
Research and transform information from raw data into an easily understood analysis that identifies trends and insights for the organization.
Compile data and prepare complex analyses of information or data.
Engage with other purchasing team members and company stakeholders to deliver process improvement.
Support the agile development, and continuous improvement of, internal systems and processes.
Provide support for large projects and take lead on smaller projects within the organization.
Supports the Purchasing Agents and Team Leads in the administrative sourcing activities that support the Supply Management goals and objectives.
Create and maintain up-to-date work instructions for all necessary functions of the role as needed.
Provide back-up support for Supply Management, Supplier Portal, and Business Process team as needed.
Provide ongoing training of Supply Management, Information Management, and Portal Management as needed.
Perform all purchasing activities in accordance with current company policies and procedures.
Spend at least 10% of your time on professional/personal development and technical upskilling.
Other duties as assigned.
Benefits & Perks
You’ll get a competitive salary and a comprehensive benefits plan that includes medical, dental, life, vision, wellness program, disability, 401k, Employee Stock Purchase Plan, Paid Time Off, including paid vacation, holidays & personal days, and tuition reimbursement. And, of course, discounts on Stanley Black & Decker tools and products and well as discount programs for many other vendors and partners.
What You’ll Also Get
Career Opportunity: Career paths aren’t linear here. Being part of our global company with 60+ brands gives you the chance to grow and develop your skills along multiple career paths.
Learning & Development: Our lifelong learning philosophy means you’ll have access to a wealth of state-of-the-art learning resources, including our Lean Academy and online university (where you can get certificates and specializations from renowned colleges and universities).
Diverse & Inclusive Culture: We pride ourselves on being an awesome place to work. We respect and embrace differences because that’s how the best work gets done. You’ll find we like to have fun here, too.
Purpose-Driven Company: You’ll help us continue to make positive changes in the local communities where we work and live as well as in the broader world through volunteerism, giving back and sustainable business practices.
Stanley Black & Decker is one of the world's largest, fastest growing and most dynamic companies. SBD, a Fortune 200 company is a world-leading provider of tools and storage, commercial electronic security and engineered fastening systems, with unique growth platforms and a track record of sustained profitable growth.
EEO:
All qualified applicants to Stanley Black & Decker are considered for employment without regard to race, color, religion, age, sex, sexual orientation, gender identity, national origin, disability, veteran’s status or any other protected characteristic.
We Don’t Just Build The World, We Build Innovative Technology Too.
Joining the Stanley Black & Decker team means working in an innovative, tech-driven and highly collaborative team environment supported by over 58,000 professionals in 60 countries across the globe. Here, you’ll get the unique chance to impact some of the world’s most iconic brands including STANLEY TOOLS, DEWALT, CRAFTSMAN, MAC TOOLS and Black + Decker. Your ideas and solutions have the potential to reach millions of customers as we work together to write the next chapter in our history. Come build with us and take your career to new heights.
Who We Are
We’re the World’s largest tool company. We’re industry visionaries. We’re solving problems and advancing the manufacturing trade through innovative technology and our Industry 4.0 Initiative. We are committed to ensuring our state-of-the-art “smart factory” products and services provide greater quality to our customers & greater environmental and social value to our planet. We are unique in that we have a rich and storied history dating back to 1843, but that hasn't stopped us from evolving into a vibrant, diverse, global growth company.
Benefits & Perks
You’ll get a competitive salary and a comprehensive benefits plan that includes medical, dental, life, vision, wellness program, disability, retirement benefits, Employee Stock Purchase Plan, Paid Time Off, including paid vacation, holidays & personal days, and tuition reimbursement. And, of course, discounts on Stanley Black & Decker tools and products and well as discount programs for many other vendors and partners.
What You’ll Also Get
Career Opportunity: Career paths aren’t linear here. Being part of our global company with 60+ brands gives you the chance to grow and develop your skills along multiple career paths.
Learning & Development:
Our lifelong learning philosophy means you’ll have access to a wealth of state-of-the-art learning resources, including our Lean Academy and online university (where you can get certificates and specializations from renowned colleges and universities).
Diverse & Inclusive Culture:
We pride ourselves on being an awesome place to work. We respect and embrace differences because that’s how the best work gets done. You’ll find we like to have fun here, too.
Purpose-Driven Company:
You’ll help us continue to make positive changes in the local communities where we work and live as well as in the broader world through volunteerism, giving back and sustainable business practices.
EEO Statement:
All qualified applicants to Stanley Black & Decker are considered for employment without regard to race, color, religion, age, sex, sexual orientation, gender identity, national origin, disability, veteran’s status or any other protected characteristic.
If you require reasonable accommodation to complete an application or access our website, please contact us at (860) 827-3923 or at accommodations@sbdinc.com. Due to volume, we cannot respond to unrelated inquiries about the status of a completed application or resetting an account password.
Know Your Rights: Workplace discrimination is illegal (eeoc.gov)
Show more
Show less","Microsoft Excel, Microsoft PowerPoint, Power BI, Smartsheet, ERP/MRP, Lean Six Sigma, Data Analysis, Information Management, Purchasing, Supply Management, Agile Development, Proces Improvement","microsoft excel, microsoft powerpoint, power bi, smartsheet, erpmrp, lean six sigma, data analysis, information management, purchasing, supply management, agile development, proces improvement","agile development, dataanalytics, erpmrp, information management, lean six sigma, microsoft excel, microsoft powerpoint, powerbi, proces improvement, purchasing, smartsheet, supply management"
Senior Operations Data Analyst,Medical Mutual,"Brooklyn, OH",https://www.linkedin.com/jobs/view/senior-operations-data-analyst-at-medical-mutual-3757915244,2023-12-17,Cuyahoga Falls,United States,Mid senior,Onsite,"Note, a minimum of (1) day per week in-office in Brooklyn, Ohio.
Ohio Candidates Strongly Preferred.
Under minimal supervision, supports business decisions using data mining and analytics, and identifying and resolving complex issues through the reconciliation of data sets. Identifies areas for improvement and/or impact of initiatives and strategic plans and presenting results to all levels of management. Serves as a subject matter, data analytics, and regulatory reporting expert in assigned functional areas. Collaborates with leaders in various internal departments and external parties to ensure timely and accurate data analyses.
Responsibilities
Collaborates with various departments (i.e., appeals, correspondence, legal, disbursements, sales) to identify and resolve complex issues and errors that involve internal and external systems, appeals, correspondence, claims and HIPAA data, and regulatory guidance; leads and executes complex data-related analytical projects to drive business decisions and efficiencies; ensures timely and accurate reporting, correction of issues and errors, implements best practices. May support audits and attestations and Request for Proposal (RFP).
Utilizes data mining and analytical expertise to understand large data sets. Provides clear, easy to understand summary documents and reports for all levels of management. Assists with gathering of data for regulatory agencies in addition to core business reporting (i.e., HIPAA, NCQA, Sherlock).Creates technical specifications, unit test plans, and dataflow diagrams. Follows quality assurance guidelines including the documentation, review, and approval of all project related artifacts.
Prepares analysis to identify trends and issues, identifies areas for improvement and/or impact of initiatives and strategic plans, troubleshoot errors and propose methods for resolution, validates data against other data sources, requirements and/or technical specifications.
Maintains up to date knowledge of regulatory requirements and industry best practices. Orients, trains, and assists more junior level Analysts.
Performs other duties as assigned.
Qualifications
Education and Experience
Bachelor’s degree in statistics, applied mathematics, data analysis, or related field. Master’s degree preferred.
5 years of experience in data or statistical analysis or equivalent relevant experience.
Health insurance or other financial services experience preferred.
Technical Skills And Knowledge
Advanced programming (SQL and SAS), visualization (PowerBI and Tableau), and Microsoft Office skills.
Ability to fulfill complex business requests and summarize results that can be understood by all levels of the organization (including executives).
Ability to present results to various audiences.
A Great Place To Work
Medical Mutual is looking to grow our team! We truly value and respect the talents and abilities of all of our employees. That's why we offer an exceptional package that includes:
Top Workplace in Northeast Ohio. Year after year we've received this recognition!
On-site wellness center at most locations. Enjoy personal trainers, towel service, locker room, weight room, elliptical machines, and a variety of classes!
On-site cafeteria serving hot breakfast and lunch, at most locations. Choices ranging from salad bar, made to order, hot and cold sandwiches, or a variety of entrees cooked fresh daily. Convenience store at most locations
Employee discount program. Discounts at many places in and around town, just for being a Medical Mutual team member
Business Casual attire
Excellent Benefits And Compensation
Competitive compensation plans
Employee bonus program
401(k) with company match and an additional company contribution
Excellent medical, dental, vision, and disability insurance
An Investment In You
Career development programs and classes
Mentoring and coaching to help you advance
Education reimbursement up to $5K per year
About Medical Mutual
We strive to create peace of mind. Our customers can trust us to do things right and to help them get value from their health plan. We're the largest health insurer in Ohio and for over 85 years, we've been serving our members and the Ohio communities where they live and work. Medical Mutual is a Top Place to Work in Northeast Ohio with exceptional career opportunities that offer challenge, growth and a great work/life balance. We want talented, innovative, and driven people to help us continue to be the best health insurance choice of Ohioans and help make Ohio the best it can be! Our headquarter building is located in the heart of downtown Cleveland and we have multiple offices throughout the state. Join us at one near you!
At Medical Mutual and its family of companies we celebrate differences and are mutually invested in our employees and our community. We are proud to be an Equal Employment Opportunity and Affirmative Action Employer. Qualified applicants will receive consideration for employment regardless of race, color, religion, sex, sexual orientation, gender perception or identity, national origin, age, marital status, veteran status, or disability status.
We maintain a drug-free workplace and perform pre-employment substance abuse and nicotine testing.
Primary Location
US-OH-Brooklyn
Work Locations
Brooklyn
Job
7 - General Staff
Organization
Statewide Operations
Schedule
Regular
Shift
Standard
Employee Status
Individual Contributor
Job Type
Full-time
Job Level
Day Job
Job Posting
Dec 5, 2023, 8:38:04 AM
Show more
Show less","SQL, SAS, PowerBI, Tableau, Microsoft Office, Data mining, Analytics, Data analysis, Data reconciliation, Data reporting, HIPAA, NCQA, Sherlock, Quality assurance, Unit test plans, Dataflow diagrams, Data visualization","sql, sas, powerbi, tableau, microsoft office, data mining, analytics, data analysis, data reconciliation, data reporting, hipaa, ncqa, sherlock, quality assurance, unit test plans, dataflow diagrams, data visualization","analytics, data mining, data reconciliation, data reporting, dataanalytics, dataflow diagrams, hipaa, microsoft office, ncqa, powerbi, quality assurance, sas, sherlock, sql, tableau, unit test plans, visualization"
Staff Data Engineer,Recruiting from Scratch,"Oxnard, CA",https://www.linkedin.com/jobs/view/staff-data-engineer-at-recruiting-from-scratch-3744394665,2023-12-17,Ojai,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our client is a dating app.
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, deliver analysis to further improve engagement in existing features, and empower our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Solve technical problems of the highest scope and complexity
Exert significant influence on the company’s analytical long-range goals and data architecture
Define and extend our internal standards for style, maintenance, and best practices for a high-scale data platform
Provide mentorship for all on your team to help them grow in their technical responsibilities
Propose ideas to improve the scale, performance, and capabilities of the Data Platform
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We'll Love About You
7+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field
7+ years experience using Python
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention
Location
: This role will be located in office twice a week minimum in Palo Alto, San Francisco or Chicago.
Salary Range: $180,000 - 250,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","SQL, Kubernetes, Spark, SRE, Big Data, ETL, Airflow, Kafka, Data modeling, Data governance, Data architecture, Git, Agile, Pair Programming, TDD, Continuous Integration, Automated Testing, Data Science, Docker, Schema Design, Data Warehouses, Python, Hadoop, Flask","sql, kubernetes, spark, sre, big data, etl, airflow, kafka, data modeling, data governance, data architecture, git, agile, pair programming, tdd, continuous integration, automated testing, data science, docker, schema design, data warehouses, python, hadoop, flask","agile, airflow, automated testing, big data, continuous integration, data architecture, data governance, data science, data warehouses, datamodeling, docker, etl, flask, git, hadoop, kafka, kubernetes, pair programming, python, schema design, spark, sql, sre, tdd"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Oxnard, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3748827521,2023-12-17,Ojai,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our next evolution is underway as a newly public company looking to expand and continue to build meaningful experiences for our users. From social issues to original content, we’re blazing innovative paths with impact for our community, all while leveraging the latest tech stacks and striving for engineering excellence. At the heart of our work in this new chapter is a shared set of core values: openness and exploration, a bias for action, and strong support of the LGBTQ community.
With a track record of strong financial performance and plans for continued headcount growth, we’re looking to build a team of talented, passionate, and open-minded people who believe in our mission, align with our values, and are excited to work at the intersection of innovative technology and social impact. Come be a part of this exciting journey with us.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location: Palo Alto, San Francisco or Chicago
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, SQL, TDD, Pair Programming, Continuous Integration, Kafka, Storm, SparkStreaming, Data Warehouses, ETL, Data management tools, Data classification","python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, tdd, pair programming, continuous integration, kafka, storm, sparkstreaming, data warehouses, etl, data management tools, data classification","airflow, continuous integration, data classification, data management tools, data warehouses, docker, etl, helm, kafka, kubernetes, pair programming, python, snowflake, spark, sparkstreaming, sql, storm, tdd"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Oxnard, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3744395695,2023-12-17,Ojai,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
They are a profitable dating application.
Location: This is a hybrid role based in their Chicago office and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location
: This role is twice a week in a hybrid role minimum.
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Data engineering, Business intelligence, Data science, SQL, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, Agile engineering, TDD, Pair Programming, Continuous Integration, Automated testing, Deployment, Kafka, Storm, SparkStreaming, Dimensional data modeling, Schema design, Data Warehouses, ETL, Legal compliance, Data classification, Retention","python, data engineering, business intelligence, data science, sql, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, agile engineering, tdd, pair programming, continuous integration, automated testing, deployment, kafka, storm, sparkstreaming, dimensional data modeling, schema design, data warehouses, etl, legal compliance, data classification, retention","agile engineering, airflow, automated testing, business intelligence, continuous integration, data classification, data engineering, data science, data warehouses, deployment, dimensional data modeling, docker, etl, helm, kafka, kubernetes, legal compliance, pair programming, python, retention, schema design, snowflake, spark, sparkstreaming, sql, storm, tdd"
Data Visualization / Business Intelligence Analyst (3019),SMX,"Port Hueneme, CA",https://www.linkedin.com/jobs/view/data-visualization-business-intelligence-analyst-3019-at-smx-3760167341,2023-12-17,Ojai,United States,Mid senior,Hybrid,"SMX is seeking a
Data Visualization / Business Intelligence Analyst
to support a government client based out of Port Hueneme, CA. Onsite support is highly preferred but hybrid or remote may be considered.
The Data Visualization / Business Intelligence Analyst will:
Develop reports, dashboards, and advanced visualizations by working closely with stakeholders.
Help define and design reporting data structures and tables. Ensure the integrity of the data and the data structures.
Write SQL queries as needed including for extract and loading, data comparison, validation, and cleaning. This could include creating stored procedures, functions, views, etc.
Help administer the Power BI deployment, workspaces, applications, and permissions.
Participate in the complete software development lifecycle from requirements, prototyping, production, deployment, and support.
Research technologies, standards and services and integrate them into the ecosystem.
Ensure compliance with all security, availability, confidentiality, and privacy policies and controls.
Create and maintain best practices, coding standards, and documentation.
Support presentations to stakeholders, partners, and customers as needed.
Required Skills and Experience:
Clearance Required: Secret or ability to obtain
Bachelor's degree in Computer Science, Information Systems or Computer Engineering or 4 years related experience
3+ years of BI development using Power BI or equivalent tool. Proficient in making DAX queries in Power BI and using advanced calculations.
3+ years of SQL and databases with emphasis on Transact-SQL, OLTP, and OLAP. Understands complex joins and time segment data.
3+ years of Excel experience.
Knowledge of ETL/ELT techniques and methodologies along with best practices.
Exceptional analytical, critical thinking and problem-solving abilities.
Highly self-directed, takes initiative, can work independently while being a strong contributor to a team.
Effective prioritization and time management skills. Can thrive in a dynamic environment in which priorities might change frequently.
Excellent communication and interpersonal skills. Strong documentation and presentation skills.
Work requires CSWF certification in accordance with DoD 8570 and SECNAV 5239.2. (Security+ or CISSP)
#cjpost
At SMX®, we are a team of technical and domain experts dedicated to enabling your mission. From priority national security initiatives for the DoD to highly assured and compliant solutions for healthcare, we understand that digital transformation is key to your future success.
We share your vision for the future and strive to accelerate your impact on the world. We bring both cutting edge technology and an expansive view of what’s possible to every engagement. Our delivery model and unique approaches harness our deep technical and domain knowledge, providing forward-looking insights and practical solutions to power secure mission acceleration.
SMX is committed to hiring and retaining a diverse workforce. All qualified candidates will receive consideration for employment without regard to disability status, protected veteran status, race, color, age, religion, national origin, citizenship, marital status, sex, sexual orientation, gender identity or expression, pregnancy or genetic information. SMX is an Equal Opportunity/Affirmative Action employer including disability and veterans.
Selected applicant will be subject to a background investigation.
Show more
Show less","Data Visualization, Business Intelligence, Reporting, Dashboards, Advanced Visualizations, Power BI, SQL, TransactSQL, OLTP, OLAP, ETL/ELT, DAX Queries, Advanced Calculations, Excel, CSWF Certification, DoD 8570, SECNAV 5239.2, Security+, CISSP","data visualization, business intelligence, reporting, dashboards, advanced visualizations, power bi, sql, transactsql, oltp, olap, etlelt, dax queries, advanced calculations, excel, cswf certification, dod 8570, secnav 52392, security, cissp","advanced calculations, advanced visualizations, business intelligence, cissp, cswf certification, dashboard, dax queries, dod 8570, etlelt, excel, olap, oltp, powerbi, reporting, secnav 52392, security, sql, transactsql, visualization"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Oxnard, CA",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3759707777,2023-12-17,Ojai,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto,
San
Francisco or Chicago
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data engineering, ML Data Ops, Data pipelines, Data mining, Data cleaning, Data normalization, Data modeling, Data visualization, Pandas, R, Airflow, KubeFlow, NLP, Python, Java, Bash, SQL, Git, Snowflake, Kubernetes, Docker, Helm, Spark, PySpark, AWS, GCP, Azure, DynamoDB, ETL, Kafka, Storm, SparkStreaming, Machine learning, Data management, Data classification, Data retention","data engineering, ml data ops, data pipelines, data mining, data cleaning, data normalization, data modeling, data visualization, pandas, r, airflow, kubeflow, nlp, python, java, bash, sql, git, snowflake, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, dynamodb, etl, kafka, storm, sparkstreaming, machine learning, data management, data classification, data retention","airflow, aws, azure, bash, data classification, data cleaning, data engineering, data management, data mining, data normalization, data retention, datamodeling, datapipeline, docker, dynamodb, etl, gcp, git, helm, java, kafka, kubeflow, kubernetes, machine learning, ml data ops, nlp, pandas, python, r, snowflake, spark, sparkstreaming, sql, storm, visualization"
Data Visualization / Business Intelligence Analyst (3019) with Security Clearance,ClearanceJobs,"Port Hueneme, CA",https://www.linkedin.com/jobs/view/data-visualization-business-intelligence-analyst-3019-with-security-clearance-at-clearancejobs-3777111110,2023-12-17,Ojai,United States,Mid senior,Hybrid,"SMX is seeking a Data Visualization / Business Intelligence Analyst to support a government client based out of Port Hueneme, CA. Onsite support is highly preferred but hybrid or remote may be considered. The Data Visualization / Business Intelligence Analyst will:
Develop reports, dashboards, and advanced visualizations by working closely with stakeholders.
Help define and design reporting data structures and tables. Ensure the integrity of the data and the data structures.
Write SQL queries as needed including for extract and loading, data comparison, validation, and cleaning. This could include creating stored procedures, functions, views, etc.
Help administer the Power BI deployment, workspaces, applications, and permissions.
Participate in the complete software development lifecycle from requirements, prototyping, production, deployment, and support.
Research technologies, standards and services and integrate them into the ecosystem.
Ensure compliance with all security, availability, confidentiality, and privacy policies and controls.
Create and maintain best practices, coding standards, and documentation.
Support presentations to stakeholders, partners, and customers as needed. Required Skills and Experience:
Clearance Required: Secret or ability to obtain
Bachelor's degree in Computer Science, Information Systems or Computer Engineering or 4 years related experience
3+ years of BI development using Power BI or equivalent tool. Proficient in making DAX queries in Power BI and using advanced calculations.
3+ years of SQL and databases with emphasis on Transact-SQL, OLTP, and OLAP. Understands complex joins and time segment data.
3+ years of Excel experience.
Knowledge of ETL/ELT techniques and methodologies along with best practices.
Exceptional analytical, critical thinking and problem-solving abilities.
Highly self-directed, takes initiative, can work independently while being a strong contributor to a team.
Effective prioritization and time management skills. Can thrive in a dynamic environment in which priorities might change frequently.
Excellent communication and interpersonal skills. Strong documentation and presentation skills.
Work requires CSWF certification in accordance with DoD 8570 and SECNAV 5239.2. (Security+ or CISSP) #cjpost
At SMX®, we are a team of technical and domain experts dedicated to enabling your mission. From priority national security initiatives for the DoD to highly assured and compliant solutions for healthcare, we understand that digital transformation is key to your future success. We share your vision for the future and strive to accelerate your impact on the world. We bring both cutting edge technology and an expansive view of what's possible to every engagement. Our delivery model and unique approaches harness our deep technical and domain knowledge, providing forward-looking insights and practical solutions to power secure mission acceleration. SMX is committed to hiring and retaining a diverse workforce. All qualified candidates will receive consideration for employment without regard to disability status, protected veteran status, race, color, age, religion, national origin, citizenship, marital status, sex, sexual orientation, gender identity or expression, pregnancy or genetic information. SMX is an Equal Opportunity/Affirmative Action employer including disability and veterans. Selected applicant will be subject to a background investigation.
Show more
Show less","Data Visualization, Business Intelligence, Power BI, SQL, ETL/ELT, DAX, TransactSQL, OLTP, OLAP, Excel","data visualization, business intelligence, power bi, sql, etlelt, dax, transactsql, oltp, olap, excel","business intelligence, dax, etlelt, excel, olap, oltp, powerbi, sql, transactsql, visualization"
Sr. Data Engineer- Underwriting (Hybrid),Selective Insurance,"Branchville, NJ",https://www.linkedin.com/jobs/view/sr-data-engineer-underwriting-hybrid-at-selective-insurance-3731009852,2023-12-17,Newton,United States,Mid senior,Onsite,"At Selective, we don't just insure uniquely, we employ uniqueness.
Our Business
Selective is a midsized U.S. domestic property and casualty insurance company with a history of strong, consistent financial performance for nearly 100 years. Selective's unique position as both a leading insurance group and an employer of choice is recognized in a wide variety of awards and honors, including listing in Forbes Best Midsize Employers in 2023 and certification as a Great Place to Work® in 2023.
Working at Selective
At Selective, we don't just insure uniquely – we employ uniqueness. Employees are empowered and encouraged to
Be Uniquely You
by being their true, unique selves and contributing their diverse talents, experiences, and perspectives to our shared success. Together, we are a high-performing team working to serve our customers responsibly by helping to mitigate loss, keep them safe, and restore their lives and businesses after an insured loss occurs. Employees receive comprehensive total rewards packages - including competitive compensation and performance awards, health benefits, and retirement savings - and professional development opportunities and flexible schedules to support their health, wealth, and well-being. Join our team and help make a difference.
Overview
Selective Insurance is seeking an energetic and collaborative Sr. Data Engineer to work on data and analytics projects supporting the Underwriting team within the Information Management group. This group is responsible for technology support of all Data Engineering, Analytics and Reporting for the Marketing, Customer Experience and Contact Center business areas. This includes Data Engineering services, Enterprise reporting support and ML Ops Engineering operations for these groups. The candidate must be hands on with very good technical skills and a proven track record of project delivery.
Responsibilities
Hands on development and support of new or existing data applications.
Work closely with business and analysts to understand data and business process and make recommendations to clients as requested on best practices or long-term solutions to resolve current issues and also for future system design
Works closely with Application and Enterprise Architects to create/review low level implementation designs, understand high level data flow designs developed by data architects.
Provide technical guidance to the team for implementing complex data solutions.
Provide support in the design, development, code reviews, test deploy and documentation of data engineering and data integration Applications.
Maintain detailed documentation to support downstream integrations
Provide support for production issues
Performs activities of a scrum master
Identify technology trends and explore opportunities for use within the organization
Qualifications
Five to seven years of experience in Data Warehousing, Data integration or Data Engineering projects
Ability to effectively work well with people in other departments and/or outside of the enterprise.
Proficient in SQL.
Experience working within Azure ecosystem
Experience in Informatica Powercenter, IICS, Cognos, Netezza Performance servers
Experienced in any of these analytical platforms - PowerBI, AzureML, Databricks or Synapse
Experience using Python or Scala.
Experience in Azure DevOps and Github is preferred
P&C Insurance experience is preferred
Possesses excellent communication skills.
Bachelor’s degree in computer science or related engineering field preferred.
Salary range: $90,600 - $149,600. The actual base salary is based on geographic location, and the range is representative of salaries for this role throughout Selective's footprint. Additional considerations include the candidate's qualifications and experience.
Selective is an Equal Employment Opportunity employer. That means we respect and value every individual’s unique opinions, beliefs, abilities, and perspectives. We are committed to promoting a welcoming culture that celebrates diverse talent, individual identity, different points of view and experiences – and empowers employees to contribute new ideas that support our continued and growing success. Building a highly engaged team is one of our core strategic imperatives, which we believe is enhanced by diversity, equity, and inclusion. We expect and encourage all employees and all of our business partners to embrace, practice, and monitor the attitudes, values, and goals of acceptance; address biases; and foster diversity of viewpoints and opinions.
Selective maintains a drug-free workplace
.
Show more
Show less","SQL, Azure, Informatica Powercenter, IICS, Cognos, Netezza Performance servers, PowerBI, AzureML, Databricks, Synapse, Python, Scala, Azure DevOps, Github, Scrum, Data Warehousing, Data integration, Data Engineering","sql, azure, informatica powercenter, iics, cognos, netezza performance servers, powerbi, azureml, databricks, synapse, python, scala, azure devops, github, scrum, data warehousing, data integration, data engineering","azure, azure devops, azureml, cognos, data engineering, data integration, databricks, datawarehouse, github, iics, informatica powercenter, netezza performance servers, powerbi, python, scala, scrum, sql, synapse"
"Senior Analyst, Data Operations",Wyndham Hotels & Resorts,"Parsippany, NJ",https://www.linkedin.com/jobs/view/senior-analyst-data-operations-at-wyndham-hotels-resorts-3783028166,2023-12-17,Newton,United States,Mid senior,Onsite,"Job description:
Why Wyndham?
By joining Wyndham Hotels & Resorts, you will play an integral role in our mission to make hotel travel possible for all. You’ll be part of the largest hotel franchise company in the world, where we strive to deliver the best value to our owners and guests. Most importantly, you’ll be entrusted to live our unique Count on Me culture, set by our core values of integrity, accountability, inclusive, caring and fun. At Wyndham, we provide all team members the opportunity to grow through best-in-class training and career development, leadership training, mentorship opportunities and educational support.  Supporting our team members is a top priority, which is why we offer competitive compensation and benefits, vacation, team member appreciation days, workplace flexibility and much more.  At Wyndham Hotels & Resorts, we value and embrace a culture of diversity, equity and inclusion that supports team members of all backgrounds and experiences. We can’t wait to welcome you!
The Role
The Senior Analyst, Data Operations will support IT Data Technology and business reporting teams. The primary responsibility will be to structure large data sets, understand and produce analysis that will be used to drive user insights, identification of performance improvement opportunities, ad hoc analysis, and reporting to support key initiatives within the organization.
What you'll do
Collecting project requirements and meeting with project teams regularly to coordinate tasks, resolve conflicts, and identify opportunities for improvement.
Create reports for internal teams and/or external clients.
Collaborate with team members to collect and analyze data.
Establish KPIs to measure the effectiveness of business decisions.
Updating project documentation, determining which tasks to prioritize and ensuring the timely completion of deliverables.
Manage and assist in prioritizing the data and maintenance backlog.
Work with Incident Management and Support teams on all data-related issues.
Communicate effectively with the business on incident and event updates.
Follow the Incident Management process as designed by Wyndham.
Identify and implement process improvement opportunities to support operational efficiencies.
Using sound judgment to escalate issues to the affected parties and other key stakeholders
Planning and estimating the specific requirements involved in a project or strategic initiative.
Developing a communication process to collaborate with customers and update stakeholders on the progress of a project.
Facilitating meetings with organizational leadership and organizing presentations.
Create presentations and reports based on recommendations and findings across the company to model data in a scalable way and present it in an impactful and digestible way that leads to better business decisions.
Updating project documentation, determining which tasks to prioritize and ensuring the timely completion of deliverables.
Manage and assist in prioritizing the data and maintenance backlog.
Application of Data Governance principles and framework across the products.
Escalate risks and issues during all phases of the project.
Establish a perspective on data needs, designs, work breakdowns, and roadmaps that meet shared goals and demonstrate incremental value.
You'll be successful if you have
Experience working in a customer facing role.
Flexible, open to changes and adapts quickly to new environments, tools and processes.
Clear understanding of relational databases, data warehouses and reporting analytics/KPI.
Analytical and problem-solving skills.
Experience with statistical software (e.g., Stata, SPSS).
Knowledge of data gathering, data warehousing, cleaning and transforming techniques.
Reporting and data visualization skills using software like Tableau.
Understanding of data warehousing and ETL techniques.
Proficiency in Microsoft Excel.
Ability to set and meet deadlines.
Ability to work in high-pressure situations.
Experience with statistical software.
Technical writing skills.
Excellent attention to detail.
Strong written/verbal communication skills.
Ability to QA and troubleshoot data.
Strong knowledge of data management and query writing on MS SQL.
Must have the ability to prioritize and handle multiple tasks in a high-pressure environment.
Resourceful and excellent organizational skills.
Process requests timely with minimal supervision required.
Methodical approach. Organized, possessing strong analytical abilities.
Strong time management skills, ability to work within deadlines.
Strong work ethic and attention to detail.
Experience working in the hospitality field, preferred.
Required Qualifications/Experience
Data mining experience, information extraction or statistical modeling.  Familiar with Data structures.
Clear understanding of relational databases, data warehouses and reporting analytics/KPI.
MS/BS in Information Technologies or related field and/or equivalent experience.
5 + years of IT experience (Data Analyst role preferred).
Able to create complex queries in SQL.
Reporting tools such as Tableau, Birst.
Knowledge of using AWS cloud services like S3, Redshift, Atena, etc.
Coding language such as Python, Java.
Show more
Show less","Data Analytics, Data Mining, Data Warehousing, Data Visualization, Business Intelligence, Statistical Modeling, Information Extraction, Relational Databases, SQL, Microsoft Excel, Tableau, Birst, AWS, Python, Java","data analytics, data mining, data warehousing, data visualization, business intelligence, statistical modeling, information extraction, relational databases, sql, microsoft excel, tableau, birst, aws, python, java","aws, birst, business intelligence, data mining, dataanalytics, datawarehouse, information extraction, java, microsoft excel, python, relational databases, sql, statistical modeling, tableau, visualization"
Senior Data Scientist,Brooksource,"Morris Plains, NJ",https://www.linkedin.com/jobs/view/senior-data-scientist-at-brooksource-3763391642,2023-12-17,Newton,United States,Mid senior,Remote,"The Data Science Advisor- Pharmacy Analytics position is an opportunity for an analytics professional to provide leadership on complex analytics projects and initiatives. This role will work with an innovative team on setting and executing the vision for how advanced embedded analytics can lead the organization to achieving our growth goals. This role will work collaboratively with our business stakeholders to provide partnership in analytics, developing analytics solutions, leveraging data science and technologic capabilities and embedding analytics driven processes.
The job responsibilities include, but are not limited to the following:
Lead analyses related to Healthcare Resource Utilization, Total Healthcare Cost and Clinical Outcomes with a focus on Specialty Medications/Rare diseases, Channel Management, Utilization Management, and specific Therapeutic area research (root cause analyses, Health Outcome Studies, Opportunity analysis including descriptive and multivariate statistics to identify patterns in the data.)
Develop new reports, models and analytic solutions with innovative ways to present data internally and externally in order to support Health Sales & Business stakeholders. This requires combining business knowledge and data acumen along with technical (SAS, SQL) skills to efficiently complete these ad-hoc requests. Query data warehouse(s) using variety of tools available. Extract data and manipulate into reports for client both internal and external. Maintain turn-around times per agreements.
Consultation with Data & Analytics matrix partners to develop best practices and help understand complex issues and requests. Cross-Functional collaboration as needed to create alignment with stakeholders.
Project management and prioritization – Advisor role will support multiple projects and will need to be able to work with Health Sales’ and Analytics team to manage multiple initiatives at the same time and negotiate timelines/priority with stakeholders.
Manage the analytic solution development efforts in the role of a Product Owner, requiring effective project management, technical and functional documentation, communication and stakeholder alignment.
Explore and visualize the data using advanced tools such as including Tableau, PowerBI, Thoughtspot and/or Looker. (Largely Tool agnostic environment)
Extraction and analysis of large healthcare claims data using state of the art big-data infrastructure leveraging cloud and on premise tools i.e. SAS, Analytic platform (Python), R, Teradata, Hadoop, etc.
Qualifications:
BS/MS/PhD inEconometrics, Actuarial Science, Data Science, Health Outcomes, Epidemiology, Statistics, or in any technical field that provides a solid basis for analytics highly desired.
Minimum 3 years of industry experience in solving Health Plan, PBM business or Commercial Pharma Analytic problems through the application of analytic approaches.
Prior experience in Healthcare analytics, Specialty Medication or Specialty Condition analytics or Health Outcomes preferred.
A demonstrated ability to understand and effectively communicate (both verbally and written) analytic and clinical data to a varied audience.
Deep healthcare data (e.g., PBM experience, Provider Networks, Billing, Medical and Pharmacy claims), statistical analysis experience, and an understanding of all the associated clinical, utilization and financial levers.
Experience with statistical software/programming languages such as SQL programming, SAS, R, Python and other tools preferred (Python knowledge not requisite but preferred).
Experience with data visualization tools such as Tableau, Thoughtspot or PowerBI.
A data-driven personality w/ Intellectual curiosity and internal motivation.
Show more
Show less","SAS, SQL, Tableau, PowerBI, Thoughtspot, Looker, Python, R, Teradata, Hadoop, Econometrics, Actuarial Science, Data Science, Health Outcomes, Epidemiology, Statistics, Healthcare analytics, Specialty Medication analytics, Specialty Condition analytics, Clinical data, Statistical analysis, Data visualization","sas, sql, tableau, powerbi, thoughtspot, looker, python, r, teradata, hadoop, econometrics, actuarial science, data science, health outcomes, epidemiology, statistics, healthcare analytics, specialty medication analytics, specialty condition analytics, clinical data, statistical analysis, data visualization","actuarial science, clinical data, data science, econometrics, epidemiology, hadoop, health outcomes, healthcare analytics, looker, powerbi, python, r, sas, specialty condition analytics, specialty medication analytics, sql, statistical analysis, statistics, tableau, teradata, thoughtspot, visualization"
Data Engineer - Hybrid-Home & Office,"Managed Health Care Associates, Inc.","Parsippany, NJ",https://www.linkedin.com/jobs/view/data-engineer-hybrid-home-office-at-managed-health-care-associates-inc-3785833597,2023-12-17,Newton,United States,Mid senior,Hybrid,"Company Overview:
Managed Health Care Associates, Inc. (MHA) is a leading health care services and technology company that offers a growing portfolio of services and solutions to support the diverse and complex needs of the alternate site health care provider. MHA provides expertise in Group Purchasing, Managed Care and Payer Contracting, Reimbursement Management, Specialty Pharmacy Solutions, Pharmaceutical Data Analytics, Consultant Pharmacy Software and Legislative Advocacy. Through the delivery of innovative health care services and solutions, MHA helps members increase operational efficiency, maximize business growth and provide optimum care for their patients.
Founded in 1989, MHA was purchased by Roper Technologies, Inc. in mid-2013. Roper Technologies is a constituent of the S&P 500, Fortune 1000, and the Russell 1000 indices. Roper designs and develops software (both software-as-a-service and licensed), and engineered products and solutions for healthcare, transportation, food, energy, water, education, and academic research markets worldwide. To learn more please visit www.mhainc.com
Job Purpose:
The Data Engineer is critical to design, develop, and maintain the infrastructure and systems necessary for the collection, storage, processing, and analysis of large volumes of data. The Data Engineer will play a critical role in enabling the organization to make data-driven decisions and derive valuable insights from their data assets, including Data Pipeline development, Data Modeling and Architecture. The Data Engineer will support our software developers, database architects and data analysts on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects.  The Data Engineer will play a crucial role in the development and support of cloud database systems for our new customer platform. You will collaborate with cross-functional teams using the Agile methodology and Azure Cloud to create modern software applications, implement new technologies, and deliver end-to-end solutions utilizing DevOps and engineering practices.
Essential Duties & Responsibilities:
Expertise in designing, building, and maintaining scalable data pipelines and ETL (Extract, Transform, Load) processes.
Experience with data modeling and database design principles.
Knowledge of data warehousing concepts and tools.
Understanding of data governance, data quality, and data integration techniques.
Proficiency in Microsoft Azure cloud services, including Azure Data Factory, Azure Databricks, Azure SQL Database, Azure Data Lake, etc.
Proficiency in data visualization tools like Power BI, Sisense
Experience with managing and optimizing cloud-based data storage and processing solutions.
Knowledge of Azure DevOps for CI/CD (Continuous Integration/Continuous Deployment).
Understanding of cloud security and compliance principles
Familiarity with data integration tools (e.g., Apache Kafka, Apache Nifi)
Understanding of machine learning concepts and experience with ML platforms like Azure Machine Learning or TensorFlow
Minimum Qualifications & Competencies:
A bachelor's or master's degree in computer science, data science, or a related field.
Strong programming skills, particularly in languages like Python, Java, or Scala.
Proficiency in SQL and experience with relational databases (e.g., MySQL, PostgreSQL).
Familiarity with distributed computing frameworks, such as Apache Hadoop and Apache Spark.
Experience with version control systems like Git.
Knowledge of software engineering principles and best practices
Strong problem-solving skills and the ability to work independently and in a team environment.
Effective communication skills to collaborate with stakeholders, data scientists, and other teams.
Attention to detail and ability to prioritize tasks in a fast-paced environment.
Continuous learning mindset to keep up with the evolving data engineering landscape.
Equal Opportunity Employer/Protected Veterans/Individuals with Disabilities
The contractor will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the contractor’s legal duty to furnish information. 41 CFR 60-1.35(c)
Show more
Show less","Data Engineering, Data Warehousing, Data Modeling, ETL (Extract Transform Load), Data Governance, Data Integration, Azure Cloud Services, Power BI, Sisense, Azure DevOps, Apache Kafka, Apache Nifi, Azure Machine Learning, TensorFlow, Python, Java, Scala, SQL, MySQL, PostgreSQL, Apache Hadoop, Apache Spark, Git, Software Engineering, ProblemSolving, Teamwork, Communication, Prioritization, Continuous Learning","data engineering, data warehousing, data modeling, etl extract transform load, data governance, data integration, azure cloud services, power bi, sisense, azure devops, apache kafka, apache nifi, azure machine learning, tensorflow, python, java, scala, sql, mysql, postgresql, apache hadoop, apache spark, git, software engineering, problemsolving, teamwork, communication, prioritization, continuous learning","apache hadoop, apache kafka, apache nifi, apache spark, azure cloud services, azure devops, azure machine learning, communication, continuous learning, data engineering, data governance, data integration, datamodeling, datawarehouse, etl extract transform load, git, java, mysql, postgresql, powerbi, prioritization, problemsolving, python, scala, sisense, software engineering, sql, teamwork, tensorflow"
Data Scientist,Brooksource,"Morris Plains, NJ",https://www.linkedin.com/jobs/view/data-scientist-at-brooksource-3774010984,2023-12-17,Newton,United States,Mid senior,Hybrid,"We are seeking a talented and motivated Data Scientist to join our team at Fortune 15 Health Insurance Company. As a Data Scientist, you will play a critical role in leveraging data to drive insights and make data-driven decisions that impact the health insurance industry. You will be responsible for conducting data analysis, developing predictive models, and generating actionable insights that will help us optimize our operations, improve customer service, and enhance decision-making processes.
Key Responsibilities:
Data Analysis:
Utilize Python, R, and SQL to explore and analyze large datasets, extracting valuable insights and trends.
Machine Learning and Predictive Modeling:
Develop and implement machine learning models to predict health insurance trends, customer behaviors, and claims processing. Collaborate with the team to help develop new machine learning models and refine existing ones.
Data Visualization:
Create informative and engaging data visualizations and reports to communicate findings and insights to various stakeholders.
Data Cleaning and Preprocessing:
Prepare and clean data to ensure its accuracy and reliability for analysis and machine learning modeling.
Feature Engineering:
Identify and engineer relevant features for machine learning and predictive modeling tasks.
Collaboration:
Collaborate with cross-functional teams, including actuaries, underwriters, and IT, to develop data-driven solutions that improve our health insurance products and services.
Model Evaluation:
Evaluate the performance of machine learning models and iterate on them to improve accuracy and effectiveness.
Documentation:
Maintain clear and comprehensive documentation of data analysis, machine learning techniques, and results.
Data Security:
Ensure compliance with data privacy and security regulations in all data-related tasks.
Qualifications:
Bachelor's or Master's degree in Data Science, Computer Science, Statistics, or a related field.
Proven experience as a Data Scientist in the health insurance or healthcare industry is a plus.
Proficiency in Python, R, and SQL for data analysis, modeling, and manipulation.
Strong knowledge of machine learning techniques, statistical analysis, and data mining.
Familiarity with data visualization tools such as Matplotlib, Seaborn, ggplot2, or similar.
Excellent problem-solving skills and the ability to work independently.
Effective communication and collaboration skills to work with cross-functional teams.
Knowledge of health insurance concepts and related data is a strong advantage.
Experience with big data technologies and cloud platforms (e.g., AWS, Azure, or GCP) is a plus.
Show more
Show less","Python, R, SQL, Machine Learning, Data Analysis, Predictive Modeling, Data Visualization, Matplotlib, Seaborn, ggplot2, Data Cleaning, Feature Engineering, Collaboration, Model Evaluation, Data Security, Statistics, Problem Solving, Communication, Health Insurance","python, r, sql, machine learning, data analysis, predictive modeling, data visualization, matplotlib, seaborn, ggplot2, data cleaning, feature engineering, collaboration, model evaluation, data security, statistics, problem solving, communication, health insurance","collaboration, communication, data cleaning, data security, dataanalytics, feature engineering, ggplot2, health insurance, machine learning, matplotlib, model evaluation, predictive modeling, problem solving, python, r, seaborn, sql, statistics, visualization"
Data Center Engineer,The Bison Group,"Boca Raton, FL",https://www.linkedin.com/jobs/view/data-center-engineer-at-the-bison-group-3784394703,2023-12-17,Palm Springs,United States,Associate,Onsite,"Purpose and Scope
The Data Center Engineer will be part of a team responsible for the design, implementation, and administration of multiple data center and critical network environments for our clients. The Engineer will also take part in hybrid cloud provisioning and cloud migrations. This is a hands-on role that requires technical knowledge of data center technologies, which include all facets of a data center environment (networking, systems, and storage).
Responsibilities
Design, implement, and maintain all converged / hyperconverged & virtual infrastructure, across the data center and critical environments.
Work with the other engineering teams and vendors to repair systems, networking, and storage equipment.
Analyzes performance indicators, such as system's response time, and number of programs being processed at once, to ensure that systems are operating efficiently.
Optimizing physical and virtual systems for proper resource utilization and provide system capacity analysis & planning
VMware Sphere / vCenter for provisioning Windows operating system on ESXi hosts
Manage the configuration and administration of high performance, mass storage arrays in SAN deployment and configuration in an Enterprise Infrastructure
Configure and support enterprise storage devices utilizing iSCSI, Fiber Channel, CIFS, NFS, FcoE protocols
Perform on-prem to cloud migrations
Develops, implements and oversees policies and procedures to ensure consistent storage provisioning, uptime, regulatory compliance and data protection.
Manage Windows server environments
Assist with the installation, removal, cabling, and troubleshooting of all equipment within data centers.
Support design and implement backup/disaster recovery solutions
Perform capacity planning and assist with storage forecasting
Plan and execute upgrades, patches and problem troubleshooting
Managing data migration for clients between old and new infrastructures.
Initiates system tests to detect errors or failed systems.
Handling escalations from internal team as well as clients.
Respond to alarms and events 24 x 7 for all data center infrastructure systems as part of an on call rotation.
Job Requirements
MUST HAVE STRONG EXPERIENCE WITH VMware!
Strong analytical and problem-solving skills.
Strong verbal and written communication skills.
In depth knowledge of EMC/Pure storage systems.
Maintain current knowledge of new hardware and software.
Must have knowledge of networking.
Experience
5+ years experience working in a datacenter production environment including network, storage & system technologies.
5+ years experience managing, configuring, or supporting VMware and Windows systems.
5+ years experience with spreadsheet and drawing software (Excel, Visio, etc.).
Vendor certifications in (Microsoft, Cisco, VMWare, or DELL/EMC or Pure).
Show more
Show less","VMware, vCenter, Windows, ESXi, SAN, iSCSI, Fiber Channel, CIFS, NFS, FcoE, EMC, Pure storage systems, Microsoft, Cisco, DELL/EMC, Excel, Visio","vmware, vcenter, windows, esxi, san, iscsi, fiber channel, cifs, nfs, fcoe, emc, pure storage systems, microsoft, cisco, dellemc, excel, visio","cifs, cisco, dellemc, emc, esxi, excel, fcoe, fiber channel, iscsi, microsoft, nfs, pure storage systems, san, vcenter, visio, vmware, windows"
Business Data Analyst - ADV,Pacer Staffing LLC,"Palm Beach Gardens, FL",https://www.linkedin.com/jobs/view/business-data-analyst-adv-at-pacer-staffing-llc-3742890107,2023-12-17,Palm Springs,United States,Associate,Onsite,"Description
Performs routine business analysis using various techniques, e.g. statistical analysis, explanatory and predictive modeling, and data mining. Researches best practices and supports developing the solutions and recommendations for the current business operations. May work with the internal or external client to identify analytical requirements. May provide business data interpretation. Additionally, may assist in developing or implementing systems to capture business operation information
Key Responsibilities Will Include But Not Be Limited To
Analyzing large data sets and identifying trends and insights
Working with a cross-functional team to gather and clean-up organizational data
Work with cross-functional teams to support strategic planning
Perform project management roles including following up on tasks and organizing sub-teams.
Performs other related duties as needed by the team Key Skills
:
Strong analytical, strategic and creative problem-solving skills.
Resourceful; comfortable reaching out to a large variety of people for info and support.
Team player with a positive, collaborative work ethic.
Cross-functional leadership skills.
Business owner mentality, willingness to roll up your sleeves and do whatever is necessary.
Extremely strong with Excel and Power BI, PPT a plus.
Ability to work with sensitive data.
Minimum of 5 years of experience in a similar role. Bachelor's degree preferred. The position is open to remote work. Charlotte, NC, or Palm Beach Gardens, FL preferred for training purposes.
Show more
Show less","Statistical analysis, Explanatory modeling, Predictive modeling, Data mining, Business analysis, Project management, Excel, Power BI, PPT, Business intelligence, Data visualization, Data interpretation, Communication skills, Teamwork, Problemsolving skills, Analytical skills, Strategic thinking, Creative problemsolving","statistical analysis, explanatory modeling, predictive modeling, data mining, business analysis, project management, excel, power bi, ppt, business intelligence, data visualization, data interpretation, communication skills, teamwork, problemsolving skills, analytical skills, strategic thinking, creative problemsolving","analytical skills, business analysis, business intelligence, communication skills, creative problemsolving, data interpretation, data mining, excel, explanatory modeling, powerbi, ppt, predictive modeling, problemsolving skills, project management, statistical analysis, strategic thinking, teamwork, visualization"
Cloud Data Platform Engineer,Kforce Inc,"Palm Beach Gardens, FL",https://www.linkedin.com/jobs/view/cloud-data-platform-engineer-at-kforce-inc-3746282775,2023-12-17,Palm Springs,United States,Associate,Onsite,"Responsibilities
Kforce has a client that is seeking a Cloud Data Platform Engineer in Palm Beach, FL. Duties Include:
Cloud Data Platform Engineer will focus on proactive and reactive end to end FPLS data optimizations
Ingestion and pipeline technology expert (EMR, Glue, Step Functions, etc.)
Build scalable data monitoring, architecture, and governance
FinOps expert in cloud data -in motion- and -at rest- cost management
FPLS EDP liaison - Focus on databases, storage, and performance optimizations (S3, DynamoDB, RDS, Redshift) - this is most important
As a Cloud Data Platform Engineer, you will query performance tuning, optimizations, and code quality
Implementing database standards (Performance testing, SonarQube)
Requirements
To be considered for this position, candidates must have experience in a similar role, or they must possess significant knowledge, experience, and abilities to successfully perform the responsibilities listed
Relevant education and/or training will be considered a plus
The pay range is the lowest to highest compensation we reasonably in good faith believe we would pay at posting for this role. We may ultimately pay more or less than this range. Employee pay is based on factors like relevant education, qualifications, certifications, experience, skills, seniority, location, performance, union contract and business needs. This range may be modified in the future.
We offer comprehensive benefits including medical/dental/vision insurance, HSA, FSA, 401(k), and life, disability & ADD insurance to eligible employees. Salaried personnel receive paid time off. Hourly employees are not eligible for paid time off unless required by law. Hourly employees on a Service Contract Act project are eligible for paid sick leave.
Note: Pay is not considered compensation until it is earned, vested and determinable. The amount and availability of any compensation remains in Kforce's sole discretion unless and until paid and may be modified in its discretion consistent with the law.
This job is not eligible for bonuses, incentives or commissions.
Kforce is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.
Salary: $120,000 - $120,000 per year
Show more
Show less","Cloud Data Platform, Data Optimization, EMR, Glue, Step Functions, Data Monitoring, Architecture, Governance, FinOps, FPLS EDP, Databases, Storage, Performance Optimization, S3, DynamoDB, RDS, Redshift, Query Performance Tuning, Database Standards, Performance Testing, SonarQube","cloud data platform, data optimization, emr, glue, step functions, data monitoring, architecture, governance, finops, fpls edp, databases, storage, performance optimization, s3, dynamodb, rds, redshift, query performance tuning, database standards, performance testing, sonarqube","architecture, cloud data platform, data monitoring, data optimization, database standards, databases, dynamodb, emr, finops, fpls edp, glue, governance, performance optimization, performance testing, query performance tuning, rds, redshift, s3, sonarqube, step functions, storage"
Data Scientist,Kforce Inc,"Juno Beach, FL",https://www.linkedin.com/jobs/view/data-scientist-at-kforce-inc-3708914251,2023-12-17,Palm Springs,United States,Associate,Onsite,"Responsibilities
Kforce has a client that is seeking a Data Scientist in Juno Beach, FL. Duties Include:
Data Scientist will develop data science models and algorithms best suited to a particular scenario
Lead processing, cleansing, and verifying the integrity of data used for analysis for data driven solutions
Solicit technical requirements and recommend appropriate technical architecture to satisfy solution requirements
As a Data Scientist, you will create and maintain the target solution design & architecture
Document reference architecture and solution implementations
Requirements
4+ years of core data science and computer vision modeling experience
Experience with AWS tech stack, regression modeling and classification modeling
Experience with EDA and data plotting
Strong knowledge of data science, machine learning, AI domain skills
Strong knowledge of using different data gathering and model classification techniques
High level of knowledge to assess and evaluate ML performance and effectiveness of ML models
Able to explain various modeling techniques and methodologies
The pay range is the lowest to highest compensation we reasonably in good faith believe we would pay at posting for this role. We may ultimately pay more or less than this range. Employee pay is based on factors like relevant education, qualifications, certifications, experience, skills, seniority, location, performance, union contract and business needs. This range may be modified in the future.
We offer comprehensive benefits including medical/dental/vision insurance, HSA, FSA, 401(k), and life, disability & ADD insurance to eligible employees. Salaried personnel receive paid time off. Hourly employees are not eligible for paid time off unless required by law. Hourly employees on a Service Contract Act project are eligible for paid sick leave.
Note: Pay is not considered compensation until it is earned, vested and determinable. The amount and availability of any compensation remains in Kforce's sole discretion unless and until paid and may be modified in its discretion consistent with the law.
This job is not eligible for bonuses, incentives or commissions.
Kforce is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.
Salary: $55 - $65 per hour
Show more
Show less","Data Science, Computer Vision, AWS, Regression Modeling, Classification Modeling, EDA, Data Plotting, Machine Learning, Artificial Intelligence, Data Gathering, Model Classification, ML Performance Assessment","data science, computer vision, aws, regression modeling, classification modeling, eda, data plotting, machine learning, artificial intelligence, data gathering, model classification, ml performance assessment","artificial intelligence, aws, classification modeling, computer vision, data gathering, data plotting, data science, eda, machine learning, ml performance assessment, model classification, regression modeling"
"Data Scientist, gt.school (Remote) - $60,000/year USD",Crossover,"West Palm Beach, FL",https://www.linkedin.com/jobs/view/data-scientist-gt-school-remote-%2460-000-year-usd-at-crossover-3783188378,2023-12-17,Palm Springs,United States,Associate,Remote,"Crossover is the world's #1 source of full-time remote jobs. Our clients offer top-tier pay for top-tier talent. We're recruiting this role for our client, gt.school. Have you got what it takes?
Are you ready to revolutionize education with the power of cutting-edge technology? We're seeking strategic problem solvers who thrive on developing and implementing innovative solutions and are eager to apply the power of generative AI to supercharge their work. Your consistent high performance and dedication will drive our mission to revolutionize education.
At gt.school, we're on a mission to accelerate the pace of learning by 5-10X, harnessing the incredible potential of technology, algorithms, and generative AI. We are creating a platform where students can discover, explore, and learn at an unprecedented rate. You'll play a pivotal role in bringing this vision to life. You'll have the opportunity to build new tools and operational processes, as well as streamline existing workflows for maximum efficiency.
But that's not all! At gt.school, we believe in your growth and development. You'll receive regular coaching and training from a team of global tech experts and operations professionals, ensuring your continuous improvement. We value collaboration and provide a supportive environment where you can meet with team members to work together on exciting projects.
If you're eager for a role that combines research, strategy, product development, and data science in the ever-evolving realm of education and technology, seize the moment and apply below. Let's embark on this exhilarating journey together!
What You Will Be Doing
Engineering generative AI prompts in Python/JavaScript/JSON
Analyze AI outputs and iterate on prompts to reach desired outcomes
Leverage generative AI and technology to develop new educational tools, iterate on current products, and finetune AI training models
Research topics to support the product engineering team
What You Won’t Be Doing
Collecting data or performing administrative tasks - we have a junior operations analyst team to do this type of work.
Working on unimportant projects - you'll be tasked with projects that are mission-critical
Data Scientist Key Responsibilities
Create technical functionalities that revolutionize the way education is delivered.
Basic Requirements
Excellent written and verbal English communication skills
Proficient in Python, R, JavaScript, or another OOP language
2+ years of work experience working in a technical field (science, technology, programming, financial services, data science, etc.)
Consistently able to work 40 hours per week, Monday through Friday, between 6AM and 6PM CST
Must be a proactive communicator who can effectively manage multiple priorities and stakeholders simultaneously
About Gt.school
GT School is an EdTech Startup that leverages technology, AI, and subject matter experts to cultivate a new way of learning. Our unique approach leverages 50+ years of learning science, cutting-edge data analytics and high-performance coaching. In doing so, we can help students learn more, learn faster, and learn better - and have fun while doing it. We are a remote-first company that hires globally via Crossover.
There is so much to cover for this exciting role, and space here is limited. Hit the Apply button if you found this interesting and want to learn more. We look forward to meeting you!
Working with Crossover
This is a full-time (40 hours per week), long-term position. The position is immediately available and requires entering into an independent contractor agreement with Crossover. The compensation level for this role is $30 USD/hour, which equates to $60,000 USD/year assuming 40 hours per week and 50 weeks per year. The payment period is weekly. Consult www.crossover.com/help-and-faqs for more details on this topic.
What to expect next:
You will receive an email with a link to start your self-paced, online job application.
Our hiring platform will guide you through a series of online “screening” assessments to check for basic job fit, job-related skills, and finally a few real-world job-specific assignments.
Important!
If you do not receive an email from us:
First, emails may take up to 15 minutes to send, refresh and check again.
Second, check your spam and junk folders for an email from Crossover.com, mark as “Not Spam” since you will receive other emails as well.
Third, we will send to whatever email account you indicated on the Apply form - by default, that is the email address you use as your LinkedIn username and it might be different than the one you have already checked.
If all else fails, just reset your password by visiting https://www.crossover.com/auth/password-recovery if you already applied using LinkedIn EasyApply.
Crossover Job Code: LJ-4880-US-WestPalm-DataScientist.008
Show more
Show less","Python, JavaScript, JSON, Generative AI, Algorithms, Data Science, Education, OOP, Research, Product Development, Technical Writing, Communication, Data Analysis","python, javascript, json, generative ai, algorithms, data science, education, oop, research, product development, technical writing, communication, data analysis","algorithms, communication, data science, dataanalytics, education, generative ai, javascript, json, oop, product development, python, research, technical writing"
Database Engineer,Steneral Consulting,"Juno Beach, FL",https://www.linkedin.com/jobs/view/database-engineer-at-steneral-consulting-3724243851,2023-12-17,Palm Springs,United States,Mid senior,Onsite,"Location: 700 UNIVERSE BLVD.Juno Beach FL USA 33408 - Onsite - Local candidates only
Position Specific Description
At Client, we are taking our Database Engineers Team to the next level. These talented employees are the drivers of analyzing, designing, developing, testing, debugging, implementing, and integrating new or existing operational efficiencies into the technical infrastructure and existing business processes. In this role you will primarily work with the many different Database technologies but with a focus on Oracle and PostgreSQL. You will also be adding value by designing and developing solutions to improve and enhance our operational processes. You will also drive efficiencies by working with application vendors for issue resolution and provides technical leadership to his/her team and customers. You will use your solutions-oriented approach and high work ethic, along with excellent communication and process management skills, to mentor internal customers and teammates on the Database platforms.
Please note this role provides 2nd level support on a rotational basis along with development activities.
Required Qualifications Include
Deep knowledge in installing, configuring and maintaining enterprise Database Technology environments such as Oracle and PostgreSQL in an on premise and AWS Cloud infrastructure
Extensive knowledge installing, upgrading and performing Disaster Recovery efforts with Oracle DB's
SQL Tuning and Database Performance Evaluation and Configuration
Systems engineering experience, including AIX / Linux performance, memory management, I/O & Performance tuning, security, clusters and troubleshooting.
Experience working directly with customers, resolving database, systems and infrastructure-related issues and also providing systems architecture leadership
Working knowledge of relational database internals (locking, consistency, serialization, disaster recovery)
Coding skills in at least one scripting language a must (TCL, Perl, Python, PowerShell, etc.)
Minimum 5 years of experience engineering, administering and managing multiple relational database technologies (e.g. Oracle, PostgreSQL, etc)
Experience in Agile/Scrum methodology
Experience in shell and python scripting with focus on DevOps tools, CI/CD
Significant exposure in various OS administration tasks
Analytical Problem Solving
Effective Written Communications
Effective Verbal Communications
Preferred Qualifications
Bachelor's or Equivalent Experience
Experience:5+ years
Show more
Show less","* Database Engineering, * Database Technologies, * Oracle, * PostgreSQL, * AWS Cloud, * Disaster Recovery, * SQL Tuning, * Database Performance Evaluation, * Systems Engineering, * AIX, * Linux, * Performance Tuning, * Security, * Clusters, * Troubleshooting, * Systems Architecture, * Relational Database Internals, * Coding, * Scripting Languages, * TCL, * Perl, * Python, * PowerShell, * Agile/Scrum Methodology, * DevOps Tools, * CI/CD, * OS Administration","database engineering, database technologies, oracle, postgresql, aws cloud, disaster recovery, sql tuning, database performance evaluation, systems engineering, aix, linux, performance tuning, security, clusters, troubleshooting, systems architecture, relational database internals, coding, scripting languages, tcl, perl, python, powershell, agilescrum methodology, devops tools, cicd, os administration","agilescrum methodology, aix, aws cloud, cicd, clusters, coding, database engineering, database performance evaluation, database technologies, devops tools, disaster recovery, linux, oracle, os administration, performance tuning, perl, postgresql, powershell, python, relational database internals, scripting languages, security, sql tuning, systems architecture, systems engineering, tcl, troubleshooting"
Data Analyst,BlueTeam,"Boca Raton, FL",https://www.linkedin.com/jobs/view/data-analyst-at-blueteam-3787754638,2023-12-17,Palm Springs,United States,Mid senior,Onsite,"Data Analyst - Private Sector – BlueTeam
We are…
BlueTeam
is a U.S. based service provider specializing in commercial restoration, construction and roofing — both domestic and international — and our clients are the world’s premier commercial portfolio owners and operators.
We are a turnkey provider for portfolio managers, owners, asset managers and operators, and we continue to grow our services to meet our clients’ needs. Our continued expansion of services, unrelenting project performance and transparent billing practices have allowed
BlueTeam
to become and remain an industry leader for clients in the hospitality, senior care, multi-family, industrial and other commercial property sectors.
Join the exciting BlueTeam!
The Data Analyst will be responsible for collecting and analyzing data across multiple business platforms to make informed decisions and/or assist other team members and leadership in making sound decisions.
What You’ll Do….
Collect and analyze large sets of data to identify patterns, trends, and insights
Develop reports and dashboards to monitor key sales metrics and provide insights to stakeholders
Monitor and report on key metrics across marketing channels, campaigns, and user experiences (i.e., Google Analytics, Social Media Metrics, Event Management)
Extract, interpret and communicate significant patterns of visitor behavior from web/mobile data that identifies meaningful business opportunities including both marketing compliance and user experiences/functionality
Create analytical insights from our customers, marketing, and CRM data to drive a more effective go-to-market strategy
Support Sales and Marketing leadership by maintaining reports and dashboards that provide a clear understanding of customer acquisition costs and campaign effectiveness
Maintain and improve monthly, quarterly, and yearly forecasts against team budgets and quotas
Develop and implement data collection systems and other strategies to optimize efficiency and data quality
Identify and recommend new data collection sources and data analysis tools
Support ad hoc data analysis requests as needed
Perform other duties and responsibilities as assigned
What You’ll Need…
Qualifications
Data analytics certification is a plus
Proficient with Excel and other MS Office applications.
Strong analytical and problem-solving skills with the ability to work with large data sets
Experience with data visualization and dashboard development (Power BI, Google Analytics, Hootsuite, CRM)
Understanding of SQL and NoSQL database design and construction.
Knowledge or experience with data science languages such as MQL, R or Python is a plus
Ability to communicate complex findings to both technical and non-technical stakeholders
Strong organizational skills and attention to detail
Excellent written and verbal communication skills
Education and Experience
Bachelor’s degree in mathematics, Statistics, Computer Science, or related field
3+ years of experience in data analysis, business intelligence, or related field
What You’ll Get In Return…
Competitive Salary
Full range of Benefits including Medical, Dental, Vision, Short Term Disability (STD), Long Term Disability (LTD), Supplemental and Voluntary Life Insurance, 401(K) with Employer Match and Paid Time Off (PTO)
About Blue Team:
BlueTeam’s Mission is to cultivate and maintain mutually beneficial relationships with our clients. In order to accomplish that mission, BlueTeam strives to be mindful, respectful and understanding of our clients and their business needs.
Becoming valuable partners with our clients starts with building the best team possible: recruiting top talent, training them, supporting them, listening to them, and empowering them to help our clients solve their restoration, construction and roofing needs. BlueTeam is not just a “team” but a family based on our core values: integrity, transparency, and accountability.
We believe that with quality people and teamwork, we will achieve our goals of being a successful and well-structured profitable organization that will treat our clients fairly and with mutual respect, while recognizing our staff for their efforts and contributions.
BlueTeam is an Equal Opportunity Employer.
Powered by JazzHR
ZgWXFJfa0S
Show more
Show less","Excel, MS Office, SQL, NoSQL, Power BI, Google Analytics, Hootsuite, CRM, MQL, R, Python, Data Analytics, Data analysis, Business intelligence, Data science, Data visualization, Dashboard development, Analytical skills, Problemsolving skills, Communication skills, Organizational skills, Attention to detail, Mathematics, Statistics, Computer Science","excel, ms office, sql, nosql, power bi, google analytics, hootsuite, crm, mql, r, python, data analytics, data analysis, business intelligence, data science, data visualization, dashboard development, analytical skills, problemsolving skills, communication skills, organizational skills, attention to detail, mathematics, statistics, computer science","analytical skills, attention to detail, business intelligence, communication skills, computer science, crm, dashboard development, data science, dataanalytics, excel, google analytics, hootsuite, mathematics, mql, ms office, nosql, organizational skills, powerbi, problemsolving skills, python, r, sql, statistics, visualization"
Sr. Data Platform Engineer,EverBright,"Juno Beach, FL",https://www.linkedin.com/jobs/view/sr-data-platform-engineer-at-everbright-3785568130,2023-12-17,Palm Springs,United States,Mid senior,Onsite,"EverBright is a clean energy company on a mission to accelerate the decarbonization of residences and businesses across the United States. We are a leading provider of financing for residential solar, storage, and energy efficiency products, fueling our rapid growth through innovative software solutions. We understand that the energy systems of tomorrow will be clean, distributed, and powered by advanced technology.
As a Senior Data Platform Engineer, you will play a critical role in designing, building, and maintaining the data infrastructure for our organization, employing a strong combination of data engineering and software engineering skills as we build out our new data platform. You will be responsible for developing and maintaining scalable and reliable data pipelines that enable data science, analytics, software, and business teams to build products and make critical data-driven strategic decisions. Your role will involve collaborating with cross-functional teams to identify business requirements and translate them into technical solutions.
You will also be responsible for designing and implementing data models and warehousesthat can efficiently handle large-scale data. In addition to technical expertise, you should have strong leadership and communication skills to effectively lead and mentor junior data engineers on the team. You should be comfortable working in an agile development environment and have experience with agile methodologies.
WHAT YOU'll DO
Design, build, and maintain scalable and reliable data pipelines, infrastructure and services
Develop and maintain data models and data warehouses
Lead and mentor junior data engineers on the team
Continuously evaluate and improve data infrastructure to ensure scalability and reliability
Qualifications
Bachelor's degree in Computer Science or other related Engineering field
4+ years of experience in data engineering
7+ years experience software engineering or related field
Strong proficiency in SQL and programming languages such as Python, Java or Scala
Hands-on experience with data warehousing and data modeling, with a strong understanding of relational database schema design
Experience building systems with cloud providers such as AWS or Azure
Good knowledge of the current data engineering landscape + available tools
Strong leadership and communication skills
Show more
Show less","Data Engineering, Software Engineering, Data Pipelines, Data Science, Analytics, Agile Development, Data Warehousing, Data Modeling, SQL, Python, Java, Scala, Cloud Providers (AWS Azure), Data Infrastructure","data engineering, software engineering, data pipelines, data science, analytics, agile development, data warehousing, data modeling, sql, python, java, scala, cloud providers aws azure, data infrastructure","agile development, analytics, cloud providers aws azure, data engineering, data infrastructure, data science, datamodeling, datapipeline, datawarehouse, java, python, scala, software engineering, sql"
Sr Data Engineer,"Voloridge Investment Management, LLC","Jupiter, FL",https://www.linkedin.com/jobs/view/sr-data-engineer-at-voloridge-investment-management-llc-3755189806,2023-12-17,Palm Springs,United States,Mid senior,Onsite,"At Voloridge Investment Management our quantitative systems are deeply dependent on vast quantities of data.
The Senior Data Engineer
must understand the many different and evolving use cases for data at Voloridge and design systems that supply high-performance datasets for advanced analytics. In this role the Sr. Data Engineer / Architect will provide mentorship and impart experience to the data engineering team.
Summary Of Job Functions
Collaborate effectively with Stakeholders, Project Managers, Software Engineers, Data Analysts, QA Analysts, DBAs, and Data Engineers
Build and maintain data pipelines based on functional and non-functional requirements
Proactively seek out information and overcome obstacles to deliver projects efficiently
Ensure that data pipelines incorporate best practices related to performance, scaling, extensibility, fault tolerance, instrumentation, and maintainability
Ensure that data pipelines are kept simple and not overly engineered
Produce and maintain design and operational documentation
Analyze complex data problems and engineer elegant solutions
Stay abreast of emerging technologies and make relevant recommendations
Upgrade existing data models and pipelines leveraging newer features and techniques
Work in a Kanban environment
Mentor less experienced data engineers
Participate in engineering standards and best practices evolution
Participate in an on-call rotation
Lead investigations to troubleshoot pipeline issues
Minimum Requirements
10+ years with hands-on data engineering and deep knowledge of data architecture fundamentals including:
Extensive experience building ETL/ELT pipelines from a variety of data sources
Broad experience with SQL Server 2019+, including advanced SQL Server features such as Table Partitioning, Columnstore
Deep knowledge and measurable experience in performance tuning TSQL, execution plan analysis blocking/deadlock analysis and index optimization
Extensive experience using SSMS to create and maintain SQL Server tables, views, functions, stored procedures, and user-defined table types
Comprehensive experience with data modeling indexes, Temporal tables, CLR, and Service Broker
Deep understanding of the development of data pipelines with either SSIS or Python and building data pipelines using multiple external data sources and transport mechanisms
Strong initiative, collaboration, accountability, impartiality, and communication
Strong analytical skills, a real passion for working with data and strong interest in solving data problems
Strong track record for judging core requirements and meeting deadlines
Experience managing master data
Experience writing C#, PowerShell, and Python
Experience with Git source control integration with SSMS
Experience working in a Kanban SDLC and a strong understanding of traditional Kanban SDLC workflows
Experience with deploying changes through segregated Development, QA, UAT and Production SDLC stages
Experience owning mission-critical processes
Bachelor’s degree in Computer Science, Information Systems, or related disciplines
Ability to work onsite in our Jupiter, FL office
Preferred Skills And Previous Experience
Python programming using libraries such as Pandas, Numpy, csv, Traceback, JSON, PyODBC, Math
Experience with source code branching and pull requests / code reviews
Experience with AWS
Experience working with trading / financial / investment / accounting data
Experience with tools such as Red Gate, Grafana, OpsGenie and JAMS
Experience with MPP databases such as Greenplum
MS/PhD in Computer Science, Information Systems, or related disciplines
Compensation And Benefits
Highly competitive base salary
Profit sharing bonus
Health, dental, vision, life, and disability insurance
401K
Additional Information
Voloridge Investment Management is an SEC registered investment advisor. A private investment company founded in 2009, our mission is to deliver superior risk-adjusted returns for qualified investors, using advanced proprietary modeling technology, conservative investment tactics and sophisticated risk management.
Voloridge Investment Management is an Equal Opportunity Employer. All qualified applicants are encouraged to apply and will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected veteran status, or any other legally protected characteristic or status.
Show more
Show less","Data Engineering, Data Architecture, ETL/ELT Pipelines, SQL Server, TSQL, SSIS, Python, Git, Kanban SDLC, AWS, Pandas, Numpy, csv, Traceback, JSON, PyODBC, Math, Red Gate, Grafana, OpsGenie, JAMS, Greenplum, Hadoop, Hive, Pig, Spark","data engineering, data architecture, etlelt pipelines, sql server, tsql, ssis, python, git, kanban sdlc, aws, pandas, numpy, csv, traceback, json, pyodbc, math, red gate, grafana, opsgenie, jams, greenplum, hadoop, hive, pig, spark","aws, csv, data architecture, data engineering, etlelt pipelines, git, grafana, greenplum, hadoop, hive, jams, json, kanban sdlc, math, numpy, opsgenie, pandas, pig, pyodbc, python, red gate, spark, sql server, ssis, traceback, tsql"
"Data Analyst - 3 positions _ Deerfield Beach, FL Corporate (Hybrid)",Ekodus INC.,"Deerfield Beach, FL",https://www.linkedin.com/jobs/view/data-analyst-3-positions-deerfield-beach-fl-corporate-hybrid-at-ekodus-inc-3726333253,2023-12-17,Palm Springs,United States,Mid senior,Onsite,"Title: Data Analyst - 3 positions
Location: Deerfield Beach, FL Corporate HDQ
Duration: 12 Months
Job Description
Sr. Data Analyst will be responsible developing an understanding of the relationship of the data that supports business processes and applications. The analyst will gain insights into the data and the movement of the data from data entry to final data destinations in the back-end files and reporting systems. Candidates must possess the technical capabilities to extract, retrieve and analyze data on their own. Be able to dive in and research new and complex data sources to understand how the data is moving in the system and how the data is stored in order to identify any anomalies that may exist. The analyst should possess strong interpersonal communication skills to be able to bridge the gap between business and technical users to relay both business and technical concepts as it relates to data. The analyst will be required to document findings and ensure that mappings and lineage are well documented and stored into knowledge storage repositories. This assignment is for a project that will interact with business and IT to understand business processes and the related data. Must be able to be on a hybrid schedule (Deerfield Beach office) with flexibility to be in the office more, as required.
Comments For Suppliers
The first 3 weeks will be required to be in the Deerfield office. Candidates will need to have hardware to support a windows-based environment. Good video and internet connectivity are critical. Candidates are expected to attend meetings as requested and work within defined business hours eastern standard.
Skills
Additional Skills:
Strong technical knowledge / expertise; experienced in data research, strong technical requirements gathering; strong SQL knowledge and data retrieval skills required; knowledge of COBOL / DB2 / Informatica a plus. Ability to use SAS is a plus.
Please share resume at msharma@ekodusinc.com or career@ekodusinc.com.
Show more
Show less","Data Analysis, Data Extraction, Data Retrieval, SQL, COBOL, DB2, Informatica, SAS","data analysis, data extraction, data retrieval, sql, cobol, db2, informatica, sas","cobol, data extraction, data retrieval, dataanalytics, db2, informatica, sas, sql"
Sr. Data Analyst,INSPYR Solutions,"Deerfield Beach, FL",https://www.linkedin.com/jobs/view/sr-data-analyst-at-inspyr-solutions-3779674995,2023-12-17,Palm Springs,United States,Mid senior,Onsite,"Job Title: Sr. Data Analyst
Location:
Hybrid - Deerfield Beach, FL
Duration:
12 months+ Contract to hire
Compensation:
70-80/hr
Work Requirements:
US Citizen, GC Holders or Authorized to Work in the U.S.
Key Skills
Data Management
SQL
Data Modeling
Data Quality Analysis
Strong verbal and written communication/presentation skills
Job Description
The Senior Data Analyst is at the forefront of DRIVE data efforts. The role is accountable for working with multiple business process teams, who are presently capturing the current state, to understand what entities are being used and how they are leveraged throughout various business functions. Once this foundation is understood, the role will be key in future decision-making and provide feedback on how processes or data may need to change as new ERP concepts are introduced into the environment. Finally, this position will help oversee how changes are implemented regarding data, to help identify and troubleshoot issues during the build process.
Responsibilities
Capture entity and relationship from business operations
Capture data states as entities move through a business process
Document key terms and definitions within a department or business process for data governance
Identify and document data requirements
Comprehend cross-functional data needs, and present/articulate findings to an internal and external audience
Provide input to report disposition/development projects to improve the efficiency of data analysis
Work with business and IT partners to conclude the best solutions to merge the ERP process with existing business functions
Review and provide feedback on the source to target mappings based on designs
Test and troubleshoot data issues using SQL and/or reporting tools
Interact with various teams and partners to ensure alignment on all the above activities
About INSPYR Solutions
As a leading information technology partner, we connect top IT talent with our clients to provide innovative business solutions through our IT Staffing, Professional Services, and Infrastructure Solutions divisions. We understand and value the unique needs of highly-skilled information technology professionals in the industry and always strive to stay above the curve. Our company was founded on the following core values: Be the Best, Understand the Urgency, Never Ever Give Up, Have the Courage to Excel, and Make a Contribution . We take pride in our business model and strive to create a positive workplace environment through an exemplary culture.
INSPYR Solutions provides Equal Employment Opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability, or genetics. In addition to federal law requirements, INSPYR complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities.
Show more
Show less","Data Management, Data Modeling, Data Quality Analysis, SQL, Data Governance, Data Requirements, Data Needs, Report Development, ERP, Data Analysis, Data Issues, Testing, Troubleshooting, Teamwork, Alignment","data management, data modeling, data quality analysis, sql, data governance, data requirements, data needs, report development, erp, data analysis, data issues, testing, troubleshooting, teamwork, alignment","alignment, data governance, data issues, data management, data needs, data quality analysis, data requirements, dataanalytics, datamodeling, erp, report development, sql, teamwork, testing, troubleshooting"
Clinical Data Analyst,HCA Florida JFK Hospital,"Atlantis, FL",https://www.linkedin.com/jobs/view/clinical-data-analyst-at-hca-florida-jfk-hospital-3759048293,2023-12-17,Palm Springs,United States,Mid senior,Onsite,"Description
Introduction
Are you looking for a work environment where diversity and inclusion thrive? Submit your application for our Clinical Data Analyst opening with HCA Florida JFK Hospital today and find out what it truly means to be a part of the HCA Healthcare team.
Benefits
HCA Florida JFK Hospital, offers a total rewards package that supports the health, life, career and retirement of our colleagues. The available plans and programs include:
Comprehensive medical coverage that covers many common services at no cost or for a low copay. Plans include prescription drug and behavioral health coverage as well as free telemedicine services and free AirMed medical transportation.
Additional options for dental and vision benefits, life and disability coverage, flexible spending accounts, supplemental health protection plans (accident, critical illness, hospital indemnity), auto and home insurance, identity theft protection, legal counseling, long-term care coverage, moving assistance, pet insurance and more.
Free counseling services and resources for emotional, physical and financial wellbeing
401(k) Plan with a 100% match on 3% to 9% of pay (based on years of service)
Employee Stock Purchase Plan with 10% off HCA Healthcare stock
Family support through fertility and family building benefits with Progyny and adoption assistance.
Referral services for child, elder and pet care, home and auto repair, event planning and more
Consumer discounts through Abenity and Consumer Discounts
Retirement readiness, rollover assistance services and preferred banking partnerships
Education assistance (tuition, student loan, certification support, dependent scholarships)
Colleague recognition program
Time Away From Work Program (paid time off, paid family leave, long- and short-term disability coverage and leaves of absence)
Employee Health Assistance Fund that offers free employee-only coverage to full-time and part-time colleagues based on income.
Learn More About Employee Benefits
Note: Eligibility for benefits may vary by location.
We are seeking a(an) Clinical Data Analyst for our team to ensure that we continue to provide all patients with high quality, efficient care. Did you get into our industry for these reasons? We are an amazing team that works hard to support each other and are seeking a phenomenal addition like you who feels patient care is as meaningful as we do. We want you to apply!
Job Summary And Qualifications
The Clinical Data Analyst is responsible for developing, interpreting and maintaining complex clinical databases related to performance improvement, utilization and resource management. Maintains department statistics and a variety of confidential reports.
Compiles clinical data and analyzes variation
Designs graphic reports in collaboration with Director
Computes statistics
Participates in various quality initiatives; as assigned
Communicates data in an effective, concise manner to varying audiences
Supports Quality Indicator data collection and analysis
Provides department with computer technical support as needed
Assists in developing educational materials and presentations
Maintains various databases; as assigned by Director
Trains organization in utilization of Meditech QM Module
Participates in Performance Improvement Teams as assigned
Participates in multidisciplinary metabolic & bariatric meetings at least twice per year
Works closely with clinical and administrative staff for MBS issues, peer review, and follow up
Enters MBSQIP data into the registry platform within prescribed data entry time frames
Attend Bariatric Wellness & Surgical Institute department meetings as assigned
Completion and maintenance of certification for the Metabolic & Bariatric Surgery Clinical Review (MBSCR)
Maintains knowledge of procedures that are being abstracted and provides proof of education
Self-motivated and creative in managing tools or systems to enhance data collection activities and presents data as required
Oversees and coordinates HCA internal audits
Serves as a facility & Division resource for registry and database education and guidance
What qualifications you will need:
RN or LPN with current state licensure required
Associates Degree or Master’s degree
Graduate from an accredited school of nursing
Clinical experience in any area (1-3 years minimum, 3-5 years clinical experience preferred)
Quality management experience with chart abstraction, EMR, chart review for quality and performance metrics ---- preferred
Quality Management experience, preferably with Society for Thoracic Surgeons (STS) and American College of Cardiology (ACC) databases.
HCA Florida JFK Hospital was named in honor of our 35th president. We have grown and have four Palm Beach County locations. Our main campus is a 558-bed acute care facility. We are known for services including: emergency services, cardiovascular care, neurosciences and robotic surgery. We offer orthopedic and spine care, bariatrics, behavioral health and obstetrics. HCA Florida JFK Hospital is a Joint Commission certified Primary Stroke Center. We are a certified Ventricular Assist Device program. We are an American College of Cardiology Chest Pain Center. Our hospital is an American College of Surgeons Commission on Cancer accredited Comprehensive Community Cancer Program. We offer two off-site ERs in Palm Beach Gardens and West Boynton Beach. They provide the same emergency treatment for adults and children. We are located in the city of Atlantis. We are near the coastal cities of Lake Worth and Boynton Beach. We are by the beach, restaurants and shopping. There is something for everybody. We invite you to join a dynamic team of clinical professionals in a fast-paced environment while living in Palm Beach County.
HCA Healthcare has been recognized as one of the World’s Most Ethical Companies® by the Ethisphere Institute more than ten times. In recent years, HCA Healthcare spent an estimated $3.7 billion in cost for the delivery of charitable care, uninsured discounts, and other uncompensated expenses.
""Across HCA Healthcare’s more than 2,000 sites of care, our nurses and colleagues have a positive impact on patients, communities and healthcare.
Together, we uplift and elevate our purpose to give people a healthier tomorrow.""- Jane Englebright, PhD, RN CENP, FAAN
Senior Vice President and Chief Nursing Executive
If you find this opportunity compelling, we encourage you to apply for our Clinical Data Analyst opening. We promptly review all applications. Highly qualified candidates will be directly contacted by a member of our team.
We are interviewing apply today!
We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.
Show more
Show less","RN or LPN, Clinical experience, Chart abstraction, EMR, Chart review, Quality management, Performance metrics, Society for Thoracic Surgeons (STS), American College of Cardiology (ACC), Data analysis, Reporting, Communication, Data entry, Auditing, Education, Presentations, Microsoft Office, Meditech QM Module, MBSQIP","rn or lpn, clinical experience, chart abstraction, emr, chart review, quality management, performance metrics, society for thoracic surgeons sts, american college of cardiology acc, data analysis, reporting, communication, data entry, auditing, education, presentations, microsoft office, meditech qm module, mbsqip","american college of cardiology acc, auditing, chart abstraction, chart review, clinical experience, communication, data entry, dataanalytics, education, emr, mbsqip, meditech qm module, microsoft office, performance metrics, presentations, quality management, reporting, rn or lpn, society for thoracic surgeons sts"
Senior Data Analyst (1012077),The Judge Group,"Deerfield Beach, FL",https://www.linkedin.com/jobs/view/senior-data-analyst-1012077-at-the-judge-group-3744923218,2023-12-17,Palm Springs,United States,Mid senior,Onsite,"Location:
Deerfield Beach, FL
Description
Role: Senior Data Analyst
Location: West Palm Beach, FL.
Duration: 12+ Months Contract
Position Description
Your future duties and responsibilities
Responsible for eliciting, understanding, interpreting and representing business requirements and act as the conduit between the customer and technical teams to ensure requirements are understood.
Provide subject matter expertise on the use of data as well as educate teams on business model, metadata and standards.
Responsible for understanding source systems and its data models.
Develop source to target mappings for data lineage.
Document source architecture to include data flows.
Responsible for analyzing data to validate business domains and requirements.
Responsible for data profiling and ensuring data quality requirements are accurate and complete.
Act in an advisory capacity in data model reviews, architecture approach and solution design to ensure high quality deliverables.
Responsible for partnering with management and business units on innovative ways to successfully utilize data and related tools to advance business objectives.
Works with governance council to establish data governance standards and guidelines.
Assist with business data lake testing / experimentation
Assist with coordinating data dictionary completions
Mentor Project DA resources
Experience working as a data analyst using SQL, BI and other data analysis tools
 hands-on experience working with SQL and a solid understanding of different data structures (flat files, relational, etc.)
 Understands data modeling concepts and techniques
 Experience working BI/Analytics tools such as Power BI and Tableau is a plus
 Experience with MICROSOFT DYNAMICS 360 A BIG PLUS
Contact:
nyadav02@judge.com
This job and many more are available through The Judge Group. Find us on the web at www.judge.com
Show more
Show less","Data Analyst, SQL, BI tools, Data modeling, Data profiling, Data architecture, Data migration, Data quality, Data governance, Data lake, Data dictionary, Power BI, Tableau, MICROSOFT DYNAMICS 360","data analyst, sql, bi tools, data modeling, data profiling, data architecture, data migration, data quality, data governance, data lake, data dictionary, power bi, tableau, microsoft dynamics 360","bi tools, data architecture, data dictionary, data governance, data lake, data migration, data profiling, data quality, dataanalytics, datamodeling, microsoft dynamics 360, powerbi, sql, tableau"
Master Data Analyst,"Cheney Brothers, Inc.","Riviera Beach, FL",https://www.linkedin.com/jobs/view/master-data-analyst-at-cheney-brothers-inc-3775206970,2023-12-17,Palm Springs,United States,Mid senior,Onsite,"Job Description
Job Details
Job Id:
4086
Employment Type:
Full time
Shift:
Day
Location:
Riviera Beach, FL
Travel:
None
Job Summary
Cheney Brothers is looking for hard-working, motivated individuals to join our IT team.
Job Responsibilities/Requirements
Monitor, analyze and maintain data across business systems.
Validate and submit change requests when needed.
Assist business process experts in defining standard operating procedures, training and related documentation.
Support various IT and Master Data related projects and tasks.
Serve as a liaison between the development teams and business users.
Requires strong analytical and computer skills, and the ability to problem solve.
SAP/AS400 knowledge is preferred.
This is an onsite position.**
Other duties as assigned.
About Us
Cheney Brothers, one of the country’s leading broadline distributors, is family-owned and operated. We are looking for hard-working, motivated individuals to join our team. In addition to being a “great place to work”, we offer competitive wages and a great benefits package, including a 401(k).
Applicants must be authorized to work in the United States. Cheney Brothers, Inc. maintains a drug free workplace and is proud to be an Equal Opportunity Employer prohibiting discrimination and harassment, including, but not limited to, in hiring, on the basis of race, color, sex, age, sexual orientation, religion, national origin, familial status, marital status, disability, military service, and any other legally protected status or class. *Above is not an all-inclusive list of requirements or qualifications.
Company Description
Cheney Brothers recognize the prosperity of our company is built on the efforts of our employees. We offer a high-impact, supportive culture enabling immensely talented people to be brilliant at what they do. Our Cheney Brothers family of over 3,000 dedicated associates have access to world-class leadership and learning experiences, comprehensive benefit packages, and endless opportunities to grow their career.
Cheney Brothers recognize the prosperity of our company is built on the efforts of our employees. We offer a high-impact, supportive culture enabling immensely talented people to be brilliant at what they do. Our Cheney Brothers family of over 3,000 dedicated associates have access to world-class leadership and learning experiences, comprehensive benefit packages, and endless opportunities to grow their career.
Show more
Show less","Data analysis, Change management, SOP definition, Training, Documentation, IT project management, Master data management, SAP, AS/400","data analysis, change management, sop definition, training, documentation, it project management, master data management, sap, as400","as400, change management, dataanalytics, documentation, it project management, master data management, sap, sop definition, training"
Sr. Data Analyst,Robert Half,"Deerfield Beach, FL",https://www.linkedin.com/jobs/view/sr-data-analyst-at-robert-half-3754539510,2023-12-17,Palm Springs,United States,Mid senior,Onsite,"Description
Are you adept at creating action plans based on business needs and a proficient problem-solver? The Business Analyst will be responsible for the documentation, analysis, and elicitation of business and technical needs across multiple business units. Exceptional communication skills and a proven ability to communicate technical and business requirements alike are keys to success for this Business Analyst role!
Key responsibilities
Gather, analyze and prioritize requests by collaborating with business units, development teams, and other stakeholders
Confirm that complex requirements are executable, testable, and complete, rather than ambiguous
Extracts and documents requirements through user stories, use cases, process flow diagrams, etc.
Requirements
Comfortable writing basic to moderately complex SQL queries, as well as technical skills including Word, Excel, PowerPoint, Visio, and Jira (or similar tool)
Background working with a variety of simple to complex projects
Bachelor's Degree in Computer Science, Management Information Systems, Business Administration or equivalent work experience
Experience with Agile methodology
Demonstrated ability to elicit and document requirements in a clear and concise manner
4+ years of business analysis experience
Proactive, flexible, adaptable, willing to take ownership, and strong communication and problem-solving skills
Technology Doesn't Change the World, People Do.®
Robert Half is the world’s first and largest specialized talent solutions firm that connects highly qualified job seekers to opportunities at great companies. We offer contract, temporary and permanent placement solutions for finance and accounting, technology, marketing and creative, legal, and administrative and customer support roles.
Robert Half works to put you in the best position to succeed. We provide access to top jobs, competitive compensation and benefits, and free online training. Stay on top of every opportunity - whenever you choose - even on the go.
All applicants applying for U.S. job openings must be legally authorized to work in the United States. Benefits are available to contract/temporary professionals, including medical, vision, dental, and life and disability insurance. Hired contract/temporary professionals are also eligible to enroll in our company 401(k) plan. Visit
© 2024 Robert Half. An Equal Opportunity Employer. M/F/Disability/Veterans. By clicking “Apply Now,” you’re agreeing to
Show more
Show less","Business Analysis, Requirements Elicitation, User Stories, Use Cases, Process Flow Diagrams, SQL, Word, Excel, PowerPoint, Visio, Jira, Agile Methodology, Problem Solving, Communication","business analysis, requirements elicitation, user stories, use cases, process flow diagrams, sql, word, excel, powerpoint, visio, jira, agile methodology, problem solving, communication","agile methodology, business analysis, communication, excel, jira, powerpoint, problem solving, process flow diagrams, requirements elicitation, sql, use cases, user stories, visio, word"
Sr. Data Governance Analyst,INSPYR Solutions,"Deerfield Beach, FL",https://www.linkedin.com/jobs/view/sr-data-governance-analyst-at-inspyr-solutions-3779683257,2023-12-17,Palm Springs,United States,Mid senior,Onsite,"Title: Sr. Data Governance Analyst
Location: Deerfield Beach, FL (Hybrid)
Duration: 6 months+
Compensation: $60-70/hr
Work Requirements: US Citizen, GC Holders or Authorized to Work in the US
Primary Responsibilities
Review documented metadata for accuracy, completeness, and consistency
Support build out of PowerBI report/dashboard documentation
Review and maintain Data Governance Program organizational structure, including Business Data Owner and Business Data Steward assignments
Assist Business Stewards and Subject Matter Experts with documentation of policies, standards and processes
Support training relating to data governance practices and the data governance tool usage throughout the organization
Support the development of teams across company to ensure data governance policies and practices are embedded throughout the company
Participate in design discussions relating to data structures to ensure standards are appropriately adhered to
Ensure all data assets subject to governance processes within the organization are addressed
Manage and administer data governance tool
Our benefits package includes: (EXCLUDE on perm placements)
Comprehensive medical benefits
Competitive pay, 401(k)
Retirement plan
…and much more!
About INSPYR Solutions
As a leading information technology partner, we connect top IT talent with our clients to provide innovative business solutions through our IT Staffing, Professional Services, and Infrastructure Solutions divisions. We understand and value the unique needs of highly-skilled information technology professionals in the industry and always strive to stay above the curve. Our company was founded on the following core values: Be the Best, Understand the Urgency, Never Ever Give Up, Have the Courage to Excel, and Make a Contribution. We take pride in our business model and strive to create a positive workplace environment through an exemplary culture.
INSPYR Solutions provides Equal Employment Opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability, or genetics. In addition to federal law requirements, INSPYR complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities.
Show more
Show less","Data Governance, PowerBI, Data Structures, Metadata, Training, Data Governance Tool, Documentation","data governance, powerbi, data structures, metadata, training, data governance tool, documentation","data governance, data governance tool, data structures, documentation, metadata, powerbi, training"
Master Data Analyst (3 OPENINGS!),Stellent IT,"Deerfield Beach, FL",https://www.linkedin.com/jobs/view/master-data-analyst-3-openings%21-at-stellent-it-3744092216,2023-12-17,Palm Springs,United States,Mid senior,Onsite,"Data Engineer
Location
: Deerfield Beach, FL (Hybrid) Must be onsite every day for the first 3 weeks then hybrid onsite after that)
Duration
: 12 Months
Work Requirements
: US Citizen, GC Holders
Qualifications
Strong technical knowledge / expertise.
Experienced In Data Research, Strong Technical Requirements Gathering.
strong SQL knowledge and data retrieval skills required.
Experience with:
Collibra is a plus, not required
Data Mapping
Data Mining
Data Cleansing
Data Validation
Data Integrity
knowledge of COBOL / DB2 / Informatica a plus.
Ability to use SAS is a plus.
Responsibilities
Sr. Data Engineer will be responsible developing an understanding of the relationship of the data that supports business processes and applications.
The analyst will gain insights into the data and the movement of the data from data entry to final data destinations in the back end files and reporting systems.
Candidates must possess the technical capabilities to extract, retrieve and analyze data on their own.
Be able to dive in and research new and complex data sources to understand how the data is moving in the system and how the data is stored in order to identify any anomalies that may exist.
The analyst should possess strong interpersonal communication skills to be able to bridge the gap between business and technical users to relay both business and technical concepts as it relates to data.
The analyst will be required to document findings and ensure that mappings and lineage are well documented and stored into knowledge storage repositories.
This assignment is for a project that will interact with business and IT to understand business processes and the related data.
Must be able to be on a hybrid schedule (Deerfield Beach office) with flexibility to be in the office more, as required
Wants someone to look at the SQL & understand what is happening to the data
Look at the data for patterns, be able to take it from technical terms to business terms
Needs someone technical look at the codes and do that research and translate that visual and verbally
Documentation and source of target mapping and transformations
Where is the best data source?
They document it in governance tools
Azure Data Migration is what is happening today
They have SQL databases and SAS data sets
All their current data is being lifted and put into Azure
They must be out of that environment by 2024
Modernization that are currently replacing our core data systems in mainframe, starting April 1st which is supposed to last about a year
PowerBI is a tool they need to use - SAS and Tableau and SQL but doesn't have to be a heavy hitter on that
SAS PowerBI, informatica they don't need to have all 3
She will take Informatica over SAS and PowerBI
Key thing is data mining with SQL***
Communication & Culture is KEY
Environment:
DB Visualizer, AWS, Azure, Mainframe, DB2, Terradata
Show more
Show less","Data Engineering, Data Research, SQL, Data Retrieval, Data Mapping, Data Mining, Data Cleansing, Data Validation, Data Integrity, COBOL, DB2, Informatica, SAS, Azure Data Migration, PowerBI, Tableau, DB Visualizer, AWS, Azure, Mainframe, Terradata","data engineering, data research, sql, data retrieval, data mapping, data mining, data cleansing, data validation, data integrity, cobol, db2, informatica, sas, azure data migration, powerbi, tableau, db visualizer, aws, azure, mainframe, terradata","aws, azure, azure data migration, cobol, data engineering, data integrity, data mapping, data mining, data research, data retrieval, data validation, datacleaning, db visualizer, db2, informatica, mainframe, powerbi, sas, sql, tableau, terradata"
Senior Data Engineer,Flybridge Staffing,"Palm Beach Gardens, FL",https://www.linkedin.com/jobs/view/senior-data-engineer-at-flybridge-staffing-3766973058,2023-12-17,Palm Springs,United States,Mid senior,Hybrid,"Flybridge Staffing is currently searching for a Sr Data Engineer for a client located in the Palm Beach Gardens area. This is a direct-hire position that will work off a hybrid schedule of 2 days remote. This person will design systems that supply high-performance datasets for advanced analytics. The Sr. Data Engineer / Architect will provide mentorship on Data Analysis, Data Engineering, and Data Modeling.
Experience:
BA degree and 10+ years of data engineering experience, including knowledge of data architecture.
Extensive experience building ETL/ELT pipelines from a variety of data sources.
Deep understanding of the development of data pipelines with either SSIS or Python
Broad experience with SQL Server including Columnstore, etc.
Experience with execution plan analysis blocking/deadlock analysis and index optimization.
Extensive experience using SSMS to create and maintain SQL Server tables, views, functions, stored procedures, and user-defined table types.
Comprehensive experience with data modeling indexes, Temporal tables, CLR, and Service Broker.
Experience writing C#, PowerShell, and Python.
Experience with Git source control integration with SSMS.
Experience working in a Kanban SDLC.
Experience with AWS.
Experience with Python programming libraries such as Pandas, Numpy, CSV, Traceback, JSON, PyODBC, and Math-Are nice to have.
Responsibilities
Collaborate effectively with Stakeholders, Project Managers, Software Engineers, Data Analysts, QA Analysts, DBAs, and Data Engineers.
Build and maintain data pipelines based on functional and non-functional requirements.
Proactively seek out information and overcome obstacles to deliver projects efficiently.
Ensure that data pipelines incorporate best practices related to performance, scaling, extensibility, fault tolerance, instrumentation, and maintainability.
Ensure that data pipelines are kept simple and not overly engineered.
Produce and maintain design and operational documentation.
Analyze complex data problems and engineer elegant solutions.
****NO SPONSORSHIP AVAILABLE**** US Citizen, GC, EAD only please.
If your background aligns with the above details and you would like to learn more, please submit your resume to jobs@flybridgestaffing.com
or on our website, www.flybridgestaffing.com and one of our recruiters will be in touch with you ASAP.
Follow us on LinkedIn to keep up with all our latest job openings and referral program.
Show more
Show less","Data Engineering, Data Architecture, ETL/ELT pipelines, SSIS, Python, SQL Server, Columnstore, SSMS, C#, PowerShell, Git, Kanban SDLC, AWS, Pandas, Numpy, CSV, Traceback, JSON, PyODBC, Math","data engineering, data architecture, etlelt pipelines, ssis, python, sql server, columnstore, ssms, c, powershell, git, kanban sdlc, aws, pandas, numpy, csv, traceback, json, pyodbc, math","aws, c, columnstore, csv, data architecture, data engineering, etlelt pipelines, git, json, kanban sdlc, math, numpy, pandas, powershell, pyodbc, python, sql server, ssis, ssms, traceback"
"Sr. Data Analyst || Deerfield Beach, FL (Hybrid Role)",Steneral Consulting,"Deerfield Beach, FL",https://www.linkedin.com/jobs/view/sr-data-analyst-deerfield-beach-fl-hybrid-role-at-steneral-consulting-3645642873,2023-12-17,Palm Springs,United States,Mid senior,Hybrid,"Hybrid role, 3 days onsite each week
Must be local or relocate prior to start
Need last 4 of SSN, DOB, LInkedIn and 10/10 comms
KEY SKILLS: The candidate needs to have strong T-SQL skills, and be familiar with Data Integration, Data Mapping, and general understanding of Data Analysis. STRONG COMMUNICATION SKILLS IS KEY!
Job Description
Sr. Data Analyst will be responsible developing an understanding of the business and department processes in order to provide data analysis, highlight insights, and recommend solutions/improvements that enable the department to operate more efficiently and improve performance.
Show more
Show less","TSQL, Data Integration, Data Mapping, Data Analysis, Communication","tsql, data integration, data mapping, data analysis, communication","communication, data integration, data mapping, dataanalytics, tsql"
Senior Cloud Data Engineer,BDO USA,"Boca Raton, FL",https://www.linkedin.com/jobs/view/senior-cloud-data-engineer-at-bdo-usa-3765467841,2023-12-17,Palm Springs,United States,Mid senior,Hybrid,"Job Description
Job Summary:
This position will work with cutting edge technology, deliver high quality solutions across various industries, and oversee team(s) on engagements that range in size and scope. This position will receive continuous career development opportunities, given the size and potential of client engagements. This role will perform hands-on delivery of data analytics projects, contributing to the development and unit testing of solutions.
Job Duties
Designs and implements best in class data ingestion strategies, data warehouse and data mart structures, semantic layers and models, visualizations, streaming processes, API integrations, and automation (RPA) solutions for end-to-end data analytics solutions on primarily, but not limited to, cloud analytics platforms such as Azure and AWS
Listens to client needs to align solution with business requirements and delivery schedule
Creates written functional and technical designs
Participates in project status and stand meetings, and assists with providing aggregated project status for project and program managers
Assists with SLA compliance of solutions, and performs performance tuning and optimization efforts of end-to-end solutions
Writes code using multiple languages and correctly applies frameworks, architectural patterns, and software development principles
Delivers high-performance, scalable, repeatable, and secure deliverables with broad impact (high throughput and low latency)
Assists with implementation of data governance programs and best practices
Performs the cleaning and transforming of data from source systems into analytics models
Implements models to support data visualizations and integrations
Assists with implementing DevOps, DataOps and MLOps methodologies on projects
Writes custom integration logic in applicable programming languages
Assists project managers with work breakdown structure creation, project estimation, resource staffing, workload planning and adjustments throughout the project lifecycle
Assists clients with licensing, security, and cost estimation of solutions
Performs code reviews to ensure adherence to standards
Works directly with clients and team members to establish secure data analytics platforms and infrastructure
Contributes to successful deployments of developed solutions and integration of DevOps tools
Maintains a broad and current understanding of data analytics and business intelligence strategies, cloud platforms, methodologies, and tools
Builds client relationships during project execution, effectively becoming a trusted advisor of the client
Participates in support activities for existing software solutions
Other duties as assigned
Supervisory Responsibilities
Supervises the day-to-day workload of Associates on assigned engagements to ensure that timelines and deliverables are met, and reviews work product
Education
Qualifications, Knowledge, Skills and Abilities:
High School Diploma or GED equivalent, required
Bachelor’s degree, preferred; focus in Information Systems, Data Science or Computer Science, preferred
Experience
Five (5) or more years of experience within Data Analytics, Business Intelligence, Artificial Intelligence, or Application Development, required
One (1) or more years of experience technically leading development projects, preferred
One (1) or more years of consulting experience or implementation of cloud-based data analytics solutions, preferred
Software
Strong SQL skills including Data Definition Language (DDL), Data Manipulation Language (DML), views, functions, stored procedures, or performance tuning, required
Experience with Data Warehousing, Data Modeling, Semantic Model Definition or Star Schema Construction, required
Hands on delivery experience of end-to-end cloud data analytics solutions within Azure or AWS, preferred
Experience with one (1) or more of the following computer languages, preferred:
C#
Python
Java
Scala
Experience with tabular modeling within Microsoft Fabric, Power BI, or Azure Analysis Services, preferred
Experience with Git and DevOps deployment technologies, preferred
Experience with Linux, preferred
Experience with one (1) or more of the following, preferred:
Data Lake Medallion Architecture
Batch and/or streaming data ingestion into a data lake
AI Algorithms/Machine Learning
Automation tools such as UiPath, Alteryx, etc.
Computer Vision based AI technologies
Other Knowledge, Skills & Abilities
Ability to work with a high degree of professionalism and autonomy
Excellent verbal and written communication skills
Solid organizational skills, especially the ability to meet project deadlines with a focus on details
Ability to successfully multi-task while working independently or within a group environment
Ability to work in a deadline-driven environment, and handle multiple projects simultaneously
Ability to interact effectively with people at all organizational levels of the Firm
Ability to effectively interact with a team of professionals and delegating work assignments, as needed
Ability to build and maintain strong relationships with internal and client personnel
Ability to encourage a team environment on engagements, and contribute to the professional development of assigned personnel
Keywords:
Data Analytics, Business Intelligence, BI, Synapse, IoT, Machine Learning, Data Lake, Stream, Cube, Microsoft, SQL Server, Tableau, .Net, C#, Qlik, Power BI, Machine Learning, Azure Data Factory, RedShift, UiPath, Cloud, RPA, AWS, Redshift, Kinesis, QuickSight, SageMaker, S3, Databricks, AWS Lake Formation, Snowflake, Python, Qlik, Athena, Data Pipeline, Glue, Star Schema, Data Modeling, SQL, SSIS, SSAS, SSRS, PySpark, Microsoft Fabric, dbt, Linux, Terraform, Bicep, Data Ops, Purview, Git, Delta, Pandas, Spark SQL
Individual salaries that are offered to a candidate are determined after consideration of numerous factors including but not limited to the candidate’s qualifications, experience, skills, and geography.
California Range: $111,000 - $152,000
Colorado Range: $111,000 - $152,000
New York City/ Valhalla Range: $111,000 - $152,000
Washington Range: $111,000 - $152,000
About Us
BDO delivers assurance, tax, digital technology solutions and financial advisory services to clients throughout the country and around the globe. We offer numerous industry-specific practices, world-class resources, and an unparalleled commitment to meeting our clients’ needs. We currently serve more than 400 publicly traded domestic and international clients.
Unparalleled partner-involvement
Deep industry knowledge and participation
Geographic coverage across the U.S.
Cohesive global network
Focused capabilities across disciplines
BDO brings world-class resources and exceptional service to each and every one of our clients. BDO USA is a member of BDO International, the world’s fifth largest accounting network.
BDO offers a competitive Total Rewards package that encompass so much more than – “traditional benefits”. Our wide range of rewards and our employees’ ability to customize rewards to their individual needs are two of the reasons why BDO has been honored with so many workplace awards, including 100 Best Companies for Working Parents, Working Mother 100 Best Companies, Top Entry Level Employer, 2022 National Best & Brightest Companies to Work For and more.
Some Examples Of Our Total Rewards Offerings Include
Competitive pay and eligibility for an annual performance bonus.
A 401k plan plus an employer match
Comprehensive, medical, dental, vision, FSA, and prescription insurance from day one
Competitive Paid Time Off with daily accrual from day one of employment, plus paid holidays
Paid Parental Leave
Adoption Assistance
Firm paid life insurance
Wellness programs
Additional offerings include BDO Flex, Group Legal insurance, Pet insurance and Long-Term Care Insurance
Above offerings may be subject to eligibility requirements.
Click here to find out more!
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability or protected veteran status.
""BDO USA, P.A. is an EO employer M/F/Veteran/Disability""
Show more
Show less","Data Analytics, Business Intelligence, Machine Learning, Artificial Intelligence, Data Science, Data Warehousing, Data Modeling, Semantic Model Definition, Star Schema Construction, SQL, Python, C#, Java, Scala, Power BI, Azure Analysis Services, Git, DevOps, Linux, Data Lake Medallion Architecture, Batch and/or streaming data ingestion, AI Algorithms, Automation tools, Computer Vision, Tableau, .Net, Qlik, Redshift, Kinesis, QuickSight, SageMaker, S3, Databricks, AWS Lake Formation, Snowflake, PySpark, Microsoft Fabric, dbt, Terraform, Bicep, Data Ops, Purview, Delta, Pandas, Spark SQL, SSIS, SSAS, SSRS","data analytics, business intelligence, machine learning, artificial intelligence, data science, data warehousing, data modeling, semantic model definition, star schema construction, sql, python, c, java, scala, power bi, azure analysis services, git, devops, linux, data lake medallion architecture, batch andor streaming data ingestion, ai algorithms, automation tools, computer vision, tableau, net, qlik, redshift, kinesis, quicksight, sagemaker, s3, databricks, aws lake formation, snowflake, pyspark, microsoft fabric, dbt, terraform, bicep, data ops, purview, delta, pandas, spark sql, ssis, ssas, ssrs","ai algorithms, artificial intelligence, automation tools, aws lake formation, azure analysis services, batch andor streaming data ingestion, bicep, business intelligence, c, computer vision, data lake medallion architecture, data ops, data science, dataanalytics, databricks, datamodeling, datawarehouse, dbt, delta, devops, git, java, kinesis, linux, machine learning, microsoft fabric, net, pandas, powerbi, purview, python, qlik, quicksight, redshift, s3, sagemaker, scala, semantic model definition, snowflake, spark, spark sql, sql, ssas, ssis, ssrs, star schema construction, tableau, terraform"
Senior Cloud Data Engineer,BDO USA,"West Palm Beach, FL",https://www.linkedin.com/jobs/view/senior-cloud-data-engineer-at-bdo-usa-3765471241,2023-12-17,Palm Springs,United States,Mid senior,Hybrid,"Job Description
Job Summary:
This position will work with cutting edge technology, deliver high quality solutions across various industries, and oversee team(s) on engagements that range in size and scope. This position will receive continuous career development opportunities, given the size and potential of client engagements. This role will perform hands-on delivery of data analytics projects, contributing to the development and unit testing of solutions.
Job Duties
Designs and implements best in class data ingestion strategies, data warehouse and data mart structures, semantic layers and models, visualizations, streaming processes, API integrations, and automation (RPA) solutions for end-to-end data analytics solutions on primarily, but not limited to, cloud analytics platforms such as Azure and AWS
Listens to client needs to align solution with business requirements and delivery schedule
Creates written functional and technical designs
Participates in project status and stand meetings, and assists with providing aggregated project status for project and program managers
Assists with SLA compliance of solutions, and performs performance tuning and optimization efforts of end-to-end solutions
Writes code using multiple languages and correctly applies frameworks, architectural patterns, and software development principles
Delivers high-performance, scalable, repeatable, and secure deliverables with broad impact (high throughput and low latency)
Assists with implementation of data governance programs and best practices
Performs the cleaning and transforming of data from source systems into analytics models
Implements models to support data visualizations and integrations
Assists with implementing DevOps, DataOps and MLOps methodologies on projects
Writes custom integration logic in applicable programming languages
Assists project managers with work breakdown structure creation, project estimation, resource staffing, workload planning and adjustments throughout the project lifecycle
Assists clients with licensing, security, and cost estimation of solutions
Performs code reviews to ensure adherence to standards
Works directly with clients and team members to establish secure data analytics platforms and infrastructure
Contributes to successful deployments of developed solutions and integration of DevOps tools
Maintains a broad and current understanding of data analytics and business intelligence strategies, cloud platforms, methodologies, and tools
Builds client relationships during project execution, effectively becoming a trusted advisor of the client
Participates in support activities for existing software solutions
Other duties as assigned
Supervisory Responsibilities
Supervises the day-to-day workload of Associates on assigned engagements to ensure that timelines and deliverables are met, and reviews work product
Education
Qualifications, Knowledge, Skills and Abilities:
High School Diploma or GED equivalent, required
Bachelor’s degree, preferred; focus in Information Systems, Data Science or Computer Science, preferred
Experience
Five (5) or more years of experience within Data Analytics, Business Intelligence, Artificial Intelligence, or Application Development, required
One (1) or more years of experience technically leading development projects, preferred
One (1) or more years of consulting experience or implementation of cloud-based data analytics solutions, preferred
Software
Strong SQL skills including Data Definition Language (DDL), Data Manipulation Language (DML), views, functions, stored procedures, or performance tuning, required
Experience with Data Warehousing, Data Modeling, Semantic Model Definition or Star Schema Construction, required
Hands on delivery experience of end-to-end cloud data analytics solutions within Azure or AWS, preferred
Experience with one (1) or more of the following computer languages, preferred:
C#
Python
Java
Scala
Experience with tabular modeling within Microsoft Fabric, Power BI, or Azure Analysis Services, preferred
Experience with Git and DevOps deployment technologies, preferred
Experience with Linux, preferred
Experience with one (1) or more of the following, preferred:
Data Lake Medallion Architecture
Batch and/or streaming data ingestion into a data lake
AI Algorithms/Machine Learning
Automation tools such as UiPath, Alteryx, etc.
Computer Vision based AI technologies
Other Knowledge, Skills & Abilities
Ability to work with a high degree of professionalism and autonomy
Excellent verbal and written communication skills
Solid organizational skills, especially the ability to meet project deadlines with a focus on details
Ability to successfully multi-task while working independently or within a group environment
Ability to work in a deadline-driven environment, and handle multiple projects simultaneously
Ability to interact effectively with people at all organizational levels of the Firm
Ability to effectively interact with a team of professionals and delegating work assignments, as needed
Ability to build and maintain strong relationships with internal and client personnel
Ability to encourage a team environment on engagements, and contribute to the professional development of assigned personnel
Keywords:
Data Analytics, Business Intelligence, BI, Synapse, IoT, Machine Learning, Data Lake, Stream, Cube, Microsoft, SQL Server, Tableau, .Net, C#, Qlik, Power BI, Machine Learning, Azure Data Factory, RedShift, UiPath, Cloud, RPA, AWS, Redshift, Kinesis, QuickSight, SageMaker, S3, Databricks, AWS Lake Formation, Snowflake, Python, Qlik, Athena, Data Pipeline, Glue, Star Schema, Data Modeling, SQL, SSIS, SSAS, SSRS, PySpark, Microsoft Fabric, dbt, Linux, Terraform, Bicep, Data Ops, Purview, Git, Delta, Pandas, Spark SQL
Individual salaries that are offered to a candidate are determined after consideration of numerous factors including but not limited to the candidate’s qualifications, experience, skills, and geography.
California Range: $111,000 - $152,000
Colorado Range: $111,000 - $152,000
New York City/ Valhalla Range: $111,000 - $152,000
Washington Range: $111,000 - $152,000
About Us
BDO delivers assurance, tax, digital technology solutions and financial advisory services to clients throughout the country and around the globe. We offer numerous industry-specific practices, world-class resources, and an unparalleled commitment to meeting our clients’ needs. We currently serve more than 400 publicly traded domestic and international clients.
Unparalleled partner-involvement
Deep industry knowledge and participation
Geographic coverage across the U.S.
Cohesive global network
Focused capabilities across disciplines
BDO brings world-class resources and exceptional service to each and every one of our clients. BDO USA is a member of BDO International, the world’s fifth largest accounting network.
BDO offers a competitive Total Rewards package that encompass so much more than – “traditional benefits”. Our wide range of rewards and our employees’ ability to customize rewards to their individual needs are two of the reasons why BDO has been honored with so many workplace awards, including 100 Best Companies for Working Parents, Working Mother 100 Best Companies, Top Entry Level Employer, 2022 National Best & Brightest Companies to Work For and more.
Some Examples Of Our Total Rewards Offerings Include
Competitive pay and eligibility for an annual performance bonus.
A 401k plan plus an employer match
Comprehensive, medical, dental, vision, FSA, and prescription insurance from day one
Competitive Paid Time Off with daily accrual from day one of employment, plus paid holidays
Paid Parental Leave
Adoption Assistance
Firm paid life insurance
Wellness programs
Additional offerings include BDO Flex, Group Legal insurance, Pet insurance and Long-Term Care Insurance
Above offerings may be subject to eligibility requirements.
Click here to find out more!
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability or protected veteran status.
""BDO USA, P.A. is an EO employer M/F/Veteran/Disability""
Show more
Show less","Data Analytics, Business Intelligence, AI, Machine Learning, Data Warehousing, Data Modeling, Semantic Model Definition, Star Schema Construction, Cloud Data Analytics, SQL, Data Definition Language, Data Manipulation Language, Views, Functions, Stored Procedures, Performance Tuning, C#, Python, Java, Scala, Tabular Modeling, Microsoft Fabric, Power BI, Azure Analysis Services, Git, DevOps, Linux, Data Lake Medallion Architecture, Batch Data Ingestion, Streaming Data Ingestion, Data Lake, AI Algorithms, Automation Tools, UiPath, Alteryx, Computer Vision, DataOps, Purview, Delta, Pandas, Spark SQL, PySpark, SSIS, SSAS, SSRS, dbt, Terraform, Bicep, Athena, Data Pipeline, Glue","data analytics, business intelligence, ai, machine learning, data warehousing, data modeling, semantic model definition, star schema construction, cloud data analytics, sql, data definition language, data manipulation language, views, functions, stored procedures, performance tuning, c, python, java, scala, tabular modeling, microsoft fabric, power bi, azure analysis services, git, devops, linux, data lake medallion architecture, batch data ingestion, streaming data ingestion, data lake, ai algorithms, automation tools, uipath, alteryx, computer vision, dataops, purview, delta, pandas, spark sql, pyspark, ssis, ssas, ssrs, dbt, terraform, bicep, athena, data pipeline, glue","ai, ai algorithms, alteryx, athena, automation tools, azure analysis services, batch data ingestion, bicep, business intelligence, c, cloud data analytics, computer vision, data definition language, data lake, data lake medallion architecture, data manipulation language, data pipeline, dataanalytics, datamodeling, dataops, datawarehouse, dbt, delta, devops, functions, git, glue, java, linux, machine learning, microsoft fabric, pandas, performance tuning, powerbi, purview, python, scala, semantic model definition, spark, spark sql, sql, ssas, ssis, ssrs, star schema construction, stored procedures, streaming data ingestion, tabular modeling, terraform, uipath, views"
Lead Data Engineer,Total Wine & More,"Boca Raton, FL",https://www.linkedin.com/jobs/view/lead-data-engineer-at-total-wine-more-3785559268,2023-12-17,Palm Springs,United States,Mid senior,Hybrid,"Lead Data Engineer
Total Wine & More is seeking a Lead Data Engineer with expertise in all aspects of data engineering, data management, and data analysis, to join our Data Services team in Bethesda, MD or Boca Raton, FL. We are embarking on a significant initiative to transform all aspects of data management services within Total Wine & More including enterprise data architecture, business intelligence, and data warehousing – leveraging the cloud to apply advanced analytics and data mining capabilities longer term. We are looking for an individual with experience in and who is passionate about data, data architecture, and data engineering to help execute the vision for data services at Total Wine & More. You will leverage advanced data engineering, cloud architecture, and programming skills to drive business value across all departments. You will work closely with the business, software development and support teams, infrastructure, and security staff and will enjoy learning and problem-solving in a fast-paced environment. You will report to the Principal Data Engineer.
You will:
In partnership with the Data Services leadership team, develop the vision for the next evolution of our Enterprise Data & Analytics Platform
Partner with Data Services product owners, data warehouse engineers, data scientists, and BI developers to support and deliver best-in-class reports, predictive models, datasets, and solutions
Create and manage batch and real time data pipelines
Manage and enhance our Kafka and Kubernetes implementations
Stay informed of new technologies and trends to guide the optimization of and continuous advancement of our platforms
Conduct exploratory data analysis to understand the patterns and potential business insights exhibited in the data
Be proactive and take ownership of the projects and tasks assigned
Coach junior team members and share knowledge with the broader team
Engage in implementing good security practices to handle sensitive information
You will have:
Bachelor's degree from four-year College or university in Computer Science, Technology, Business, or related field. Master’s or PhD preferred.
5+ years of experience in data engineering, data architecture, data warehousing or related fields
Proven track record in building impactful platforms and solutions using cloud-native tools
Experience using ETL/ELT tools – Airflow, Data Fusion, Dataprep, Informatica, Fivetran
Experience using Apache Kafka or Confluent Kafka or Google Pub/Sub
Experienced with general programing languages, such as Python, SQL, and SQL-like query languages for NoSQL databases
Experience using MapReduce or distributed computing technologies like PySpark, Apache Beam
Experience in leveraging the cloud in the development of data & analytics platforms
Familiarity with Dev-ops, Kubernetes, Docker
Experience with CI/CD including GitHub, Cloud Build, Jenkins
Passionate about data, data engineering, and platform development
Strong analytic skills, attention to detail, ability to multi-task, troubleshoot and problem solve along with the ability to make recommendations based on analysis are extremely important
Must possess excellent verbal, written, and presentation communication skills
We offer
Paid Time Off (PTO)
Generous store discounts
Health care plans (medical, prescription, dental, and vision)
401(k), HSA, FSA, Pretax commuter benefits
Disability & life insurance coverage
Paid parental leave
Pet insurance
Critical illness and accident insurance
Discounted home and auto insurance
College tuition assistance
Career development & product training
Consumer classes
& More!
Grow with us
Total Wine & More is the country's largest independent retailer of fine wine, beer and spirits, and we continue to grow our footprint year over year. Total Wine offers exciting and unique career opportunities across the country and in our corporate office. Our strength is our people. We have a commitment to training and career growth, all in an environment that values new ideas and teamwork. If you share our entrepreneurial spirit and a passion for providing best-in-class customer experience, take a moment to apply or learn more at
www.TotalWine.com/About-Us/Careers
!
Total Wine & More considers several factors when establishing compensation. Estimated salaries determined by third parties have not been validated by Total Wine & More. Total Wine & More is an equal opportunity employer and all qualified applicants will receive consideration for employment without discrimination based on race, color, religion, national origin, sex, sexual orientation, age, marital status, veteran status, disability, or any other characteristic protected by applicable law. Total Wine & More makes reasonable accommodations during all aspects of the employment process, including during the interview process. Total Wine & More is a Drug Free Workplace.
The information provided above indicates the general nature and level of work required of the position and is not a comprehensive list of all responsibilities or qualifications. Benefits list is only a highlight of some of the benefits offered to team members; eligibility for certain benefits apply.
About Total Wine & More
Total Wine & More is America‘s Wine Superstore®—the country‘s largest independent retailer of fine wine. We started in 1991 when brothers David and Robert Trone opened two wine stores in Delaware. Today, our typical store carries more than 8,000 wines from every wine-producing region in the world. In addition, Total Wine & More carries more than 2,500 beers, from America‘s most popular beers to hard-to-find microbrews and imports, and more than 3,000 different spirits from every price range and category.
Our strength is our people. We are always looking for motivated, talented team members who are interested in working for a company with entrepreneurial spirit and a passion for providing best-in-class service. Our retail stores and corporate office (called the Store Support Center) provide opportunities for career growth and advancement. Offering competitive compensation and comprehensive benefits for qualifying positions, we strive to ensure that all Team Members feel that they are a part of the business, as they are valuable resources to our customers, co-workers, and communities.
Show more
Show less","data engineering, data management, data analysis, cloud architecture, programming, enterprise data architecture, business intelligence, data warehousing, Apache Kafka, Kubernetes, data pipelines, ETL/ELT tools, Python, SQL, NoSQL, MapReduce, PySpark, Apache Beam, DevOps, Docker, CI/CD, GitHub, Cloud Build, Jenkins","data engineering, data management, data analysis, cloud architecture, programming, enterprise data architecture, business intelligence, data warehousing, apache kafka, kubernetes, data pipelines, etlelt tools, python, sql, nosql, mapreduce, pyspark, apache beam, devops, docker, cicd, github, cloud build, jenkins","apache beam, apache kafka, business intelligence, cicd, cloud architecture, cloud build, data engineering, data management, dataanalytics, datapipeline, datawarehouse, devops, docker, enterprise data architecture, etlelt tools, github, jenkins, kubernetes, mapreduce, nosql, programming, python, spark, sql"
Data Analyst,Groupe SII,"Delray Beach, FL",https://www.linkedin.com/jobs/view/data-analyst-at-groupe-sii-3745310661,2023-12-17,Palm Springs,United States,Mid senior,Hybrid,"SII Belgium is currently seeking a highly skilled and experienced Big Data to join our team. As a Big Data Analyst, you will be responsible for designing, developing, and implementing big data solutions to support business objectives. You will work with large, complex data sets and be responsible for analyzing and interpreting data to identify trends and insights. . You would join various teams working in the Energy, Retail, Banking or Telecom sectors.
SII is a European digital and engineering services company of 16,000 experts. SII Belgium supports large businesses in adopting new technologies and innovation management. SII values empowerment and entrepreneurship, providing a fulfilling work environment for employees. Our mission is to contribute to the design and development of future products and services for customers. SII is committed to a sustainable digital world and is recognized as a great workplace.
Key Responsibilities
Develop and maintain big data solutions to support business objectives
Analyze and interpret large, complex data sets to identify trends and insights
Collaborate with cross-functional teams to identify and implement data-driven solutions
Create and maintain data visualization and reporting tools to support decision-making
Integration between enterprise architecture and data collection.
Stay up to date with industry trends and technologies to identify new opportunities for data analysis
Qualifications
You hold a bachelor’s or master’s degree in IT.
Minimum of 3 years of experience working with data technologies such as Hadoop, Spark, Hive…
Strong analytical skills and experience using statistical techniques to interpret data
Experience with data visualization and reporting tools such as Tableau and Power BI
Excellent communication and collaboration skills
Strong problem-solving abilities and attention to detail
You are fluent in Dutch and/or French, and fluent in English.
We Offer
A challenging and varied role in a growing company
Opportunity to work with the latest technologies
Competitive salary and benefits
Opportunity to improve your skills and knowledge through training and education
Warm and open company culture, supported by an international group
If you are passionate about Data and want to work in a dynamic and fast-paced environment, we encourage you to apply for this exciting opportunity at SII Belgium.
Show more
Show less","Hadoop, Spark, Hive, Tableau, Power BI, Data Analytics, Data Visualization, Statistical Techniques, Data Interpretation, DataDriven Solutions, Enterprise Architecture, Data Collection, Data Analysis, Big Data Solutions","hadoop, spark, hive, tableau, power bi, data analytics, data visualization, statistical techniques, data interpretation, datadriven solutions, enterprise architecture, data collection, data analysis, big data solutions","big data solutions, data collection, data interpretation, dataanalytics, datadriven solutions, enterprise architecture, hadoop, hive, powerbi, spark, statistical techniques, tableau, visualization"
Business Data Analyst,SPECTRAFORCE,"Waynesburg, PA",https://www.linkedin.com/jobs/view/business-data-analyst-at-spectraforce-3756756504,2023-12-17,Uniontown,United States,Mid senior,Onsite,"Title: Business Data Analyst
Location: Remote (West Coast time ideal)
Duration: 12 Months
Pay Rate: $48 per hr
Summary
Senior Healthcare Business/Data Analyst with Athena IDX database backend and RCM Module experience, these skills are required. This Resource will serve as an SME supporting our development team with navigating the backend/data environment. Centricity application is also comparable to Athena and would be acceptable. The main function of a business operations specialist is to maintain vendor relationships, acting as a business analyst for non-IT projects or filling organizational roles above an administrative level, but below a management level. Other responsibilities may include maintaining all project management processes, including project goals, deadlines, metrics and budget. The business operations specialist usually reports to a project manager or director.
Job Responsibilities
Review and approve contracts, cost proposals and contract supplements.
Approve project and work breakdown structures.
Review and approve the documentation of business processes.
Approve project budgets and expenditures, monitor transaction controls and costs against budgets.
Predict potential budget overruns and offer solutions.
Consult with corporate offices, project managers, auditors, client and sub-consultant representatives regarding future proposals.
Prepare sub-agreements and review and input sub-consultant invoices to ensure contact compliance.
Skills:
Verbal and written communication skills, attention to detail, customer service and interpersonal skills.
Strong ability to work independently and manage one's time.
Strong knowledge of business and management principles involved in strategic planning, resource allocation, and production methods.
Strong ability to apply accounting and mathematical principles to work as needed.
Strong ability to analyze business trends and project future revenues and expenses.
Previous experience with computer applications, such as Microsoft Word and Excel; Enterprise Application experience a plus.
Education/Experience:
Bachelor's degree in business management, economics, finance, accounting or relevant field required.
About Us
Established in 2004, SPECTRAFORCE is one of the largest and fastest-growing diversity-owned staffing firms in the US. SPECTRAFORCE is built on the concept of “human connection,” defined by our branding tagline NEWJOBPHORIA®, which is the excitement of bringing joy and freedom to the work lifestyle so our people (and clients) can reach their highest potential. Our entire workflow cultivates NEWJOBPHORIA® with candidates and employees throughout their engagement with SPECTRAFORCE. http://www.spectraforce.com
We have built our business by providing talent and project-based solutions, including Contingent, Permanent, and Statement of Work (SOW) services to over 130 clients in the US, Canada, Puerto Rico, Costa Rica, and India. Key industries that we service include Technology, Financial Services, Life Sciences, Healthcare, Telecom, Retail, Utilities and Transportation.
Benefits: SPECTRAFORCE offers ACA compliant health benefits as well as dental, vision, accident, and hospital indemnity insurances. Additional benefits SPECTRAFORCE offers to the eligible employees include commuter benefits, 401K plan with matching and a referral bonus program. SPECTRAFORCE offers unpaid leave as well as paid sick leave when required by law.
Equal Opportunity Employer: SPECTRAFORCE is an equal opportunity employer and does not discriminate against any employee or applicant for employment because of race, religion, color, sex, national origin, age, sexual orientation, gender identity, genetic information, disability or veteran status, or any other category protected by applicable federal, state, or local laws. Please contact Human Resources at nahr@spectraforce.com if you require reasonable accommodation.
Show more
Show less","Business Analysis, Data Analysis, Athena IDX, RCM Module, Centricity, Microsoft Word, Microsoft Excel, Business Management, Economics, Finance, Accounting","business analysis, data analysis, athena idx, rcm module, centricity, microsoft word, microsoft excel, business management, economics, finance, accounting","accounting, athena idx, business analysis, business management, centricity, dataanalytics, economics, finance, microsoft excel, microsoft word, rcm module"
Data Analyst - SQL & Power BI,Energy Jobline,"Peterborough, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-analyst-sql-power-bi-at-energy-jobline-3781316545,2023-12-17,Peterborough, United Kingdom,Mid senior,Onsite,"A healthcare provider are looking for an Information Analyst with skills in SQL and either SSRS or Power BI to join their established Management Information team, where you'll be responsible for delivering timely and accurate data and reporting solutions for users across the organisation.
This organisation are headquartered in Hertfordshire, although this is a home-based role, so you can be based anywhere in the UK. It's likely you will be asked to travel to their Head Office around twice per year for full IT team meetings, and this will be fully expensed.
We are looking for an individual with strong SQL and T-SQL skills in order to effectively work with the data in their SQL and Snowflake database, as well as experience with either SSRS or Power BI to produce insightful reports and visualisations - currently they're on a journey to migrate most of their SSRS reports over to Power BI, with any new reports being created in Power BI.
This is a really exciting opportunity for an ambitious data professional to progress their career with an organisation who will be committed to your ongoing professional development, with the chance to work towards your Microsoft certifications - you'll have the opportunity to complete your Power BI and Snowflake certifications, and get your ITIL qualifications too.
Requirements
Strong SQL and T-SQL skills - including complex queries, stored procedures, tables, views etc.
SSRS or Power BI Report builder experience (including DAX)
Excellent Communication, Problem Solving And Stakeholder Management SkillsBenefits
Salary of £25-32,000, depending upon experience
25 days' annual leave plus bank holidays plus day off for your birthday (increasing by 1 day for every 2 years' service up to 28 days)
Pension with 5% employer contribution and 3% employee contribution
Enhanced maternity package
Death in service (2X salary)
Learning and development opportunities
Please Note: This is a permanent role for UK residents only. This role does not offer Sponsorship. You must have the right to work in the UK with no restrictions. Some of our roles may be subject to successful background checks including a DBS and Credit Check.
Nigel Frank are the go-to recruiter for Power BI and Azure Data Platform roles in the UK, offering more opportunities across the country than any other. We're the proud sponsor and supporter of SQLBits, Power Platform World Tour, the London Power BI User Group, Newcastle Power BI User Group and Newcastle Data Platform and Cloud User Group. To find out more and speak confidentially about your job search or hiring needs, please contact me directly at
Show more
Show less","SQL, TSQL, SSRS, Power BI, DAX, Snowflake, Microsoft Certifications, ITIL, Queries, Stored Procedures, Tables, Views, Data Visualization, Reporting, Communication, Problem Solving, Stakeholder Management","sql, tsql, ssrs, power bi, dax, snowflake, microsoft certifications, itil, queries, stored procedures, tables, views, data visualization, reporting, communication, problem solving, stakeholder management","communication, dax, itil, microsoft certifications, powerbi, problem solving, queries, reporting, snowflake, sql, ssrs, stakeholder management, stored procedures, tables, tsql, views, visualization"
Data Analyst - SQL & Power BI,Nigel Frank International,"Peterborough, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-analyst-sql-power-bi-at-nigel-frank-international-3780309693,2023-12-17,Peterborough, United Kingdom,Mid senior,Onsite,"A healthcare provider are looking for an Information Analyst with skills in SQL and either SSRS or Power BI to join their established Management Information team, where you'll be responsible for delivering timely and accurate data and reporting solutions for users across the organisation.
This organisation are headquartered in Hertfordshire, although this is a home-based role, so you can be based anywhere in the UK. It's likely you will be asked to travel to their Head Office around twice per year for full IT team meetings, and this will be fully expensed.
We are looking for an individual with strong SQL and T-SQL skills in order to effectively work with the data in their SQL and Snowflake database, as well as experience with either SSRS or Power BI to produce insightful reports and visualisations - currently they're on a journey to migrate most of their SSRS reports over to Power BI, with any new reports being created in Power BI.
This is a really exciting opportunity for an ambitious data professional to progress their career with an organisation who will be committed to your ongoing professional development, with the chance to work towards your Microsoft certifications - you'll have the opportunity to complete your Power BI and Snowflake certifications, and get your ITIL qualifications too.
Requirements:
Strong SQL and T-SQL skills - including complex queries, stored procedures, tables, views etc.
SSRS or Power BI Report builder experience (including DAX)
Excellent communication, problem solving and stakeholder management skills
Benefits:
Salary of £25-32,000, depending upon experience
25 days' annual leave plus bank holidays plus day off for your birthday (increasing by 1 day for every 2 years' service up to 28 days)
Pension with 5% employer contribution and 3% employee contribution
Enhanced maternity package
Death in service (2X salary)
Learning and development opportunities
Please Note: This is a permanent role for UK residents only. This role does not offer Sponsorship. You must have the right to work in the UK with no restrictions. Some of our roles may be subject to successful background checks including a DBS and Credit Check.
Nigel Frank are the go-to recruiter for Power BI and Azure Data Platform roles in the UK, offering more opportunities across the country than any other. We're the proud sponsor and supporter of SQLBits, Power Platform World Tour, the London Power BI User Group, Newcastle Power BI User Group and Newcastle Data Platform and Cloud User Group. To find out more and speak confidentially about your job search or hiring needs, please contact me directly at v.simpson@nigelfrank.com
Show more
Show less","SQL, TSQL, SSRS, Power BI, DAX, Snowflake, Microsoft Certifications, ITIL","sql, tsql, ssrs, power bi, dax, snowflake, microsoft certifications, itil","dax, itil, microsoft certifications, powerbi, snowflake, sql, ssrs, tsql"
Senior Data Analyst,GL Assessment,"Houghton, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-analyst-at-gl-assessment-3779842551,2023-12-17,Peterborough, United Kingdom,Mid senior,Onsite,"GL Assessment, a part of Renaissance Learning Group, are recruiting for a Senior Data Analyst
JOB PURPOSE
The Senior Data Analyst will work with the Data Science Lead to build relationships with stakeholders across the business. As a key role in delivering the Data Strategy, the successful candidate will build and deliver analytical reports or dashboards using Power BI or any other visualisation tool as would be appropriate. Moreover, the Senior Data Analyst would guide conversations on how the insights align with the business objectives and must be able to communicate in a clear and non-technical manner. In particular, the Senior Data Analyst will be presenting their findings to the business. Most of the time will be spent engaging in stakeholder discussions and the candidate must be willing to take ownership of initiatives and management.
We are looking for a Senior role with the motivation and ambition to potentially own the data analytics section of the Data team. The successful candidate will lead analytical discussions and propose data analytics solutions that would support the decision making of the business. Additionally, the Senior Data Analyst will be involved in collaboration sessions with business stakeholders to share and educate on Data and best practices.
The successful candidate will take part in building the Data Analytics roadmap with the Lead. More importantly, the Senior Data Analyst must be willing and eager to adhere to Data Governance & Ethics practices.
The Data team is part of the TECH function and works alongside other teams. The Senior Data Analyst may be required to attend other team's ceremonies for the duration of a project which require cross collaboration from different teams.
RESPONSIBILITIES:
Extract data from different data sources and merge together in an efficient and accurate manner.
Use Python for data cleaning, preparation and wrangling.
Conduct & return analytical insights using Power BI tools or other visualisation as would be required.
Develop clear visualisations and reporting in a clear and concise fashion.
Work with Data Engineers to define metrics around data quality and monitoring solutions.
Participate in discussions on analytical pipelines that would facilitate the business.
Research and identify new data sources to improve data quality.
Collaborate with different department to define KPIs.
Familiar with data validation methods.
Build relationships with internal and external stakeholders.
Own any code or data tables created.
Good communication skills to collaborate with technical and non-technical stakeholders.
Understand day-to-day issues that impact the business and propose solutions.
Train end-users on how to interact with Power BI reports and dashboards.
Eager to engage in Data Governance & Ethics discussions around proper use and share of the data.
Skills, Knowledge And Behaviours
2+ years of experience as Data Analyst or similar role.
Experience with Microsoft Power BI, Power Apps and other Microsoft Power Platform tools.
Knowledge of Python and SQL, Git
Experience UI/UX in Data.
Strong analytical and problem-solving skills.
Worked in an Agile set up.
Strong communication & presentation skills.
Business oriented Data Analyst.
Knowledge or willingness to learn about Data Governance.
Attention to details.
Ability to work autonomously.
Education And Qualifications
Bachelor's degree in Computer Science, Information Systems, related field or equivalent.
Microsoft Power BI certification.
Microsoft Azure certification.
This role is based on hybrid basis from our Houghton-le-Spring office.
Benefits
Private Medical Scheme - Single membership
Medical Cash Plan - Single Membership. Entitles employees to claim back expenses relating to medical, optical and dental work.
Pension - Employee - 3% and Employer - 8%
Annual Leave 22 days per annum (excluding 3 days off between Christmas and the NY)
Annual Leave Purchase 5 up to days
Income Protection Scheme - 75% of basic salary (subject to scheme T's & C's)
Life Assurance Scheme Salary x 4
Summer and winter hours Friday afternoons off during summer holidays and Friday afternoons off during December
Car Scheme - Employees can hire/purchase vehicles via salary sacrifice
Cycle Scheme - Employees can hire/purchase bicycles and/or cycling gear via salary sacrifice
GymFlex - Employee can join a gym at a discounted rate via salary sacrifice
Smarttech - Employee can purchase technology and pay for it over 12 months
Flu Vaccination - Yearly flu vaccination
We have an ongoing commitment to Diversity, Equity and Inclusion and have taken strides to become a more welcoming and inclusive workplace, including the introduction of our Diversity Matters Network which is owned and driven by our staff.
We are an equal opportunities employer! We encourage and welcome applications from all underrepresented groups, as we believe and strive for community representation within our organisation.
If you have a disability and would prefer to apply in a different format or would like us to make reasonable adjustments to enable you to apply or attend any interview, please contact us and we will talk this through with you.
If you interested in hearing more, please apply today.
This role is subject to DBS and background checks
Show more
Show less","Data Analytics, Power BI, Python, SQL, Git, UI/UX, Agile, Microsoft Power Platform, Microsoft Azure, Data Governance, Data Quality, Data Cleaning, Data Wrangling, Data Visualization, KPI, Data Validation, Communication, Presentation, Problem Solving, Bachelor's Degree, Computer Science, Information Systems","data analytics, power bi, python, sql, git, uiux, agile, microsoft power platform, microsoft azure, data governance, data quality, data cleaning, data wrangling, data visualization, kpi, data validation, communication, presentation, problem solving, bachelors degree, computer science, information systems","agile, bachelors degree, communication, computer science, data cleaning, data governance, data quality, data validation, data wrangling, dataanalytics, git, information systems, kpi, microsoft azure, microsoft power platform, powerbi, presentation, problem solving, python, sql, uiux, visualization"
Senior Data Analyst,Orion Electrotech Manufacturing,"Peterborough, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-analyst-at-orion-electrotech-manufacturing-3779217221,2023-12-17,Peterborough, United Kingdom,Mid senior,Onsite,"Job Description: Senior Data Analyst (Data Modelling / BI / Reporting Specialist)
Location: Hybrid / Peterborough (one to two days onsite monthly)
Overview Of This Senior Data Analyst Role
This role offers a fully remote position with occasional onsite presence (one to two days a month) in a hybrid work setting. The upcoming project, starting in January, focuses on an ERP migration program. The role emphasizes Azure SQL server expertise and hands-on involvement in data warehousing. Candidates should possess a strong foundation in old-school SQL, complex query handling for at least 5 years, primarily within the Azure platform. Power BI mastery of at least 5 years is essential, showcasing unique and sleek dashboards, ideally deployed in commercial settings, emphasizing storytelling through data visualization.
Roles And Responsibilities Of This Senior Data Analyst Role
This role is a key member of the data development team, focusing on data warehousing, Power BI, SQL, and a blend of SQL and BI expertise. The individual will tackle complex data analysis, data cleaning, and engage in the development of data-driven strategies. The role involves deploying comprehensive SQL and BI skills, leveraging Azure-based solutions, and contributing significantly to the creation of data warehouses. Candidates should demonstrate their ability to handle challenging technical situations, showcase their contributions to various projects, and detail the technology utilized.
Key Requirements For Senior Data Analyst Role
Proficiency in SQL/BI with a focus on Azure platform (not AWS/GCP).
5+ years of experience in old-school SQL, handling complex queries, data analysis, and data cleaning.
A minimum of 5 years of hands-on experience in Power BI, demonstrating unique, commercially deployed dashboards with a strong emphasis on storytelling through data visualization.
Expertise in creating data warehouses, emphasizing the back-end processes, like the Kimble model, with demonstrated experience in 2-3 projects.
Showcase of past projects, technologies utilized, challenging situations, technical obstacles faced, and detailed contributions.
Interview Process For This Senior Data Analyst Role
Interviews are conducted entirely via Teams. Local candidates involved in on-site interviews. The process consists of a two-stage evaluation:
Initial interview focusing on experience and technology understanding.
Testing interview centered on Power BI skills, involving the creation of a Power BI dashboard. Candidates are expected to narrate the story behind their dashboard creation, elaborate on challenges faced, and explain how they overcame them.
Timeline And Offer Details
CV submissions followed by interviews are scheduled for mid-December. The aim is to conclude the initial stage before Christmas and proceed with the second stage in the new year. Interview slots are available on the 12th, 14th, or 15th of December.
Compensation For This Senior Data Analyst Role
Senior Analyst '' BI Reporting: £60,000 to £65,000 + yearly bonus of 10% (5/5 personal/business)
Additional Notes
The role is heavily focused on thorough vetting to ensure applicants can substantiate their claims. The team provides a wealth of knowledge and support, offering templates and working extensively on permissions. The successful candidate will take ownership of their assigned projects with ample support from the team.
""Thank you for your application. Due to the volume of applications, we receive, unfortunately we are not able to respond to every application personally, therefore, if you have not heard back from us within 5 working days please assume your application has been unsuccessful. To see our other available vacancies please visit our website.""
Show more
Show less","Azure SQL Server, SQL, Power BI, Data Warehousing, Data Analysis, Data Cleaning, Data Visualization, Storytelling, Kimble Model, DataDriven Strategies, AzureBased Solutions, Teams, Power BI Dashboard","azure sql server, sql, power bi, data warehousing, data analysis, data cleaning, data visualization, storytelling, kimble model, datadriven strategies, azurebased solutions, teams, power bi dashboard","azure sql server, azurebased solutions, data cleaning, dataanalytics, datadriven strategies, datawarehouse, kimble model, power bi dashboard, powerbi, sql, storytelling, teams, visualization"
Senior Data Engineer - up to GBP75k,Nigel Frank International,"Peterborough, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-up-to-gbp75k-at-nigel-frank-international-3728579890,2023-12-17,Peterborough, United Kingdom,Mid senior,Hybrid,"Are you a Data Engineer with a passion for Databricks?
My client is a leader in the legal industry and they are looking to grow their data team. They offer an amazing opportunity for a Senior Data Engineer, with heavy focus on Databricks. Within this role, you will join an established, small team of data engineers and you will be working alongside the reporting team too.
You will be developing pipelines with Databricks on premises and you will be using Azure Data Factory for orchestration too. Also, you will be supporting internal business apps as they are moving into new bespoke applications which relies on Databricks.
There is great room for progression within this role, and support and training will be provided as well.
Experience required:
Very strong experience with Databricks and SQL
Strong experience with Python / PySpark
Azure Data Factory
Excellent communication skills
This is a permanent position and pays between £60,000 and £70,000, depending on experience and the strength of your skillset.
Benefits
Well-being fund
Hybrid working (1-2x/month in London)
Great pension package
Private medical health insurance
Support and training provided
If this sounds like you, then apply NOW as my client is hiring ASAP and you don't want to miss this opportunity!
To discuss in more detail, please send your CV s.fokas@nigelfrank.com or alternatively, call Spiros Fokas on 191 243 5445
NOTE: This role is for UK Residents only and does not offer sponsorship.
Nigel Frank International is the leading Microsoft Business Intelligence recruitment firm in the UK, advertising more Business Intelligence jobs than any other agency. We deal with both Microsoft Partners & End Users throughout the UK and Europe and we have never had more live requirements jobs for Microsoft Business Intelligence professionals. By specialising solely in placing candidates in the market I have built relationships with key employers in the UK and have an unrivalled understanding of where the best opportunities & Business Intelligence jobs are.
Show more
Show less","Databricks, SQL, Python, PySpark, Azure Data Factory","databricks, sql, python, pyspark, azure data factory","azure data factory, databricks, python, spark, sql"
Interim Data Engineer,Finatal,"Cambridgeshire, England, United Kingdom",https://uk.linkedin.com/jobs/view/interim-data-engineer-at-finatal-3787121438,2023-12-17,Peterborough, United Kingdom,Mid senior,Hybrid,"Job Title:
Interim Data Engineer
Location:
Ely, Cambridgeshire (2-3 days per week in the office)
Duration:
Interim, Immediate Start (within the first 2 weeks of January)
LW1030
About the Company:
Our client, based just outside of Cambridge in Ely, is a dynamic and forward-thinking organization seeking an experienced Interim Data Engineer to spearhead a crucial project aimed at developing a 'single version of the truth data pipeline.' The company currently relies on various data sources within MS Dynamics and bespoke systems, necessitating a comprehensive ETL (Extract Transform Load) process for standardization and cleansing. The organization utilizes a Microsoft Stack, with Azure and Power BI for reporting.
Job Responsibilities:
Data Pipeline Development:
Lead the design and implementation of a robust 'single version of the truth' data pipeline.
Execute ETL processes to extract, transform, and load data from diverse sources into a standardized format.
Stakeholder Management:
Engage with C-level executives to understand reporting needs and align data engineering efforts with organizational objectives.
Collaborate with business stakeholders to ensure the delivery of clean, accurate, and timely data for reporting purposes.
Microsoft Stack Expertise:
Leverage expertise in the Microsoft Stack, particularly Azure and Power BI, for effective data processing and reporting.
Data Standardization and Cleansing:
Implement data cleansing methodologies to ensure data accuracy, consistency, and integrity.
Establish and enforce data standardization practices to facilitate seamless reporting.
Automation:
Drive the automation of reporting processes to enhance efficiency and reduce manual intervention.
Implement best practices for maintaining and updating the automated reporting infrastructure.
Qualifications and Experience:
Proven experience as a Data Engineer, with a focus on Microsoft Stack technologies.
Strong background in ETL processes, data standardization, and cleansing.
Expertise in utilizing Azure and Power BI for reporting and analytics.
Demonstrated ability to manage stakeholders, particularly at the C-level.
Effective communication skills to convey complex technical concepts to non-technical stakeholders.
Immediate availability and the ability to start within the first 2 weeks of January.
Show more
Show less","Data Engineering, ETL, Microsoft Stack, Azure, Power BI, Data Standardization, Data Cleansing, Reporting Automation","data engineering, etl, microsoft stack, azure, power bi, data standardization, data cleansing, reporting automation","azure, data engineering, data standardization, datacleaning, etl, microsoft stack, powerbi, reporting automation"
Data Analyst / Reporting Analyst - POWER BI / SQL,Global Technology Solutions Ltd.,"Peterborough, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-analyst-reporting-analyst-power-bi-sql-at-global-technology-solutions-ltd-3778563882,2023-12-17,Peterborough, United Kingdom,Mid senior,Hybrid,"Data Analyst / Reporting Analyst - POWER BI / SQL
Salary £24,000 - £28,000
Hybrid 2 days per week onsite - Hampton, Peterborough
Working Hours - 08:00 - 16:30
A fantastic opportunity has arisen for a Reporting / Data Analyst to join well established blue chip organisation .
As a Reporting Analyst you will have a love of data and spreadsheets.
Our Reporting Analyst provides timely, accurate and insightful reports for the business. This will include gathering data from various sources and in multiple formats, collating the data and generating reports in suitable format for the audience. You will actively identify any data conflicts and patterns to ensure we have data accuracy, integrity and the best possible datasets for our reports. From time to time you will be involved in; cross-functional projects, ad-hoc report building and producing presentations for the leadership team.
Key responsibilities:
- Run daily, weekly monthly reports as requested by Team Leaders and Service Desk Managers, using Excel and CRM systems
- Report findings back to key stakeholders in a presentable fashion to suggest service improvements
- Monitoring staff performance by analysing if KPIs (Key Performance Indicators) and SLAs (Service Level Agreements) are being met, and if so, suggesting methods to address
Key skills:
- Advanced Excel reporting skills (Spreadsheet formulas, Pivot Tables, V-Lookups)
- An analytical mind with an interest in working with large amounts of data
- A data driven degree would be highly advantageous (Economics, Statistical, Finance, Mathematics)
• SQL
• Power BI or Equivalent
• Degree or background in data / Mathematics
Essential Attributes:
• Possess an analytical mind, be passionate about data and solution focused.
• Methodical, quality driven with a high attention to detail.
• An effective communicator both verbal and written with all levels of stakeholders.
• Able to communicate with both technical and non-technical audiences.
• A proactive, sound, and impartial decision maker, able to highlight improvement opportunities and work collaboratively to deliver them.
So if you are a Data Analyst/Reporting Analyst looking for your next exciting career move please apply now!
Show more
Show less","SQL, Power BI, Excel, Reporting, Data Analysis, Advanced Excel Reporting, Spreadsheet Formulas, Pivot Tables, VLookups, Key Performance Indicators (KPIs), Service Level Agreements (SLAs), CRM Systems, Presentations, DataDriven Degree, Economics, Statistics, Finance, Mathematics, Analytical Mind, CrossFunctional Projects, AdHoc Report Building","sql, power bi, excel, reporting, data analysis, advanced excel reporting, spreadsheet formulas, pivot tables, vlookups, key performance indicators kpis, service level agreements slas, crm systems, presentations, datadriven degree, economics, statistics, finance, mathematics, analytical mind, crossfunctional projects, adhoc report building","adhoc report building, advanced excel reporting, analytical mind, crm systems, crossfunctional projects, dataanalytics, datadriven degree, economics, excel, finance, key performance indicators kpis, mathematics, pivot tables, powerbi, presentations, reporting, service level agreements slas, spreadsheet formulas, sql, statistics, vlookups"
Lead Data Engineer,"LanceSoft, Inc.","Michigan, United States",https://www.linkedin.com/jobs/view/lead-data-engineer-at-lancesoft-inc-3764201232,2023-12-17,Interlochen,United States,Mid senior,Onsite,"Engagement Description
Responsible for leading complex projects and independently executing analyses that support the healthcare value portfolio.
Translates technical results into answers for healthcare business needs and applies a strong understanding of healthcare industry when creating those solutions.
Partners effectively outside of the business team to execute work.
Works closely with team members to fully understand the business question and source associated data required for a solution.
Executes work efficiently, bringing forward process enhancements with a continuous improvement mindset.
Top 3 Required Skills/Experience
Minimum 6 years experience in Advanced Data Analytics.
Proficient in advanced functions in SQL.
Advanced understanding of data warehousing and data mining concepts such as: DataMart design, fact tables, dimension tables, data views within data marts/data warehouses, data models such as star schemas and snowflake schemas.
Required Skills/Experience
Ability to recognize data patterns, quantify potential issues and identify solutions.
Ability to standardize, harmonize, clean, prepare and use data for developing datasets, reporting and analysis.
Ability to perform root cause analysis on external and internal processes and data to identify opportunities for improvement and answer questions
Assemble large, complex data sets that meet functional / non-functional business requirements.
Conduct data gathering and analysis to understand business requirements. Provide direction for short and long-term solutions and provide the team direction to ensure understanding of business goals and direction. Provide input from a business and IT perspective.
Preferred Skills/Experience Optional But Preferred Skills/experience. Include
Healthcare Experience
Experience with big data tools: Hadoop, Spark, and scripting languages: Python or Scala
Solid working knowledge of Microsoft tools and software (i.e., Excel, Visio, Access, PowerPoint)
Display innovation in identifying, proposing, and overseeing the execution of business solutions.
Written and verbal communication skills interpersonal skills.
Ability to work independently, or within a team environment.
Education/Certifications Include
Bachelors Degree in Computer Science, Information Systems, Business, Mathematics, Healthcare Information Technology or similar degree required. Masters Degree preferred.
Show more
Show less","Advanced Data Analytics, SQL, Data Warehousing, Data Mining, DataMart Design, Fact Tables, Dimension Tables, Data Views, Star Schemas, Snowflake Schemas, Data Patterns, Data Standardization, Data Harmonization, Data Cleaning, Data Preparation, Data Analysis, Root Cause Analysis, Data Gathering, Data Understanding, Business Requirements, Business Goals, Business Direction, Healthcare Experience, Hadoop, Spark, Python, Scala, Microsoft Tools, Software, Excel, Visio, Access, PowerPoint, Innovation, Communication Skills, Interpersonal Skills, Teamwork, Computer Science, Information Systems, Business, Mathematics, Healthcare Information Technology, Master's Degree","advanced data analytics, sql, data warehousing, data mining, datamart design, fact tables, dimension tables, data views, star schemas, snowflake schemas, data patterns, data standardization, data harmonization, data cleaning, data preparation, data analysis, root cause analysis, data gathering, data understanding, business requirements, business goals, business direction, healthcare experience, hadoop, spark, python, scala, microsoft tools, software, excel, visio, access, powerpoint, innovation, communication skills, interpersonal skills, teamwork, computer science, information systems, business, mathematics, healthcare information technology, masters degree","access, advanced data analytics, business, business direction, business goals, business requirements, communication skills, computer science, data cleaning, data gathering, data harmonization, data mining, data patterns, data preparation, data standardization, data understanding, data views, dataanalytics, datamart design, datawarehouse, dimension tables, excel, fact tables, hadoop, healthcare experience, healthcare information technology, information systems, innovation, interpersonal skills, masters degree, mathematics, microsoft tools, powerpoint, python, root cause analysis, scala, snowflake schemas, software, spark, sql, star schemas, teamwork, visio"
Sr. Data Analyst,USAFacts,"Bellevue, WA",https://www.linkedin.com/jobs/view/sr-data-analyst-at-usafacts-3760630724,2023-12-17,Kent,United States,Associate,Hybrid,"The Organization
At USAFacts, we believe facts deserve to be heard. We are on a mission to provide a comprehensive and unbiased look at our country by the numbers. Democracy thrives on serious, reasoned, and informed debate. And we’re here for it.
Founded and funded by Steve Ballmer, former CEO of Microsoft, USAFacts answers questions around government spending and outcomes. While these are basic questions, finding the answers is hard. But it’s also pretty exciting. We pull together data from thousands of jurisdictions to normalize, contextualize, and visualize these metrics so the data is approachable and understandable.
Every day, we learn something about our country, how to make complex data understandable, and how to make the American people interested in the numbers. We’re a collaborative group and are always learning from each other, too. The team is mission-driven, with no political agenda nor commercial incentive; we do this work purely as a public service.
Oh, and to top it off? USAFacts was named to Fast Company’s List of the World’s Most Innovative Companies for 2023! Learn more about us at usafacts.org and linkedin.com/company/usafacts.
We’d love for you to bring your talents and your curiosity to come make a difference!
The Opportunity
At USAFacts, we are committed to providing unbiased and transparent data about the United States. We are seeking a Senior Data Analyst who is experienced in conducting analysis of and communicating findings from government data to join our mission-driven research team.
In this role, you will conduct data analysis, develop insightful written content and data visualizations, respond to questions from policymakers through data exploration and analysis, and support the organization’s efforts to improve government data. The individual in this role contributes to the delivery of data-driven insights for the organization’s major reports and ensures that their work is aligned with audience needs and expectations. They play a key role in maintaining the trustworthiness and transparency of the organization's work.
This role
does not
include direct people management. It is a collaborative, individual contributor role.
Key Priorities & Responsibilities
Develop trustworthy, unbiased, and contextual data-driven insights by independently conducting analysis of government data; surfacing interesting findings; employing data best practices; and similar responsibilities.
Contribute to the delivery of research team products that are usable, understandable, and aligned with audience needs and expectations by independently executing user research; employing search engine optimization tools; applying learnings from the latest audience and user experience research; and similar responsibilities..
Work to establish USAFacts as a destination for civically engaged people interested in data and information about the U.S. and its government by translating data-driven insights into visualizations and written formats such as long-form reports, memos, and web pages; identifying and communicating opportunities for innovative or experimental content forms; and similar responsibilities.
Enhance the trustworthiness and transparency of USAFacts analysis content by providing feedback and support to other analysts; developing and maintaining organization-wide data analysis standards and support for their use; providing reviews of content; and similar responsibilities.
Support the organization’s public relations efforts by developing materials for organization’s leaders; conducting fact-checking of public-facing documents when required; creating presentations; and similar responsibilities.
Qualifications:
Skills
:
Understanding of basic statistical and econometric concepts and ability to use appropriate research methods
Excellent writing and research skills
Ability to analyze datasets and identify interesting findings
Attention to detail
Strong Excel skills
Ability to work with others collaboratively and effectively
Self-starter who can work in a fast-moving and evolving environment
Familiarity working with government data
Experience:
At least five (5) years of experience in policy, research, or a social science field, including internships or undergraduate research assistantships.
Education:
Bachelor's degree in public policy, statistics, economics or other social science fields
Preferred Experience, Skills, Education:
Skills, in addition to the above: Familiarity with data analysis tools and software such as R, Python, or Stata.
Education: Minor in statistics or economics, or 3-6 courses in statistics, econometrics, or data analytics.
Compensation
The starting salary range for the Senior Data Analyst role is $101,500 - $126,900, based on aptitude and experience. Over time, the employee in this role may earn up to $152,000 per year with demonstrated skill in the role and context, progress and performance against goals, and increased experience.
Featured Benefits & Perks
Medical, dental, and vision insurance with employee and dependent premiums entirely paid for by Ballmer Group
3 weeks of paid vacation annually
11 paid holidays, plus paid days off for the weekdays between Christmas and New Year
80 hours of paid sick leave annually
401(k) with 6% employer contribution
2:1 Ballmer Family Giving Match for charitable contributions
Professional development reimbursement up to $2,000 per year
Employer-paid life insurance of 3x salary to $1,000,000 ($750k guarantee issue)
Fertility & Family Building reimbursement to $40,000 (lifetime max)
Healthcare and Dependent Care Flexible Spending Account (FSA) options
Parking or transportation (Orca card) up to $250 monthly
Cash stipend of $300/month (for cell phone, wellness expenses, etc.)
Location & Hybrid Work
The position is based in Bellevue, Washington.
Our organization practices intentional hybrid work​. All USAFacts employees are expected to work on-site Tuesdays, Wednesdays, and Thursdays during overlapping core hours of 10:00 am to 3:00 pm. Mondays and Fridays remain flexible work-from-home or office workdays.
Ballmer Group/USAFacts is an equal opportunity employer and dedicated to building an inclusive workforce where diversity is valued. Individuals seeking employment at Ballmer Group are considered without regards to race, color, religion, national origin, age, sex, marital status, ancestry, physical or mental disability, veteran status, gender identity, or sexual orientation.
Show more
Show less","Data Analysis, Statistical and Econometric Concepts, Writing, Research, Excel, Collaborative Work, Attention to Detail, Government Data, R, Python, Stata, Statistics, Econometrics, Data Analytics","data analysis, statistical and econometric concepts, writing, research, excel, collaborative work, attention to detail, government data, r, python, stata, statistics, econometrics, data analytics","attention to detail, collaborative work, dataanalytics, econometrics, excel, government data, python, r, research, stata, statistical and econometric concepts, statistics, writing"
Data Engineer III,Fred Hutch,"Seattle, WA",https://www.linkedin.com/jobs/view/data-engineer-iii-at-fred-hutch-3756744979,2023-12-17,Kent,United States,Mid senior,Onsite,"Overview
Fred Hutchinson Cancer Center is an independent, nonprofit organization providing adult cancer treatment and groundbreaking research focused on cancer and infectious diseases. Based in Seattle, Fred Hutch is the only National Cancer Institute-designated cancer center in Washington.
With a track record of global leadership in bone marrow transplantation, HIV/AIDS prevention, immunotherapy and COVID-19 vaccines, Fred Hutch has earned a reputation as one of the world’s leading cancer, infectious disease and biomedical research centers. Fred Hutch operates eight clinical care sites that provide medical oncology, infusion, radiation, proton therapy and related services, and network affiliations with hospitals in five states. Together, our fully integrated research and clinical care teams seek to discover new cures to the world’s deadliest diseases and make life beyond cancer a reality.
At Fred Hutch we value collaboration, compassion, determination, excellence, innovation, integrity and respect. These values are grounded in and expressed through the principles of diversity, equity and inclusion. Our mission is directly tied to the humanity, dignity and inherent value of each employee, patient, community member and supporter. Our commitment to learning across our differences and similarities make us stronger. We seek employees who bring different and innovative ways of seeing the world and solving problems. Fred Hutch is in pursuit of becoming an anti-racist organization. We are committed to ensuring that all candidates hired share our commitment to diversity, anti-racism and inclusion.
The Data Platform Team within Fred Hutch's Information Technology Department is responsible for designing, developing, and supporting FHCC data systems and integration for both operational and analytical purposes.
Under the guidance of the Data Engineer Manager, the
Data Engineer (Data Engineer
III)
is responsible for the development and implementation of scalable, stable, and secure solutions for data acquisition, data distribution, and workflow orchestration in FHCC's data warehouse. The role will also be responsible for leading the Data Engineering function in the maintenance and optimization of existing data pipelines from a wide variety of internal and external data sources, including partner data warehouses and repositories. They will work closely with team members, end users, and other stakeholders to provide solutions that support and empower internal business partners and drive enterprise analytics and data science objectives.
Successful candidates will have strong engineering, communication, and expert level knowledge in on-prem SQL Server data systems. Familiarity with common cloud data tools, best practices and migration strategies is a plus as well as agile development methodologies.
This is a remote position.
Responsibilities
Design, implement and support ETL/ELT processes sourcing data from various internal applications and external data repositories including our partner EPIC EMR implementations
Monitor and maintain build and release pipelines in Azure Devops using continuous development best practices
Develop and support data infrastructure primarily built with Microsoft technologies
Partner with Test Engineer to target areas of the warehouse and associated processes where testing infrastructure improvements are needed
Foster a collaborative environment with those on the Data Platform team and larger IT department and Business partners
Work with Data Engineers, and systems and business analysts to support and empower technical and non-technical internal stakeholders in their analytics and reporting requirements
Communicate and resolve issues and questions during development, testing, and release of new code
Foster a collaborative environment
Qualifications
Required:
High school diploma or equivalent
Excellent knowledge and experience developing, performance tuning, and supporting on-prem data engineering systems using Microsoft SQL Server
Excellent knowledge of ETL tools and various data processing techniques and best practices
Expert in database architecture and design, performance tuning, coding, and administration
Experience with complex relational data models and very large data sets
Excellent knowledge of data warehousing design and concepts
Experience in dealing with large and complex data sets and performance tuning
Strong verbal/written communication skills, including an ability to effectively communicate with the business: technical and non-technical teams.
Ability to work independently, manage competing priorities and to adapt to new and changing technologies
Ability to manage business expectations and deliver customer-focused solutions
Strong analytical and organizational skills and an ability to multi-task
Ability to work independently and to adapt to new and changing technologies
Ability to communicate in English, both verbally and in writing
Preferred
B.Sc or M.Sc degree in quantitative field (Engineering, Computer Science, Information Systems, etc.), or equivalent training or professional experience
Prior experience in scripting languages (Powershell, Python, or similar)
Prior experience in AWS or Azure cloud data tools/technologies
Experience leading Data Engineering teams
Experience with Cerner and/or Epic healthcare systems
Familiarity with healthcare financial and/or clinical data
Familiarity with HIPAA regulations and data warehousing security best practices
A statement describing your commitment and contributions toward greater diversity, equity, inclusion, and antiracism in your career or that will be made through your work at Fred Hutch is requested of all finalists.
The annual base salary range for this position is from $120,869 to $191,027 and pay offered will be based on experience and qualifications.
Fred Hutchinson Cancer Center offers employees a comprehensive benefits package designed to enhance health, well-being, and financial security. Benefits include medical/vision, dental, flexible spending accounts, life, disability, retirement, family life support, employee assistance program, onsite health clinic, tuition reimbursement, paid vacation (12-22 days per year), paid sick leave (12-25 days per year), paid holidays (13 days per year), paid parental leave (up to 4 weeks), and partially paid sabbatical leave (up to 6 months).
Our Commitment to Diversity
We are proud to be an Equal Employment Opportunity (EEO) and Vietnam Era Veterans Readjustment Assistance Act (VEVRAA) Employer. We are committed to cultivating a workplace in which diverse perspectives and experiences are welcomed and respected. We do not discriminate on the basis of race, color, religion, creed, ancestry, national origin, sex, age, disability (physical or mental), marital or veteran status, genetic information, sexual orientation, gender identity, political ideology, or membership in any other legally protected class. We are an Affirmative Action employer. We encourage individuals with diverse backgrounds to apply and desire priority referrals of protected veterans. If due to a disability you need assistance/and or a reasonable accommodation during the application or recruiting process, please send a request to Human Resources at hrops@fredhutch.org or by calling 206-667-4700.
Show more
Show less","SQL Server, ETL/ELT, Azure Devops, Microsoft Technologies, Data Warehousing, Data Engineering, Data Infrastructure, Healthcare Financial Data, Healthcare Clinical Data, HIPAA Regulations, Data Warehousing Security, Powershell, Python, AWS, Azure, Cerner, Epic, Agile Development","sql server, etlelt, azure devops, microsoft technologies, data warehousing, data engineering, data infrastructure, healthcare financial data, healthcare clinical data, hipaa regulations, data warehousing security, powershell, python, aws, azure, cerner, epic, agile development","agile development, aws, azure, azure devops, cerner, data engineering, data infrastructure, data warehousing security, datawarehouse, epic, etlelt, healthcare clinical data, healthcare financial data, hipaa regulations, microsoft technologies, powershell, python, sql server"
Senior Data Engineer,Professional Diversity Network,"Seattle, WA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-professional-diversity-network-3788314242,2023-12-17,Kent,United States,Mid senior,Onsite,"Responsibilities
The Senior Data Engineer responsibilites include building a data processing pipeline that collects, connects, centralizes, and curates data from various internal and external sources using a variety of languages and tools to marry systems together for the Enterprise Data Warehouse. Develop highly scalable and reliable data engineering solutions for moving data efficiently across systems; design, implement, test and deploy data processing infrastructure; perform work in an Agile team setting; and break down, estimate and provide just-in-time design for small increments of work. This role is pivotal to the mission and vision of Seattle Children's Enterprise Analytics team to transform healthcare for children by providing patient safety, predictive analysis to cure diseases, lowering cost of treatement, etc.
Requirements
Required Education/Experience: - Bachelor's Degree in computer science or related field, or equivalent combination of education and experience/technical training that demonstrates analytical and technical competency - Minimum of five (5) years technology industry or related experience, including items such as: - Build highly scalable, scaled-out architectures on large scale database platforms - Experience working in a complex data infrastructure environment - Five (5) years of experience in a data engineering role - Extensive and in depth data pipeline development experience with industry standard data integration tools - Advanced competency in SQL with ability to perform query optimization in large scale database platforms - Experience in SDLC process with requirements gathering, analysis, architecture, design, implementation, testing, deployment and technical support. - Experience with any industry standard tool for Source Control and Project Management - Experience wrting test cases and test scripts for data quality assurance - Experience creating stored procedures and functions - Experience developing dimensional data model with any industry standard tool. Required Credentials: - N/A Preferred: - Experience in Healthcare or related industry - Experience utilizing Netezza, Datastage, BitBucket, JIRA, Confluence a plus - Experience productizing/automating predictive models that use R, SAS, Python, SPSS, etc. - Continuous delivery and deployment automation for analytic solutions - Familiarity with Agile framework and test driven development methodology for analytic solutions - API development - Data visualization and/or dashboard development
Min to Max Hourly Salary
$59.18 - $88.77 /hr
Min to Max Annual Salary
$123,094.00 - $184,640.00 /yr
Salary Information
This compensation range was calculated based on full-time employment (2080 hours worked per calendar year). Offers are determined by multiple factors including equity, skills, experience, and expertise, and may vary within the range provided.
The union pay ranges can be found on the Seattle Children's website here: WSNA - UFCW
Seattle Children's offers annual incentive pay based upon performance that is commensurate with the level of the position.
Disclaimer for Out of State Applicants
This compensation range is specific to Seattle, positions located outside of Seattle may be compensated differently depending on various factors
.
Benefits Information
Seattle Children's offers a generous benefit package, including medical, dental, and vision plans, 403(b), life insurance, paid time off, tuition reimbursement, and more. Click here for more information.
About Us
Hope. Care. Cure. These three simple words capture what we do at Seattle Children's -- to help every child live the healthiest and most fulfilling life possible. Are you ready to engage with a mission-driven organization that is life-changing to many, and touches the hearts of all? #HOPECARECURE
Our founding promise to the community is as valid today as it was over a century ago: we will care for all children in our region, regardless of the families' ability to pay. Together, we deliver superior patient care, advance new discoveries and treatments through pediatric research, and serve as the pediatric and adolescent, academic medical center for Washington, Alaska, Montana and Idaho - the largest region of any children's hospital in the country.
In 2022, U.S. News & World Report once again ranked Seattle Children's among the nation's best children's hospitals - for the 30th year in a row. For more than a decade, Seattle Children's has been nationally ranked in all 10 specialty areas evaluated by U.S. News & World Report. We are honored to be the top-ranked pediatric hospital in Washington and the Pacific Northwest.
As a Magnet designated institution, and classified among America's best large employers by Forbes, we recognize the importance of hiring and developing great talent to provide best-in-class care to the patients and families we serve. Our organizational DNA takes form in our core values: Compassion, Excellence, Integrity, Collaboration, Equity and Innovation. Whether it's delivering frontline care to our patients in a kind and caring manner, practicing the highest standards of quality and safety, or being relentlessly curious as we work towards eradicating childhood diseases, these values are the fabric of our culture and community. The future starts here.
Our Commitment to Diversity
Our community welcomes diverse experiences, backgrounds, and thoughts as this is what drives our spirit of inquiry and allows us to better connect with our increasingly diverse patients and families. Our organization recruits, employs, trains, compensates, and promotes without regard to race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.
The people who work at Seattle Children's are members of a community that seeks to respect and celebrate all the qualities that make each of us unique. Each of us is empowered to be ourselves within this community, which cultivates and promotes equity, diversity, and inclusion at all levels.
Seattle Children's is proud to be an Equal Opportunity Workplace and Affirmative Action Employer.
PDN-9a2ad626-95e6-4093-9be8-52640c269422
Show more
Show less","Data Engineering, Data Integration, SQL, Netezza, Datastage, BitBucket, JIRA, Confluence, R, SAS, Python, SPSS, API development, Data visualization, Dashboard development, Agile framework, Test driven development, Source Control, Project Management","data engineering, data integration, sql, netezza, datastage, bitbucket, jira, confluence, r, sas, python, spss, api development, data visualization, dashboard development, agile framework, test driven development, source control, project management","agile framework, api development, bitbucket, confluence, dashboard development, data engineering, data integration, datastage, jira, netezza, project management, python, r, sas, source control, spss, sql, test driven development, visualization"
Sr. Data Platform Engineer,EverBright,"Seattle, WA",https://www.linkedin.com/jobs/view/sr-data-platform-engineer-at-everbright-3785566237,2023-12-17,Kent,United States,Mid senior,Onsite,"EverBright is a clean energy company on a mission to accelerate the decarbonization of residences and businesses across the United States. We are a leading provider of financing for residential solar, storage, and energy efficiency products, fueling our rapid growth through innovative software solutions. We understand that the energy systems of tomorrow will be clean, distributed, and powered by advanced technology.
As a Senior Data Platform Engineer, you will play a critical role in designing, building, and maintaining the data infrastructure for our organization, employing a strong combination of data engineering and software engineering skills as we build out our new data platform. You will be responsible for developing and maintaining scalable and reliable data pipelines that enable data science, analytics, software, and business teams to build products and make critical data-driven strategic decisions. Your role will involve collaborating with cross-functional teams to identify business requirements and translate them into technical solutions.
You will also be responsible for designing and implementing data models and warehousesthat can efficiently handle large-scale data. In addition to technical expertise, you should have strong leadership and communication skills to effectively lead and mentor junior data engineers on the team. You should be comfortable working in an agile development environment and have experience with agile methodologies.
WHAT YOU'll DO
Design, build, and maintain scalable and reliable data pipelines, infrastructure and services
Develop and maintain data models and data warehouses
Lead and mentor junior data engineers on the team
Continuously evaluate and improve data infrastructure to ensure scalability and reliability
Qualifications
Bachelor's degree in Computer Science or other related Engineering field
4+ years of experience in data engineering
7+ years experience software engineering or related field
Strong proficiency in SQL and programming languages such as Python, Java or Scala
Hands-on experience with data warehousing and data modeling, with a strong understanding of relational database schema design
Experience building systems with cloud providers such as AWS or Azure
Good knowledge of the current data engineering landscape + available tools
Strong leadership and communication skills
Show more
Show less","Data Engineering, Software Engineering, Data Pipelines, Data Science, Analytics, Software Development, Data Models, Data Warehouses, Hadoop, Spark, Hive, Pig, Scala, Python, Java, SQL, NoSQL, AWS, Azure, Agile Development, Scrum, Kanban","data engineering, software engineering, data pipelines, data science, analytics, software development, data models, data warehouses, hadoop, spark, hive, pig, scala, python, java, sql, nosql, aws, azure, agile development, scrum, kanban","agile development, analytics, aws, azure, data engineering, data models, data science, data warehouses, datapipeline, hadoop, hive, java, kanban, nosql, pig, python, scala, scrum, software development, software engineering, spark, sql"
Senior Data Engineer,Seattle Children's,"Seattle, WA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-seattle-children-s-3781593972,2023-12-17,Kent,United States,Mid senior,Onsite,"Responsibilities The Senior Data Engineer responsibilites include building a data processing pipeline that collects, connects, centralizes, and curates data from various internal and external sources using a variety of languages and tools to marry systems together for the Enterprise Data Warehouse. Develop highly scalable and reliable data engineering solutions for moving data efficiently across systems; design, implement, test and deploy data processing infrastructure; perform work in an Agile team setting; and break down, estimate and provide just-in-time design for small increments of work. This role is pivotal to the mission and vision of Seattle Children's Enterprise Analytics team to transform healthcare for children by providing patient safety, predictive analysis to cure diseases, lowering cost of treatement, etc.
Requirements Required Education/Experience: - Bachelor's Degree in computer science or related field, or equivalent combination of education and experience/technical training that demonstrates analytical and technical competency - Minimum of five (5) years technology industry or related experience, including items such as: - Build highly scalable, scaled-out architectures on large scale database platforms - Experience working in a complex data infrastructure environment - Five (5) years of experience in a data engineering role - Extensive and in depth data pipeline development experience with industry standard data integration tools - Advanced competency in SQL with ability to perform query optimization in large scale database platforms - Experience in SDLC process with requirements gathering, analysis, architecture, design, implementation, testing, deployment and technical support. - Experience with any industry standard tool for Source Control and Project Management - Experience wrting test cases and test scripts for data quality assurance - Experience creating stored procedures and functions - Experience developing dimensional data model with any industry standard tool. Required Credentials: - N/A Preferred: - Experience in Healthcare or related industry - Experience utilizing Netezza, Datastage, BitBucket, JIRA, Confluence a plus - Experience productizing/automating predictive models that use R, SAS, Python, SPSS, etc. - Continuous delivery and deployment automation for analytic solutions - Familiarity with Agile framework and test driven development methodology for analytic solutions - API development - Data visualization and/or dashboard development
Min to Max Hourly Salary $59. 18 - $88.77 /hr Min to Max Annual Salary $123,094.00 - $184,640.00 /yr Salary Information This compensation range was calculated based on full-time employment (2080 hours worked per calendar year). Offers are determined by multiple factors including equity, skills, experience, and expertise, and may vary within the range provided.
The union pay ranges can be found on the Seattle Children's website here: -
Seattle Children's offers annual incentive pay based upon performance that is commensurate with the level of the position.
Disclaimer for Out of State Applicants This compensation range is specific to Seattle, positions located outside of Seattle may be compensated differently depending on various factors*.*
Benefits Information Seattle Children's offers a generous benefit package, including medical, dental, and vision plans, 403(b), life insurance, paid time off, tuition reimbursement, and more. for more information.
About Us *Hope. Care. Cure. These three simple words capture what we do at Seattle Children's -- to help every child live the healthiest and most fulfilling life possible. Are you ready to engage with a mission-driven organization that is life-changing to many, and touches the hearts of all? #HOPECARECURE*
Our founding promise to the community is as valid today as it was over a century ago: we will care for all children in our region, regardless of the families' ability to pay. Together, we deliver superior patient care, advance new discoveries and treatments through pediatric research, and serve as the pediatric and adolescent, academic medical center for Washington, Alaska, Montana and Idaho - the largest region of any children's hospital in the country.
In 2022, U.S. News & World Report once again ranked Seattle Children's among the nation's best children's hospitals - for the 30th year in a row. For more than a decade, Seattle Children's has been nationally ranked in all 10 specialty areas evaluated by U.S. News & World Report. We are honored to be the top-ranked pediatric hospital in Washington and the Pacific Northwest.
As a Magnet designated institution, and classified among America's best large employers by Forbes, we recognize the importance of hiring and developing great talent to provide best-in-class care to the patients and families we serve. Our organizational DNA takes form in our core values: Compassion, Excellence, Integrity, Collaboration, Equity and Innovation. Whether it's delivering frontline care to our patients in a kind and caring manner, practicing the highest standards of quality and safety, or being relentlessly curious as we work towards eradicating childhood diseases, these values are the fabric of our culture and community. The future starts here.
Our Commitment to Diversity Our community welcomes diverse experiences, backgrounds, and thoughts as this is what drives our spirit of inquiry and allows us to better connect with our increasingly diverse patients and families. Our organization recruits, employs, trains, compensates, and promotes without regard to race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.
The people who work at Seattle Children's are members of a community that seeks to respect and celebrate all the qualities that make each of us unique. Each of us is empowered to be ourselves within this community, which cultivates and promotes equity, diversity, and inclusion at all levels.
Seattle Children's is proud to be an Equal Opportunity Workplace and Affirmative Action Employer.
Show more
Show less","SQL, Data Integration Tools, Data Modeling, Netezza, Datastage, BitBucket, JIRA, Confluence, Agile, Test Driven Development, API Development, Data Visualization, Dashboard Development, R, SAS, Python, SPSS, Hadoop, Spark, Scala, AWS, Azure, GCP, Docker, Kubernetes, Jenkins, Git, Continuous Delivery, Continuous Deployment, DevOps","sql, data integration tools, data modeling, netezza, datastage, bitbucket, jira, confluence, agile, test driven development, api development, data visualization, dashboard development, r, sas, python, spss, hadoop, spark, scala, aws, azure, gcp, docker, kubernetes, jenkins, git, continuous delivery, continuous deployment, devops","agile, api development, aws, azure, bitbucket, confluence, continuous delivery, continuous deployment, dashboard development, data integration tools, datamodeling, datastage, devops, docker, gcp, git, hadoop, jenkins, jira, kubernetes, netezza, python, r, sas, scala, spark, spss, sql, test driven development, visualization"
