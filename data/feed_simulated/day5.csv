job_title,company,job_location,job_link,first_seen,search_city,search_country,job level,job_type,job_summary,job_skills,job_skills_raw,job_skills_cleaned
"Senior Data Engineer, Gigster Network - Chicago",Gigster,"Oakbrook Terrace, IL",https://www.linkedin.com/jobs/view/senior-data-engineer-gigster-network-chicago-at-gigster-3742674829,2023-12-17,Oswego,United States,Mid senior,Onsite,"Do you want to work on cutting-edge projects with the world’s best developers? Do you wish you could control which projects to work on and choose your own pay rate? Are you interested in the future of work and how the cloud will form teams? If so - the Gigster Talent Network is for you.
At Gigster, whether working with entrepreneurs to realize 'the next great vision' or with Fortune 500 companies to deliver a big product launch, we build really cool solutions that make a difference! Gigster builds enterprise software on cutting-edge technology, from blockchain to AI/ML to VR and more.
We are seeking a talented Senior Data Engineer with strong experience in ETL and ML Ops in the Chicago area to join our Talent Network and help shape the future of our business through valuable contributions to our projects. The successful candidate will be passionate about data and machine learning, with a strong background in Python and experience developing and implementing machine learning models.
Responsibilities
Creating, designing, and deploying ETL and ELT flows for consuming from third party data sources.
Building the tools to monitor data pipelines and performing troubleshooting and debugging.
Collaborating with data scientists and resolving data quality and performance issues.
Collaborating with software and data engineers.
Collaborating with Machine Learning Engineers to deploy, monitor, and maintain Machine Learning models in a production environment.
Leading the integration of Machine Learning models with CI/CD pipelines, ensuring consistent and seamless model versioning, testing, and roll-out.
Requirements
Minimum of 5 years of software development experience.
Strong programming skills in Python.
Experience in MLOps.
Experience analyzing large datasets and uncovering insights using tools such as Spark, Scala, or PySpark.
Experience with modern cloud platforms (AWS, Azure, or GCP).
Experience in DevOps for CI/CD, IaC (Infrastructure as Code).
Experience with modern ETL tools such as Apache Beam, Kafka, Spark, or similar.
Strong communication and collaboration skills, with the ability to work effectively in cross-functional teams.
The Gigster Talent Network is a highly curated set of the best software developers in the world. It’s not easy to become part of this select network - but when you do - you will work amongst the best from Silicon Valley and around the world.
Our model is unique in the software development industry. We do the hard work of finding the US clients and scoping their projects - and you get to choose from a large variety of ‘Gigs’. You can choose Gigs that fit your schedule - from 10, 20, or 40 hours a week. You also get to choose your pay rate. All projects are staffed with a project manager, full stack team, QA, and DevOps.
All of our projects are for top-tier US companies and are delivered with the highest quality. Projects range from developing NFT marketplaces to VR imaging for medical use to large AI/ML projects. We even produce a case study for every project delivered - so you can take that with you as part of your portfolio.
In parallel - you will have access to an exclusive and energized network of the world’s most skilled experts. Community members collaborate inside and outside of Gigs - as well as at local community events, online hackathons, competitions, etc. The Gigster Talent Network is more than a simple marketplace - it’s truly an exclusive club.
Are you talented enough to be in the club?
Show more
Show less","Python, Machine Learning, Data Science, Data Engineering, ETL, ELT, MLOps, Cloud Computing, AWS, Azure, GCP, Data Analysis, Data Pipelines, Apache Beam, Kafka, Spark, PySpark, Scala, DevOps, CI/CD, IaC (Infrastructure as Code), Communication, Collaboration, Crossfunctional Teams","python, machine learning, data science, data engineering, etl, elt, mlops, cloud computing, aws, azure, gcp, data analysis, data pipelines, apache beam, kafka, spark, pyspark, scala, devops, cicd, iac infrastructure as code, communication, collaboration, crossfunctional teams","apache beam, aws, azure, cicd, cloud computing, collaboration, communication, crossfunctional teams, data engineering, data science, dataanalytics, datapipeline, devops, elt, etl, gcp, iac infrastructure as code, kafka, machine learning, mlops, python, scala, spark"
"Senior Data Engineer, Gigster Network - Chicago",Gigster,"Hinsdale, IL",https://www.linkedin.com/jobs/view/senior-data-engineer-gigster-network-chicago-at-gigster-3742678355,2023-12-17,Oswego,United States,Mid senior,Onsite,"Do you want to work on cutting-edge projects with the world’s best developers? Do you wish you could control which projects to work on and choose your own pay rate? Are you interested in the future of work and how the cloud will form teams? If so - the Gigster Talent Network is for you.
At Gigster, whether working with entrepreneurs to realize 'the next great vision' or with Fortune 500 companies to deliver a big product launch, we build really cool solutions that make a difference! Gigster builds enterprise software on cutting-edge technology, from blockchain to AI/ML to VR and more.
We are seeking a talented Senior Data Engineer with strong experience in ETL and ML Ops in the Chicago area to join our Talent Network and help shape the future of our business through valuable contributions to our projects. The successful candidate will be passionate about data and machine learning, with a strong background in Python and experience developing and implementing machine learning models.
Responsibilities
Creating, designing, and deploying ETL and ELT flows for consuming from third party data sources.
Building the tools to monitor data pipelines and performing troubleshooting and debugging.
Collaborating with data scientists and resolving data quality and performance issues.
Collaborating with software and data engineers.
Collaborating with Machine Learning Engineers to deploy, monitor, and maintain Machine Learning models in a production environment.
Leading the integration of Machine Learning models with CI/CD pipelines, ensuring consistent and seamless model versioning, testing, and roll-out.
Requirements
Minimum of 5 years of software development experience.
Strong programming skills in Python.
Experience in MLOps.
Experience analyzing large datasets and uncovering insights using tools such as Spark, Scala, or PySpark.
Experience with modern cloud platforms (AWS, Azure, or GCP).
Experience in DevOps for CI/CD, IaC (Infrastructure as Code).
Experience with modern ETL tools such as Apache Beam, Kafka, Spark, or similar.
Strong communication and collaboration skills, with the ability to work effectively in cross-functional teams.
The Gigster Talent Network is a highly curated set of the best software developers in the world. It’s not easy to become part of this select network - but when you do - you will work amongst the best from Silicon Valley and around the world.
Our model is unique in the software development industry. We do the hard work of finding the US clients and scoping their projects - and you get to choose from a large variety of ‘Gigs’. You can choose Gigs that fit your schedule - from 10, 20, or 40 hours a week. You also get to choose your pay rate. All projects are staffed with a project manager, full stack team, QA, and DevOps.
All of our projects are for top-tier US companies and are delivered with the highest quality. Projects range from developing NFT marketplaces to VR imaging for medical use to large AI/ML projects. We even produce a case study for every project delivered - so you can take that with you as part of your portfolio.
In parallel - you will have access to an exclusive and energized network of the world’s most skilled experts. Community members collaborate inside and outside of Gigs - as well as at local community events, online hackathons, competitions, etc. The Gigster Talent Network is more than a simple marketplace - it’s truly an exclusive club.
Are you talented enough to be in the club?
Show more
Show less","Data Engineering, ETL, ELT, Machine Learning, Python, MLOps, Spark, Scala, PySpark, AWS, Azure, GCP, DevOps, CI/CD, IaC, Apache Beam, Kafka, Communication, Collaboration, NFT marketplaces, VR imaging, AI/ML","data engineering, etl, elt, machine learning, python, mlops, spark, scala, pyspark, aws, azure, gcp, devops, cicd, iac, apache beam, kafka, communication, collaboration, nft marketplaces, vr imaging, aiml","aiml, apache beam, aws, azure, cicd, collaboration, communication, data engineering, devops, elt, etl, gcp, iac, kafka, machine learning, mlops, nft marketplaces, python, scala, spark, vr imaging"
Customer Service Representative/Data Analyst/Data Entry Clerk/part time,Carrollbalistreri,"Saguenay, Quebec, Canada",https://ca.linkedin.com/jobs/view/customer-service-representative-data-analyst-data-entry-clerk-part-time-at-carrollbalistreri-3740516375,2023-12-17,Jonquière, Canada,Mid senior,Hybrid,"Summary:
We are seeking a detail-oriented and highly organized Customer Service Representative/Data Analyst/Data Entry Clerk/part time to join our team. The ideal candidate will be responsible for accurately inputting and maintaining various types of data across all industries. This role requires exceptional attention to detail, strong computer skills, and the ability to work efficiently in a fast-paced environment.
Responsibilities:
Accurately input and update data into the company's database or designated systems.Verify and review data for errors or discrepancies, ensuring data integrity and consistency.Perform regular data quality checks to identify and resolve any inconsistencies or inaccuracies.Maintain confidentiality and adhere to data protection policies while handling sensitive information.Collaborate with team members to ensure timely and accurate completion of data entry tasks.Assist in generating reports and analyzing data as required.Stay updated with industry trends and best practices related to data entry and management.
Qualifications:
High school diploma or equivalent; additional certifications in data entry or related fields are a plus.Proven experience in data entry or a similar role, preferably in a fast-paced environment.Proficient in using data entry software, spreadsheets, and other relevant tools.Excellent attention to detail and accuracy, with the ability to identify errors and inconsistencies.Strong organizational and time management skills to prioritize tasks effectively.Ability to work independently and collaboratively within a team.Excellent written and verbal communication skills.Familiarity with data protection and confidentiality regulations.
Powered by Webbtree
Show more
Show less","Data Entry, Data Analysis, Data Management, Data Quality Assurance, Data Integrity, Data Consistency, Data Confidentiality, Data Protection, Data Reporting, Data Analysis, Industry Trends, Data Entry Software, Spreadsheets, Data Management Tools, Attention to Detail, Accuracy, Error Identification, Organizational Skills, Time Management, Team Collaboration, Communication Skills","data entry, data analysis, data management, data quality assurance, data integrity, data consistency, data confidentiality, data protection, data reporting, data analysis, industry trends, data entry software, spreadsheets, data management tools, attention to detail, accuracy, error identification, organizational skills, time management, team collaboration, communication skills","accuracy, attention to detail, communication skills, data confidentiality, data consistency, data entry, data entry software, data integrity, data management, data management tools, data protection, data quality assurance, data reporting, dataanalytics, error identification, industry trends, organizational skills, spreadsheets, team collaboration, time management"
Customer Service Representative/Data Analyst/Data Entry Clerk/Urgent,Toyandsons,"Montreal, Quebec, Canada",https://ca.linkedin.com/jobs/view/customer-service-representative-data-analyst-data-entry-clerk-urgent-at-toyandsons-3750808174,2023-12-17,Dorval, Canada,Mid senior,Onsite,"Summary:
Customer Service Representative/Data Analyst/Data Entry Clerk/Urgent will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : hiring@jobsai.live
Powered by Webbtree
Show more
Show less","Data Analysis, Data Mining, Data Interpretation, Statistical Analysis, Data Visualization, SQL, R, Python, Tableau, Power BI, Data Quality Management, Data Cleansing, Data Manipulation, Data Presentation, Business Intelligence, DataDriven Decision Making, A/B Testing, Data Warehousing, ETL (Extract Transform Load), Data Integrity, Hypothesis Testing, Statistical Modeling","data analysis, data mining, data interpretation, statistical analysis, data visualization, sql, r, python, tableau, power bi, data quality management, data cleansing, data manipulation, data presentation, business intelligence, datadriven decision making, ab testing, data warehousing, etl extract transform load, data integrity, hypothesis testing, statistical modeling","ab testing, business intelligence, data integrity, data interpretation, data manipulation, data mining, data presentation, data quality management, dataanalytics, datacleaning, datadriven decision making, datawarehouse, etl extract transform load, hypothesis testing, powerbi, python, r, sql, statistical analysis, statistical modeling, tableau, visualization"
Customer Service Representative/Data Analyst/Data Entry Clerk/,Krajcikcassin,"Longueuil, Quebec, Canada",https://ca.linkedin.com/jobs/view/customer-service-representative-data-analyst-data-entry-clerk-at-krajcikcassin-3742521836,2023-12-17,Dorval, Canada,Mid senior,Onsite,"Summary:
Customer Service Representative/Data Analyst/Data Entry Clerk will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : hiring@jobsolutionsai.online
Powered by Webbtree
Show more
Show less","Data Analysis, Data Manipulation, SQL, Python, R, A/B, SQL, ETL, Tableau, Data Management, Statistical Modeling, Communication, Collaboration","data analysis, data manipulation, sql, python, r, ab, sql, etl, tableau, data management, statistical modeling, communication, collaboration","ab, collaboration, communication, data management, data manipulation, dataanalytics, etl, python, r, sql, statistical modeling, tableau"
Data Cabling Engineer,Digital Waffle,"Edinburgh, Scotland, United Kingdom",https://uk.linkedin.com/jobs/view/data-cabling-engineer-at-digital-waffle-3748511983,2023-12-17,Edinburgh, United Kingdom,Mid senior,Onsite,"Job Description
Position: Data Cabling Engineer (Temp/Contract/Freelance)
Department: Information Technology / Network Infrastructure
Location: Edinburgh
Job Summary: We are seeking a skilled and dedicated Data Cabling Engineer. The successful candidate will be responsible for the installation, testing, and termination of Cat6 and Cat6a branded copper cables within metal trunking and containment to ensure a reliable and efficient network infrastructure. The role involves working closely with the IT team to maintain high-quality data connectivity, support network expansion, and ensure optimal performance.
Responsibilities:
Cabling Installation:
Plan, install, and route Cat6 and Cat6a copper cables according to industry standards and best practices
Mount, secure, and organize cable pathways, including cable trays, conduits, and wall penetrations
Install cable connectors, outlets, and termination panels in compliance with design specifications
Termination and Labeling:
Terminate copper cables onto jacks, connectors, and patch panels with precise attention to detail and adherence to industry standards
Ensure proper labeling of cables, connectors, and outlets for easy identification and maintenance
Maintain accurate documentation of cable layouts, terminations, and labeling
Testing and Troubleshooting:
Conduct comprehensive cable testing using appropriate tools to validate signal quality, continuity, and performance
Troubleshoot and resolve connectivity issues, signal degradation, and other cable-related problems
Utilize testing equipment such as cable testers and certification tools to ensure compliance with required specifications
Quality Assurance:
Ensure all cabling work is performed to high quality standards and follows company policies and guidelines
Perform quality checks on cable installations to confirm compliance with industry standards and design specifications
Network Expansion:
Collaborate with IT team members to support network expansion projects and ensure seamless integration of new cabling
Participate in designing and implementing cable layouts for new network infrastructure components
Health and Safety:
Adhere to safety protocols and guidelines to prevent accidents and maintain a safe working environment
Operate tools and equipment safely and responsibly, following industry best practices
Requirements:
High school diploma or equivalent; additional technical certifications or coursework in cabling and network infrastructure is a plus
Proven experience as a Data Cabling Engineer or similar role, with expertise in Cat6 and Cat6a copper cable installation, termination, and testing
Familiarity with relevant cabling standards, such as TIA/EIA and ISO/IEC
Proficiency in using cable testing and certification tools
Strong understanding of network topologies, protocols, and network equipment
Ability to interpret technical drawings, schematics, and cabling diagrams
Excellent problem-solving skills with a keen attention to detail
Effective communication skills to collaborate with IT team members and project stakeholders
Ability to work independently or as part of a team in various environments, including office spaces, data centers, and remote locations
Strong commitment to safety and adherence to industry regulations
General tools required for this job:-
Cable Stripping and Cutting Tools:
Cable stripper: Used to remove the outer insulation from cables
Cable cutter: Used to cut cables to appropriate lengths
Termination Tools:
RJ45 crimping tool: Used to attach RJ45 connectors to the ends of Ethernet cables
Punch-down tool: Used for terminating cables on patch panels, keystone jacks, and outlets
Testing and Certification Tools:
Cable tester: Used to test cable continuity, signal quality, and detect faults
Cable certifier: Provides detailed analysis and certification of cable performance against industry standards
Tone generator and probe: Used to trace and identify cables within a bundle or behind walls
Labeling and Identification Tools:
Cable labels and markers: Used to label and identify cables, connectors, and outlets
Label printer: Used to create professional labels for cables and equipment
Measuring and Alignment Tools:
Tape measure or ruler: Used to measure cable lengths accurately
Level: Ensures proper alignment of cable pathways and connectors
Cable Management Tools:
Cable ties and Velcro straps: Used for bundling and organizing cables
Cable clips and mounts: Secure cables along walls, ceilings, or other surfaces
Drilling and Mounting Tools:
Power drill and bits: Used to create holes for cable pass-throughs and mounting hardware
Screwdrivers and wall anchors: Required for mounting cable trays, conduits, and outlets
Safety Gear:
Safety glasses: Protects eyes from debris and potential hazards
Work gloves: Provides hand protection during installation and cable handling
Tool Bag or Pouch:
Keeps tools organized and easily accessible during installation tasks
Documentation Tools:
Pen and notepad or mobile device: Used to document cable layouts, terminations, and labeling
Personal Protective Equipment (PPE):
Steel-toed boots: Provides foot protection in potentially hazardous environments
Hard hat: Required in construction or industrial settings
Optional Tools:
Cable fish tape or rods: Used to guide cables through walls, ceilings, or conduits
Cable lubricant: Assists in pulling cables through tight spaces
Cable toner and probe: Helps identify specific cables in a bundle
If you are a motivated Data Cabling Engineer with a passion for maintaining robust network infrastructure and ensuring seamless connectivity, we encourage you to apply. Contact Joe on joe@digitalwaffle.co.uk
Show more
Show less","Cat6, Cat6a, Copper Cables, TIA/EIA, ISO/IEC, RJ45, Patch Panels, Keystone Jacks, Cable Stripper, Cable Cutter, RJ45 Crimping Tool, PunchDown Tool, Cable Tester, Cable Certifier, Tone Generator, Probe, Cable Labels, Cable Markers, Label Printer, Tape Measure, Ruler, Level, Cable Ties, Velcro Straps, Cable Clips, Mounts, Power Drill, Drill Bits, Screwdrivers, Wall Anchors, Safety Glasses, Work Gloves, Tool Bag, Pouch, Pen, Notepad, Mobile Device, SteelToe Boots, Hard Hat, Cable Fish Tape, Rods, Cable Lubricant, Cable Toner","cat6, cat6a, copper cables, tiaeia, isoiec, rj45, patch panels, keystone jacks, cable stripper, cable cutter, rj45 crimping tool, punchdown tool, cable tester, cable certifier, tone generator, probe, cable labels, cable markers, label printer, tape measure, ruler, level, cable ties, velcro straps, cable clips, mounts, power drill, drill bits, screwdrivers, wall anchors, safety glasses, work gloves, tool bag, pouch, pen, notepad, mobile device, steeltoe boots, hard hat, cable fish tape, rods, cable lubricant, cable toner","cable certifier, cable clips, cable cutter, cable fish tape, cable labels, cable lubricant, cable markers, cable stripper, cable tester, cable ties, cable toner, cat6, cat6a, copper cables, drill bits, hard hat, isoiec, keystone jacks, label printer, level, mobile device, mounts, notepad, patch panels, pen, pouch, power drill, probe, punchdown tool, rj45, rj45 crimping tool, rods, ruler, safety glasses, screwdrivers, steeltoe boots, tape measure, tiaeia, tone generator, tool bag, velcro straps, wall anchors, work gloves"
Senior Azure Data Engineer,Energy Jobline,"Edinburgh, Scotland, United Kingdom",https://uk.linkedin.com/jobs/view/senior-azure-data-engineer-at-energy-jobline-3786096934,2023-12-17,Edinburgh, United Kingdom,Mid senior,Onsite,"Role: Senior Azure Data Engineer
Salary: approx. £65,000
Location: Edinburgh with 3 days in office
We're working with a client in the energy sector who are looking to bring in a Senior Azure Data Engineer.
This company has very recently had a big data push, recognising the valuable insights that they provide to both their colleagues and external partners. As a result, they're building out a brand-new team, you'd be coming in at a senior level, so you'll have plenty of responsibility and influence on how their Azure/Databricks platform is going to look.
New leadership has been brought in to develop a modern and broad data environment which will include data scientists, engineers and analysts. To help this team develop they're eager for people to come in 3 days a week to their offices in central Edinburgh. They're eager to apply that modern CI/CD and Dev-Ops mindset to achieve rapid and reliable delivery.
Desirable Skills
Experience using SQL
Strong programming skills in Python/PySpark
Experience in an Azure Cloud environment
Experience of building and maintaining a Databricks platform
Benefits
Salary approx. £65,000
Bonus scheme
Private health care
35 days of holiday
If you're interested in hearing more apply or reach out to Daniel at Cathcart Technology
Show more
Show less","Azure, Databricks, SQL, Python, PySpark, CI/CD, DevOps, Data Science, Data Engineering, Data Analysis","azure, databricks, sql, python, pyspark, cicd, devops, data science, data engineering, data analysis","azure, cicd, data engineering, data science, dataanalytics, databricks, devops, python, spark, sql"
Data Engineer - Arcadis Gen,Arcadis,"Edinburgh, Scotland, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-arcadis-gen-at-arcadis-3770618699,2023-12-17,Edinburgh, United Kingdom,Mid senior,Onsite,"Role: Data Engineer
Location: UK, Remote
As an Arcadian, you already help us deliver world leading sustainable design, engineering, and consultancy solutions for natural and built assets. You are part of our global business comprising 36,000 people, in over 70 countries, dedicated to improving quality of life.
Everyone has an important role to play. With the power of many curious minds, together we can continue to solve the world’s most complex challenges and deliver more impact together.
Role description:
The Data Engineer is responsible for migrating the data systems, delivery of the technical leadership in data migration, support of data migration strategies, tools, design and build. As well as formulating various methods to transform raw data into useful data systems. Overall, you’ll strive for efficiency by aligning data systems with business goals. This includes data ETL and modelling pipelines, hosting environments, automated testing and deployment, information storage, and retrieval and support enabling infrastructure across multiple client projects.
The Data Engineer will execute migration projects and may be a coach to other Junior Engineers within the Service Engineering Group (SEG). They will work together with Data architects, Data Consultants, and QA Consultants to enable technical solutions throughout the project lifecycle, including assessments, tool evaluations, demonstrations, requirements gathering, results presentation.
Role accountabilities:
Support and own the end-to-end migration journey on cloud-based products.
Technical delivery of ETL data migration with framework agreed.
Execute data migration runs.
Perform data migration using Snowflake, Dbt and Airflow or similar toolset.
Analyse and organize raw data; build data systems and pipelines.
Assess the compatibility with your cloud destination.
Ensure that the design fully meets the client’s needs.
Manage outputs and quality throughout delivery of a project ensuring all work packages align to the project scope.
Qualifications & Experience:
The ideal candidate will have a background in computer science and/or data engineering, ideally within a data migration consultancy or physical asset owning company context. They will have good leadership, communication, and organisational skills. They will have a strong understanding of how to deploy and scale machine learning and advance analytics techniques in a modern cloud environment. A keen interest in the latest technologies is important, to help ensure that the product (as well as Gen in general) stays on the cutting edge.
More importantly than these technical skills, however, we are looking for someone to be a data solution leader, pushing forward the boundaries of what we do, and the standards expected of the wider Gen data architecture. The successful candidate will need to be resilient and maintain a positive attitude when faced with challenges, be an excellent problem-solver, and be happy to be out of their comfort zone. They will need to have excellent communication skills, not just at a technical level, but also with more business-focused colleagues. They will not shy away from a challenge and will stand up and be counted. They will never be satisfied, and always look to find the next improvement in what Gen does, and how we do it.
We are looking for someone who possesses:
Must haves:
Experience in cloud-based data migration tools and validations
Experience using AWS and/or Azure
Experience using the following tools: Snowflake & DBT
Excellent understanding of SQL
Familiarity with DevOps processes/best practices
Part of assessment, tool options and data migration enablement
Experience of delivery of client facing projects, ideally in a data migration consultancy
Knowledge of programming languages (e.g., Java and Python)
You have experience in the use of automated testing, CI/CD, the use and creation of APIs and of virtual package environments to ensure repeatability of code.
Demonstrable skills in database management
Desirable:
Experience using Airflow
Experience using Azure Data Factory
Continue your career journey as an Arcadian.
We can only achieve our goals when everyone is empowered to be their best. We believe everyone's contribution matters. It’s why we are pioneering a skills based approach, where you can harness your unique experience and expertise to carve your career path and maximize the impact we can make together.
You do meaningful work, and no matter where your next role in Arcadis takes you, you’ll be helping to deliver sustainable solutions for a more prosperous planet. Make your mark; on your career, your colleagues, your clients, your life and the world around you.
Together, we can create a lasting legacy.
Our Commitment to Equality, Diversity, Inclusion & Belonging
We want you to be able to bring your best self to work every day which is why we take equality and inclusion seriously and hold ourselves to account for our actions. Our ambition is to be an employer of choice and provide a great place to work for all our people. We believe that by working together diverse people with different experiences develop the most innovative ideas. Equality, diversity and inclusion is at the heart of how we improve quality of life and we work closely with our people across six ED&I Workstreams: Age, Disability, Faith, Gender, LGBT+ and Race. A diverse and skilled workforce is essential to our success.
Show more
Show less","Data migration, ETL, Data modelling, Data storage, Data retrieval, Data architecture, Cloud computing, AWS, Azure, Snowflake, Dbt, Airflow, DevOps, CI/CD, SQL, Python, Java, APIs, Database management","data migration, etl, data modelling, data storage, data retrieval, data architecture, cloud computing, aws, azure, snowflake, dbt, airflow, devops, cicd, sql, python, java, apis, database management","airflow, apis, aws, azure, cicd, cloud computing, data architecture, data migration, data modelling, data retrieval, data storage, database management, dbt, devops, etl, java, python, snowflake, sql"
Senior Data Engineer - Edinburgh,Jefferson Frank,"Edinburgh, Scotland, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-edinburgh-at-jefferson-frank-3736342528,2023-12-17,Edinburgh, United Kingdom,Mid senior,Onsite,"About Us
We are the market leader in the growing Q-commerce convenience grocery sector, worth £41bn in the UK. In addition to our independent retailers, we serve most of the major convenience store players, including Spar, Nisa, Premier and Booker.
We exist to enable local high street businesses to serve their customers online so that local communities thrive, and revenue stays local. Retailers have seen revenue uplifts of between £5k and £120k per month, making the platform mission critical to local business.
About The Role
This newly created Senior Data Engineer position is the latest in our growing data team. The successful candidate will have experience of taking the lead in designing, building and managing scalable solutions within a modern data stack. They will also relish working within a fast-paced, rapidly changing start-up environment, and love helping others to leverage data as they strive towards their goals. While the scope and focus of this role may change as we grow, your key responsibilities will include:
Immerse yourself in our data and work collaboratively with our Data Analyst, Data Scientist and ProdEng Team as you grow your understanding of the data we capture and how we're currently using it.
Spend time with key leaders around the business to understand their data requirements and develop best in class solutions.
Take ownership of our existing data infrastructure and pipelines, including PostgreSQL DB and Apache Airflow workflow management platform.
Integrate new data from our production DBs as new features are released, and from other systems and external sources as new requirements arise.
Recommend new tooling that is scalable and that will enable us to next-level the analysis prepared by the data team.
Build and maintain data pipelines to process diverse data sets at scale and with low latency.
Explore ways to measure and improve data quality, and to identify fraud patterns within our data.
Create and maintain technical documentation of data flows and data structures within the analytics data stack.
Ensure reliability and security, including implementing GDPR policies.
Share your knowledge and expertise with others to help increase data literacy around the business.
About You
The ideal candidate for this position will have:
2:1 or higher degree in a STEM subject, or equivalent qualification by experience.
3+ years' experience in a similar role.
Advanced knowledge of SQL is essential.
High proficiency in Python.
Significant experience of working with cloud-based data infrastructure, such as Snowflake, Redshift and Databricks.
Experience of working with a range of ETL/ELT tools within a modern data stack, as well as Apache Airflow.
Experience of extracting data through API calls.
High level of pride in the quality of their code and documentation.
Desire to take on and solve complex problems, and to decipher solutions into tasks that can be managed within an Agile development framework.
Highly proactive nature, taking immediate steps to communicate issues and find solutions, as they arise.
Familiar with data regulations, including GDPR and the Data Protection Act (2018).
Experience with Tableau is desirable, but non-essential.
Department
Finance, Data & Risk
Locations
Scotland
Show more
Show less","Data Engineering, PostgreSQL, Apache Airflow, Cloudbased data infrastructure, Snowflake, Redshift, Databricks, ETL/ELT tools, Agile development framework, SQL, Python, API calls, Tableau, GDPR, Data Protection Act (2018)","data engineering, postgresql, apache airflow, cloudbased data infrastructure, snowflake, redshift, databricks, etlelt tools, agile development framework, sql, python, api calls, tableau, gdpr, data protection act 2018","agile development framework, apache airflow, api calls, cloudbased data infrastructure, data engineering, data protection act 2018, databricks, etlelt tools, gdpr, postgresql, python, redshift, snowflake, sql, tableau"
Senior Data Engineer,InterGen,"Edinburgh, Scotland, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-at-intergen-3783982462,2023-12-17,Edinburgh, United Kingdom,Mid senior,Onsite,"Senior Data Engineer
Edinburgh, Scotland, United Kingdom
The Role
We’re looking for a Senior Data Engineer to join our Data and Analytics team! You will be responsible for building and maintaining the ingestion and processing of data, managing the data platform and working with stakeholders across the business to develop and support new projects and deployments here at InterGen. Reporting to the Head of Data, you will play a key role in transforming our data into valuable insights that will help inform and drive our business decisions.
The Team
It’s an exciting time to join our team as we are currently building out our Data Platform using Azure Databricks and Python to develop models and analytics that will support new vital trading insights. We’re also working on growing the data culture within InterGen to realise efficiency savings and optimisations through automation and scale that cloud data enables, this is a great opportunity to contribute to driving robust data solutions to power InterGen’s business operations.
We will give you the flexibility you need to do your best work with hybrid work options, where possible but ideally it would be great if you could be in our Edinburgh Office three days a week along with the rest of the Data and Analytics team. Our Edinburgh Office is a friendly and exciting environment enhanced with regular company-funded social events, seasonal gatherings and raising funds for our chosen charities.
Why you?
Our new Senior Data Engineer will have superior analytical and problem solving skills in abundance. You will have experience in several different technologies, but essential for this role is that you will be an expert with Databricks, SQL and Python as well as having hands on PySpark experience. You are skilled at building and maintaining data platforms, workflows and environments in a commercial/enterprise setting. We’d love to hear from you if you can take ownership of delivering significant projects and have the ability to collaborate with (or even mentor) others.
It would also be great, but not essential if you have energy industry experience!
Your Qualifications/Experience
Education
Educated to degree level, preferably in a scientific or numerical subject but as long as you can demonstrate an analytical approach and problem-solving ability, we will consider applicants with other relevant experience and degree backgrounds
Experience
A minimum of 3+ years’ industry experience
Using Azure technologies, such as Data Factory, Data/Delta Lake and associated storage tools, Version control (git), release and deployment CI/CD within DevOps
Hands on experience using and developing with orchestration tools like Workflows, Prefect and Airflow.
What’s in it for you?
35 days holiday (includes bank holidays)
Discretionary bonus scheme
Life Assurance
Income Protection
Pension Scheme
Private Medical Insurance
Health & Wellbeing Programs
Company Sick Pay Scheme
Eyecare Vouchers
Savings Platform
Maternity / Paternity benefits
Educational Assistance Programmes
Why InterGen?
Whether you’re working from our Edinburgh office, remote location or at one of our Power Stations, you will be working in a dynamic environment where your integrity, entrepreneurial spirit and teamwork are championed.
We welcome applications from all sections of the community, and you will help us foster a culture of equity, diversity and inclusion – a safe and respectful workplace, where we invite everyone to bring their whole selves to work using their unique talents, backgrounds and expertise to create transformational outcomes and help shape our culture.
We provide a wide array of compensation, benefits and well-being programs to meet the diverse needs of our employees.We are open to candidates wanting to work part-time or full-time, on a permanent or fixed term contract basis.We believe in each person’s potential and we’ll help you with yours by offering the development resources you need to advance your career.
With the strength of our company and reputation behind you, a culture of high-performance that motivates you and the freedom to drive your career your way, our shared purpose and values will inspire you to make an impact in an organisation whose vision is working towards a world with stable and dependable energy for generations.
Click Here To Download The Full Job Description
To apply, please fill in the application form here .
Show more
Show less","Data Engineering, Data Analytics, Azure Databricks, Python, SQL, PySpark, Data Platforms, Data Workflows, Environments, DevOps, CI/CD, Orchestration Tools, Workflows, Prefect, Airflow, Azure, Data Factory, Data/Delta Lake, Version Control, Git","data engineering, data analytics, azure databricks, python, sql, pyspark, data platforms, data workflows, environments, devops, cicd, orchestration tools, workflows, prefect, airflow, azure, data factory, datadelta lake, version control, git","airflow, azure, azure databricks, cicd, data engineering, data factory, data platforms, data workflows, dataanalytics, datadelta lake, devops, environments, git, orchestration tools, prefect, python, spark, sql, version control, workflows"
Contract AWS Data Engineer,Energy Jobline,"Edinburgh, Scotland, United Kingdom",https://uk.linkedin.com/jobs/view/contract-aws-data-engineer-at-energy-jobline-3775019428,2023-12-17,Edinburgh, United Kingdom,Mid senior,Onsite,"Data Engineer - 6 Months - Outside IR35
Data Engineer required by a leading tech company for an initial 6-month contract. It's fully remote, outside IR35, and will likely extend long term due to the amount of work they have in the pipeline.
The Role
You will be part of a rapidly growing data team helping to deliver their leading data platform that is being used to help drive key decision making in renewable energy and its use around the world - tech for good, you could say!
Responsibilities
As a Data Engineer, you will be responsible for various tasks but mainly focused on the ETL pipeline, which is bringing in data from various sources and ensuring its quality as it is used around the organisation. The main tools you will be using day to day are:
Python
Modern Data ETL Pipelines
AWS
Desirable
Perl
You
Our customer is looking for someone to come in who is well versed with the above tools / technologies, and they need someone who understands the principles of Data Engineering and is willing to roll their sleeves up as this project is critical for them.
As a Senior member of the team, they expect you to pick up their tooling pretty quickly, work closely with other people in the team and help build upon an already industry leading product.
Rate / Process
This is an urgent role which is based in Edinburgh City Centre but is fully remote.
The interview process will include a short video call with the Head of Data & then a final meeting with other team members.
We can organise both meetings within a few days as this is an absolute priority across the entire business.
Rate-wise, we have between £500 - £550 per day + VAT, depending on experience and availability.
If this sounds interesting and relevant to you now, please apply immediately and call Andy Weir at Cathcart Technology
Show more
Show less","Python, Data Engineering, Data ETL Pipelines, AWS, Perl","python, data engineering, data etl pipelines, aws, perl","aws, data engineering, data etl pipelines, perl, python"
Senior Data Analyst,Michael Page,"Edinburgh, Scotland, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-analyst-at-michael-page-3712700010,2023-12-17,Edinburgh, United Kingdom,Mid senior,Onsite,"Well established and rapidly expanding business open to new ideas|Excellent salary and benefits including hybrid working and free products
About Our Client
As you'd expect this is a key role within the business and you'll enjoy a salary in the region of £50k to £60k (depending on experience) plus a host of benefits including; hybrid working, free products delivered to your door (monthly), private health care etc.
Job Description
What you'll bring to the role:
Previous experience as a Senior Data Analyst leading Junior Data Analysts and working with Senior Management
Demonstrable experience working in a small entrepreneurial business is advantageous
A deep understanding of how data is used to generate recurring revenue via subscriptions
Working knowledge of data visualisation tools (Qlik, Tableau etc.)
Strong working knowledge of SQL, Python etc.
Ability to work with more junior staff to help develop their skills as part of the wider team
Knowledge of statistical modelling tools (Strata, R etc.)
The Successful Applicant
You will be a forward thinking self-starter able to quickly understand the longer term goals of the business, using your skills and experience to help them achieve these.
What's On Offer
This is a small and expanding business where your entrepreneurial spirit will be encouraged and you'll be able to add value across the entire data team.
If this sounds like your ideal role and you're able to commute to Edinburgh City Centre then please apply
NOW
for immediate consideration.
Contact: Natalie Wood
Quote job ref: JN-082023-6146920
Show more
Show less","Senior Data Analyst, Data Visualization Tools (Qlik Tableau), SQL, Python, Statistical Modelling Tools (Strata R)","senior data analyst, data visualization tools qlik tableau, sql, python, statistical modelling tools strata r","data visualization tools qlik tableau, python, senior data analyst, sql, statistical modelling tools strata r"
Senior Oracle database Design Engineer,Eden Brown Built Environment,"Edinburgh City, Scotland, United Kingdom",https://uk.linkedin.com/jobs/view/senior-oracle-database-design-engineer-at-eden-brown-built-environment-3784975940,2023-12-17,Edinburgh, United Kingdom,Mid senior,Onsite,"Are you in search of a temporary yet fulfilling role? Look no further! We're thrilled to offer an exciting position for our client based in
Edinburgh
who are looking to appoint a
Senior Oracle database Design Engineer
for the 3 months ongoing, at the rate of £575 per day (umbrella)
Job responsibilities
Job Summary
We are seeking a highly skilled and experienced Database Designer with experience of Enterprise database design, preferably experienced with the Oracle stack, to join our team. They will be responsible for designing and developing a database for managing observing site metadata, using an Oracle database, as well as designing and optimising SQL/PLSQL and Apex applications. They will work closely with other developers in the team and stakeholders to ensure the project delivers an efficient, reliable, and secure database with optimised user tools.=
Skills
Design and develop complex databases preferably using Oracle.
Write and optimize SQL statements, PL/SQL code, and Apex applications to allow user interactions.
Ensure data integrity, security, and performance of databases.
Collaborate with other developers and stakeholders to gather requirements and develop solutions.
Troubleshoot and resolve database related issues and happy working in a dynamic environment.
Support and provide technical advice on these technologies and promote best practise to the wider team.
Qualifications
Bachelor's degree in Computer Science, Information Systems, or a related field.
At least 5 years of experience in database design and development.
Strong knowledge of SQL, PL/SQL, and Apex.
Experience with database design, performance tuning, and security.
Excellent problem-solving and analytical skills.
Strong communication and collaboration skills.
Should your skills match the above please send through your updated CV.
Eden Brown is committed to equality in the workplace and is an equal opportunity employer. Eden Brown is acting as an Employment Business in relation to this vacancy.
Show more
Show less","Oracle, SQL, PL/SQL, Apex, Database design, Database development, Data integrity, Data security, Data performance, Troubleshooting, Problemsolving, Analytical skills, Communication, Collaboration","oracle, sql, plsql, apex, database design, database development, data integrity, data security, data performance, troubleshooting, problemsolving, analytical skills, communication, collaboration","analytical skills, apex, collaboration, communication, data integrity, data performance, data security, database design, database development, oracle, plsql, problemsolving, sql, troubleshooting"
Senior Oracle database Design Engineer,Energy Jobline,"Edinburgh, Scotland, United Kingdom",https://uk.linkedin.com/jobs/view/senior-oracle-database-design-engineer-at-energy-jobline-3786099536,2023-12-17,Edinburgh, United Kingdom,Mid senior,Onsite,"Are you in search of a temporary yet fulfilling role? Look no further! We're thrilled to offer an exciting position for our client based in Edinburgh who are looking to appoint a Senior Oracle database Design Engineer for the 3 months ongoing, at the rate of £575 per day (umbrella)
Job responsibilities
Job Summary
We are seeking a highly skilled and experienced Database Designer with experience of Enterprise database design, preferably experienced with the Oracle stack, to join our team. They will be responsible for designing and developing a database for managing observing site metadata, using an Oracle database, as well as designing and optimising SQL/PLSQL and Apex applications. They will work closely with other developers in the team and stakeholders to ensure the project delivers an efficient, reliable, and secure database with optimised user tools.=
Skills
Design and develop complex databases preferably using Oracle.
Write and optimize SQL statements, PL/SQL code, and Apex applications to allow user interactions.
Ensure data integrity, security, and performance of databases.
Collaborate with other developers and stakeholders to gather requirements and develop solutions.
Troubleshoot and resolve database related issues and happy working in a dynamic environment.
Support and provide technical advice on these technologies and promote best practise to the wider team.
Qualifications
Bachelor's degree in Computer Science, Information Systems, or a related field.
At least 5 years of experience in database design and development.
Strong knowledge of SQL, PL/SQL, and Apex.
Experience with database design, performance tuning, and security.
Excellent problem-solving and analytical skills.
Strong communication and collaboration skills.
Should your skills match the above please send through your updated CV.
Eden Brown is committed to equality in the workplace and is an equal opportunity employer. Eden Brown is acting as an Employment Business in relation to this vacancy
Show more
Show less","Oracle, SQL, PL/SQL, Apex, Database design, Database development, Data integrity, Data security, Data performance, Troubleshooting, Problemsolving, Analytical skills, Communication skills, Collaboration skills","oracle, sql, plsql, apex, database design, database development, data integrity, data security, data performance, troubleshooting, problemsolving, analytical skills, communication skills, collaboration skills","analytical skills, apex, collaboration skills, communication skills, data integrity, data performance, data security, database design, database development, oracle, plsql, problemsolving, sql, troubleshooting"
"Senior Data Engineer, Financial Crime Technology",Jobs via eFinancialCareers,"Edinburgh, Scotland, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-financial-crime-technology-at-jobs-via-efinancialcareers-3784980923,2023-12-17,Edinburgh, United Kingdom,Mid senior,Onsite,"Join us as a Senior Data Engineer, Financial Crime Technology
This is an opportunity for a technically minded individual to join us as a Senior Data Engineer within Financial Crime Technology
You'll be working with new and innovative technology to deliver high impact solutions
Hone your existing data engineering skills and advance your career in this critical role
What You'll Do
You'll apply widely agreed data software engineering principles and methodologies to design, develop, test and maintain applications and services to achieve the stated business and technology goals within required budgets and timelines. We'll also look to you to oversee the quality of work, making sure that it meets the technical standards for all services output.
You'll also:
Design and develop new reusable services and APIs for use across the bank
Design and develop data components that are amenable for a greater automation of build, release testing and deployment process on all environments
Drive the reuse and sharing of platform components and technologies within the software engineering teams
Deliver data components to enable the delivery of platforms, applications and services
Write unit and integration tests, in automated test environments to ensure code quality
The Skills You'll Need
You'll need a strong background in data engineering, software design or database design and architecture, as well as extensive experience of developing in a micro-services architecture. You should also have development experience in a programming language, experience of using industry recognised frameworks and development tooling, and a background of implementing programming best practice, especially around scalability, availability and performance.
You'll also need:
Proven track record of solving business and technical problems using data in micro-service architecture
Extensive experience of test-driven development alongside the use of automated test frameworks, mocking and stubbing and unit testing tools
Knowledge of the key phases of software delivery lifecycle and established software development methodologies
Experience of working in an environment where products must be delivered to specific timescales
An understanding of data engineering tools including Hadoop, streamSets, MongoDB, postGresDB, REST and python
The ability to understand and support, modify and maintain systems and code developed by other engineering teams
Show more
Show less","Data Engineering, Software Design, Database Design, MicroServices Architecture, Scalability, Availability, Performance, TestDriven Development, Automated Test Frameworks, Mocking, Stubbing, Unit Testing Tools, Hadoop, StreamSets, MongoDB, PostGresDB, REST, Python","data engineering, software design, database design, microservices architecture, scalability, availability, performance, testdriven development, automated test frameworks, mocking, stubbing, unit testing tools, hadoop, streamsets, mongodb, postgresdb, rest, python","automated test frameworks, availability, data engineering, database design, hadoop, microservices architecture, mocking, mongodb, performance, postgresdb, python, rest, scalability, software design, streamsets, stubbing, testdriven development, unit testing tools"
Data Engineer,MBN Solutions,"Edinburgh, Scotland, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-mbn-solutions-3778507444,2023-12-17,Edinburgh, United Kingdom,Mid senior,Onsite,"Data Engineer £65k, Edinburgh , Hybrid
You will be responsible for building and maintaining the ingestion and processing of data, managing the data platform and working with stakeholders across the business to develop and support new projects and deployment. Reporting to the Head of Data, you will play a key role in transforming our data into valuable insights that will help inform and drive our business decisions
Who are we?
We're the world class developer, owner and operator of power generation facilities. Our extensive expertise helps to ensure a stable, dependable energy supply for all. With over 25 years’ experience in energy markets across the world. It’s an exciting time to join our team as we are currently building out our Data Platform using Azure Databricks and Python to develop models and analytics that will support
What should my background look like?
You will have superior analytical and problem solving skills. You will have experience in several different technologies, but essential for this role is that you will be an expert with Databricks, SQL and Python as well as having hands on PySpark experience. You are skilled at building and maintaining data platforms, workflows and environments in a commercial/enterprise setting. We’d love to hear from you if you can take ownership of delivering significant projects and have the ability to collaborate with (or even mentor) others.
It would also be great, but not essential if you have energy industry experience!
What skills should I have?
As our Data Engineer you WILL have:
2/3years minimum with Azure
Experience with Azure Databricks with Python/PySpark/SQL is key
Experience building pipelines, data ingestion, Data Factory etc
Skillset to take some ownership of the platform and being a Lead, so any
experience managing, building resources in Azure
What’s on offer?
There’s a generous base salary of up to £65k on offer alongside a generous package which includes, 20% bonus based on business performance, pension, fitness budget and private medical options . This position will have flexible/Hybrid working but you will be required to attend our office in Edinburgh 2/3 days a week.
Show more
Show less","SQL, Python, Databricks, Azure Databricks, PySpark, Data ingestion, Data Factory, Azure","sql, python, databricks, azure databricks, pyspark, data ingestion, data factory, azure","azure, azure databricks, data factory, data ingestion, databricks, python, spark, sql"
Azure Data Engineer,Stellar Omada,"Edinburgh, Scotland, United Kingdom",https://uk.linkedin.com/jobs/view/azure-data-engineer-at-stellar-omada-3775215138,2023-12-17,Edinburgh, United Kingdom,Mid senior,Remote,"Role Overview:
As an Azure Data Engineer at Stellar Omada, you will be responsible for designing, implementing, and maintaining data solutions on the Azure cloud platform. You will work closely with cross-functional teams to understand data requirements, develop robust data pipelines, and ensure the scalability and performance of our data infrastructure.
Responsibilities:
Design, implement, and maintain data pipelines and ETL processes on the Azure cloud platform.
Collaborate with data scientists, analysts, and other stakeholders to understand data requirements and deliver solutions that meet business needs.
Implement data security and privacy measures in accordance with industry best practices and compliance standards.
Optimize and tune data solutions for performance and scalability.
Troubleshoot and resolve data-related issues, ensuring data quality and integrity.
Stay current with Azure data services, tools, and best practices.
Qualifications:
Bachelor's degree in Computer Science, Information Technology, or a related field.
Proven experience as a Data Engineer with a focus on Azure cloud services.
Strong proficiency in Azure Data Factory, Azure Databricks, and Azure Synapse Analytics.
Experience with data modeling, database design, and performance tuning.
Proficiency in SQL and scripting languages.
Familiarity with data lakes, data warehouses, and big data technologies.
Excellent problem-solving and analytical skills.
Strong communication and collaboration skills.
Preferred Qualifications:
Azure certifications, such as Microsoft Certified: Azure Data Engineer Associate.
Knowledge of data governance and compliance best practices.
Experience with machine learning and advanced analytics.
Show more
Show less","Azure Data Services, Azure Data Factory, Azure Databricks, Azure Synapse Analytics, Data modeling, Database Design, Performance Tuning, SQL, Scripting Languages, Data Lakes, Data Warehouses, Big Data Technologies, Data Governance, Compliance Best Practices, Machine Learning, Advanced Analytics","azure data services, azure data factory, azure databricks, azure synapse analytics, data modeling, database design, performance tuning, sql, scripting languages, data lakes, data warehouses, big data technologies, data governance, compliance best practices, machine learning, advanced analytics","advanced analytics, azure data factory, azure data services, azure databricks, azure synapse analytics, big data technologies, compliance best practices, data governance, data lakes, data warehouses, database design, datamodeling, machine learning, performance tuning, scripting languages, sql"
Senior AI Data Engineer (UK REMOTE),Turnitin,"Edinburgh, Scotland, United Kingdom",https://uk.linkedin.com/jobs/view/senior-ai-data-engineer-uk-remote-at-turnitin-3759704200,2023-12-17,Edinburgh, United Kingdom,Mid senior,Remote,"Company Description
100% REMOTE
MUST BE UK BASED
At Turnitin, an AI-centric leader in the educational and research sectors, we've been innovating and promoting academic integrity for over two decades. We have an established reputation for our advanced solutions, utilized by numerous academic institutions, corporations, and publishers worldwide.
Offering remote work as a default arrangement, we honor individual choices, value diversity, and respect local cultures. However, for those who prefer the office environment, we have multiple locations across the globe including Oakland, Dallas, Pittsburgh, Kyiv (Ukraine), Newcastle (UK), and Utrecht (Netherlands). Our team is diverse, but unified by our commitment to significantly impacting the realm of education.
As a Senior Data Engineer at Turnitin, you will be part of a global team of proactive, supportive, and independent professionals, striving to deliver sophisticated, well-structured AI and data systems. Collaborating closely with different teams within Turnitin, you'll integrate AI and data science across our broad suite of products, further enriching learning, teaching, and academic integrity.
Job Description
Your role as a Senior Data Engineer entails a range of responsibilities, necessitating a balanced skillset:
AI Data Engineering: Design, build, operate and deploy real-time data pipelines at scale using AI techniques and best practices. Support Turnitin's AI R&D efforts by applying advanced data warehousing, data science, and data engineering technologies. Aim for automation to enable a faster time-to-market and better reusability of new AI initiatives.
Collaboration: Work in tandem with the AI R&D teams and the Data Platform Team to collect, create, curate and maintain high-quality AI datasets. Ensure alignment of data architecture and data models across different products and platforms.
Innovation: Unearth insights from Turnitin's rich data resources through innovative research and development.
Hands-on Involvement: Engage in data engineering and data science tasks as required to support the team and the projects. Conduct and own external data collection efforts - including state of the art prompt engineering techniques - to support the construction of state of the art AI models.
Communication: Foster clear communication within the team and the organization, and ensure understanding of the company's vision and mission.
Continuous Learning: Keep abreast of new tools and development strategies, bringing innovative recommendations to leadership.
Qualifications
At least 4 years of experience in data engineering, ideally focused on enabling and accelerating AI R&D.
Strong proficiency in Python, Java, and SQL.
Proficiency with Redshift, Hadoop, Elasticsearch, and cloud platforms (AWS, Azure, GCP).
Familiarity interacting with AI frameworks including PyTorch and TensorFlow and AI libraries such as Huggingface and Scikit-Learn.
Experience with Large Language Models (LLMs) and LLM APIs.
Strong problem-solving, analytical, and communication skills, along with the ability to thrive in a fast-paced, collaborative environment.
Desired Qualifications
6+ years of experience in data engineering with a focus on AI and machine learning projects.
Experience in a technical leadership role.
Familiarity with natural language processing (NLP) techniques and tools.
Experience in the education or education technology sectors.
Experience with data visualization and data communications.
Characteristics for Success
As a Senior Data Engineer, you should possess:
A passion for creatively solving complex data problems.
The ability to work collaboratively and cross-functionally.
A continuous learning mindset, always striving to improve your skills and knowledge.
A proven track record of delivering results and ensuring a high level of quality.
Strong written and verbal communication skills.
Curiosity about the problems at hand, the field at large, and the best solutions.
Strong system-level problem-solving skills.
Additional Information
Total Rewards @ Turnitin
Turnitin maintains a Total Rewards package that is competitive within the local job market. People tend to think about their Total Rewards monetarily – solely as regular pay plus bonus or commission. This what they earn in exchange for what they do. However, Turnitin delivers more than just these components. Beyond the intrinsic rewards of making a difference in the lives of educators, administrators, learners and researchers around the world, and thriving in an organization that is free of politics and full of humble, inclusive and collaborative teammates, the extrinsic rewards at Turnitin include generous time off and health and wellness programs that offer choice and flexibility and provide a safety net for the challenges that life presents from time to time. In our Remote-First approach to collaborating, you are also able to work the way that best fits your style and situation – whether that be remote, in one of our offices/rented spaces or hybrid.
Our Mission
is to ensure the integrity of global education and meaningfully improve learning outcomes.
Our Values
underpin everything we do.
Customer Centric
- We realize our mission to ensure integrity and improve learning outcomes by  putting educators and learners at the center of everything we do.
Passion for Learning
- We seek out teammates that are constantly learning and growing and build a workplace which enables them to do so.
Integrity
- We believe integrity is the heartbeat of ExamSoft. It shapes our products, the way we treat each other, and how we work with our customers and vendors.
Action & Ownership
- We have a bias toward action and empower teammates to make decisions.
One Team
- We strive to break down silos, collaborate effectively, and celebrate each other’s successes.
Global Mindset
- We respect local cultures and embrace diversity. We think globally and act locally to maximize our impact on education.
Global Benefits
Flexible/hybrid working
Remote First Culture
Health Care Coverage*
Tuition Reimbursement*
Competitive Paid Time Off
4 Self-Care Days per year
National Holidays*
3 all-company global holidays (Juneteenth + 2 Founder’s Days)
Paid Volunteer Time*
Charitable cContribution Match*
Monthly Wellness Reimbursement/Home Office Equipment*
Access to Modern Health (mental health platform)
Parental Leave*
Retirement Plan with match/contribution*
* varies by country
Turnitin, LLC is committed to the policy that all persons have equal access to its programs, facilities and employment. We strongly encourage applications from people of color, persons with disabilities, women, and the LGBTQ+ community, regardless of age, gender, religion, marital or veterans status.
Show more
Show less","AI Data Engineering, Data Engineering, Data Warehousing, Data Science, Data Modeling, LLM, LLM API, NLP, Prompt Engineering, PyTorch, TensorFlow, Huggingface, ScikitLearn, AWS, Azure, GCP, Python, Java, SQL, Redshift, Hadoop, Elasticsearch","ai data engineering, data engineering, data warehousing, data science, data modeling, llm, llm api, nlp, prompt engineering, pytorch, tensorflow, huggingface, scikitlearn, aws, azure, gcp, python, java, sql, redshift, hadoop, elasticsearch","ai data engineering, aws, azure, data engineering, data science, datamodeling, datawarehouse, elasticsearch, gcp, hadoop, huggingface, java, llm, llm api, nlp, prompt engineering, python, pytorch, redshift, scikitlearn, sql, tensorflow"
"Senior AI Data Engineer (UK must reside in England, Scotland, or Northern Ireland) 100% REMOTE",Turnitin,"Edinburgh, Scotland, United Kingdom",https://uk.linkedin.com/jobs/view/senior-ai-data-engineer-uk-must-reside-in-england-scotland-or-northern-ireland-100%25-remote-at-turnitin-3763839179,2023-12-17,Edinburgh, United Kingdom,Mid senior,Remote,"Company Description
100% REMOTE
MUST BE U.S. BASED
At Turnitin, an AI-centric leader in the educational and research sectors, we've been innovating and promoting academic integrity for over two decades. We have an established reputation for our advanced solutions, utilized by numerous academic institutions, corporations, and publishers worldwide.
Offering remote work as a default arrangement, we honor individual choices, value diversity, and respect local cultures. However, for those who prefer the office environment, we have multiple locations across the globe including Oakland, Dallas, Pittsburgh, Kyiv (Ukraine), Newcastle (UK), and Utrecht (Netherlands). Our team is diverse, but unified by our commitment to significantly impacting the realm of education.
As a Senior Data Engineer at Turnitin, you will be part of a global team of proactive, supportive, and independent professionals, striving to deliver sophisticated, well-structured AI and data systems. Collaborating closely with different teams within Turnitin, you'll integrate AI and data science across our broad suite of products, further enriching learning, teaching, and academic integrity.
Job Description
Your role as a Senior Data Engineer entails a range of responsibilities, necessitating a balanced skillset:
AI Data Engineering: Design, build, operate and deploy real-time data pipelines at scale using AI techniques and best practices. Support Turnitin's AI R&D efforts by applying advanced data warehousing, data science, and data engineering technologies. Aim for automation to enable a faster time-to-market and better reusability of new AI initiatives.
Collaboration: Work in tandem with the AI R&D teams and the Data Platform Team to collect, create, curate and maintain high-quality AI datasets. Ensure alignment of data architecture and data models across different products and platforms.
Innovation: Unearth insights from Turnitin's rich data resources through innovative research and development.
Hands-on Involvement: Engage in data engineering and data science tasks as required to support the team and the projects. Conduct and own external data collection efforts - including state of the art prompt engineering techniques - to support the construction of state of the art AI models.
Communication: Foster clear communication within the team and the organization, and ensure understanding of the company's vision and mission.
Continuous Learning: Keep abreast of new tools and development strategies, bringing innovative recommendations to leadership.
Qualifications
At least 4 years of experience in data engineering, ideally focused on enabling and accelerating AI R&D.
Strong proficiency in Python, Java, and SQL.
Proficiency with Redshift, Hadoop, Elasticsearch, and cloud platforms (AWS, Azure, GCP).
Familiarity interacting with AI frameworks including PyTorch and TensorFlow and AI libraries such as Huggingface and Scikit-Learn.
Experience with Large Language Models (LLMs) and LLM APIs.
Strong problem-solving, analytical, and communication skills, along with the ability to thrive in a fast-paced, collaborative environment.
Desired Qualifications
6+ years of experience in data engineering with a focus on AI and machine learning projects.
Experience in a technical leadership role.
Familiarity with natural language processing (NLP) techniques and tools.
Experience in the education or education technology sectors.
Experience with data visualization and data communications.
Characteristics for Success
As a Senior Data Engineer, you should possess:
A passion for creatively solving complex data problems.
The ability to work collaboratively and cross-functionally.
A continuous learning mindset, always striving to improve your skills and knowledge.
A proven track record of delivering results and ensuring a high level of quality.
Strong written and verbal communication skills.
Curiosity about the problems at hand, the field at large, and the best solutions.
Strong system-level problem-solving skills.
Additional Information
The
expected annual base salary range
for this position is:
$108,308/year
to
$180,514/year
. This position is bonus eligible / commission-based. As a Remote-First company, actual compensation will be provided in writing at the time of offer, if extended, and is determined by work location and a range of other relevant factors, including but not limited to: experience, skills, degrees, licensures, certifications, and other job-related factors. Internal equity, market and organizational factors are also considered.
Total Rewards @ Turnitin
Turnitin maintains a Total Rewards package that is competitive within the local job market. People tend to think about their Total Rewards monetarily – solely as regular pay plus bonus or commission. This what they earn in exchange for what they do. However, Turnitin delivers more than just these components. Beyond the intrinsic rewards of making a difference in the lives of educators, administrators, learners and researchers around the world, and thriving in an organization that is free of politics and full of humble, inclusive and collaborative teammates, the extrinsic rewards at Turnitin include generous time off and health and wellness programs that offer choice and flexibility and provide a safety net for the challenges that life presents from time to time. In our Remote-First approach to collaborating, you are also able to work the way that best fits your style and situation – whether that be remote, in one of our offices/rented spaces or hybrid.
Our Mission
is to ensure the integrity of global education and meaningfully improve learning outcomes.
Our Values
underpin everything we do.
Customer Centric
- We realize our mission to ensure integrity and improve learning outcomes by  putting educators and learners at the center of everything we do.
Passion for Learning
- We seek out teammates that are constantly learning and growing and build a workplace which enables them to do so.
Integrity
- We believe integrity is the heartbeat of ExamSoft. It shapes our products, the way we treat each other, and how we work with our customers and vendors.
Action & Ownership
- We have a bias toward action and empower teammates to make decisions.
One Team
- We strive to break down silos, collaborate effectively, and celebrate each other’s successes.
Global Mindset
- We respect local cultures and embrace diversity. We think globally and act locally to maximize our impact on education.
Global Benefits
Flexible/hybrid working
Remote First Culture
Health Care Coverage*
Tuition Reimbursement*
Competitive Paid Time Off
4 Self-Care Days per year
National Holidays*
3 all-company global holidays (Juneteenth + 2 Founder’s Days)
Paid Volunteer Time*
Charitable cContribution Match*
Monthly Wellness Reimbursement/Home Office Equipment*
Access to Modern Health (mental health platform)
Parental Leave*
Retirement Plan with match/contribution*
* varies by country
Turnitin, LLC is committed to the policy that all persons have equal access to its programs, facilities and employment. We strongly encourage applications from people of color, persons with disabilities, women, and the LGBTQ+ community, regardless of age, gender, religion, marital or veterans status.
Show more
Show less","AI Data Engineering, Python, Java, SQL, Redshift, Hadoop, Elasticsearch, AWS, Azure, GCP, PyTorch, TensorFlow, Huggingface, ScikitLearn, Large Language Models (LLMs), Natural language processing (NLP), Data visualization, Data communications","ai data engineering, python, java, sql, redshift, hadoop, elasticsearch, aws, azure, gcp, pytorch, tensorflow, huggingface, scikitlearn, large language models llms, natural language processing nlp, data visualization, data communications","ai data engineering, aws, azure, data communications, elasticsearch, gcp, hadoop, huggingface, java, large language models llms, natural language processing nlp, python, pytorch, redshift, scikitlearn, sql, tensorflow, visualization"
Senior Data Engineer,Blend,"Edinburgh, Scotland, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-at-blend-3766651166,2023-12-17,Edinburgh, United Kingdom,Mid senior,Remote,"Company Description
We are looking for a Senior Data Engineer to join the Data Engineering practice for Forth Point which is a Data Science and Engineering company headquartered in Edinburgh UK with plans to grow a large team across EMEA in the next 5 years.
At Forth Point, we are award-winning experts who transform businesses by delivering valuable insights that make a difference. From crafting a data strategy that focuses resources on what will make the biggest difference to your company, to standing up infrastructure, and turning raw data into value through data science and visualisation: we do it all.
We believe that data that doesn't drive value is lost opportunity, and we are passionate about helping our clients drive better outcome through applied analytics.
We are obsessed with delivering world class solutions to our customers through our network of industry leading partners. If this sounds like your kind of challenge, we would love to hear from you.
Job Description
Life as a Senior Data Engineer at Forth Point
We are looking for someone who is ready for the next step in their career and is excited by the idea of solving problems and designing best in class.
However, they also need to be aware of the practicalities of making a difference in the real world – whilst we love innovative advanced solutions, we also believe that sometimes a simple solution can have the most impact.
Our Data Engineer is someone who feels the most comfortable around solving problems, answering questions and proposing solutions. We place a high value on the ability to communicate and translate complex analytical thinking into non-technical and commercially oriented concepts, and experience working on difficult projects and/or with demanding stakeholders is always appreciated.
Reporting to a Lead Data Engineer and working closely with the Data Science and Business Development teams, this role will be responsible for driving high delivery standards and innovation in the company. Typically, this involves delivering data solutions to support the provision of actionable insights for stakeholders.
What can you expect from the role?
Own tasks end-to-end and lead on project delivery and project governance.
Management of Data Engineer(s).
Preparing and presenting data driven solutions to stakeholders.
Design, develop, deploy and maintain ingestion, transformation and storage solutions.
Use a variety of Data Engineering tools and methods to deliver.
Contributing to solutions design and proposal submissions.
Supporting the development of the data engineering team within Forth Point.
Maintain in-depth knowledge of data ecosystems and trends.
Mentor junior colleagues.
Contributing to proposal submissions and business development initiatives under the direction of the Leadership team.
Qualifications
What you need to have?
Proven track record of designing, building, deploying an analytical data infrastructure.
Working knowledge of large-scale data such as data warehouses and their best practices and principles in managing them.
Experience with development, test and production environments and knowledge and experience of using CI/CD.
ETL technical design, development and support.
Advanced level understanding, both conceptually and in practice of Python.
Traditional relational database and distributed data lake architecture experience.
Advanced SQL skills both conceptually and in practice.
Experience of build and delivering a solution in at least one of the cloud platforms (AWS, Azure or GCP).
Good understanding of enterprise coding best practices as well as general CI/CD practices.
Top tier Git practices with experience managing repositories with many contributors.
Self-starter and strong interpersonal skills.
Effective communication and coaching skills.
Nice to have
Knowledge in container deployment.
Experience of creating ARM template design and production (or other IaC, e.g., CloudFormation, Terraform).
Experience in cloud infrastructure management.
Experience of Machine Learning deployment.
Experience in Azure tools and services such as Azure ADFv2, Azure Databricks, Storage, Azure SQL, Synapse and Azure IoT.
Strong understanding and experience with Scala, Spark, PySpark.
Experience of leveraging data out of SAP or S/4HANA.
Management experience.
Previous experience setting up code review frameworks.
Good understanding of API connectivity.
Proven ability to prioritize work and projects.
Good appreciation for the Agile/Scrum methodology.
Accustomed to working on multiple projects simultaneously.
Additional Information
No agencies please.
Must be eligible to work in the UK, we are currently not able to provide sponsorship.
Show more
Show less","Python, SQL, Data Engineering, Data Warehousing, CI/CD, ETL, Git, AWS, Azure, GCP, Cloud Platforms, ARM templates, Terraform, CloudFormation, Machine Learning, Azure ADFv2, Azure Databricks, Azure Storage, Azure SQL, Synapse, Azure IoT, Scala, Spark, PySpark, SAP, S/4HANA, API connectivity, Agile, Scrum","python, sql, data engineering, data warehousing, cicd, etl, git, aws, azure, gcp, cloud platforms, arm templates, terraform, cloudformation, machine learning, azure adfv2, azure databricks, azure storage, azure sql, synapse, azure iot, scala, spark, pyspark, sap, s4hana, api connectivity, agile, scrum","agile, api connectivity, arm templates, aws, azure, azure adfv2, azure databricks, azure iot, azure sql, azure storage, cicd, cloud platforms, cloudformation, data engineering, datawarehouse, etl, gcp, git, machine learning, python, s4hana, sap, scala, scrum, spark, sql, synapse, terraform"
"Senior AI Data Engineer (UK must reside in England, Scotland, or Northern Ireland) 100% REMOTE",Jobs for Humanity,"Edinburgh, Scotland, United Kingdom",https://uk.linkedin.com/jobs/view/senior-ai-data-engineer-uk-must-reside-in-england-scotland-or-northern-ireland-100%25-remote-at-jobs-for-humanity-3773353702,2023-12-17,Edinburgh, United Kingdom,Mid senior,Remote,"Company Description
Jobs for Humanity is partnering with TurnItIn to build an inclusive and just employment ecosystem. Therefore, we prioritize individuals coming from the following communities: Refugee, Neurodivergent, Single Parent, Blind or Low Vision, Deaf or Hard of Hearing, Black, Hispanic, Asian, Military Veterans, the Elderly, the LGBTQ, and Justice Impacted individuals. This position is open to candidates who reside in and have the legal right to work in the country where the job is located.
Company Name: TurnItIn
Job Description
Company Description
100% REMOTE. MUST BE U.S. BASED
At Turnitin, we have been working in the educational and research sectors for over twenty years, using AI technology to promote academic integrity. Our advanced solutions are used by academic institutions, corporations, and publishers worldwide.
We value diversity and respect local cultures. While remote work is our default arrangement, we also have office locations in various places like Oakland, Dallas, Pittsburgh, Kyiv (Ukraine), Newcastle (UK), and Utrecht (Netherlands). Our team is diverse and united in our commitment to make a significant impact in the field of education.
As a Senior Data Engineer at Turnitin, you will join a global team of proactive and supportive professionals. Together, we strive to develop sophisticated AI and data systems that enhance learning, teaching, and academic integrity.
Job Description
Your role as a Senior Data Engineer at Turnitin includes:
Designing, building, and operating real-time data pipelines using AI techniques
Supporting AI research and development by applying data warehousing and engineering technologies
Collaborating with teams to collect and maintain high-quality AI datasets
Uncovering insights from our data resources through innovative research and development
Engaging in data engineering and science tasks as required
Ensuring clear communication within the team and the organization
Staying updated with new tools and development strategies
Qualifications
At least 4 years of experience in data engineering, preferably in AI R&D
Strong proficiency in Python, Java, and SQL
Experience with Redshift, Hadoop, Elasticsearch, and cloud platforms
Familiarity with AI frameworks and libraries
Strong problem-solving, analytical, and communication skills
Desired Qualifications
6+ years of experience in data engineering with a focus on AI and machine learning
Experience in a technical leadership role
Familiarity with natural language processing (NLP) techniques and tools
Experience in the education or education technology sectors
Experience with data visualization and communication
Characteristics for Success
A passion for solving complex data problems
Collaborative and cross-functional mindset
A continuous learning mindset
A proven track record of delivering results with high quality
Strong written and verbal communication skills
Curiosity and strong problem-solving skills
Additional Information
The expected annual base salary range for this position is $108,308/year to $180,514/year. This position is bonus eligible and commission-based. Actual compensation will be provided in writing at the time of offer, considering factors like work location and relevant qualifications.
Turnitin offers a competitive Total Rewards package that goes beyond monetary rewards. We provide generous time off and health and wellness programs to support our employees. We also offer flexibility in work arrangements, allowing you to work remotely, in our offices, or in a hybrid manner.
Our mission is to ensure the integrity of global education and improve learning outcomes. We value customer centricity, passion for learning, integrity, action and ownership, collaboration, and a global mindset.
We have a variety of benefits including flexible and remote working options, health care coverage, tuition reimbursement, competitive paid time off, self-care days, national holidays, paid volunteer time, charitable contribution match, monthly wellness reimbursement, access to mental health platforms, parental leave, and retirement plans.
Turnitin is committed to equal access in employment opportunities and encourages applications from people of color, persons with disabilities, women, and the LGBTQ+ community, regardless of age, gender, religion, marital or veteran status.
Show more
Show less","Data engineering, AI R&D, Python, Java, SQL, Redshift, Hadoop, Elasticsearch, Cloud platforms, AI frameworks, AI libraries, NLP, Natural language processing, Data visualization, Communication","data engineering, ai rd, python, java, sql, redshift, hadoop, elasticsearch, cloud platforms, ai frameworks, ai libraries, nlp, natural language processing, data visualization, communication","ai frameworks, ai libraries, ai rd, cloud platforms, communication, data engineering, elasticsearch, hadoop, java, natural language processing, nlp, python, redshift, sql, visualization"
Marketing Data Analyst,IVC Evidensia,"Edinburgh, Scotland, United Kingdom",https://uk.linkedin.com/jobs/view/marketing-data-analyst-at-ivc-evidensia-3779183469,2023-12-17,Edinburgh, United Kingdom,Mid senior,Remote,"Marketing Data Analyst
Remote or hybrid (if you prefer. We have teams in Bristol, Scotland and Sweden)
We are focused on delivering our purpose of healthy animals, happy owners and benefit every clinic, customer, and patient. Join us and be part of our mission to deliver the highest quality care to millions of animals.
About The Role
We are excited to be looking for a Data Analyst with knowledge of Google Cloud and Microsoft Azure to join our team. As a Data Analyst, you will work closely with our senior analysts and data team to collect, analyse, and interpret data, providing valuable insights that drive informed decision-making..
This is an excellent opportunity for candidates with 2 - 3 years of experience. Looking to progress their career in data analysis.
Responsibilities:
Collect, clean, and organise data from diverse sources, ensuring data quality and integrity
Assist in analysing data using statistical techniques and data mining methods to identify patterns, trends, and correlations
Gain proficiency in using Google Cloud, Google Big Query, Looker and, to an extent, Microsoft Azure tools and platforms to support data analysis and reporting
Support the creation of reports, dashboards, and data visualisations using tools like Power BI or Looker
Help monitor data quality and assist in resolving data-related issues or anomalies.
Assist in documentation and communication of analytical findings and recommendations to stakeholders
What you get in return:
At
IVC Evidensia
we recognise that our people are fundamental to the success of our business. Investing in our people, premises and processes is at the heart of what we do. In addition to a competitive salary, you can enjoy.
5 weeks holiday (increasing with service) plus bank holidays.
Your birthday, as a paid day off each year
Cycle to work scheme.
Discounted staff pet care
Company pension
EAP and supportive well-being programmes
Single Private Healthcare
Family-Friendly policies
Ideally, you will have:
Basic understanding of data analysis concepts and techniques
Familiarity with Google Cloud services (e.g., Google Big Query, API integrations, Looker, Data Transfers and Google Analytics (GA4) and, to an extent, Microsoft Azure services
Strong communication skills
Knowledge of data visualization tools like Tableau, Power BI, or Looker is a plus.
Enthusiasm for data analysis and eagerness to grow your skills in the field
Next Steps:
After receiving your up-to-date CV and being shortlisted, you will be contacted by one of our experienced recruitment team.
As a BAME and LGBTQ+ inclusive employer, we are keen to hear from candidates from all minority and diverse groups. As a Disability Confident Employer, we are keen to hear from candidates with disabilities and long-term health conditions and would be happy to discuss any reasonable adjustments needed during the recruitment pro
Show more
Show less","Data Analysis, Google Cloud, Microsoft Azure, Google Big Query, Looker, Power BI, Tableau, Data Visualization, Data Mining, Data Integrity, Data Quality, Data Reporting, Analytics, Google Analytics (GA4), API Integrations, Statistical Techniques","data analysis, google cloud, microsoft azure, google big query, looker, power bi, tableau, data visualization, data mining, data integrity, data quality, data reporting, analytics, google analytics ga4, api integrations, statistical techniques","analytics, api integrations, data integrity, data mining, data quality, data reporting, dataanalytics, google analytics ga4, google big query, google cloud, looker, microsoft azure, powerbi, statistical techniques, tableau, visualization"
Senior Data Engineer,Forth Point | A Blend Company,"Edinburgh, Scotland, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-at-forth-point-a-blend-company-3766686561,2023-12-17,Edinburgh, United Kingdom,Mid senior,Hybrid,"Company Description
We are looking for a Senior Data Engineer to join the Data Engineering practice for Forth Point which is a Data Science and Engineering company headquartered in Edinburgh UK with plans to grow a large team across EMEA in the next 5 years.
At Forth Point, we are award-winning experts who transform businesses by delivering valuable insights that make a difference. From crafting a data strategy that focuses resources on what will make the biggest difference to your company, to standing up infrastructure, and turning raw data into value through data science and visualisation: we do it all.
We believe that data that doesn't drive value is lost opportunity, and we are passionate about helping our clients drive better outcome through applied analytics.
We are obsessed with delivering world class solutions to our customers through our network of industry leading partners. If this sounds like your kind of challenge, we would love to hear from you.
Job Description
Life as a Senior Data Engineer at Forth Point
We are looking for someone who is ready for the next step in their career and is excited by the idea of solving problems and designing best in class.
However, they also need to be aware of the practicalities of making a difference in the real world – whilst we love innovative advanced solutions, we also believe that sometimes a simple solution can have the most impact.
Our Data Engineer is someone who feels the most comfortable around solving problems, answering questions and proposing solutions. We place a high value on the ability to communicate and translate complex analytical thinking into non-technical and commercially oriented concepts, and experience working on difficult projects and/or with demanding stakeholders is always appreciated.
Reporting to a Lead Data Engineer and working closely with the Data Science and Business Development teams, this role will be responsible for driving high delivery standards and innovation in the company. Typically, this involves delivering data solutions to support the provision of actionable insights for stakeholders.
What can you expect from the role?
Own tasks end-to-end and lead on project delivery and project governance.
Management of Data Engineer(s).
Preparing and presenting data driven solutions to stakeholders.
Design, develop, deploy and maintain ingestion, transformation and storage solutions.
Use a variety of Data Engineering tools and methods to deliver.
Contributing to solutions design and proposal submissions.
Supporting the development of the data engineering team within Forth Point.
Maintain in-depth knowledge of data ecosystems and trends.
Mentor junior colleagues.
Contributing to proposal submissions and business development initiatives under the direction of the Leadership team.
Qualifications
What you need to have?
Proven track record of designing, building, deploying an analytical data infrastructure.
Working knowledge of large-scale data such as data warehouses and their best practices and principles in managing them.
Experience with development, test and production environments and knowledge and experience of using CI/CD.
ETL technical design, development and support.
Advanced level understanding, both conceptually and in practice of Python.
Traditional relational database and distributed data lake architecture experience.
Advanced SQL skills both conceptually and in practice.
Experience of build and delivering a solution in at least one of the cloud platforms (AWS, Azure or GCP).
Good understanding of enterprise coding best practices as well as general CI/CD practices.
Top tier Git practices with experience managing repositories with many contributors.
Self-starter and strong interpersonal skills.
Effective communication and coaching skills.
Nice to have
Knowledge in container deployment.
Experience of creating ARM template design and production (or other IaC, e.g., CloudFormation, Terraform).
Experience in cloud infrastructure management.
Experience of Machine Learning deployment.
Experience in Azure tools and services such as Azure ADFv2, Azure Databricks, Storage, Azure SQL, Synapse and Azure IoT.
Strong understanding and experience with Scala, Spark, PySpark.
Experience of leveraging data out of SAP or S/4HANA.
Management experience.
Previous experience setting up code review frameworks.
Good understanding of API connectivity.
Proven ability to prioritize work and projects.
Good appreciation for the Agile/Scrum methodology.
Accustomed to working on multiple projects simultaneously.
Additional Information
No agencies please.
Must be eligible to work in the UK, we are currently not able to provide sponsorship.
Show more
Show less","Data Engineering, Data Science, Python, SQL, AWS, Azure, GCP, Git, CI/CD, Agile/Scrum, Data Warehouses, Data Lakes, ETL, Cloud Infrastructure Management, Machine Learning Deployment, SAP, S/4HANA, API Connectivity, ARM Template Design, CloudFormation, Terraform","data engineering, data science, python, sql, aws, azure, gcp, git, cicd, agilescrum, data warehouses, data lakes, etl, cloud infrastructure management, machine learning deployment, sap, s4hana, api connectivity, arm template design, cloudformation, terraform","agilescrum, api connectivity, arm template design, aws, azure, cicd, cloud infrastructure management, cloudformation, data engineering, data lakes, data science, data warehouses, etl, gcp, git, machine learning deployment, python, s4hana, sap, sql, terraform"
"Senior Data Engineer - Salary to £75,000 - Hybrid/Remote",Jefferson Frank,"Edinburgh, Scotland, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-salary-to-%C2%A375-000-hybrid-remote-at-jefferson-frank-3742382231,2023-12-17,Edinburgh, United Kingdom,Mid senior,Hybrid,"Senior Data Engineer - Salary to £75,000 - Remote/Hybrid
An extremely experienced enabler of high street tech has created a new position for a Senior Data Engineer to join the data team. You will have the experience of taking lead in designing, building and managing scalable solutions whilst engaging with a modern data stack. The role will suit those who enjoy fast-paced projects and working in a rapidly changing start up environment.
The Role
Work collaboratively with our Data Analyst, Data Scientist and ProdEng Team as you grow your understanding of the data that is captured and how it is used.
Spend time with key leaders around the business to understand their data requirements and develop best in class solutions.
Take ownership of the existing data infrastructure and pipelines, including PostgreSQL DB and Apache Airflow workflow management platform.
Recommend new tooling that is scalable.
Build and maintain data pipelines to process diverse data sets at scale and with low latency.
Explore ways to measure and improve data quality, and to identify fraud patterns within the data.
Create and maintain technical documentation of data flows and data structures within the analytics data stack.
Ensure reliability and security, including implementing GDPR policies.
Share your knowledge and expertise with others to help increase data literacy around the business.
What You Will Bring The Business
3+ years' experience in a similar role.
Advanced knowledge of SQL is essential.
High proficiency in Python.
Significant experience of working with cloud-based data infrastructure, such as Snowflake, Redshift and Databricks.
Experience of working with a range of ETL/ELT tools within a modern data stack, as well as Apache Airflow.
Experience of extracting data through API calls.
High level of pride in the quality of their code and documentation.
Desire to take on and solve complex problems, and to decipher solutions into tasks that can be managed within an Agile development framework.
Highly proactive nature, taking immediate steps to communicate issues and find solutions, as they arise.
Familiar with data regulations, including GDPR and the Data Protection Act (2018).
Experience with Tableau is desirable, but non-essential.
This role will require you to be able to go into the office every month (Hybrid) so we will only be looking for those candidates local to Edinburgh.
Sponsorship is NOT available for this role.
Show more
Show less","Data Engineering, Data Analysis, Data Science, Software Development, SQL, Python, Cloud Computing, Snowflake, Redshift, Databricks, ETL/ELT Tools, Apache Airflow, API Integration, Agile Development, GDPR, Data Protection Act (2018), Tableau","data engineering, data analysis, data science, software development, sql, python, cloud computing, snowflake, redshift, databricks, etlelt tools, apache airflow, api integration, agile development, gdpr, data protection act 2018, tableau","agile development, apache airflow, api integration, cloud computing, data engineering, data protection act 2018, data science, dataanalytics, databricks, etlelt tools, gdpr, python, redshift, snowflake, software development, sql, tableau"
Azure Data Engineer,Cathcart Technology,"Edinburgh, Scotland, United Kingdom",https://uk.linkedin.com/jobs/view/azure-data-engineer-at-cathcart-technology-3784812180,2023-12-17,Edinburgh, United Kingdom,Mid senior,Hybrid,"Role: Senior Azure Data Engineer
Salary: approx. £65,000
Location: Edinburgh with 3 days in office
We’re working with a client in the energy sector who are looking to bring in a Senior Azure Data Engineer.
This company has very recently had a big data push, recognising the valuable insights that they provide to both their colleagues and external partners. As a result, they’re building out a brand-new team, you’d be coming in at a senior level, so you’ll have plenty of responsibility and influence on how their Azure/Databricks platform is going to look.
New leadership has been brought in to develop a modern and broad data environment which will include data scientists, engineers and analysts. To help this team develop they’re eager for people to come in 3 days a week to their offices in central Edinburgh. They’re eager to apply that modern CI/CD and Dev-Ops mindset to achieve rapid and reliable delivery.
Desirable Skills:
Experience using SQL
Strong programming skills in Python/PySpark
Experience in an Azure Cloud environment
Experience of building and maintaining a Databricks platform
Benefits:
Salary approx. £65,000
Bonus scheme
Private health care
35 days of holiday
If you’re interested in hearing more apply or reach out to Daniel at Cathcart Technology.
Show more
Show less","Azure, Databricks, SQL, Python, PySpark, CI/CD, DevOps","azure, databricks, sql, python, pyspark, cicd, devops","azure, cicd, databricks, devops, python, spark, sql"
Data Engineer,BJSS,"Edinburgh, Scotland, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineer-at-bjss-3039183652,2023-12-17,Edinburgh, United Kingdom,Mid senior,Hybrid,"About Us
We’re an award-winning innovative tech consultancy - a team of creative problem solvers. Since 1993 we’ve been finding better, more sustainable ways to solve complex technology problems for some of the world’s leading organisations and delivered solutions that millions of people use every day.
In the last 30 years we won several awards, including a prestigious Queen’s Award for Enterprise in the Innovation category for our Enterprise Agile delivery approach.
Operating from 26 locations across the world, we bring together teams of creative experts with diverse backgrounds and experiences, who enjoy working and learning in our collaborative and open culture and are committed to world-class delivery.
We want to continue to grow our team with people just like you!
We are DataOps advocates and use software engineering best practices to build scalable and re-usable data solutions to help clients use their data to gain insights, drive decisions and deliver business value. Clients don’t engage BJSS to do the straightforward things, they ask us to help on their biggest challenges which means we get to work with a wide range of tools and technologies and there are always new things to learn.
About the Role
BJSS data engineers are specialist software engineers that build, optimise and maintain data applications, systems and services. This role combines the discipline of software engineering with the knowledge and experience of building data solutions in order to deliver business value.
As a BJSS data engineer you’ll help our clients deploy data pipelines and processes in a production-safe manner, using the latest technologies and with a DataOps culture.
You’ll work in a fast moving, agile environment, within multi-disciplinary teams of highly skilled consultants, delivering modern data platforms into large organisations.
You can expect to get involved in variety of projects in the cloud (AWS, Azure, GCP), learning about and using data services such as Databricks, Data Factory, Synapse, Kafka, Redshift, Glue, Athena, BigQuery, S3, Cloud Data Fusion etc.
About You
You're an engineer at heart and enjoy the challenge of building reliable, efficient data applications systems, services and platforms.
You have a good understanding of coding best practices and design patterns and experience with code and data versioning, dependency management, code quality and optimisation, error handling, logging, monitoring, validation and alerting.
You have experience in writing well tested object-oriented Python.
You have experience with using CI/CD tooling to analyse, build, test and deploy your code.
You have a good understanding of design choices for data storage and data processing, with a particular focus on cloud data services.
You have experience in using parallel computing to process large datasets and to optimise computationally intensive tasks.
You have experience in programmatically deploying, scheduling and monitoring components in a workflow.
You have experience in writing complex queries against relational and non-relational data stores.
Some of the Perks
Flexible benefits allowance – you choose how to spend your allowance (additional pension contributions, healthcare, dental and more)
Industry leading health and wellbeing plan - we partner with several wellbeing support functions to cater to each individual's need, including 24/7 GP services, mental health support, and other
Life Assurance (4 x annual salary)
25 days annual leave plus bank holidays
Hybrid working - Our roles are not fully remote as we take pride in the tight knit communities we have created at our local offices. But we offer plenty of flexibility and you can split your time between the office, client site and WFH
Discounts – we have preferred rates from dozens of retail, lifestyle, and utility brands
An industry-leading referral scheme with no limits on the number of referrals
Flexible holiday buy/sell option
Electric vehicle scheme
Training opportunities and incentives – we support professional certifications across engineering and non-engineering roles, including unlimited access to O’Reilly
Giving back – the ability to get involved nationally and regionally with partnerships to get people from diverse backgrounds into tech
You will become part of a squad with people from different areas within the business who will help you grow at BJSS
We have a busy social calendar that you can chose to join– quarterly town halls/squad nights out/weekends away with families included/office get togethers
GymFlex gym membership programme
Show more
Show less","DataOps, Software Engineering, Coding, Data Storage, Data Processing, Parallel Computing, Workflow Management, Query Writing, Python, CI/CD Tooling, Cloud Data Services, Cloud Platforms, AWS, Azure, GCP, Databricks, Data Factory, Synapse, Kafka, Redshift, Glue, Athena, BigQuery, S3, Cloud Data Fusion, Agile, MultiDisciplinary Teams, Data Pipelines, Data Platforms, ObjectOriented Programming, Design Patterns, Code Versioning, Data Versioning, Dependency Management, Code Quality, Code Optimization, Error Handling, Logging, Monitoring, Validation, Alerting","dataops, software engineering, coding, data storage, data processing, parallel computing, workflow management, query writing, python, cicd tooling, cloud data services, cloud platforms, aws, azure, gcp, databricks, data factory, synapse, kafka, redshift, glue, athena, bigquery, s3, cloud data fusion, agile, multidisciplinary teams, data pipelines, data platforms, objectoriented programming, design patterns, code versioning, data versioning, dependency management, code quality, code optimization, error handling, logging, monitoring, validation, alerting","agile, alerting, athena, aws, azure, bigquery, cicd tooling, cloud data fusion, cloud data services, cloud platforms, code optimization, code quality, code versioning, coding, data factory, data platforms, data processing, data storage, data versioning, databricks, dataops, datapipeline, dependency management, design patterns, error handling, gcp, glue, kafka, logging, monitoring, multidisciplinary teams, objectoriented programming, parallel computing, python, query writing, redshift, s3, software engineering, synapse, validation, workflow management"
Senior Data Analyst,Eames Consulting,"Edinburgh, Scotland, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-analyst-at-eames-consulting-3775448208,2023-12-17,Edinburgh, United Kingdom,Mid senior,Hybrid,"Eames are currently working exclusively with an investment management company that offers a range of global strategies and are now on the look out for an experience Senior Data analyst to join the team in Edinburgh.
The successful candidate will have experience in the fund management or financial sectors, ideally with a background in the Bloomberg technology estate, Python & SQL.
Responsibilities:
Develop enterprise data & efficiency strategy.
Ensure smooth operation of data processes.
Code and troubleshoot on Bloomberg BQuant platform in Python.
Establish robust data reconciliation infrastructure.
Design, modify, and test ETL packages using SSIS and SSRS.
Analyze data for improved verification.
Collaborate onboarding new capabilities.
Support team decisions and department representation.
Requirements:
BSc in Computer Science or related field.
Strong SQL experience
Understanding of Python programming concepts.
Expertise in MS BI stack, SQL querying, views, and procedures.
Skilled in SSIS for ETL, performance tuning, and troubleshooting.
Knowledgeable in relational database theory.
Familiarity with source control and database administration.
Understanding of TDD, Agile, Scrum methodologies.
Awareness/experience with SQL unit testing frameworks.
Show more
Show less","Bloomberg BQuant, Python, SQL, SSIS, SSRS, MS BI Stack, TDD, Agile, Scrum, SQL Unit Testing Frameworks, Relational Database Theory, Source Control, Database Administration","bloomberg bquant, python, sql, ssis, ssrs, ms bi stack, tdd, agile, scrum, sql unit testing frameworks, relational database theory, source control, database administration","agile, bloomberg bquant, database administration, ms bi stack, python, relational database theory, scrum, source control, sql, sql unit testing frameworks, ssis, ssrs, tdd"
"Senior Data Engineer, Financial Crime Technology",NatWest Group,"Edinburgh, Scotland, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-financial-crime-technology-at-natwest-group-3783818073,2023-12-17,Edinburgh, United Kingdom,Mid senior,Hybrid,"Our people work differently depending on their jobs and needs. From hybrid working to flexible hours , we have plenty of options that help our people to thrive.
This role is based in the United Kingdom and as such all normal working days must be carried out in the United Kingdom.
Join us as a Senior Data Engineer, Financial Crime Technology
This is an opportunity for a technically minded individual to join us as a Senior Data Engineer within Financial Crime Technology
You’ll be working with new and innovative technology to deliver high impact solutions
Hone your existing data engineering skills and advance your career in this critical role
What you'll do
You’ll apply widely agreed data software engineering principles and methodologies to design, develop, test and maintain applications and services to achieve the stated business and technology goals within required budgets and timelines. We’ll also look to you to oversee the quality of work, making sure that it meets the technical standards for all services output.
You’ll also:
Design and develop new reusable services and APIs for use across the bank
Design and develop data components that are amenable for a greater automation of build, release testing and deployment process on all environments
Drive the reuse and sharing of platform components and technologies within the software engineering teams
Deliver data components to enable the delivery of platforms, applications and services
Write unit and integration tests, in automated test environments to ensure code quality
The skills you'll need
You’ll need a strong background in data engineering, software design or database design and architecture, as well as extensive experience of developing in a micro-services architecture. You should also have development experience in a programming language, experience of using industry recognised frameworks and development tooling, and a background of implementing programming best practice, especially around scalability, availability and performance.
You’ll also need:
Proven track record of solving business and technical problems using data in micro-service architecture
Extensive experience of test-driven development alongside the use of automated test frameworks, mocking and stubbing and unit testing tools
Knowledge of the key phases of software delivery lifecycle and established software development methodologies
Experience of working in an environment where products must be delivered to specific timescales
An understanding of data engineering tools including Hadoop, streamSets, MongoDB, postGresDB, REST and python
The ability to understand and support, modify and maintain systems and code developed by other engineering teams
If you need any adjustments to support your application, such as information in alternative formats or special requirements to access our buildings, or if you’re eligible under the Disability Confident Scheme please contact us and we’ll do everything we can to help.
Apply for this job
Show more
Show less","Data Engineering, Software Design, Database Design, Microservices Architecture, Programming Languages, Industry Recognized Frameworks, Automated Test Frameworks, Mocking and Stubbing, Unit Testing Tools, Software Delivery Lifecycle, Hadoop, StreamSets, MongoDB, PostGresDB, REST, Python","data engineering, software design, database design, microservices architecture, programming languages, industry recognized frameworks, automated test frameworks, mocking and stubbing, unit testing tools, software delivery lifecycle, hadoop, streamsets, mongodb, postgresdb, rest, python","automated test frameworks, data engineering, database design, hadoop, industry recognized frameworks, microservices architecture, mocking and stubbing, mongodb, postgresdb, programming languages, python, rest, software delivery lifecycle, software design, streamsets, unit testing tools"
Senior Azure Data Developer – UK Wide,WSP in the UK,"Edinburgh, Scotland, United Kingdom",https://uk.linkedin.com/jobs/view/senior-azure-data-developer-%E2%80%93-uk-wide-at-wsp-in-the-uk-3780773927,2023-12-17,Edinburgh, United Kingdom,Mid senior,Hybrid,"We are WSP - Join us and make your career future ready!
In today’s world it’s important to work for a company that has clear purpose, giving back to communities and supporting what is truly important in the world.
When considering a career move it’s vital to work for a business that is aligned to your values and goals, a place where you can belong. See what WSP stands for in 2024 and beyond…
To find out more about our
Intelligent Transportation Services (ITS)
business click on the following link and discover what awaits you at WSP: Intelligent transportation services (ITS) | WSP
Your new role, what's involved?
Join our Intelligent Infrastructure discipline and make a real-world difference by helping to deliver crucial projects for public sector clients, which include examples from using big data to help the DfT provide better rail services, to helping road safety officers visualise accident data geospatially, or gathering vital transport planning data using AI-enabled traffic cameras that distinguish and count the different modes of transport.
Work on amazing projects, large and small, to deliver tangible value for our clients through technology enabled change
Engage with clients and technical stakeholders to understand requirements and solve problems
Estimate, Design, Build and Deliver digital solutions using industry best practice
Carry out peer reviews and provide direction and guidance to other team members
Input to the continuous improvement of practice and standards within the team
Provide contributions to the future technical strategy of the team
Help to support and maintain new and legacy software systems, ensuring timely and effective resolutions while maintaining clear communication with stakeholders.
You will benefit from Personal Development Reviews that will align your personal development plan to your own career goals, and those of the business, to help grow and develop your skillset and elevate your position within the business over time.
YOUR TEAM
This role sits within the Intelligent Infrastructure business and will form a key part of the growing Data and Architecture Capability, which is our centre of excellence for digital skills. The Capability team provides a community of colleagues working in and around technology to help you deliver effectively and support your professional development through training, mentoring and opportunities to really stretch your capabilities.
You’ll work as part of an Agile team with specialist developers, testers and analysts to design, develop, deliver and maintain software applications and systems using the latest technologies, helping to make people’s journeys greener, safer, cheaper, and faster to deliver innovative solutions across the property and infrastructure sectors.
We operate a hybrid working policy and expect colleagues to spend time on client sites and collaborating with colleagues face-to-face, as well as working from a local office. We have offices across the UK.
We'd love to hear from you if you have:
Excellent analytical skills and a proven ability to think creatively to solve problems.
Significant experience in the development of data solutions within Azure, utilising functionality such as Azure Data Factory / Lake, Synapse Analytics, Logic and Function Apps and Azure SQL
In depth knowledge of data transformations, pipelines, structured / unstructured data and data ingestion from various sources such as blob, Excel, API.
Strong experience of working in an Agile delivery team and producing and deploying applications in a production environment through a CI/CD pipeline.
Experience with building enterprise-level web applications or services, predominantly using ASP .Net and Azure.
Strong leadership skills, demonstrating the ability to effectively lead and collaborate with other developers to achieve project goals and deliver high-quality solutions.
Experience of working within a DevOps environment
Experience working with clients and customers and excellent communications skills.
A degree in an analytical discipline such as engineering, mathematics, computer science, economics, social sciences, or equivalent industry experience.
Desirable Skills And Experience
Non-Azure cloud platforms (AWS / Google)
Wider programming experience, such a Python, R, PHP, etc
Associated Microsoft (or other) certificates and qualifications
Data Analytics using AI/ML
Visual analytics tools, e.g. Power BI / Tableau / etc.
Good general understanding of data governance/quality/privacy practices
Experience of NOSQL data stores such as MongoDB / CosmosDB
Experience of applying Software Architecture Patterns.
Infrastructure as code technologies and cloud technologies and distributed systems
Business Analysis, Solutions Architecture
Experience in both Cloud and On-Premise server infrastructure
Experience of applying Design Patterns to complex problems and architecting solutions
What's in it for you?
Work-life balance?
WSP recognises that work is only one part of your life and making time for other things is important – whether that’s for your families, friends, or yourself.
Our hybrid working policy allows the flexibility to work from the comfort of your own home as well as collaborating in our contemporary offices across the UK.
Inclusivity & Diversity?
We want our people to achieve rewarding careers, bringing their whole selves to work. We celebrate integrity and treat people with respect, supporting each other and embracing diversity to create a culture of inclusion and belonging at WSP.
Our employee resource groups VIBE (LGBTQ+ employees), CREED (Championing Racial Equality and Ethnic Diversity) and our Gender Balance Group, in tandem with WSP’s Neurodiverse Community Group, WSP Connect Group (visible and non-visible disabilities) help us promote the right environment for you to reach your full potential.
Health & Wellbeing?
We are committed to supporting our people, giving you the tools to make improvements to your health and wellbeing through our Thrive programme.
Med24 gives you and your family unrestricted telephone access to an NHS doctor where you can call day or night or have a face-to-face video consultation.
Flex your time?
For improved work life balance, WSP offers the “WSP Hour” which enables you to take one hour per day to do as you wish and make up the time earlier or later that day. We also offer part time and flexible working arrangements plus the option to flex your bank holiday entitlement to suit you.
Your development?
We appreciate that development and training is important to you and that’s why we have a supportive environment that invests in your development, whether that’s chartership, training or mentoring.
Apply now and be the future of WSP!
#WeAreWSP
Here at WSP we positively encourage applications from suitably qualified and eligible candidates regardless of sex, race, disability, age, sexual orientation, gender reassignment, religion or belief, marital status, pregnancy or maternity/paternity. As a Disability Confident leader, we will interview all disabled applicants who meet the essential criteria, please let us know if you require any workplace adjustments in support of your application.
Please note WSP reserves the right to close the vacancy before the advertised closing date.
Show more
Show less","Azure, Data Factory, Synapse Analytics, Logic Apps, Function Apps, Azure SQL, ASP .Net, DevOps, NOSQL, MongoDB, CosmosDB, Cloud technologies, Data Analytics, AI/ML, Power BI, Tableau, AWS, Google, Python, R, PHP, Microsoft certifications, Data governance/quality/privacy, Design Patterns, Software Architecture Patterns, Agile, CI/CD, Infrastructure as code, Distributed systems, Business Analysis, Solutions Architecture","azure, data factory, synapse analytics, logic apps, function apps, azure sql, asp net, devops, nosql, mongodb, cosmosdb, cloud technologies, data analytics, aiml, power bi, tableau, aws, google, python, r, php, microsoft certifications, data governancequalityprivacy, design patterns, software architecture patterns, agile, cicd, infrastructure as code, distributed systems, business analysis, solutions architecture","agile, aiml, asp net, aws, azure, azure sql, business analysis, cicd, cloud technologies, cosmosdb, data factory, data governancequalityprivacy, dataanalytics, design patterns, devops, distributed systems, function apps, google, infrastructure as code, logic apps, microsoft certifications, mongodb, nosql, php, powerbi, python, r, software architecture patterns, solutions architecture, synapse analytics, tableau"
Data Senior Analyst,Calero,"Edinburgh, Scotland, United Kingdom",https://uk.linkedin.com/jobs/view/data-senior-analyst-at-calero-3783592162,2023-12-17,Edinburgh, United Kingdom,Mid senior,Hybrid,"Job Summary
Calero, the leading technology expense management solution for complex enterprises, is looking for a detail-oriented Senior Data Analyst in their UK offices (Cannock, Tunbridge Wells or Edinburgh). Reporting to the Service Operations Manager, you will ensure proper, timely and accurate invoice processing for assigned group of customers. In this role, you will monitor all open helpdesk tickets for assigned customer(s) and escalate to SOM (Service Operation Manager) as required. You will also be responsible for interacting with customer(s) and carrier(s) relating to invoice processing to resolving any items which prevent completion.
This role is ideal for someone who has experience in the telecom industry and enjoys working in a fast paced, dynamic environment!
Essential Job Responsibilites
Execute customer requirements for GL Coding and Cost Allocations
Create and submit AP files to the customer(s) for payment in line with SLA/KPIs
Identify if past due balances are the result of missing payment
Validate that all batched invoices have completed payment information (Reverse AP Feed)
Submit and track system-based alerts disputes on behalf of the customer(s)
Interact with the Audit team to ensure alignment of identified disputes and resolution
EDUCATION/SPECIAL LICENSES OR CERTIFICATION
Bachelors’ degree in accounting, math, or analytical related field preferred, or equivalent combination of education and experience
Experience
3-5 Years’ Experience in accounting, finance or data entry type roles preferred
Bilingual in Italian or Spanish is preferred but not essential
Skills
Excellent customer service skills
Attention to detail
Agility to fast paced environment
Ability to prioritize and manage multiple responsibilities
Demonstrates a sense of urgency and ability to meet deadlines
COMPETENCIES
Excellent verbal, written, and non-verbal communication skills
Lead by example by maintaining strong ethics, values, trust and integrity
High emotional intelligence and critical/forward thinking
Action oriented, results driven, and ensures accountability
Customer Focused
Quality decision making
Analytical skills
Organizational skills and planning/priority setting
Excellent ability to work collaboratively and confidentially with all levels in the organization
BENEFITS:
Private Healthcare for you and your immediate family
Gym Membership at Nuffield Health or equivalent
Life Insurance (pay out being 4 times your salary)
Competitive Paternity and Maternity Package
Pension Contribution
Team Building Budget and Employee Events
Office Drinks, Snacks, Coffee etc.
Volunteer Day
Flexible Bank Holiday Policy
Calero is an equal opportunity employer. All applicants will be considered for employment without attention to race, colour, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.
Show more
Show less","Customer service, Attention to detail, Agility, Multitasking, Time management, Verbal communication, Written communication, Nonverbal communication, Ethics, Values, Trust, Integrity, Emotional intelligence, Critical thinking, Forward thinking, Action orientation, Results orientation, Accountability, Customer focus, Decision making, Analytical skills, Organization, Planning, Priority setting, Collaboration, Confidentiality, AP files, GL Coding, Cost Allocations, Reverse AP Feed","customer service, attention to detail, agility, multitasking, time management, verbal communication, written communication, nonverbal communication, ethics, values, trust, integrity, emotional intelligence, critical thinking, forward thinking, action orientation, results orientation, accountability, customer focus, decision making, analytical skills, organization, planning, priority setting, collaboration, confidentiality, ap files, gl coding, cost allocations, reverse ap feed","accountability, action orientation, agility, analytical skills, ap files, attention to detail, collaboration, confidentiality, cost allocations, critical thinking, customer focus, customer service, decision making, emotional intelligence, ethics, forward thinking, gl coding, integrity, multitasking, nonverbal communication, organization, planning, priority setting, results orientation, reverse ap feed, time management, trust, values, verbal communication, written communication"
Senior Data Engineer,Hays,"Edinburgh, Scotland, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-at-hays-3782166171,2023-12-17,Edinburgh, United Kingdom,Mid senior,Hybrid,"Your New Company
It is an established player within the energy industry based out of Scotland. They are in the midst of an exciting data infrastructure development programme and have a pipeline of cutting edge projects planned for the next few years.
Your New Role
You will be a senior and trusted member of the data team and have input into project shaping. You will own the data platform and be partly responsible for the development and architecture of it to meet business requirements. You will own data pipeline development, data migration and tooling optimisation while working on scalable data solutions to augment how the business interacts and derives value from its data.
What You'll Need To Succeed
Essential
Extensive SQL and Python
PySpark
Data Factory (and SSIS)
Databricks
Experience building cloud data platforms on Azure
Desirable
Container services (e.g., Docker)
Azure Logic Apps
Power BI
Azure Synapse
What You'll Get In Return
Salary up to £65k DoE
20% performance-based bonus
35 days annual leave
Generous pension contribution
Tons more benefits included
Please Note
Applicants must be willing to commute to the office based in Edinburgh 3 days a week. Remote workers are not considered
Regretfully, our client cannot consider any applicants who need VISA sponsorship.
What You Need To Do Now
If you're interested in this role, click 'apply now' to forward an up-to-date copy of your CV, or call us now.
If this job isn't quite right for you, but you are looking for a new position, please contact us for a confidential discussion on your career.
#4498907 - Blake Joubert
Show more
Show less","Python, SQL, PySpark, Data Factory, SSIS, Databricks, Azure, Docker, Azure Logic Apps, Power BI, Azure Synapse","python, sql, pyspark, data factory, ssis, databricks, azure, docker, azure logic apps, power bi, azure synapse","azure, azure logic apps, azure synapse, data factory, databricks, docker, powerbi, python, spark, sql, ssis"
Senior Data Analyst,"PruTech Solutions, Inc.",New York City Metropolitan Area,https://www.linkedin.com/jobs/view/senior-data-analyst-at-prutech-solutions-inc-3768702819,2023-12-17,Oyster Bay,United States,Mid senior,Onsite,"Founded in 1997, PruTech is dedicated to problem-solving, creating solutions, and maintaining strong partnerships with its clients. PruTech serves a diverse list of clients in different industries from government to finance, retail, and manufacturing. PruTech has offices in New York City, Washington DC, North Carolina, and two nearshore offices in Mexico City and India.
With over 20 years of Information Technology and system integration experience, PruTech provides multiple ways to assist organizations with future technology requirements including:
Project based system integration
Custom software solutions
Package implementations
Consulting and advisory services
Big data and analytics
Nearshore and offshore services
Data Analyst
Location - NY,NY 10007
Skills:
Minimum of 6+ years working experience as a data analyst or business analyst, contributing significantly to complex business system solutions.
Proven experience (4+ years) in gathering business requirements, writing user stories, and conducting user acceptance testing.
3+ years of technical expertise in database design and development.
Strong knowledge and experience with database and data analytic programs such as Microsoft SQL, Access, and Excel; familiarity with Informix; knowledge in Attachmate Reflection is a plus.
Advanced SQL query skills and a solid understanding of relational database principles.
Knowledge of statistics and experience using statistical packages for analyzing datasets (SPSS, SAS, etc.).
Excellent written communication, including validating/verifying and technical writing skills.
Experience working as a Business or Data Analyst in Case Management and Case Tracking Systems.
Ability to multitask and handle priorities effectively while working against tight deadlines.
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, or national origin.
Show more
Show less","Data Analysis, Business Analysis, Database Design, SQL, Microsoft SQL, Access, Excel, Informix, Attachmate Reflection, Relational Databases, Statistics, SPSS, SAS, Written Communication, Case Management, Case Tracking Systems, Multitasking","data analysis, business analysis, database design, sql, microsoft sql, access, excel, informix, attachmate reflection, relational databases, statistics, spss, sas, written communication, case management, case tracking systems, multitasking","access, attachmate reflection, business analysis, case management, case tracking systems, dataanalytics, database design, excel, informix, microsoft sql, multitasking, relational databases, sas, spss, sql, statistics, written communication"
Cassandra Database Engineer,Grubhub,New York City Metropolitan Area,https://www.linkedin.com/jobs/view/cassandra-database-engineer-at-grubhub-3760315346,2023-12-17,Oyster Bay,United States,Mid senior,Hybrid,"We are looking for a Senior Site Reliability Engineer to join our Database Engineering organization. At Grubhub, the Database Engineering organization owns the top-level reliability, observability, and availability of the Datastore platforms, including but not limited to Cassandra, ElasticSearch and Kafka. This team contributes to projects, services, designs, and processes with the aim to steward good architecture and provide tools and services to enable software engineering teams to measure and meet reliability agreements.
The Impact You Will Make
Manage large critical Cassandra and Elasticsearch clusters supporting millions of transactions per day
Build systems to automate all build and maintenance tasks using Ansible and python
Develop self-service tools to allow engineers to manage and provision resources with GrubHub best practices
Monitor cluster availability, read/ write latencies, and other important performance metrics to proactively identify SLO misses and help mitigate issues
Evaluate new technologies and software versions. Test and develop roadmaps
Tune Cassandra and ES databases for optimizing throughput and read /write latencies
24X7 on-call rotation support with rest of team for rapid incident response
Implement DR strategies, including backups and recovery techniques with minimal downtime.
Work with other engineers to manage our data persistence integration and performance with the GrubHub platform.
Monitor and scale Elasticsearch/Cassandra clusters to handle growth in traffic
What You Bring To The Table
Experience managing large Cassandra clusters in production
Experience managing, working or developing large Elasticsearch clusters in highly available 24x7 production environments
Experience automating the maintenance of infrastructure using Python and Ansible or similar tools.
Experience managing automated cloud infrastructures on AWS or other major cloud providers.
Experience developing backend applications in Python or Java
Experience working with Docker is a plus
Ability to quickly learn new concepts and technologies and adapt to changing needs
About Our Tech
Most of our internal tooling is written in Python.
Most of our microservices are written in Java
Observability tools we use: Datadog, Splunk, Lightstep.
Our primary persistence store is Cassandra
We operate in 3 Amazon regions (hot+hot+hot)
We primarily rely on AWS and its services: EC2, S3, SNS/SQS, ElastiCache, Lambda, etc.
​
​New York Salary Range: $161,500-$210,000
The Base Pay component of our Total Rewards package is based on a variety of factors including but not limited to: job responsibilities, location, experience and, education/certifications. In addition to the pay component, this role may be eligible for equity and benefits as part of the Total Rewards package. The Total Rewards package will be determined at the time a contingent offer is made.
And Of Course, Perks!
Flexible PTO.
Grubhub employees enjoy a generous amount of time to recharge.
Health and Wellness.
Excellent medical, dental and vision benefits, 401k matching, employee network groups and paid parental leave are just a few of our programs to support your overall well-being.
Compensation.
You'll receive a great compensation package with eligibility for generous incentives, bonuses, commission, or RSUs (role-specific).
Free Meals.
Our employees get a weekly Grubhub credit to enjoy and support local restaurants.
Social Impact.
We believe in giving back through programs like the Grubhub Community Relief Fund, and provide our employees opportunities to support causes that are important to them.
Grubhub is an equal opportunity employer. We welcome diversity and encourage a workplace that is just as diverse as the customers we serve. We evaluate qualified applicants without regard to race, color, religion, age, sex, sexual orientation, gender identity, national origin, disability, veteran status, and other legally protected characteristics. If you're applying for a job in the U.S. and need an accommodation for any part of the employment process, please send an email to TalentAcquisition@grubhub.com and let us know the nature of your request and contact information.
CA Privacy Notice: If you are a resident of the State of California and would like a copy of our CA privacy notice, please email privacy@grubhub.com.
Show more
Show less","Cassandra, Elasticsearch, Kafka, Ansible, Python, Datadog, Splunk, Lightstep, AWS, Java, Docker","cassandra, elasticsearch, kafka, ansible, python, datadog, splunk, lightstep, aws, java, docker","ansible, aws, cassandra, datadog, docker, elasticsearch, java, kafka, lightstep, python, splunk"
"Senior Software Engineer, Data",Material Bank®,New York City Metropolitan Area,https://www.linkedin.com/jobs/view/senior-software-engineer-data-at-material-bank%C2%AE-3784247465,2023-12-17,Oyster Bay,United States,Mid senior,Hybrid,"Material Bank is a fast-paced, high-growth technology company and created
the world's largest material marketplace for the Architecture and Design industry
, providing the fastest and most powerful way to start and manage a design project. Learn more about us at www.materialbank.com or see below.
Material Bank is seeking an experienced Senior Software Engineer to join our dynamic team, contributing to the development and enhancement of Material Bank's innovative Data Nerve Center. As a key contributor, you will play a pivotal role in developing the data nerve center of Material Bank and driving data excellence within our organization. Your expertise will extend to collaborating on data catalog development, implementing access controls, and integrating with Continuous Integration and Continuous Delivery (CICD) pipelines.
This position can be based in the metropolitan areas of New York, Boston, or Miami.
What you’ll do:
Contribute to maintaining a unified schema for consistent data representation.
Participate in the development of a single view and source of truth for the organization.
Collaborate on the development and maintenance of a comprehensive data catalog.
Assist in implementing tracking mechanisms for schema changes, data lineage, and metadata tagging.
Support the implementation of robust access controls to secure data accessibility.
Contribute to the integration with CI/CD pipelines for efficient updates.
Participate in the implementation of advanced monitoring and alerting capabilities.
Contribute to enhancing observability features for proactive issue identification.
Collaborate on the introduction of structured taxonomy and ontology.
Support adherence to standards for a standardized and organized data classification approach.
Collaborate on initiatives to consolidate all data efforts under Palladio.
Participate in streamlining workflows for a cohesive and integrated data environment.
Support efforts to maintain Palladio as the central hub for data-related activities.
Contribute to the integration of real-time messaging systems.
Participate in the realization of real-time logistics capabilities and data insights.
Support Data Science teams in the creation of custom Large Language Models (LLMs).
Participate in empowering users to derive insights and ask intuitive questions.
Support Data Science teams in reducing the effort required for AI model creation.
Support adherence to industry best practices, contributing to streamlining the model creation process.
What you’ll bring:
Minimum 5+ Years of experience in Data Engineering
Bachelor's degree in Computer Science or related field (or equivalent practical knowledge).
Python
Golang
Java
SQL
What you’ll get from us:
Our people: If you thrive in an inclusive, innovative, and fast-paced organization, look no further! You will get to work alongside some of the brightest minds - Join a genuinely fun and supportive workplace where we keep our employees consistently engaged through internal communication and corporate events
Relaxation and Celebrations: Generous PTO, Sick Days, Paid National Holidays, and even more (ask us about this when we connect).
Health Benefits: We contribute to your medical, dental, vision and short-term/long-term disability plans and have a strong employee assistance program.
Plan for your Retirement: 401(k) eligible after your first 90 day's employed!
Giving Back: We sponsor multiple events throughout the year to help out our communities. You will receive time off to give back as well.
Growth: We’ll help you take your career to the next level. We want you to be creative and take initiative which will allow you to grow and create within the company. Most importantly, be the best at what matters!
Flexible Work Schedules: With business units and employees across the globe, Material Technologies has embraced a hybrid  working model allowing department leaders to decide on the best approach for their respective teams, whether that be remote, in person, or a little of both.
About Material Bank
Material Bank is the world’s largest material marketplace for the architecture and design industry, providing the fastest and most powerful way to search and sample materials. Material Bank connects design professionals to hundreds of manufacturers through facilitating brand discovery, rep engagement, and material sampling.
Material Bank has transformed the way an entire industry discovers and samples materials. By removing the friction that exists in the process, we drive business between architects and designers (members) and our Brand Partners (clients).
Our powerful material database and proprietary robotic distribution facility allow members to order samples until midnight (ET) to be delivered free of charge anywhere in the US, in one box, by 10:30 AM the next morning.
Connect with us and discover your career at Material Bank.
--
Material Bank is proud to be an equal opportunity employer. We value diversity, and all applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, age, national origin, veteran or disability status or other status protected under any applicable federal, state or local law.
Show more
Show less","Data Engineering, Data Science, Python, Golang, Java, SQL, Large Language Models, Palladio, CI/CD, Data Catalog","data engineering, data science, python, golang, java, sql, large language models, palladio, cicd, data catalog","cicd, data catalog, data engineering, data science, golang, java, large language models, palladio, python, sql"
Sr. Business Intelligence Analyst (Data Scientist),Corebridge Financial,"Queens County, NY",https://www.linkedin.com/jobs/view/sr-business-intelligence-analyst-data-scientist-at-corebridge-financial-3772697887,2023-12-17,Oyster Bay,United States,Mid senior,Hybrid,"Who We Are
Corebridge Financial is an outstanding franchise that brings together a broad portfolio of life insurance, retirement and institutional products offered through an extensive, multichannel distribution network. We hold long-standing, leading market positions in many of the markets we serve. With our strong capital position, customer orientation, breadth of product expertise and deep distribution relationships across various channels, we are well positioned to serve growing market needs.
We have a legacy of working to make the world a better place, and that begins with our most important asset, our employees. We’re proud to offer a range of employee benefits and resources that help you protect what matters most – your health care, savings, financial protection, and wellbeing.
Sr. Business Intelligence Analyst (Data Scientist)
About The Role
We are looking for an analytically savvy individual to join the Business Intelligence and Analytics team. The work product of this role informs decisioning across the Life & Retirement business. To thrive in this position you, will be curiosity-driven, technically adept, communicative, and accomplished at navigating all stages of data science projects. You would:
Apply a full spectrum of data science techniques to projects and to structure analyses
Work directly with business owners to build relevant use cases
Analyze internal and industry data to find insights, formulate recommendations, and deliver findings to non-technical and executive stakeholders
Collaborate with business, reporting, and IT partners across the organization to operationalize results
Consult with team members to inform parallel projects
Be a leader in furthering team practices and capabilities
What You Need To Know
The Business Intelligence Analyst will lead complex data-driven analyses to support executive decision making and drive growth across the Life & Retirement business.
The ideal candidate is a critical thinker who balances technical skills with business-focus and has an established analytics foundation.
Works with stakeholders to assess needs, structure analyses, and apply analytical methodologies to derive actionable insights.
Communicates impacts and applicability of results to the business through visuals and storytelling.
Implements analytical best-practices and knowledge sharing with the team.
What We’re Looking For
In-depth understanding and expertise applying statistical models, predictive models, clustering approaches, and regression techniques
A minimum of 5 years of experience working in analytics or as a data scientist in a business setting. An advanced degree in a quantitative field or an MBA is a plus
Strong interpretation skills and the ability to distill results into business-focused scenarios and recommendations
Aptitude navigating all classifications of data, assessing relevance and limitations. This includes extensive experience integrating data and building models
Strong SQL skills, proficiency in Python, R, or at least one scripting/statistical programming language
Skilled at producing compelling visuals with tools such as Tableau, PowerBI, or packages such as Matplotlib or Plotly.
Work in cloud-based environments (Snowflake, AWS)
Ability to prioritize across individual and team deliverables
Project management experience & experience with Machine learning, a plus
Knowledge of financial services or life insurance, a plus
What Our Employees Like Most About Working
We care about your professional development. Our career progression program will provide you with the opportunity to develop your skills, strengthen your productivity and be eligible to progressively advance to positions with an increased responsibility and increased compensation.
Our “Giving Back” policy is at the core of our daily operations and guides our future progress. We offer up to 16 hours a year paid time off to volunteer in the community.
Our people are our most important asset therefore we provide a generous benefits plan; Medical, Dental and Vision, 401(k) and company match, paid time off (PTO) plus company paid holidays. We’re proud to offer a range of competitive benefits, a summary of which can be viewed here: Benefits Overview.
For positions based in Woodland Hills California, the base salary range is $100,000 - $130,000 and the position is eligible for a bonus in accordance with the terms of the applicable incentive plan. In addition, we’re proud to offer a range of competitive benefits.
Remote Role, Work from Home
We are an Equal Opportunity Employer
Corebridge Financial, Inc., its subsidiaries and affiliates are committed to be an Equal Opportunity Employer and its policies and procedures reflect this commitment. We provide equal opportunity to all qualified individuals regardless of race, color, religion, age, gender, gender expression, national origin, veteran status, disability or any other legally protected categories such as sexual orientation. At Corebridge Financial, we believe that diversity and inclusion are critical to our future and our mission – creating a foundation for a creative workplace that leads to innovation, growth, and profitability. Through a wide variety of programs and initiatives, we invest in each employee, seeking to ensure that our people are not only respected as individuals, but also truly valued for their unique perspectives.
To learn more please visit: www.corebridgefinancial.com
Corebridge Financial is committed to working with and providing reasonable accommodations to job applicants and employees with physical or mental disabilities. If you believe you need a reasonable accommodation in order to search for a job opening or to complete any part of the application or hiring process, please send an email to TalentandInclusion@corebridgefinancial.com. Reasonable accommodations will be determined on a case-by-case basis.
We consider qualified applicants with criminal histories, consistent with applicable law.
Functional Area
DT - Data
Estimated Travel Percentage (%): Up to 25%
Relocation Provided: No
American General Life Insurance Company
Show more
Show less","Data Science, Business Intelligence, Statistical Models, Predictive Models, Clustering Approaches, Regression Techniques, SQL, Python, R, Tableau, PowerBI, Matplotlib, Plotly, Machine Learning, Snowflake, AWS","data science, business intelligence, statistical models, predictive models, clustering approaches, regression techniques, sql, python, r, tableau, powerbi, matplotlib, plotly, machine learning, snowflake, aws","aws, business intelligence, clustering approaches, data science, machine learning, matplotlib, plotly, powerbi, predictive models, python, r, regression techniques, snowflake, sql, statistical models, tableau"
Sr. Data Analyst,Aston Carter,"Las Vegas, NV",https://www.linkedin.com/jobs/view/sr-data-analyst-at-aston-carter-3788742949,2023-12-17,Enterprise,United States,Mid senior,Remote,"Description:*
Essential Job Functions:*
Support clients: Work and communicate closely with their clients throughout the report/dashboard development process. Provide status updates. Help clients understand their report/dashboard. Provide on-going maintenance and support.
Support HCS projects: Participate in outcome evaluation, program evaluation, and quality improvement studies. Attend meetings, answer data-related questions, and offer suggestions.
Create reports and dashboards: Create ad-hoc and routine reports. Design and develop dashboards to display key metrics and trends.
Manage data: Collect, organize, store, and share a wide variety of data.
Transform data: Clean and optimize data for analyses.
Ensure data quality: Audit data, data transformation processes, workflow, deliverables and outputs.
Perform analyses: Perform statistical analyses (descriptive and inferential analyses).
Present findings: Present data and findings in a clear and concise manner, using appropriate reporting and data visualization tools.
Create and maintain documentation: Create FDD, document report requirement, business logic and workflow. Create data dictionaries. Ensure documentation is up-to-date.
Maintain up-to-date knowledge on HCS information management systems, processes and data.
Manage compliance reporting: Maintain up-to-date knowledge of CMS, DHCS and internal compliance reporting requirement. Translate reporting requirement into reports. Work with clients to ensure accuracy of data. Submit report to external and internal agencies in a timely manner. Attend compliance trainings, meetings, and data validation webinars.
Support HCS system enhancement/implementation: Perform data-related research and testing. Stay informed of system and process changes. Identify impact on existing reports and dashboards. Modify existing reports and dashboards accordingly.
Prioritize work and keep supervisor informed: Work on multiple projects at the same time. Organize and prioritize work effectively. Inform management when requirement or due date cannot be met.
Adheres to all quality, compliance and regulatory standards to achieve HCS and SCAN outcomes.
Additional Skills & Qualifications:*
Required Education: *
Bachelor’s or Master’s degree in Analytics, Healthcare Informatics, Statistics, Computer Science, or related field.
Required Experience*
At least 5 years of experience analyzing and compiling data, preferably in a health plan setting.
Required Skills/Abilities:*
Ability to manipulate and analyze data to produce accurate results. Present results in data visualizations, dashboards, and reports.
Knowledge in CMS Medicare Advantage (Part C), CMS Prescription Drug Coverage (Part D), and CMS Special Needs Plan (SNP), and DHCS Medi-Cal Managed Care reporting requirements.
Knowledge in authorization, claims, and encounter data. Clinical code knowledge (ICD, CPT, etc) related to utilization data.
Advanced skills in Microsoft Office, SQL Transactional SQL (T-SQL), SQL Server Reporting Services (SSRS), and Tableau.
Experience in SQL Server Integration Services (SSIS), Visual Basic for Applications (VBA).
Must have analytical, communication, documentation, interpersonal, planning, presentation, problem-solving and research skills.
About Aston Carter:
Please Note: Scammers are posing as Aston Carter. We'll never contact you via Gmail, Telegram, or WhatsApp and we'll never solicit money from you.
At Aston Carter, we’re dedicated to expanding career opportunities for the skilled professionals who power our business. Our success is driven by the talented, motivated people who join our team across a range of positions – from recruiting, sales and delivery to corporate roles. As part of our team, employees have the opportunity for long-term career success, where hard work is rewarded and the potential for growth is limitless. Established in 1997, Aston Carter is a leading staffing and consulting firm, providing high-caliber talent and premium services to more than 7,000 companies across North America. Spanning four continents and more than 200 offices, we extend our clients’ capabilities by seeking solvers and delivering solutions to address today’s workforce challenges. For organizations looking for innovative solutions shaped by critical-thinking professionals, visit [AstonCarter.com.](AstonCarter.com) Aston Carter is a company within Allegis Group, a global leader in talent solutions. The company is an equal opportunity employer and will consider all applications without regards to race, sex, age, color, religion, national origin, veteran status, disability, sexual orientation, gender identity, genetic information or any characteristic protected by law. If you would like to request a reasonable accommodation, such as the modification or adjustment of the job application process or interviewing process due to a disability, please call 888-237-6835 or email [astoncarteraccommodation@astoncarter.com](mailto:%20astoncarteraccommodation@astoncarter.com) for other accommodation options. However, if you have questions about this position, please contact the Recruiter located at the bottom of the job posting. The Recruiter is the sole point of contact for questions about this position.
Show more
Show less","Data Analysis, Data Visualization, Reporting, Data Management, Data Transformation, Data Quality Assurance, Statistical Analysis, Data Presentation, Documentation, Microsoft Office, SQL, TSQL, SSRS, Tableau, SSIS, VBA, Analytical Skills, Communication Skills, Documentation Skills, Interpersonal Skills, Planning Skills, Presentation Skills, ProblemSolving Skills, Research Skills, Healthcare Informatics, Statistics, Computer Science, Medicare Advantage, Prescription Drug Coverage, Special Needs Plan, MediCal Managed Care, Authorization, Claims, Encounter Data, Clinical Code Knowledge, ICD, CPT","data analysis, data visualization, reporting, data management, data transformation, data quality assurance, statistical analysis, data presentation, documentation, microsoft office, sql, tsql, ssrs, tableau, ssis, vba, analytical skills, communication skills, documentation skills, interpersonal skills, planning skills, presentation skills, problemsolving skills, research skills, healthcare informatics, statistics, computer science, medicare advantage, prescription drug coverage, special needs plan, medical managed care, authorization, claims, encounter data, clinical code knowledge, icd, cpt","analytical skills, authorization, claims, clinical code knowledge, communication skills, computer science, cpt, data management, data presentation, data quality assurance, data transformation, dataanalytics, documentation, documentation skills, encounter data, healthcare informatics, icd, interpersonal skills, medical managed care, medicare advantage, microsoft office, planning skills, prescription drug coverage, presentation skills, problemsolving skills, reporting, research skills, special needs plan, sql, ssis, ssrs, statistical analysis, statistics, tableau, tsql, vba, visualization"
Senior Cloud Data Engineer,BDO USA,"Las Vegas, NV",https://www.linkedin.com/jobs/view/senior-cloud-data-engineer-at-bdo-usa-3765471247,2023-12-17,Enterprise,United States,Mid senior,Hybrid,"Job Description
Job Summary:
This position will work with cutting edge technology, deliver high quality solutions across various industries, and oversee team(s) on engagements that range in size and scope. This position will receive continuous career development opportunities, given the size and potential of client engagements. This role will perform hands-on delivery of data analytics projects, contributing to the development and unit testing of solutions.
Job Duties
Designs and implements best in class data ingestion strategies, data warehouse and data mart structures, semantic layers and models, visualizations, streaming processes, API integrations, and automation (RPA) solutions for end-to-end data analytics solutions on primarily, but not limited to, cloud analytics platforms such as Azure and AWS
Listens to client needs to align solution with business requirements and delivery schedule
Creates written functional and technical designs
Participates in project status and stand meetings, and assists with providing aggregated project status for project and program managers
Assists with SLA compliance of solutions, and performs performance tuning and optimization efforts of end-to-end solutions
Writes code using multiple languages and correctly applies frameworks, architectural patterns, and software development principles
Delivers high-performance, scalable, repeatable, and secure deliverables with broad impact (high throughput and low latency)
Assists with implementation of data governance programs and best practices
Performs the cleaning and transforming of data from source systems into analytics models
Implements models to support data visualizations and integrations
Assists with implementing DevOps, DataOps and MLOps methodologies on projects
Writes custom integration logic in applicable programming languages
Assists project managers with work breakdown structure creation, project estimation, resource staffing, workload planning and adjustments throughout the project lifecycle
Assists clients with licensing, security, and cost estimation of solutions
Performs code reviews to ensure adherence to standards
Works directly with clients and team members to establish secure data analytics platforms and infrastructure
Contributes to successful deployments of developed solutions and integration of DevOps tools
Maintains a broad and current understanding of data analytics and business intelligence strategies, cloud platforms, methodologies, and tools
Builds client relationships during project execution, effectively becoming a trusted advisor of the client
Participates in support activities for existing software solutions
Other duties as assigned
Supervisory Responsibilities
Supervises the day-to-day workload of Associates on assigned engagements to ensure that timelines and deliverables are met, and reviews work product
Education
Qualifications, Knowledge, Skills and Abilities:
High School Diploma or GED equivalent, required
Bachelor’s degree, preferred; focus in Information Systems, Data Science or Computer Science, preferred
Experience
Five (5) or more years of experience within Data Analytics, Business Intelligence, Artificial Intelligence, or Application Development, required
One (1) or more years of experience technically leading development projects, preferred
One (1) or more years of consulting experience or implementation of cloud-based data analytics solutions, preferred
Software
Strong SQL skills including Data Definition Language (DDL), Data Manipulation Language (DML), views, functions, stored procedures, or performance tuning, required
Experience with Data Warehousing, Data Modeling, Semantic Model Definition or Star Schema Construction, required
Hands on delivery experience of end-to-end cloud data analytics solutions within Azure or AWS, preferred
Experience with one (1) or more of the following computer languages, preferred:
C#
Python
Java
Scala
Experience with tabular modeling within Microsoft Fabric, Power BI, or Azure Analysis Services, preferred
Experience with Git and DevOps deployment technologies, preferred
Experience with Linux, preferred
Experience with one (1) or more of the following, preferred:
Data Lake Medallion Architecture
Batch and/or streaming data ingestion into a data lake
AI Algorithms/Machine Learning
Automation tools such as UiPath, Alteryx, etc.
Computer Vision based AI technologies
Other Knowledge, Skills & Abilities
Ability to work with a high degree of professionalism and autonomy
Excellent verbal and written communication skills
Solid organizational skills, especially the ability to meet project deadlines with a focus on details
Ability to successfully multi-task while working independently or within a group environment
Ability to work in a deadline-driven environment, and handle multiple projects simultaneously
Ability to interact effectively with people at all organizational levels of the Firm
Ability to effectively interact with a team of professionals and delegating work assignments, as needed
Ability to build and maintain strong relationships with internal and client personnel
Ability to encourage a team environment on engagements, and contribute to the professional development of assigned personnel
Keywords:
Data Analytics, Business Intelligence, BI, Synapse, IoT, Machine Learning, Data Lake, Stream, Cube, Microsoft, SQL Server, Tableau, .Net, C#, Qlik, Power BI, Machine Learning, Azure Data Factory, RedShift, UiPath, Cloud, RPA, AWS, Redshift, Kinesis, QuickSight, SageMaker, S3, Databricks, AWS Lake Formation, Snowflake, Python, Qlik, Athena, Data Pipeline, Glue, Star Schema, Data Modeling, SQL, SSIS, SSAS, SSRS, PySpark, Microsoft Fabric, dbt, Linux, Terraform, Bicep, Data Ops, Purview, Git, Delta, Pandas, Spark SQL
Individual salaries that are offered to a candidate are determined after consideration of numerous factors including but not limited to the candidate’s qualifications, experience, skills, and geography.
California Range: $111,000 - $152,000
Colorado Range: $111,000 - $152,000
New York City/ Valhalla Range: $111,000 - $152,000
Washington Range: $111,000 - $152,000
About Us
BDO delivers assurance, tax, digital technology solutions and financial advisory services to clients throughout the country and around the globe. We offer numerous industry-specific practices, world-class resources, and an unparalleled commitment to meeting our clients’ needs. We currently serve more than 400 publicly traded domestic and international clients.
Unparalleled partner-involvement
Deep industry knowledge and participation
Geographic coverage across the U.S.
Cohesive global network
Focused capabilities across disciplines
BDO brings world-class resources and exceptional service to each and every one of our clients. BDO USA is a member of BDO International, the world’s fifth largest accounting network.
BDO offers a competitive Total Rewards package that encompass so much more than – “traditional benefits”. Our wide range of rewards and our employees’ ability to customize rewards to their individual needs are two of the reasons why BDO has been honored with so many workplace awards, including 100 Best Companies for Working Parents, Working Mother 100 Best Companies, Top Entry Level Employer, 2022 National Best & Brightest Companies to Work For and more.
Some Examples Of Our Total Rewards Offerings Include
Competitive pay and eligibility for an annual performance bonus.
A 401k plan plus an employer match
Comprehensive, medical, dental, vision, FSA, and prescription insurance from day one
Competitive Paid Time Off with daily accrual from day one of employment, plus paid holidays
Paid Parental Leave
Adoption Assistance
Firm paid life insurance
Wellness programs
Additional offerings include BDO Flex, Group Legal insurance, Pet insurance and Long-Term Care Insurance
Above offerings may be subject to eligibility requirements.
Click here to find out more!
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability or protected veteran status.
""BDO USA, P.A. is an EO employer M/F/Veteran/Disability""
Show more
Show less","Data Analytics, Business Intelligence, Artificial Intelligence, Application Development, Cloudbased Data Analytics Solutions, SQL, Data Warehousing, Data Modeling, Semantic Model Definition, Star Schema Construction, Azure, AWS, C#, Python, Java, Scala, Tabular Modeling, Microsoft Fabric, Power BI, Azure Analysis Services, Git, DevOps, Linux, Data Lake Medallion Architecture, Batch Data Ingestion, Streaming Data Ingestion, AI Algorithms, Machine Learning, Automation Tools, Computer Vision, Professionalism, Autonomy, Verbal Communication, Written Communication, Organizational Skills, Multitasking, Teamwork, DeadlineDriven Environment, Relationship Building, Delegation, GitOps","data analytics, business intelligence, artificial intelligence, application development, cloudbased data analytics solutions, sql, data warehousing, data modeling, semantic model definition, star schema construction, azure, aws, c, python, java, scala, tabular modeling, microsoft fabric, power bi, azure analysis services, git, devops, linux, data lake medallion architecture, batch data ingestion, streaming data ingestion, ai algorithms, machine learning, automation tools, computer vision, professionalism, autonomy, verbal communication, written communication, organizational skills, multitasking, teamwork, deadlinedriven environment, relationship building, delegation, gitops","ai algorithms, application development, artificial intelligence, automation tools, autonomy, aws, azure, azure analysis services, batch data ingestion, business intelligence, c, cloudbased data analytics solutions, computer vision, data lake medallion architecture, dataanalytics, datamodeling, datawarehouse, deadlinedriven environment, delegation, devops, git, gitops, java, linux, machine learning, microsoft fabric, multitasking, organizational skills, powerbi, professionalism, python, relationship building, scala, semantic model definition, sql, star schema construction, streaming data ingestion, tabular modeling, teamwork, verbal communication, written communication"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Las Vegas, NV",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3773088741,2023-12-17,Enterprise,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto
or
San
Francisco
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","ML Data Engine, ML Data OPs, Data pipelines, Data mining, Data cleaning, Data modeling, Statistical analysis, Data visualization, Orchestration frameworks, NLP, Data governance, Risk and compliance, Data infrastructure, Python, Java, Bash, SQL, Git, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, AWS, GCP, Azure, NoSQL, ETL, Kafka, Storm, SparkStreaming, Machine learning, Data management tools, Data classification, Data retention","ml data engine, ml data ops, data pipelines, data mining, data cleaning, data modeling, statistical analysis, data visualization, orchestration frameworks, nlp, data governance, risk and compliance, data infrastructure, python, java, bash, sql, git, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, nosql, etl, kafka, storm, sparkstreaming, machine learning, data management tools, data classification, data retention","airflow, aws, azure, bash, data classification, data cleaning, data governance, data infrastructure, data management tools, data mining, data retention, datamodeling, datapipeline, docker, etl, gcp, git, helm, java, kafka, kubernetes, machine learning, ml data engine, ml data ops, nlp, nosql, orchestration frameworks, python, risk and compliance, snowflake, spark, sparkstreaming, sql, statistical analysis, storm, visualization"
ANALYST I DATA STRATEGIES,Boyd Gaming,"Las Vegas, NV",https://www.linkedin.com/jobs/view/analyst-i-data-strategies-at-boyd-gaming-3742058652,2023-12-17,Enterprise,United States,Mid senior,Hybrid,"Position Description
Responsible for the analysis and reporting of Corporate, Regional, and Property marketing initiatives, including, but not limited to, customer trends, database marketing campaigns, and promotions.
Develops analytical frameworks and repeatable models to support high-impact, corporate initiatives related to marketing.
Develops and presents comprehensive presentations to senior leadership that provide insights and recommendations for implementation.
Conduct ad-hoc analyses of marketing programs and customer trends.
Create, maintain, and update marketing databases.
Write and perform queries to extract data from Boyd data warehouses.
Analyze Marketing programs/processes and make recommendations for improvement of their timeliness, accuracy, and effectiveness.
Work with data models, identify and utilize reporting tools, and aid in the development of solutions and decisions on managing and evolving the regional direct marketing function
Create job flows and maintain documentation of processes.
Maintain all files of all completed jobs.
Ensure all necessary approvals are obtained for assigned jobs.
Adhere to program and department budgets.
Serve as back-up for other Data Strategies team members.
Assist properties in their marketing campaigns when needed.
Develop and maintain a positive working relationship with outside vendors, the properties and other corporate departments vital to the successful and timely completion of direct marketing campaigns including.
Other duties as assigned.
Position Requirements
Bachelor’s degree with quantitative focus or in related field required.
Strong knowledge of database/direct mail program design, production, and fulfillment.
Working knowledge of Cognos, SQL, Microstrategy, or other query programs preferred.
Proficient in Microsoft Excel.
Organized and detail oriented with strong attention to accuracy.
Works well under pressure.
Ability to obtain/maintain any necessary licenses and/or certifications.
We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity, status as a veteran, and basis of disability or any other federal, state or local protected class.
Show more
Show less","SQL, Cognos, Microstrategy, Excel, Data Warehouses, Data Analysis, Reporting, Marketing","sql, cognos, microstrategy, excel, data warehouses, data analysis, reporting, marketing","cognos, data warehouses, dataanalytics, excel, marketing, microstrategy, reporting, sql"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Las Vegas, NV",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3759711079,2023-12-17,Enterprise,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto,
San
Francisco or Chicago
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Java, Bash, SQL, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, AWS, GCP, Azure, Kafka, Storm, SparkStreaming, NoSQL, DynamoDB, Machine Learning, Statistical Analysis, Data Visualization, Pandas, R, Git, ETL, Docker Images, Microservices, Distributed Systems, Applied Machine Learning","python, java, bash, sql, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, kafka, storm, sparkstreaming, nosql, dynamodb, machine learning, statistical analysis, data visualization, pandas, r, git, etl, docker images, microservices, distributed systems, applied machine learning","airflow, applied machine learning, aws, azure, bash, distributed systems, docker, docker images, dynamodb, etl, gcp, git, helm, java, kafka, kubernetes, machine learning, microservices, nosql, pandas, python, r, snowflake, spark, sparkstreaming, sql, statistical analysis, storm, visualization"
Lead Mechanical Engineer (Mission Critical/Data Center),WSP in the U.S.,"Las Vegas, NV",https://www.linkedin.com/jobs/view/lead-mechanical-engineer-mission-critical-data-center-at-wsp-in-the-u-s-3786436464,2023-12-17,Enterprise,United States,Mid senior,Hybrid,"Who We Are
At WSP, we are driven by inspiring future-ready pioneers to innovate. We’re looking to grow our teams with people who are ready to collaborate in building communities and expanding our skylines. To do this, we hire candidates of all experiences, skillsets, backgrounds and walks of life. We actively foster a work environment and culture where inclusion and diversity is part of our fundamental structure. This is delivered behaviorally, through our policies, trainings, local partnerships with professional diverse organizations, internal networks and most importantly with the support and sponsorship of our leaders who help drive our commitment to an inclusive, diverse, welcoming and equitable work environment. Anything is within our reach and yours as a WSP employee. Come join us and help shape the future!
Great people. Great places. Great projects. kW Mission Critical Engineering, a WSP company, is a high-performance, fast-paced consulting engineering firm designing data centers and mission critical environments across the globe. We hire smart, responsive, team players to work in collaborative and mentoring office settings. Our mechanical, electrical, plumbing, fire protection, controls, telecommunications, and security building system designs keep many of the world’s top Fortune 100 financial, technology, enterprise, hyperscale, and colocation companies up and running 24 hours a day, 365 days a year.
We work on innovative, award-winning, large-scale projects. We travel to construction sites to see our designs being built. As part of WSP, we are able to offer our employees increased professional development and career opportunities in addition to kW MCE’s office culture which is consistently recognized as one of the “Best Places to Work.” Join our great people at our great places designing great projects.
This Opportunity
kW Mission Critical Engineering, a member of WSP USA
, is currently initiating a search for a
Lead Mechanical Engineer
that can be located out of our
kW Tempe, Arizona office or St. Louis, MO office.
As a Mechanical Engineer with us, you will design complex cooling and HVAC systems including air distribution systems, chiller plants, and alternative energy solutions for mission critical facilities.
Your Impact
Independently support the project team during design and construction stages of projects
Design air distribution, hydronic and automated temperature controls systems
Integrate complex mechanical engineering requirements into facility designs
Work within multi-discipline project teams to develop drawing and specification documents for issuance to architects, contractors and building owners
Attend client meetings and contribute to discussions
Collaborate and coordinate with internal project discipline team members and external vendors and manufacturers
Communicate mechanical engineering concepts and decisions to clients and stakeholders
Interact regularly with clients, which includes maintaining current relationships and developing new relationships
Mentor and train junior engineers
Track and coordinate all mechanical disciplines: HVAC, Energy, Controls, Fire Protection, Fire Alarm, Plumbing, Fuel Oil Storage / Management / Distribution
Provide oversight of all aspects of mechanical design, review systems, drawings prior to issuance
Perform Computational Fluid Dynamic (CFD) evaluations for existing and new facilities, both internal and external
Select and schedule major equipment
Develop project specifications
Survey and evaluate existing conditions
Perform construction administration tasks
Who You Are
The ideal candidate has familiarity with Building Information Modeling using Revit, has strong communication skills, and an interest in liaising with internal and external design, client, and construction team members. Candidate should be willing to travel to client sites.
Required Qualifications
Bachelor’s degree in Mechanical Engineering or Architectural Engineering with mechanical building systems emphasis
7+ years of experience in designing mechanical systems for the high performing, commercial, industrial or mission critical/data center buildings
EIT or Registered Professional Engineer (PE), if eligible
Excellent interpersonal skills, teamwork, and written and verbal communication skills
Proficiency with applicable software including AutoCAD, Revit, Trane Trace and Pipeflow
Knowledge of building, mechanical and energy codes
Preferred Qualifications:
Experience with the analysis and modeling of interior and exterior Computational Fluid Dynamics of airflow
Mission Critical/Data Center experience
Experience with international projects
Additional Requirements
To perform this job successfully, an individual must be able to perform each essential job duty satisfactorily. Reasonable accommodations may be made to enable qualified individuals with disabilities to perform essential job functions.
Additional Details
Travel Required: 10%
Job Status: Regular
Employee Type: Full
Primary Location: TEMPE - E RIO SALADO PKWY
All locations: US-AZ-Phoenix, US-AZ-Tempe, US-AZ-Tucson, US-NV-Las Vegas
About WSP
WSP USA is the U.S. operating company of WSP, one of the world's leading engineering and professional services firms. Dedicated to serving local communities, we are engineers, planners, technical experts, strategic advisors and construction management professionals. WSP USA designs lasting solutions in the buildings, transportation, energy, water and environment markets. With more than 15,000 employees in over 300 offices across the U.S., we partner with our clients to help communities prosper. www.wsp.com
WSP provides a flexible and agile workplace model while meeting client needs. Employees are also afforded a comprehensive suite of benefits including medical, dental, vision, disability, life, and retirement savings focused on providing health and financial stability throughout the employee’s career.
At WSP, we want to give our employees the challenges they seek to grow their careers and knowledge base. Your daily contributions to your team will be essential in meeting client objectives, goals and challenges. Are you ready to get started?
WSP USA (and all of its U.S. companies) is an Equal Opportunity Employer Race/Age/Color/Religion/Sex/Sexual Orientation/Gender Identity/National Origin/Disability or Protected Veteran Status.
The selected candidate must be authorized to work in the United States.
NOTICE TO THIRD PARTY AGENCIES:
WSP does not accept unsolicited resumes from recruiters, employment agencies, or other staffing services. Unsolicited resumes include any resume or hiring document sent to WSP in the absence of a signed Service Agreement where WSP has expressly requested recruitment/staffing services specific to the position at hand. Any unsolicited resumes, including those submitted to hiring managers or other business leaders, will become the property of WSP and WSP will have the right to hire that candidate without reservation – no fee or other compensation will be owed or paid to the recruiter, employment agency, or other staffing service.
Show more
Show less","Revit, AutoCAD, Trane Trace, Pipeflow, Building Information Modeling, Computational Fluid Dynamics (CFD), HVAC, Energy, Controls, Fire Protection, Fire Alarm, Plumbing, Fuel Oil Storage / Management / Distribution, Mechanical Engineering, Architectural Engineering, EIT, Registered Professional Engineer (PE)","revit, autocad, trane trace, pipeflow, building information modeling, computational fluid dynamics cfd, hvac, energy, controls, fire protection, fire alarm, plumbing, fuel oil storage management distribution, mechanical engineering, architectural engineering, eit, registered professional engineer pe","architectural engineering, autocad, building information modeling, computational fluid dynamics cfd, controls, eit, energy, fire alarm, fire protection, fuel oil storage management distribution, hvac, mechanical engineering, pipeflow, plumbing, registered professional engineer pe, revit, trane trace"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Castro Valley, CA",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3759712034,2023-12-17,San Leandro,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto,
San
Francisco or Chicago
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data engineering, Machine learning, Python, Java, SQL, Bash, Airflow, Kubernetes, Docker, Spark, PySpark, Snowflake, DynamoDB, ETL, Kafka, Storm, SparkStreaming, Applied machine learning, Data governance, Risk and compliance, Statistical analysis, Data mining, Data cleaning, Data normalization, Data modeling, Data visualization, Pandas, R, Git, NLP, Large language models, Relational databases, NoSQL databases","data engineering, machine learning, python, java, sql, bash, airflow, kubernetes, docker, spark, pyspark, snowflake, dynamodb, etl, kafka, storm, sparkstreaming, applied machine learning, data governance, risk and compliance, statistical analysis, data mining, data cleaning, data normalization, data modeling, data visualization, pandas, r, git, nlp, large language models, relational databases, nosql databases","airflow, applied machine learning, bash, data cleaning, data engineering, data governance, data mining, data normalization, datamodeling, docker, dynamodb, etl, git, java, kafka, kubernetes, large language models, machine learning, nlp, nosql databases, pandas, python, r, relational databases, risk and compliance, snowflake, spark, sparkstreaming, sql, statistical analysis, storm, visualization"
Data Scientist/GeoSpatial Engineer (SOUTHCOM),SimIS Inc.,"Miami, FL",https://www.linkedin.com/jobs/view/data-scientist-geospatial-engineer-southcom-at-simis-inc-3787737875,2023-12-17,Coral Gables,United States,Mid senior,Onsite,"Data Scientist and GeoSpatial Engineer- SOUTHCOM
Position Description
. Serve as the Data Scientist and GeoSpatial Engineer responsible for creation of innovative solutions and data connection frameworks for the design, development, and integration of the Enhanced Domain Awareness (EDA) Architecture Services for the United States Southern Command (USSOUTHCOM) so the staff can begin sharing Publicly Available Information (PAI) and Commercially Available Information (CAI) to include source-agnostic data and other non-traditional data sources such as Non-Government Organizations (NGOs), and USG and interagency in an unclassified (CUI) environment with Partner Nations in order to enhance overall USSOUTHCOM mission effectiveness and to have a more profound effect on counter-narcotics/counter-transnational organized crime(CN/CTOC) operations.
Knowledge
Expert knowledge for adapting broad or vague requirements and creatively engineering solutions while informing government leadership and customers on product development progress. Must be able to envision product ideas independently.
Working knowledge to develop, implement, and automate Extract, Transform, Load (ETL) multi-stage processes, retrieving data from API’s or flat files, using analytical programming tools to extract and fuse data from multiple sources across work units or vendors into a consistent format and load data into a specified target database.
Working knowledge building knowledge graphs and knowledge of Kubernetes and docker, experience deploying code in Jupyter Notebooks or Esri ArcGIS Enterprise Notebook Server, various data ingestion techniques, and significant script writing (primarily Python) for data ingestion.
Expert knowledge of ArcGIS Enterprise suite, ArcGIS Pro, and ArcGIS Web Application development.
Working knowledge of computer vision and machine learning concepts and OSINT methodologies, production, and intelligence writing.
Required Skills
Translating user stories into requirements, and writing scripts for ingesting new live feeds, documents, and data.
Creating mission domain awareness solutions using remote sensing technologies and real-time sensors.
Working directly with customers internal and external to the organization to analyze large amounts of raw data to help find patterns and enable informed decision making at the highest levels.
Providing technical leadership and influencing cross-functional teams to identify requirements, design, develop, and deploy geospatial solutions that support decision-making processes.
Working with the customer to develop conceptual solution designs, monitor and conduct regular maintenance on systems and data ingest feeds, provide troubleshooting and technical support, and occasionally provide executive level briefings and presentations.
Experience
Required: 2 yrs experience developing data solutions for client IT requirements and supporting platforms
Preferred:
USSOUTHCOM/DOD/Combatant Command/ Joint Staff military or civil service experience.
Currently residing in the Miami area to commute to USSOUTHCOM Headquarters.
Education
Required:
Bachelor’s Degree with 2+ years’ experience supporting DoD and/or Interagency data solution development and application requirements.
Certifications. None
Security.
Required: A
ctive personnel security clearance at the Top Secret clearance based upon a Tier 5 investigation completed within the past five (5) years, and eligible for access to Sensitive Compartmented Information (SCI) level.
Travel.
US Passport required to travel as required in USSOUTHCOM area of responsibility (AOR); anticipate 10-15% of contract time allocated to travel in the supporting/supported agencies in the AOR.
Place of Performance.
On-site at USSOUTHCOM Headquarters, Miami, Fl; office space with utilities and Government-provided office equipment.
Full-Time employment
. 11 Federal Holidays; duty day is 7:30AM to 4:40PM Monday-Friday and adjusted as necessary to meet USSOUTHCOM requirements.
Period of Performance.
1 year Base period (starting Aug 2023) and four 1-year option periods.
SimIS Offers:
Flexible Spending Account (FSA)
Medical, Dental, and Vision
Short Term Disability (SimIS provides Short-Term Disability benefits at no cost to you)
LTD
Life Insurance
401(k) Savings Plan
Tuition Assistance Program
Paid Time Off (PTO)
10 Holidays each year
SimIS, Inc. is an AA / EOE / M / F / Disability / Vet / V3 certified / Drug Free Employer
Powered by JazzHR
3ukiTqB0vL
Show more
Show less","Data Science, GeoSpatial Engineering, Enhanced Domain Awareness (EDA) Architecture Services, Publicly Available Information (PAI), Commercially Available Information (CAI), NonGovernment Organizations (NGOs), Extract Transform Load (ETL), Analytical Programming Tools, Knowledge Graphs, Kubernetes, Docker, Jupyter Notebooks, Esri ArcGIS Enterprise Notebook Server, ArcGIS Enterprise suite, ArcGIS Pro, ArcGIS Web Application, Computer Vision, Machine Learning, OSINT Methodologies, Remote Sensing Technologies, Geospatial Solutions, Sensitive Compartmented Information (SCI)","data science, geospatial engineering, enhanced domain awareness eda architecture services, publicly available information pai, commercially available information cai, nongovernment organizations ngos, extract transform load etl, analytical programming tools, knowledge graphs, kubernetes, docker, jupyter notebooks, esri arcgis enterprise notebook server, arcgis enterprise suite, arcgis pro, arcgis web application, computer vision, machine learning, osint methodologies, remote sensing technologies, geospatial solutions, sensitive compartmented information sci","analytical programming tools, arcgis enterprise suite, arcgis pro, arcgis web application, commercially available information cai, computer vision, data science, docker, enhanced domain awareness eda architecture services, esri arcgis enterprise notebook server, extract transform load etl, geospatial engineering, geospatial solutions, jupyter notebooks, knowledge graphs, kubernetes, machine learning, nongovernment organizations ngos, osint methodologies, publicly available information pai, remote sensing technologies, sensitive compartmented information sci"
"Sr. Scala Engineer, Database Engineering",Experfy,"Miami, FL",https://www.linkedin.com/jobs/view/sr-scala-engineer-database-engineering-at-experfy-3590298798,2023-12-17,Coral Gables,United States,Mid senior,Onsite,"As a Sr. Software Engineer for our Data Platform Engineering team you will join skilled Scala engineers and core database developers responsible for developing hosted cloud analytics infrastructure (Apache Spark-based), distributed SQL processing frameworks, proprietary data science platforms, and core database optimization. This team is responsible for building the automated, intelligent, and highly performant query planner and execution engines, RPC calls between data warehouse clusters, shared secondary cold storage, etc. This includes building new SQL features and customer-facing functionality, developing novel query optimization techniques for industry-leading performance, and building a database system that's highly parallel, efficient and fault-tolerant. This is a vital role reporting to exec leadership and senior engineering leadership.
Requirements
Writing Scala code with tools like Apache Spark + Apache Arrow to build a hosted, multi-cluster data warehouse for Web3
Developing database optimizers, query planners, query and data routing mechanisms, cluster-to-cluster communication, and workload management techniques
Scaling up from proof of concept to ""cluster scale"" (and eventually hundreds of clusters with hundreds of terabytes each), in terms of both infrastructure/architecture and problem structure
Codifying best practices for future reuse in the form of accessible, reusable patterns, templates, and code bases to facilitate meta data capturing and management
Managing a team of software engineers writing new code to build a bigger, better, faster, more optimized HTAP database (using Apache Spark, Apache Arrow and a wealth of other open source data tools)
Interacting with exec team and senior engineering leadership to define, prioritize, and ensure smooth deployments with other operational components
Highly engaged with industry trends within analytics domain from a data acquisition processing, engineering, management perspective
Understand data and analytics use cases across Web3 / blockchains
Skills & Qualifications
Bachelor's degree in computer science or related technical field. Masters or PhD a plus
6+ years experience engineering software and data platforms / enterprise-scale data warehouses, preferably with knowledge of open source Apache stack (especially Apache Spark, Apache Arrow, and others)
3+ years experience with Scala and Apache Spark
A track record of recruiting and leading technical teams in a demanding talent market
Rock solid engineering fundamentals; query planning, optimizing and distributed data warehouse systems experience is preferred but not required
Nice to have: Knowledge of blockchain indexing, web3 compute paradigms, Proofs and consensus mechanisms... is a strong plus but not required
Experience with rapid development cycles in a web-based environment
Strong scripting and test automation knowledge
Nice to have: Passionate about Web3, blockchain, decentralization, and a base understanding of how data/analytics plays into this
Show more
Show less","SQL, Apache Spark, Apache Arrow, Scala, Database optimization, Query planning, Query routing, Cluster communication, Workload management, Scaling systems, Cluster architecture, Metadata management, Team lead, Data acquisition, Blockchain indexing, Consensus mechanisms, Distributed systems, Proofs, Scripting, Test automation","sql, apache spark, apache arrow, scala, database optimization, query planning, query routing, cluster communication, workload management, scaling systems, cluster architecture, metadata management, team lead, data acquisition, blockchain indexing, consensus mechanisms, distributed systems, proofs, scripting, test automation","apache arrow, apache spark, blockchain indexing, cluster architecture, cluster communication, consensus mechanisms, data acquisition, database optimization, distributed systems, metadata management, proofs, query planning, query routing, scala, scaling systems, scripting, sql, team lead, test automation, workload management"
Geospatial Data Engineer 2,University of Miami,"Miami, FL",https://www.linkedin.com/jobs/view/geospatial-data-engineer-2-at-university-of-miami-3685849776,2023-12-17,Coral Gables,United States,Mid senior,Onsite,"Current Employees:
If you are a current Staff, Faculty or Temporary employee at the University of Miami, please click here to log in to Workday to use the internal application process. To learn how to apply for a faculty or staff position using the Career worklet, please review this tip sheet.
The Center for Southeastern Tropical Advanced Remote Sensing (CSTARS) at the University of Miami Rosenstiel School of Marine, Atmospheric and Earth Science has an exciting full-time opportunity for a Geospatial Data Engineer 2. We seek a candidate whose technical skills and interests are focused on data management, documentation, and data curation approaches applied to remote sensing and Earth science data systems. Applicants should have experience in data management, particularly in evaluation and integration of heterogeneous data. Applicants should have experience in metadata standards, scientific data formats such as HDF and netCDF, remote sensing, GIS, and data documentation experience.
Position is non-remote
.
Essential Requirement:
Due to the nature of our work and existing contractual requirements, qualified candidates must be a U.S. citizen and be able to obtain and maintain a DoD security clearance.
The candidate will:
Lead the CSTARS data team for curation for all datasets acquired
Be responsible for continuous optimization of the dataset lifecycle policy, curation workflow and product preservation/retirement
Be responsible for the execution of day-to-day data curation activities
Be responsible for planning and execution of key milestones and deliverables
Be responsible for generating, tracking, validating, and closing out data quality issues
Work with the CSTARS staff and data providers to track and move datasets through reception and curation.
Support the Development of processes for efficient effective QA/QC of datasets received
Provide feedback to team members to enhance the usability of all datasets, including documentation
Develop and support the preparation of programs, processes, scripts, tutorials to provide seamless access to commercial and public domain data
Provide scientific and technical support for remote sensing data processing tools and services and usability
This lead Geospatial Data Engineer 2 will primarily involve commercial synthetic aperture radar data, but will also be responsible for acquiring publicly available data from national and international agencies such as NASA, NOAA and ESA.
This position will lead CSTARS data management for oceanographic, atmospheric and hydrological data. Primarily, this will be data derived from space-based remote sensing platforms. The data managed will include airborne (e.g., drones) and in situ platforms (e.g., ships, buoys, HF radar.
This candidate be able to implement open-source methods for data format translation and product generation, subsetting and reprojecting data.
In this primary role, this candidate will be the lead developer and maintainer of software that will be used to import various data sets from a variety of sources to more standardized and interoperable data formats.
Required Skills:
M.S. in Geography or Oceanography, Information Science, or Earth Sciences, with 4-5 years of experience in a related field
Demonstrated ability to manage a data from variety of sources and track updates and modification
Strong experience in building and coordinating the development of dataset documentation for Earth science datasets
Knowledge and Experience with Geographic Information Systems (GIS)
technologies and software packages and tools
Proficiency with interpreted data languages for Earth science data, including one or more of the following: Python, MATLAB, R, and/or IDL (Python required)
Experience with reading, subsetting, interpreting, and utilizing geospatially referenced and structured data for research and applications.
Proficient with scientific software tools (e.g., R, NCO, HDF library, ArcGIS, and Python)
Familiarity and experience in metadata standards and conventions
Excellent data documentation skills, strong interpersonal skills, and working effectively as a team member
Experience with version control (i.e. GitHub) and agile development practices
Proficiency working in a UNIX/Linux command-line environment and shell scripting
Demonstrated ability to work both independently and in a dynamic team environment while dealing with multiple issues, tasks, and priorities concurrently
The University of Miami offers competitive salaries and a comprehensive benefits package including medical, dental, tuition remission and more.
UHealth-University of Miami Health System, South Florida's only university-based health system, provides leading-edge patient care powered by the ground breaking research and medical education at the Miller School of Medicine. As an academic medical center, we are proud to serve South Florida, Latin America and the Caribbean. Our physicians represent more than 100 specialties and sub-specialties, and have more than one million patient encounters each year. Our tradition of excellence has earned worldwide recognition for outstanding teaching, research and patient care. We're the challenge you've been looking for.
Patient safety is a top priority. As a result, during the Influenza (""the flu"") season (September through April), the University Of Miami Miller School Of Medicine requires all employees who provide ongoing services to patients, work in a location (all Hospitals and clinics) where patient care is provided, or work in patient care or clinical care areas, to have an annual influenza vaccination. Failure to meet this requirement will result in rescinding or termination of employment.
The University of Miami is an Equal Opportunity Employer - Females/Minorities/Protected Veterans/Individuals with Disabilities are encouraged to apply. Applicants and employees are protected from discrimination based on certain categories protected by Federal law. Click here for additional information.
Job Status:
Full time
Employee Type:
Staff
Pay Grade:
i107
Show more
Show less","Geospatial Data Management, Data Curation, Data Documentation, Remote Sensing, Earth Science, Data Quality Control, GIS, Python, MATLAB, R, IDL, NCO, HDF library, ArcGIS, Version Control, GitHub, Agile Development, UNIX/Linux, Shell Scripting","geospatial data management, data curation, data documentation, remote sensing, earth science, data quality control, gis, python, matlab, r, idl, nco, hdf library, arcgis, version control, github, agile development, unixlinux, shell scripting","agile development, arcgis, data curation, data documentation, data quality control, earth science, geospatial data management, gis, github, hdf library, idl, matlab, nco, python, r, remote sensing, shell scripting, unixlinux, version control"
Business Data Analyst,ASK Consulting,"Coral Gables, FL",https://www.linkedin.com/jobs/view/business-data-analyst-at-ask-consulting-3783668716,2023-12-17,Coral Gables,United States,Mid senior,Onsite,"""All candidates must be directly contracted by ASK Consulting on their payroll and cannot be subcontracted. We are unable to provide sponsorship at this moment"".
Job Title: Business Data Analyst
Location: Coral Gables , FL (on-site )
Duration: 04-Month Contract (Possible extensions)
Pay Range is $50 to $56.66/hr. on W2
Job Description:
Strong business analyst
Strong knowledge of data applications: Excel, Pivot Tables, summarizing data sets - not coding
Will be part of a small, nimble team
High aptitude, self-starter, communicate effectively
6 Sigma blackbelt
Initiative management
Analysis and interpretive skills processing data and information. Synthesizing and distilling important points.
Ability to highlight key insights and implications from data
Proficient with Tableau, Dataiku, SAAS a plus
Technical Skills:
Ability to work with data: connect varying data sets, create pivot tables, models and summarizations on growth initiatives, experience /onboarding, product penetration and client and advisors' segmentations. Track balance and revenue growth. Ability to synthesize data, identify and graph trends and pro
PowerPoint: Prepare high- impact, executive level presentations for project reporting, business updates and strategy. Consolidate inputs from project leads to prepare summaries in support of project meetings, milestone reporting, etc.
Problem Solving: Identify, communicate and escalate watch items and issues to move the projects forward. Offer suggestions and seek workarounds when encountering obstacles
Analytical Skills: Analysis and interpretive skills processing data and information. Synthesizing and distilling important points. Ability to highlight key insights and implications from evaluation of alternatives.
Other data applications (e.g. Tableau, Dataiku, SAAS) a plus
Communications Skills:
Written: Developing high impact, executive level presentations for various updates, strategic and initiative management presentations.
Verbal: Articulate clearly key concepts, requirements, accountabilities and timelines. Collaborate with team members, key constituents and stakeholders to achieve business objectives and drive progress.
About ASK:
ASK Consulting is an award-winning technology and professional services recruiting firm servicing Fortune 500 organizations nationally. With 5 nationwide offices, two global delivery centers, and employees in 42 states-ASK Consulting connects people with amazing opportunities
ASK Consulting is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all associates.
Show more
Show less","Business Analyst, Excel, Pivot Tables, Dataiku, Tableau, SAAS, PowerPoint, 6 Sigma Blackbelt, Initiative Management, Problem Solving, Analytical Skills","business analyst, excel, pivot tables, dataiku, tableau, saas, powerpoint, 6 sigma blackbelt, initiative management, problem solving, analytical skills","6 sigma blackbelt, analytical skills, business analyst, dataiku, excel, initiative management, pivot tables, powerpoint, problem solving, saas, tableau"
Lead Data Analyst,Hays,"Miami, FL",https://www.linkedin.com/jobs/view/lead-data-analyst-at-hays-3780695161,2023-12-17,Coral Gables,United States,Mid senior,Onsite,"Lead Data Analyst – Permanent – Miami, FL / Irving, TX – $ 125000.00-$ 135000.00
The final salary or hourly wage, as applicable, paid to each candidate/applicant for this position is ultimately dependent on a variety of factors, including, but not limited to, the candidate’s/applicant’s qualifications, skills, and level of experience as well as the geographical location of the position.
The end client is unable to sponsor or transfer visas for this position; all parties authorized to work in the US without sponsorship are encouraged to apply.
Our client is seeking a Lead Data Analyst in Miami, FL / Irving, TX.
Role Description
• Develop a deep understanding of core business processes and how performance is measured, and work with business and business analytics teams to discern strategic and analytical data and analytics needs and gaps
• Utilize diagnostic/predictive/prescriptive analysis to derive business insights and direct/drive action that increases revenue, decreases cost, and/or mitigates risk
• Work with management to prioritize work according to business value
• Develop appropriate visualizations, notifications and/or alerts to drive business action, and ensure results of actions can be measured and monitored
• Drive data quality by validating data & metrics calculations, surfacing quality issues, documenting metadata, and remediating data quality issues
• Participate in regular code reviews, promoting engineering best practices, including reuse and testing
• Work collaboratively with other team members and able to provide guidance, mentorship, and foster a collaborative environment to achieve departmental goals
• Mentor junior analysts on standards, best practices, and advanced analytics techniques
• Gathers and incorporate business intelligence data from a variety of sources including company data, industry and field reports, public information, or purchased sources
• Test and validate ETL pipelines
• Perform and facilitate data warehouse analysis
• Communicate effectively complex analytical concepts to non-technical stakeholders, present findings, and create clear and concise reports, dashboards, and presentations
Skills & Requirements
• 7+ years of Data Analytics experience in a large enterprise environment(s)
• Possess expertise in analyzing, interpreting, and deriving insights, and communicating those to drive change in business
• Possess expertise in SQL and RDBMS
• Possess proficiency in the ‘modern data stack’ – Snowflake, dbt, Fivetran, Airbyte, Prefect, and similar tools
• Possess proficiency with programming languages used in data science, such as Python
• Possess an understanding of fundamentals of Cloud (Azure, AWS) technologies and databases
• Possess experience with financial modeling
• Possess strong working knowledge of advanced probability and statistics - hypothesis testing, advanced regression, statistical process control
• Experience working with NoSQL and graph database systems is a significant plus
• Relevant Certifications
• Bachelor’s Degree.
Benefits/Other Compensation
Medical, Dental, Life Insurance, 401K.
Why Hays?
You will be working with a professional recruiter who has intimate knowledge of the industry and market trends. Your Hays recruiter will lead you through a thorough screening process in order to understand your skills, experience, needs, and drivers. You will also get support on resume writing, interview tips, and career planning, so when there’s a position you really want, you’re fully prepared to get it.
Nervous about an upcoming interview? Unsure how to write a new resume?
Visit the Hays Career Advice section to learn top tips to help you stand out from the crowd when job hunting.
Hays is committed to building a thriving culture of diversity that embraces people with different backgrounds, perspectives, and experiences. We believe that the more inclusive we are, the better we serve our candidates, clients, and employees. We are an equal employment opportunity employer, and we comply with all applicable laws prohibiting discrimination based on race, color, creed, sex (including pregnancy, sexual orientation, or gender identity), age, national origin or ancestry, physical or mental disability, veteran status, marital status, genetic information, HIV-positive status, as well as any other characteristic protected by federal, state, or local law. One of Hays’ guiding principles is ‘do the right thing’. We also believe that actions speak louder than words. In that regard, we train our staff on ensuring inclusivity throughout the entire recruitment process and counsel our clients on these principles. If you have any questions about Hays or any of our processes, please contact us.
In accordance with applicable federal, state, and local law protecting qualified individuals with known disabilities, Hays will attempt to reasonably accommodate those individuals unless doing so would create an undue hardship on the company. Any qualified applicant or consultant with a disability who requires an accommodation in order to perform the essential functions of the job should call or text 813.336.5570.
Drug testing may be required; please contact a recruiter for more information.
#LI-DNI
Show more
Show less","Data Analytics, SQL, RDBMS, Snowflake, Dbt, Fivetran, Airbyte, Prefect, Python, Cloud Technologies, Databases, Financial Modeling, Probability, Statistics, Hypothesis Testing, Regression, Statistical Process Control, NoSQL, Graph Database Systems","data analytics, sql, rdbms, snowflake, dbt, fivetran, airbyte, prefect, python, cloud technologies, databases, financial modeling, probability, statistics, hypothesis testing, regression, statistical process control, nosql, graph database systems","airbyte, cloud technologies, dataanalytics, databases, dbt, financial modeling, fivetran, graph database systems, hypothesis testing, nosql, prefect, probability, python, rdbms, regression, snowflake, sql, statistical process control, statistics"
Sr. Database Analyst - Data Engineer,University of Miami,"Miami, FL",https://www.linkedin.com/jobs/view/sr-database-analyst-data-engineer-at-university-of-miami-3769034993,2023-12-17,Coral Gables,United States,Mid senior,Onsite,"Current Employees:
If you are a current Staff, Faculty or Temporary employee at the University of Miami, please click here to log in to Workday to use the internal application process. To learn how to apply for a faculty or staff position using the Career worklet, please review this tip sheet.
The SCCC Data Engineer integrates healthcare data from multiple sources and builds and maintains systems for end-user consumption. This position also leads projects and serves as a liaison to various internal units, teams, and committees, and external vendors.
Primary Responsibilities:
Leads the design, development, implementation, and support of systems through the utilization of industry-current systems and technologies. Ensures the proper definition, control, and coordination of data elements and standards, and the facilitation of efficient data relationships, systems integration, and physical structure. Participates in the development and implementation of database management policies and standards for efficient quality control and data administration. Designs and operates client-server database performance monitoring software and utilizes hardware, software, and manual procedures to verify the integrity of database systems. Ensures the efficient implementation and modification of database systems. Develops and carries out project plans, including schedules and budgets. Reports variances from project plans and budgets with suggested corrective actions. Leads in the creation and maintenance of data dictionaries to improve organizational data integrity and facilitate user access to data. Develops and administers policies and controls for data security and integrity, including procedures for recovery and backup. Provides protection against unauthorized access and supports security and confidentiality requirements. Serves as lead technical resource to user community and other divisions in the areas of database design, development, and administration. Maintains knowledge of current products, database management and programming techniques and related information systems technology. Adheres to University and unit-level policies and procedures and safeguards University assets.
This list of duties and responsibilities is not intended to be all-inclusive and may be expanded to include other duties or responsibilities as necessary.
CORE QUALIFICATIONS Education:
Bachelor’s degree in Computer Science, Information Systems/Technology or relevant field; Master’s degree preferred.
Experience:
Minimum 5 years of experience in building/maintaining/supporting data systems in the healthcare or university academics fields.
Knowledge, Skills and Abilities:
Effective work experience developing full stack data analytics platforms, integrating, and supporting data systems and working with a combination of both relational and NoSQL databases
Experience with data visualization tools such as PowerBI, and programming languages/tools such as Python, SQL, SSIS, Node.js, Angular 2.0, Docker
Technical experience with cloud and hybrid infrastructures, architecture designs, data integrations and technology management
Experience with Windows and Linux server operating systems
Knowledge of Azure cloud computing technologies and data platforms
Ability to aggregate and write complex, efficient queries against large, disparate data sources
Experience developing custom API/Microservices/REST Web Services
Experience with XML, JSON, HL7, FHIR
Experience in full life cycle of complex software deployment projects using Agile/Scrum and SDLC
Understanding of applied statistics skills (distributions, regression, statistical testing)
The University of Miami offers competitive salaries and a comprehensive benefits package including medical, dental, tuition remission and more.
UHealth-University of Miami Health System, South Florida's only university-based health system, provides leading-edge patient care powered by the ground breaking research and medical education at the Miller School of Medicine. As an academic medical center, we are proud to serve South Florida, Latin America and the Caribbean. Our physicians represent more than 100 specialties and sub-specialties, and have more than one million patient encounters each year. Our tradition of excellence has earned worldwide recognition for outstanding teaching, research and patient care. We're the challenge you've been looking for.
Patient safety is a top priority. As a result, during the Influenza (""the flu"") season (September through April), the University Of Miami Miller School Of Medicine requires all employees who provide ongoing services to patients, work in a location (all Hospitals and clinics) where patient care is provided, or work in patient care or clinical care areas, to have an annual influenza vaccination. Failure to meet this requirement will result in rescinding or termination of employment.
The University of Miami is an Equal Opportunity Employer - Females/Minorities/Protected Veterans/Individuals with Disabilities are encouraged to apply. Applicants and employees are protected from discrimination based on certain categories protected by Federal law. Click here for additional information.
Job Status:
Full time
Employee Type:
Staff
Pay Grade:
i106
Show more
Show less","Data Analytics, Data Visualization, NoSQL Databases, PowerBI, Python, SQL, SSIS, Node.js, Angular, Docker, Azure, Linux, Agile, Scrum, SDLC, Statistics, API, Microservices, REST, XML, JSON, HL7, FHIR","data analytics, data visualization, nosql databases, powerbi, python, sql, ssis, nodejs, angular, docker, azure, linux, agile, scrum, sdlc, statistics, api, microservices, rest, xml, json, hl7, fhir","agile, angular, api, azure, dataanalytics, docker, fhir, hl7, json, linux, microservices, nodejs, nosql databases, powerbi, python, rest, scrum, sdlc, sql, ssis, statistics, visualization, xml"
Data Analyst 2,University of Miami,"Miami, FL",https://www.linkedin.com/jobs/view/data-analyst-2-at-university-of-miami-3715847023,2023-12-17,Coral Gables,United States,Mid senior,Onsite,"Current Employees:
If you are a current Staff, Faculty or Temporary employee at the University of Miami, please click here to log in to Workday to use the internal application process. To learn how to apply for a faculty or staff position using the Career worklet, please review this tip sheet.
The Department of Human Genetics is currently seeking a full time Data Analyst 2 to work in Miami. The Data Analyst 2 assists in developing or implementing systems to capture business operation information. Key responsibilities include analyzing data to ensure accuracy and reports any issues to management and identifying opportunities to enhance data collection and analysis. This role also works collaboratively with other internal areas and may guide less experienced data analysts.
Core Job Functions
Performs data analysis using various techniques, e.g. statistical analysis, explanatory and predictive modeling, data mining.
Determines best practices and recommends actionable insights for the current business operations.
Performs ad-hoc analyses and reporting, to aid management in the decision-making process. May build dashboards or reporting systems for end users pertaining to assigned project(s).
Provides business data interpretation, such as identifying trends or patterns of data sets.
Assists in the dissemination and reporting data, which may include presentations, workshops, and related activities.
Analyzes data to ensure accuracy and reports any issues to management. Identifies opportunities to enhance data collection and analysis.
Assists in developing or implementing systems to capture business operation information.
Guides less experienced data analysts.
Adheres to University and unit-level policies and procedures and safeguards University assets.
Department Specific Functions
Run expense reports and use Pivot Tables to generate breakdowns (monthly). Update “actuals” in reconciliation.
Run open encumbrance reports and identify POs that may be closed (quarterly).
Run reconciliation report to update reconciliation “obligations” and confirm accuracy of “actual expenses”, bringing attention to potential salaries that fell to clearing fund.
Maintain effort log and assist with effort reporting.
Process journal entries, payroll accounting adjustments, and payroll allocations.
This list of duties and responsibilities is not intended to be all-inclusive and may be expanded to include other duties or responsibilities as necessary.
Core Qualifications
Education:
Bachelor's Degree in relevant field
Experience:
Minimum 3 Years Of Relevant Experience
Knowledge, Skills and Attitudes:
Ability to exercise sound judgment in making critical decisions.
Skill in completing assignments accurately and with attention to detail.
Ability to analyze, organize and prioritize work under pressure while meeting deadlines.
Ability to work independently and/or in a collaborative environment.
Ability to communicate effectively in both oral and written form.
Ability to maintain effective interpersonal relationships.
Proficiency in computer software (i.e. Microsoft Office) especially ability to be advanced in Excel; for example, Pivot tables
The University of Miami offers competitive salaries and a comprehensive benefits package including medical, dental, tuition remission and more.
UHealth-University of Miami Health System, South Florida's only university-based health system, provides leading-edge patient care powered by the ground breaking research and medical education at the Miller School of Medicine. As an academic medical center, we are proud to serve South Florida, Latin America and the Caribbean. Our physicians represent more than 100 specialties and sub-specialties, and have more than one million patient encounters each year. Our tradition of excellence has earned worldwide recognition for outstanding teaching, research and patient care. We're the challenge you've been looking for.
Patient safety is a top priority. As a result, during the Influenza (""the flu"") season (September through April), the University Of Miami Miller School Of Medicine requires all employees who provide ongoing services to patients, work in a location (all Hospitals and clinics) where patient care is provided, or work in patient care or clinical care areas, to have an annual influenza vaccination. Failure to meet this requirement will result in rescinding or termination of employment.
The University of Miami is an Equal Opportunity Employer - Females/Minorities/Protected Veterans/Individuals with Disabilities are encouraged to apply. Applicants and employees are protected from discrimination based on certain categories protected by Federal law. Click here for additional information.
Job Status:
Full time
Employee Type:
Staff
Pay Grade:
c105
Show more
Show less","Data Analysis, Statistical Analysis, Explanatory and Predictive Modeling, Data Mining, Microsoft Office, Excel, Pivot Tables, Data Visualization, Reporting, Communication, Teamwork, Project Management, Attention to Detail, Time Management, Problem Solving, Critical Thinking, Decision Making, Initiative, Flexibility, Adaptability, Stress Tolerance, Interpersonal Skills, Problem Solving","data analysis, statistical analysis, explanatory and predictive modeling, data mining, microsoft office, excel, pivot tables, data visualization, reporting, communication, teamwork, project management, attention to detail, time management, problem solving, critical thinking, decision making, initiative, flexibility, adaptability, stress tolerance, interpersonal skills, problem solving","adaptability, attention to detail, communication, critical thinking, data mining, dataanalytics, decision making, excel, explanatory and predictive modeling, flexibility, initiative, interpersonal skills, microsoft office, pivot tables, problem solving, project management, reporting, statistical analysis, stress tolerance, teamwork, time management, visualization"
Lead Data Analyst,Lennar,"Miami, FL",https://www.linkedin.com/jobs/view/lead-data-analyst-at-lennar-3664937490,2023-12-17,Coral Gables,United States,Mid senior,Onsite,"Overview
The primary mission of the Lead Data Analyst role is to help our business evolve into a data-driven organization in which data and analytics drive differentiation in Lennar’s products and customer experiences. The position aims to drive improved business outcomes using insights gleaned from data and analytics and infuse them into Lennar’s corporate fabric. The Lead Data Analyst will wrangle, model, and analyze data, and will also understand the data flow from different applications to and from Enterprise Data Warehouse, to provide better quality data to drive improved decision making.
Responsibilities
Principal Duties and Responsibilities:
Partner very closely with business analytics teams to understand core business processes and identify analytics opportunities
Develop a deep understanding of core business processes and how performance is measured, and work with business and business analytics teams to discern strategic and analytical data and analytics needs and gaps
Utilize diagnostic/predictive/prescriptive analysis to derive business insights and direct/drive action that increases revenue, decreases cost, and/or mitigates risk
Work with management to prioritize work according to business value
Develop appropriate visualizations, notifications and/or alerts to drive business action, and ensure results of actions can be measured and monitored
Drive data quality by validating data & metrics calculations, surfacing quality issues, documenting metadata, and remediating data quality issues
Participate in regular code reviews, promoting engineering best practices, including reuse and testing
Work collaboratively with other team members and able to provide guidance, mentorship, and foster a collaborative environment to achieve departmental goals
Mentor junior analysts on standards, best practices, and advanced analytics techniques
Gathers and incorporate business intelligence data from a variety of sources including company data, industry and field reports, public information, or purchased sources
Test and validate ETL pipelines
Perform and facilitate data warehouse analysis
Communicate effectively complex analytical concepts to non-technical stakeholders, present findings, and create clear and concise reports, dashboards, and presentations
Qualifications
Educ ation and Experience Requirements:
7-10+ years providing business analysis & querying data, analyze and understand data issues and help resolve them
Undergraduate degree in data analytics, mathematics, statistics, engineering or equivalent field, or equivalent experience; Master’s degree is a plus
Creative self-starter who can analyze, interpret, and derive insights, and communicate those to drive change in business
Working knowledge of advanced probability and statistics - hypothesis testing, advanced regression, statistical process control
Expertise in SQL and RDBMS
Proficiency in the ‘modern data stack’ – Snowflake, dbt, Fivetran, Airbyte, Prefect, and similar tools
Proficiency with programming languages used in data science, such as Python
Strong analytical skills to effectively analyze complex business data, identify trends, and provide actionable insights
Ability and willingness to quickly learn new technologies
Expertise cleansing and integrating data from disparate data sources
Proficiency working with engineers on creating and testing ETL pipelines
Ability to learn the department's structure and business processes quickly and identify inefficiencies through analysis and translate that understanding into meaningful analytics
Strong adherence to core software engineering principles (code modularization, versioning, git, testing etc.)
Familiarity with Agile development such as Scrum or Kanban
Understanding of fundamentals of cloud (Azure, AWS) technologies and databases
Experience with financial modeling
Experience working with NoSQL and graph database systems is a significant plus
Lennar is an equal opportunity employer and complies with all applicable federal, state, and local fair employment practices laws.
Physical Requirements:
This is primarily a sedentary office position which requires the incumbent to have the ability to operate computer equipment. Finger dexterity is necessary.
Additional Requirements:
Team player
May be required to cross train for position(s) within the team organizational structure from time to time, as required by the Leadership
This description outlines the basic responsibilities and requirements for the position noted. This is not a comprehensive listing of all job duties of the Associates. Duties, responsibilities and activities may change at any time with or without notice.
Type
Regular Full-Time
Show more
Show less","SQL, RDBMS, Snowflake, dbt, Fivetran, Airbyte, Prefect, Python, Agile, Scrum, Kanban, Azure, AWS, NoSQL, Graph databases, ETL pipelines, Data visualization, Data quality management, Data integration, Data analysis, Business intelligence, Datadriven decision making, Hypothesis testing, Regression analysis, Statistical process control, Machine learning, Cloud computing, Financial modeling","sql, rdbms, snowflake, dbt, fivetran, airbyte, prefect, python, agile, scrum, kanban, azure, aws, nosql, graph databases, etl pipelines, data visualization, data quality management, data integration, data analysis, business intelligence, datadriven decision making, hypothesis testing, regression analysis, statistical process control, machine learning, cloud computing, financial modeling","agile, airbyte, aws, azure, business intelligence, cloud computing, data integration, data quality management, dataanalytics, datadriven decision making, dbt, etl pipelines, financial modeling, fivetran, graph databases, hypothesis testing, kanban, machine learning, nosql, prefect, python, rdbms, regression analysis, scrum, snowflake, sql, statistical process control, visualization"
Lead Data Analyst,Lennar,"Miami, FL",https://www.linkedin.com/jobs/view/lead-data-analyst-at-lennar-3679053555,2023-12-17,Coral Gables,United States,Mid senior,Onsite,"Overview
The primary mission of the Lead Data Analyst role is to help our business evolve into a data-driven organization in which data and analytics drive differentiation in Lennar’s products and customer experiences. The position sits in our Enterprise Data and Analytics team, which aims to drive improved business outcomes using insights gleaned from data and analytics and infuse them into Lennar’s corporate fabric. The Lead Data Analyst will wrangle, model, and analyze data, and will also understand the data flow from different applications to and from Enterprise Data Warehouse, to provide better quality data to drive improved decision making.
Responsibilities
Principal Duties and Responsibilities:
Partner very closely with business analytics teams to understand core business processes and identify analytics opportunities
Develop a deep understanding of core business processes and how performance is measured, and work with business and business analytics teams to discern strategic and analytical data and analytics needs and gaps
Utilize diagnostic/predictive/prescriptive analysis to derive business insights and direct/drive action that increases revenue, decreases cost, and/or mitigates risk
Work with management to prioritize work according to business value
Develop appropriate visualizations, notifications and/or alerts to drive business action, and ensure results of actions can be measured and monitored
Drive data quality by validating data & metrics calculations, surfacing quality issues, documenting metadata, and remediating data quality issues
Participate in regular code reviews, promoting engineering best practices, including reuse and testing
Mentor junior analysts on standards, best practices, and advanced analytics techniques
Query and transform data from multiple data sets, and integrate potentially large volumes of data from a variety of sources
Test and validate ETL pipelines
Qualifications
Education and Experience Requirements:
7-10+ years providing business analysis & querying data, analyze and understand data issues and help resolve them.
Undergraduate degree in data analytics, mathematics, statistics, engineering or equivalent field, or equivalent experience. Master’s degree is a plus
Creative self-starter who can analyze, interpret, and derive insights, and communicate those to drive change in business
Working knowledge of advanced probability and statistics - hypothesis testing, advanced regression, statistical process control
Expertise in SQL and RDBMS
Proficiency in the ‘modern data stack’ – Snowflake, dbt, Fivetran, Airbyte, Prefect, and similar tools
Proficiency with programming languages used in data science, such as Python
Ability and willingness to quickly learn new technologies
Expertise cleansing and integrating data from disparate data sources
Proficiency working with engineers on creating and testing ETL pipelines
Ability to learn business processes quickly and identify inefficiencies through analysis
Strong adherence to core software engineering principles (code modularization, versioning, git, testing etc.)
Familiarity with Agile development such as Scrum or Kanban
Understanding of fundamentals of cloud (Azure, AWS) technologies and databases
Experience with financial modeling
Experience working with NoSQL and graph database systems is a significant plus
Physical Requirements:
This is primarily a sedentary office position which requires the incumbent to have the ability to operate computer equipment. Finger dexterity is necessary.
Additional Requirements:
Team player
May be required to cross train for position(s) within the team organizational structure from time to time, as required by the Leadership
This description outlines the basic responsibilities and requirements for the position noted. This is not a comprehensive listing of all job duties of the Associates. Duties, responsibilities and activities may change at any time with or without notice.
Type
Regular Full-Time
Show more
Show less","Data Analytics, Business Analytics, Data Warehousing, Data Modeling, Data Visualization, ETL Pipelines, SQL, RDBMS, Snowflake, dbt, Fivetran, Airbyte, Prefect, Python, Agile Development, Scrum, Kanban, Cloud Computing, Azure, AWS, NoSQL, Graph Databases, Financial Modeling","data analytics, business analytics, data warehousing, data modeling, data visualization, etl pipelines, sql, rdbms, snowflake, dbt, fivetran, airbyte, prefect, python, agile development, scrum, kanban, cloud computing, azure, aws, nosql, graph databases, financial modeling","agile development, airbyte, aws, azure, business analytics, cloud computing, dataanalytics, datamodeling, datawarehouse, dbt, etl pipelines, financial modeling, fivetran, graph databases, kanban, nosql, prefect, python, rdbms, scrum, snowflake, sql, visualization"
Staff Data Engineer,Recruiting from Scratch,"Miami, FL",https://www.linkedin.com/jobs/view/staff-data-engineer-at-recruiting-from-scratch-3744388817,2023-12-17,Coral Gables,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our client is a dating app.
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, deliver analysis to further improve engagement in existing features, and empower our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Solve technical problems of the highest scope and complexity
Exert significant influence on the company’s analytical long-range goals and data architecture
Define and extend our internal standards for style, maintenance, and best practices for a high-scale data platform
Provide mentorship for all on your team to help them grow in their technical responsibilities
Propose ideas to improve the scale, performance, and capabilities of the Data Platform
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We'll Love About You
7+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field
7+ years experience using Python
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention
Location
: This role will be located in office twice a week minimum in Palo Alto, San Francisco or Chicago.
Salary Range: $180,000 - 250,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Business Intelligence, Data Science, Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, SQL, Kafka, Storm, SparkStreaming, Dimensional Data Modeling, Data Warehouses, ETL, Legal Compliance, Data Management, Data Classification, Data Retention","data engineering, business intelligence, data science, python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, kafka, storm, sparkstreaming, dimensional data modeling, data warehouses, etl, legal compliance, data management, data classification, data retention","airflow, business intelligence, data classification, data engineering, data management, data retention, data science, data warehouses, dimensional data modeling, docker, etl, helm, kafka, kubernetes, legal compliance, python, snowflake, spark, sparkstreaming, sql, storm"
Lead Data Analyst,Lennar,"Miami, FL",https://www.linkedin.com/jobs/view/lead-data-analyst-at-lennar-3651760996,2023-12-17,Coral Gables,United States,Mid senior,Onsite,"Overview
The primary mission of the Lead Data Analyst role is to help our business evolve into a data-driven organization in which data and analytics drive differentiation in Lennar’s products and customer experiences. The position sits in our Enterprise Data and Analytics team, which aims to drive improved business outcomes using insights gleaned from data and analytics and infuse them into Lennar’s corporate fabric. The Lead Data Analyst will wrangle, model, and analyze data, and will also understand the data flow from different applications to and from Enterprise Data Warehouse, to provide better quality data to drive improved decision making.
Responsibilities
Principal Duties and Responsibilities:
Partner very closely with business analytics teams to understand core business processes and identify analytics opportunities
Develop a deep understanding of core business processes and how performance is measured, and work with business and business analytics teams to discern strategic and analytical data and analytics needs and gaps
Utilize diagnostic/predictive/prescriptive analysis to derive business insights and direct/drive action that increases revenue, decreases cost, and/or mitigates risk
Work with management to prioritize work according to business value
Develop appropriate visualizations, notifications and/or alerts to drive business action, and ensure results of actions can be measured and monitored
Drive data quality by validating data & metrics calculations, surfacing quality issues, documenting metadata, and remediating data quality issues
Participate in regular code reviews, promoting engineering best practices, including reuse and testing
Mentor junior analysts on standards, best practices, and advanced analytics techniques
Query and transform data from multiple data sets, and integrate potentially large volumes of data from a variety of sources
Test and validate ETL pipelines
Qualifications
Education and Experience Requirements:
7-10+ years providing business analysis & querying data, analyze and understand data issues and help resolve them.
Undergraduate degree in data analytics, mathematics, statistics, engineering or equivalent field, or equivalent experience. Master’s degree is a plus
Creative self-starter who can analyze, interpret, and derive insights, and communicate those to drive change in business
Working knowledge of advanced probability and statistics - hypothesis testing, advanced regression, statistical process control
Expertise in SQL and RDBMS
Proficiency in the ‘modern data stack’ – Snowflake, dbt, Fivetran, Airbyte, Prefect, and similar tools
Proficiency with programming languages used in data science, such as Python
Ability and willingness to quickly learn new technologies
Expertise cleansing and integrating data from disparate data sources
Proficiency working with engineers on creating and testing ETL pipelines
Ability to learn business processes quickly and identify inefficiencies through analysis
Strong adherence to core software engineering principles (code modularization, versioning, git, testing etc.)
Familiarity with Agile development such as Scrum or Kanban
Understanding of fundamentals of cloud (Azure, AWS) technologies and databases
Experience with financial modeling
Experience working with NoSQL and graph database systems is a significant plus
Physical Requirements:
This is primarily a sedentary office position which requires the incumbent to have the ability to operate computer equipment. Finger dexterity is necessary.
Additional Requirements:
Team player
May be required to cross train for position(s) within the team organizational structure from time to time, as required by the Leadership
This description outlines the basic responsibilities and requirements for the position noted. This is not a comprehensive listing of all job duties of the Associates. Duties, responsibilities and activities may change at any time with or without notice.
Type
Regular Full-Time
Show more
Show less","Data Analysis, Data Warehousing, Data Modeling, Data Visualization, Data Wrangling, SQL, RDBMS, Python, Snowflake, DBT, Fivetran, Airbyte, Prefect, Agile, Scrum, Kanban, NoSQL, Graph databases, Cloud Computing, Azure, AWS, Financial Modeling, ETL Pipelines, Data Quality, Data Engineering, Data Integration, Hypothesis Testing, Regression Analysis, Statistical Process Control","data analysis, data warehousing, data modeling, data visualization, data wrangling, sql, rdbms, python, snowflake, dbt, fivetran, airbyte, prefect, agile, scrum, kanban, nosql, graph databases, cloud computing, azure, aws, financial modeling, etl pipelines, data quality, data engineering, data integration, hypothesis testing, regression analysis, statistical process control","agile, airbyte, aws, azure, cloud computing, data engineering, data integration, data quality, data wrangling, dataanalytics, datamodeling, datawarehouse, dbt, etl pipelines, financial modeling, fivetran, graph databases, hypothesis testing, kanban, nosql, prefect, python, rdbms, regression analysis, scrum, snowflake, sql, statistical process control, visualization"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Miami, FL",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3748830431,2023-12-17,Coral Gables,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our next evolution is underway as a newly public company looking to expand and continue to build meaningful experiences for our users. From social issues to original content, we’re blazing innovative paths with impact for our community, all while leveraging the latest tech stacks and striving for engineering excellence. At the heart of our work in this new chapter is a shared set of core values: openness and exploration, a bias for action, and strong support of the LGBTQ community.
With a track record of strong financial performance and plans for continued headcount growth, we’re looking to build a team of talented, passionate, and open-minded people who believe in our mission, align with our values, and are excited to work at the intersection of innovative technology and social impact. Come be a part of this exciting journey with us.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location: Palo Alto, San Francisco or Chicago
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Kafka, TDD, Data engineering, AWS, Data governance, Docker, Spark, SRE, Airflow, Jupyter Notebook, Kubernetes, ETL, SQL, Python, Distributed systems, Continuous integration, Continuous delivery, Data modelling, Storm, Realtime streaming, Data Warehouses, Dimensional data modeling, Data management tools, PySpark, Helm","kafka, tdd, data engineering, aws, data governance, docker, spark, sre, airflow, jupyter notebook, kubernetes, etl, sql, python, distributed systems, continuous integration, continuous delivery, data modelling, storm, realtime streaming, data warehouses, dimensional data modeling, data management tools, pyspark, helm","airflow, aws, continuous delivery, continuous integration, data engineering, data governance, data management tools, data modelling, data warehouses, dimensional data modeling, distributed systems, docker, etl, helm, jupyter notebook, kafka, kubernetes, python, realtime streaming, spark, sql, sre, storm, tdd"
"Senior Software Engineer, Data",Doximity,"Miami, FL",https://www.linkedin.com/jobs/view/senior-software-engineer-data-at-doximity-3784857732,2023-12-17,Coral Gables,United States,Mid senior,Remote,"We're looking for a pragmatic, senior engineer with significant experience building Data Engineering Pipelines and supporting Data Infrastructure systems.
Doximity is transforming the healthcare industry. Our mission is to help physicians save time so they can provide better care for patients. We provide doctors modern communication, workflow, and continuing education tools. Over two million healthcare professionals (including over 80% of U.S. doctors) rely on our products.
Our goal is to improve inefficiencies in our $4 trillion U.S. healthcare system, and we love building innovative technology that has a real, meaningful impact on physicians’ and patients’ lives. Check out our technology stack and read our blog.
We are open to hiring someone in North, Central, or South America.
About you
Expert in Python and SQL. You have developed maintainable data pipelines with them.
Foremost an engineer. You exemplify high code quality and guide others.
Good engineering tools. You are experienced in creating automated testing, applying design patterns, and other engineering best practices.
See a project from end-to-end. You have led several projects, your own ideas and other's. This means from idea generation to planning, execution, through delivering.
You see your teammates as your first “customer”. You strive for clarity and simplicity in your approach to engineering and communication.
Example of how you'll make and impact
Help us migrate from EMR/Spark to Snowpark.
Migrate pipelines from self-managed EC2 to Kubernetes runtime environments.
Collaborate with product managers, data analysts, and machine learning engineers to develop pipelines and ETL tasks in order to facilitate the extraction of insights.
Establish data architecture processes and practices that can be scheduled, automated, replicated and serve as standards for other teams to leverage.
Develop data pipelines and dashboards from data platform metadata to assist with visualization and analysis of the data platform health and its costs.
Find inefficient, wasteful, or redundant processes, pipelines, & queries used on the data platform and seek to address their shortcomings.
Develop tools to empower product focused data teams to develop pipelines efficiently.
Compensation
The US total compensation range for this full-time position is $175,000 - $220,000 (inclusive of salary + equity). Our ranges are determined by role and level. The range displayed on each job posting reflects the approximate total target compensation for the position across the US. Within the range, individual pay is determined by factors including relevant skills, experience, and education/training. Please note that the compensation listed does not include benefits.
More on /Benefits/Perks
Doximity is proud to offer industry-leading benefits to our full time employees. Some of our offerings include:
Medical, dental, vision offerings for you and your family
401k with matching program
Employee stock purchase plan
Family planning support, Childcare FSA, and parental leave
Life, AD&D, and Disability
Generous time off, holidays and paid company trips
Wellness benefits…plus many more!
More About Doximity…
For the past decade, it’s been our mission to help every physician be more productive so they can provide better care for their patients. We believe that when doctors are connected, the healthcare system works better and patients benefit. Doximity enables our verified clinician members to collaborate with colleagues, stay up-to-date with the latest medical news and research, manage their careers, and conduct virtual patient visits. Today, Doximity is the leading digital platform for U.S. medical professionals, with over 80% of physicians, 50% of all nurse practitioners and physician assistants, and 90% of graduating medical students as members.
Joining Doximity means being part of an incredibly talented and humble team passionate about improving inefficiencies in our $4.3 trillion U.S. healthcare system. We are a team of doers who solve problems everyday by treating obstacles like an adventure, and we love creating technology that has a real, meaningful impact on people’s lives. Doxers are committed to working towards a more equitable world both within and beyond our office walls. This starts by fostering an inclusive and diverse work environment where differences are valued and all employees are encouraged to bring their full, authentic selves to work daily. To learn more about our team, culture, and users, check out our careers page, company blog, and engineering blog. We’re growing fast, and there’s plenty of opportunity for you to make an impact—join us! For more information, visit Doximity.com.
____________________________________________
EEOC Statement
Doximity is proud to be an equal opportunity employer, and committed to providing employment opportunities regardless of race, religious creed, color, national origin, ancestry, physical disability, mental disability, medical condition, genetic information, marital status, sex, gender, gender identity, gender expression, pregnancy, childbirth and breastfeeding, age, sexual orientation, military or veteran status, or any other protected classification. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law.
Show more
Show less","Python, SQL, Data Pipelines, Data Infrastructure, EMR/Spark, Snowpark, Kubernetes, ETL, Data Architecture, Dashboards, Data Visualization, Data Analysis, Machine Learning, Automated Testing, Design Patterns, Engineering Best Practices, Project Management, Communication, Collaboration, Problem Solving, Innovation, Efficiency, Healthcare, Medical, Technology","python, sql, data pipelines, data infrastructure, emrspark, snowpark, kubernetes, etl, data architecture, dashboards, data visualization, data analysis, machine learning, automated testing, design patterns, engineering best practices, project management, communication, collaboration, problem solving, innovation, efficiency, healthcare, medical, technology","automated testing, collaboration, communication, dashboard, data architecture, data infrastructure, dataanalytics, datapipeline, design patterns, efficiency, emrspark, engineering best practices, etl, healthcare, innovation, kubernetes, machine learning, medical, problem solving, project management, python, snowpark, sql, technology, visualization"
Data Architect (Azure / Big Data),"J. J. Keller & Associates, Inc.","Miami Beach, FL",https://www.linkedin.com/jobs/view/data-architect-azure-big-data-at-j-j-keller-associates-inc-3760255234,2023-12-17,Coral Gables,United States,Mid senior,Remote,"Requisition #:
18969
Functional Area:
Software Development
Employment Type:
Full-Time
Work Options:
Remote / Work from Home in the US
Work Hours:
Standard Business Hours
Why work at J.J. Keller?
Latest MS tech stack: Our projects use the latest MS tech stack, Microservices, CI/CD, Automated Testing, Cloud Architecture and Predictive Analytics/AI/ML.
Emerging technologies: We strive to keep current with emerging technologies to prevent platforms from slipping into the dreaded “legacy applications” category.
Native solutions: Our 200+ Developers are building J. J. Keller’s line of native cloud robust solutions (web/mobile/IoT) from the ground up. They are passionate about technology, growing their skills and doing meaningful work.
User-centered focus: Although we still have some proprietary applications, we’re continually evolving our technology footprint to provide a better, more intuitive user experience for our customers.
Collaborative environment: The average software development team has 7-10 Developers who work closely with our QA’s and a dedicated Product Management team (Product Owner, Business Analyst & UI/UX resource).
80% Remote: We’ve embraced remote work for many years – long before the pandemic – so you can work remotely and feel confident in joining our remote culture.
Position Summary
The Data Architect is responsible for expanding the use of the company’s data as a strategic enabler. This position will create an inventory of the enterprise’s data and establish the tools to ensure that data will be stored and utilized effectively, and that the architecture adheres to industry best-practices for scalability, maintainability, usability, performance, security, and portability. This includes capturing, integrating, organizing, centralizing and maintaining data.
This individual will partner with internal stakeholders to provide strategic, technical leadership related to data architecture and utilization. Additionally, the Data Architect will perform research to discover new data management models and techniques and will be responsible for connecting existing data architecture with new ones as technology continues to emerge.
Job Responsibilities
Develops strategic plans for structured and non-structured data solutions, including but not limited to: transition strategies, bundling strategies, risk assessment, and roadmap planning.
Develops, constructs, tests and maintains structured and non-structured data architectures, aligning them with business requirements.
Ensures data is properly received, transformed, stored, and made accessible for analytics.
Identifies ways to improve data and analytics reliability, efficiency and quality.
Provides technical leadership, mentorship and training to other technical resources.
Collaborates with internal stakeholders to understand future internal needs tied to data and makes recommendations on technologies to support business strategies.
Assists with the development of visualizations of analytics results.
Evaluates new technological developments and evolving business requirements that impact big data/AI/Machine Learning strategies. Implements recommendations as appropriate.
Identifies gaps, strategic impacts, financial impacts and the risk profile of recommended technical solutions and provides technical support of technologies in use.
Troubleshoots complex technical issues for big data solutions. Identifies and mitigates risks on projects.
Qualifications
Experience:
10+ years experience as a data architect or data engineer.
Strong experience working with SQL Server is required.
Experience working with Big Data is required.
Working experience with Azure (DevOps, Functions, Data Lake, etc.).
Experience in query and programming languages.
Experience with data visualization and reporting packages such as Cognos Analytics, Watson Analytics, PowerBI, etc. is a plus.
Education
Bachelor's degree in a technology-related field.
Microsoft Professional Program for Big Data and/or AWS Big Data Certified a plus.
Other Skills/Qualifications
Demonstrated ability to architect complex systems.
Expertise with data models, database architecture and design for structured and unstructured data.
Ability to quickly digest and analyze new requirements, and then execute emerging technologies to facilitate rapid prototyping.
Strong initiative with the ability to identify and plan the execution of a technical or infrastructure architecture strategy.
Strong verbal and written communication skills -ability to communicate and collaborate effectively with various stakeholders including technical, non-technical and executive leadership.
Benefits
Medical / Dental / Vision Insurance
Annual Reviews, Merit Increases + Quarterly Bonus Program
401(k) with Employer Match + Annual Profit Sharing
17 PTO Days + 8 Paid Company Holidays + 1 Paid Floating Holiday
Work/Life Balance & Flex Time
Annual Learning & Development Subscriptions
Free Onsite Wellness Clinic for those associates near our corporate office + free telehealth coverage for all associates regardless of where you live
Free access to FLEX by Fitness on Demand providing 24/7 access to online workout videos
Strong company culture that fosters internal growth and development
Computer Equipment Provided for Home Office
We Protect People & The Businesses They Run™
Every associate at J. J. Keller makes a difference by creating safer, more respectful workplaces. Whether serving our customers directly with expertise in safety and regulatory compliance or supporting the business with specialized skills, together we contribute to better workplaces for people across North America.
J. J. Keller History:
November 1, 2023, marked 70 years of business for J. J. Keller & Associates, Inc. Click HERE to take a tour through three generations of this family-owned business – from our founding as a one -man consulting firm through decades of delivering on our purpose of protecting people and the businesses they run.
J. J. Keller Career Stories
: Click HERE to hear from our associates about what they have to say about life as an associate at J. J. Keller.
J. J. Keller Earns 7th Great Place to Work Certification™:
Click HERE to find out what makes J. J. Keller great.
J. J. Keller Certified as a Top 100 Most Loved Workplace® in America:
Click HERE to find out why our associates LOVE working at J. J. Keller.
2023 Top Company for Women to Work for in Transportation:
Click HERE to learn more about this prestigious recognition.
J. J. Keller provides a competitive benefit package which includes the following (eligibility requirements apply): Medical, Dental, and Vision Insurance, 401(k) and Profit Sharing Plan, etc. The compensation range for this role is $143,000.00 to $168,000.00 which varies depending on factors including, but not limited to, a candidate’s overall experience and geographic location. Note that J. J. Keller is not currently recruiting employees to work in California.
If you experience system-related issues or need assistance with the online application, please call (920) 720-7700.
Professional Referral Program:
Not the right role for you, but know someone who could be a great fit? Click HERE to refer them to us through our Professional Referral Programand you will earn a
cash payment
if your referral is hired.
J. J. Keller & Associates, Inc. is an Equal Opportunity Employer and does not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity, status as a veteran, and basis of disability or any other federal, state or local protected class.
Show more
Show less","Azure, Machine Learning, SQL Server, DevOps, SQL, Cognos Analytics, Watson Analytics, PowerBI, AWS, Data visualization, Reporting, AI, Big Data, Microservices, CI/CD, Cloud Architecture, Data Analytics, Java, Python, C#, R, Data Engineer, Data Scientist, Data model design, Data architecture","azure, machine learning, sql server, devops, sql, cognos analytics, watson analytics, powerbi, aws, data visualization, reporting, ai, big data, microservices, cicd, cloud architecture, data analytics, java, python, c, r, data engineer, data scientist, data model design, data architecture","ai, aws, azure, big data, c, cicd, cloud architecture, cognos analytics, data architecture, data model design, data scientist, dataanalytics, dataengineering, devops, java, machine learning, microservices, powerbi, python, r, reporting, sql, sql server, visualization, watson analytics"
"Data Conversion Developer, Senior Associate",PwC,"Miami, FL",https://www.linkedin.com/jobs/view/data-conversion-developer-senior-associate-at-pwc-3749933947,2023-12-17,Coral Gables,United States,Mid senior,Remote,"Specialty/Competency:
Functional & Industry Technologies
Industry/Sector:
Not Applicable
Time Type:
Full time
Travel Requirements:
Up to 80%
A career within Functional and Industry Technologies services will provide you with the opportunity to build secure and new digital experiences for customers, employees, and suppliers. We focus on improving apps or developing new apps for traditional and mobile devices as well as conducting usability testing to find ways to improve our clients’ user experience. Our team helps clients transform their business through enabling technologies across marketing, finance and operations in the functional areas such as Maximo and PowerPlant.
To really stand out and make us fit for the future in a constantly changing world, each and every one of us at PwC needs to be a purpose-led and values-driven leader at every level. To help us achieve this we have the PwC Professional; our global leadership development framework. It gives us a single set of expectations across our lines, geographies and career paths, and provides transparency on the skills we need as individuals to be successful and progress in our careers, now and in the future.
Responsibilities
As a Senior Associate, you'll work as part of a team of problem solvers, helping to solve complex business issues from strategy to execution. PwC Professional skills and responsibilities for this management level include but are not limited to:
Use feedback and reflection to develop self awareness, personal strengths and address development areas.
Delegate to others to provide stretch opportunities, coaching them to deliver results.
Demonstrate critical thinking and the ability to bring order to unstructured problems.
Use a broad range of tools and techniques to extract insights from current industry or sector trends.
Review your work and that of others for quality, accuracy and relevance.
Know how and when to use tools available for a given situation and can explain the reasons for this choice.
Seek and embrace opportunities which give exposure to different situations, environments and perspectives.
Use straightforward communication, in a structured way, when influencing and connecting with others.
Able to read situations and modify behavior to build quality relationships.
Uphold the firm's code of ethics and business conduct.
Basic Qualifications
Minimum Degree Required:
Bachelor Degree
Minimum Years Of Experience
4 years
Preferred Qualifications
Degree Preferred:
Master Degree
Certification(s) Preferred
Azure Data Engineer Associate
Databricks Certified Data Engineer Associate
Preferred Fields Of Study
Computer and Information Science, Computer Engineering, Computer Management, Management Information Systems, Information Technology
Preferred Knowledge/Skills
Demonstrates a thorough level of abilities with, and/or a proven record of success as both an individual contributor and team member, identifying and addressing client needs:
Supports in data analysis techniques to assess source data structures, identify mapping requirements, and define transformation rules for data conversion into Maximo;
Leads Maximo's modules and functionalities related to Asset Management and Work Order Management and IBM Maximo, including its data structures, configuration settings, and integration capabilities;
Identifies relational databases, preferably experience in working with databases commonly used in Maximo, such as IBM DB2, Oracle, Microsoft SQL Server, along with familiarity with Maximo's Integration Framework (MIF) and its capabilities for data integration and conversion;
Supports in designing and implementing data extraction, transformation, and loading processes for Maximo data conversion;
Showcases understanding in SQL and database querying languages to extract and manipulate data from source systems along with understanding ETL tools and methodologies commonly used in Maximo data conversion;
Identifies technologies commonly used with Maximo, including web services (SOAP, RESTful APIs), XML, JSON, and other relevant data exchange formats;
Supports in pipeline architecture and development using one of the tools such as Azure ADF, AWS Glue, SSIS, DataBricks (multiple preferred);
Utilizes data cleansing techniques and methodologies to ensure the integrity and accuracy of converted data in Maximo;
Developes data cleansing functional business rules as per Maximo Business Object (MBO) definitions for source to Maximo conversion requirements;and comprehensive testing plans and executing validation processes to verify the accuracy and integrity of converted data in Maximo;
Customizes Maximo options, such as Automation Scripts, Java Customizations, Database Configuration, or Application Designer, to support data conversion requirements;
Identifies integrations within Maximo with other enterprise systems, such as ERP systems, GIS systems, or asset management systems. Knowledge of integration patterns, data synchronization, and data exchange protocols; and,
Showcases work experience as a Data Engineer, Data Architect or similar role, along with experience in programming languages including Python, PySpark, Scala, SQL.
Learn more about how we work: https://pwc.to/how-we-work
PwC does not intend to hire experienced or entry level job seekers who will need, now or in the future, PwC sponsorship through the H-1B lottery, except as set forth within the following policy: https://pwc.to/H-1B-Lottery-Policy.
All qualified applicants will receive consideration for employment at PwC without regard to race; creed; color; religion; national origin; sex; age; disability; sexual orientation; gender identity or expression; genetic predisposition or carrier status; veteran, marital, or citizenship status; or any other status protected by law. PwC is proud to be an affirmative action and equal opportunity employer.
For positions based in San Francisco, consideration of qualified candidates with arrest and conviction records will be in a manner consistent with the San Francisco Fair Chance Ordinance.
Applications will be accepted until the position is filled or the posting is removed, unless otherwise set forth on the following webpage. Please visit this link for information about anticipated application deadlines: https://pwc.to/us-application-deadlines
For positions in California, Colorado, Hawaii, Nevada, New York State, or Washington State, or for opportunities that will report to a supervisor, office or other work site in New York State, please visit the following link for pay range information: https://pwc.to/payrange-v1-advisoryseniorassociate
Show more
Show less","Azure Data Engineer Associate, Databricks Certified Data Engineer Associate, SQL, ETL tools, Python, PySpark, Scala, Java, Maximo, PowerPlant, Data conversion, Data cleansing, Data integration, Data migration, Web services, SOAP, RESTful APIs, XML, JSON, Data architecture, Data engineering, Business intelligence, Data analysis, Data management","azure data engineer associate, databricks certified data engineer associate, sql, etl tools, python, pyspark, scala, java, maximo, powerplant, data conversion, data cleansing, data integration, data migration, web services, soap, restful apis, xml, json, data architecture, data engineering, business intelligence, data analysis, data management","azure data engineer associate, business intelligence, data architecture, data conversion, data engineering, data integration, data management, data migration, dataanalytics, databricks certified data engineer associate, datacleaning, etl tools, java, json, maximo, powerplant, python, restful apis, scala, soap, spark, sql, web services, xml"
Business Data Analyst,INSPYR Solutions,"Miami, FL",https://www.linkedin.com/jobs/view/business-data-analyst-at-inspyr-solutions-3779675922,2023-12-17,Coral Gables,United States,Mid senior,Hybrid,"Title
: Business Data Analyst
Location
: Miami, FL
Duration
: Contract to Hire
Compensation:
$35-45/hour
Work Requirements:
US Citizen, GC Holders or Authorized to Work in the U.S.
The Business Data Analyst role reports within the IT Controls and Compliance organization. This candidate will be responsible for analyzing and interpreting data sets used to drive compliance across multiple disciplines including Record Retention, Privacy, and other regulatory and operational requirements. They will elicit and analyze data and overall project requirements to achieve organizational goals and are responsible for working with multiple teams bridging communication gaps.
Principal Duties And Responsibilities
System Analysis
Proficient in SQL programming or comparable to query databases, conducting statistical analysis, and creating insightful reports.
Ability to present findings in a clear and measurable way.
Ability to create process flow documentation.
Experience with project management tools including Microsoft project, Jira, Smartsheet, or other equivalent.
Above average skill in Microsoft Word, Excel, PowerPoint, and Visual Basic.
Business Analysis
Strong analytical skills with the ability to understand data structure across multiple applications.
Ability to understand IT system configurations and data structures and play key role in analyzing data to identify data ready for destruction.
Leverage advanced analytics and assist with data cleansing, migration, and integration activities.
Ability to prepare business requirements documents and work as part of a team to build out new processes and test outcomes to ensure it meets requirements.
Ability to create reports using standard reporting tools for key applications.
Project Tracking
Ability to lead multiple project deliverables and ensure multiple teams are delivering based on due dates.
Identify project stakeholders, requirements, and overall timelines
Define scope, schedule, risks, and stakeholder roles and responsibilities.
Maintain the business support tracking software in a timely manner.
Provide regular project status updates and overall communication on project deliverables
Our benefits package includes:
(EXCLUDE on perm placements)
Comprehensive medical benefits
Competitive pay, 401(k)
Retirement plan
…and much more!
About INSPYR Solutions
Technology is our focus and quality is our commitment. As a national expert in delivering flexible technology and talent solutions, we strategically align industry and technical expertise with our clients’ business objectives and cultural needs. Our solutions are tailored to each client and include a wide variety of professional services, project, and talent solutions. By always striving for excellence and focusing on the human aspect of our business, we work seamlessly with our talent and clients to match the right solutions to the right opportunities. Learn more about us at inspyrsolutions.com.
INSPYR Solutions provides Equal Employment Opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability, or genetics. In addition to federal law requirements, INSPYR Solutions complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities.
Show more
Show less","SQL, Statistical Analysis, Data Cleansing, Data Migration, Data Integration, Microsoft Project, Jira, Smartsheet, Microsoft Word, Microsoft Excel, Microsoft PowerPoint, Visual Basic, Business Requirements Documents, Reporting Tools, Project Deliverables, Project Stakeholders, Project Requirements, Project Timelines, Project Scope, Project Schedule, Project Risks, Stakeholder Roles, Stakeholder Responsibilities, Business Support Tracking Software, Project Status Updates","sql, statistical analysis, data cleansing, data migration, data integration, microsoft project, jira, smartsheet, microsoft word, microsoft excel, microsoft powerpoint, visual basic, business requirements documents, reporting tools, project deliverables, project stakeholders, project requirements, project timelines, project scope, project schedule, project risks, stakeholder roles, stakeholder responsibilities, business support tracking software, project status updates","business requirements documents, business support tracking software, data integration, data migration, datacleaning, jira, microsoft excel, microsoft powerpoint, microsoft project, microsoft word, project deliverables, project requirements, project risks, project schedule, project scope, project stakeholders, project status updates, project timelines, reporting tools, smartsheet, sql, stakeholder responsibilities, stakeholder roles, statistical analysis, visual basic"
Cloud Data Architect,"GSPANN Technologies, Inc","Miami, FL",https://www.linkedin.com/jobs/view/cloud-data-architect-at-gspann-technologies-inc-3782582914,2023-12-17,Coral Gables,United States,Mid senior,Hybrid,"About GSPANN
Headquartered in Milpitas, California (U.S.A.), GSPANN provides consulting and IT services to global clients, ranging from mid-size to Fortune 500 companies. With our experience in retail, high technology, and manufacturing, we help our clients transform and deliver business value by optimizing their IT capabilities, practices, and operations. Counting on our nine offices, including five delivery centers, and over 1850 employees globally, we offer the intimacy of a boutique consultancy with the capabilities of a large IT services firm.
Cloud Data Architect
Location-Los Angeles, CA / Orlando, FL/ Miami, FL (Locals only or willing to relocate)
Job Type-Fulltime
Responsibilities –
Technical Leadership (Data Architecture & Engineering )
Program Leadership
Coordination - project coordination with client & GSPANN distributed team
Skills required: (Multi Cloud Data Exposure)
Experienced & proficient in
Architecting & implementing full lifecycle data engineering projects on AWS & Azure cloud; Exposure to Google Cloud would be a plus
Designing solutions using leading data platforms like Snowflake, Databricks, Synapse, BigQuery and Redshift
Programming – SQL, PySpark
Ideal Profile should have
~ 12 years IT experience (not more than 15 years), out of which ~8 years working on cloud data platforms
Relevant experience working as an architect or engineer on cloud data engineering projects with proven leadership
Good communication and presentation skills
Highly organized, collaborative and team player
Working at GSPANN
GSPANN is a diverse, prosperous, and rewarding place to work. We provide competitive benefits, educational assistance, and career growth opportunities to our employees. Every employee is valued for their talent and contribution. Working with us will give you an opportunity to work globally with some of the best brands in the industry.
The company does and will take affirmative action to employ and advance in the employment of individuals with disabilities and protected veterans and to treat qualified individuals without discrimination based on their physical or mental disability status. GSPANN is an equal opportunity employer for minorities/females/veterans/disabled.
Show more
Show less","AWS, Azure, Google Cloud, Snowflake, Databricks, Synapse, BigQuery, Redshift, SQL, PySpark, Cloud data engineering projects","aws, azure, google cloud, snowflake, databricks, synapse, bigquery, redshift, sql, pyspark, cloud data engineering projects","aws, azure, bigquery, cloud data engineering projects, databricks, google cloud, redshift, snowflake, spark, sql, synapse"
"Lead, Data Platform Engineer",Royal Caribbean Group,"Miami, FL",https://www.linkedin.com/jobs/view/lead-data-platform-engineer-at-royal-caribbean-group-3712724625,2023-12-17,Coral Gables,United States,Mid senior,Hybrid,"Position Overview
Data Analytics & AI department at RCG is looking for a highly skilled and experienced Data Platform Lead to design, build, and manage our data platform on Azure and Databricks. The successful candidate will be responsible for managing the end-to-end data platform development process, including data ingestion, processing, storage, and analysis. This role requires a strong technical background in data platform architecture, cloud computing, and big data technologies.
This role carries primary responsibilities as a Platform Lead / Platform Engineer and Azure Platform Admin, while also contributing to ML Ops.
Responsibilities
Design, build and configure a highly scalable and robust data platform on Azure and Databricks.
Data platform shuld have capabilities of seamless integration & extensibility of existing cloud services to any new program/project initiative.
Develop and implement data ingestion and processing pipelines using Azure services such as Azure Data Factory, Azure Stream Analytics, and Databricks
Collaborate with cross-functional teams to ensure data platform meets business requirements and industry best practices
Manage cloud infrastructure on Azure platform and maintain Azure environment to ensure maxium efficiency, scalability and security.
Develop & implement cloud automation and orchestration using tools auch as Azure resource manager, Azure DevOps etc.
Collaborate with offshore team members with development & deployment activities, per project requirements.
Required Skills
Platform Engineering:
Proficiency in Python, Scala, or SQL.
Expertise in Apache Spark, Hadoop, or Apache Kafka.
Cloud experience in Azure (preferred) or AWS.
Deployment and management of Databricks clusters.
Mastery of Azure Data Factory (ADF) for integration.
Expertise with Databricks workspace, runtime, and Databricks Connect.
Understanding of Databricks Unity Catalog.
Knowledge of Delta Live Tables and Delta Engine.
Familiarity with Delta Lake architecture.
Strong SQL skills.
Azure Platform Administration
Management of Azure Cloud Services, including VMs, Storage Accounts, and networks.
Azure Security Management for policies and access controls.
Knowledge of Azure Active Directory and network security.
Performance monitoring, resource optimization, backup, and recovery.
High Availability (HA), Disaster Recovery (DR), and failover expertise.
Azure Services/Resources cost optimization and monitoring.
Azure DevOps and CI/CD proficiency.
Configure Azure resources, native services, security, and monitoring.
Design platform architecture with Azure Cloud Services and Databricks Delta Lake.
Create and publish guidelines for Data Integration services.
Participate in platform solution architecture, including data architecture.
Implement Databricks workspace, runtime, and Databricks Connect.
Configure Kafka integration services for real-time streaming and batch processing.
Enable Databricks Unity Catalog (UC) for lineage and governance.
Design and implement ML Ops for model deployments and inference pipelines.
Configure Azure DevOps, CI/CD for seamless deployment.
Implement monitoring and logging for Azure Service Groups and system metrics.
Ensure platform security, compliance, and data privacy.
ML Ops
Proficient in Python, R, or Java for ML algorithms.
Experience with TensorFlow, PyTorch, and other ML frameworks.
Efficient code for data processing, model training, and deployment.
Cloud platform experience, preferably Azure.
Familiarity with Azure ML Services and Azure DevOps.
Strong data analytics skills for model training.
Qualifications
Bachelor's degree in Computer Science, Information Technology, or related field.
7+ years of experience in cloud infrastructure management, Azure services, and infrastructure architecture
Strong understanding of Azure services such as Virtual Machines, Virtual Networks, Storage Accounts, and App Services
Experience in monitoring and optimizing Azure infrastructure for performance and efficiency
Proven experience and expertise in the respective role's technical skillsets.
Strong problem-solving skills and ability to work collaboratively.
Excellent communication skills for effective cross-team collaboration.
If you're passionate about leveraging data for informed decisions and thrive in a dynamic, collaborative environment, we invite you to join our team.
Show more
Show less","Data Platform Lead, Azure, Databricks, Data Analytics, Data Engineering, Big Data, Machine Learning, Cloud Computing, Python, Scala, SQL, Apache Spark, Hadoop, Apache Kafka, Azure Data Factory (ADF), Databricks Workspace, Databricks Runtime, Databricks Connect, Databricks Unity Catalog, Delta Live Tables, Delta Engine, Delta Lake, SQL, Azure Cloud Services, Azure Virtual Machines, Azure Storage Accounts, Azure Networks, Azure Security Management, Azure Active Directory, Azure DevOps, CI/CD, Azure Resources, Native Services, Security, Monitoring, Data Integration Services, Platform Solution Architecture, Data Architecture, Kafka Integration Services, Realtime Streaming, Batch Processing, ML Ops, Model Deployments, Inference Pipelines, TensorFlow, PyTorch, Data Processing, Model Training, Deployment, Azure ML Services, Data Analytics, Computer Science, Information Technology, Virtual Networks, App Services, ProblemSolving, Collaborative Work, Communication, CrossTeam Collaboration, Informed Decisions, Dynamic Environment","data platform lead, azure, databricks, data analytics, data engineering, big data, machine learning, cloud computing, python, scala, sql, apache spark, hadoop, apache kafka, azure data factory adf, databricks workspace, databricks runtime, databricks connect, databricks unity catalog, delta live tables, delta engine, delta lake, sql, azure cloud services, azure virtual machines, azure storage accounts, azure networks, azure security management, azure active directory, azure devops, cicd, azure resources, native services, security, monitoring, data integration services, platform solution architecture, data architecture, kafka integration services, realtime streaming, batch processing, ml ops, model deployments, inference pipelines, tensorflow, pytorch, data processing, model training, deployment, azure ml services, data analytics, computer science, information technology, virtual networks, app services, problemsolving, collaborative work, communication, crossteam collaboration, informed decisions, dynamic environment","apache kafka, apache spark, app services, azure, azure active directory, azure cloud services, azure data factory adf, azure devops, azure ml services, azure networks, azure resources, azure security management, azure storage accounts, azure virtual machines, batch processing, big data, cicd, cloud computing, collaborative work, communication, computer science, crossteam collaboration, data architecture, data engineering, data integration services, data platform lead, data processing, dataanalytics, databricks, databricks connect, databricks runtime, databricks unity catalog, databricks workspace, delta engine, delta lake, delta live tables, deployment, dynamic environment, hadoop, inference pipelines, information technology, informed decisions, kafka integration services, machine learning, ml ops, model deployments, model training, monitoring, native services, platform solution architecture, problemsolving, python, pytorch, realtime streaming, scala, security, sql, tensorflow, virtual networks"
Fleet Efficiency Data Analyst (CONTRACT),Airbus,"Miami, FL",https://www.linkedin.com/jobs/view/fleet-efficiency-data-analyst-contract-at-airbus-3782536041,2023-12-17,Coral Gables,United States,Mid senior,Hybrid,"Airbus is committed to providing reasonable accommodations as an Equal Opportunity Employer to applicants with disabilities. If you require assistance or an accommodation to complete your application, please contact us at USCareers@airbus.com
Notice: Know Your Rights: Workplace Discrimination is Illegal
Notice: Pay Transparency Nondiscrimination (English)
Aviso: Transparencia en el Pago No Discriminación (Spanish)
Job Description
Airbus is an international pioneer in the aerospace industry. We are a leader in designing, manufacturing and delivering aerospace products, services and solutions to customers on a global scale. We aim for a better-connected, safer and more prosperous world.
A commercial aircraft manufacturer, with Space and Defence as well as Helicopters Divisions, Airbus is the largest aeronautics and space company in Europe and a worldwide leader.
Airbus has built on its strong European heritage to become truly international - with roughly 180 locations and 12,000 direct suppliers globally. The company has aircraft and helicopter final assembly lines across Asia, Europe and the Americas, and has achieved a more than sixfold order book increase since 2000.
Airbus Commercial Aircraft
is looking for a Fleet Efficiency Data Analyst to join our customer support department based in
Miami, FL.
You will be part of a team maintaining fleet efficiency customer dashboards and digital control room, focused on our LATAM airline customers.
Meet The Team
The Customer Support team provides aircraft operations support throughout the aircraft's operational life. The team coordinates, implements and monitors all services customized to meet the customer’s specific needs, to ensure a positive customer relationship. Our Field Service teams provide technical support so our customers can operate safely and efficiently. Being based at our customers' facilities enables us to have a strong customer relationship and understanding of the airline's technical and operational needs.
Your Working Environment
The Airbus Training Center (ATC) in Miami provides flight and maintenance crew training for Airbus airline customers – primarily for those based in the Americas. The custom-designed, 110,000-square-foot complex trains more than 2,000 cadets each year in a range of (six) full-flight simulators, innovative computer-based training classrooms, and cabin door and slide trainers – all simulating the A320, A330, A340 and A350 XWB families of aircraft. In Miami you’ll also find the Airbus Latin America headquarters. A team of 60 employees there provides sales, marketing, contracts, customer support and communications for the region’s commercial aircraft, helicopters and space & defense divisions.
How We Care For You
Financial Rewards: Competitive base salary, incentive compensation which may include profit sharing schemes, retirement savings plan and the ability to participate in an Employee Stock Ownership Plan (“ESOP”)
Work/Life Balance: Paid time off including personal time, holidays and a generous paid parental leave program.
Health & Welfare: Comprehensive insurance coverage including medical (traditional and high-deductible health plans), prescription, dental, vision, life, disability, Employee Assistance Plan (“EAP”) and other supplemental benefit coverages.
Individual Development: Upskilling and development opportunities through our global Leadership University, including unlimited access to 10,000+ e-learning courses focusing on ways to develop your employability, certifications, career path as well as the opportunity to participate in accelerated development programmes and both national and international mobility.
At Airbus, we support you to work, connect and collaborate more easily and accessibly. Wherever possible, we foster flexible working arrangements to stimulate innovative thinking.
Your Challenges
Supporting all programs and customer services organizations
Promoting fleet performance tools and support to LATAM customers
Monitor fleet performance of the Americas customers in respect to safety, relicability, and economics
Assist operators in reducing the learning curve and acquire good handling methods by providing necessary transfer of knowledge support
Your Boarding Pass
Graduated in engineering, aeronautical studies or aircraft systems
Bachelor’s degree, preferred
2 years of experience in related fields
Knowledge in airline maintenance & engineering activities.
Knowledge in line maintenance operation on Airbus A/C (preferably on A320 or A330 or A340 family)
Knowledge of airline engineering
Knowledge of FAA/EASA Regulation (minimum JAR121, JAR145 and JAR66) is a plus.
Knowledge about airlines’ organization and operation
Good knowledge of customer needs in terms of maintenance & engineering operational activities
Experience in technical service/support field is a plus
Authorized to work in the United States.
Not a 100% match? No worries! Airbus supports your personal growth with customized development solutions.
Take your career to a new level and apply online now!
A full job description will be provided to candidates whom progress to interview stage or any candidate upon request.
As a leader in our field, Airbus in America provides relocation assistance for qualified positions and a comprehensive compensation and benefits package.
Airbus in America provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, genetics, pregnancy, marital status, veteran status or other legally protected status. In addition to federal law requirements, Airbus in America complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, demotion, termination, layoff, recall, transfer, leaves of absence, compensation, benefits and training.
Airbus in America expressly prohibits any form of workplace harassment based on race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, genetics, pregnancy, marital status, veteran status or other legally protected status. Improper interference with the ability of Airbus' employees to perform their job duties may result in discipline up to and including discharge.
As a matter of policy, Airbus does not sponsor visas for US positions unless specified. Only applicants with current work authorization will be considered.
Airbus does not offer tenured or guaranteed employment. Either the company or the employee can terminate the employment relationship at any time, with or without cause, with or without notice.
This job requires an awareness of any potential compliance risks and a commitment to act with integrity, as the foundation for the Company’s success, reputation and sustainable growth.
Company
Airbus Americas Customer Services, Inc.
Employment Type
Agency / Temporary
Experience Level
Entry Level
Remote Type
Flexible
Job Family
Customer Account and Service Management
Airbus provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, genetics, pregnancy, marital status, veteran status or other legally protected status. In addition to federal law requirements, Airbus complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, demotion, termination, layoff, recall, transfer, leaves of absence, compensation, benefits and training. Airbus expressly prohibits any form of workplace harassment based on race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, genetics, pregnancy, marital status, veteran status or other legally protected status. As a matter of policy, Airbus does not sponsor visas for US positions unless specified. Only applicants with current work authorization will be considered. Airbus does not offer tenured or guaranteed employment. Employment with Airbus is at will, meaning either the company or the employee can terminate the employment relationship at any time, with or without cause, with or without notice. Airbus reserves the right to revise or change job duties and responsibilities as the need arises. This position description does not constitute a written or implied contract of employment.
By submitting your CV or application you are consenting to Airbus using and storing information about you for monitoring purposes relating to your application or future employment. This information will only be used by Airbus.
Airbus is committed to achieving workforce diversity and creating an inclusive working environment. We welcome all applications irrespective of social and cultural background, age, gender, disability, sexual orientation or religious belief.
Airbus is, and always has been, committed to equal opportunities for all. As such, we will never ask for any type of monetary exchange in the frame of a recruitment process. Any impersonation of Airbus to do so should be reported to emsom@airbus.com.
Show more
Show less","Data Analysis, Customer Relationship Management, Aircraft Maintenance, Aircraft Engineering, Safety Analysis, Problem Solving, Teamwork, Communication","data analysis, customer relationship management, aircraft maintenance, aircraft engineering, safety analysis, problem solving, teamwork, communication","aircraft engineering, aircraft maintenance, communication, customer relationship management, dataanalytics, problem solving, safety analysis, teamwork"
"Senior Data Analyst, Revenue Strategy",Royal Caribbean Group,"Miami, FL",https://www.linkedin.com/jobs/view/senior-data-analyst-revenue-strategy-at-royal-caribbean-group-3781726677,2023-12-17,Coral Gables,United States,Mid senior,Hybrid,"Position Summary
As Senior Data Analyst, Revenue Strategy, this individual is responsible for supporting our revenue strategy and automation initiatives by measuring revenue initiative effectiveness and execution of revenue opportunities in excess of $100M annually. This includes pricing and inventory strategies through the use of automation techniques to impact over 15 million price points for Revenue Management. This person will work across numerous departments to build consensus across pricing strategies, inventory utilization strategies, automation, and other revenue initiatives. They will have exposure to and work closely with Revenue Management, Data Science & Analytics, IT and Commercial Strategy, from Senior Leadership to Analyst level.
Essential Duties And Responsibilities
Responsible for continuous analysis on revenue performance of ongoing RM Automation Projects.
Identify new revenue generating opportunities through internal research and analyses; build business cases that support long-term financial impact of growth initiatives.
Report on status updates and findings through written and verbal means to Senior Leadership.
Spearhead growth initiatives through change management with all necessary stakeholders including, but not limited to Revenue Management Product Team, Data Science & Analytics, IT and Commercial Strategy.
Drive continuous improvement and refinement of all initiatives after implementation.
Performance of these tasks requires an entrepreneurial mindset with very limited supervision.
Employees will be required to perform any other job-related duties assigned by their supervisor or management.
Qualifications And Education
To perform this position successfully, the individual should be a self-starter and be able to complete a project with little direction. Furthermore, the candidate must be able to perform each essential duty in a timely and accurate manner. The requirements listed below are representative of the knowledge, skills, and or ability required to satisfy this position:
Master of Business Administration (MBA) or master’s degree (MA) preferred but not required.
Bachelor’s degree (BA) from four-year college or university required with focus in quantitative discipline preferred.
One to Three years of related experience in financial / business analysis or related field preferred. Familiarization with revenue management concepts and tools preferred.
Must be able to demonstrate understanding of Revenue Management concepts and tools or basic economics.
Previous experience with Sales, Marketing, Analytics preferred but not required.
Previous experience in implementing system enhancements and decision-making tools preferred but not required.
Demonstrated ability to develop and deliver high quality, clear and concise presentations.
Demonstrated experience in project management.
Proficiency in Excel, Brio Intelligence/Hyperion Studio, Toad Oracle SQL, PowerBI, Word, and PowerPoint required and ability to manipulate data essential.
Knowledge of reservations system and revenue management data preferred but not required.
Attention to detail required.
Willingness to accept immediate product and project responsibility.
Ability to operate in a team environment.
Excellent interpersonal skills and demonstrated maturity. Demonstrated aptitude for problem solving and problem identification.
Ability to collect, analyze and interpret revenue performance data.
Ability to logically structure quantitative analyses of complex issues.
Strong written and verbal communication skills and ability to work in global environment.
Ability to manage multiple projects and responsibilities at once
Financial/QUantitative Responsabilities
Determines strategies and executes decision making to drive in excess of $100M by 2025 in annual incremental net ticket revenues.
Responsible for supporting initiatives to optimally manage 100% of price points on each sailing & category across the Royal Caribbean International brand. Understand implications of automated changes and alter revenue initiative strategies to execute initiatives in a revenue optimal manner.
Physical Requirements
The physical demands described here are representative of those requirements employees must meet to perform the essential functions of this job with or without reasonable accommodations. While performing job functions the employee is regularly required to sit, stand, write, review and type reports, compile data, operate a pc, communicate, listen, and assess information. The employee may move about the office complex, may travel to other office locations, and may lift, push, pull or move 10 - 15 pounds. Visual requirements include distant, close and color vision, and ability to adjust focus.
Working Conditions
The work environment characteristics described here are representative of those an employee encounters while performing the essential functions of the job. The environment includes work inside/outside the office.
Show more
Show less","Business Analysis, Revenue Management, Financial Analysis, Data Science, Analytics, Project Management, Excel, Brio Intelligence, Hyperion Studio, Toad Oracle SQL, PowerBI, Word, PowerPoint, Reservations Systems, Written Communication, Verbal Communication, Problem Solving, Problem Identification, Data Analysis, Quantitative Analysis","business analysis, revenue management, financial analysis, data science, analytics, project management, excel, brio intelligence, hyperion studio, toad oracle sql, powerbi, word, powerpoint, reservations systems, written communication, verbal communication, problem solving, problem identification, data analysis, quantitative analysis","analytics, brio intelligence, business analysis, data science, dataanalytics, excel, financial analysis, hyperion studio, powerbi, powerpoint, problem identification, problem solving, project management, quantitative analysis, reservations systems, revenue management, toad oracle sql, verbal communication, word, written communication"
"Sr Data Analyst, Strategy",Royal Caribbean Group,"Miami, FL",https://www.linkedin.com/jobs/view/sr-data-analyst-strategy-at-royal-caribbean-group-3776655298,2023-12-17,Coral Gables,United States,Mid senior,Hybrid,"Position Summary
As Sr Data Analyst, Strategy, this individual is responsible for supporting our revenue strategy and growth initiatives by measuring revenue initiative effectiveness and execution of revenue opportunities more than $150M annually. As a Senior Strategy Analyst, you will support brand wide revenue initiatives by accurately forecasting future growth opportunities based on recent and historic trends to appropriately set targets at the market and channel level. This position proactively identifies opportunities through best-in-class data extraction and translation to contribute towards profitable growth for the Royal Caribbean International brand. They will work closely with senior business analysts to provide necessary forecast modeling and track creation projects to adequately track revenue initiatives.
Essential Duties And Responsibilities
Responsible for the development of new models, as well as improvements to our current models, used to analyze and predict patterns and behavior related to demand, revenue, and market nuances.
Responsible for development, accuracy, and maintenance of tools, including automated Power BI reporting, for RM Ancillary and Incremental Revenue Projects.
Responsible for continuous analysis on revenue performance of ongoing RM Projects.
Identify new revenue generating opportunities through internal and external research and analyses; build business cases that support long-term financial impact of growth initiatives.
Ability to leverage both technical and advanced mathematical skills to procure, interpret, and convert data into insightful analysis.
Ability to translate and develop an overall story to deliver to stakeholders across various departments in the organization.
Project Manage timelines and meetings with necessary stakeholders ensuring projects remain on time and on budget.
Responsible for working cross-functionally to identify opportunities to drive and improve revenue, presenting findings to Management.
Prepare and deliver performance updates to Senior Leadership on a reocurring basis.
Spearhead growth initiatives through change management with all necessary stakeholders.
Performance of these tasks requires an entrepreneurial mindset with very limited supervision.
Employees will be required to perform any other job-related duties assigned by their supervisor or management.
Qualifications And Education
To perform this position successfully, the individual should be a self-starter and be able to complete a project with little direction. Furthermore, the candidate must be able to perform each essential duty in a timely and accurate manner. The requirements listed below are representative of the knowledge, skills, and or ability required to satisfy this position:
Master of Business Administration (MBA) or master’s degree (MA) preferred but not required.
Bachelor’s degree (BA) from four-year college or university required with focus in quantitative discipline preferred.
One to Three years of related experience in financial / business analysis or related field preferred. Familiarization with revenue management concepts and tools preferred.
Must be able to demonstrate understanding of Revenue Management concepts and tools or basic economics.
Previous experience with Sales, Marketing, Analytics preferred but not required.
Previous experience in implementing system enhancements and decision-making tools preferred but not required.
Demonstrated ability to develop and deliver high quality, clear and concise presentations.
Demonstrated experience in project management.
Proficiency in Excel, Brio Intelligence/Hyperion Studio, Toad Oracle SQL, PowerBI, Word, and PowerPoint required and ability to manipulate data essential.
Knowledge of reservations system and revenue management data preferred but not required.
Attention to detail required.
Willingness to accept immediate product and project responsibility.
Ability to operate in a team environment.
Excellent interpersonal skills and demonstrated maturity. Demonstrated aptitude for problem solving and problem identification.
Ability to collect, analyze and interpret revenue performance data.
Ability to logically structure quantitative analyses of complex issues.
Strong written and verbal communication skills and ability to work in global environment.
Ability to manage multiple projects and responsibilities at once
Physical Requirements
The physical demands described here are representative of those requirements employees must meet to perform the essential functions of this job with or without reasonable accommodations. While performing job functions the employee is regularly required to sit, stand, write, review and type reports, compile data, operate a pc, communicate, listen, and assess information. The employee may move about the office complex, may travel to other office locations, and may lift, push, pull or move 10 - 15 pounds. Visual requirements include distant, close and color vision, and ability to adjust focus.
Show more
Show less","Data Analysis, Revenue Management, Forecasting, Financial Analysis, Business Intelligence, Data Extraction, Power BI, SQL, Microsoft Office Suite, Project Management, Business Analytics, Economics, Sales, Marketing, Presentations, Team Environment, Problem Solving, Communication","data analysis, revenue management, forecasting, financial analysis, business intelligence, data extraction, power bi, sql, microsoft office suite, project management, business analytics, economics, sales, marketing, presentations, team environment, problem solving, communication","business analytics, business intelligence, communication, data extraction, dataanalytics, economics, financial analysis, forecasting, marketing, microsoft office suite, powerbi, presentations, problem solving, project management, revenue management, sales, sql, team environment"
Freelancing Expert - Data Analyst,"Fud, Inc.","Miami, FL",https://www.linkedin.com/jobs/view/freelancing-expert-data-analyst-at-fud-inc-3777034562,2023-12-17,Coral Gables,United States,Mid senior,Hybrid,"Fud, the ""World's First Social Hustling Community"" is seeking a
Freelancing Expert - Data Analyst
to join our team. As a Freelancing Expert, you will be instrumental in providing comprehensive know-how and mentorship to our community of freelancers and independent workers.
Join the Fud community to monetize your free time and let it become your next favorite side hustle.
You can experience our community for yourself by
downloading our free to use app
.
You will join a community focused on side hustles, including:
Content Creator — YouTube, TikTok, or Instagram
Reselling and Thrifting — including eBay, Etsy, Mercari, Poshmark, and others
E-Commerce — including Amazon, Shopify, and your own online store
Gig Economy — like DoorDash, Uber, or Lyft
Freelancing and Consulting — social media marketing, graphic design, and more
Online Business — including blogging, podcasting, and online courses
Local Services — including cleaning, knife sharpening, notary services, pet waste removal, and more
Self-Publishing — including Kindle Direct Publishing and Canva
Responsibilities
Provide one-on-one and group mentoring to side hustlers
Help side hustlers develop their side hustle ideas
Set goals and track progress with side hustlers
Provide support and motivation to side hustlers
Share knowledge and resources with side hustlers
Stay up-to-date on the latest trends in side hustles
Build relationships with side hustlers and other mentors
Requirements
You are aware of the trends in future of work, personal finance, and entrepreneurship
Ability to work independently and manage your time effectively
High level of initiative and positive approach
Strong communication skills
Comfortable with using social media and other digital tools
A willingness to share know-how through video, livestreaming, and text content
Passion for innovation and entrepreneurship
Benefits
Be your own boss
Learn how to make money from your passion and interests
Find a community of like minded hard working solopreneurs and entrepreneurs
Choose your own schedule and work when you want
We are a community that values diversity. We encourage everyone including women and people of color to join us!
Show more
Show less","Side hustles, Monetization, Entrepreneurship, Community management, Content creation, Graphic design, Social media marketing, Freelancing, Consulting, Personal finance, Business development, Online business, ECommerce, Podcasting, Blogging, Digital tools","side hustles, monetization, entrepreneurship, community management, content creation, graphic design, social media marketing, freelancing, consulting, personal finance, business development, online business, ecommerce, podcasting, blogging, digital tools","blogging, business development, community management, consulting, content creation, digital tools, ecommerce, entrepreneurship, freelancing, graphic design, monetization, online business, personal finance, podcasting, side hustles, social media marketing"
Principal Analytics Data Engineer,ADP,"Miami, FL",https://www.linkedin.com/jobs/view/principal-analytics-data-engineer-at-adp-3778820488,2023-12-17,Coral Gables,United States,Mid senior,Hybrid,"ADP
is hiring a
Principal Analytics Data Engineer
for our newly formed HR Outsourcing (HRO) Data and Analytics organization. The
Principal Analytics Data Engineer
will play a key role in growing our newest chapter of analytics engineering professionals while interacting with cross-functional teams to address complex business requirements. We’re seeking a value seeking, self-motivated, and analytical profession who act as a player and coach to multiple workstreams at the same time. The role demands the individual to possess technical skills required to perform the job in an effective manner. The right candidate will be a technical expert, should have the passion for data & analytics and works along with the team they manage.
What are we looking for?
An analytics and data engineering professional with a passion and track record for designing analytics and delivery methods to increase accuracy of reporting and advanced analytics in an agile environment to unlock transformational growth. Someone with intellectual curiosity who wakes up excited to work with a team towards excellence and partner with leaders to drive business outcomes and deliver analytical solutions. The ideal candidate is business-minded, customer-centric, team-oriented, self-motivated, a strategic thinker and results-driven.
Like what you see?
Apply now!
Learn more about ADP at tech.adp.com/careers
Learn more about Client Services at ADP
:
https://adp.careers/Client_Services_Videos
A little about ADP:
We are a global leader in HR technology, offering the latest AI and machine learning-enhanced payroll, tax, HR, benefits, and much more. We believe our people make all the difference in cultivating an inclusive, down-to-earth culture that welcomes ideas, encourages innovation, and values belonging. ADP has a deep commitment to diversity, equity, and inclusion as a global Best Places to Work, DiversityInc® Top 50 Company, Best CEO and company for women, LGBTQ+, multicultural talent, and more. Learn more about ADP's commitment on our YouTube channel:
http://adp.careers/DEI_Videos
WHAT YOU'LL DO:
Behaviors:
Lead workstreams that thrive.
As an experienced leader, you want everyone to shine. You are constantly looking for ways to share your knowledge, motivate others, and keep everyone engaged and productive.
Customer focused.
You marry data strategy to business strategy. A trusted partner to key stakeholders across business and analytics functions who understands how data can be activated to deliver value. Your teams are responsible for democratizing data for data science and advanced applications.
Clear Communicator
. Ability to tell data stories to senior leaders and communicate working plans to the team. Excellent communication skills, both verbal and written; able to communicate effectively with multiple leadership layers and across a broad base of team members. Effective at increasingly complex stakeholder alignment, where priorities often compete, drives transparency on priorities and expected return
Create Results.
You’re proactive and hands-on when need. When you see a potential issue, you never leave things hanging and unfinished. When you and your team deliver a finished product, it’s as polished as you could make it.
Variety of work.
There is no typical day. You could be checking in with the CDO one minute, meeting with leadership to review initiatives for the coming quarter later and tomorrow helping your team overcome blockers.
Influence and inspire confidence.
You are comfortable presenting to senior leaders, analytics and program management leads, and peers with a compelling voice that you demonstrate through executive presence, leading change, and creating clear executive-level communications on milestone achievements.
Continuous improvement.
Document current analytics and automation processes and recommend and implement best practices to improve and optimize.
Challenges.
Inevitable challenges will arise, and we’ll rely on you to look for a solution.
Responsibilities:
Catalog current analytics and automation processes and recommend best practice methods for improvement and optimization
Partner with Analytics, Data Product, and Business stakeholders to create roadmaps and execute project plans in fast-paced agile development environment
Create
use-case specific data tables
by joining and engineering critical elements across multiple data domains. Ingests HRO-specific data, ensuring it is
well-structured
, adheres to
data quality
rules, and
traceable
from consumption layer back to raw layer.
Partner with the Advanced Analytics and Strategic Operations to streamline data integration, maintain security and access best practices, and create end-to-end data analytics solutions
Mentor more junior team members that will work in matrix pod to support advanced analytics initiative
Research and recommend technologies and processes to support rapid scale and future state growth initiatives
Help your team prioritize Business Needs, Leadership Questions, and Ad Hoc Requests for on-time delivery
Manage workstream leadership reporting, and continuous improvement initiatives
Drive QA and Data Quality efforts to improve development timelines, reduce bugs, and maintain reliable analytics products
Support projects that improve our performance (e.g. Retention & Future of Work): aligning with product strategy and understanding stakeholder requirements
Experienced getting to a v0.5 solution, getting real world feedback, and then iterating to v1.0+ with a continuous improvement mindset
Successful track record of superior service delivery and change management in an enterprise organization
To succeed in this role:
Bachelor’s degree in computer science, engineering, business, statistics, or related fields; advanced degree preferred
At least 8-10 years of experience with analytics, engineering and leading implementation, development, improvement and support projects
3+ years in team lead role
Work experience with ETL, Data Modelling and Data Architecture
Preferably working in PEO, Service, or Risk analytics, data engineering, data analytics and visualization, business intelligence, or analytical consulting
Proven ability to guide team to use data, analytics, and business knowledge to solve complex business problems
Experience with designing and maintaining data warehouses and/or data lakes with big data technologies like Hadoop, Spark, or distributed databases like Redshift and Snowflake, and experience with housing, accessing, and transforming data in a variety of relational databases
Experience in building data pipelines and deploying / maintaining them with tools like Git and Jenkins
Knowledge with MLOps infrastructure (e.g., Databricks, MLflow) and containerization and managing production pipelines and microservices (e.g., Docker, Kubernetes)
Experience and skill with data mining methods, data modeling, and working with data warehouses
Pyspark and SQL expertise
Understanding of agile methodologies and what makes teams successful
A college degree is great but not required. What's more important is having the skills to do the job. If you don't have a college degree, other acceptable experiences could include:
Experience noted above, OR Military experience where skills including teamwork, adaptability, organization, and follow-through will help you build team and client relationships, identify solutions, and achieve success.
Preferred Qualifications
Advanced degree (Masters) in Software engineering, Data engineering, Computer Science, or other data management and data visualization disciplines. MBA.
Hands-on experience on Bigdata Technologies like Apache Spark , Hive, Hadoop
Experience with developing frameworks and utility services including logging/monitoring
Strong technical knowledge of DataBricks
Knowledge of UNIX/Python programming language.
Experience delivering high quality software following continuous delivery and using code quality tools (JIRA,GitHub, Jenkin, Sonar, etc.).
Experience creating large-scale, multi-tiered, distributed applications with Hadoop and ETL tool like Abinitio
Comfortable in Windows and Linux environments.
Comfortable with different data storage solutions such as RDMBS(Oracle), Hive, HBase, Impala etc.
Knowledge on NOSQL Databases like MongoDB, Hbase, Cassandra etc is a plus.
Jira and Confluence preferred
Nice to have data solutions architect experience
Knowledge with dashboarding tools (e.g., Dash, Shiny, Tableau)
Experience with cloud database technologies (e.g., AWS) and developing solutions on cloud computing services and infrastructure in the data and analytics space
Comfortable using PySpark APIs to perform advanced data transformations
YOU'LL LOVE WORKING HERE BECAUSE YOU CAN:
Have courageous team collaboration.
Courage comes from how associates are willing to have difficult conversations, speak up, be an owner, and challenge one another's ideas to net out the best solution.
Deliver at epic scale.
We deliver real user outcomes using strong judgment and good instincts. We're obsessed with the art of achieving simplicity with a focus on client happiness and productivity.
Be surrounded by curious learners.
We align ourselves with other smart people in an environment where we grow and elevate one another to the next level. We encourage our associates to listen, stay agile, and learn from mistakes.
Act like an owner & doer.
Mission-driven and committed to leading change, you will be encouraged to take on any challenge and solve complex problems. No tasks are beneath or too great for us. We are hands-on and willing to master our craft.
Give back to others.
Always do the right thing for our clients and our community and humbly give back to the community where we live and work. Support our associates in times of need through ADP's Philanthropic Foundation.
Join a company committed to equality and equity.
Our goal is to impact lasting change through our actions.
Show more
Show less","Analytics, Data Engineering, Agile, Data Warehousing, Data Lakes, Hadoop, Spark, Redshift, Snowflake, Git, Jenkins, MLOps, Docker, Kubernetes, Pyspark, SQL, Unix, Python, JIRA, GitHub, Sonar, Abinitio, Oracle, Hive, HBase, Impala, MongoDB, Cassandra, Dash, Shiny, Tableau, AWS, PySpark","analytics, data engineering, agile, data warehousing, data lakes, hadoop, spark, redshift, snowflake, git, jenkins, mlops, docker, kubernetes, pyspark, sql, unix, python, jira, github, sonar, abinitio, oracle, hive, hbase, impala, mongodb, cassandra, dash, shiny, tableau, aws, pyspark","abinitio, agile, analytics, aws, cassandra, dash, data engineering, data lakes, datawarehouse, docker, git, github, hadoop, hbase, hive, impala, jenkins, jira, kubernetes, mlops, mongodb, oracle, python, redshift, shiny, snowflake, sonar, spark, sql, tableau, unix"
Lead Data Engineer - USA,Pelico,"Miami, FL",https://www.linkedin.com/jobs/view/lead-data-engineer-usa-at-pelico-3772865231,2023-12-17,Coral Gables,United States,Mid senior,Hybrid,"After becoming a leader in the European market with logos like Cartier, Safran and Arcelor Mittal, Pelico is now expanding its operations on the US market. This is an opportunity to join the company at a critical moment of its growth.
As a Lead Data Engineer, you will play a pivotal role in overseeing and developing our team dedicated to data integration of our clients for US territory.
You will be responsible for leading but also contributing in the structure of data integration activities of our clients, by defining data pipelines architectures, automatisation process and make our Software & Data Engineering team grow.
As a Lead Data Engineer, you will play a pivotal role in overseeing and developing our team dedicated to data integration of our clients for US territory.
You will be responsible for leading but also contributing in the structure of data integration activities of our clients, by defining data pipelines architectures, automatisation process and make our Software & Data Engineering team grow.
What You’ll Do & Learn 📖
Plan, coordinate, and manage software and data integration projects, ensuring adherence to timelines and budgets agreed with the clients
Work closely with other departments to understand business needs and translate them into technical solutions.
Oversee the development, maintenance, and updating of practice, ensuring compliance with quality and security standards.
Establish rigorous testing processes to ensure application stability.
Supervise data collection, storage, quality, and security.
Collaborate with the Customer Operations team to extract valuable insights from data.
Efficiency through Standardization: Standardize processes across the organization. Reduces the risk of errors, improves consistency, and streamlines operations.
Optimization: Regularly assess processes, gather feedback from team members, and implement improvements to respond to changing needs.
Data-Driven Decision-Making: Utilize data and analytics to provide objective guidance to obtain the most significant impact
Alignment with customers to embrace a perfect data integration: Tailor your interactions and services to each customer's preferences and needs using data and insights to personalized experiences.
Embrace multiple channels to communicate with customers effectively.
Implement systems and processes to proactively identify and resolve issues before they impact the data integration experience.
Stay updated with relevant technological trends and advancements for the team.
Design, develop, maintain and evolve the data stack to answer both business and product needs (especially back end features within the platform)
Build data pipelines and configure the platform (incl. algorithms) to on-board new customers
What You Embody 🎯
At least 7 years of Data engineering experience, ideally in a SAAS environment
You are mastering on Python Javascript, C++, Kotlin, You-name-it… but note that we hire based on engineering fundamentals rather than familiarity with specific technologies
Experience in building data-rich products or complex data pipelines (professional or personal projects) or working with data (data transformation & event driven pipeline)
SQL expert
Bilingual English mandatory (verbal & written), other language is a plus
Proactive, Autonomous, results oriented and you excel in stimulating environments
Willingness to create impact for customers and see the product in the hand of happy users
Team player and comfortable working with others
Comfortable in client facing if needed
Miami based or willingness to relocate
Technical Stack 💻
Python
Airflow
PostgreSQL
Gitlab CI/CD
Linus
Docker
Pelico promotes inclusion and non-discrimination, and acts daily in favour of social mix, gender equality, senior citizens & disability
What We Offer💡
Join an exciting adventure with a lot of challenges at all levels!
Work on a highly impactful product that users love!
Office location in Miami - Florida
Stock Options
20 annual paid leaves + 10 bank holidays
Remote flexibility
Healthcare Insurance with dental & vision
401 K
Team events every quarter
Show more
Show less","Data Engineering, Leadership, Data Integration, Data Pipelines, Automation, Software Engineering, Python, Javascript, C++, Kotlin, SQL, Airflow, PostgreSQL, Gitlab CI/CD, Linus, Docker","data engineering, leadership, data integration, data pipelines, automation, software engineering, python, javascript, c, kotlin, sql, airflow, postgresql, gitlab cicd, linus, docker","airflow, automation, c, data engineering, data integration, datapipeline, docker, gitlab cicd, javascript, kotlin, leadership, linus, postgresql, python, software engineering, sql"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Miami, FL",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3759712018,2023-12-17,Coral Gables,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto,
San
Francisco or Chicago
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Java, Bash, SQL, Git, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, AWS, GCP, Azure, DynamoDB, Kafka, Storm, SparkStreaming","python, java, bash, sql, git, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, dynamodb, kafka, storm, sparkstreaming","airflow, aws, azure, bash, docker, dynamodb, gcp, git, helm, java, kafka, kubernetes, python, snowflake, spark, sparkstreaming, sql, storm"
Sr. Lead Data Engineer,Royal Caribbean Group,"Miami, FL",https://www.linkedin.com/jobs/view/sr-lead-data-engineer-at-royal-caribbean-group-3774191635,2023-12-17,Coral Gables,United States,Mid senior,Hybrid,"Position Summary
The Senior Lead Data Solutions Engineer is responsible for leading the delivery, managing and operationalizing data solutions pipelines in support of data and analytics use cases using appropriate methodologies, technologies including extract, transform and load of data. As Lead, Sr. Data Solutions Engineer, you will have responsibility for leading Data solution design for enterprise data and analytics. In addition, you will provide technical and thought leadership, emerging technology evaluation, and mentorship. The Senior Lead Data Solutions Engineer is also responsible for recommending The Senior Lead Data Solutions Engineer develops and improves standards and procedures to support quality development, testing, and production support. The role is expected to drive/lead the delivery of one or more data solution components from project inception through its delivery in close collaboration with cross-team members.
Essential Duties And Responsibilities
Lead data engineering development projects which include the design of complex flexible, durable, reusable and scalable data pipelines and frameworks to automate the ingestion, processing and delivery of both structured and unstructured batch and real-time streaming data employing a variety of data integration and data preparation tools for on premises and cloud for different business scenarios.
Lead collaborative team working with other areas to ensure that all solutions are complete, operable, conform to business processes, and meet the business needs with the agreed quality of service
Evaluates ETL/ELT best practices within the industry to learn principles and applications and drive development adoption and trains others
Evaluates the effectiveness of data integration tools and assist in platform discussions and decisions
Evaluate, design, build refine data and analytics capabilities
Lead the technical roadmap for the enterprise analytics capabilities engineering
Coach and mentor a team of data solution engineers
Communicate the technical dependencies to the concerned teams and establish coordination for seamless implementation of the individual components
Design robust data solutions in support of enterprise data solutions engineering
Help craft and execute enterprise data engineering strategy
Recommends improvements to data architecture processes to ensure high quality of data architecture deliverables and consistency.
Solves complex issues in target data architecture design processes (e.g. data management process modeling, data entity matrix design).
Ensure the end-to-end solution is fit for purpose, meet the needs of business, the agreed requirements, and are both pragmatic and supportive of the strategic architecture direction
Monitors data architecture design processes (e.g. interface to data), in accordance with existing processes and standards.
Develops standards, processes and methodologies to develop each phase of data architecture (e.g. data manipulating processes, database technology
Financial Responsibilities
Significant understanding of effort estimation activities. Will lead large/complex estimation activities.
Drives expectations in meeting deadline within budget, schedule and appropriate quality.
Qualifications
Bachelor or Master of Science in Engineering, Computer Science, Information Technology or equivalent
10+ years of data warehousing, big data, data engineering, and other related specialization
Expertise in cloud providers (Azure, AWS, or GCP) preferably with Azure
Expertise in cloud data technologies such as Snowflake, Synapse, but preferably in Databricks Lakehouse
Expertise in Data Integration tools
Expertise in large scale data solution architecture
Experience working with large scale projects
Experience working with business and technical stakeholders
Must be able to perform complex tasks and handle multiple priorities, and can perform exceptionally under high stress conditions.
Must be capable of fully articulating concisely technical concepts to non-technical audiences.
Knowledge And Skills
Extensive ability to design, build and manage data pipelines for data structures encompassing data transformation, data models, schemas, metadata and workload management.
Extensive experience with popular database programming languages including SQL, PL/SQL, T-SQL, others for relational databases
Extensive experience in one of the following tools: Informatica, ADF or Talend
Extensive experience with database technologies: Oracle, MSSQL, Hadoop and NoSQL
Extensive experience with various Data Management architectures like Data Warehouse, Data Lake and the supporting processes like Data Integration, Governance, Metadata Management
Extensive experience in working with large, heterogeneous datasets in building and optimizing data pipelines, pipeline architectures and integrated datasets using traditional data integration technologies. These should include [ETL/ELT, data replication/CDC, message-oriented data movement] and data ingestion and integration technologies such as stream data integration, and data virtualization.
Extensive experience in working with and optimizing existing ETL processes and data integration and data preparation flows and helping to move them in production
Extensive experience writing and optimizing advanced SQL queries in a business environment with large-scale, complex datasets
Extensive experience of data warehousing and data lake best practices within the industry
Extensive experience of various development and deployment environments on-premises, in the cloud and across multi-cloud environments such as: AWS, Azure, Google
Extensive experience and hands-on experience with scripting languages: Python, Scala, Java, etc …
Physical Demands
The physical demands described here are representative of those requirements employees must meet to perform the essential functions of this job with or without reasonable accommodations. While performing job functions the employee is regularly required to sit, stand, write, review and type reports, compile data, operate a pc, communicate, listen, and assess information. The employee may move about the office complex, may travel to other office locations and may lift, push, pull or move 10 - 15 pounds. Visual requirements include distant, close and color vision, and ability to adjust focus.
Work Environment
The work environment characteristics described here are representative of those an employee encounters while performing the essential functions of the job. The environment includes work inside/outside the office.
Show more
Show less","Data Engineering, Data Warehousing, Data Integration, Data Lake, ETL/ELT, Cloud Data Technologies, SQL, PL/SQL, TSQL, Informatica, ADF, Talend, Oracle, MSSQL, Hadoop, NoSQL, Python, Scala, Java","data engineering, data warehousing, data integration, data lake, etlelt, cloud data technologies, sql, plsql, tsql, informatica, adf, talend, oracle, mssql, hadoop, nosql, python, scala, java","adf, cloud data technologies, data engineering, data integration, data lake, datawarehouse, etlelt, hadoop, informatica, java, mssql, nosql, oracle, plsql, python, scala, sql, talend, tsql"
Senior Cloud Data Engineer,BDO USA,"Miami, FL",https://www.linkedin.com/jobs/view/senior-cloud-data-engineer-at-bdo-usa-3765468594,2023-12-17,Coral Gables,United States,Mid senior,Hybrid,"Job Description
Job Summary:
This position will work with cutting edge technology, deliver high quality solutions across various industries, and oversee team(s) on engagements that range in size and scope. This position will receive continuous career development opportunities, given the size and potential of client engagements. This role will perform hands-on delivery of data analytics projects, contributing to the development and unit testing of solutions.
Job Duties
Designs and implements best in class data ingestion strategies, data warehouse and data mart structures, semantic layers and models, visualizations, streaming processes, API integrations, and automation (RPA) solutions for end-to-end data analytics solutions on primarily, but not limited to, cloud analytics platforms such as Azure and AWS
Listens to client needs to align solution with business requirements and delivery schedule
Creates written functional and technical designs
Participates in project status and stand meetings, and assists with providing aggregated project status for project and program managers
Assists with SLA compliance of solutions, and performs performance tuning and optimization efforts of end-to-end solutions
Writes code using multiple languages and correctly applies frameworks, architectural patterns, and software development principles
Delivers high-performance, scalable, repeatable, and secure deliverables with broad impact (high throughput and low latency)
Assists with implementation of data governance programs and best practices
Performs the cleaning and transforming of data from source systems into analytics models
Implements models to support data visualizations and integrations
Assists with implementing DevOps, DataOps and MLOps methodologies on projects
Writes custom integration logic in applicable programming languages
Assists project managers with work breakdown structure creation, project estimation, resource staffing, workload planning and adjustments throughout the project lifecycle
Assists clients with licensing, security, and cost estimation of solutions
Performs code reviews to ensure adherence to standards
Works directly with clients and team members to establish secure data analytics platforms and infrastructure
Contributes to successful deployments of developed solutions and integration of DevOps tools
Maintains a broad and current understanding of data analytics and business intelligence strategies, cloud platforms, methodologies, and tools
Builds client relationships during project execution, effectively becoming a trusted advisor of the client
Participates in support activities for existing software solutions
Other duties as assigned
Supervisory Responsibilities
Supervises the day-to-day workload of Associates on assigned engagements to ensure that timelines and deliverables are met, and reviews work product
Education
Qualifications, Knowledge, Skills and Abilities:
High School Diploma or GED equivalent, required
Bachelor’s degree, preferred; focus in Information Systems, Data Science or Computer Science, preferred
Experience
Five (5) or more years of experience within Data Analytics, Business Intelligence, Artificial Intelligence, or Application Development, required
One (1) or more years of experience technically leading development projects, preferred
One (1) or more years of consulting experience or implementation of cloud-based data analytics solutions, preferred
Software
Strong SQL skills including Data Definition Language (DDL), Data Manipulation Language (DML), views, functions, stored procedures, or performance tuning, required
Experience with Data Warehousing, Data Modeling, Semantic Model Definition or Star Schema Construction, required
Hands on delivery experience of end-to-end cloud data analytics solutions within Azure or AWS, preferred
Experience with one (1) or more of the following computer languages, preferred:
C#
Python
Java
Scala
Experience with tabular modeling within Microsoft Fabric, Power BI, or Azure Analysis Services, preferred
Experience with Git and DevOps deployment technologies, preferred
Experience with Linux, preferred
Experience with one (1) or more of the following, preferred:
Data Lake Medallion Architecture
Batch and/or streaming data ingestion into a data lake
AI Algorithms/Machine Learning
Automation tools such as UiPath, Alteryx, etc.
Computer Vision based AI technologies
Other Knowledge, Skills & Abilities
Ability to work with a high degree of professionalism and autonomy
Excellent verbal and written communication skills
Solid organizational skills, especially the ability to meet project deadlines with a focus on details
Ability to successfully multi-task while working independently or within a group environment
Ability to work in a deadline-driven environment, and handle multiple projects simultaneously
Ability to interact effectively with people at all organizational levels of the Firm
Ability to effectively interact with a team of professionals and delegating work assignments, as needed
Ability to build and maintain strong relationships with internal and client personnel
Ability to encourage a team environment on engagements, and contribute to the professional development of assigned personnel
Keywords:
Data Analytics, Business Intelligence, BI, Synapse, IoT, Machine Learning, Data Lake, Stream, Cube, Microsoft, SQL Server, Tableau, .Net, C#, Qlik, Power BI, Machine Learning, Azure Data Factory, RedShift, UiPath, Cloud, RPA, AWS, Redshift, Kinesis, QuickSight, SageMaker, S3, Databricks, AWS Lake Formation, Snowflake, Python, Qlik, Athena, Data Pipeline, Glue, Star Schema, Data Modeling, SQL, SSIS, SSAS, SSRS, PySpark, Microsoft Fabric, dbt, Linux, Terraform, Bicep, Data Ops, Purview, Git, Delta, Pandas, Spark SQL
Individual salaries that are offered to a candidate are determined after consideration of numerous factors including but not limited to the candidate’s qualifications, experience, skills, and geography.
California Range: $111,000 - $152,000
Colorado Range: $111,000 - $152,000
New York City/ Valhalla Range: $111,000 - $152,000
Washington Range: $111,000 - $152,000
About Us
BDO delivers assurance, tax, digital technology solutions and financial advisory services to clients throughout the country and around the globe. We offer numerous industry-specific practices, world-class resources, and an unparalleled commitment to meeting our clients’ needs. We currently serve more than 400 publicly traded domestic and international clients.
Unparalleled partner-involvement
Deep industry knowledge and participation
Geographic coverage across the U.S.
Cohesive global network
Focused capabilities across disciplines
BDO brings world-class resources and exceptional service to each and every one of our clients. BDO USA is a member of BDO International, the world’s fifth largest accounting network.
BDO offers a competitive Total Rewards package that encompass so much more than – “traditional benefits”. Our wide range of rewards and our employees’ ability to customize rewards to their individual needs are two of the reasons why BDO has been honored with so many workplace awards, including 100 Best Companies for Working Parents, Working Mother 100 Best Companies, Top Entry Level Employer, 2022 National Best & Brightest Companies to Work For and more.
Some Examples Of Our Total Rewards Offerings Include
Competitive pay and eligibility for an annual performance bonus.
A 401k plan plus an employer match
Comprehensive, medical, dental, vision, FSA, and prescription insurance from day one
Competitive Paid Time Off with daily accrual from day one of employment, plus paid holidays
Paid Parental Leave
Adoption Assistance
Firm paid life insurance
Wellness programs
Additional offerings include BDO Flex, Group Legal insurance, Pet insurance and Long-Term Care Insurance
Above offerings may be subject to eligibility requirements.
Click here to find out more!
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability or protected veteran status.
""BDO USA, P.A. is an EO employer M/F/Veteran/Disability""
Show more
Show less","Data Analytics, Business Intelligence, Data Warehousing, Star Schema Construction, SQL, C#, Python, Java, Scala, Azure, AWS, Power BI, Data Lake, DevOps, Git, Linux, Machine Learning, AI Algorithms, Automation, UiPath, Alteryx, Azure Data Factory, RedShift, Kinesis, QuickSight, SageMaker, S3, Databricks, Snowflake, PySpark, Microsoft Fabric, dbt, Terraform, Bicep, Data Ops, Purview, Delta, Pandas, Spark SQL, SSIS, SSAS, SSRS","data analytics, business intelligence, data warehousing, star schema construction, sql, c, python, java, scala, azure, aws, power bi, data lake, devops, git, linux, machine learning, ai algorithms, automation, uipath, alteryx, azure data factory, redshift, kinesis, quicksight, sagemaker, s3, databricks, snowflake, pyspark, microsoft fabric, dbt, terraform, bicep, data ops, purview, delta, pandas, spark sql, ssis, ssas, ssrs","ai algorithms, alteryx, automation, aws, azure, azure data factory, bicep, business intelligence, c, data lake, data ops, dataanalytics, databricks, datawarehouse, dbt, delta, devops, git, java, kinesis, linux, machine learning, microsoft fabric, pandas, powerbi, purview, python, quicksight, redshift, s3, sagemaker, scala, snowflake, spark, spark sql, sql, ssas, ssis, ssrs, star schema construction, terraform, uipath"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Miami, FL",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3773091260,2023-12-17,Coral Gables,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto
or
San
Francisco
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Machine Learning, Data Analytics, Data Mining, Data Cleaning, Data Normalization, Data Modeling, Statistical Analysis, Data Visualization, Pandas, R, Data Platforms, Data Pipelines, Distributed Systems, Data Extraction, Data Ingestion, Data Processing, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, SQL, Git, Python, Java, Bash, Snowflake, Kafka, Storm, SparkStreaming, ETL, Conversational AI APIs, Recommender Systems, Microservices, Kubernetes, Docker Images, StreamProcessing Systems","data engineering, machine learning, data analytics, data mining, data cleaning, data normalization, data modeling, statistical analysis, data visualization, pandas, r, data platforms, data pipelines, distributed systems, data extraction, data ingestion, data processing, airflow, kubernetes, docker, helm, spark, pyspark, sql, git, python, java, bash, snowflake, kafka, storm, sparkstreaming, etl, conversational ai apis, recommender systems, microservices, kubernetes, docker images, streamprocessing systems","airflow, bash, conversational ai apis, data cleaning, data engineering, data extraction, data ingestion, data mining, data normalization, data platforms, data processing, dataanalytics, datamodeling, datapipeline, distributed systems, docker, docker images, etl, git, helm, java, kafka, kubernetes, machine learning, microservices, pandas, python, r, recommender systems, snowflake, spark, sparkstreaming, sql, statistical analysis, storm, streamprocessing systems, visualization"
"Senior Associate, Data Governance - Ontology and Data Modeling",Jobs for Humanity,Buffalo-Niagara Falls Area,https://www.linkedin.com/jobs/view/senior-associate-data-governance-ontology-and-data-modeling-at-jobs-for-humanity-3782884873,2023-12-17,East Aurora,United States,Mid senior,Hybrid,"Job Description
Job Title: Senior Associate, Data Governance - Ontology and Data Modeling Location: New York, NY Job Description: We are looking for a Senior Associate in Data Governance who will focus on Ontology and Data Modeling. The primary responsibility of this role is to develop, implement, and maintain enterprise ontologies to support Capital One's Data Strategy. As part of Commercial Banking, you will work collaboratively with various teams across the organization to develop domain ontologies for enterprise initiatives. You will also partner with Technology, Machine Learning, and other teams to integrate semantic technology into our products and services. It is important for the Senior Associate to effectively communicate and advocate the value of using semantic technology and knowledge organization concepts. Primary Responsibilities: - Develop, implement, and govern ontologies and optimized data models to enhance data discoverability, search, and product development - Contribute to data integration and mapping efforts to align data with Capital One's ontologies - Communicate and promote the value of ontologies, semantics, and standardization across the business - Execute policies and standards established by data governing bodies to meet regulatory and audit requirements - Collaborate with Technology and other teams to integrate semantic technology into Capital One's products and services - Analyze and implement knowledge organization strategies using machine learning, ontology management, and semantic enrichment tools - Stay updated on competitor and industry developments related to ontology use, knowledge organization, data modeling, and machine learning - Adhere to W3C standards related to ontologies, such as RDF, RDFS, OWL, SKOS, and SHACL - Develop standards, guidelines, and direction for ontology, data modeling, semantics, and Data Standardization at Capital One - Test and troubleshoot new or updated models and systems - Manage and oversee other assigned projects Role-Based Competencies: - Intellectual Curiosity: You are eager to learn, ask questions, and explore new ideas and challenges. - Communicator & Influencer: You can effectively communicate complex ideas to different audiences and rally your colleagues to work together. - Do-er: You take action, experiment, and have the ability to overcome obstacles while staying focused on goals. - Passionate & Customer Focus: You care about the growth of others, enjoy building and shipping products, and understand the needs of our customers. - Learner: You are open to learning new things and technologies while being willing to ask for help when needed. - Team Player: You work well with diverse teams, prioritize team goals, and build lasting relationships. Basic Qualifications: - Bachelor's degree in information science, computer science, engineering, library science, ontology, semantics, or computational linguistics; or - Master's degree in information science, computer science, engineering, library science, ontology, semantics, or computational linguistics - At least 2 years' experience in a metadata field or relevant training - At least 2 years' experience or training in using W3C standards (JSON, XML, RDF, RDFS, OWL, SKOS) - At least 2 years' experience or training in ontology and linked data tools (Protégé, TopQuadrant, PoolParty, Stardog, AnzoGraph, Neptune, or Data.World) - At least 2 years' experience or training with SQL or SPARQL Preferred Qualifications: - Understanding of ontologies development and the use of controlled vocabularies and thesauri to enhance data management - Familiarity with graph databases and technologies - Familiarity with Python or R - Familiarity with JSON, OpenAPI/YAML, AVRO - Familiarity with Agile principles, processes, and methodologies - Familiarity with Amazon Web Services - Strong attention to detail and problem-solving skills - Strong reading and writing skills - Strong project management experience - Excellent communication skills Salary Range: - New York City (Hybrid On-Site): $112,400 - $128,300 for Senior Associate, Ontology and Data Modeling Benefits: Capital One offers a comprehensive and competitive set of health, financial, and other benefits that support your total well-being. Eligibility varies based on employment status and level. To apply for this position or for more information, please visit the Capital One Careers website. Note: Capital One is committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex, race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited by law. Capital One promotes a drug-free workplace and considers applicants with a criminal history in accordance with applicable laws and regulations. If you require an accommodation during the application process, please contact Capital One Recruiting for assistance. Your information will be kept confidential and used only for providing necessary accommodations. For technical support or questions about the recruiting process, please contact Careers@capitalone.com. Please note that positions posted in Canada, the United Kingdom, and the Philippines are for specific entities within Capital One. Thank you for your interest in joining Capital One!
Show more
Show less","Data Governance, Ontology, Data Modeling, Machine Learning, Semantic Technology, Data Integration, Data Mapping, RDF, RDFS, OWL, SKOS, SHACL, SQL, SPARQL, Python, R, JSON, OpenAPI/YAML, AVRO, Agile, Amazon Web Services, Protégé, TopQuadrant, PoolParty, Stardog, AnzoGraph, Neptune, Data.World, Graph Databases","data governance, ontology, data modeling, machine learning, semantic technology, data integration, data mapping, rdf, rdfs, owl, skos, shacl, sql, sparql, python, r, json, openapiyaml, avro, agile, amazon web services, protg, topquadrant, poolparty, stardog, anzograph, neptune, dataworld, graph databases","agile, amazon web services, anzograph, avro, data governance, data integration, data mapping, datamodeling, dataworld, graph databases, json, machine learning, neptune, ontology, openapiyaml, owl, poolparty, protg, python, r, rdf, rdfs, semantic technology, shacl, skos, sparql, sql, stardog, topquadrant"
Data Engineer,"U.S. Venture, Inc.","Green Bay, WI",https://www.linkedin.com/jobs/view/data-engineer-at-u-s-venture-inc-3779390728,2023-12-17,Stillwater,United States,Associate,Onsite,"POSITION SUMMARY
Our People Analytics team is expanding! The Data Engineer I - People Analytics, is responsible for designing, developing, implementing, and supporting data warehouse and cloud storage solutions that support company analytics. This role will support People Analytics and contribute to our growth in the strategic use of data to guide business decisions.
This position is based in Appleton, WI.
JOB RESPONSIBILITIES
Consult with business counterparts to understand new data requirements. Design data models to support the requirements.
Data profiling and source system analysis to present insights to peers and business partners to support the end use of data
Collaborate with senior engineers and architects to ensure data models fit within the company data and systems architecture.
Develop, test, and implement Extraction, Transform and Load (ETL) processes to acquire and load data from internal and external sources to the data lake or data warehouse to be used for analytical purposes.
Design, build and test data products based on feeds from multiple systems using a range of different storage technologies and/or access methods
Monitor and support ETL jobs. Research poor performing ETL jobs and collaborate with database administers and other resources to improve the overall efficiency and stability of the data warehouse environment.
Support and partner with business analytics users by identifying relevant data and delivering views, cubes, models, and other semantic objects to ensure ease of access to data for non-technical individuals.
Deliver data solutions in accordance with agreed organizational standards that ensure services are resilient, scalable and future-proof
Provide technical and project documentation, utilizing agile project management methodologies.
QUALIFICATIONS
Bachelor’s degree or equivalent work experience in Computer Science, Management Information Systems (MIS), Information Technology (IT), or related field
2+ years of experience with ETL technologies. Experience in Microsoft Azure Databricks, Data Factory, ADLS, Python, and Microsoft SQL Server Integration Services (SSIS).
2+ years of experience in Structured Query Language (SQL) programming
2+ years of experience in utilizing Python or PySpark or Scala for data engineering and transformation
Familiarity with cloud services (AWS, Azure (preferred), Google Cloud)
Familiarity with business intelligence tools such as Power BI, Tableau, MicroStrategy, Business Objects, DAX, and Power Query preferred
Strong analytical skills, detail oriented, and organized
Strong communication skills and self-motivated
Our company is an equal opportunity employer that is committed to inclusion and diversity. We take affirmative action to ensure equal opportunity for all applicants without regard to race, color, religion, sex, sexual orientation, gender, gender identity or expression, marital status, age, national origin, disability, veteran status, genetic information, or other protected characteristic. If you need assistance or an accommodation due to a disability, you may call Human Resources at (920) 739-6101.
Show more
Show less","Python, Microsoft Azure Databricks, Data Factory, ADLS, Microsoft SQL Server Integration Services (SSIS), Structured Query Language (SQL), PySpark, Scala, AWS, Azure, Google Cloud, Power BI, Tableau, MicroStrategy, Business Objects, DAX, Power Query, ETL, Data profiling, Data warehousing, Data modeling, Data engineering, Data transformation, Business intelligence, Cloud services, Business analytics","python, microsoft azure databricks, data factory, adls, microsoft sql server integration services ssis, structured query language sql, pyspark, scala, aws, azure, google cloud, power bi, tableau, microstrategy, business objects, dax, power query, etl, data profiling, data warehousing, data modeling, data engineering, data transformation, business intelligence, cloud services, business analytics","adls, aws, azure, business analytics, business intelligence, business objects, cloud services, data engineering, data factory, data profiling, data transformation, datamodeling, datawarehouse, dax, etl, google cloud, microsoft azure databricks, microsoft sql server integration services ssis, microstrategy, power query, powerbi, python, scala, spark, structured query language sql, tableau"
Senior Data Analyst,NSW Health,"Kingswood, New South Wales, Australia",https://au.linkedin.com/jobs/view/senior-data-analyst-at-nsw-health-3779081159,2023-12-17,Tamworth, Australia,Mid senior,Onsite,"Employment Type
: Permanent Full Time
Position Classification
: Health Manager Level 3
Location:
Station Street Office, Penrith
Remuneration
: $122,850.00 - $139,559.00 per annum
Hours Per Week
: 38
Requisition ID
: REQ452470
Applications Close:
14/01/2024
With
CORE
Values of
C
ollaboration,
O
penness,
R
espect and
E
mpowerment,
working with us will ensure your professional life is provided every opportunity to succeed and develop in your chosen career role.
What We Can Offer You (for Eligible Employees)
Accrued Day Off (ADO) (for full time employees)
Opportunity for extra tax savings through Salary Packaging
Novated Leasing
Great education opportunities through Education Training Service which offers over 110 courses each year
Access to our Employee Assistance Program (EAP) for staff and family members
Fitness Passport
What You'll Be Doing
The Senior Data Analyst in Healthcare is responsible for providing data analysis and management support to enhance decision-making in healthcare. The role involves collecting, interpreting, and analysing complex data to identify trends, patterns, and insights that will inform strategic and operational planning within the district. This role will also be responsible for DBA admin tasks on HIE (until mid-2024), DBA admin tasks on EDWARD and BI report development.
This role will work collaboratively with stakeholders across the LHD and the broader NSW Health landscape to generate valuable insights, improve the quality of data analysis and ensure availability of accurate, consistent and useful reports and dashboards.
An eligibility list may be created for future vacancies.
About Us
Nepean Blue Mountains Local Health District (NBMLHD) is a wonderful place to expand your career and grow your skills and knowledge. As a recognised leader in the healthcare industry, we provide a range of public health services to the Nepean and Blue Mountains and Lithgow Region.
NBMLHD:
is committed to achieving a diverse workforce. We strongly encourage and welcome applications from all ages and genders, Aboriginal and Torres Strait Islander people, people from culturally and linguistically diverse backgrounds, refugees, people of diverse genders and sexuality, and people with disability.
All NSW Health workers are required to have completed a primary course of a COVID-19 vaccine which has been approved or recognised by the Therapeutics Goods Administration (TGA). Additionally, Category A workers are required to receive a booster dose three months after completing the primary course of COVID-19 vaccinations. New applicants must have completed the vaccination course prior to commencement with NSW Health, or provide an approved medical contraindication certificate (IM011 immunisation medical exemption form) certifying the worker cannot have any approved COVID-19 vaccines available in NSW.
Acceptable proof of vaccination is the Australian Immunisation Register (AIR) Immunisation History Statement or AIR COVID-19 Digital Certificate. Booster doses are highly recommended for all health care workers who have completed the primary course of COVID-19 vaccinations.
Employment of a temporary visa holder may only occur if no suitable permanent resident or citizen of Australia has been identified and must not exceed the duration or conditions associated with the current visa.
Selection Criteria To Be Addressed
Tertiary qualifications in data analytics, data science, ICT, health analytics or equivalent and/or relevant experience.
Demonstrated experience developing and publishing PowerBI dashboards end-to-end – from gathering requirements, designing the front end, setting up the data model, testing and iterating on dashboards to publishing, training, and maintaining dashboards on an ongoing basis.
Demonstrated experience in critically analysing, interpreting and reporting on data in the healthcare industry or related. Examples may include analysis impact on patient outcomes, KPI reporting, performance reporting, etc.
Proficient in PowerBI, SSIS, SQL Server, C# (or any other programming language).
Demonstrated high level experience or at a minimal understanding of DBA tasks and UNIX.
Proven skills in effectively communicating and collaborating with both technical and non-technical stakeholders across a variety of specialisations and domains.
Understanding of the health system and the relevant processes, systems, and regulatory requirements.
Experience in R or Python not essential but well regarded.
Need more information?
Click here for the Position Description
Find out more about applying for this position
For role related queries or questions contact Shilpa Shetty on shilpa.shetty@health.nsw.gov.au
Show more
Show less","Data analytics, Data science, ICT, Health analytics, PowerBI, SQL Server, C#, DBA, UNIX, R, Python","data analytics, data science, ict, health analytics, powerbi, sql server, c, dba, unix, r, python","c, data science, dataanalytics, dba, health analytics, ict, powerbi, python, r, sql server, unix"
Senior Data Analyst,Study Group,"Brighton, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-analyst-at-study-group-3765764208,2023-12-17,Crawley, United Kingdom,Associate,Remote,"Contract type:
Full Time (37.5 hours per week) - Permanent
Location:
Brighton Head Office (Remote working)
Join our team as an Analytics Support Specialist, dedicated to providing crucial support to key business areas. Your role will involve delivering insightful management information to a diverse range of internal and external stakeholders. A pivotal aspect of this position is the ability to collaborate effectively with stakeholders at all organizational levels, translating their requirements into both technical and non-technical deliverables. As an integral team member, you will play a key role in advancing Study Group's transition to a more data-driven culture, all while supporting data governance initiatives.
ABOUT THE ROLE
Undertake simple and complex statistical analysis of a variety of different datasets whilst producing (and monitoring) performance measures, modelling scenarios and outcomes as necessary, for a variety of stakeholders across the organisation. This will include internal and external data and the use of quantitative and qualitative analytics techniques. Data quality checks and screening are often necessary as is the ability to provide quality assurance regarding: imported data.
Derive insights and explain/interpret trends identified, and their implications which can include presenting findings to stakeholders. This will involve preparing and leading on reports and producing suitable metrics which are appropriately benchmarked. An integral component of the role is ensuring that data is fully understood and utilised appropriately, providing further information or context as required, and training end users on new reports and dashboards.
Manage and design the reporting environment, working to identify areas to increase efficiency and automation of processes, whilst establishing and maintaining automated data processes. This will involve working with the Data Architecture Team around the data lake by identifying and revising reporting requirements. Will need to develop and maintain MI/data reports and dashboards. Reports often must be generated from multiple systems and some data integration work will be required. Also, there is a responsibility to maintain documentation relating to reports and dashboards.
Create appropriate data visualisations, utilising PowerBI, Tableau and other appropriate tools like R, all in service of providing an accessible way to see and understand trends, outliers, and patterns in data.
Manage complex data-related projects and the production of data products from their inception to conclusion, including those involving complex data modelling and forecasting. This will include supporting initiatives for data integrity and normalisation.
Support ad hoc requests for data.
Process confidential data and information according to guidelines, with a complete understanding of GDPR and the Data Protection Act and the implications for the work – advising stakeholders as appropriate.
Work collaboratively with other colleagues to share knowledge and understanding of data analysis tools and techniques to ensure that data is effectively used. This will also involve informing discussions on appropriate systems, databases, platforms, software and data tools – especially in relation to data and analytics requirements for new systems (or upgrades to existing systems).
Play a central role in encouraging better data literacy within the Study Group and help foster a data culture across the organisation. This includes advocating for better use and management of data at Study Group.
ABOUT YOU
Undergraduate degree from a good university with a 2:1 or above, or equivalent experience.
Ideally a master’s level and/or PhD qualification.
Extensive experience working in a Management Information/Analyst role using Cognos Business Analytics or PowerBI, Tableau, advanced Excel, and SQL
Proven experience of working effectively with stakeholders in specialist and technical roles, including IT infrastructure and other data and analytics specialists as well as senior executives across the organisation.
Proven experience using sound judgement and initiative to manage competing priorities and requirements, whilst ensuring the delivery of key objectives.
Ideally, experience using SQL servers, Microsoft Azure Data Lake, or Oracle databases and data warehouses.
Highly desirable if there is experience with Alteryx.
Highly desirable if there is experience with R and/or Python programming language.
ABOUT US
Study Group is a leading international education provider that is dedicated and passionate about helping students around the world, creating a brighter future and reaching their full potential. With university partnerships and a variety of face-to-face, online, and hybrid study programmes, we can provide students with the resources and guidance they need to succeed. Our global network of offices and university partnerships ensure that students receive the best educational services available. Additionally, our Insendi platform enables us to deliver innovative, accessible, and intuitive digital learning experiences with the most advanced digital tools available.
ORGANISATIONAL COMPLIANCE
Study Group is proud of its high standards in safeguarding and recognises its importance in enabling the best possible student experience and outcomes. For successful applicants the following checks will be conducted- Criminal Background Check (country specific), x2 References, ID and Right to Work checks as well as Education Verification.
We are proud to be an equal opportunity employer and we strive to create a diverse and inclusive workplace. We are committed to providing equal opportunities for all individuals, regardless of race, gender, religion, nationality, or any other factor. We believe that the best person for the job should be selected based on skills and experience alone. Your information will be kept confidential and stored securely. Please see our Privacy Policy.
We adopt a rolling recruitment process meaning that we review applications as they are submitted. To avoid disappointment, please ensure that you apply ASAP.
Show more
Show less","SQL, Python, R, Data Analysis, Data Integration, Data Modeling, Data Visualization, Data Governance, Data Management, Business Analytics, Tableau, PowerBI, Alteryx, Reporting, Data Quality, GDPR, Data Protection Act, Data Lake, Data Warehouse, Databases, Analytics, Dashboards, Automation, Statistical Analysis","sql, python, r, data analysis, data integration, data modeling, data visualization, data governance, data management, business analytics, tableau, powerbi, alteryx, reporting, data quality, gdpr, data protection act, data lake, data warehouse, databases, analytics, dashboards, automation, statistical analysis","alteryx, analytics, automation, business analytics, dashboard, data governance, data integration, data lake, data management, data protection act, data quality, dataanalytics, databases, datamodeling, datawarehouse, gdpr, powerbi, python, r, reporting, sql, statistical analysis, tableau, visualization"
Data Analyst,Women in Data®,"Lewisham, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-analyst-at-women-in-data%C2%AE-3776605180,2023-12-17,Crawley, United Kingdom,Mid senior,Hybrid,"Apply Here:
www.womenindata.co.uk/job/data-analyst-2-metpolice-bromley-lewisham/
Closing date: 11:55pm, 20 December 2023
Salary: The starting salary is £37,289, which includes allowances totalling £2,841. The salary is broken down as £34,448 basic salary, which will increase annually until you reach the top of the scale of £36,864. Plus, a location allowance of £1,841 and a non-pensionable allowance of £1,000.
Recruitment code: Oleeo 15856
Location: Bromley, Lewisham
A full-time role is required within the CONNECT Data and Configuration team (CDAC) to support the development of data quality reporting, CDAC performance monitoring and the analysis of CONNECT data from a system management perspective.
The functions of the role are as follows:
work with the CDAC team to define relevant data streams to measure the quality of data within CONNECT, across business areas and object types
work with the current Met staff to understand legacy data quality issues and develop reporting within CONNECT
extrapolate trends and expected data quality benchmarks from the legacy data
work with the data extraction and management workstream to develop mechanism for obtaining relevant CONNECT data
support CONNECT Data Quality team by providing regular reports to prioritise and target core data quality issues
develop both strategic and tactical reporting against data quality issue and support the regular creation and delivery of these reports
develop processes for identify data quality problems within the CONNECT data
assess the effectiveness of the CDAC teams, providing oversight of demand and activity through metrics from both CONNECT and locally captured data
present key information at appropriate meetings
provide analysis of data to help identify drivers of data quality and system issues, supported by robust evidence
perform any additional data analysis or MI support for the CDAC team
VETTING
This post requires access to the most sensitive intelligence material on a daily basis. Applicants must hold or be prepared to undergo National Security Vetting (NSV) Security Check (enhanced) (SC(e)) level before taking up the post; with a willingness to undergo Developed Vetting (DV) once in post, if required.
Additionally, this material is of particular sensitivity to the UK and its distribution must be restricted to UK nationals. In approved circumstances dual nationals (of which one element is British) may also be granted access. However, in the event that potential conflicts of interest cannot be managed the post holder will not be able to see the intelligence material and will not be able to perform their duties.
For the purpose of safeguarding national security and in line with Cabinet Office Policy, supported by Section 82(2) to (4) of the Police Reform Act (2002), applicants who do not hold or acquire Security Check (enhanced) (SC(e)) OR Developed Vetting (DV) clearance and meet the nationality requirements cannot be offered the post.
We are proud supporters of Women in Data®. Connect, engage and belong to the largest free female data community in the UK – visit: www.womenindata.co.uk to join our community.
Show more
Show less","Data Analysis, Data Quality, Data Extraction, Data Management, Data Reporting, Performance Monitoring, Data Visualization, Trend Analysis, Data Quality Assessment, Data Quality Management, KPI Reporting, Data Extraction, Data Integration, Data Security, National Security Vetting (NSV), Security Check (SC), Developed Vetting (DV)","data analysis, data quality, data extraction, data management, data reporting, performance monitoring, data visualization, trend analysis, data quality assessment, data quality management, kpi reporting, data extraction, data integration, data security, national security vetting nsv, security check sc, developed vetting dv","data extraction, data integration, data management, data quality, data quality assessment, data quality management, data reporting, data security, dataanalytics, developed vetting dv, kpi reporting, national security vetting nsv, performance monitoring, security check sc, trend analysis, visualization"
Reference Data Analyst,ADLIB Recruitment | B Corp™,"Guildford, England, United Kingdom",https://uk.linkedin.com/jobs/view/reference-data-analyst-at-adlib-recruitment-b-corp%E2%84%A2-3783572452,2023-12-17,Crawley, United Kingdom,Mid senior,Hybrid,"Global Insurance Leader
Excellent opportunity to join a large scale global transformation programme.
Gain great experience working with great tech and people.
Build strong relationships in a very visible role for the business.
This role is looking for someone who is able to ensure the quality, accuracy and consistency of the data. You’ll be working across critical projects where you’ll be reviewing and managing data integrity. Your role is key to their transformation journey where they’re making sure the customer, the data and the digital aspects are at the forefront of what they do and this shows in their solutions.
What you’ll be doing
Within the Master & Reference Data Management (M&RDM) and part of the Agile delivery team you’ll be focused on developing the reference data capability. You’ll be working alongside your team to identify reference data across multiple business functions. You’ll be working cross data use cases within the transformation project to create function specifications and you’ll be conducting detailed analysis of data changes.
You’ll look at policies and standards in order to contribute to the reference data management process changes and enhancement. In this role you’ll asl be tasked with showing users the core concepts of reference data to ensure the plan is being followed.
You’ll work alongside the data governance team to establish ownership frameworks for reference data and help with data isse dbe, title vi, americans with disabilities act ada","americans with disabilities act ada, data visualization tools, database implementation, datamodeling, disadvantaged business enterprise dbe, microsoft access, microsoft excel, powerbi, programming languages, python, sql, tableau, title vi",,
Research Data Analyst II,CalRecycle,"Sacramento, CA",https://www.linkedin.com/jobs/view/research-data-analyst-ii-at-calrecycle-3762625036,2023-12-17,Davis,United States,Mid senior,Onsite,"Equal Opportunity Employer
The State of California is an equal opportunity employer to all, regardless of age, ancestry, color, disability (mental and physical), exercising the right to family care and medical leave, gender, gender expression, gender identity, genetic information, marital status, medical condition, military or veteran status, national origin, political affiliation, race, religious creed, sex (includes pregnancy, childbirth, breastfeeding and related medical conditions), and sexual orientation.
It is an objective of the State of California to achieve a drug-free work place. Any applicant for state employment will be expected to behave in accordance with this objective because the use of illegal drugs is inconsistent with the law of the State, the rules governing Civil Service, and the special trust placed in public servants.
Position Details
Job Code #:
JC-398598
Position #(s):
835-894-5731-001
Working Title:
Research Data Analyst II
Classification:
RESEARCH DATA ANALYST II
$5,793.00 - $7,472.00
# of Positions:
1
Work Location:
Sacramento County
Job Type:
Permanent, Full Time
Department Information
California’s Department of Resources Recycling and Recovery (CalRecycle) brings together the state’s recycling and waste management programs to move the state towards a circular economy that reduces waste and reuses all materials.
Through landmark initiatives like the Integrated Waste Management Act and Beverage Container Recycling and Litter Reduction Act, California works toward a society that uses less, recycles more, and takes resource conservation to higher and higher levels.
CalRecycle’s Mission: Protecting California’s environment and climate for the health and prosperity of future generations through the reduction, reuse and recycling of California resources, environmental education, disaster recovery and the transition from a disposable to a fully circular economy.
CalRecycle values diversity, equity, and inclusion throughout the organization. We foster an environment where employees from a variety of backgrounds, cultures, and personal experiences are welcomed and can thrive. We believe the diversity of our employees is essential to inspiring innovative solutions. Together we further our mission to protect the environment and preserve resources by empowering Californians to reduce, reuse, and recycle. Join CalRecycle to improve the lives of all Californians.
Department Website: http://www.calrecycle.ca.gov
Department Website: http://www.calrecycle.ca.gov
Job Description And Duties
Do you have a desire to work to clean up waste and help protect the health and well-being of California's people and environment? Do you have knowledge of research and statistical methods and a passion for overseeing the research and evaluation of a wide variety of information? Do you enjoy data analysis and advising all levels of government and business establishments on complex, technical, and sensitive issues concerning financial responsibility?
CalRecycle is looking for a strong communicator with tact and professionalism who is self-driven, creative, and customer focused to conduct qualitative and quantitative data research and analysis related to financial assurance mechanisms for closure, postclosure maintenance, corrective action and operating liability for solid waste landfills, and closure and operating liability for major waste tire facilities to ensure financial assurance regulatory requirements, programs and policies are met. The classification for this position is Research Data Analyst II.
This position will leverage your skills to interact and communicate effectively and frequently on sensitive policy issues with stakeholders. Perform complex or sensitive work, accurately analyze data, interpret and apply laws and regulations, assist with training, and consult with management on financial assurance related issues.
This position requires that you possess and maintain a valid California Driver’s License (CDL). You must answer the questions addressing your CDL on your applications. Ensure you provide your CDL number, class, expiration date, and any endorsements and/or restrictions. Failing to maintain a valid CDL will result in a failure to meet job requirements.
This position may be eligible for hybrid telework under Government Code 14200 for eligible applicants residing in California, subject to the candidate meeting telework eligibility criteria set forth in the CalEPA telework policy and/or future program need. Employees not residing in California are not eligible for telework. Regardless of telework eligibility, all employees may be required to report to the position’s designated headquarters location, as indicated on their duty statement, at their own expense.
You will find additional information about the job in the
Duty Statement
.
Special Requirements
When submitting application in hard copy, applicants are required to submit a standard state application (Form Std.678) with an original signature, and clearly indicate RPA 23-051 & Position # 835-894-5731-001 in the “Job Title(s)” section of
the Standard State Application or you will not be considered for the position.
If you would like to use education in conjunction with experience to satisfy the minimum qualifications, you must attach a copy of your degree and/or transcripts (unofficial or official) to your application to ensure the educational requirements have been met for this position. Failure to do so will result in your application being rejected.
Application Instructions
Dates printed on Mobile Bar Codes, such as the Quick Response (QR) Codes available at the USPS, are not considered Postmark dates for the purpose of determining timely filing of an application.
Final Filing Date: Until Filled
Who May Apply
Individuals who are currently in the classification, eligible for lateral transfer, eligible for reinstatement, have list eligibility, are in the process of obtaining list eligibility, or have SROA and/or Surplus eligibility (please attach your letter, if available). SROA and Surplus candidates are given priority; therefore, individuals with other eligibility may be considered in the event no SROA or Surplus candidates apply.
Applications will be screened and only the most qualified applicants will be selected to move forward in the selection process. Applicants must meet the Minimum Qualifications stated in the Classification Specification(s).
How To Apply
Complete Application Packages (including your Examination/Employment Application (STD 678) and applicable or required documents) must be submitted to apply for this Job Posting. Application Packages may be submitted electronically through your CalCareer Account at www.CalCareers.ca.gov. When submitting your application in hard copy, a completed copy of the Application Package listing must be included. If you choose to not apply electronically, a hard copy application package may be submitted through an alternative method listed below:
Address for Mailing Application Packages
You may submit your application and any applicable or required documents to:
Department of Resources Recycling & Recovery
Attn: Human Resources RPA 23-051
P.O. Box 4025, MS 19A
Sacramento , CA 95812-4025
Address for Drop-Off Application Packages
You may drop off your application and any applicable or required documents at:
Department of Resources Recycling & Recovery
Human Resources RPA 23-051
P.O. Box 4025, MS 19A
Sacramento , CA 95812-4025
08:00 AM - 05:00 PM
Required Application Package Documents
The following items are required to be submitted with your application. Applicants who do not submit the required items timely may not be considered for this job:
Current version of the State Examination/Employment Application STD Form 678 (when not applying electronically), or the Electronic State Employment Application through your Applicant Account at www.CalCareers.ca.gov. All Experience and Education relating to the Minimum Qualifications listed on the Classification Specification should be included to demonstrate how you meet the Minimum Qualifications for the position.
Resume is required and must be included.
Degree and/or School Transcripts
Other - Cover Letter
Statement of Qualifications - Applicants must complete a Statement of Qualifications (SOQ) for this recruitment. The SOQ is a narrative discussion prepared by the applicant describing how the applicant's skills, knowledge, abilities, education, training, and experience qualify them for the position. Applications received without this information may not be considered for this position. SOQ should have a title of “Statement of Qualifications, Your Name,” be no more than 2 pages, and no smaller than 12-point font.
Please note Resumes, letters, Supplemental Questionnaire, transcripts, degrees, your state application, cover letters and other materials will not take the place of the Statement of Qualifications. Simply copying and pasting your resume will also not be accepted. Responses must be numbered and submitted in the same order. Applications received without an SOQ may be rejected.
Applicants requiring reasonable accommodations for the hiring interview process must request the necessary accommodations if scheduled for a hiring interview. The request should be made at the time of contact to schedule the interview. Questions regarding reasonable accommodations may be directed to the EEO contact listed on this job posting.
Show more
Show less","Data Analysis, Financial Assurance, Solid Waste Management, Statistical Methods, Laws and Regulations, Communication Skills, CDL, Telework, Microsoft Office Suite, State Examination/Employment Application, EEO","data analysis, financial assurance, solid waste management, statistical methods, laws and regulations, communication skills, cdl, telework, microsoft office suite, state examinationemployment application, eeo","cdl, communication skills, dataanalytics, eeo, financial assurance, laws and regulations, microsoft office suite, solid waste management, state examinationemployment application, statistical methods, telework"
Data Scientist,Net2Source Inc.,"West Sacramento, CA",https://www.linkedin.com/jobs/view/data-scientist-at-net2source-inc-3787107999,2023-12-17,Davis,United States,Mid senior,Onsite,"Please find below mentioned job details:
Job Title- Data Scientist
Job Location- West Sacramento CA 95691
Job Duration- 09 months Contract
Description:
The Data Scientist will be responsible for data analytics processes and results for the Platform R&D team. This position will be a contributor on the team responsible for:
• Identifying customer populations and key data elements in support of data enrichment and analysis input
• Define and build data repositories to support long-term analytics needs and centralize commonly referenced data
• Identifying data sources and data gaps
• Perform analytics required to support various projects or information requests
• Identify requirement for and work with teams to build reports to support ongoing information needs relative to client's processes and controls
• Defining and executing queries or logic to retrieve data from various sources within and possibly outside of client.
• Analyzing data and creating summaries in MS Word or PowerPoint for engineering lead review and communication
Required Qualifications
• Bachelors degree in a quantitative field such as applied math, statistics, engineering, physics, accounting, finance, economics, econometrics, computer sciences, or business AND 2+ years of experience in one or a combination of the following: reporting, analytics, or modeling.
Desired Qualifications
• SQL and R programming experience
• Statistical analysis experience (regression, correlation, variability analysis)
• Ability to manage multiple and competing priorities
• Ability to work independently
• Excellent verbal, written, and interpersonal communication skills
• Extensive knowledge and understanding of research and analysis
• Strong analytical skills with high attention to detail and accuracy
Other Desired Qualifications
• 1+ years’ experience in performing in a data science role within analytics teams
• Experience implementing and defining process to support data analytics and result communication
• Experience building and retaining documentation to track analysis results and logic
• Experience working with multiple database solutions and retrieving and consolidating data from disparate information sources
• Experience as data scientist in the medical laboratory instrumentation line of business.
About Net2Source, Inc.
Net2Source is a total talent management solutions company with its presence in 50+ countries. Our creative solution service offerings aim at becoming your one stop destination for hiring talent needs globally.
Want to read more about Net2Source? Visit us at www.net2source.com
Show more
Show less","Data Analytics, Data Enrichment, Data Analysis, Data Repositories, Data Sources, Data Gaps, Analytics, Reporting, SQL, R Programming, Statistical Analysis, Regression, Correlation, Variability Analysis, Data Science, Process Implementation, Documentation, Database Solutions, Medical Laboratory Instrumentation","data analytics, data enrichment, data analysis, data repositories, data sources, data gaps, analytics, reporting, sql, r programming, statistical analysis, regression, correlation, variability analysis, data science, process implementation, documentation, database solutions, medical laboratory instrumentation","analytics, correlation, data enrichment, data gaps, data repositories, data science, data sources, dataanalytics, database solutions, documentation, medical laboratory instrumentation, process implementation, r programming, regression, reporting, sql, statistical analysis, variability analysis"
Research Data Analyst 3 - 62727,"University of California, Davis","Sacramento, CA",https://www.linkedin.com/jobs/view/research-data-analyst-3-62727-at-university-of-california-davis-3785188858,2023-12-17,Davis,United States,Mid senior,Onsite,"Department Description
The UC Davis Violence Prevention Research Program (VPRP), a unit of the School of Medicine and an internationally-recognized leader in research on violence, in coordination with the University of California Firearm Violence Research Center (UCFC), is seeking an outstanding research analyst with a master’s degree or commensurate experience in public health, public policy, criminology, economics, political science, community development, sociology, or a related field to serve as a Research Data Analyst for a multi-disciplinary research program focused on policy evaluation, crime and violence prevention, and interventions on the social determinants of violence.
VPRP was founded more than 30 years ago. Our continuing mission is to develop and disseminate the scientific evidence on which informed violence prevention policy and practice are based. VPRP is a multidisciplinary, collaborative program with multiple independent investigators. At any given time, VPRP is conducting and planning a large array of projects relating to violence prevention and health equity. Our status as a reliable source for rigorous research and our location in Sacramento increase our ability to translate findings into practice and public policy. VPRP investigators now also form the core of the newly-established UCFC, a major multi year initiative for firearm violence research authorized and funded by the State of California and located at UC Davis.
The appointee will work under the supervision of Drs Kagawa and Laqueur, core members of the VPRP and UCFC faculty. Activities will include study and analysis plan design; data collection, management, maintenance, and linkage; data analysis and coding; interpretation of results; literature reviews; writing of manuscripts, oral and poster presentations, progress reports, IRB protocols, and new project proposals; and liaison with collaborators at other institutions and relevant nonprofit organizations or governmental agencies. The analyst will support other shared duties related to project coordination and administration as assigned.
VPRP has a strong commitment to building a staff that is rich with cultural, social, and experiential diversity. Candidates are encouraged to identify their strengths and experiences in this area.
Apply By Date
(1/11/2024 at 11:59pm); Interviews and recruiting process may occur at any time
Minimum Qualifications
Experience with scientific manuscript writing and data analysis in an academic or research setting.
Experience with performing and summarizing scientific literature reviews. Experience with statistical data analysis and execution
Master’s or doctoral degree or commensurate experience in public health, public policy, criminology, economics or related field.
Experience programming, managing and analyzing data using R or Stata statistical software.
Understanding of principles of study design and causal inference
Preferred Qualifications
Experience obtaining, managing, and/or analyzing large datasets for multidisciplinary research projects.
Experience with GIS methods to examine spatiotemporal patterns of events under study.
Experience leading or facilitating research on health and racial equity
Ability to independently learn new statistical methods and become proficient in new software packages to apply in epidemiologic studies.
Ability to learn new statistical methods and become proficient in new software packages to apply in epidemiologic studies.
Ability to work independently and collaboratively, making significant creative contributions, with a multi-disciplinary group of researchers and staff.
Knowledge and application of project management principles and concepts.
Ability to work on several projects concurrently involving several sources of oversight.
Ability to communicate clearly and effectively in English, both verbally and in writing, and prepare well-organized text, tables, and figures, conveying complex information intelligibly for both professional and lay audiences.
Possession of excellent organizational and interpersonal skills appropriate to project coordination responsibilities.
Knowledge of and ability to adhere to University and departmental policies and procedures, and/or Sponsor requirements for maintaining confidentiality of all private information (clinical, criminal, and personal).
Position Information
Salary or Pay Range: $3,076.63-$5,651.34
Salary Frequency: Biweekly
Salary Grade: 23
Payroll Title: RSCH DATA ANL 3
Number of Positions: 1
Appointment Type: Contract
Percentage of Time: 100%
Shift Hours: 08
Location: Western Fairs Building
Union Representation: No
Benefits Eligible: Yes
Hybrid/Remote/On-Site: Hybrid
Benefits
Outstanding benefits and perks are among the many rewards of working for the University of California. UC Davis Health offers a full range of benefits, resources and programs to help you bring your best self to work, as well as to help you and your family achieve your health, wellness, financial and career goals. Learn more about the benefits below and eligibility rules by visiting our handy
Benefits Summary
and our
Benefits Page
High quality and low-cost medical plans to choose from to fit your family’s needs
UC pays for Dental and Vision insurance premiums for you and your family
Extensive leave benefits including Maternity, Paternity, Family & Medical Leave
14 paid Holidays annually
Vacation/Paid Time Off
Sick Leave with unlimited accrual
Continuing Education (CE) allowance and Education Reimbursement Program
Access to free professional development courses and learning opportunities for personal and professional growth
Wellness and on-site Employee Assistance Program including access to free mental health services
Supplemental insurance offered including additional life, disability, short/long term disability, pet insurance and legal coverage
Public Service Loan Forgiveness (PSFL) Qualified Employer & Student Loan Repayment Assistant for qualified roles
Retirement benefit options for eligible roles including Pension and other Retirement Saving Plans. More information on our retirement benefits can be found here
UC Davis cares about building a community, which is why we provide resources to enhance diversity, equity and inclusion as well as Employee Resource Groups (ERGs) to support our staff
Special Requirements
This position is a critical position and subject to a background check and drug screen. Employment is contingent upon successful completion of background investigation including criminal history, identity checks and drug screen.
As a condition of employment, you will be required to comply with the University of California Policy on Vaccination Programs , as may be amended or revised from time to time. Federal, state, or local public health directives may impose additional requirements.
Diversity, Equity, Inclusion and Belonging
At UC Davis, we’re solving life’s most urgent challenges to bring a fuller, healthier, and more resilient world within reach. We grow from every challenge we take on and we don’t just maintain - we improve. We recognize that creating an inclusive and intellectually vibrant organization means understanding and valuing both our individual differences and our common ground. The most comprehensive solutions come from the most diverse minds, and you belong here.
As you consider joining UC Davis, please explore our Principles of Community , our Clinical Strategic Plan and strategic vision for research and education, and our latest efforts to outgrow the expected . The University of California is an Equal Opportunity/Affirmative Action Employer advancing inclusive excellence. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, age, protected veteran status, or other protected categories covered by the UC nondiscrimination policy.
Because we want you to be seen, our recruiting process at UC Davis fosters authenticity, diversity, and inclusion. Studies have shown that some people may not apply to jobs unless they meet every single qualification. Each unique role at UC Davis has a set of requirements and you could be perfect for this role, or you could be perfect for the next role! Don’t meet all the requirements? We still encourage you to apply! #YouBelongHere
Show more
Show less","R, Stata, GIS, Epidemiology, Project Management, Data Management, Data Analysis, Causal inference, Statistics, Scientific Writing, Literature Reviews","r, stata, gis, epidemiology, project management, data management, data analysis, causal inference, statistics, scientific writing, literature reviews","causal inference, data management, dataanalytics, epidemiology, gis, literature reviews, project management, r, scientific writing, stata, statistics"
Data Research Analyst,California Department of Justice,"Sacramento, CA",https://www.linkedin.com/jobs/view/data-research-analyst-at-california-department-of-justice-3782261924,2023-12-17,Davis,United States,Mid senior,Remote,"***Please note that the State of California requires you to complete the Civil Service application process and applying on LinkedIn does not complete this process. If you would like to complete the application process, please click on the Apply button. Your information will be sent to one of our specialized recruiters and they will contact you to discuss next steps.***
This position is located in our Sacramento office. Applicants are required to live within a commutable distance from the office to provide for schedule flexibility.
The
California Department of Justice
is looking for a
Research Data Analyst
, who will perform research and statistical work in the Criminal Justice Statistics Center (CJSC).
The Research Data Analyst (RDA II Classification) will be responsible for planning and producing CJSC publications and responses to information requests. The RDA II will be expected to identify necessary data for a project, summarize the data using appropriate statistical techniques, and present the information orally and/or in writing.
They will be asked to perform statistical analyses on the data from 15 different databases for publications. This would include the creation of tables and charts that will be used in the statutorily mandated publications; assisting in conceptualizing a publication; establishing and maintaining cooperative, professional working relationships with staff of the Department, law enforcement agencies, courts, and other organizations; and ensuring full completion of assigned publications.
Desired Qualifications:
Experience with statistical packages, such as SPSS or SAS.
Ability to undertake multiple priorities/projects, and pay attention to detail.
Communicate effectively both orally and in writing.
Knowledge of the criminal justice process.
Ability to work independently with minimal supervision.
Develop and maintain effective and cooperative working relationships.
Demonstrate tact and good judgment.
Salary Range
$5,793.00 - $7,256.00
Benefits of working for the State of California:
Work-life balance
11 paid holidays
Flexible paid leave options
Consistent wage increases
Pension retirement
Job Stability
Health/Dental/Vision Insurance
Public Service Loan Forgiveness (PSLF)
And more!
Show more
Show less","Statistical analysis, SPSS, SAS, Criminal justice process, Data presentation, Cooperative relationships, Effective communication, Research, Data collection, Data summarization, Report writing, Attention to detail, Independence, Tact, Judgment, Publication planning, Database management","statistical analysis, spss, sas, criminal justice process, data presentation, cooperative relationships, effective communication, research, data collection, data summarization, report writing, attention to detail, independence, tact, judgment, publication planning, database management","attention to detail, cooperative relationships, criminal justice process, data collection, data presentation, data summarization, database management, effective communication, independence, judgment, publication planning, report writing, research, sas, spss, statistical analysis, tact"
Lead Data Statistician,Jackson Lewis P.C.,Greater Sacramento,https://www.linkedin.com/jobs/view/lead-data-statistician-at-jackson-lewis-p-c-3755011161,2023-12-17,Davis,United States,Mid senior,Remote,"Focused on labor and employment law since 1958, Jackson Lewis P.C.’s 950+ attorneys located in major cities nationwide consistently identify and respond to new ways workplace law intersects business. We help employers develop proactive strategies, strong policies and business-oriented solutions to cultivate high-functioning workforces that are engaged, stable and diverse, and share our clients' goals to emphasize inclusivity and respect for the contribution of every employee.
The Firm is ranked in the First Tier nationally in the category of Labor and Employment Litigation, as well as in both Employment Law and Labor Law on behalf of Management, in the U.S. News - Best Lawyers® “Best Law Firms”.
Job Summary
The Lead Data Statistician role in the Wage & Hour Data Analytics group provides support on various projects, including but not limited to, single plaintiff, class action, collective action, and DOL audit client exposure analyses, largely based on client timekeeping and payroll data. This individual will work autonomously to handle complex attorney/client issues and assist in overseeing the work of analysts.
Essential Functions
Regularly work as primary contact with attorneys, clients, and team manager to determine strategies on projects involving but not limited to class action data analytics.
Provide support on Practice Group initiatives, marketing efforts and pitches
Assess client exposure under various litigation scenarios – class action, collective action, etc.
Provide clear and concise communications regarding the analysis processes and results – both written and oral – to attorneys and client contacts, with the ability to explain complex topics in an easy-to-understand manner.
Develop methods and strategies to reach the end goal of a project based on various types/formats of raw client data.
Work with attorneys and analysts for analysis revisions, often on-the-fly during mediation proceedings.
Develop strong and effective relationships with clients
Execute assigned tasks in a self-directed, proactive manner, while actively managing own time and quality of work
Appropriately identify areas of discrete code and/or analytical methods that should be utilized for project analysis.
Expand scope of data tools employed by analysis team, including R, in conjunction with data and process integration, which is ongoing.
Ability to lead a project, as needed, from data intake and methodology development to analysis completion.
Qualifications/Skills Required
Minimum three years of experience directly working with Wage & Hour topics and data (timekeeping and payroll data especially) preferred, ideally within a legal or HR context.
Direct experience with California Wage & Hour laws a large plus.
Expert working knowledge with Excel, especially with basic and intermediate Excel formulas (vlookups, sumifs, etc.). VBA experience a large plus.
Ability to use Excel formulas and mathematics to manipulate raw data into dollar-figure exposure estimates.
Critical thinking and problem-solving skills with attention to detail. Experience in R, SQL, Python, other statistical programs a plus.
Ability to write programs/macros to automate repetitive statistical and data tasks.
Strong organizational and communication skills.
Ability to determine client and project needs and independently design and prepare data and analyses
Ability to multi-task and manage multiple projects, from start to finish, with shifting priority levels.
Willingness to conform to standardized group practices.
Educational Requirements
Master’s degree in applied statistics, mathematics, econometrics or a similar field or commensurate experience or equivalent level of training.
For Dallas, TX, the expected hourly wage range for this position is between $38.00/hour – $51.00/hour. The actual compensation will be determined based on experience and other factors permitted by law.
In accordance with the Colorado Equal Pay for Equal Work Act, the expected salary range for this Colorado position is between $38.00/hour – $51.00/hour. The actual compensation will be determined based on experience and other factors permitted by law.
Jackson Lewis offers a competitive benefits package that includes:
medical, dental, vision, life and disability insurance
401(k) Retirement Plan
Flexible Spending & Health Savings Account
firm-paid holidays, vacation, and sick time
For California and NY State, the expected salary range for this position is between $38/hour and $51/hour. The actual compensation will be determined based on experience and other factors permitted by law.
Jackson Lewis understands that embracing our differences makes us a stronger, better firm. We appreciate the importance of having a workforce that reflects the various communities in which we work, and we strive to create an inclusive environment where diverse employees want to work and where they can flourish professionally. In furtherance of our culture, all qualified applicants will receive consideration for employment without regard to race, national origin, gender, age, religion, disability, sexual orientation, veteran status, marital status or any other characteristics protected by law.
Show more
Show less","Wage & Hour Data Analytics, Class Action Data Analytics, California Wage & Hour Laws, Microsoft Excel, Vlookup, Sumifs, VBA, R, SQL, Python, Statistical Programs, Mathematics, Econometrics, Applied Statistics","wage hour data analytics, class action data analytics, california wage hour laws, microsoft excel, vlookup, sumifs, vba, r, sql, python, statistical programs, mathematics, econometrics, applied statistics","applied statistics, california wage hour laws, class action data analytics, econometrics, mathematics, microsoft excel, python, r, sql, statistical programs, sumifs, vba, vlookup, wage hour data analytics"
RESEARCH DATA ANALYST I,CA Department of Corrections & Rehabilitation,"Sacramento, CA",https://www.linkedin.com/jobs/view/research-data-analyst-i-at-ca-department-of-corrections-rehabilitation-3784947428,2023-12-17,Davis,United States,Mid senior,Hybrid,"Job Description And Duties
Individuals who are eligible for a Training and Development Assignment may also be considered for this position.
This position is eligible for a hybrid telework schedule for employees who can satisfactorily perform their essential work functions independently and remotely. Specific telework schedules may be discussed further during the interview process. Telework is only available to residents of California.
The California Department of Corrections Rehabilitation (CDCR) Headquarters has implemented a new automated system to streamline the hiring process. The Human Resources Technology System (HRTS) is an intuitive, user-friendly system that provides the applicant access to real-time updates throughout the hiring process. HRTS will require new candidates to set up an account, which allows them to track their application’s progress. Through their account portal, candidates will also be able to provide any additional information needed in the hiring process, including setting up interviews as needed. After the final filing date, applicants will receive an email from DoNotReply@cdcr.ns2cloud.com or DoNotReply@CDCR (depending on the email provider), with direction on creating and/or accessing their account.
For HRTS communication, this is reference to Job Requisition (JR) 328 .
CDCR employees are encouraged to use their existing CDCR email address on their application.
You will find additional information about the job in the
Duty Statement
.
Working Conditions
Regular and consistent attendance is critical to the successful performance of this position. Must be regularly available and willing to work the hours necessary to meet business needs. Hybrid telework is available based on operational needs and is subject to change, with a current minimum one in-office work day per month requirement. In-office work requirements for new employees may increase during the first thirty (30) days of employment based on training needs. Please note that beyond the first thirty (30) days of employment, additional in-office workdays may be required based on operational need.
Minimum Requirements
You will find the Minimum Requirements in the Class Specification.
RESEARCH DATA ANALYST I
Additional Documents
Job Application Package Checklist
Duty Statement
Position Details
Job Code #:
JC-399495
Position #(s):
065-185-5729-002
Working Title:
Research Data Analyst I
Classification:
RESEARCH DATA ANALYST I
$3,895.00 - $4,426.00 A
$4,019.00 - $4,789.00 B
$4,819.00 - $6,032.00 C
# of Positions:
1
Work Location:
Sacramento County
Telework:
Hybrid
Job Type:
Permanent, Full Time
Department Information
Vision
We enhance public safety and promote successful community reintegration through education, treatment and active participation in rehabilitative and restorative justice programs.
Mission
To facilitate the successful reintegration of the individuals in our care back to their communities equipped with the tools to be drug-free, healthy, and employable members of society by providing education, treatment, rehabilitative, and restorative justice programs, all in a safe and humane environment.
The California Department of Corrections and Rehabilitation (CDCR) and California Correctional Health Care Services (CCHCS) are committed to building and fostering a diverse workplace. We believe cultural diversity, backgrounds, experiences, perspectives, and unique identities should be honored, valued, and supported. We believe all staff should be empowered. CDCR/CCHCS are proud to foster inclusion and representation at all levels of both Departments.
Founded in 1885, the California Department of Corrections and Rehabilitation (CDCR) is one of the nation’s largest and most diverse correctional departments in the country. We are dedicated to enhancing public safety through the safe and secure incarceration of individuals in our care, by providing effective parole supervision, and implementing
rehabilitative strategies for their successful reintegration into our communities.
This position is in CDCR's Office of Research, which is located at Hwy 50 and Folsom Blvd. in Rancho Cordova.
Special Requirements
Do not include any confidential information on any documents you submit for a job vacancy, such as your State application, resume, or educational transcripts. Confidential information that should be excluded or removed from these documents includes, but is not limited to, your Social Security Number (SSN), birthdate, student identification number, driver’s license number (unless required), basis of eligibility, examination results, LEAP status, marital status, and age. Confidential information on the first page of applications submitted electronically online, such as Easy ID number, SSN, examination related information, and driver’s license number will automatically be redacted upon submission.
Possession of Minimum Qualifications will be verified prior to interview and/or appointment. If you are meeting Minimum Qualifications with education, you must include your unofficial transcripts/diploma for verification. Official transcripts may be required upon appointment. If it is determined a candidate does not meet the Minimum Qualifications, the candidate will be withheld from the eligibility list, rendering them inactive and ineligible to be hired for the position. Candidates who have been withheld may file an appeal with the State Personnel Board.
Candidates new to CDCR or the Division of Juvenile Justice are required to submit to a background investigation process utilizing Live Scan fingerprinting, and Tuberculosis testing prior to appointment followed by departmental annual Tuberculosis testing/evaluation thereafter.
Desirable Qualifications
In addition to evaluating each candidate's relative ability, as demonstrated by quality and breadth of experience, the following factors will provide the basis for competitively evaluating each candidate:
Willingness to create and promote an equitable and inclusive workplace where diversity is celebrated and valued.
Contact Information
The Hiring Unit Contact is available to answer questions regarding the position or application process.
Hiring Unit Contact:
Demetrio Martinez
(279) 223-1767 ext:
demetrio.martinez@cdcr.ca.gov
Please direct requests for Reasonable Accommodations to the interview scheduler at the time the interview is being scheduled. You may direct any additional questions regarding Reasonable Accommodations or Equal Employment Opportunity for this position(s) to the Department's EEO Office.
EEO Contact:
CDCR EEO OFFICE
(916) 255-1301
Personnel-HelpDesk@cdcr.ca.gov
California Relay Service: 1-800-735-2929 (TTY), 1-800-735-2922 (Voice) TTY is a Telecommunications Device for the Deaf, and is reachable only from phones equipped with a TTY Device.
Supplemental Questionnaire
A Supplemental Questionnaire (not to exceed one page, double spaced, Calibri 12-point font) is required to be submitted with your application. The Supplemental Questionnaire consists of a series of questions that may be scored as part of the application screening process.
Applications submitted without the Supplemental Questionnaire or with a Supplemental Questionnaire that does not respond to each of the questions provided will not be considered.
Based on the Duty Statement / Job Description for this position, please describe how your relevant work experience and/or education qualifies you for this position.
Please describe your knowledge, skills, and abilities related to Tableau and/or statistical analysis software.
Requirements
Exam Requirements
To obtain list eligibility for the Research Data Analyst I, before applying for the position, you must first take and pass the Research Data Analyst I exam:
Research Data Analyst I Exam
Equal Opportunity Employer
The State of California is an equal opportunity employer to all, regardless of age, ancestry, color, disability (mental and physical), exercising the right to family care and medical leave, gender, gender expression, gender identity, genetic information, marital status, medical condition, military or veteran status, national origin, political affiliation, race, religious creed, sex (includes pregnancy, childbirth, breastfeeding and related medical conditions), and sexual orientation.
It is an objective of the State of California to achieve a drug-free work place. Any applicant for state employment will be expected to behave in accordance with this objective because the use of illegal drugs is inconsistent with the law of the State, the rules governing Civil Service, and the special trust placed in public servants.
Show more
Show less","Tableau, Statistical analysis software, Data analysis, Research skills, Microsoft Office","tableau, statistical analysis software, data analysis, research skills, microsoft office","dataanalytics, microsoft office, research skills, statistical analysis software, tableau"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Elk Grove, CA",https://www.linkedin.com/jobs/view/senior-staff-ai-data-engineer-at-recruiting-from-scratch-3773086811,2023-12-17,Davis,United States,Mid senior,Hybrid,"Who is Recruiting from Scratch :
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
https://www.recruitingfromscratch.com/
This is a hybrid role based in our
Palo Alto
or
San
Francisco
offices and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.
What’s the job?
We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.
In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.
Responsibilities:
Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models
Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling
Be self-motivated in seeking solutions when the correct path isn’t always known
Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders
Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams
Build data processing streams for cleaning and modeling text data for LLMs
Research and evaluate new technologies in the big data space to guide our continuous improvement
Collaborate with multi-functional teams to help tune the performance of large data applications
Work with Privacy and Security team on data governance, risk and compliance initiatives
Work on initiatives to ensure stability, performance and reliability of our data infrastructure
What We’ll Love About You
Bachelors in Computer Science, Mathematics, Physics, or a related fields
5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience
Experience in statistical analysis & visualization on datasets using Pandas or R
Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools
Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models
Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy
Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Experience with any public cloud environment - AWS, GCP or Azure
Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc
Experience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have
2+ years of experience of technical leadership in building data engineering pipelines for AI
Previous experience in building data pipeline for conversational AI APIs and recommender systems
Experience with distributed systems and microservices
Experience with Kubernetes and building Docker images
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Strong understanding of applied machine learning topics
Be familiar with legal compliance (with data management tools) data classification, and retention
Consistent track record of managing and implementing complex data projects
What You'll Love About Us
Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world
Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto
Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents
Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US
Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs
Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more
Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
Base Pay Range
$160,000—$280,000 USD
https://www.recruitingfromscratch.com/
Show more
Show less","Data engineering, Machine learning, Data mining, Data cleaning, Data normalization, Data modeling, Python, Java, Bash, SQL, Git, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, AWS, GCP, Azure, DynamoDB, ETL, Kafka, Storm, SparkStreaming, Applied machine learning, Data classification, Data retention, Data management tools, Legal compliance","data engineering, machine learning, data mining, data cleaning, data normalization, data modeling, python, java, bash, sql, git, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, aws, gcp, azure, dynamodb, etl, kafka, storm, sparkstreaming, applied machine learning, data classification, data retention, data management tools, legal compliance","airflow, applied machine learning, aws, azure, bash, data classification, data cleaning, data engineering, data management tools, data mining, data normalization, data retention, datamodeling, docker, dynamodb, etl, gcp, git, helm, java, kafka, kubernetes, legal compliance, machine learning, python, snowflake, spark, sparkstreaming, sql, storm"
Research Data Specialist II (JC-406424),Caltrans,"Sacramento County, CA",https://www.linkedin.com/jobs/view/research-data-specialist-ii-jc-406424-at-caltrans-3780019290,2023-12-17,Davis,United States,Mid senior,Hybrid,"Research Data Specialist II
Local Civil Compliance Data Specialist
Job Control:
JC-406424
Classification:
Research Data Specialist II
Working Title:
Local Civil Compliance Data Specialist
Annual Salary:
$79,896.00 - $100,008.00
Apply by:
12/29/23
All applications must be submitted through the external link on CalCareers.
""At Caltrans, we foster a supportive environment that encourages open communication and mutual respect, empowering you to bring your best ideas forward. Our commitment to work-life balance ensures you have time to recharge and pursue personal passions, while our dedication to job security guarantees a stable and fulfilling career.""
About the Position:
Under the general direction of the Branch Chief of Local Civil Compliance Data Management, a Senior Transportation Planner, the incumbent serves as a Local Civil Rights Data Specialist. This position requires a highly skilled data specialist with the ability to collect, evaluate, clean, and interpret data from complex data collection systems. The incumbent will be responsible for
developing and maintaining new and existing data collection systems and databases for the Disadvantage Business Enterprise (DBE), Title VI, and ADA programs. The incumbent must possess journey level expertise with structured query language, python, or other computer programming languages along with the expertise using software such as Microsoft Excel, Access, Power BI,
Tableau, and other data visualization tools. In addition, the incumbent assists with the continued development and implementation of policies, programs, and reporting to comply with Federal and State laws related to these programs.
*See more details on the Duty Statement located on the job posting.
For questions please contact:
Hiring Unit Contact:
Lionel McCrory
(279) 789-2042
lionel.mccrory@dot.ca.gov
Show more
Show less","Python, SQL, Data visualization, Microsoft Excel, Microsoft Access, Power BI, Tableau, DBE, Title VI, ADA, Data cleaning, Data interpretation, Data collection, Compliance, Reporting, Structured query language","python, sql, data visualization, microsoft excel, microsoft access, power bi, tableau, dbe, title vi, ada, data cleaning, data interpretation, data collection, compliance, reporting, structured query language","ada, compliance, data cleaning, data collection, data interpretation, dbe, microsoft access, microsoft excel, powerbi, python, reporting, sql, tableau, title vi, visualization"
"Rapid Data Surveillance and Analytics Lead | Research Scientist III (Epi/Bio), JC-407021",California Department of Public Health,"Sacramento, CA",https://www.linkedin.com/jobs/view/rapid-data-surveillance-and-analytics-lead-research-scientist-iii-epi-bio-jc-407021-at-california-department-of-public-health-3785895155,2023-12-17,Davis,United States,Mid senior,Hybrid,"Department Information
At the California Department of Public Health (CDPH), equity, diversity, and inclusion are at the core of our mission to advance the health and well-being of California’s diverse people and communities. We are genuinely and strongly committed to cultivating and preserving a culture of inclusion and connectedness where we can grow and learn together with a diverse team of employees. In recruiting for team members, we welcome the unique contributions that you can bring to us and the work we do.
The Substance and Addiction Prevention Branch (SAPB), within the Center for Healthy Communities, aims to reduce individual, social, and environmental harm from addiction and substance use through research-driven prevention, education, and treatment. SAPB houses California Department of Public Health’s (CDPH) youth cannabis prevention, alcohol, overdose, and problem gambling programs, creating synergy between substance use and addiction prevention efforts. In addition to efforts related to addiction and substance use, SAPB examines a wide range of societal, community, and social influences on individual consumption and the resulting social and behavioral outcomes.
Job Description And Duties
The Research Scientist III (RS III) (Epidemiology/Biostatistics) works under the general direction of the Research Scientist Supervisor I (Social/Behavioral Sciences), Chief, Surveillance and Analytics Unit of the Substance and Addiction Prevention Branch.
The RS III leads the development and implementation of highly complex surveillance and research studies related to substance misuse and overdose using rapid data and advanced data analytics.
The RS III will evaluate and prepare data sources, develop surveillance and research indicators and tools, conduct ongoing analyses and surveillance, and write up reports, data briefs, and manuscripts for dissemination to diverse audiences. The RS III will be responsible for ensuring the scientific quality of all administrative and research reporting requirements.
Minimal travel (up to 5%) is required to participate in local, regional, and national activities.
Some overnight stays may be required.
The attached duty statement indicates whether this position is eligible for telework. All employees who telework are required to be California residents in accordance with Government Code 14200, and may be required to report to a CDPH office, when needed. Candidates who reside outside of the state of California may be interviewed; however, the selected candidate must have a primary residency in the state of California prior to appointment (and continue to maintain California residency) as a condition of employment. Failure to meet this requirement may result in the job offer being rescinded.
Final Filing Date: 1/10/2024
Working Title: Rapid Data Surveillance and Analytics Lead
Classification: RESEARCH SCIENTIST III (EPIDEMIOLOGY/BIOSTATISTICS)
Monthly Salary Range: $7,811.00 - $9,777.00
Work Location: Sacramento County
Telework: Hybrid
Job Type: Permanent, Full Time
Shall Consider:
RESEARCH SCIENTIST III (SOCIAL BEHAVIORAL SCIENCES)
Monthly Salary Range: $7,811.00 - $9,777.00
Should you have any questions regarding the application process, our Recruitment Team is ready to help you at Apply@cdph.ca.gov or (916) 445-0983.
Show more
Show less","Epidemiology, Biostatistics, Data analytics, Rapid data surveillance, Research studies, Data sources, Surveillance indicators, Writing reports, Data briefs, Manuscripts, Telework","epidemiology, biostatistics, data analytics, rapid data surveillance, research studies, data sources, surveillance indicators, writing reports, data briefs, manuscripts, telework","biostatistics, data briefs, data sources, dataanalytics, epidemiology, manuscripts, rapid data surveillance, research studies, surveillance indicators, telework, writing reports"
"Data and Integrity Branch Chief | Research Scientist Supervisor II (Social/Behavioral), JC-399813",California Department of Public Health,"Sacramento, CA",https://www.linkedin.com/jobs/view/data-and-integrity-branch-chief-research-scientist-supervisor-ii-social-behavioral-jc-399813-at-california-department-of-public-health-3786804958,2023-12-17,Davis,United States,Mid senior,Hybrid,"Department Information
At the California Department of Public Health (CDPH), equity, diversity, and inclusion are at the core of our mission to advance the health and well-being of California’s diverse people and communities. We are genuinely and strongly committed to cultivating and preserving a culture of inclusion and connectedness where we can grow and learn together with a diverse team of employees. In recruiting for team members, we welcome the unique contributions that you can bring to us and the work we do.
The Women, Infants, and Children Program (WIC) has been part of the nation’s nutrition safety net for over 40 years.
Extensive research has found WIC to be a cost-effective investment that leads to healthier infants, more nutritious diets, health care for children, and subsequently higher academic achievement for students. WIC helps families receive healthy food, nutrition education, breastfeeding support, and referrals to healthcare and other community services.
Like other divisions within the California Department of Public Health (CDPH), WIC builds its program by addressing social determinants of health. WIC impacts the life course by promoting health practices that can ultimately improve life outcomes often experienced by vulnerable communities.
WIC serves babies and children up to age five, pregnant women, and new mothers. Dads, grandparents, foster parents of young children, and working families are welcome at WIC too!
Monthly, nearly one million WIC participants can find support at one of 84 local agencies that offer services at more than 500 sites throughout California. WIC participants can redeem their food benefits at approximately 3,800 grocers that are authorized to serve WIC families.
In 2019 - 2020, the WIC Division implemented an Electronic Benefits Transfer (EBT) card called the California WIC Card and a new management information system that allows WIC participants to be served more efficiently.
About The CDPH/WIC Family
Over 200 state staff work at CDPH’s WIC Division, headquartered at the Natomas campus in Sacramento. WIC is committed to a hybrid workforce and the state provides a modest stipend for those who fully and partially work from home. Staff are provided with a computer or laptop, camera, and large screen to conduct their work. We build our unit teams virtually and maintain contact with stakeholders by being on camera to solidify our relationships.
The WIC Division is led by senior managers administering a Local Policy and Health Programs Branch, Local Services Branch, Vendor Management Branch, Communications, Food, and Vendor Policy Branch, Systems Integration Branch, Data and Integrity Branch, and the Operations team. Through these branches, our support staff, analysts, specialists, researchers, and nutritionists serve the needs of WIC families.
Job Description And Duties
Incumbents in one of these classifications, where a Master’s Degree is required to meet the minimum qualifications, shall receive an educational pay differential equivalent to 2% of their monthly base pay. Those where a Doctoral Degree is a required to meet the minimum qualifications, shall receive an educational pay differential equivalent to 3% of their monthly base pay. Individuals with both a Master’s and Doctoral Degree will be given the best benefit in relation to salary shall only be eligible for only one amount.
Incumbents in one of these classifications, who have evidence of being accepted for an examination for a Medical Specialty Certificate or Osteopathic Specialty Certificate shall receive 15.5% of monthly base salary.
Incumbents in one of these classifications, who have evidence of being certified by an American Medical Specialty Board or American Osteopathic Specialty Board will receive 19.0% of monthly base salary.
This position supports the CDPH mission and strategic plan by advancing the health and well-being of California’s diverse people and communities . As an employee of the Data and Integrity Branch, the incumbent serves as part of a team that collects, analyzes, monitors, and leverages data, research, and policies to benefit, optimize, and protect the WIC program and its participants, local agencies, and vendors.
The incumbent works under the general direction from the Division Chief, Career Executive Assignment (C.E.A.). The Research Scientist Supervisor II (RSS II) manages the Data and Integrity Branch which includes the Program Integrity and Audits Section, the Participant Analysis, Research, and Evaluation Section, and the Vendor Analysis, Research, and Evaluation Section.
The RSS II represents California’s perspective in communications with the United States Department of Agriculture (USDA), National WIC Association (NWA), and research partners to identify WIC research and program evaluation needs, data collection and analysis requirements, and program integrity strategies; evaluates and guides opportunities to participate in national and statewide research projects. Provides technical assistance necessary to complete the more complex and highly sensitive assignments related to WIC research, monitoring, and program evaluation activities; leads efforts to improve program integrity activities via advanced data analytics. Assures compliance with state and federal policies and data reporting requirements.
The RSS II actively participates as a member of the WIC Division’s Management Team and Senior Leadership Team.
This position is primarily a remote-centered position (telework), with an occasional need to report to the office in Natomas/Sacramento based on business need.
“This is a re-advertisement to increase candidate pool. If you previously applied, there is no need to re-apply as your application is on file.”
The attached duty statement indicates whether this position is eligible for telework. All employees who telework are required to be California residents in accordance with Government Code 14200, and may be required to report to a CDPH office, when needed. Candidates who reside outside of the state of California may be interviewed; however, the selected candidate must have a primary residency in the state of California prior to appointment (and continue to maintain California residency) as a condition of employment. Failure to meet this requirement may result in the job offer being rescinded.
Final Filing Date: 12/29/2023
Working Title: Data and Integrity Branch Chief
Classification: RESEARCH SCIENTIST SUPERVISOR II (SOCIAL/BEHAVIORAL)
Monthly Salary Range: $10,615.00 - $13,225.00
Work Location: Sacramento County
Telework: Telework
Job Type: Permanent, Full Time
Should you have any questions regarding the application process, our Recruitment Team is ready to help you at Apply@cdph.ca.gov or (916) 445-0983.
Show more
Show less","Research, Data analysis, Data collection, Program evaluation, Data reporting, Data analytics, Program integrity, Program improvement, Policy analysis, Policy development, Communication, Collaboration, Leadership, Management, Statistics, Research methods, Research design, Research ethics, Data visualization, Data mining, Data modeling, Machine learning, Artificial intelligence, R, SAS, SPSS, STATA, Python, SQL, Excel, PowerPoint, Tableau, Power BI","research, data analysis, data collection, program evaluation, data reporting, data analytics, program integrity, program improvement, policy analysis, policy development, communication, collaboration, leadership, management, statistics, research methods, research design, research ethics, data visualization, data mining, data modeling, machine learning, artificial intelligence, r, sas, spss, stata, python, sql, excel, powerpoint, tableau, power bi","artificial intelligence, collaboration, communication, data collection, data mining, data reporting, dataanalytics, datamodeling, excel, leadership, machine learning, management, policy analysis, policy development, powerbi, powerpoint, program evaluation, program improvement, program integrity, python, r, research, research design, research ethics, research methods, sas, spss, sql, stata, statistics, tableau, visualization"
Data Analyst-Power BI,"First Brands Group, LLC","Ciudad Juárez, Chihuahua, Mexico",https://mx.linkedin.com/jobs/view/data-analyst-power-bi-at-first-brands-group-llc-3786585892,2023-12-17,El Centro,United States,Mid senior,Onsite,"First Brands Group is the parent company of many of the automotive industry’s most recognizable OEM and Aftermarket brands. First Brands Group is a privately held organization and leader in automotive repair and maintenance products. Headquartered in Cleveland, Ohio with operations found around the globe, we are a team of professionals dedicated to building profitable, enduring growth among all our customers with an unrelenting drive to exceed customers’ expectations.
First Brands Group is hiring a DATA ANALYST. This role on site
The DATA ANALYST
will assist by:
Principal activities
·
Significant experience in Procurement within his/her category/commodity.
·
Demonstrated business and financial acumen
·
Analitical skills.
·
Strong negotiation skills
·
Demonstrated project management skills.
Education and Experience:
Bachelor’s Degree
Experiencie with POWER BI.
Good EXCEL skills such as Macros.
Advanced English
Experience with Dashboards
Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability, or protected veteran status
Mostrar más
Mostrar menos","Data Analysis, Procurement, POWER BI, Excel, Macros, Dashboards","data analysis, procurement, power bi, excel, macros, dashboards","dashboard, dataanalytics, excel, macros, powerbi, procurement"
Business Intelligence Data Analyst - ELP,Education at Work,"El Paso, TX",https://www.linkedin.com/jobs/view/business-intelligence-data-analyst-elp-at-education-at-work-3779625206,2023-12-17,El Centro,United States,Mid senior,Onsite,"Education at Work is an innovative, mission driven business process outsourcing (BPO) company putting college students at the forefront of our unique business model. As a rapidly growing company with a start-up mindset, we are committed to fostering a collaborative environment and entrepreneurial culture which values and progresses out of the box thinkers and savvy problem solvers.
Position Summary
The Data Analyst who is able to turn project requirements into custom-formatted data reports. The ideal candidate for this position is able to do complete life cycle data generation and outline critical information for the company. Ideally, the Data Analyst is able to analyze business procedures and recommend specific types of data that can be used to improve upon them. A data analyst's job is to take that data and use it to help the company make better business decisions.
Essential Functions
Development and maintain various reports required to analyze call trends and historical patterns.
Synthesize current business intelligence or trend data to support recommendations for action.
Manage timely flow of business intelligence information to users.
Gathering user requirements.
Provide error free decision-making reports to management on a daily, weekly, monthly and annual basis.
Designing and automating reporting using Power BI.
Various performance related analysis as needed.
Other duties as deemed by Management.
Minimum Job Requirements
Proficient knowledge of Power BI – including multiple data connections, visualizations, DAX, M.
Expert knowledge of Excel- including graphs, pivot tables, formulas.
Excellent communication skills.
Strong organizational skills.
Detail orientated.
Ability to work effectively as an individual and in a team environment.
Ability to meet company attendance and dependability guidelines.
Must be able to sit for long periods of time and repeatedly use a computer or other operational hardware.
Education
Bachelor’s degree, preferred. Equivalent level of work experience considered.
Working Conditions
Ability to travel on occasion.
Ability to function at a computer workstation for long periods of time.
Ability to work independently.
Ability to work with team members offsite.
$50,000 - $60,000 a year
Exempt Position
THE AMERICANS WITH DISABILITIES ACT OF 1990 (ADA) PROHIBITS DISCRIMINATION IN COMPENSATION AND EMPLOYMENT OPPORTUNITIES AGAINST QUALIFIED INDIVIDUALS WITH DISABILITIES. TO DETERMINE WHETHER AN INDIVIDUAL IS QUALIFIED, THE ESSENTIAL FUNCTIONS OF EACH JOB MUST BE IDENTIFIED. ESSENTIAL FUNCTIONS ARE THOSE THAT ARE INTRINSIC TO THE POSITION, AND THAT THE INDIVIDUAL(S) WHO HOLDS THE JOBS MUST BE ABLE TO PERFORM WITH OR WITHOUT REASONABLE ACCOMMODATION.
Show more
Show less","Power BI, DAX, M, Excel, Pivot tables, Data connections, Communication, Organization, Detail orientation, Team work, Attendance, Reliability, Bachelor's degree, Travel, Independent work, Offsite team work","power bi, dax, m, excel, pivot tables, data connections, communication, organization, detail orientation, team work, attendance, reliability, bachelors degree, travel, independent work, offsite team work","attendance, bachelors degree, communication, data connections, dax, detail orientation, excel, independent work, m, offsite team work, organization, pivot tables, powerbi, reliability, team work, travel"
Data Scientist,Archetype Permanent Solutions,"El Paso, TX",https://www.linkedin.com/jobs/view/data-scientist-at-archetype-permanent-solutions-3787916075,2023-12-17,El Centro,United States,Mid senior,Onsite,"We are seeking a highly skilled and analytical Data Scientist to join our team. The ideal candidate will be adept at using large data sets to find opportunities for product and process optimization, and using models to test the effectiveness of different courses of action. This role will require both a strong technical background in data analysis and the ability to communicate findings to non-technical teams and stakeholders.
Key Responsibilities:
Work with stakeholders to identify opportunities for leveraging company data to drive business solutions.
Mine and analyze data from company databases to drive optimization and improvement of product development, marketing techniques, and business strategies.
Assess the effectiveness and accuracy of new data sources and data gathering techniques.
Develop custom data models and algorithms to apply to data sets.
Use predictive modeling to increase and optimize customer experiences, revenue generation, ad targeting, and other business outcomes.
Coordinate with different functional teams to implement models and monitor outcomes.
Develop processes and tools to monitor and analyze model performance and data accuracy.
Qualifications:
Bachelor’s or Master’s degree in Statistics, Mathematics, Computer Science, or another quantitative field.
2 years of experience in a Data Scientist or Data Analyst role.
Strong problem-solving skills with an emphasis on product development.
Experience using statistical computer languages (R, Python, SQL, etc.) to manipulate data and draw insights from large data sets.
Experience working with and creating data architectures.
Knowledge of a variety of machine learning techniques (clustering, decision tree learning, artificial neural networks, etc.) and their real-world advantages/drawbacks.
Excellent written and verbal communication skills for coordinating across teams.
Preferred Skills:
Experience with distributed data/computing tools: Map/Reduce, Hadoop, Hive, Spark, Gurobi, MySQL, etc.
Experience analyzing data from 3rd party providers: Google Analytics, Site Catalyst, Coremetrics, Adwords, Crimson Hexagon, Facebook Insights, etc.
Experience with AWS, Azure, or another cloud service.
Experience visualizing/presenting data for stakeholders using: Periscope, Business Objects, D3, ggplot, etc.
Benefits:
Competitive salary and benefits package.
Dynamic and innovative work environment.
Opportunities for professional growth and development.
[Other company-specific benefits like remote work options, flexible schedules, wellness programs, etc.]
How to Apply:
Please submit your resume, cover letter, and any relevant work samples
Powered by JazzHR
2c7D8cKfPG
Show more
Show less","Data Science, Data Analysis, Data Processing, Statistics, Mathematics, Computer Science, Predictive Modeling, Machine Learning, R, Python, SQL, Data Architectures, Clustering, Decision Tree Learning, Artificial Neural Networks, Distributed Data Tools, Hadoop, Hive, Spark, Gurobi, MySQL, AWS, Azure, Cloud Services, Data Visualization, Periscope, Business Objects, D3, ggplot","data science, data analysis, data processing, statistics, mathematics, computer science, predictive modeling, machine learning, r, python, sql, data architectures, clustering, decision tree learning, artificial neural networks, distributed data tools, hadoop, hive, spark, gurobi, mysql, aws, azure, cloud services, data visualization, periscope, business objects, d3, ggplot","artificial neural networks, aws, azure, business objects, cloud services, clustering, computer science, d3, data architectures, data processing, data science, dataanalytics, decision tree learning, distributed data tools, ggplot, gurobi, hadoop, hive, machine learning, mathematics, mysql, periscope, predictive modeling, python, r, spark, sql, statistics, visualization"
Data Process Analyst with Security Clearance,ClearanceJobs,"El Paso, TX",https://www.linkedin.com/jobs/view/data-process-analyst-with-security-clearance-at-clearancejobs-3753486308,2023-12-17,El Centro,United States,Mid senior,Hybrid,"Responsibilities PeopleTec is currently seeking a Data Process Analyst to support our El Paso, TX (Fort Bliss) location. Our team is looking for an exceptionally motivated self-starting professional with a background supporting front-end/UI software development, intelligence analysis, and/or big data projects . The candidate will support our growing team of cyber, space, and engineering professionals who design, implement, test, and deploy end-to-end 2D/3D geospatial solutions. The Data Entry Analyst is responsible for ensuring the accurate entry of operational and intelligence data from operational reporting into collaboration environments like Command and Control of the Information Environment (C2IE). Th Data Entry Analyst supports an operational joint task force with a critical National defense mission in a fast-paced environment. This Data Entry Analyst is uniquely gifted to enjoy detailed data entry processes and solve different problems each day. This position involves collaborating with intelligence, operational, & data professionals across multiple echelons of Command. Qualifications Required Skills/Experience :
Develop statistical material and reports
Build briefing products that provide situational awareness for the counter-narcotics mission
Transfer data from various formats & locations (e.g., paper, storyboards, other databases) into a common collaboration environment like C2IE
Create spreadsheets and other products that are used to make data-driven decisions based on data trends & synthesis
Clean, process, and retrieve data from a variety of databases & sources to ingest into the collaboration environment
Perform regular backups to ensure data preservation
Sort and organize paperwork after entering data to ensure it is not lost
Collaborate with operational staff (e.g., intelligence, operations, IT, Command) to gain user requirements for data insights
Travel: 0 %
Must be a U.S. Citizen
An active DoD Secret clearance is required to perform this work. Candidates are required to have an active Secret clearance upon hire, and the ability to maintain this level of clearance during their employment. Education Requirements :
High school degree or equivalent & proven experience as data entry professional Desired Skills :
Have general knowledge and access to decision support systems (i.e., C2IE, Advana) and the experience working with data in the environment
Exposure to/experience in USG organizations that have a counter-narcotics mission
AS or BS in data, statistics, or another relevant subject Projected Timeframe to Employee :
Feb 1, 2024 Overview People First. Technology Always. PeopleTec, Inc. is an employee-owned small business founded in Huntsville, AL that provides exceptional customer support by employing and retaining a highly skilled workforce. Culture: The name ""PeopleTec"" was deliberately chosen to remind us of our core value system - our people. Our company's foundation was built on placing our employees and customers first. With an award-winning atmosphere, we have matured into a company that boasts the best and brightest across multiple technical fields. Career: At PeopleTec, we value your long-term goals. Whether it's through our continuing-education opportunities, our robust training programs, or our ""People First"" benefits package, PeopleTec truly believes that our best investments are our people. Come Experience It. #cjpost #dpost EEO Statement PeopleTec, Inc. is an Equal Employment Opportunity employer and provides reasonable accommodation for qualified individuals with disabilities and disabled veterans in its job application procedures. If you have any difficulty using our online system and you need an accommodation due to a disability, you may use the following email address, and/or phone number (256.319.3800) to contact us about your interest in employment with PeopleTec, Inc. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, genetic information, citizenship, ancestry, marital status, protected veteran status, disability status or any other status protected by federal, state, or local law. PeopleTec, Inc. participates in E-Verify.
Show more
Show less","Data Entry, Geospatial, Databases, C2IE, Advana, Spreadsheets, Statistical Material, Briefing Products, Big Data, Data Processing","data entry, geospatial, databases, c2ie, advana, spreadsheets, statistical material, briefing products, big data, data processing","advana, big data, briefing products, c2ie, data entry, data processing, databases, geospatial, spreadsheets, statistical material"
Data Analyst,CDx Diagnostics®,"Suffern, NY",https://www.linkedin.com/jobs/view/data-analyst-at-cdx-diagnostics%C2%AE-3776292021,2023-12-17,Ossining,United States,Associate,Onsite,"JOB SUMMARY:
Candidate will have responsibilities in organizing, managing, and analyzing large and varied healthcare data sets, to optimize business operations. We seek a quantitative specialist who understands the business needs of a laboratory and how to use data to meet those needs. The qualified applicant must have the ability to multi-task, impeccable attention to detail, be a collaborative team member, and be a diligent problem solver who uses data in creative ways to reach business goals.
RESPONSIBILITIES:
Proficiency in database programming using SQL scripts, and Excel formulas
Compiling and organizing healthcare data
Import, export, and manipulate large data sets in million row database
Daily extract and import data from SFTP to SQL
Design and populate tables for tracking and reporting of data
Able to develop, and review queries
Maintain best practices in data storage and data sharing methods
Investigating data to find patterns and trends
Utilizing different data sources for analyses
Converting data into usable information that is easy to understand
Developing reports and presentations
In network payer expect fee file maintenance and communication with billing vendor
Daily deposit administrative prep and reconciliation
Daily charge reconciliation
Understand the data we receive from external billing vendor daily for use in routine and adhoc reporting
Commitment to meeting established reporting timelines
Assists on special projects and/or assignments
Reconcile ancillary A/R vendor activity and fees reconciliation
Adheres to HIPAA regulations, company, and departmental policies
Other duties as assigned
EDUCATION AND EXPERIENCE:
Bachelor's degree or equivalent experience
Two+ years of experience in data analytics
1+ year in medical billing with a healthcare facility or billing company
SKILLS, KNOWLEDGE, AND ABILITIES:
Proficiency with database programming languages including SQL
Proficient in Microsoft Excel, Power BI, Word, and Outlook
Proficiency managing electronic medical records
Ability to identify, analyze and solve problems
Basic understanding laboratory business operations
Self-motivated and approach tasks with a positive, proactive demeanor
Excellent organization and time management skills required
Ability to communicate effectively in writing, over the telephone, and in-person while always remaining HIPAA compliant
Familiar with medical billing terminology
Familiarity with commercial insurances, Medicare, Medicaid, PPO, and HMO plans, and their guidelines
Work effectively in a team environment
We are an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status or any other status protected under federal, state or local law.
We participate in E-Verify.
Show more
Show less","SQL, Excel, Power BI, Outlook, Electronic Medical Records, Data Analytics, Medical Billing, HIPAA Compliance, Communication Skills, Teamwork","sql, excel, power bi, outlook, electronic medical records, data analytics, medical billing, hipaa compliance, communication skills, teamwork","communication skills, dataanalytics, electronic medical records, excel, hipaa compliance, medical billing, outlook, powerbi, sql, teamwork"
Data Engineer (Contract),Michael Kors,"New York, NY",https://www.linkedin.com/jobs/view/data-engineer-contract-at-michael-kors-3759818852,2023-12-17,Dauphin, Canada,Mid senior,Hybrid,"*Contract position
Who You Are:
You are a talented Data Engineer looking to grow, gaining visibility working with business leadership on day-to-day support and long-term projects. You are an analytical thinker, problem solver, and innovator that is looking to work on the latest technologies with employees up and down the organization. You love taking on projects where you interact throughout a global organization and are not afraid to be challenged. You have strong customer data experience with high volume transactions.
What You’ll Do:
Join the global data team in building data delivery services that support critical operational and analytical applications for our internal business operations, customers and partners.
Prepare, clean, and format analytical datasets for processing and analysis.
Build and maintain custom ETL pipelines.
Become an expert in our datasets, their strengths and weaknesses, and write code to pull and verify data.
Conduct database feature engineering to support ongoing quantitative research.
Work with developers to create and deploy systems for anomaly detection.
Interface with data scientists, software developers, and other analytics operations staff as needed.
Serve as a point-of-contact for questions about data structures, definitions, and quality.
Work directly with Product and Systems Owners to deliver data products in a collaborative environment.
Design department-wide principles and workflow for data quality management.
You'll Need to Have:
B.S. Computer Science/Engineering/Technology or Statistics/Mathematics or equivalent work experience
5+ years of experience working in Data Engineering
5+ years of experience developing in SQL
Experience with Language R
Experience using ETL tools such as Talend, Informatica, Data Services
5+ years of experience with data profiling and data pipeline development
Experience with Python and Snowflake
Experience with Azure
Experience working with large data sets
We’d Love to See:
Experience in the retail industry
Experience working with customer data
Participation in tool selection processes regarding data tools and software
Hands-on experience working through the entire software development lifecycle of data management and business intelligence solutions
Knowledge of industry leading data architecture and data management practices
Excellent communication skills, including both oral and written
Excellent active listening and critical thinking skills
Ability to multi-task, exercise excellent time management, and meet multiple deadlines
Demonstrated excellence in project management and organization
High level of critical thinking and analytical skills
Ability to consistently deliver excellent customer service
Excellent attention to detail and ability to document information accurately
Capability to resolve escalated issues arising from operations and requiring coordination with other departments
At Capri, we are all responsible for creating a diverse and inclusive workplace. We try to inspire change and growth within each other and believe success is a result that comes from our differences. Capri is proud to be an equal opportunity employer committed to inclusivity regardless of age, sex, sexual orientation, gender identity, genetic characteristics, race, color, creed, religion, ethnicity, national origin, alienage, citizenship, disability, marital status, military status, pregnancy, or any other legally-recognized protected basis prohibited by applicable law. M/D/F/V
In compliance with certain Pay Transparency laws, employers are required to disclose a salary range. The salary for this position will vary based on role requirements, skill set and years of experience.
Show more
Show less","Data Engineering, Data Analytics, Data Delivery Services, ETL Pipelines, Data Profiling, Data Pipeline Development, SQL, Language R, Talend, Informatica, Data Services, Python, Snowflake, Azure, Data Architecture, Data Management Practices, Project Management, Organization, Customer Service, Attention to Detail, Documentation, Escalated Issues Coordination","data engineering, data analytics, data delivery services, etl pipelines, data profiling, data pipeline development, sql, language r, talend, informatica, data services, python, snowflake, azure, data architecture, data management practices, project management, organization, customer service, attention to detail, documentation, escalated issues coordination","attention to detail, azure, customer service, data architecture, data delivery services, data engineering, data management practices, data pipeline development, data profiling, data services, dataanalytics, documentation, escalated issues coordination, etl pipelines, informatica, language r, organization, project management, python, snowflake, sql, talend"
Senior Data Engineer,Skechers,"Manhattan Beach, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-skechers-3752369008,2023-12-17,Dauphin, Canada,Mid senior,Hybrid,"Company Description
Headquartered in Southern California, Skechers has spent nearly 30 years helping men, women and kids everywhere look and feel good. Developing comfort technologies is at the foundation of all that we do—delivering stylish, innovative, and quality products at a reasonable price. From our diverse footwear offering to a growing range of apparel and accessories, Skechers is a complete lifestyle brand.
With international business representing over half of our total sales, we have product available in more than 170 countries and significant opportunities for continued expansion worldwide. We sell our collections direct to consumers through more than 4,000 Skechers stores around the globe and Skechers e-commerce sites, as well as through a network of third-party partners.
Job Description
Are you excited about high performance Big Data environments? Then great! The Data Engineering team is growing, and we need Data Engineers who are thorough and agile, capable of breaking down and solving problems, and have a strong will to get things done. In the Data Engineering team, you will work on real-world problems working on-premise or multi cloud tech stack where reliability, accuracy and speed are paramount, take responsibility for your systems end-to-end and influence the direction of our technology that impacts customers around the world. We are looking for a Senior Data Engineer with both conceptual and hands-on experience working on structured/semi structured/complex data processing and streaming frameworks, RDBMS, and NoSQL data stores. As a member of our Data Services team, you will be a member of a service group responsible for continuing organizational expansion of our data processing projects. Ideal candidate must be enthused about all spectrum of data development, including data transport, data processing, data warehouse/ETL integration, quick learning, and self-starting. This position includes 24x7 production support.
Responsibilities
Collaborate with data stewards, data architects, and data engineers to design, implement and deliver successful data solutions.
Drive engineering best practices, set standards and propose larger projects which may require cross-team collaboration.
Define technical requirements and implementation details for the underlying data lake, data warehouse and data marts involved in the design and implementation of full cycle of data services, from data ingestion, data processing, ETL to data delivery for reporting.
Identify, troubleshoot, and resolve production data integrity and performance issues.
Design, develop and support various data platform applications.
Design and develop applications to process large amounts of critical information in batch and near real-time to power business insights.
Demonstrate ability to mentor junior engineers.
Proficiency in leading meetings and facilitating discussions.
Strong self-starter with capability to drive initiatives independently with minimal oversight.
Qualifications
Experience in managed services for data ingestion/processing with hands on experience working in AWS environment and operational experience of Kinesis/Kafka, S3, Glue and Athena.
Experience with the following data processing technologies: Spark, Kafka, Kinesis
A solid understanding of NoSQL data stores with extensive experience in working with SQL, script languages (Python, shell etc.)
Proven experience of distributed systems driving large-scale data processing and analytics Expertise with RDMS and Data Warehousing (Strong SQL)
Experience working with BI and data warehousing tools building data pipelines and real-time data streams.
Experience with Linux KSH/bash scripting and java
Experience working with any one of ETL toolsets: Talend, Informatica
Experience with any of the following message / file formats: Parquet, Avro, ORC Protobuf
Excellent communication and presentation skills (verbal, written, presentation) across all levels of the organization.
Ability to translate ambiguous concepts into tangible ideas.
Experience with version control systems like Git
Proficient in writing technical specifications and documentation.
Plus
Experience with Presto, Hive, Impala or similar SQL based engine for Big Data
Experience with Cassandra, MongoDB, or similar NoSQL databases.
Experience with Scala, Nodejs
Work Experience
10+ years of experience defining, designing, and delivering data pipelines and solutions.
8+ years of experience working with Linux based operating systems.
6+ years relevant experience developing and integrating frameworks and database technologies that support highly scalable data processing.
5+ years of programming experience with Python
3+ years documented experience in a data engineering role on a variety of big data projects.
3+ year experience with cloud-based data warehousing systems (e.g., AWS Redshift, Snowflake, Google Big Query)
Proficient in any flavor of SQL
Demonstrable ability in data modeling, ETL development, data warehousing, batch, and real time data processing
Demonstrable experience with Stream Processing and workload management for data transformation, augmentation, analysis, etc.
Job-Related Education
B.S. in Computer Science, Computer Information Systems, Engineering, or another technical field, or equivalent work experience.
Additional Information
To perform this job successfully, an individual must be able to perform each essential duty satisfactorily. The skills, abilities and physical demands described are representative of those duties that must be met by an employee to successfully perform the essential functions of this job. Reasonable accommodation may be made to enable individuals with disabilities, who are otherwise qualified for the job position, to perform the essential functions.
PHYSICAL DEMANDS-
While performing the duties of this job, the employee is regularly required to stand; use hands to finger, handle, or feel, and talk or hear. The employee frequently is required to walk; sit, reach with hands and arms, and stoop, kneel. The employee is occasionally required to sit for long period of times.
All your information will be kept confidential according to EEO guidelines.
The salary range for this position is $130,000-$175,000.
Show more
Show less","AWS, Kinesis, Kafka, S3, Glue, Athena, Spark, SQL, Python, Linux, KSH, bash, Java, Talend, Informatica, Parquet, Avro, ORC, Protobuf, Presto, Hive, Impala, Cassandra, MongoDB, Scala, Nodejs, ETL, Data warehousing, Data modeling, Stream Processing, Workload management, Data transformation, Data augmentation, Data analysis","aws, kinesis, kafka, s3, glue, athena, spark, sql, python, linux, ksh, bash, java, talend, informatica, parquet, avro, orc, protobuf, presto, hive, impala, cassandra, mongodb, scala, nodejs, etl, data warehousing, data modeling, stream processing, workload management, data transformation, data augmentation, data analysis","athena, avro, aws, bash, cassandra, data augmentation, data transformation, dataanalytics, datamodeling, datawarehouse, etl, glue, hive, impala, informatica, java, kafka, kinesis, ksh, linux, mongodb, nodejs, orc, parquet, presto, protobuf, python, s3, scala, spark, sql, stream processing, talend, workload management"
Data Engineer,Stott and May,"New York, United States",https://www.linkedin.com/jobs/view/data-engineer-at-stott-and-may-3774118590,2023-12-17,Dauphin, Canada,Mid senior,Hybrid,"A renowned luxury brand located in the heart of New York City, is looking for a skilled GCP Data Engineer to join their innovative Data team. As a key player in their digital transformation, you will be instrumental in leveraging Google Cloud Platform (GCP) to enhance their data capabilities and drive their brand forward.
Key Responsibilities:
Develop and maintain scalable data pipelines within the GCP ecosystem.
Implement big data solutions for data collection, processing, and analysis.
Collaborate with cross-functional teams to integrate complex data sources.
Ensure data quality and optimize performance of data solutions.
Stay abreast of industry trends and advancements in cloud and data technologies.
Qualifications:
3+ years experience in data engineering with a focus on GCP.
Bachelor's Degree in Computer Science, or related field
Strong understanding of cloud services, particularly Google Cloud services like BigQuery, Dataflow, and Cloud Storage.
Proficiency in SQL and experience with programming languages such as Python.
Familiarity with data modeling, ETL processes, and data warehousing principles.
Development in an agile team and the tools used in CI / CD (Azure devops, Jira, Confluence)
Knowledge of Microsoft Power BI, data catalog tool, data quality, data management
Knowledge of Terraform and the administration of Google Cloud Platform appreciated (rights management, API activation, network settings, etc.)
Excellent problem-solving skills and ability to work in a fast-paced environment.
They Offer:
Competitive salary and benefits package.
Hybrid work model blending in-office collaboration with remote flexibility.
A dynamic and creative work environment in a leading luxury brand.
Opportunities for professional growth and development.
Show more
Show less","Data Engineering, GCP, Data Pipelines, Data Analytics, Cloud Services, BigQuery, Dataflow, Cloud Storage, Python, SQL, Data Modeling, ETL, Data Warehousing, Agile Development, CI / CD, Azure DevOps, Jira, Confluence, Microsoft Power BI, Data Catalog Tool, Data Quality, Data Management, Terraform, Google Cloud Platform Administration, ProblemSolving Skills, FastPaced Environment","data engineering, gcp, data pipelines, data analytics, cloud services, bigquery, dataflow, cloud storage, python, sql, data modeling, etl, data warehousing, agile development, ci cd, azure devops, jira, confluence, microsoft power bi, data catalog tool, data quality, data management, terraform, google cloud platform administration, problemsolving skills, fastpaced environment","agile development, azure devops, bigquery, ci cd, cloud services, cloud storage, confluence, data catalog tool, data engineering, data management, data quality, dataanalytics, dataflow, datamodeling, datapipeline, datawarehouse, etl, fastpaced environment, gcp, google cloud platform administration, jira, microsoft power bi, problemsolving skills, python, sql, terraform"
Data Engineer,Agility Partners,"Columbus, Ohio Metropolitan Area",https://www.linkedin.com/jobs/view/data-engineer-at-agility-partners-3777159808,2023-12-17,Dauphin, Canada,Mid senior,Hybrid,"""A Little About This Gig""
Agility Partners is seeking qualified applicants to fill an open Data Engineer position in Columbus, OH. They're in the business of connecting merchants with major wireless carriers, offering innovative solutions that enhance their operations. If you have a strong foundation in engineering principles and are ready to contribute to a dynamic, expanding team while solving complex problems in the wireless industry, we want to hear from you.
Apply advanced knowledge of technical concepts and principals to design and develop a variety of data warehousing and integration projects.
Design, develop and maintain accurate and timely data models to meet business needs.
Develop and maintain extensible integration workflows and ELT jobs connecting internal business applications.
Collaborate with application development teams to design and develop integrations that maintain data integrity.
Participate in the gathering and refinement of project requirements with product managers and business stakeholders.
Oversee deployments and support projects in production after release.
Recommend new practices, processes, and procedures to improve quality and productivity.
Review and approve changes to models and code.
Stay current with emerging technologies and industry trends.
Evaluate frameworks, technology, platforms, and tooling solutions.
The Perfect Candidate
5+ years of experience contributing to and supporting highly available, scalable, data systems
5+ years of experience with relational database design, implementation, and performance tuning optimization for MS SQL, SSIS, and SSRS
3+ years of experience building both OLAP and OTLP Data Platforms
3+ years of Experience writing data pipelines and tools using Python or similar languages
Experience integrating with Relational Databases, HTTP APIs, File Systems, and SaaS solutions
Experience with Redshift, Snowflake, Big Query, or other Data Warehousing solutions
Proficiency with AWS or other modern cloud hosting platforms
Proficiency with at least one major programing language and API framework
Experience with Apache Airflow is a plus
Experience with Docker, Redis, and CI/CD Pipelines is a plus
Show more
Show less","Apache Airflow, AWS, Big Query, CI/CD Pipelines, Data Integration, Data Modeling, Data Warehousing, Docker, ELT, HTTP APIs, Microsoft SQL Server, Python, Redshift, Redis, Relational Databases, SaaS, Snowflake, SSIS, SSRS","apache airflow, aws, big query, cicd pipelines, data integration, data modeling, data warehousing, docker, elt, http apis, microsoft sql server, python, redshift, redis, relational databases, saas, snowflake, ssis, ssrs","apache airflow, aws, big query, cicd pipelines, data integration, datamodeling, datawarehouse, docker, elt, http apis, microsoft sql server, python, redis, redshift, relational databases, saas, snowflake, ssis, ssrs"
Data Engineer,Clarity Recruiting,New York City Metropolitan Area,https://www.linkedin.com/jobs/view/data-engineer-at-clarity-recruiting-3778229801,2023-12-17,Dauphin, Canada,Mid senior,Hybrid,"DATA ENGINEER
Our client, a securities litigation law firm located in midtown Manhattan, is looking for a Data Engineer, to aid in the development and maintenance of data pipelines for new and existing institutional investor clients. You will work with new institutional clients to analyze their investment holding structure, including developing an understanding of institutional structure, related entities, how those entities interact, and any sub-funds relevant to our data collection efforts. You will work with new clients’ data teams to determine the best strategy for effectively onboarding client data to provide the firm with seamless ongoing data downloads. This will include working to identify formatting challenges between client data and our database structure, and continually identifying and addressing issues that arise related to client data integrity.
This role requires the utmost security and confidentiality, and expertise in Python and SQL.
Responsibilities:
Design, implement and optimize end-to-end data pipelines for efficient and scalable data processing and storage.
Conduct data analysis on large data sets to derive actionable insights, supporting informed decision-making across the firm.
Identify patterns, relationships, and problems in data and statistical sets.
Determine limitations in data reliability and usability, troubleshoot data-related problems.
Evaluate the performance of data systems; Identify and implement strategies to enhance the performance and efficiency of data processing workflows and systems, ensuring timely and accurate data delivery.
Develop techniques that allow for the collection of accurate data, which may include leveraging your mastery of Python to develop and optimize code for data extraction, transformation, and loading (ETL) processes.
Utilize advanced SQL skills for crafting complex queries, optimizing database performance, and ensuring seamless data retrieval. Develop and maintain stored procedures in both MySQL and MSSQL environments to encapsulate and streamline data processing logic.
Create, implement, and enforce policies and procedures related to the secure and efficient handling and sharing of client data.
Collaborate with cross-functional teams to create and maintain data models aligned with business requirements.
Manage two Data Analyst on the team.
Qualifications:
Bachelor’s degree in computer science, information technology, statistics, mathematics, or related field.
Experience working with institutional investor portfolios, banking and/or financial data is a plus.
Proficiency in Python and SQL for data manipulation and analysis.
A deep understanding of relational databases including MySQL and MSSQL, including performing schema design, indexing, performance optimization, and writing or modifying complex queries to apply transformation and logic to data sets.
Proven experience as a data engineer with a focus on managing data and building and maintaining data pipelines.
Highly proficient in data designing, modeling and mapping processes to load data based on business requirements.
Highly proficient at digesting, understanding, and analyzing large amounts of data.
Familiar with modern, high-tech databases and IT systems.
Ability to complete multiple time-sensitive tasks with competing deadlines.
Excellent verbal and written communication skills for conveying complex technical concepts.
Strong problem-solving skills with a keen attention to detail.
Work Environment:
This is a full-time, exempt position based in New York City.
This is a hybrid role (3 days in-office (Tuesday/Wednesdays required)
All candidates must be US citizens or Green Card holders.
All employees must be vaccinated and boosted.
Salary Range:
$115,000 to $160,000
Show more
Show less","Python, SQL, MySQL, MSSQL, Data pipelines, Data analysis, Data processing, Data engineering, Data modeling, Data mapping, Database design, Data warehousing, ETL (extract transform load), Data integration, Data quality, Data governance, Business intelligence, Data visualization, Cloud computing, Big data","python, sql, mysql, mssql, data pipelines, data analysis, data processing, data engineering, data modeling, data mapping, database design, data warehousing, etl extract transform load, data integration, data quality, data governance, business intelligence, data visualization, cloud computing, big data","big data, business intelligence, cloud computing, data engineering, data governance, data integration, data mapping, data processing, data quality, dataanalytics, database design, datamodeling, datapipeline, datawarehouse, etl extract transform load, mssql, mysql, python, sql, visualization"
Data Engineer,Murad,"El Segundo, CA",https://www.linkedin.com/jobs/view/data-engineer-at-murad-3776656441,2023-12-17,Dauphin, Canada,Mid senior,Hybrid,"Murad Skincare has a 30-year legacy of developing clinically proven products that are backed by science and cruelty-free. Founded by dermatologist Dr. Howard Murad, we follow four pillars of wellness to create products and experiences for happier lives: Eat your water, be kind to your mind, nourish your skin, and awaken your body. We’re proud to be part of the Unilever prestige family that works every day towards enhancing sustainability and inclusivity. We stand against discrimination and hate, and actively advocate for inclusion: so much so that one of our core values is respect, connect, and appreciate. Our team members are as diverse as they are talented—each bringing a unique perspective to an environment that encourages coloring outside of the lines, speaking up, and collaboration—and we’re ready for you to join our team!
The ideal candidate will be responsible for creating and supporting interactive dashboards and reports, specifically utilizing tools such as Microsoft Power BI to provide valuable business insights. This person should be well-versed in Power BI technology and will be in charge of creating and managing BI tools and have a thorough understanding of the BI system. This position supports across the business with information and will also support our entrepreneurial culture through the building, testing, and iterating of hypotheses. To do this, we need people with analytical expertise and the motivation to apply it to prestige skin care.
You will be responsible for the design, development, implementation and ongoing management of our data analytics function and platform. You will help with establishing a standard data model within a future-proofed environment; continuing to develop Business Analytics tools across Power BI and Oracle; and building analytical skills across the business. The Data Engineer should demonstrate a passion and curiosity for identifying and interpreting trends within complex datasets to deliver actionable insights. The ideal candidate will have the right analytical mindset, enjoy problem solving, and understand how to ask the right questions to deliver solution-focused outcomes.
This role reports through the Information Technology function to the Senior Manager Development but is broader in scope than simply IT Function.
RESPONSIBILTIES:
As a Data engineer, you will be responsible for designing, developing, and maintaining data visualizations and reports using the Power BI platform.
Work closely with team members and business stakeholders to understand and analyze business requirements for data reporting and analytics.
Connect and integrate data from various sources, such as databases, spreadsheets, and cloud services, into Power BI datasets.
Understanding of data management operations and database administration, including data modeling, data definition, data conversion and management of content or unstructured information.
Ability to design and maintain efficient data models, schemas, and database structures to optimize storage.
Create and optimize Power BI reports and dashboards that provide valuable insights to support business decision-making.
Comprehend and work with Key Performance Indicators (KPIs) and metrics.
Stay up to date with the latest data engineering technologies.
Ability to work in fast-paced teams with a great work ethic and an enthusiasm for problem solving and data analysis required.
REQUIREMENTS/QUALIFICATIONS:
Bachelor's degree in Computer Science, IT, or related field, or equivalent experience, Masters degree is a plus
3+ years relevant work experience in data modeling, data engineering and enterprise information architecture.
Expertise in SQL, data pipeline and workflow management tools (Apache Airflow), as well as data warehousing (BigQuery).
Familiarity with data visualization and analytics with Power BI.
Expertise with ETL solutions, including analyzing, compiling, cleansing, interpreting, joining and staging data.
Able to facilitate user requirement gathering to inform design, or reverse engineer requirements from existing data sources & applications.
Extensive experience in developing end-to-end Power BI reporting and analytics solutions.
Proficiency in Microsoft Power BI, including data modeling and DAX (Data Analysis Expressions).
Experience with data integration from various sources
Proficient in SQL with a detailed understanding of data. Experience extracting data from databases using SQL (queries, stored procedures, functions).
Strong understanding of KPIs and metrics.
Familiarity with Oracle databases and data warehousing.
Ability to adapt to changing tools and technologies.
Knowledge of UiPath & Oracle APEX is a plus.
WHAT WE OFFER:
Unlimited Paid Time Off
Medical, Dental, Vision Insurance
FSA/HSA
Life/Accident Insurance
Employee Assistance Program (EAP)
Professional Development Resources
401(k) Employer Match Program
This position is based in our El Segundo, CA headquarters, which is currently 40% hybrid onsite/remote work.
The expected base salary for this position is $105,000 to $125,000. The exact base salary is determined by various factors including experience, skills, education, geographic location, and budget. In addition to base salary, Murad offers a competitive benefits package to eligible employees, including medical insurance, dental insurance, vision insurance, life insurance, accident insurance, critical illness insurance, disability insurance, retirement savings plans and company match, paid family leave, education-related programs, paid holidays, discretionary vacation time, and more.
Murad is committed to equal treatment and opportunity in all aspects of recruitment, selection, and employment without regard to race, color, religion, national origin, ethnicity, age, sex, marital status, physical or mental disability, gender identity, sexual orientation, veteran or military status, or any other category protected under the law. Murad is an equal opportunity employer; committed to creating a community of inclusion, and an environment free from discrimination, harassment, and retaliation.
Show more
Show less","Microsoft Power BI, DAX, ETL, Apache Airflow, BigQuery, SQL, Oracle, UiPath, Oracle APEX, Data visualization, Data analytics, Data engineering, Data modeling, Data warehousing, Business intelligence, Data integration, Data management, Database administration, Key Performance Indicators (KPIs), Problem solving, Analytical mindset, Communication skills, Teamwork","microsoft power bi, dax, etl, apache airflow, bigquery, sql, oracle, uipath, oracle apex, data visualization, data analytics, data engineering, data modeling, data warehousing, business intelligence, data integration, data management, database administration, key performance indicators kpis, problem solving, analytical mindset, communication skills, teamwork","analytical mindset, apache airflow, bigquery, business intelligence, communication skills, data engineering, data integration, data management, dataanalytics, database administration, datamodeling, datawarehouse, dax, etl, key performance indicators kpis, microsoft power bi, oracle, oracle apex, problem solving, sql, teamwork, uipath, visualization"
Python Data Engineer,Synechron,"New York, NY",https://www.linkedin.com/jobs/view/python-data-engineer-at-synechron-3784861425,2023-12-17,Dauphin, Canada,Mid senior,Hybrid,"Role: Python Developer/Data Engineer
Location: New York, New York, United States
Day 1 Hybrid Onsite
We are looking for strong Python Developer with strong background in data engineering and integration experience.
Duties :
Integration Engineer responsible for daily support and project based development of credit risk management systems.
ETL developers are responsible for designing and creating the data warehouse and all related extraction, transformation and load of data functions.
This is an opportunity to gain experience in risk management processing using new technologies.
You are :
5 years of full-time development experience using Python.
Experience building data piplines using Azure Data Factory and Databricks.
Experience with Python application frameworks (Django, Flask, Pyramid, Tornado).
Experience with Python testing and code analysis tools (Pytest, Pylint).
Strong SQL skills.
Familiarity with SSIS.
Strong troubleshooting skills.
On-point communication skills.
Education :
Bachelor’s degree in computer science or finance.
We can offer you:
A highly competitive compensation and benefits package
A multinational organization with 44 offices in 19 countries and the possibility to work abroad
Laptop and a mobile phone
10 days of paid annual leave (plus sick leave and national holidays)
Maternity & Paternity leave plans
A comprehensive insurance plan including: medical, dental, vision, life insurance, and long-/short-term disability (plans vary by region)
Retirement savings plans
A higher education certification policy
Commuter benefits (varies by region)
Extensive training opportunities, focused on skills, substantive knowledge, and personal development
On-demand Udemy for Business for all Synechron employees with free access to more than 5000 curated courses
Coaching opportunities with experienced colleagues from our Financial Innovation Labs (FinLabs) and Center of Excellences (CoE) groups
Cutting edge projects at the world’s leading tier-one banks, financial institutions and insurance firms
A flat and approachable organization
A truly diverse, fun-loving and global work culture
SYNECHRON’S DIVERSITY & INCLUSION STATEMENT
Diversity & Inclusion are fundamental to our culture, and Synechron is proud to be an equal opportunity workplace and is an affirmative action employer. Our Diversity, Equity, and Inclusion (DEI) initiative ‘Synclusive’ is committed to fostering an inclusive culture – promoting equality, diversity and an environment that is respectful to all. We strongly believe that a diverse workforce helps build stronger, successful businesses as a global company. We encourage applicants from across diverse backgrounds, race, ethnicities, religion, age, marital status, gender, sexual orientations, or disabilities to apply. We empower our global workforce by offering flexible workplace arrangements, mentoring, internal mobility, learning and development programs, and more.
All employment decisions at Synechron are based on business needs, job requirements and individual qualifications, without regard to the applicant’s gender, gender identity, sexual orientation, race, ethnicity, disabled or veteran status, or any other characteristic protected by law.
Thanks & Regards
Vikarant Kumar
Show more
Show less","Python, Azure Data Factory, Databricks, Django, Flask, Pyramid, Tornado, Pytest, Pylint, SQL, SSIS, Bachelor's degree in computer science or finance","python, azure data factory, databricks, django, flask, pyramid, tornado, pytest, pylint, sql, ssis, bachelors degree in computer science or finance","azure data factory, bachelors degree in computer science or finance, databricks, django, flask, pylint, pyramid, pytest, python, sql, ssis, tornado"
Senior Data Engineer,Alpine Solutions Group,"Charlotte, NC",https://www.linkedin.com/jobs/view/senior-data-engineer-at-alpine-solutions-group-3769503312,2023-12-17,Dauphin, Canada,Mid senior,Hybrid,"DAY TO DAY:
This large insurance organization with headquarters in NJ and a presence in Charlotte is seeking a Senior Data Engineer to join their IT team. The data engineering team works on a day to day basis with data scientists and data analysts as well as the business. This group receives requirements from product owners and works to first understand those requirements and then focuses on developing the code in a multitude of applications. The Data Engineer will utilize SQL and Python for programming which includes testing and working with the business for sign offs. The Data Engineers also work with ETL tools specifically Informatica.
QUALIFICATIONS:
6+ years of data engineering experience with a focus within data modeling
Extremely hands on with EITHER Databricks OR Azure Data Factory (most elaborate on experience)
ETL OR ELT exposure with expertise in EITHER IICS or Informatica Power Center
Great communication
SQL programming background
Exposure to Cloud platforms (Azure preferred but not required)
Show more
Show less","Data Engineering, Data Modeling, SQL, Python, Testing, ETL, Databricks, Azure Data Factory, Informatica Power Center, Informatica Cloud Integration Services (IICS), Cloud Platform, Azure","data engineering, data modeling, sql, python, testing, etl, databricks, azure data factory, informatica power center, informatica cloud integration services iics, cloud platform, azure","azure, azure data factory, cloud platform, data engineering, databricks, datamodeling, etl, informatica cloud integration services iics, informatica power center, python, sql, testing"
Senior Data Engineer,Brady Corporation,"Milwaukee, WI",https://www.linkedin.com/jobs/view/senior-data-engineer-at-brady-corporation-3777075095,2023-12-17,Dauphin, Canada,Mid senior,Hybrid,"Who we are:
Brady makes products that make the world a safer and more productive place. We are a global leader in safety, identification and compliance solutions for a diverse range of workplaces. From the depths of the ocean to outer space, from the factory floor to the delivery room - we’re just about everywhere you look. Companies around the world trust Brady because of our deep expertise and knowledge across a wide range of industries and applications - powered by our world-class manufacturing capabilities.
We have a diverse customer base in industries including electronics, telecommunications, manufacturing, electrical, construction, healthcare, aerospace and more. As of July 31, 2023, Brady employed approximately 5,600 people worldwide. Our fiscal 2023 sales were approximately $1.33 billion. Brady stock trades on the New York Stock Exchange under the symbol BRC. You can learn more about us at www.bradycorp.com.
Why work at Brady:
A career at Brady means working for a global company that has thrived for over 100 years, and whose innovative spirit drives our future growth.
Brady offers competitive pay and great benefits, supported by a culture that encourages collaboration and innovation. We strive to foster an inclusive workplace where diverse talent can learn, grow, and succeed. And with deeply rooted values, no matter where you work at Brady, you’ll feel connected to the community through our charitable contributions and opportunities to give back.
Our headquarters are in Milwaukee, Wisconsin, but we have more than 70 locations globally, giving our employees the opportunity to work with colleagues around the world.
What we need:
Brady is seeking a Senior BI Developer/Data Engineer who will design, develop, and support of data pipelines from multiple applications and sources to facilitate data analytics needs. Our team partners with Sales & Marketing, Finance, and Operations business users and data scientists to help develop data-driven insights and make data-driven decisions. The Senior BI Developer works with various data and BI tools and technologies, such as Python, SQL Server, Google Big Query, Google Cloud Composer, and etc.
What you'll be doing:
Develop data pipelines from multiple sources using Python with API calls.
Develop SQL code or data flow to integrate and transform data from multiple sources into useful, consistent and easily consumable data elements.
Work with business analysts and business users to understand data requirements and map into easy consumable data elements.
Provide ongoing enhancement and support for data pipelines in the production environment. Understand critical needs of the business, identify and fix issues with a sense of urgency.
Participate in discussions on design options and approaches, propose solutions and develop the new capabilities.
Perform and lead the code reviews to identify coding, process and functionality improvements.
Work on troubleshooting, identifying issues and performance bottlenecks, problem-solving and impact analysis.
Follow the Scrum process and take ownership of Jira stories.
Lead the offshore developers, provide directions and guide them with development and support processes.
What you'll need to be successful:
Bachelor of Science in Computer Science, Engineering or related field
Self-motivated with a strong work ethic, ability to work independently as well as ability to work within a team
Strong communication skills, ability to communicate well with technical and non-technical staff
3+ years experience working with Python or .NET programming
3+ years of experience working with ETL and SQL development
3+ years of experience working with cloud data platforms, GCP, Azure or AWS
Strong analytical and problem-solving skills including the ability to independently troubleshoot issues and determine and develop the best solution while considering all impacts to the production system
Willing to learn and able to adapt to new technologies as needed
Benefits:
Complete insurance coverage starting on the first day of employment – medical, dental, vision, life
401(k) with company match
Tuition reimbursement
Bonus opportunity
Vacation and holiday pay
#LI-Hybrid
Show more
Show less","Python, SQL Server, Google Big Query, Google Cloud Composer, ETL, GCP, Azure, AWS","python, sql server, google big query, google cloud composer, etl, gcp, azure, aws","aws, azure, etl, gcp, google big query, google cloud composer, python, sql server"
Senior Data Engineer,Venturi,Cincinnati Metropolitan Area,https://www.linkedin.com/jobs/view/senior-data-engineer-at-venturi-3772486821,2023-12-17,Dauphin, Canada,Mid senior,Hybrid,"A large grocery retail client is looking to bring on a Data Engineer to help them with their Product Pillar. Please see the job description below.
Job Description
Accountable for developing and delivering technological responses to targeted business outcomes. Analyze, design and develop enterprise data and information architecture deliverables, focusing on data as an asset for the enterprise. Understand and follow reusable standards, design patterns, guidelines, and configurations to deliver valuable data and information across the enterprise, including direct collaboration with our partners, where needed.
• Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
• Experience building and optimizing 'big data' data pipelines, architectures and data sets.
• Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
• Strong analytic skills related to working with unstructured datasets.
• Build processes supporting data transformation, data structures, metadata, dependency and workload management.
• A successful history of manipulating, processing and extracting value from large disconnected datasets.
• Working knowledge of message queuing, stream processing, and highly scalable 'big data' data stores.
• Strong project management and organizational skills.
Key Responsibilities
• Utilize enterprise standards for data domains and data solutions, focusing on simplified integration and streamlined operational and analytical uses
• Ensure there is clarity between ongoing projects, escalating when necessary, including direct collaboration with our partners
• Leverage innovative new technologies and approaches to renovate, extend, and transform the existing core data assets, including SQL-based, NoSQL-based, and Cloud-based data platforms
• Define high-level migration plans to address the gaps between the current and future state
• Contribute to the development of cost/benefit analysis for leadership to shape sound architectural decisions
• Analyze technology environments to detect critical deficiencies and recommend solutions for improvement
• Promote the reuse of data assets, including the management of the data catalog for reference
• Draft architectural diagrams, interface specifications and other design documents
This role is in the Customer Experience pillar supporting Customer Data Acceleration.
The team comes in to the office 1-2 days per week. The manager has found it to be more productive for this role to be local to Cincinnati.
If this looks like a good opportunity for you, please apply with an up to date resume.
Venturi is an equal opportunity employer, committed to supporting and creating a diverse and inclusive workforce that fosters mutual respect for our employees and the communities we serve.All qualified applicants will receive consideration for employment without regard to sex, race, color, national origin, sexual orientation, gender, gender identity, genetic information, religion, disability, age, veteran status, or any other legally protected status under national, federal, state, or local law
Show more
Show less","SQL, Data pipelines, Big data, Data analysis, Data transformation, Data structures, Metadata, Message queuing, Stream processing, Data stores, Data domains, Data solutions, NoSQL, Cloud platforms, Migration plans, Data catalog, Architectural diagrams, Interface specifications","sql, data pipelines, big data, data analysis, data transformation, data structures, metadata, message queuing, stream processing, data stores, data domains, data solutions, nosql, cloud platforms, migration plans, data catalog, architectural diagrams, interface specifications","architectural diagrams, big data, cloud platforms, data catalog, data domains, data solutions, data stores, data structures, data transformation, dataanalytics, datapipeline, interface specifications, message queuing, metadata, migration plans, nosql, sql, stream processing"
Senior Data Engineer,In Technology Group Inc (US),"New York, United States",https://www.linkedin.com/jobs/view/senior-data-engineer-at-in-technology-group-inc-us-3775748658,2023-12-17,Dauphin, Canada,Mid senior,Hybrid,"Job Title
: Lead Data Engineer
Location
: Hybrid (NYC)
Salary
: $175,000
About Us:
My client is a cutting-edge AI-enabled Consumer Intelligence platform that specializes in tracking and forecasting consumer trends. Their platform leverages data from over a dozen different sources, including news, social media, scientific literature, and search data, to offer their clients a comprehensive understanding of consumer behavior. They provide insights into the ""who,"" ""why,"" ""what,"" and ""where"" behind the latest trends.
About the Job:
Engineer and manage an ETL architecture using cutting-edge technologies such as Apache Beam and Airflow.
Design and implement an efficient data architecture for streamlined data processing and integration.
Operationalize machine learning models, taking charge of deployment, reliability, scalability, and performance optimization.
NLP experience is a must-have.
Monitoring & Reliability-
Create, deploy, and oversee a real-time monitoring system for data ingestion processes.
Swiftly identify and resolve any issues within the system to maintain a high level of data integrity and system reliability.
Key Qualifications-
Proven experience in leading engineering teams, particularly in a dynamic startup environment.
A strong commitment to writing clean, well-documented code, with a preference for proficiency in Python and GCP.
Expertise in building data pipelines and ensuring data accuracy using unit and integration tests, emphasis on
streaming data.
Proficiency in distributed computing frameworks, including Apache Beam and Spark.
A solid understanding of ETL processes and their alignment with data science requirements.
Exceptional problem-solving skills and the ability to troubleshoot complex data issues.
Effective communication skills to facilitate cross-functional collaboration.
A willingness to learn and adapt to emerging technologies and concepts.
Bonus Qualifications-
Experience working with Big Data is a significant advantage, demonstrating familiarity with handling extensive datasets and distributed computing.
Proficiency with DevOps tools such as Kubernetes, Terraform, and Docker for infrastructure management and automation.
Familiarity with ML (Machine Learning), AI (Artificial Intelligence), NLP (Natural Language Processing), and Data Science concepts
.
An interest in staying informed about cultural trends and current events, which can be valuable for understanding and interpreting consumer trends and streaming data.
Why Join Us?
Work in a cutting-edge AI environment where you can shape the future of consumer intelligence.
Collaborate with a talented and diverse team of professionals.
Access opportunities for continuous learning and growth.
Contribute to the development of a platform that is at the forefront of consumer trend analysis.
If you are a dynamic technical leader with a passion for AI and data, and you're ready to take on a leadership role in a startup environment, we want to hear from you!
Show more
Show less","Apache Beam, Airflow, NLP, Stream processing, Kafka, Kubernetes, Terraform, Docker, Python, Python libraries, GCP, Hadoop, Apache Spark, ETL, Machine Learning, Artificial Intelligence, Data Science, Data integration, Data architecture, Big Data, Distributed computing","apache beam, airflow, nlp, stream processing, kafka, kubernetes, terraform, docker, python, python libraries, gcp, hadoop, apache spark, etl, machine learning, artificial intelligence, data science, data integration, data architecture, big data, distributed computing","airflow, apache beam, apache spark, artificial intelligence, big data, data architecture, data integration, data science, distributed computing, docker, etl, gcp, hadoop, kafka, kubernetes, machine learning, nlp, python, python libraries, stream processing, terraform"
Data Engineer,Insight Global,"Manhattan, NY",https://www.linkedin.com/jobs/view/data-engineer-at-insight-global-3769099682,2023-12-17,Dauphin, Canada,Mid senior,Hybrid,"Duration: Full Time - Permanent
Location: Hybrid Manhattan, NY
Citizenship: US Citizen or Green Card Holders Only
**No C2C**
Must Haves:
4+ years as a Data Engineer – Preferably someone coming from a start-up (
fast paced, high stress environment, urgent)
Looking at despaired data, data collection, data manipulation, data reformation, building pipelines, identifying discrepancies in data streams
Experience with
Databricks
and AWS - Proficient in SQL and Spark
Strong coding skills in Scala or Python - familiarity with software engineering principles around testing, code reviews and deployment
Experience owning and managing a full pipeline within a large project
Strong communication, interpersonal, and presentation skills - Experience working heavily cross-functionally
.
Plusses:
Experience working within the media, entertainment, television, and/or streaming industry.
Familiarity with ML and MLOps concepts and technologies, such as model training, deployment, and monitoring.
Day-to-day:
Insight Global is seeking out a Senior Data Engineer to sit hybrid in New York City for a large media/entertainment client. This Sr. Engineer will be joining the Playback Analytics Engineering team who is responsible for building out data pipelines and applications for the operations of current products and all new international/global product launches. As part of the Global Video Engineering organization, this team collects client-side and server-side requirements and prepares it for all analytics workloads. This engineer will be working closely with the Content Delivery team to gather all generic information/requirements (package drops, retransmissions, disconnections) and Decision Sciences teams for the creation of all visualizations of the data. This person will be owning full segments of a data engineering project, managing expectations, and driving delivery with the appropriate stakeholders while maintaining the consensus for engineering architecture. This engineer will be a major contributor to the overall data product roadmap by working closely with business partners to understand their challenges and develop analytical tools to help drive business decisions. They will design, build, and scale data pipelines across a variety of source systems and streams, different environments, and downstream applications and implement the appropriate design patterns with a focus on performance, security, scale, and end user experience. This person will be expected to build and manage relationships with supporting IT teams to effectively deliver work products to production and participate in development sprints, demos, and retrospectives, as well as release and deployment. This person must be action orientated as they will be constantly figuring out new problems and regularly showing results with a positive attitude by displaying ethical behavior, integrity, and building trust. The ideal candidate has dealt with ambiguity and can make quality decisions in a dynamic, fast-paced environment.
Show more
Show less","Data Engineering, Databricks, AWS, SQL, Spark, Scala, Python, Software Engineering Principles, Testing, Code Reviews, Deployment, Communication, Interpersonal Skills, Presentation Skills, Collaboration, ML, MLOps, Model Training, Model Deployment, Model Monitoring, Data Pipelines, Data Applications, Clientside Requirements, Serverside Requirements, Data Visualization, Data Product Roadmap, Business Intelligence, Analytical Tools, Data Source Systems, Data Streams, Environments, Downstream Applications, Design Patterns, Performance, Security, Scale, End User Experience, IT Teams, Production, Development Sprints, Demos, Retrospectives, Release, Deployment, Action Orientation, Problem Solving, Results Orientation, Positive Attitude, Ethical Behavior, Integrity, Trust, Ambiguity, Dynamic Environment, Fastpaced Environment","data engineering, databricks, aws, sql, spark, scala, python, software engineering principles, testing, code reviews, deployment, communication, interpersonal skills, presentation skills, collaboration, ml, mlops, model training, model deployment, model monitoring, data pipelines, data applications, clientside requirements, serverside requirements, data visualization, data product roadmap, business intelligence, analytical tools, data source systems, data streams, environments, downstream applications, design patterns, performance, security, scale, end user experience, it teams, production, development sprints, demos, retrospectives, release, deployment, action orientation, problem solving, results orientation, positive attitude, ethical behavior, integrity, trust, ambiguity, dynamic environment, fastpaced environment","action orientation, ambiguity, analytical tools, aws, business intelligence, clientside requirements, code reviews, collaboration, communication, data applications, data engineering, data product roadmap, data source systems, data streams, databricks, datapipeline, demos, deployment, design patterns, development sprints, downstream applications, dynamic environment, end user experience, environments, ethical behavior, fastpaced environment, integrity, interpersonal skills, it teams, ml, mlops, model deployment, model monitoring, model training, performance, positive attitude, presentation skills, problem solving, production, python, release, results orientation, retrospectives, scala, scale, security, serverside requirements, software engineering principles, spark, sql, testing, trust, visualization"
Data Engineer,Prime Team Partners,"Seattle, WA",https://www.linkedin.com/jobs/view/data-engineer-at-prime-team-partners-3775473763,2023-12-17,Dauphin, Canada,Mid senior,Hybrid,"Data Engineer
Seattle, WA.
Open to Hybrid (2-3 days a week in office) or Mostly Remote (about once a month in office).
Salary: $96,000 - $144,000.
No work visa sponsorship or relocation assistance.
The Client
Our client is a cutting-edge biotech company who is at the forefront of genetic medicine research and development. This is a great fit for motivated individuals excited by the opportunity to make a positive impact on the world. Based in Seattle, WA, their team is growing like crazy! Join this exciting and innovative team now!
The Role
Our client is seeking a talented Data Engineer to join their team in Seattle, WA. This will be the first and only Data Engineer in the company taking ownership of their data transformation process. This Data Engineer would work closely with the ML team and Experimental Scientists in the org to ensure that all data collected from experiments is standardized and easily interoperable. This Data Engineer would be responsible for things like ingesting data from multiple different sources (proprietary, Excel, etc.), transforming these datasets into standardized formats within a database, tagging that data with metadata for context, and ensuring data quality. Due to the nature of this role, it will be important for this Data Engineer to have experience working with life-science datasets.
Requirements for Data Engineer
3+ Years as a Data Engineer
Experience working with scientific data
Experience with life sciences lab instruments
Python
PostgreSQL
Data warehousing
Prime Team Partners is an equal opportunity employer. Prime Team Partners does not discriminate on the basis of race, color, religion, national origin, pregnancy status, gender, age, marital status, disability, medical condition, sexual orientation, or any other characteristics protected by applicable state or federal civil rights laws.
Show more
Show less","Data Engineer, Python, PostgreSQL, Data warehousing, Scientific data, Life sciences lab instruments","data engineer, python, postgresql, data warehousing, scientific data, life sciences lab instruments","dataengineering, datawarehouse, life sciences lab instruments, postgresql, python, scientific data"
Senior Data Engineer,Nayya,"New York, NY",https://www.linkedin.com/jobs/view/senior-data-engineer-at-nayya-3748332832,2023-12-17,Dauphin, Canada,Mid senior,Hybrid,"About Nayya
At Nayya, we believe there’s a better way to choose benefits. A more transparent, less confusing way for employees to control their health and financial potential. Powered by billions of data points and machine learning, our benefits experience platform delivers personalized decision support and guidance during open enrollment, new employee onboarding, qualifying life events, and in the moments that matter all year round. This is one of the most stressful and challenging situations consumers face – and we see that as an opportunity to build an innovative response that can help millions of Americans possess the control and understanding they deserve.
Your Role
Work directly with the Nayya health data infrastructure team to power the claims data extraction and integration platform. In addition to maintaining and working on claims file feeds and API integrations, you will join the team as we build a high volume, batch file claims data pipeline.
The team maintains an internal facing API to power end user applications that leverage claims to enhance user experiences. You will help with maintaining that infrastructure, building new endpoints to power new use cases as well as working on an emerging, centralized data strategy for the organization. We will be working as a team to enhance data validation, governance, and access to organizational data from a single source of truth while building new infrastructure to exceed product requirements for our 2024 product goals and beyond. This is an excellent opportunity to get hands-on experience solidifying and scaling data infrastructure at a fast-growing startup.
Responsibilities
Build and maintain our Interoperability API and claims file feed integrations.
Help design and implement new batch processing infrastructure.
Enhance our data enrichment service to interface with external/third party data sources.
Help design and implement a de-identified reporting and analytics platform.
Maintain and improve an existing Python tech stack with a focus on security and scalability for data storage and API endpoints.
Work with the internal stakeholders to build and maintain data and pipelines for various use cases.
Recommend monitoring and analytics tools to automate common data needs and visibility.
Requirements
Strong SQL skills including query performance and optimization techniques.
Experience building and maintaining APIs to serve data to internal stakeholders.
Familiarity with common warehousing and data lake services.
Data pipeline development and maintenance experience.
Familiarity with batch processing pipelines.
Familiarity with data pipeline tuning and performance testing.
Familiarity with building data processing infrastructure from the ground up.
Familiarity with one or more RDBMS.
Ability to identify tradeoffs for warehousing vs data lake infrastructure and applying solutions to the appropriate use case.
Familiar with common pitfalls in high volume, partitioned data ingestion pipelines such as orphaned records and table locks.
Nice To Haves
Experience with Terraform.
Experience with data streaming pipelines and sources.
Experience building and maintaining APIs (Flask, FastAPI, etc).
Experience with AWS managed services and data engineering solutions.
Experience with other languages such as Go, Ruby, and Javascript.
The salary range for New York based candidates for this role is $140,000-$175,000. We use a location factor to adjust this range for candidates that are located outside of geographic region of our New York office. Placement within the salary band is determined based on experience.
#BI-Hybrid
Why Join Nayya?
Be an early employee of a quickly growing, VC-backed start-up - grow with us!
Have a meaningful impact on a platform that is scaling very rapidly
Contribute to a values-based culture with an emphasis on empowerment and autonomy
Work in a highly collaborative, cross-functional environment
Benefits include: Competitive pay, employer-paid healthcare, stock options
Nayya is proud to be an Equal Employment Opportunity employer. We do not discriminate based upon race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics
Show more
Show less","SQL, Python, RDBMS, API, Data warehousing, Data lake, Data pipeline, Batch processing, Terraform, AWS, Go, Ruby, Javascript, Flask, FastAPI","sql, python, rdbms, api, data warehousing, data lake, data pipeline, batch processing, terraform, aws, go, ruby, javascript, flask, fastapi","api, aws, batch processing, data lake, data pipeline, datawarehouse, fastapi, flask, go, javascript, python, rdbms, ruby, sql, terraform"
Senior Data Engineer,Oakridge Staffing,"New York, NY",https://www.linkedin.com/jobs/view/senior-data-engineer-at-oakridge-staffing-3775558977,2023-12-17,Dauphin, Canada,Mid senior,Hybrid,"Expanding buy side firm is building out their data function and team to partner with the business. The greenfield platform will be built on AWS. The data pipelines will be built for portfolio teams, data scientist, ect... There is a large appetite for alternative data sets.
Python libraries
AWS
Airflow
Continuous Integration/ Continuous Deployment
snowflake
Show more
Show less","Python, AWS, Airflow, Continuous Integration, Continuous Deployment, Snowflake, Portfolio management, Data science","python, aws, airflow, continuous integration, continuous deployment, snowflake, portfolio management, data science","airflow, aws, continuous deployment, continuous integration, data science, portfolio management, python, snowflake"
Data Engineer,Brooksource,"Denver, CO",https://www.linkedin.com/jobs/view/data-engineer-at-brooksource-3770189346,2023-12-17,Dauphin, Canada,Mid senior,Hybrid,"Title: ETL/Data Engineer
Location: Denver Tech Center in Greenwood Village, CO (3 days onsite/2 days remote)
Compensation: $75/hr
Duration: 12-24month Contract to Hire
ETL/Data Engineer
We are on the lookout for a skilled Data Engineer to join a team being assembled from the ground up. As part of an analytics team, you will collaborate closely with stakeholders, including a Tableau Developer, to support customer-facing applications like a News & Sports app, agent-facing desktop portal, and intranet portal. In this role, you'll be responsible for designing and maintaining ETL pipelines, ensuring seamless data flow and integration for the analytics team. The goal is to enhance both internal and external customer experiences through detailed analysis, cross-team collaboration, and the delivery of reliable data solutions.
Responsibilities
Collaborate with stakeholders to gather and execute on requirements and construct ETL pipelines for data processing
Devote attention to providing actionable insights, robust metrics, and continuous improvement in quality, processes, visualization strategy, and agility
Participate in ongoing reviews of requirements, roadmaps, and contribute to the development of optimization strategies
Effectively communicate findings to team members and stakeholders
Translate feedback and action items from meetings into actionable tickets and team notes
Ensure data integrity during the release of new product features through thorough testing and monitoring alongside test groups pre and post-launch
Required Technical Skills
Proficient in building, troubleshooting, and optimizing ETL pipelines
Strong expertise in SQL, including a deep understanding of SQL fundamentals and the ability to write and interpret complex queries
Experience with Python to automate ETL jobs
Data visualization skills are a nice to have
Nice to Have: Familiarity with AWS, Gitlab, Kibana, YAML, Spark
Critical to Success
Demonstrate curiosity and proactivity in investigating issues and opportunities, encouraging collaboration and knowledge sharing within the team
Challenge norms in a positive manner to promote conversation and collaboration for better solutions
Approach complex issues with an open mind, iterating towards the simplest and most effective solution
Effectively manage time and expectations, communicating complex concepts in a way that engages non-technical stakeholders.
Brooksource provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, national origin, age, sex, citizenship, disability, genetic information, gender, sexual orientation, gender identity, marital status, amnesty, or status as a covered veteran in accordance with applicable federal, state, and local laws.
Show more
Show less","ETL, Data Engineering, SQL, Python, Data Visualization, AWS, Gitlab, Kibana, YAML, Spark, Tableau","etl, data engineering, sql, python, data visualization, aws, gitlab, kibana, yaml, spark, tableau","aws, data engineering, etl, gitlab, kibana, python, spark, sql, tableau, visualization, yaml"
Data Engineer,Red Oak Technologies,"Sunnyvale, CA",https://www.linkedin.com/jobs/view/data-engineer-at-red-oak-technologies-3774274989,2023-12-17,Dauphin, Canada,Mid senior,Hybrid,"Red Oak Technologies is a leading provider of comprehensive resourcing solutions across a variety of industries and sectors including IT, Marketing, Finance, Business Operations, Manufacturing and Engineering. We specialize in quickly acquiring and efficiently matching top-tier professional talent with clients in immediate need of highly skilled contract, permanent or project management based resources.
“NOTE: If selected for this position, you are required to perform ALL work onsite, based on the client’s specified hybrid work schedule (currently onsite 3 days a week: Tuesday, Wednesday and Thursday).”
Red Oak Technologies in looking for a Data Engineer
for our high-tech giant
client located in Sunnyvale, CA for a
Contract
position.
Qualifications:
5-10 years of strong Python development and software design patterns experience
2-5 years of Data Engineering experience building data pipelines and be able to deploy them
2-5 years of experience writing complex SQL queries
2-5 years of developing Rest API’s and Git
Experience with MongoDB is preferred but will consider experience with any SQL Database
Familiarity with Snowflake , Data Lake concepts , Kubernetes , Docker and Tableau , CLI and Shell Scripting
Nice to have:
Big Data
Dremio/Presto/Trino platforms
-------------------------------------------------------------------------------------------------------------
Red Oak Technologies is a leading provider of comprehensive resourcing solutions across a variety of industries and sectors including IT, Marketing, Finance, Business Operations, Manufacturing and Engineering. We specialize in quickly acquiring and efficiently matching top-tier professional talent with clients in immediate need of highly skilled contract, permanent or project management based resources.
Red Oak Technologies is made up of people from a wide variety of backgrounds and lifestyles. We embrace diversity and invite applications from people of all walks of life. See what it’s like to be at the top; connect with one of our recruiters and apply today.
Let us help you find your next career opportunity! JOIN RED OAK TECHNOLOGIES!
Learn what it’s like to be a Red Oak Consultant!
Red Oak Tech: Quality | Talent | Integrity
Show more
Show less","Python, Data Engineering, SQL, Rest API, Git, MongoDB, Snowflake, Data Lake, Kubernetes, Docker, Tableau, CLI, Shell Scripting, Big Data, Dremio/Presto/Trino","python, data engineering, sql, rest api, git, mongodb, snowflake, data lake, kubernetes, docker, tableau, cli, shell scripting, big data, dremioprestotrino","big data, cli, data engineering, data lake, docker, dremioprestotrino, git, kubernetes, mongodb, python, rest api, shell scripting, snowflake, sql, tableau"
Senior Data Engineer,Russell Tobin,"San Jose, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-russell-tobin-3778951596,2023-12-17,Dauphin, Canada,Mid senior,Hybrid,"6+ Month Contract
Pay $95-105/ HR
Location: San Jose, California, Hybrid Schedule
Technical Skills:
• 8+ Years of Data Engineering Exp
• Databricks Exp
o Python is a PLUS
• Great SQL Coding Exp
o Good Data Modeling Exp
Soft Skills (Very Important):
• Great Communication
• Problem Solving Skills
o Ability to think on their own
o Be given a project and run with it.
Our team is looking for a data engineer to build the foundational data sets and analytical reports for the B2B customer journey analytics. The data sets and reports will provide quantitative and qualitative analysis for acquisition, engagement, and retention. In addition, the candidate will contribute on cross-functional projects to help business achieve its potential in terms of revenue, customer success, and operational excellence.
For this role, we are looking for an ambitious and driven individual - an analytical wiz with outstanding communication skills, high initiative, and strong leadership potential.
What you'll Do
Analyze the business needs, profile large data sets, design, develop and tune data products on big data platforms (Hadoop, Databricks) with emphasis on data quality and performance.
Build foundational data sets and reporting solutions and make them available for consumption through dash boards and ad hoc reports.
Develop and extend data models, processes, standards, frameworks, and reusable components for various data engineering functions/areas.
Transform and migrate on-prem hosted data applications to cloud platform.
Collaborate with key partners including product marketing teams, sales and support teams, engineering leads, architects, BSA's & program managers.
Show more
Show less","Data Engineering, Databricks, Python, SQL, Data Modeling, Hadoop, Data Quality, Performance, Dashboards, Data Models, Data Processes, Data Standards, Data Frameworks, Reusable Components, Cloud Platform, Product Marketing, Sales, Support, Engineering, Architects, Business Systems Analysts, Program Managers","data engineering, databricks, python, sql, data modeling, hadoop, data quality, performance, dashboards, data models, data processes, data standards, data frameworks, reusable components, cloud platform, product marketing, sales, support, engineering, architects, business systems analysts, program managers","architects, business systems analysts, cloud platform, dashboard, data engineering, data frameworks, data models, data processes, data quality, data standards, databricks, datamodeling, engineering, hadoop, performance, product marketing, program managers, python, reusable components, sales, sql, support"
Senior Data Engineer,CSG Talent,"Boston, MA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-csg-talent-3783582961,2023-12-17,Dauphin, Canada,Mid senior,Hybrid,"Senior Cloud Data Engineer
Data Lake, AWS, Big Data Platforms, Programming (Golang/Python/C++/Typescript)
Boston, Massachusetts - 3 days a week in the office
Up to $150,000
Working with a Start-up that is developing groundbreaking RADAR solutions for Autonomous Vehicles, making self-driving more reliable in harsh and tricky conditions. They are growing, and now looking for a Senior Cloud Data Engineer to join the team.
What will you be doing:
You will be responsible for designing, developing, deploying, and maintaining the data infrastructure, including a distributed big data platform, data lake on top of the cloud storage system (S3), metadata systems, data catalogs, data governance, data search, and discovery and related services.
Additionally, you will take full ownership of the data pipeline for my client's next-generation HD-MAP data process.
Key Requirements:
Experienced and proven track record working on multiple cloud platforms (AWS, GCP, Azure, etc.)
Worked on business-critical and high traffic applications.
Track record of scaling ETL on big data platforms
Well versed in working with varied forms of Data Infrastructure
Strong understanding of different Data Sources, been involved in design and able to suggest best practices.
Solid programming skills with one of the following languages (Golang, Python, C++, Typescript, etc...)
Experience using Docker and Kubernetes
Nice to haves:
Experience working with Apache Atlas
Knowledge of Map related technologies
Experience with Apache Kafka/Pulsar technologies
Experience with relational databases, especially Postgres
Experience with deploying Machine Learning or Mapping Algorithms
If this sounds like something you want to learn more about, please apply.
Show more
Show less","Data Lake, AWS, Big Data Platforms, Go, Python, C++, TypeScript, Docker, Kubernetes, Apache Atlas, Map Technology, Apache Kafka, Pulsar, Postgres, Machine Learning, Mapping Algorithms","data lake, aws, big data platforms, go, python, c, typescript, docker, kubernetes, apache atlas, map technology, apache kafka, pulsar, postgres, machine learning, mapping algorithms","apache atlas, apache kafka, aws, big data platforms, c, data lake, docker, go, kubernetes, machine learning, map technology, mapping algorithms, postgres, pulsar, python, typescript"
Data Engineer,Darwill,"Hillside, IL",https://www.linkedin.com/jobs/view/data-engineer-at-darwill-3743017137,2023-12-17,Dauphin, Canada,Mid senior,Hybrid,"Overview: Darwill is a nationally recognized print and marketing communications firm based in the west suburbs of Chicago. As a premier provider of complex marketing products, including direct mail, employee communications and marketing collateral, we influence excellent results for CMO’s, Directors of Marketing and Print Production Professionals by providing ideas, workflow solutions, cutting edge production technologies and a seamless execution process. Our diverse product offering includes data acquisition, email appends, integrated marketing services, production workflows, custom print production, direct mail solutions, fulfillment and complete lettershop, bindery and mailing services.
Location: Oakbrook, IL (WFH Hybrid Eligible)
Reports to: Sr. Manager, Data Engineering
Responsibilities/Essential Functions:
Darwill is growing and we are looking to add a Data Engineer to our existing Data Engineering Practice. The Data Engineer will work as part of a collaborative team to build and maintain scalable data platforms that support data-informed decision-making across the organization. This role will have significant impact as Darwill modernizes its data platform to a best-in-breed solution.
Build and maintain data infrastructure and manage data storage and use.
Build data expertise and own data quality across organization.
Design, build, and launch new data models and efficient data pipelines (including ETL processes) in production.
Partner with cross-functional teams of Account Executives, Data Scientists, and Software Engineers to understand data needs and deliver on those needs.
Build and maintain reporting dashboards using BI tools like Tableau.
Support data processing and reporting at scale on a cloud-based platform.
Triage mission-critical issues and drive to resolution.
Support existing processes running in production.
Qualifications:
Master of Science in Computer Science, Data Science, or other technical field
5 years of hands-on engineering experience, with relevant certifications, accepted in lieu of degree.
5+ years’ experience working in SQL and relational database management systems.
5+ years’ experience with dimensional data modeling, schema design, and data warehousing.
3+ years’ experience in ETL design, implementation, and maintenance.
3+ years’ experience with AWS.
3+ years’ experience with Spark/Databricks in a production environment.
Ability to write well-abstracted, reusable code.
Ability to analyze data to identify deliverables, gaps, and inconsistencies.
Ability to decompose and solve data problems and to find answers on own.
Very strong communication skills and ability to work collaboratively with peers and stakeholders.
Strong leadership skills. DE are expected to be able to work independently and in collaboration with technical and non-technical roles. You will have the opportunity to lead projects independently.
Preferred Qualifications:
Experience in a Direct Marketing/Measurement company.
Databricks Certified Data Engineer and/or Certified Machine Learning credentials.
AWS Big Data/Analytics and/or Machine Learning Certifications.
Experience developing and/or maintaining feature stores in support of machine learning model development.
Consulting or start-up experience.
Experience in shipping production data products.
Work Environment/Physical Demands:
This job requires you to sit in a cubicle or office, on a computer (1 or more monitors) for the majority of the day.
May be required to work extended/evening hours as needed.
This job is work-from-home eligible-hybrid
Show more
Show less","SQL, Data Platforms, Data Infrastructure, Data Quality, Data Modeling, Data Pipelines, ETL Processes, Data Warehousing, AWS, Spark, Databricks, Tableau, Cloudbased Platforms, Machine Learning, Big Data, Analytics","sql, data platforms, data infrastructure, data quality, data modeling, data pipelines, etl processes, data warehousing, aws, spark, databricks, tableau, cloudbased platforms, machine learning, big data, analytics","analytics, aws, big data, cloudbased platforms, data infrastructure, data platforms, data quality, databricks, datamodeling, datapipeline, datawarehouse, etl, machine learning, spark, sql, tableau"
Senior Data Engineer,Formula.Monks,"Cupertino, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-formula-monks-3742635443,2023-12-17,Dauphin, Canada,Mid senior,Hybrid,"Senior Data Engineer
Location:
Cupertino, CA (hybrid-on site 3 days per week)
Job Type:
This is a Full-Time W2 position with us here at Formula.Monks where you will be working as a consultant at this top FAANG company. Since you’re a full time-employee, you will receive an annual salary, full benefits, PTO, etc. This position is on a high impact team working on a long-term engagement. You are also able to explore employment opportunities at this tech company while working via Formula.Monks!
Key Qualifications:
6+ years of hands-on data modeling and data engineering experience
Strong expertise in dimensional modeling and data warehousing
Design and development experience with Cloud Data warehouses like Snowflake, Redshift, BigQuery etc
Hands on experience with Big-Data platforms like Spark, Dremio, Hadoop, Map Reduce, Hive etc
Proficiency in design and development of custom ETL pipelines using SQL and scripting languages (Python/Shell/ Golang) and workflow management tools like Airflow
Proficiency in advanced SQL, performance tuning
Experience with cloud computing platforms like AWS, Google Cloud
Experience with version control systems, CI/CD practices, testing, and migration tools for database and software
Experience working with APIs
Experience with modern data architecture like datalake house is a big plus
Experience with real time data processing using Apache Kafka or Spark Streaming is a big plus
Ability to learn and adapt to new tools and technologies
Analytical and mathematical mind, capable of evaluating and solving various complex problems
Ability to work individually or as part of a team
Ability to learn quickly in a fast paced environment
Excellent oral and written communication skills
Description:
You will architect, develop, and test large scale data solutions, to provide efficient analytical and reporting capabilities across our global and regional sales and finance teams.
You will develop highly scalable data pipelines to load data from various source systems, use Apache Airflow to orchestrate, schedule and monitor the workflows.
Build generic and reusable solutions that can scale and utilize various technologies and frameworks to solve our complex business requirements.
You will be required to understand existing solutions, fine-tune them and support them as needed.
Data quality is our goal and we expect you to meet our high standards on data and software quality.
We are a rapidly growing team with plenty of interesting technical and business challenges to solve.
We seek a self-starter, who is willing to learn fast, adapt well to changing requirements and work with cross functional teams.
Show more
Show less","Data modeling, Data warehousing, Cloud Data warehouses, BigData platforms, Spark, Dremio, Hadoop, Map Reduce, Hive, ETL pipelines, SQL, Airflow, Advanced SQL, AWS, Google Cloud, CI/CD, Real time data processing, Apache Kafka, Spark Streaming, Databricks, Apache Parquet, Linux, Git, Python, Shell, Golang, Datalake house, Data quality","data modeling, data warehousing, cloud data warehouses, bigdata platforms, spark, dremio, hadoop, map reduce, hive, etl pipelines, sql, airflow, advanced sql, aws, google cloud, cicd, real time data processing, apache kafka, spark streaming, databricks, apache parquet, linux, git, python, shell, golang, datalake house, data quality","advanced sql, airflow, apache kafka, apache parquet, aws, bigdata platforms, cicd, cloud data warehouses, data quality, databricks, datalake house, datamodeling, datawarehouse, dremio, etl pipelines, git, golang, google cloud, hadoop, hive, linux, map reduce, python, real time data processing, shell, spark, spark streaming, sql"
Data Engineer,INSPYR Solutions,"Lehi, UT",https://www.linkedin.com/jobs/view/data-engineer-at-inspyr-solutions-3773558985,2023-12-17,Dauphin, Canada,Mid senior,Hybrid,"Title:
Senior Software Developer
Location:
Lehi, UT Hybrid work. On site 3 days per week.
Candidates must be local to be considered.
Compensation:
$95K to $100K
Work Requirements:
US Citizen, Green Card Holders, or Authorized to Work in the U.S.
Job Description
As a senior software developer, you will be responsible for gathering and analyzing requirements for new enhancement. Coordinate with other teams and take full ownership of enhancement till smooth transition into production including documentation. Ensure any problem or issue is reported needs to be understood, recreated, and provide solution in give time with minimum supervision. Need to follow all best practices and standards of software development. Expecting continues improvement in coding skills with new technologies. Participate in meetings and discussions related to projects.
Job Statement
As a skilled Data Engineer, your role encompasses a solid understanding of data science principles and will be responsible for building and maintaining an efficient data architecture that supports advanced data analysis and data science projects that enhance Nok Products and Business operations.
Essential Functions
Collect, store, and process large volumes of data using AWS tools like Kinesis, Glue, or Data Pipeline to collect, store, and process large volumes of data.
Experience with design and manage large datasets, finding pattern/anomalies, data quality (EDA), etc. for data analysis using AWS services such as Amazon S3, RDS, and Redshift.
Develop, Train, and Deploy machine learning models with ML Ops E.g., classification, regression, clustering, NLP, CNN etc. using AWS SageMaker or related services.
Implement analytical algorithms, working on predictive and prescriptive modeling.
Create visualizations and dashboards using tools like Amazon Quicksight or integration with tools like Tableau, PowerBI.
Creating DB objects e.g., Tables, procedure, triggers, functions etc.
Write complex SQL queries and performance tuning/query optimization.
Extract data from SQL/NoSQL databases to generate ad hoc reports for analysis.
Script & write programs with Python and R
Document procedures, workflows, and best practices related to data processing and modeling.
Skillset / Experience
JOB SPECIFICATIONS:
5+ years of experience as a Data Engineer working with large datasets, data analysis with an understanding of data science methodologies.
3+ years of experience with SQL based database technologies (Oracle, SQL, MySQL, Postgres) including store procedure, functions, triggers etc.
3+ years of experience with data visualization tools like Power BI, AWS QuickSight etc.
3+ years of experience with cloud services (preferably AWS), including SageMaker, S3, EC2, EMR, RDS, Airflow and Redshift.
Familiarity with big data tools like Hadoop, Spark, Kafka, etc.
Strong programming skills in languages such as Python, Java, or Scala.
Experience with machine learning frameworks (like TensorFlow or PyTorch) and ETL tools is a plus.
Excellent communication and teamwork skills.
Our benefits package includes:
(EXCLUDE on perm placements)
Comprehensive medical benefits
Competitive pay, 401(k)
Retirement plan
…and much more!
About INSPYR Solutions
Technology is our focus and quality is our commitment. As a national expert in delivering flexible technology and talent solutions, we strategically align industry and technical expertise with our clients’ business objectives and cultural needs. Our solutions are tailored to each client and include a wide variety of professional services, project, and talent solutions. By always striving for excellence and focusing on the human aspect of our business, we work seamlessly with our talent and clients to match the right solutions to the right opportunities. Learn more about us at inspyrsolutions.com.
INSPYR Solutions provides Equal Employment Opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability, or genetics. In addition to federal law requirements, INSPYR Solutions complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities.
Show more
Show less","AWS, Kinesis, Glue, Data Pipeline, Amazon S3, RDS, Redshift, SageMaker, Tableau, PowerBI, Python, R, SQL, Oracle, MySQL, Postgres, Hadoop, Spark, Kafka, TensorFlow, PyTorch, ETL","aws, kinesis, glue, data pipeline, amazon s3, rds, redshift, sagemaker, tableau, powerbi, python, r, sql, oracle, mysql, postgres, hadoop, spark, kafka, tensorflow, pytorch, etl","amazon s3, aws, data pipeline, etl, glue, hadoop, kafka, kinesis, mysql, oracle, postgres, powerbi, python, pytorch, r, rds, redshift, sagemaker, spark, sql, tableau, tensorflow"
Data Engineer,Intelliswift Software,"Portland, OR",https://www.linkedin.com/jobs/view/data-engineer-at-intelliswift-software-3782268010,2023-12-17,Dauphin, Canada,Mid senior,Hybrid,"Intelliswift Job Ad Template
Job Title: Data Engineer
Duration: 12+ Months
Location: Portland, OR
Intelliswift Software Inc. conceptualizes, builds, and supports the world's most amazing technology products and solutions. Our team of rich experts from diverse backgrounds contributes to making Intelliswift one of the most reliable partners in IT and Talent solutions. We specialize in delivering world-class Digital Product Engineering, Data Management and Analytics, and Staffing Solutions services to Fortune companies, SMBs, ISVs, and fast-growing startups.
Job Description:
Develop ETL processes to move third-party data into Clients data warehouse, including housing and rental supply, census and demographics, legislation that may/does impact Client, and social listening.
Implement a refresh cadence for each source based on value and how often it’s updated.
Design, implement, and maintain data pipelines to connect Clients internal and external data.
This foundational data set drives Client models that predict where legislative challenges/opportunities are likely to arise and the Clients users most likely to engage politically.
Partner with E&I and DS to build data visualizations that enable service reporting on clients legislative and regulatory landscape, incorporating available internal data and new external data sources as ETL processes are implemented.
Support the launch and optimization of Iterable (Email marketing automation platform) that Public Policy is using to engage legislators, journalists, partners, and Hosts by moving audience data into Iterable for automated communications and extracting engagement events and marketing campaign metadata to support data visualization.
Implement the integration of regulatory product and compliance data from clients data warehouse -> the Policy Cloud (Salesforce CRM) where the Public Policy team manages legislative and regulatory efforts.
Maintain and expand upon data pipelines that move structured internal clients data into the Policy Cloud including host, listing, regulatory, compliance, and business value.
Skills:
Expertise in data and analytics engineering/data architecture.
Expertise in Python and SQL. Expertise in R is a plus.
Experience leveraging disparate data sets, in particular legislative/regulatory/economic/geospatial.
Expertise in transforming data to be leveraged for self-service data visualization resources.
Experience building and implementing Client models is a plus
Talent for breaking down complex technical concepts into common language and acting as a bridge between technical and departmental stakeholders.
Experience working with complex and big data systems across a multitude of relationships and metrics. Ability to apply a creative and nuanced perspective to look beyond common data indicators in order to meet business goals.
Ability to self-serve and take the initiative to find answers to technical questions.
Education:
Bachelor degree in Computer Science or Computer Engineering
Intelliswift is committed to fair and equitable compensation practices. The range provided for this position is determined by various factors including, but not limited to, relevant work experience, skills, certifications, and location. Intelliswift offers medical insurance, dental insurance, vision insurance, supplemental life and AD&D insurance, and any other benefits per state law.
Equal Employment Opportunity Statement
Intelliswift celebrates a diverse and inclusive workforce. We offer equal employment opportunities to all applicants and employees. All qualified applicants will be considered regardless of race, color, sex, gender identity, gender expressions, religion, age, national origin or ancestry, citizenship, physical or mental disability, medical condition, family care status, marital status, domestic partner status, sexual orientation, genetic information, military or veteran status, or any other protected basis under the law.
Americans with Disabilities Act (ADA)
If you require a reasonable accommodation in completing this application, interviewing, completing any pre-employment testing, or otherwise participating in the employee selection process, please contact Intelliswift Human Resources Department
Other Employment Statements
Intelliswift participates in the E-Verify program.
Learn More
For information on Intelliswift Software, Inc., visit our website at www.intelliswift.com.
Show more
Show less","Data Architecture, Data Engineering, ETL, SQL, Python, R, Geospatial Data, Data Visualization, Data Modeling, Client Data Integration, Salesforce, Iterable, Data Pipelines","data architecture, data engineering, etl, sql, python, r, geospatial data, data visualization, data modeling, client data integration, salesforce, iterable, data pipelines","client data integration, data architecture, data engineering, datamodeling, datapipeline, etl, geospatial data, iterable, python, r, salesforce, sql, visualization"
Data Engineer,Marathon TS,"Springfield, VA",https://www.linkedin.com/jobs/view/data-engineer-at-marathon-ts-3772657878,2023-12-17,Dauphin, Canada,Mid senior,Hybrid,"Marathon TS is looking for a Data Engineer to support our end client.
The ideal candidate will provide Data integration and senior level engineering and design services related to data integration in support of an enterprise-level ICAM solutions using Radiant Logic, SailPoint and other appropriate platform, tools, and technologies. The selected candidate will have daily interactions with government clients related to meeting technical requirements, and implementation efforts. Domain knowledge and technical decision-making will have a critical impact on overall project implementation and execution.
This position requires an innovative engineer who is passionate about data & data quality
The ideal candidate will possess data warehousing experience and the ability to develop scalable data pipelines that make data ingestion into an enterprise Identity management solution faster, and more efficient
Capacity to understand specific needs or requirements to apply skills/knowledge
Architect, engineer, and lead data integration efforts to deploy a single ICAM solution/Identity management platform (SailPoint & Radiant Logic) to manage identities for 100K+ users spanning numerous identity sources and platforms.
Evaluate project requirements in terms of compatibility, scope, integration, design, and analysis.
Establishing a detailed program specification through discussion with clients.
Prepare Unified Modeling Language (UML) diagrams to show programmers the software code needed for the application.
Design and prepare Software Design Document (SDD) and the overview of system architecture.
Formulate business logic to meet client requirements and create new algorithms to meet customization in operations and policies.
Develop REST API using Java to integrate and configure connector development framework.
Use XML based connector library to integrate with the technical development project.
Testing sample datasets to check that output from the program works as intended.
Work with customers to determine their functional needs.
Assist in the development of Enterprise policies and directives.
Participate in functional or user acceptance testing to ensure final product meets intent of customer's needs.
Review and document functionality of existing legacy applications.
Create technical specifications and white papers as required.
Required Qualifications
Bachelors degree in IT or related field and nine (9) years or more experience; Masters degree in IT or related field with seven (7) years or more experience; Prefer academic backgrounds in Computer Science, Mathematics, Statistics, or related technical field
3-5 years of SME experience in projects requiring the transforming, integrating, and delivering of structured data to a scalable data warehouse platform.
Must have experience in data engineering, developing and maintaining large-scale data processing systems for preparing structured and unstructured data integration and analytical modeling
Candidate must be analytically strong, and able to develop creative solutions in a rapidly changing environment.
Show more
Show less","Data Integration, Radiant Logic, SailPoint, Unified Modeling Language (UML), Software Design Document (SDD), REST API, Java, XML, Functional Testing, User Acceptance Testing, Technical Specifications, White Papers, Structured Data, Scalable Data Warehouse Platform, Data Engineering, Data Processing Systems, Structured Data Integration, Unstructured Data Integration, Analytical Modeling","data integration, radiant logic, sailpoint, unified modeling language uml, software design document sdd, rest api, java, xml, functional testing, user acceptance testing, technical specifications, white papers, structured data, scalable data warehouse platform, data engineering, data processing systems, structured data integration, unstructured data integration, analytical modeling","analytical modeling, data engineering, data integration, data processing systems, functional testing, java, radiant logic, rest api, sailpoint, scalable data warehouse platform, software design document sdd, structured data, structured data integration, technical specifications, unified modeling language uml, unstructured data integration, user acceptance testing, white papers, xml"
Data Engineer,InterEx Group,"New Jersey, United States",https://www.linkedin.com/jobs/view/data-engineer-at-interex-group-3784288888,2023-12-17,Dauphin, Canada,Mid senior,Hybrid,"EXCITING OPPORTUNITY - FORTUNE 50 ORGANIZATION
Start Date: ASAP
Location: Hybrid 3-4 days per week in New Jersey (Non-negotiable)
Interview Process: 2 rounds
***Data Engineer***
Responsibilities:
• Develop, maintain, and optimize data pipelines to extract, transform, and load large datasets from diverse sources into our data ecosystem.
• Design and implement efficient and scalable data models that align with business requirements, ensuring data integrity and performance.
• Collaborate with cross-functional teams to understand data needs and deliver solutions that meet those requirements.
• Work closely with data scientists, analysts, and software engineers to ensure seamless integration of data solutions into larger systems.
• Identify and resolve data quality issues, ensuring accuracy, reliability, and consistency of the data infrastructure.
• Continuously monitor and improve data pipelines and processes, identifying opportunities for automation and optimization.
• Stay updated with emerging trends, technologies, and best practices in data engineering, data modeling, and backend Java engineering.
• Provide technical guidance and mentorship to junior team members, fostering their growth and development.
Requirements:
• 5+ years of hands-on experience as a Data Engineer, working on complex data projects and implementing data modeling solutions.
• Solid understanding of
SQL
and expertise in working with
relational databases
(e.g., PostgreSQL, MySQL).
• In-depth knowledge of data modeling techniques and experience with
data modeling tools
.
• Proficiency in designing and optimizing data pipelines using
ETL/ELT
frameworks and tools (e.g., Informatica, Apache Spark, Airflow, AWS Glue).
• Working knowledge on
Data warehousing
• Familiarity with
cloud-based data platforms
and services (e.g.,
Snowflake
, AWS, Google Cloud, Azure).
• Experience with version control systems (e.g.,
Git
) and agile software development methodologies.
• Strong communication skills to effectively convey technical concepts to both technical and non-technical stakeholders.
• Excellent problem-solving skills and the ability to work independently and collaboratively in a fast-paced environment.
Show more
Show less","Java, SQL, PostgreSQL, MySQL, Data modeling, ETL/ELT, Informatica, Apache Spark, Airflow, AWS Glue, Data warehousing, Snowflake, Git, Agile","java, sql, postgresql, mysql, data modeling, etlelt, informatica, apache spark, airflow, aws glue, data warehousing, snowflake, git, agile","agile, airflow, apache spark, aws glue, datamodeling, datawarehouse, etlelt, git, informatica, java, mysql, postgresql, snowflake, sql"
Senior Data Engineer,Ascendion,Atlanta Metropolitan Area,https://www.linkedin.com/jobs/view/senior-data-engineer-at-ascendion-3747029340,2023-12-17,Dauphin, Canada,Mid senior,Hybrid,"About Us:
We are seeking a highly skilled and experienced Senior Data Engineer to join our team. As a Senior Data Engineer, you will play a critical role in ensuring that data is collected, stored, and made available for analysis and reporting. If you have a passion for data engineering, a strong technical skillset, and a desire to work in a dynamic and innovative environment, we want to hear from you.
Role Overview:
As a Senior Data Engineer, you will be responsible for designing, developing, and maintaining data models for our Enterprise Data Warehouse. You will work closely with business stakeholders to gather requirements and create optimized data models. Your expertise in ETL development and data integration will ensure that our data warehouse meets the needs of data analysts, data scientists, and business stakeholders. In addition, you will play a leadership role by collaborating with cross-functional teams, mentoring junior data engineers, and conveying technical information to non-technical stakeholders.
Key Responsibilities:
Data Modeling: Design and develop data models for the Enterprise Data Warehouse, optimizing data storage, retrieval, and analytical performance.
ETL Development: Create and maintain ETL processes for data extraction, transformation, and loading from various source systems.
Data Integration: Collaborate with data analysts, data scientists, and business stakeholders to ensure the data warehouse meets their requirements.
Leadership: Work with cross-functional teams, mentor junior data engineers, and communicate technical information to non-technical stakeholders.
Documentation: Create and maintain data architecture assets, including business and functional requirements, data flow, and diagrams.
Performance Tuning: Optimize SQL queries, indexes, and database configurations for improved data warehouse performance.
Version Control: Implement version control for code and track changes for consistency.
Automation: Automate ETL processes and data loading to improve reliability and reduce manual intervention.
Continuous Learning: Stay updated with industry best practices and trends in data engineering, ETL, and SSIS to ensure our processes remain cutting-edge.
Qualifications:
Bachelor's degree or higher in Management Information Systems (MIS), Computer Science (CS), or a related field.
5+ years of experience in relational database management systems.
5+ years of experience with
Microsoft SQL Server and SQL Server Integration Services (SSIS)
.
5+ years of experience in designing, developing, and maintaining Enterprise Data Warehouses.
3+ years of experience in
Microsoft Azure Data Engineering, including Azure Data Factory, Data Lakes, Synapse, and Event Grid
.
Strong understanding of data modeling concepts, including dimensional modeling.
Expertise with
SSIS
and data governance principles.
Proficiency in
SQL
and data extraction, transformation, and querying within a data warehouse environment.
Experience with
Azure DevOps boards, CI/CD
, and code repositories.
If you are a self-motivated, proactive, and detail-oriented data engineer with a strong track record, we encourage you to apply. Join our team and contribute to our mission of turning data into actionable insights.
Show more
Show less","SQL Server, Data Warehousing, Data Extraction, Data Integration, SQL, ETL Development, SSIS, Azure Data Factory, Azure Data Lakes, Azure Synapse, Data Governance, Dimensional Modeling, Data Modeling Concepts, Code Repositories, Azure DevOps Boards, CI/CD, Version Control","sql server, data warehousing, data extraction, data integration, sql, etl development, ssis, azure data factory, azure data lakes, azure synapse, data governance, dimensional modeling, data modeling concepts, code repositories, azure devops boards, cicd, version control","azure data factory, azure data lakes, azure devops boards, azure synapse, cicd, code repositories, data extraction, data governance, data integration, data modeling concepts, datawarehouse, dimensional modeling, etl development, sql, sql server, ssis, version control"
Data Engineer,Planet Technology,"Boston, MA",https://www.linkedin.com/jobs/view/data-engineer-at-planet-technology-3779397827,2023-12-17,Dauphin, Canada,Mid senior,Hybrid,"Data Engineer Role
Perm role - Hybrid - 2 days per week in the office
Boston MA
As a lead Data Engineer you will a critical team member in expanding the organizations digital transformation. As they drive the mission forward you will be responsible for designing and building reliable data models and pipelines to provide large-scale data ingestion, aggregation, analysis and visualization projects for strategy and innovation insights, marketing and operations effectiveness, and outcomes measurement and reporting.
Key Responsibilities:
1) Identify and acquire data from multiple and diverse sources for actionable insights: Web data, Salesforce and other CRM/CDP/IAM platforms, Google Analytics and Google BigQuery, etc.
2) Manage the inflow and governance of that data while building proper data quality detection to identify data issues in the transformation stages and fix problems to meet pipelines/table health SLAs, etc.
3) Deliver timely and quality analytics to internal and external stakeholders
4) Provide oversight/quality control direction to internal and external engineers/analysts, including development of talent, management of key 3rd party development and support services, etc.
Education & Experience:
6+ years in a data analyst, business analytics engineer, business intelligence analyst or data analyst or related role with SQL experience and object oriented programming/scripting such as Python
BS Degree
Technology: Business intelligence, data visualization and dashboarding platforms (e.g., Tableau, Dataiku, Looker, etc.) as well as knowledge of data management and modern data warehouse (Snowflake, Big Query) and ELT data acquisition/model building tools (Rivery, Fivetran)
Dimensional data modeling and schema design, statistics and predictive modeling
Agile and Scrum
job id#609800
Show more
Show less","Data Acquisition, Data Analytics, Data Integration, Data Modeling, Data Pipeline, Data Quality, Data Visualization, Agile, Scrum, Software Development, Python, SQL, Tableau, Snowflake, BigQuery, Rivery, Fivetran, BigQuery","data acquisition, data analytics, data integration, data modeling, data pipeline, data quality, data visualization, agile, scrum, software development, python, sql, tableau, snowflake, bigquery, rivery, fivetran, bigquery","agile, bigquery, data acquisition, data integration, data pipeline, data quality, dataanalytics, datamodeling, fivetran, python, rivery, scrum, snowflake, software development, sql, tableau, visualization"
Senior Data Engineer,Alpine Solutions Group,"Branchville, NJ",https://www.linkedin.com/jobs/view/senior-data-engineer-at-alpine-solutions-group-3769501491,2023-12-17,Dauphin, Canada,Mid senior,Hybrid,"DAY TO DAY:
This large insurance organization with headquarters in NJ and a presence in Charlotte is seeking a Senior Data Engineer to join their IT team. The data engineering team works on a day to day basis with data scientists and data analysts as well as the business. This group receives requirements from product owners and works to first understand those requirements and then focuses on developing the code in a multitude of applications. The Data Engineer will utilize SQL and Python for programming which includes testing and working with the business for sign offs. The Data Engineers also work with ETL tools specifically Informatica.
QUALIFICATIONS:
6+ years of data engineering experience with a focus within data modeling
Extremely hands on with EITHER Databricks OR Azure Data Factory (most elaborate on experience)
ETL OR ELT exposure with expertise in EITHER IICS or Informatica Power Center
Great communication
SQL programming background
Exposure to Cloud platforms (Azure preferred but not required)
Show more
Show less","Data Engineering, Data Modeling, SQL, Python, Unit Testing, Databricks, Azure Data Factory, Informatica, Power Center, Business Communication, Cloud Platforms, Azure","data engineering, data modeling, sql, python, unit testing, databricks, azure data factory, informatica, power center, business communication, cloud platforms, azure","azure, azure data factory, business communication, cloud platforms, data engineering, databricks, datamodeling, informatica, power center, python, sql, unit testing"
Senior Data Engineer,Harvey Nash,"New Haven, CT",https://www.linkedin.com/jobs/view/senior-data-engineer-at-harvey-nash-3780000688,2023-12-17,Dauphin, Canada,Mid senior,Hybrid,"Title: Data Movement Engineer
Job type: contract to hire.
Location: New Haven, CT (Hybrid)
""US citizens and Green Card Holders and those authorized to work in the US are encouraged to apply. We are unable to sponsor H1b candidates at this time”
Core Responsibilities
Complete full life cycle of ETL/ELT development to address business needs or resolve issues including design, mappings, data transformations, scheduling and testing.
Translate data movement requirements into technical ETL design.
Develop ETL/ELT workflows and mappings to extract, transform, and load data into target environments.
Develop data extraction and transmissions to external vendors.
Develop test plans and perform unit testing.
Create supporting documentation for new ETL processes.
Work closely with data analysts to gain understanding of business processes and corporate data.
Determine impacts to data warehouse structures and information flows due to changes in business or technical requirements.
Contribute to architectural decisions to support business processes.
Provide production support for data solutions.
Complete root cause analysis and contribute to remediation planning and implementation.
Perform data quality analysis, report data quality issues and propose solutions for data quality management.
Learn and expand upon internal controls and participate in customer support.
Perform design and code reviews in support of organizational standards.
Mentor less experienced team members.
Prepare effort estimation including researching and estimating costs of software development, unit testing. May provide estimates for upgrades of vendor packages upgrades and integration with existing systems.
On Call and/or after hours work required.
Essential Competencies
Professionalism/Personal Accountability, Collaboration and Teamwork, Communication, Flexible and Adapts to Change, Service to Customers and Clients
Skill Qualifications
Required:
Minimum of 8 years of relevant experience in data warehousing, business intelligence tools, and the analysis of data
Minimum of 5 years of SQL query development, preferably across multiple database management platforms, and working with dimensional and star schema data warehouse implementations
Proficiency in ETL/ELT concepts and tools (Informatica IICS,
Talend, Boomi
and PowerCenter preferred)
Working knowledge of data warehousing concepts, structures and ETL best practices
Experience using query tools (e.g.. AQT, MS Query)
Ability to problem solve using analytical thinking skills
Must work well independently - must be inquisitive and seek answers to complex questions without being prompted
Strong organizational and time management skills
Strong communication skills including verbal and written to communicate effectively with clients and management
Strong project management skills to ensure that projects get done on time and within budget
Effectively participates in teams and moves the team toward completion of goals
Ability to resolve conflicts
Preferred:
Some experience with cloud database technologies preferred
Some experience with data visualization tools (e.g. Tableau) desirable
A reasonable, good faith estimate of the minimum and maximum hourly billing rate on W2 for this position is $50 to $67 with limited benefits.
Show more
Show less","ETL, ELT, SQL, Informatica IICS, Talend, Boomi, PowerCenter, Data warehousing, Dimensional modeling, Star schema, Query tools, AQT, MS Query, Analytical thinking, Problemsolving, Organizational skills, Time management, Communication skills, Project management, Conflict resolution, Cloud database technologies, Data visualization, Tableau","etl, elt, sql, informatica iics, talend, boomi, powercenter, data warehousing, dimensional modeling, star schema, query tools, aqt, ms query, analytical thinking, problemsolving, organizational skills, time management, communication skills, project management, conflict resolution, cloud database technologies, data visualization, tableau","analytical thinking, aqt, boomi, cloud database technologies, communication skills, conflict resolution, datawarehouse, dimensional modeling, elt, etl, informatica iics, ms query, organizational skills, powercenter, problemsolving, project management, query tools, sql, star schema, tableau, talend, time management, visualization"
Senior Data Engineer,Annalect,New York City Metropolitan Area,https://www.linkedin.com/jobs/view/senior-data-engineer-at-annalect-3768711188,2023-12-17,Dauphin, Canada,Mid senior,Hybrid,"Annalect is the Data & Technology arm of Omnicom Media Group.
About Annalect
Annalect’s 2,000+ innovators leverage data and technology to help clients across Omnicom build relationships that matter — whether that means fostering consumers’ trust in brands, building new experiences, or delivering advanced analytics where it’s most needed. Annalect is the driving force behind Omni, Omnicom’s unique open operating system, which works hand-in-hand with clients’ and partners’ data and tools, to orchestrate better marketing outcomes.
Annalect’s unique approach to data and technology – one that relies on transparency, neutrality, and interoperability – allows us to deliver purpose-built and scalable solutions that make data actionable. Our advanced teams of product leaders, data scientists, consultants, and engineers enable us to meet the business goals of our internal and external clients.
Team Overview
The Annalect Engineering team is an engaged and innovative group of technologists that includes backend, frontend, full-stack, and data engineers. It is comprised of remote team members who are highly collaborative and committed to a culture of work/life balance and continual learning and development.
Position Overview
Annalect is currently seeking a data engineer to join our technology team. In this role you will build Annalect products which sit atop cloud-based data infrastructure. We are looking for people who have a shared passion for technology, design & development, data, and fusing these disciplines together to build cool things. In this role, you will work on one or more software and data products in the Annalect Engineering Team. You will participate in technical architecture, design and development of software products as well as research and evaluation of new technical solutions.
Key Responsibilities
Steward data and compute environments to facilitate usage of data assets
Design, build, test and deploy scalable and reusable systems that handle large amounts of data
Manage small team of developers
Perform code reviews and provide leadership and guidance to junior developers
Learn and teach new technologies
Required Skills
BS, MS or PhD in Computer Science, Engineering, or equivalent real-world experience
Significant experience with Python is essential
Experience with big data and/or infrastructure. Bonus for having experience in setting up Petabytes of data so they can be easily accessed. Understanding of data organization, i.e., partitioning, clustering, file sizes, file formats. Data cataloging with Hive/Hive metastore or Glue or something similar.
Experience working with relational databases.
Experience with Hadoop, Hive, Spark, or other data processing tools
Experience building scalable data pipelines (Airflow experience a plus)
Significant experience working with AWS and/or GCP
Proven ability to independently execute projects from concept to implementation to launch and to maintain a live product
Experience designing and managing data flows
Experience designing systems and APIs to integrate data into applications
8+ years of Linux, Bash, Python, and SQL experience
4+ years using Spark and other Hadoop ecosystem software
4+ years using AWS cloud services, esp. EMR, Glue, Athena, and Redshift
4+ years managing team of developers
Passion for Technology: Excitement for new technology, bleeding edge applications, and a positive attitude towards solving real world challenges
Perks of working at Annalect
Culture! We have an incredibly fun, collaborative environment that encourages engagement and work/life balance
Generous PTO that includes vacation days, personal days, a fantastic Summer Friday program and extended time off around the holiday season.
As part of Omnicom, we have the backing and resources of a global billion-dollar company, but also have the flexibility and pace of a “startup” - we move fast, break things, and innovate.
Compensation Range: $120,000 to $150,000 annual. This is the pay range the Company believes it will pay for this position at the time of this posting. Consistent with applicable law, compensation will be determined based on relevant experience, other job-related qualifications/skills, and geographic location (to account for comparative cost of living). The Company reserves the right to modify this pay range at any time. For this role, Benefits include: health/vision/dental insurance, 401(k), Healthcare Flexible Spending Account, Dependence Care Flexible Spending Account, vacation, sick, personal days, paid parental leave, paid medical leave, and STD/LTD insurance benefits.
We are an equal opportunity employer.
Annalect, a division of Omnicom Media Group, reaffirms its commitment to the policy of Equal Employment Opportunity and to carrying out this policy at all of its offices. It shall be the policy of Omnicom Media Group to (1) recruit, select, hire, train, promote, pay, discipline and terminate employees in all job classifications without regard to age, race, color, creed, national origin, citizenship status, alienage, religion, sex, sexual orientation, marital status, veteran status, disability or any other basis upon which discrimination against or harassment of employees or applicants for employment is prohibited under any applicable federal, state or local equal opportunity employment laws and (2) ensure that all personnel actions are administered without discrimination in violation of applicable law.
Show more
Show less","Python, Data architecture, Data engineering, Software development, Data cataloging, Data pipelines, Airflow, AWS, Glue, Athena, Redshift, Linux, Bash, Spark, Hadoop, SQL","python, data architecture, data engineering, software development, data cataloging, data pipelines, airflow, aws, glue, athena, redshift, linux, bash, spark, hadoop, sql","airflow, athena, aws, bash, data architecture, data cataloging, data engineering, datapipeline, glue, hadoop, linux, python, redshift, software development, spark, sql"
Data Engineer,Socium - Teams Done Differently,"Houston, TX",https://www.linkedin.com/jobs/view/data-engineer-at-socium-teams-done-differently-3773594191,2023-12-17,Dauphin, Canada,Mid senior,Hybrid,"DATA ENGINEER - HOUSTON (HYBRID)
Socium are exclusively partnered with one of the largest transportation and shipping businesses in the world who are going through an exciting period of transformation. Over the past six years the company have gone through a sustained period of change to their logistics services, shipping processes and are beginning to implement exceptional technology and data-insight processes to take the business forward. The business has overseen record revenues, have stream-lined operations and delivered change end-to-end.
They are continuously looking to digitalise and optimise their approaches & processes and are looking to recruit a Data Engineer to be a key player in the company’s digital transformation. Including transitioning to a cloud native, automated Data Platform and contributing to long-term, future-proof solutions that are true to our clients long-term vision!
The right candidate will hold the following experience.;
· Strong hands-on experience of working with Python and SQL
· Experience using Databricks and PySpark
· Hands-on knowledge of end-to-end ELT pipelines process (Maintain and Build)
· Knowledge of Azure ADF
· Stakeholder management
The business are offering a salary up-to $115,000 p/annum with an added benefits package. They are based in Houston and are in the office three days a week. There is a sleek agreed two-stage interview process which will include a meet with managers and data teams.
If interested, please apply within or forward your CV to jack.hudgell@socium.team
Show more
Show less","Python, SQL, Databricks, PySpark, ELT, Azure ADF, Stakeholder Management","python, sql, databricks, pyspark, elt, azure adf, stakeholder management","azure adf, databricks, elt, python, spark, sql, stakeholder management"
Data Engineer,Agility Partners,"Columbus, Ohio Metropolitan Area",https://www.linkedin.com/jobs/view/data-engineer-at-agility-partners-3784379084,2023-12-17,Dauphin, Canada,Mid senior,Hybrid,"Agility Partners is seeking qualified applicants to fill an open Sr. Data Engineer position for a Fortune 15 Healthcare company. This is an exciting opportunity to work for a growing, global company that is an essential link that allows healthcare providers to reduce costs, improve operations and increase quality, so that they can offer better care to their patients. You will be working on a team whose focus is on Data & Analytics for Supply Chain.
Develop BQ views per business requirements and best practices. Perform data mapping with source systems.
Ensure on time delivery of project work solve technical issues and provide quick resolution.
Should have advanced SQL programming experience with GCP BQ. Hands on skills with GCP, BigQuery, Airflow, needed.
Work closely with Product Owners on creating estimates/designs and realizing business value
Ensure quality by conducting code review, providing direction to other data engineers
Participate in technical platform strategy as tools, products, and business needs evolve
Define and execute database and data movement standards, design reviews, pipeline CI/CD process, and data container policies to ensure high quality data management
Define how our data analytics and ML/AI capabilities will apply to business needs and result in dependable business solutions
Partner with external consultants, solution providers, and managed services organizations to enable product/solution development as well as meeting documented standards
Interact with multiple organizations to track project progress, identify risks, communicate risks and status to leadership, and to assess potential impacts to the business.
Ensure platforms and tools meet or exceed data security standards, including internal and external audits performed.
Use strong verbal and written communication skills that non-technical business and end-users can understand.
The ideal candidate would bring the following:
5+ years’ experience with data platforms including GCP, Teradata, MySQL and SQL Server, Airflow
Expert working knowledge of SQL and Python
Demonstrated expertise of database design and modeling
Expert knowledge of BI Reporting and Data Discovery tools
Expert knowledge of Cloud technologies
Experience with business-critical applications
Experience on large-scale implementation programs preferred
Experience with SAP, Manhattan SCORE/Warehouse management data highly desired
Excellent written and oral communication skills
Reasons to Love It
This is a great opportunity to join a team that is responsible for creating cutting-edge solutions and creating healthcare systems that improve patient outcomes.
Remote work with a collaborative team!
You will have the ability to shape the future of the products as they expand across the enterprise platforms
Working in an environment where technology and healthcare converge to help solve tough healthcare issues
Medical, Dental and Vision Insurance
401(k)
Paid Holidays
Life Insurance
Disability coverage
Show more
Show less","BigQuery, GCP, Airflow, SQL, Python, Teradata, MySQL, SQL Server, BI Reporting, Data Discovery, Cloud technologies, SAP, Manhattan SCORE, Warehouse management","bigquery, gcp, airflow, sql, python, teradata, mysql, sql server, bi reporting, data discovery, cloud technologies, sap, manhattan score, warehouse management","airflow, bi reporting, bigquery, cloud technologies, data discovery, gcp, manhattan score, mysql, python, sap, sql, sql server, teradata, warehouse management"
Data Engineer,Top Prospect Group,"Westport, CT",https://www.linkedin.com/jobs/view/data-engineer-at-top-prospect-group-3774948142,2023-12-17,Dauphin, Canada,Mid senior,Hybrid,"If you're a Data Engineer that is passionate about designing cutting-edge data solutions a we want you to be a key player in shaping the future of our client's data infrastructure!
FT, direct hire
$100-115K-Benefits
4-5days a week onsite-Westport, CT
Azure, Azure Synapse, Data Warehouse, Azure Data Lake, SQL, Big Data Platforms, ETL, database design, Azure DevOps
The ideal candidate will have a strong background in data architecture, database management, and ETL processes. As a Data Engineer, you will play a crucial role in designing, implementing, and optimizing our data infrastructure to support the organization's data-driven initiatives.
Qualifications/Competencies:
Key Technical Knowledge, Skills, and Abilities:
• Must have experience deploying modern data solutions leveraging components like
Azure functions, Azure Synapse, Azure Data Factory, Data Flows, Azure Data
Lake, Azure SQL.
• Strong level of understanding on Azure Synapse, ADLS, and Azure DevOps.
• Exhibit an understanding of Data Lake architectures, including raw, enriched, and
curated layer concepts, and ETL/ELT operations.
• Exhibit a solid understanding of database design, data warehousing concepts, big
data platforms, and ETL operations.
• Experience working with data integration techniques & self-service data
preparation.
• Experience in requirements analysis, design, and prototyping.
• Experience with DevOps tools like Azure DevOps, Jenkins, Maven etc.
• Experience in building/operating/maintaining fault tolerant and scalable data
processing integrations.
• Demonstrated experience of turning business use cases and requirements into
technical solutions.
• Ability to conduct data profiling, cataloging, and mappings for technical design and
construction of data flows.
• Strong collaboration and experience working with remote teams.
• Strong problem-solving skills with emphasis on optimization data pipelines.
• Showcase excellent communication and presentation skills for effective
collaboration with technical and non-technical stakeholders.
• Strong analytical skills and a drive to learn and master new technologies and
techniques.
• Experience working with third party providers and vendors for critical support
requirements
Education and Experience:
• Demonstrated expertise in Microsoft Azure development.
• 3-5 years of hands-on Data Warehouse architecture and development experience
withing the Microsoft Azure environment.
• Bachelor’s degree with a minimum of 3-5 years of related work experience outside
of educational studies.
Show more
Show less","Azure, Azure Functions, Azure Synapse, Azure Data Factory, Azure Data Flow, Azure Data Lake, SQL, ADLS, Azure Devops, Jenkins, Maven, ETL, ELT, Requirements analysis, Design, Prototyping, DevOps, Data profiling, Data cataloging, Data mappings, Technical design, Data flows, Big data platforms, Data integration techniques, Selfservice data preparation, Data warehousing, Database design, Communication, Presentation","azure, azure functions, azure synapse, azure data factory, azure data flow, azure data lake, sql, adls, azure devops, jenkins, maven, etl, elt, requirements analysis, design, prototyping, devops, data profiling, data cataloging, data mappings, technical design, data flows, big data platforms, data integration techniques, selfservice data preparation, data warehousing, database design, communication, presentation","adls, azure, azure data factory, azure data flow, azure data lake, azure devops, azure functions, azure synapse, big data platforms, communication, data cataloging, data flows, data integration techniques, data mappings, data profiling, database design, datawarehouse, design, devops, elt, etl, jenkins, maven, presentation, prototyping, requirements analysis, selfservice data preparation, sql, technical design"
Data Engineer,Planet Technology,"Boston, MA",https://www.linkedin.com/jobs/view/data-engineer-at-planet-technology-3769550339,2023-12-17,Dauphin, Canada,Mid senior,Hybrid,"Data Engineer
Full Time - Perm
Hybrid in MA (2 to 3 days per week in office)
Data Engineer
This is your chance to join a growing data analytics team. As the client expands its digital transformation they are seeking a new and talented data analyst. You can be part of a team responsible for designing and building reliable data models and pipelines that will be used to provide analysis and visualization strategies and insights.
The ideal candidate will Client is looking for a Sr PM to join an existing and evolving PMO. Will work on a variety of projects but partnering with stakeholders, business leaders and vendors.
Musts:
5+ years of data engineering and analytics experience
Looking for strong experience using Snowflake
Need to be strong with designing and building data models.
Excellent communication skills
Preferences:
Experience with tools Google Analytics 360 and Tableau and SQL
job id #609800
Show more
Show less","Data Engineering, Data Analytics, Snowflake, Data Models, ETL, SQL, Google Analytics 360, Tableau","data engineering, data analytics, snowflake, data models, etl, sql, google analytics 360, tableau","data engineering, data models, dataanalytics, etl, google analytics 360, snowflake, sql, tableau"
Data Engineer,Red Oak Technologies,"Austin, Texas Metropolitan Area",https://www.linkedin.com/jobs/view/data-engineer-at-red-oak-technologies-3774279282,2023-12-17,Dauphin, Canada,Mid senior,Hybrid,"Red Oak Technologies is a leading provider of comprehensive resourcing solutions across a variety of industries and sectors including IT, Marketing, Finance, Business Operations, Manufacturing and Engineering. We specialize in quickly acquiring and efficiently matching top-tier professional talent with clients in immediate need of highly skilled contract, permanent or project management based resources.
“NOTE: If selected for this position, you are required to perform ALL work onsite, based on the client’s specified hybrid work schedule (currently onsite 3 days a week: Tuesday, Wednesday and Thursday).”
Red Oak Technologies in looking for a Data Engineer for our high-tech giant client located in Austin, TX for a Contract position.
Qualifications:
5-10 years of strong Python development and software design patterns experience
2-5 years of Data Engineering experience building data pipelines and be able to deploy them
2-5 years of experience writing complex SQL queries
2-5 years of developing Rest API’s and Git
Experience with MongoDB is preferred but will consider experience with any SQL Database
Familiarity with Snowflake , Data Lake concepts , Kubernetes , Docker and Tableau , CLI and Shell Scripting
Nice to have:
Big Data
Dremio/Presto/Trino platforms
-------------------------------------------------------------------------------------------------------------
Red Oak Technologies is a leading provider of comprehensive resourcing solutions across a variety of industries and sectors including IT, Marketing, Finance, Business Operations, Manufacturing and Engineering. We specialize in quickly acquiring and efficiently matching top-tier professional talent with clients in immediate need of highly skilled contract, permanent or project management based resources.
Red Oak Technologies is made up of people from a wide variety of backgrounds and lifestyles. We embrace diversity and invite applications from people of all walks of life. See what it’s like to be at the top; connect with one of our recruiters and apply today.
Let us help you find your next career opportunity! JOIN RED OAK TECHNOLOGIES!
Learn what it’s like to be a Red Oak Consultant!
Red Oak Tech: Quality | Talent | Integrity
Show more
Show less","Python, Data Engineering, SQL, Rest API, Git, MongoDB, SQL Database, Snowflake, Data Lake, Kubernetes, Docker, Tableau, CLI, Shell Scripting, Big Data, Dremio, Presto, Trino","python, data engineering, sql, rest api, git, mongodb, sql database, snowflake, data lake, kubernetes, docker, tableau, cli, shell scripting, big data, dremio, presto, trino","big data, cli, data engineering, data lake, docker, dremio, git, kubernetes, mongodb, presto, python, rest api, shell scripting, snowflake, sql, sql database, tableau, trino"
Customer Service Representative/Data Analyst/Data Entry Clerk Part Time,Dukeduchessinternational,"Courtenay, British Columbia, Canada",https://ca.linkedin.com/jobs/view/customer-service-representative-data-analyst-data-entry-clerk-part-time-at-dukeduchessinternational-3752434736,2023-12-17,Courtenay, Canada,Mid senior,Onsite,"Summary:
The Data Analyst will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : hiring@jobsai.live
Powered by Webbtree
Show more
Show less","Data analysis, Data mining, Data visualization, SQL, R, Python, Machine learning, Algorithms, A/B testing, Data quality, Data integrity, Data collection, Data cleansing, Data manipulation, Tableau, Power BI, Statistical modeling, Hypothesis testing, ETL","data analysis, data mining, data visualization, sql, r, python, machine learning, algorithms, ab testing, data quality, data integrity, data collection, data cleansing, data manipulation, tableau, power bi, statistical modeling, hypothesis testing, etl","ab testing, algorithms, data collection, data integrity, data manipulation, data mining, data quality, dataanalytics, datacleaning, etl, hypothesis testing, machine learning, powerbi, python, r, sql, statistical modeling, tableau, visualization"
Data Scientist,Parkview Health,Greater Fort Wayne,https://www.linkedin.com/jobs/view/data-scientist-at-parkview-health-3759983385,2023-12-17,Fort Wayne,United States,Associate,Onsite,"Summary:
Performs analysis of complex healthcare and varied data to develop innovations for pressing healthcare delivery challenges through visualization, machine learning, optimization and statistical inference solutions. Analyzes and models structured data and implements algorithms to support analysis using advanced statistical and mathematical methods from statistics, machine learning, data mining, and predictive modeling. Develops descriptive and predictive models, from conception to completion and provides long- term support. Performs exploratory data analysis, generates and tests working hypotheses, and uncovers interesting trends and relationships.
Education:
Master's Degree in Computer Science, Mathematics, Statistics or related quantitative discipline required. Ph.D. preferred.
Experience:
Five or more years relevant quantitative and qualitative analytics experience using industry leading tools like Python or R. Two or more years of healthcare related analytics experience a plus. Two or more years of experience working within EPIC data ecosystem a plus.
Show more
Show less","Python, R, Statistics, Machine learning, Data mining, Predictive modeling, Descriptive modeling, Exploratory data analysis, Hypothesis testing, Trend analysis, Relationship analysis, Advanced statistical methods, Mathematical methods, EPIC data ecosystem","python, r, statistics, machine learning, data mining, predictive modeling, descriptive modeling, exploratory data analysis, hypothesis testing, trend analysis, relationship analysis, advanced statistical methods, mathematical methods, epic data ecosystem","advanced statistical methods, data mining, descriptive modeling, epic data ecosystem, exploratory data analysis, hypothesis testing, machine learning, mathematical methods, predictive modeling, python, r, relationship analysis, statistics, trend analysis"
Business Data Analyst,"Kros-Wise, Inc.","Norco, CA",https://www.linkedin.com/jobs/view/business-data-analyst-at-kros-wise-inc-3671909807,2023-12-17,Fontana,United States,Mid senior,Onsite,"Kros-Wise is looking for a Business Data Analyst to support our Information Technology customer at NSWC Corona.
Responsibilities Include
Conducting inventory of various IT equipment. Organizing data on inventory into comprehensive metrics for presentations to leadership. Perform general administrative duties such as word processing on technical documents and materials using available Windows based software
Receive phone calls, emails, and facsimiles
Perform general administrative and clerical duties which include but are not limited to:
Word Processing duties
Typing memos, letters, and prepare various documents using Microsoft Office Suite
General clerical duties to include forms updates, proofreading documents, and filing
Assist Program Manager with preparation and delivery of monthly deliverables to the customer
Maintain customer deliverable schedule
Other duties as assigned
Position Requirements
Experience Requirements
Must has an Active Secret Clearance
Must have a minimum of one year experience in word processing
Working knowledge of Microsoft Office products, including Outlook, Word, Excel, PowerPoint, is required; knowledge of MS Access is a plus
Knowledge of technical terminology and office terminology, practices
Aptitude to acclimate to new subjects quickly
Educational Requirements
High School/Technical School Diploma or GED certificate
Show more
Show less","Data Analysis, Inventory Management, Technical Writing, Microsoft Office Suite (Word Outlook Excel PowerPoint), Active Secret Clearance, Knowledge of Technical Terminology, Knowledge of Office Terminology and Practices, Aptitude to Quickly Acclimate to New Subjects, High School/Technical School Diploma or GED Certificate","data analysis, inventory management, technical writing, microsoft office suite word outlook excel powerpoint, active secret clearance, knowledge of technical terminology, knowledge of office terminology and practices, aptitude to quickly acclimate to new subjects, high schooltechnical school diploma or ged certificate","active secret clearance, aptitude to quickly acclimate to new subjects, dataanalytics, high schooltechnical school diploma or ged certificate, inventory management, knowledge of office terminology and practices, knowledge of technical terminology, microsoft office suite word outlook excel powerpoint, technical writing"
Healthcare Data Analyst II (Hybrid),IEHP,"Rancho Cucamonga, CA",https://www.linkedin.com/jobs/view/healthcare-data-analyst-ii-hybrid-at-iehp-3786023226,2023-12-17,Fontana,United States,Mid senior,Onsite,"Job Requisition ID:
6747
Position Summary/Position
Under the direction of the Quality Systems Leadership Team, the Healthcare Data Analyst II (HDA II) is responsible for leading various data and analysis efforts for the Health Plan’s Clinical and Provider departments, programs and quality activities. The HDA II must have significant experience in designing relational databases, complex query logic and statistical analysis. In addition to technical data concepts, the HDA II will utilize sound research methods and statistical concepts in designing reliable studies/analyses, and payment algorithms supporting IEHP’s quality programs. The HDA II presents study/analysis results to various audiences throughout the organization, including the leadership team.
Major Functions (Duties And Responsibilities)
Serve as technical lead in analysis and reporting efforts that support the Health Plan’s Clinical and Provider teams, programs and quality activities.
Significant participation in analysis, interpretation, and translating of complex health plan data, issues, trends, and relationships into effective strategies and action plans.
Present findings of analyses to various staff levels including executive team, leadership team and staff levels. Present results in formats that are easy to understand and appropriate to the audience.
Use analytical skills to determine solutions and resolve problems.
Exercises timely and accurate data mining techniques and develops new or modifies existing reports that ensure consistent and congruent conclusions to data analysis.
Applies statistical modeling methodologies when appropriate in the development of health plan, provider, and member analysis.
Develops and prepares reports to support IEHP’s clinical and provider units utilizing available analytics and data mining tools, including but not limited to: Microsoft Applications (including MS Access), SAS, SPSS, Crystal Reports, system-specific reporting tools (e.g., MedHOK, CRM,) or other related reporting software.
Responsible for evaluating report requests to determine requestor’s needs and objectives, identifying correct methodology for extracting data including data sources and criteria, and ensuring the delivered report is accurate, timely, and formatted appropriately.
Responsible for understanding the various data sources available within IEHP for reporting and analysis, as well as a clear understanding of IEHP business operations to be able to appropriately guide the development of quality program activities, including analysis and reporting.
Responsible for managing assigned projects and effectively communicating with the QS leadership team any project deadlines that may be in jeopardy of being met, well in advance of the due date.
Document all analysis / reporting projects following established protocols, ensuring replication of results by an independent reviewer.
Designs and enhances databases to support reporting and analysis needs, including testing, validating, and maintaining data integrity and accuracy, across multiple databases.
Demonstrate a commitment to incorporate LEAN principles into daily work.
Establish and maintain effective working relationships with others throughout the organization.
Supervisory Responsibilities
Leading: Self
Experience Qualifications
3 - 5 years of relevant experience.
Demonstrated experience working with large data sets and relational database structures.
Preferred Experience
Education Qualifications
Master’s degree from an accredited institution required.
In lieu of a Master's degree, a Bachelor's degre from an accredited institution with 6 - 10 years of relevant work experience is required.
Preferred Education
Professional Certification
Professional Licenses
Drivers License Required
No
Knowledge Requirement
Understanding of statistical methods and experience applying statistical methods to answer data questions.
Working knowledge of data analysis, study methodology, and preparing professional documents.
Skills Requirement
Data analysis, data presentation, spread sheet and database capabilities.
Excellent interpersonal/ communication skills.
Proficient in of Microsoft Access, SAS, SSRS or other statistical applications are essential.
Proficiency in Microsoft Excel, Word and Power Point required.
Abilities Requirement
Ability to present reports / data analysis findings in group settings.
Ability to manage multiple projects with time-sensitive deadlines a must.
Strong problem solving capability.
Strong attention to detail.
Commitment to Team Culture
The IEHP Team environment requires a Team Member to participate in the IEHP Team Culture. A Team Member demonstrates support of the Culture by developing professional and effective working relationships that include elements of respect and cooperation with Team Members, Members and associates outside of our organization.
Working Conditions
Physical Requirements
A reasonable salary expectation is between $90,992.10 - $116,016.58, based upon experience and internal equity.
Inland Empire Health Plan (IEHP) is the largest not-for-profit Medi-Cal and Medicare health plan in the Inland Empire. We are also one of the largest employers in the region. With a provider network of more than 6,000 and a team of more than 2,000 employees, IEHP provides quality, accessible healthcare services to more than 1.2 million members. And our mission and core values help guide us in the development of innovative programs and the creation of an award winning workplace. As the healthcare landscape is transformed, we’re ready to make a difference today and in the years to come. Join our Team and Make a Difference with us! IEHP offers a Competitive salary and a benefit package with a value estimated at 35% of the annual salary, including medical, dental, vision, team bonus, and retirement plan.
Show more
Show less","Data Analysis, Statistical Analysis, Relational Databases, Complex Query Logic, Research Methods, Payment Algorithms, Data Mining, Data Interpretation, Data Presentation, Analytic Skills, Report Writing, Microsoft Access, SAS, SPSS, Crystal Reports, MedHOK, CRM, Microsoft Applications, Statistical Modeling, Data Integrity, Data Accuracy, LEAN Principles, Interpersonal/Communication Skills, Spreadsheet Capabilities, Database Capabilities, Microsoft Excel, Microsoft Word, Microsoft PowerPoint, SQL, Data Extraction, Project Management, Problem Solving, Attention to Detail","data analysis, statistical analysis, relational databases, complex query logic, research methods, payment algorithms, data mining, data interpretation, data presentation, analytic skills, report writing, microsoft access, sas, spss, crystal reports, medhok, crm, microsoft applications, statistical modeling, data integrity, data accuracy, lean principles, interpersonalcommunication skills, spreadsheet capabilities, database capabilities, microsoft excel, microsoft word, microsoft powerpoint, sql, data extraction, project management, problem solving, attention to detail","analytic skills, attention to detail, complex query logic, crm, crystal reports, data accuracy, data extraction, data integrity, data interpretation, data mining, data presentation, dataanalytics, database capabilities, interpersonalcommunication skills, lean principles, medhok, microsoft access, microsoft applications, microsoft excel, microsoft powerpoint, microsoft word, payment algorithms, problem solving, project management, relational databases, report writing, research methods, sas, spreadsheet capabilities, spss, sql, statistical analysis, statistical modeling"
Engineer III - Test Data Manager,Central Business Solutions Inc.,"Rancho Cucamonga, CA",https://www.linkedin.com/jobs/view/engineer-iii-test-data-manager-at-central-business-solutions-inc-3581007335,2023-12-17,Fontana,United States,Mid senior,Onsite,"Position Role/Tile: Engineer III - Test Data Manager
Location: Rancho Cucamonga, CA .
Position Summary/Position
The Engineer III - Test Data Management is the lead position assisting the Manager, Technical Test Data & Environment in leading the enterprise-level test data Management (ensuring the protection of PHI data) on several Test environments.
It consists of using procedures, data masking tools, and resources to manage multiple components like software, hardware, test data, and applications.
This position works in collaboration with TEM Engineers to ensure Test Environments are ready with necessary test data for Test execution and Delivery teams on all IT projects.
This is a senior position and trains and guides Engineer I/IIs in the Test Data management area.
Major Functions (Duties And Responsibilities)
Analyze existing Test data in several environments and identify test data and security gaps.
Develop and execute Test Data Management Strategy and plans based on the assessment.
Lead enterprise-level Test Data management across several applications and test environments.
Provision On-demand, self-service test data.
Perform in-flight data masking, also for unstructured data, while ensuring referential integrity.
Set test data based on user-defined criteria on various projects.
Synthesize test data based on production data.
Integrate TDM into CI/CD pipelines with an open API.
Set up integrated test data in multiple test environments of integrated applications.
Use APIs to refresh to the latest data, rewind for cleanup, branch datasets, and version data alongside code.
Research and implement automatic test data generation tools .
Find then mask sensitive data value automatically.
Resolve application team/business team queries related to obfuscated data for all non-prod databases.
Deliver virtual data to environments for development, testing, integration, and UAT automatically.
Store and version control virtual datasets efficiently. Bookmark data and pair with specific test cases.
Create test data generation templates for QE Engineers to generate test data files in several formats (HL7/ CCDA/X12 etc.) based on various test scenarios
Build complex data reconciliation scripts for volume data validation in ETL and Data integration testing processes
Collaborate with Test Environment Management team, Application support team, Product vendors, internal database, and Data integration teams to support the Test Delivery on several projects in a timely manner.
Train and guide Engineer I/IIs in the Test Data management.
Supervisory Responsibilities
Leading: Guides Others
Experience Qualifications
A minimum of eight (8) years of Information systems and Quality Engineering experiences. Minimum five (5) years of experience working in Test Data Management. Demonstrated ability in the creation of data quality standards and methodologies. Demonstrated ability in Design and developing test Data Management Strategies and plans. Experience in data masking and setting up test data in integrated test environments of multiple applications. Experience writing and executing complex SQL query commands. Experience in coordination with external vendor teams. Test Automation and CI/CD experience. Experience in Test Automation tools. Experience communicating with management on topics related to Test Data roadblocks and risks. Strong technical background. A demonstrated track record of organizing test data at various stages of test life cycle and on multiple integrated systems across multiple projects. Proven experience with Data Masking.
Education Qualifications
Bachelor's degree in Computer Science, Information Management or similar technical field from an accredited institution required.
Knowledge Requirement
Thorough understanding of Test Data Management tools
knowledge of TDM, data Obfuscation and Test Lifecycle
knowledge of Agile, SDLC, and ITIL
Understanding of Agile environment and DevOps
Extensive familiarity with quality assurance processes and procedures, and the ability to devise methods to enforce data quality standards
Should have an overall grasp of the work effort involved to set up Test data for end-to-end testing environments.
Skills Requirement
The ability to use Test Data Management tools
Strong planning, organization, critical thinking, decision-making and communication (verbal and written) skills.
Strong interpersonal skills
Robust analytical skills for both existing processes and identifying areas of opportunity
Excellent decision-making, problem-solving, team and time management skills
Good interpersonal, communication and organizational skills
Abilities Requirement
Ability to clearly articulate both problems and proposed solutions
Ability to prioritize personal and team workloads to best meet organizational objectives
Ability to work effectively on a high-performing team
Analytical and process skills
Effective problem resolution
Resiliency and Ability to adapt in a dynamic environment
Present issues and challenges in senior management forums.
Work with a team of professionals from various disciplines.
Express action-oriented and creative approaches to IT system issues and problems.
Ability to lead, guide and train other resources
Motivated by delivery and speed to outcome
Ability to effectively prioritize and execute tasks in a high-pressure environment
Central Business Solutions, Inc,
37600 Central Ct.
Suite #214
Newark, CA 94560.
Central Business Solutions, Inc(A Certified Minority Owned Organization)
Checkout our excellent assessment tool: http://www.skillexam.com/
Checkout our job board : http://www.job-360.net/
=====================================================
Central Business Solutions, Inc
37600 Central Court Suite 214 Newark CA, 94560
Phone: (833)247-8800 Fax: (510)-740-3677
Web: http://www.cbsinfosys.com
=====================================================
Show more
Show less","Test data management, Data masking, Software testing, Quality assurance, Agile development, DevOps, Data integration, ETL testing, SQL programming, API integration, Data Obfuscation, CI/CD pipeline, Test Automation, Analytical skills, Problemsolving skills, Communication skills, Teamwork skills, Leadership skills, Time management skills, Decisionmaking skills, Critical thinking skills, Planning and organization skills","test data management, data masking, software testing, quality assurance, agile development, devops, data integration, etl testing, sql programming, api integration, data obfuscation, cicd pipeline, test automation, analytical skills, problemsolving skills, communication skills, teamwork skills, leadership skills, time management skills, decisionmaking skills, critical thinking skills, planning and organization skills","agile development, analytical skills, api integration, cicd pipeline, communication skills, critical thinking skills, data integration, data masking, data obfuscation, decisionmaking skills, devops, etl testing, leadership skills, planning and organization skills, problemsolving skills, quality assurance, software testing, sql, teamwork skills, test automation, test data management, time management skills"
BI & Data Visualization Developer,Citizens Business Bank,"Rancho Cucamonga, CA",https://www.linkedin.com/jobs/view/bi-data-visualization-developer-at-citizens-business-bank-3666509998,2023-12-17,Fontana,United States,Mid senior,Onsite,"Exact compensation may vary based on skills, experience and/or education, and location. This position is also eligible for an annual bonus.
Summary
This role will enable the company’s use of data as a strategic asset of corporate goals and objectives. The incumbent will be responsible for strategically designing, developing, and implementing enterprise-level BI solutions, data integrations and analytical applications. This role is responsible for understanding and documenting data sources, usage and lineage for enterprise information. The DW-BI & Visualization developer must drive the completion of projects within the established scope, while simultaneously plan for and manage the unknown future of BI requirements in a dynamic environment.
The ideal candidate will have extensive hands on experience as a BI Visualization Developer working with Power BI, creating prototypes, dashboards and data marts.
Essential Duties And Responsibilities
Complies with and stays abreast of all policies and procedures, federal and state laws applicable to the job.
Plays an instrumental role in developing the data warehouse and BI strategy and initiatives.
Analyzes business and technical requirements, translates them to high level design documents.
Design and implement BI tools and processes for data intake from multiple sources.
Establish and maintain data security and compliance, access controls, data governance.
Design and implement user analytical and visualization tools.
Provide support to the BI users on the effective use of reporting tools and data structure
Identify requirements, design, create, deploy, monitor and support BI reports and dashboards, shared object and cubes to empower discovery and information-sharing for business users.
Utilize expertise and knowledge of data modeling, ETL, and report design for department initiatives, and produce detailed documentation including data flow diagrams, logical diagrams, and data lineage documents.
With minimal supervision work on a wide range of projects interfacing with development teams, business analysts, IT management and members of the business units
Other duties as assigned.
Provide, present and promote The Citizens Experience to all external and internal customers.
Supervisory Responsibilities
This position has no supervisory responsibilities.
Qualifications
To perform this job successfully, an individual must be able to perform each essential duty satisfactorily. The requirements listed below are representative of the knowledge, skill, and/or ability required. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.
EDUCATION And/or EXPERIENCE
Bachelor’s Degree (B.A.) from four-year College or university in Computer Sciences, Information Systems, or other related field. 2 – 5 years of experience in Power BI and Power BI Administration (setting up workspaces and users with permissions, roles and security, etc.), SQL Server and Reporting Services SSRS, data warehouse, ETL and BI technologies, and advanced query and SQL skills.
Language Skills
This position requires strong communication skills, both written and oral, with the ability to write reports, business correspondence, and procedure manuals, as well as the ability to effectively present information and respond to questions. Requires the ability to read, analyze, and interpret general business periodicals, professional journals, technical literature, and government regulations.
Mathematical Skills
Ability to work with mathematical concepts such as probability and statistical inference, discounts, interest, commissions, proportions, percentages, area, circumference, and volume. Ability to apply concepts of basic algebra and geometry.
REASONING ABILITY
Strong analytical skills with the ability to solve practical problems and deal with a variety of concrete variables in situations where only limited standardization exists. Ability to interpret a variety of instructions furnished in written, oral, diagram, or schedule form.
Computer Skills
To perform this job successfully, an individual should have the following experience:
2 - 5 years of most recent experience using Data Visualization tool and Microsoft Power BI.
2+ years of experience in MS Excel, Pivot Tables, charts, graphs, dashboards, Power Pivot, Power view, and Power Map.
Proficient in developing reports, dashboards, KPI Scorecards, DAX solutions, and data source connections.
5 years’ experience in developing ETL solutions using SSIS and other SQL Server tools
2 – 5 years’ experience and knowledge of Data Modeling for Business Intelligence and Data Visualization.
Experience and knowledge of working with cloud-based IT infrastructure and platforms, specifically Microsoft Azure Power Platform (Power Bi, Power Automate (Flow), and Power Apps) a plus
Other Qualifications/Skills
Experience in data management through data modeling, data architecture, and data warehouse is desired.
Strong analytical and troubleshooting skills
Understanding of best practices for common ETL design techniques such as change data capture, key generation, and optimization.
Deep understanding of relational and dimensional database domains.
Knowledge of master data management, metadata management, information lifecycle design, and development.
Motivated self-started, detail and quality oriented and ability to handle multiple competing priorities.
Must be a strong team player with a positive attitude and enthusiasm to get things done.
Ability to present to various levels of the organization, including executive management.
Excellent communication (both written and verbal) and interpersonal skills.
PHYSICAL DEMANDS
The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this job. Reasonably accommodations may be made to enable individuals with disabilities to perform the essential functions.
While performing the duties of this job, the employee is regularly required to use hands to finger, handle, or feel; reach with hands and arms; and talk or hear. The employee frequently is required to sit. The employee is occasionally required to stand; walk; and stoop, kneed, crouch, or crawl. The employee must occasionally lift and/or move up to 10 pounds. Specific vision abilities required by this position include close vision, and ability to adjust focus.
WORK ENVIRONMENT
The work environment characteristics described here are representative of those an employee encounters while performing the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.
The noise level in the work environment is usually moderate.
Salary Range: $104,061.00 To 150,888.00 Annually
40 hours per week.
Show more
Show less","Power BI, SQL, Reporting Services, ETL, Data Modeling, Data Visualization, Microsoft Azure Power Platform, Data Mining, Machine Learning, Data Analysis, Business Intelligence, Cloud Computing, SSRS, SSIS, Data Mart, Dashboard, KPI, DAX, Data Architecture, Data Warehouse, Change Data Capture, Relational Database, Dimensional Database, Master Data Management, Metadata Management","power bi, sql, reporting services, etl, data modeling, data visualization, microsoft azure power platform, data mining, machine learning, data analysis, business intelligence, cloud computing, ssrs, ssis, data mart, dashboard, kpi, dax, data architecture, data warehouse, change data capture, relational database, dimensional database, master data management, metadata management","business intelligence, change data capture, cloud computing, dashboard, data architecture, data mart, data mining, dataanalytics, datamodeling, datawarehouse, dax, dimensional database, etl, kpi, machine learning, master data management, metadata management, microsoft azure power platform, powerbi, relational database, reporting services, sql, ssis, ssrs, visualization"
Engineer III - Test Data Management (Hybrid Work Schedule),IEHP,"Rancho Cucamonga, CA",https://www.linkedin.com/jobs/view/engineer-iii-test-data-management-hybrid-work-schedule-at-iehp-3618442946,2023-12-17,Fontana,United States,Mid senior,Onsite,"Job Requisition ID:
7759
Position Summary/Position
The Engineer III - Test Data Management is the lead position assisting the Manager, Technical Test Data & Environment in leading the enterprise-level test data Management (ensuring the protection of PHI data) on several Test environments. It consists of using procedures, data masking tools, and resources to manage multiple components like software, hardware, test data, and applications. This position works in collaboration with TEM Engineers to ensure Test Environments are ready with necessary test data for Test execution and Delivery teams on all IT projects. This is a senior position and trains and guides Engineer I/IIs in the Test Data management area.
Major Functions (Duties And Responsibilities)
Analyze existing Test data in several environments and identify test data and security gaps.
Develop and execute Test Data Management Strategy and plans based on the assessment.
Lead enterprise-level Test Data management across several applications and test environments.
Provision On-demand, self-service test data.
Perform in-flight data masking, also for unstructured data, while ensuring referential integrity.
Set test data based on user-defined criteria on various projects.
Synthesize test data based on production data.
Integrate TDM into CI/CD pipelines with an open API.
Set up integrated test data in multiple test environments of integrated applications.
Use APIs to refresh to the latest data, rewind for cleanup, branch datasets, and version data alongside code.
Research and implement automatic test data generation tools .
Find then mask sensitive data value automatically.
Resolve application team/business team queries related to obfuscated data for all non-prod databases.
Deliver virtual data to environments for development, testing, integration, and UAT automatically.
Store and version control virtual datasets efficiently. Bookmark data and pair with specific test cases.
Create test data generation templates for QE Engineers to generate test data files in several formats (HL7/ CCDA/X12 etc.) based on various test scenarios
Build complex data reconciliation scripts for volume data validation in ETL and Data integration testing processes
Collaborate with Test Environment Management team, Application support team, Product vendors, internal database, and Data integration teams to support the Test Delivery on several projects in a timely manner.
Train and guide Engineer I/IIs in the Test Data management.
Supervisory Responsibilities
Leading: Guides Others
Experience Qualifications
A minimum of eight (8) years of Information systems and Quality Engineering experiences. Minimum five (5) years of experience working in Test Data Management. Demonstrated ability in the creation of data quality standards and methodologies. Demonstrated ability in Design and developing test Data Management Strategies and plans. Experience in data masking and setting up test data in integrated test environments of multiple applications. Experience writing and executing complex SQL query commands. Experience in coordination with external vendor teams. Test Automation and CI/CD experience. Experience in Test Automation tools. Experience communicating with management on topics related to Test Data roadblocks and risks. Strong technical background. A demonstrated track record of organizing test data at various stages of test life cycle and on multiple integrated systems across multiple projects. Proven experience with Data Masking.
Education Qualifications
Bachelor’s degree in Computer Science, Information Management or similar technical field from an accredited institution required.
Drivers License Required
Yes, must have a valid California Driver's License.
Knowledge Requirement
Thorough understanding of Test Data Management tools
knowledge of TDM, data Obfuscation and Test Lifecycle
knowledge of Agile, SDLC, and ITIL
Understanding of Agile environment and DevOps
Extensive familiarity with quality assurance processes and procedures, and the ability to devise methods to enforce data quality standards
Should have an overall grasp of the work effort involved to set up Test data for end-to-end testing environments.
Skills Requirement
The ability to use Test Data Management tools
Strong planning, organization, critical thinking, decision-making and communication (verbal and written) skills.
Strong interpersonal skills
Robust analytical skills for both existing processes and identifying areas of opportunity
Excellent decision-making, problem-solving, team and time management skills
Good interpersonal, communication and organizational skills
Abilities Requirement
Ability to clearly articulate both problems and proposed solutions
Ability to prioritize personal and team workloads to best meet organizational objectives
Ability to work effectively on a high-performing team
Analytical and process skills
Effective problem resolution
Resiliency and Ability to adapt in a dynamic environment
Present issues and challenges in senior management forums.
Work with a team of professionals from various disciplines.
Express action-oriented and creative approaches to IT system issues and problems.
Ability to lead, guide and train other resources
Motivated by delivery and speed to outcome
Ability to effectively prioritize and execute tasks in a high-pressure environment
Commitment to Team Culture
The IEHP Team environment requires a Team Member to participate in the IEHP Team Culture. A Team Member demonstrates support of the Culture by developing professional and effective working relationships that include elements of respect and cooperation with Team Members, Members and associates outside of our organization.
Working Conditions
Makes necessary adjustments to work shift ensure completion of projects and to meet deadlines, e.g., system releases, critical milestones
A reasonable salary expectation is between $103,708.80 - $132,225.60, based upon experience and internal equity
Inland Empire Health Plan (IEHP) is the largest not-for-profit Medi-Cal and Medicare health plan in the Inland Empire. We are also one of the largest employers in the region. With a provider network of more than 7,000 and a team of more than 2,500 employees, IEHP provides quality, accessible healthcare services to more than 1.5 million members. And our Mission, Vision, and Values help guide us in the development of innovative programs and the creation of an award winning workplace. As the healthcare landscape is transformed, we’re ready to make a difference today and in the years to come. Join our Team and make a difference with us! IEHP offers a competitive salary and a benefit package with a value estimated at 35% of the annual salary, including medical, dental, vision, team bonus, and state pension plan.
Show more
Show less","Test Data Management, Data Masking, TDM, CI/CD, SQL, DevOps, Agile, SDLC, ITIL, Test Automation, Data Quality, Test Life Cycle","test data management, data masking, tdm, cicd, sql, devops, agile, sdlc, itil, test automation, data quality, test life cycle","agile, cicd, data masking, data quality, devops, itil, sdlc, sql, tdm, test automation, test data management, test life cycle"
Analyst III - Member Data Configuration (Telecommute),IEHP,"Rancho Cucamonga, CA",https://www.linkedin.com/jobs/view/analyst-iii-member-data-configuration-telecommute-at-iehp-3772433427,2023-12-17,Fontana,United States,Mid senior,Remote,"Job Requisition ID:
10651
Position Summary/Position
Under the direction of the Director of Business Systems & Transformation, the Analyst III - Member Data Configuration is responsible for developing and maintaining the high-level configuration plan for the Member Business Systems and will provide guidance and recommendations based on Member System functionality, regulatory requirements, and have business acumen. The Analyst III - Member Data Configuration will work together with various business units in the organization and participate in projects to evaluate and implement configuration methodologies, leveraging systematic algorithms. The Analyst III - Member Data Configuration will work closely with leadership and business stakeholders to ensure that regulatory and contractual guidelines are met, and use of appropriate configuration standards and automation are developed and utilized in IEHP. The Analyst III - Member Data Configuration is responsible for the successful delivery of membership systems configurations and the optimization of related components and will act as a subject matter expert on technical and business matters and assume a lead role for the Business Systems & Transformation team.
Major Functions (Duties And Responsibilities)
Identify the needs to drive optimization and automation enhancements to membership systems and other enterprise transformation initiatives to meet the needs of IEHP business functionality.
Subject matter expert (SME) for complex Membership System algorithms, business logic, data mapping and process engine outcomes.
Designs, tests, and implements system infrastructure for various Member Eligibility Systems.
Partner with all levels of management, data analysts, application vendors, and other relevant staff to maintain proficiencies for constantly evolving industry standards and practices to deploy and optimize Member System application architectures.
Collaborate with and guide a team of analysts with membership systems configuration design, development, and optimization.
Coordinate and work closely with software vendors and Application Support teams to identify, evaluate, test, implement and tune system enhancements and integrated software.
Design and develop test plans, audits, and batch jobs to ensure quality and process efficiency and lead in their execution and validation, while performing Debugging and Root Cause Analysis on test failures as needed.
Develop and draft Business Requirement Documents (BRDs), Enhancement Requests (ERs), Change Requests (CRs), Business Rule Change Request (BRCRs), and other decision recommendation documents as required.
Responsible to analyze and translate complex business logic, data mapping and business specification requirements into technical specifications and vice versa, based on eligibility data from governing agencies and system functionality and redesign and optimize processes whenever possible to increase efficiency for all lines of businesses.
Maintain a comprehensive understanding of the member data system features and capabilities in order to provide effective support to the organization.
Identify, design, and communicate impact of system enhancements or configuration changes on integrated systems and processes.
Apply LEAN/A3 methodologies to business process and system configuration design and development.
In conjunction with BST (Business Systems & Transformation) Analysts, research complex eligibility issues for Medicare, Medi-Cal and Covered CA, as well as identify new opportunities in the Membership Systems to improve processes and performance, while reducing manual intervention of repetitive tasks.
Partner with IT and external application vendors to apply robotic process automation to redesign processes and automate relevant tasks that meet the needs of business and regulations.
Train, mentor, and guide teammates on optimization and automation capabilities.
Any other duties as required ensuring Health Plan operations are successful.
Major Functions (Duties And Responsibilities) Cont
Supervisory Responsibilities
Leading: Guides Others
Experience Qualifications
A minimum of five (5) years of business systems analysis, configuration and healthcare experience required, including the design, development, validation, and/or implementation of complex systems within a healthcare setting. In addition, candidate must have experience working with Medicare, Medicaid, and Covered CA government programs and regulatory agencies.
Preferred Experience
Education Qualifications
Bachelor’s Degree in Business Administration, Health Care Administration, Computer Information Systems (CIS), Management Information Systems (MIS), or other related field from an accredited institution required.
In lieu of the required degree, a minimum of four (4) years of additional relevant work experience is required for this position: two (2) years of general healthcare experience plus two (2) years of configuration experience is required. This experience is in addition to the minimum years listed in the Experience Requirements above.
Preferred Education
Professional Certification
Professional Licenses
Drivers License Required
No
Knowledge Requirement
Strong understanding of Member system processing, membership configuration, concepts, principles, and best practices. Knowledge of CMS, DHCS, DMHC, NCQA rules and regulations preferred. Extensive knowledge of general managed care operations required. Proficient knowledge of SQL and relational database structure required. Knowledge of 834 5010 file format including codes and guidelines. Knowledge of eligibility file processing for multiple lines of businesses preferred.
Functional knowledge of IEHP membership systems including Core Membership and Eligibility File Processing, IEHP membership databases, and the MHK suite, are highly preferred. In addition, working knowledge of vendor systems including HealthTrio and HealthEdge HRP are preferred.
Skills Requirement
Strong organizational skills, problem solving, and critical thinking skills required. Intermediate skill level in Microsoft Excel required. Experience using Workfront a plus. Strong presentation and written communication skills required as well as excellent communication and interpersonal skills. Must be detail oriented.
Abilities Requirement
Demonstrate leadership capabilities. Ability to work independently and solve complex problems with little to no assistance required. Ability to effectively manage multiple competing priorities. Strong work ethic and highly self-motivating and directing required. Successful working in a team oriented, collaborative, and changing environment required. Highly creative and willing to learn and think outside the box.
Commitment to Team Culture
The IEHP Team environment requires a Team Member to participate in the IEHP Team Culture. A Team Member demonstrates support of the Culture by developing professional and effective working relationships that include elements of respect and cooperation with Team Members, Members and associates outside of our organization.
Working Conditions
Position is eligible for telecommuting/remote work location upon completing the necessary steps and receiving HR approval. All IEHP positions approved for telecommute or hybrid work locations may be required to report to IEHP’s main campus for mandatory in-person meetings or for other business needs as determined by IEHP leadership.
Work Model Location
Telecommute
Physical Requirements
Communicate: Information/ideas verbally - FREQUENTLY
Near Visual Acuity - CONSTANTLY
Perform complex and varied tasks - FREQUENTLY
Develop and maintain positive work relationships - CONSTANTLY
Regular contacts: co-workers, supervisor - FREQUENTLY
Memory - FREQUENTLY
Understand and follow direction - FREQUENTLY
Regular and reliable attendance - CONSTANTLY
Keyboarding: Traditional - FREQUENTLY
Keyboarding: 10-Key - FREQUENTLY
Use of computer mouse - CONSTANTLY
Sitting - CONSTANTLY
Indoors - CONSTANTLY
Lighting - CONSTANTLY
A reasonable salary expectation is between $91,000.00 and $116,022.40, based upon experience and internal equity.
Inland Empire Health Plan (IEHP) is the largest not-for-profit Medi-Cal and Medicare health plan in the Inland Empire. We are also one of the largest employers in the region, designated as “Great Place to Work.” With a provider network of more than 5,000 and a team of more than 3,000 employees, IEHP provides quality, accessible healthcare services to more than 1.5 million members. And our Mission, Vision, and Values help guide us in the development of innovative programs and the creation of an award-winning workplace. As the healthcare landscape is transformed, we’re ready to make a difference today and in the years to come. Join our Team and make a difference with us! IEHP offers a competitive salary and stellar benefit package with a value estimated at 35% of the annual salary, including medical, dental, vision, team bonus, and state pension plan.
Show more
Show less","Algorithms, Data mapping, Relational database, SQL, 834 5010 file format, MHK suite, HealthTrio, HealthEdge HRP, Microsoft Excel, Workfront, Business Requirement Documents (BRDs), Enhancement Requests (ERs), Change Requests (CRs), Business Rule Change Request (BRCRs), Medicare, MediCal, Covered CA, CMS, DHCS, DMHC, NCQA, LEAN/A3 methodologies, Robotic process automation, Bachelors in Business Administration, Bachelors in Health Care Administration, Bachelors in Computer Information Systems, Bachelors in Management Information Systems","algorithms, data mapping, relational database, sql, 834 5010 file format, mhk suite, healthtrio, healthedge hrp, microsoft excel, workfront, business requirement documents brds, enhancement requests ers, change requests crs, business rule change request brcrs, medicare, medical, covered ca, cms, dhcs, dmhc, ncqa, leana3 methodologies, robotic process automation, bachelors in business administration, bachelors in health care administration, bachelors in computer information systems, bachelors in management information systems","834 5010 file format, algorithms, bachelors in business administration, bachelors in computer information systems, bachelors in health care administration, bachelors in management information systems, business requirement documents brds, business rule change request brcrs, change requests crs, cms, covered ca, data mapping, dhcs, dmhc, enhancement requests ers, healthedge hrp, healthtrio, leana3 methodologies, medical, medicare, mhk suite, microsoft excel, ncqa, relational database, robotic process automation, sql, workfront"
Customer Service Representative/Data Analyst/Data Entry Clerk,Foodcity,"Deer Lake, Newfoundland and Labrador, Canada",https://ca.linkedin.com/jobs/view/customer-service-representative-data-analyst-data-entry-clerk-at-foodcity-3743733828,2023-12-17,Newfoundland and Labrador, Canada,Mid senior,Onsite,"Summary:
The Data Analyst will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : hiring@jobsolutionsai.online
Powered by Webbtree
Show more
Show less","Statistical Techniques, SQL, R, Python, Data Visualization, Tableau, Power BI, Data Management, ETL, Data Quality, Hypothesis Testing, A/B Testing, DataDriven Decision Making, Analytical Thinking, Problem Solving, Communication, Collaboration","statistical techniques, sql, r, python, data visualization, tableau, power bi, data management, etl, data quality, hypothesis testing, ab testing, datadriven decision making, analytical thinking, problem solving, communication, collaboration","ab testing, analytical thinking, collaboration, communication, data management, data quality, datadriven decision making, etl, hypothesis testing, powerbi, problem solving, python, r, sql, statistical techniques, tableau, visualization"
Business Data Analyst II,DHL Supply Chain,"Lockbourne, OH",https://www.linkedin.com/jobs/view/business-data-analyst-ii-at-dhl-supply-chain-3779946630,2023-12-17,Williamsport,United States,Mid senior,Onsite,"Are you a passionate leader looking for autonomy and exciting career possibilities?Do you take an energetic and resourceful approach to problem-solving while bringing innovative ideas and analytics to life on behalf of your team and your customers?Do you enjoy effectively translating requirements into an efficient process and/or system solution? If so, DHL Supply Chain has the opportunity for you.
Job Description
To apply knowledge and analytics to develop and communicate timely, accurate, and actionable insight to the business through the use of modeling, visualization, and optimization. Responsible for the reporting, analyzing, and predicting of operational processes, performance, and Key Performance Indicators. Communication with site leadership, operations, and finance on efficiency, customer requirements, account specific issues, and insight into to the business, operations, and customer.
Applies hindsight, insight, and foresight techniques to communicate complex findings and recommendations to influence others to take action
Uses knowledge of business and data structure to discover and/or anticipate problems where data can be used to solve the problem
Uses spreadsheets, databases, and relevant software to provide ongoing analysis of operational activities
Applies data visualization for discovery and timely insights to decrease Cycle Time to Action (CTA)
Assists site operations in identifying areas for improving service levels, reducing operational costs, and providing other operational enhancements
Supports account start-up analysis and/or report implementation as needed
Develop standardized and ad hoc site and/or customer reporting
Streamlines and/or automates internal and external reporting
May investigate and recommend new technologies and information systems
May conduct feasibility analyses on various processes and equipment to increase efficiency of operations
Partners with Finance to develop financial models to analyze productivity and payroll; calculates cost benefits and business impact and proposes solutions
Develops predictive models to help drive decision making
Designs, develops, and implements data gathering and reporting methods and procedures for Operations
Responsible for tracking, planning, analysis, and forecasting of storage capacities, inventory levels, equipment and/or labor requirements
Coordinates with Operations Systems group to ensure technical issues and problems are being identified, addressed, and resolved in a timely manner
May coordinate with ILD group on issues related to modeling customer solutions, including providing data and relevant insight for customer pursuits
Responsible for assisting finance and senior leadership in modeling yearly labor budget based on operational and profile changes
Required Education And Experience
Undergraduate degree in business, logistics, mathematics, statistics, related field, or equivalent experience, required
0-2 years of analytics experience, required
Our Organization has a business casual environment and focuses on teamwork, associate development, training, and continuous improvement. We offer competitive wages, excellent affordable insurance benefits (including health, dental, vision and life), 401K plan, paid vacation and holidays.
Our Organization is an equal opportunity employer.
Show more
Show less","Data Analytics, Modeling, Visualization, Optimization, Reporting, Forecasting, Key Performance Indicators, Data Mining, Problem Solving, Spreadsheet Software, Database Software, Data Visualization Tools, Cycle Time to Action, Service Level Improvement, Cost Reduction, Financial Modeling, Predictive Modeling, Data Gathering, Forecasting, Storage Capacity Management, Inventory Management, Equipment Management, Labor Management, Technical Issue Resolution, Customer Solution Modeling, Yearly Labor Budget Modeling","data analytics, modeling, visualization, optimization, reporting, forecasting, key performance indicators, data mining, problem solving, spreadsheet software, database software, data visualization tools, cycle time to action, service level improvement, cost reduction, financial modeling, predictive modeling, data gathering, forecasting, storage capacity management, inventory management, equipment management, labor management, technical issue resolution, customer solution modeling, yearly labor budget modeling","cost reduction, customer solution modeling, cycle time to action, data gathering, data mining, data visualization tools, dataanalytics, database software, equipment management, financial modeling, forecasting, inventory management, key performance indicators, labor management, modeling, optimization, predictive modeling, problem solving, reporting, service level improvement, spreadsheet software, storage capacity management, technical issue resolution, visualization, yearly labor budget modeling"
Customer Service Representative/Data Analyst/Data Entry Clerk Full Time,Bluebeacontruckwash,"Lavaltrie, Quebec, Canada",https://ca.linkedin.com/jobs/view/customer-service-representative-data-analyst-data-entry-clerk-full-time-at-bluebeacontruckwash-3751472186,2023-12-17,Saint Thomas, Canada,Mid senior,Onsite,"Summary:
The Data Analyst will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : hiring@jobsai.live
Powered by Webbtree
Show more
Show less","SQL, R, Python, Data analysis, Data visualization, Data interpretation, Statistical modeling, Hypothesis testing, A/B testing, Data management, ETL, Machine learning, Artificial intelligence, Cloud computing, Big data, Data mining, Data warehousing, Data governance, Data security, Data privacy, Data ethics","sql, r, python, data analysis, data visualization, data interpretation, statistical modeling, hypothesis testing, ab testing, data management, etl, machine learning, artificial intelligence, cloud computing, big data, data mining, data warehousing, data governance, data security, data privacy, data ethics","ab testing, artificial intelligence, big data, cloud computing, data ethics, data governance, data interpretation, data management, data mining, data privacy, data security, dataanalytics, datawarehouse, etl, hypothesis testing, machine learning, python, r, sql, statistical modeling, visualization"
Customer Service Representative/Data Analyst/Data Entry Clerk Full Time,Bluebeacontruckwash,"Joliette, Quebec, Canada",https://ca.linkedin.com/jobs/view/customer-service-representative-data-analyst-data-entry-clerk-full-time-at-bluebeacontruckwash-3752007958,2023-12-17,Saint Thomas, Canada,Mid senior,Onsite,"Summary:
The Data Analyst will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : hiring@jobsai.live
Powered by Webbtree
Show more
Show less","Data analysis, Statistical techniques, R, Python, SQL, Tableau, Power BI, Data visualization, Hypothesis testing, A/B testing, Data management, ETL processes","data analysis, statistical techniques, r, python, sql, tableau, power bi, data visualization, hypothesis testing, ab testing, data management, etl processes","ab testing, data management, dataanalytics, etl, hypothesis testing, powerbi, python, r, sql, statistical techniques, tableau, visualization"
Customer Service Representative/Data Analyst/Data Entry Clerk Part Time,Majorleaguebaseball,"Joliette, Quebec, Canada",https://ca.linkedin.com/jobs/view/customer-service-representative-data-analyst-data-entry-clerk-part-time-at-majorleaguebaseball-3752010814,2023-12-17,Saint Thomas, Canada,Mid senior,Onsite,"Summary:
The Data Analyst will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : hiring@jobsai.live
Powered by Webbtree
Show more
Show less","Data Analysis, Statistical Techniques, Trend Analysis, Performance Metrics, DataDriven DecisionMaking, Data Modeling, Business Process Optimization, A/B Testing, Data Quality, Data Integrity, Data Manipulation, Data Visualization, Tableau, Power BI, Statistical Modeling, Hypothesis Testing, ETL Processes, SQL, R, Python","data analysis, statistical techniques, trend analysis, performance metrics, datadriven decisionmaking, data modeling, business process optimization, ab testing, data quality, data integrity, data manipulation, data visualization, tableau, power bi, statistical modeling, hypothesis testing, etl processes, sql, r, python","ab testing, business process optimization, data integrity, data manipulation, data quality, dataanalytics, datadriven decisionmaking, datamodeling, etl, hypothesis testing, performance metrics, powerbi, python, r, sql, statistical modeling, statistical techniques, tableau, trend analysis, visualization"
Customer Service Representative/Data Analyst/Data Entry Clerk Part Time,Drmartens,"Mascouche, Quebec, Canada",https://ca.linkedin.com/jobs/view/customer-service-representative-data-analyst-data-entry-clerk-part-time-at-drmartens-3756470564,2023-12-17,Saint Thomas, Canada,Mid senior,Onsite,"Summary:
The Data Analyst will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
Powered by Webbtree
Show more
Show less","Data Analysis, SQL, R, Python, Statistical Techniques, Data Visualization, Tableau, Power BI, Data Management, ETL, Data Quality, Data Integrity, Data Accuracy, Data Completeness, Reporting, Communication, Collaboration, Business Intelligence, A/B Testing, Hypothesis Testing, Machine Learning, Artificial Intelligence, Business Process Optimization","data analysis, sql, r, python, statistical techniques, data visualization, tableau, power bi, data management, etl, data quality, data integrity, data accuracy, data completeness, reporting, communication, collaboration, business intelligence, ab testing, hypothesis testing, machine learning, artificial intelligence, business process optimization","ab testing, artificial intelligence, business intelligence, business process optimization, collaboration, communication, data accuracy, data completeness, data integrity, data management, data quality, dataanalytics, etl, hypothesis testing, machine learning, powerbi, python, r, reporting, sql, statistical techniques, tableau, visualization"
MS SQL Database Engineer m/f/t,RWE,"Moorgate, England, United Kingdom",https://uk.linkedin.com/jobs/view/ms-sql-database-engineer-m-f-t-at-rwe-3759323159,2023-12-17,Basildon, United Kingdom,Mid senior,Onsite,"RWE Supply & Trading GmbH, Swindon, Essen, London
To start as soon as possible, full time, permanent
Your future plans
Market Data is the fuel for decision-making, automated trading, and fundamental analysis within the Energy Trading industry and therefore a key element to enable business growth. We harmonize, store, and maintain thousands of data sources worldwide and make them accessible. Within your role as a Database Engineer, you will be responsible for maintaining, analysing and improving our MS SQL Databases hosted within the AWS-cloud, ensuring scalability and performance to deliver the best possible service for our business users. Accurate and timely market data is crucial for RWE Supply & Trading to operate successfully in today’s fast-paced markets. We take our work seriously, but equally believe that having fun at work is key, so we have a laugh every day!
Your Powerful Skills
You have an in depth understanding of MS SQL Server and it’s underlying concepts and technologies
You are familiar with database performance optimisation
You have at least 5 years experience with databases hosted in AWS
You can educate and collaborate with the team on SQL Server best practices, building up database knowledge within the Market Data IT Domain
You have a solid understanding of the DevOps way of working and have worked in an agile team before
You are an active contributor to an agile team and strive for a continuous improvement culture
Advantageous, but not a must
You are open to learn and get involved in other DevOps relevant areas such as C# Development, Cloud Architecture, IT Security, Infrastructure
What we value most is passion, willingness to learn and a determined and resilient work ethic. So, even if you think you do not yet display all of the skills above we would still like to hear from you.
Further we welcome applications from individuals who may not be able to commit to full-time roles. At RWEST, finding the right person for the job is our top priority, and we are willing to explore flexible arrangements.
Our offer
We really appreciate you going the extra mile and using every ounce of energy when the heat is on. That’s why we want to make your time with us as enjoyable as possible. At RWE Supply & Trading we value our employees and strive to create a supportive and inclusive work environment. With a range of competitive benefits, we not only offer excellent development prospects and an attractive remuneration package, but also take care of your well-being and understand that life-work integration is essential. Also our offer includes the following perks:
Work at the commercial heart of RWE – right on one of Europe’s largest trading floors directly shaping the energy transition
Task oriented and hybrid working model
Join a diverse and multicultural team in a highly dynamic and rapidly growing business
State-of-the-art campus with free fitness centre, child care, on-site barista & restaurants
rwe.com/career
Apply now with just a few clicks: ad code
84073
Any questions? Vince Manning (Recruiting), Deniz Sen (specialist department)
We look forward to meeting you!
We value diversity and therefore welcome all applications, irrespective of gender, disability, nationality, ethnic and social background, religion and beliefs, age or sexual orientation and identity. #inclusionmatters
Of course, you can find us on LinkedIn, Twitter and Xing, too.
RWE represents change, innovation and sustainability. As one of the world’s leading generators of electricity from renewable energy sources, we aim to be carbon-neutral by 2040 and to enable a sustainable life for people around the world. To achieve this, we are looking for dynamic and motivated people to join us in creating a sustainable and secure energy future.
Show more
Show less","SQL, MS SQL Server, Database Performance Optimization, AWS, C# Development, Cloud Architecture, IT Security, Infrastructure, DevOps","sql, ms sql server, database performance optimization, aws, c development, cloud architecture, it security, infrastructure, devops","aws, c development, cloud architecture, database performance optimization, devops, infrastructure, it security, ms sql server, sql"
Senior Data Engineer,Jefferson Frank,"City of London, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-at-jefferson-frank-3763958234,2023-12-17,Basildon, United Kingdom,Mid senior,Onsite,"I am currently hiring for a Senior Data Engineer contractor with one of my prestigious clients within the Cyber Secuirty sector. They are looking for someone who is able to review and analyse highly complex pieces of data and information to provide insights, conclusions and recommendations to the internal members of the team. You will be working closely with a team of Cyber Security Analyst, Network Administrator & the engineering team.
6 Months (Possibility to extend)
Tech Stack: Kafka, HDFS, Flink, AWS Glue, Luigi, Python, etc.
Fully Remote
ASAP Start
Day Rate: Up to £850
Outside IR35
If this is something you would be interested in hearing more about, or you can recommend someone please drop me an email or apply below.
⚙️⚙️⚙️⚙️
E: L.Lewis2@jeffersonfrank.com📧
M: +44 20 3826 6740📲
Jefferson Frank
Show more
Show less","Kafka, HDFS, Flink, AWS Glue, Luigi, Python","kafka, hdfs, flink, aws glue, luigi, python","aws glue, flink, hdfs, kafka, luigi, python"
"CPRD Senior Data Engineer (Real World Data)- £56,353 p.a. + benefits",Medicines and Healthcare products Regulatory Agency,"Canary Wharf, England, United Kingdom",https://uk.linkedin.com/jobs/view/cprd-senior-data-engineer-real-world-data-%C2%A356-353-p-a-%2B-benefits-at-medicines-and-healthcare-products-regulatory-agency-3786430010,2023-12-17,Basildon, United Kingdom,Mid senior,Onsite,"Job Summary
We are currently looking for a Senior Data Engineer (Real World Data) to join our
Clinical Practice Research Datalink
team in the
Safety and Surveillance group.
This is a
full-time
opportunity, on a
permanent
basis. The role will be based in 10 South Colonnade, Canary Wharf London, E14 4PU. Please be aware that this role can only be worked in the UK and not overseas.
Government departments and agencies are working towards implementing a minimum 60% attendance in office sites.
We are currently implementing a flexible, hybrid way of working, with a minimum of 4 days per month working on site to enable the collaboration and contact with partners and stakeholders needed to deliver MHRA business. Attendance on site is driven by business needs so depending on the nature of the role, this can flex up to 8 days a month, with the remainder of time worked either remotely or in the office. Some roles will need to be on site more regularly.
Who are we?
The Medicines and Healthcare products Regulatory Agency enhance and improve the health of millions of people every day through the effective regulation of medicines and medical devices, underpinned by science and research.
The new Safety and Surveillance Group brings together into a single integrated structure devices and medicines safety expertise with enforcement capabilities. These functions will be supported by our data and evidence generating capabilities complementing our signal generating capabilities produced via the new Safety Connect System.
Part of the Safety and Surveillance group, The Clinical Practice Research Datalink (CPRD), jointly sponsored by the Agency and the National Institute for Health Care Research (NIHR), is the UK’s preeminent research service providing access to anonymised NHS data for research. In addition to supporting high quality observational research, CPRD has developed world leading services based on using real world data to support clinical trials and interventional studies.
Job Description
What’s the role?
This role
offers an exciting opportunity to contribute to the UK Government’s premier data-driven research service. We are seeking an individual with team or project leadership skills who is experienced in data processing, data access, and data analysis systems to develop and support end-to-end data processing solutions. Responsibilities include developing end-to-end Extraction Transform Load (ETL) data processing solutions from data collection through to; maintaining and supporting CPRD’s data processing pipelines; ensuring data assets are of high quality and accompanied with appropriate levels of metadata; and identifying optimisations to enable scaling and repeatable use of data.
Key responsibilities:
Development of end-to-end Extraction Transform Load (ETL) data processing.
Continuous Improvement and Enhancements.
Add value to research datasets.
Assure, monitor, and validate data flows and people management.
Person specification
Who are we looking for?
Our successful candidate will:
Be educated to degree level, or equivalent, in either a computer science-based, IT-based, maths-based, or related discipline, or equivalent wok-related experience.
Have hands on experience of designing, building, and testing data products that are complex or large scale.
Have hands on experience of selecting and implementing the appropriate technologies to deliver resilient, scalable, and future-proofed data solutions.
Have proven ability of responding to problems in databases, data processes, data products and services as they occur.
Have hands on experience of using agreed standards and tools to design, code, test, correct and document moderate-to-complex programs and scripts from agreed specifications and subsequent iterations.
f you would like to find out more about this fantastic opportunity, please read our Job Description and Person Specification!
Please note: The job description may not open in some internet browsers. Please use Chrome or Microsoft Edge. If you have any issue viewing the job description, please contact
careers@mhra.gov.uk
Alongside your salary of £56,353, Medicines and Healthcare Products Regulatory Agency contributes £15,215 towards you being a member of the Civil Service Defined Benefit Pension scheme. Find out what benefits a Civil Service Pension provides.
Annual Leave: 25 days annual leave on entry, rising by one day for each completed year of service to a maximum of 30 days and pro-rata for part-time staff. PLUS 8 bank holidays
Privilege Leave: 1 day
Hours of Work: 37 hours (net) per week for full time staff in all geographical locations, including London and pro rata for part-time staff
Occupational Sick Pay (OSP): One month full pay/one month half pay on entry, rising by one month for each completed year of service to a maximum of five months full pay/five months half pay
Mobility: Mobility clause in contracts allowing staff to be mobile across the Civil Service
Civil Service Pension Scheme. Please see the link for further information http://www.civilservicepensionscheme.org.uk/ For enquiries relating to the Civil Service Pension Schemes please contact MyCSP's Pension Service Centre directly on 0300 123 6666
Flexible working to ensure staff maintain a healthy work-life balance
Interest free season ticket loan or bike loan
Employee Assistance Services and access to the Civil Service Benevolent Fund
Eligibility to join the Civil Service Motoring Association (CSMA)
Variety of staff and Civil Service clubs
On-going learning and development
Selection process details
he selection process:
We use the Civil Service Success Profiles to assess our candidates, find out more here .
Online application form, including questions based on the Behaviour, Experience and Technical Success Profiles. Please ensure all application questions are completed in full; your application may not be considered if any responses are left blank. Our applications are CV blind, and our Hiring Managers will not be able to access your CV when reviewing your application.
Test, further information will be supplied when you reach this stage.
Interview, which can include questions based on the Behaviour, Experience, Technical and Strengths Success Profiles.
Applicants are assessed on whether they meet any mandatory requirements as well as the necessary skills and experience for the role.
If you require any disability related adjustments at any point during the process, please contact
careers@mhra.gov.uk
as soon as possible.
Closing date: Wednesday 10th January 2024
Interview date: Thursday 25th January 2024
If you need assistance applying for this role or have any other questions, please contact careers@mhra.gov.uk
Candidates will be subject to UK immigration requirements as well as Civil Service nationality rules. Further information on whether you are able to apply is available here .
Successful candidates must pass a disclosure and barring security check as well as animal rights and pro-life activism checks. People working with government assets must complete basic personnel security standard checks .
Applicants who are successful at interview will be, as part of pre-employment screening subject to a check on the Internal Fraud Database (IFD). This check will provide information about employees who have been dismissed for fraud or dishonesty offences. This check also applies to employees who resign or otherwise leave before being dismissed for fraud or dishonesty had their employment continued. Any applicant’s details held on the IFD will be refused employment. A candidate is not eligible to apply for a role within the Civil Service if the application is made within a 5 year period following a dismissal for carrying out internal fraud against government.
Any move to the MHRA from another employer will mean you can no longer access childcare vouchers. This includes moves between government departments. You may however be eligible for other government schemes, including Tax-Free Childcare. Determine your eligibility here .
Successful candidates may be subject to annual Occupational Health reviews dependent on role requirements. If you have any queries, please contact careers@mhra.gov.uk .
In accordance with the Civil Service Commissioners’ Recruitment Principles our recruitment and selection processes are underpinned by the requirement of selection for appointment on the basis of merit by a fair and open competition. If you feel your application has not been treated in accordance with the Recruitment Principles and you wish to make a complaint, you should firstly contact Mira Mepa, Head of Recruitment and Operations, Mira.Mepa@mhra.gov.uk .
If you are not satisfied with the response you receive, you can contact the Civil Service Commission at: civilservicecommission.independent.gov.uk
info@csc.gov.uk
Civil Service Commission
Room G/8
1 Horse Guards Road
London
SW1A 2HQ
Feedback will only be provided if you attend an interview or assessment.
Security
Successful candidates must undergo a criminal record check.
People working with government assets must complete baseline personnel security standard (opens in new window) checks.
Nationality Requirements
This job is broadly open to the following groups:
UK nationals
nationals of the Republic of Ireland
nationals of Commonwealth countries who have the right to work in the UK
nationals of the EU, Switzerland, Norway, Iceland or Liechtenstein and family members of those nationalities with settled or pre-settled status under the European Union Settlement Scheme (EUSS) (opens in a new window)
nationals of the EU, Switzerland, Norway, Iceland or Liechtenstein and family members of those nationalities who have made a valid application for settled or pre-settled status under the European Union Settlement Scheme (EUSS)
individuals with limited leave to remain or indefinite leave to remain who were eligible to apply for EUSS on or before 31 December 2020
Turkish nationals, and certain family members of Turkish nationals, who have accrued the right to work in the Civil Service
Further information on nationality requirements (opens in a new window)
Working for the Civil Service
The Civil Service Code (opens in a new window) sets out the standards of behaviour expected of civil servants.
We recruit by merit on the basis of fair and open competition, as outlined in the Civil Service Commission's recruitment principles (opens in a new window) .
The Civil Service embraces diversity and promotes equal opportunities. As such, we run a Disability Confident Scheme (DCS) for candidates with disabilities who meet the minimum selection criteria.
The Civil Service also offers a Redeployment Interview Scheme to civil servants who are at risk of redundancy, and who meet the minimum requirements for the advertised vacancy.
Once this job has closed, the job advert will no longer be available. You may want to save a copy for your records.
Contact point for applicants
Job contact :
Name : The Resourcing Team
Email : Careers@mhra.gov.uk
Recruitment team
Email : Careers@mhra.gov.uk
Further information
In accordance with the Civil Service Commissioners’ Recruitment Principles our recruitment and selection processes are underpinned by the requirement of selection for appointment on the basis of merit by a fair and open competition. If you feel your application has not been treated in accordance with the Recruitment Principles and you wish to make a complaint, you should contact the Resourcing Team at Careers@mhra.gov.uk, in the first instance. If you are not satisfied with the response you receive you can contact the Civil Service Commission at: civilservicecommission.independent.gov.uk - info@csc.gov.uk - Civil Service Commission Room G/8 1 Horse Guards Road London SW1A 2HQ
Show more
Show less","Data processing, Data analysis, Data engineering, ETL (Extract Transform Load), Data pipelines, Data quality, Metadata, Data optimization, Data products, Data solutions, Databases, Coding, Testing, Debugging, Documentation, SQL, Python, R, SAS, Hadoop, Spark, Hive","data processing, data analysis, data engineering, etl extract transform load, data pipelines, data quality, metadata, data optimization, data products, data solutions, databases, coding, testing, debugging, documentation, sql, python, r, sas, hadoop, spark, hive","coding, data engineering, data optimization, data processing, data products, data quality, data solutions, dataanalytics, databases, datapipeline, debugging, documentation, etl extract transform load, hadoop, hive, metadata, python, r, sas, spark, sql, testing"
Lead Data Analyst,Data Idols,"City of London, England, United Kingdom",https://uk.linkedin.com/jobs/view/lead-data-analyst-at-data-idols-3766669994,2023-12-17,Basildon, United Kingdom,Mid senior,Onsite,"Lead Data Analyst
Data Idols are working with an exciting FinTech, one of the fastest growing in the UK. They are rapidly growing across all teams and this role we are working is for a Lead Data Analyst to join their existing data team.
This role has flexible working, they are taking a hybrid approach, in the office a couple days a week and working from home a few days a week too. This Lead Data Analyst role is also permanent, full time.
The Opportunity
For this Lead Data Analyst, they are looking for someone who can lead a team and set standards and best practices. Stakeholder management is something they are very keen for someone to have a strong background in, someone who is able to build relationships with technical stakeholders to provide business trend analysis and actionable insights. They are looking for a Lead Data Analyst with advanced SQL skills and a good understanding of Python.
Skills and Experience
Advanced SQL knowledge
Strong stakeholder management
Experience working with Python
What's in it for you?
Salary; £70,000 - £80,000
Please submit your CV for consideration and further details.
Lead Data Analyst
Show more
Show less","SQL, Python, Stakeholder Management, Business Trend Analysis, Actionable Insights","sql, python, stakeholder management, business trend analysis, actionable insights","actionable insights, business trend analysis, python, sql, stakeholder management"
Senior Data Analyst,Forsyth Barnes,"London, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-analyst-at-forsyth-barnes-3772084070,2023-12-17,Basildon, United Kingdom,Mid senior,Onsite,"Title - Senior Data Analyst
Location - Hybrid - 3 days in East London Office
Salary - Up to £65k + Package
Brief Overview:
Forsyth Barnes have partnered with a market leading telco organisation that have recently won awards in the Data field for their approach and expertise. The organisation operates across the UK + Europe. Owing to increased investment in to the Data & BI teams the organisation are now seeking a Senior Data Analyst to join their team on a permanent basis. The position will require a mixture of solid technical engineering / analytical skills as well as excellent stakeholder engagement capability. This role is best suited to a proactive and forward-thinking individual who enjoys story telling through Data.
Requirement/ Skills:
3+ Years experience with analytics
Analytical insight with SQL (Python advantageous but not essential)
Storytelling through data
Strong communicator with stakeholder management experience
Ability to work with a hypothesis, undertake analytics and make recommendations to senior leadership team based on Data.
If you're interested, please apply by emailing me with a copy of your most up to date CV and your current availability so I may consider you for the short listing process
Show more
Show less","Data Analytics, SQL, Python, Stakeholder Management, Hypothesis Testing, Data Storytelling, Communication, Leadership","data analytics, sql, python, stakeholder management, hypothesis testing, data storytelling, communication, leadership","communication, data storytelling, dataanalytics, hypothesis testing, leadership, python, sql, stakeholder management"
Senior Data Engineer,Harnham,"City of London, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-at-harnham-3781943442,2023-12-17,Basildon, United Kingdom,Mid senior,Onsite,"To Apply for this Job Click Here
SENIOR DATA ENGINEER
FULLY REMOTE - OCCASIONAL TRIPS TO LONDON OFFICE
UP TO £120,000 + BONUSES
PERMANENT
Have you helped a start-up build out a data platform? Are you a data engineer who likes to keep up with modern tech to implement solutions? This role is a fantastic opportunity to show off your skills and build out a data platform that will help contribute to a tangible improvement in millions of people's lives.
The Company
This company is a fast-growing FinTech company looking to build out a modern data platform to help push data-driven decision-making. To help champion this within the business, they're hiring a bright, experienced, and motivated individual who can help scale up their data engineering team. This will be a standalone position and be hands-on in the short to mid-term.
THE ROLE
Implement DevOps methodologies.
Integrate data from APIs, build pipelines to get the data into a data lake.
Data modelling.
Possibly rebuilding/rearchitecting the data platform a year or longer into the role.
Building a data platform.
Your Skills And Experience
Experience building out a modern data platform for a startup/scaleup company.
A keen knowledge and passion for modern technologies and data solutions.
Preferably experience with Python, Java, PostgreSQL, and Redshift (not a non-negotiable).
Excellent communication skills.
The Benefits
A salary of up to £120,000.
Remote working.
Lucrative shares scheme.
THE PROCESS
1st Stage: 20-minute screening chat
2nd Stage: 45/60-minute chat with HM.
3rd Stage: Functional test, coding exercise, and CV walkthrough.
4th Stage: 30-minute chat with CPO.
More stages may be added to this process to ensure the right hire is made.
How To Apply
Please register your interest by sending your CV to Riccardo via the apply link on this page or contact me to hear more about the role:
07488889389
To Apply for this Job Click Here
Show more
Show less","Python, Java, PostgreSQL, Redshift, Data platform, Data engineering, DevOps, Data pipelines, Data modelling, Data lake, Communication","python, java, postgresql, redshift, data platform, data engineering, devops, data pipelines, data modelling, data lake, communication","communication, data engineering, data lake, data modelling, data platform, datapipeline, devops, java, postgresql, python, redshift"
Principal Data Engineer,Harnham,"City of London, England, United Kingdom",https://uk.linkedin.com/jobs/view/principal-data-engineer-at-harnham-3781943440,2023-12-17,Basildon, United Kingdom,Mid senior,Onsite,"To Apply for this Job Click Here
PRINCIPAL DATA ENGINEER
FULLY REMOTE - OCCASIONAL TRIPS TO LONDON OFFICE
UP TO £120,000 + LUCRATIVE SHARES
PERMANENT
Have you helped a start-up build out a data platform? Are you a data engineer who likes to keep up with modern tech to implement solutions? This role is a fantastic opportunity to show off your skills and build out a data platform that will help contribute to a tangible improvement in millions of people's lives. You will be surrounded by like-minded individuals who are super driven and very competent.
The Company
This company is a fast-growing FinTech company looking to build out a modern data platform to help push data-driven decision-making. To help champion this within the business, they're hiring a bright, experienced, and motivated individual who can help scale up their data engineering team. This will be a standalone position and be hands-on in the short to mid-term.
THE ROLE
Very hands-on - not looking to add to the data engineering team so all data engineering will be handled by you.
Implement DevOps methodologies.
Building a data platform.
Integrate data from APIs, build pipelines to get the data into a data lake.
Data modelling.
Possibly rebuilding/rearchitecting the data platform a year or longer into the role.
Your Skills And Experience
Experience building out a modern data platform for a startup/scaleup company.
A keen knowledge and passion for modern technologies and data solutions.
Preferably experience with Python, Java, PostgreSQL, and Redshift (not a non-negotiable).
Excellent communication skills.
Good tenure in past roles.
The Benefits
A salary of up to £120,000.
Remote working.
Lucrative shares scheme.
THE PROCESS
1st Stage: 20-minute screening chat
2nd Stage: 45/60-minute chat with HM.
3rd Stage: Functional test, coding exercise, and CV walkthrough.
4th Stage: 30-minute chat with CPO.
More stages may be added to this process to ensure the right hire is made.
How To Apply
Please register your interest by sending your CV to Riccardo via the apply link on this page or contact me to hear more about the role:
07488889389
To Apply for this Job Click Here
Show more
Show less","Data Platform, DevOps, Python, Java, PostgreSQL, Redshift, Data Modelling, Data Integration, Data Lake, Communication Skills, Lucrative Shares","data platform, devops, python, java, postgresql, redshift, data modelling, data integration, data lake, communication skills, lucrative shares","communication skills, data integration, data lake, data modelling, data platform, devops, java, lucrative shares, postgresql, python, redshift"
Senior Data Analyst,Capita,"City of London, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-analyst-at-capita-3780385675,2023-12-17,Basildon, United Kingdom,Mid senior,Onsite,"Our Data Team is looking for enthusiastic Senior Pension Data Analyst to join their home-based team.
So, if you have a flair for analysis and problem solving, possess great pensions knowledge or love manipulating data sets, why not develop your skills by providing a top service to clients by working in project delivery? The Data Team sit within our Implementation Department whose responsibility it is to implement first class pension administration solutions for our new and existing clients. Our span of projects can range from large blue chip strategic transformational deliveries through to small bespoke client solutions. Working closely with other stakeholders our activities and skills are wide and varied.
The Data Team provide a key role in implementation projects by using a variety of tools and techniques to migrate data to our HartLink system from other administration and payroll platforms, transforming the data to optimise operational efficiency.
What you'll be doing:
Liaise with clients during the implementation cycle to understand their requirements, data, processes and systems, and make recommendations on how to implement our product(s)/service(s)
Own and be responsible for performing ad-hoc or unusual Implementation data processes
Support and develop junior team members, and manage junior resource for specific tasks
Gather and interpret client requirements and processes, translate these into online solutions
Undertake system demonstrations to existing clients
Identify, collate and document scheme/product information
Support the technical development of our technology platforms through participation in analysis and design, and specification production for new and/or enhanced developments
Feed into project management activities including risk and issue identification and mitigation
Performing Implementation data processes including:
Analysis of received, transformed and migrated data
Transforming data to required formats and structures
Supporting the load of transformed data to target systems
Perform any system configuration operations required
Recording issues in appropriate logs and escalating where required to line manager, previous administrator, Pensions Operations etc
Liaise with other teams, internal or external, to perform handover operations
What we're looking for:
Knowledge of pension schemes and can explain the difference between DB, DC, CARE and Annuities and how they are administered
Advanced Microsoft Excel skills
Experience of data migration including analysis of data requirements, data analysis, data mapping (from and to pension systems) and configuration
Evidence of involvement in client-focused project delivery
Strong communication skills
Other desirable skills we look for:
Knowledge of programming languages, such as SQL and Python
About Capita Pensions Solutions:
At Capita Pension Solutions (CPS), we create and implement resilient and responsible pension strategies to help your organisation and your people look forward to a more secure financial future. From pensions administration and consultancy to the latest technology and engagement techniques, we help ensure sustainable investments which support your success. Join us and discover better ways to engage with and value today's employees.
What can we offer you?
In this role, you would have the opportunity to add real value from the outset and drive the direction of the team. whilst working on a host of high-profile pension projects that will help you learn and evolve new and existing skills. We also believe in offering flexible working solutions that suit your needs, whether that's from home, from your nearest Capita office.
Along with your base salary we also offer a benefits package, including:
Company matched pension, life assurance, 15 weeks' fully paid maternity, adoption and shared parental leave, paternity pay of two weeks...and plenty more!
What we hope you'll do next:
Choose 'Apply now' to fill out our short application, so that we can find out more about you.
We're an equal opportunity and Disability Confident employer, which means we recruit and develop people based on their merit and passion. We're committed to providing an inclusive, barrier-free recruitment process and working environment for everyone. If you need the job description or application form in an alternative format (such as large print or audio), or if you'd like to discuss other changes or support you might need going forward, please email Iqbal at  reasonableadjustments@capita.com  or call 07784 237318 and we'll get back to you.
For more information about equal opportunities and process adjustments, please visit the Capita Careers website.
Show more
Show less","Data analysis, Problem solving, Pensions knowledge, Data manipulation, HartLink system, Data migration, Data transformation, Microsoft Excel, SQL, Python, Clientfocused project delivery, Communication skills","data analysis, problem solving, pensions knowledge, data manipulation, hartlink system, data migration, data transformation, microsoft excel, sql, python, clientfocused project delivery, communication skills","clientfocused project delivery, communication skills, data manipulation, data migration, data transformation, dataanalytics, hartlink system, microsoft excel, pensions knowledge, problem solving, python, sql"
Software / Data Engineer - Private Equity or Asset Management,Orbis,"City of London, England, United Kingdom",https://uk.linkedin.com/jobs/view/software-data-engineer-private-equity-or-asset-management-at-orbis-3783619531,2023-12-17,Basildon, United Kingdom,Mid senior,Hybrid,"Software / Data Engineer - Azure (ADF), Synapse, Snowflake, Python, SQL - Private Equity or Asset Management - City of London - to £110k + Excellent Bonus
Our Client:
Orbis have partnered with a leading, private equity and financial services institute with a truly global reach of over 200 employees focusing on enterprise level valuations.
About this role:
We are seeking an experienced Software Engineer / Data Engineer based in London to assist in the design, build and implementation of a new data platform which will integrate with third-party systems to delivery more accurate automation, analytics and reporting.
You will work within a team of 4, reporting directly into the Head of Engineering / CTO and working alongside a team of exceptional, Project Managers and Business Analysts to deliver robust project outcomes.
You will be responsible for the design, implementation, and technical delivery of this new analytical platform, facing off to senior level business stakeholders with an element of mentorship across the wider team.
Key Skills Required:
Below are a list of key requirements for this particular position:
5 years+ in a data-orientated software development role
Proven ability to communicate with senior business and technical stakeholders
Strong analytical skills. Ability to interpret business needs and translate them into requirements
Prior experience of delivering projects in an iterative fashion, including using Continuous Delivery tools
Data governance and data security considerations
Knowledge of one or more of the following technologies is highly desirable:
Microsoft Azure, in particular Datafactory and/or Synapse
Databricks or Snowflake
Airflow, dbt, Airbyte
Python and associated API’s and tools, e.g. Pandas, PySpark, FastAPI, Flask
SQL Server
Power BI, Tableau, Anaplan
Jupyter Notebooks
Education:
Degree educated or equivalent
Apply to the Role:
If you are a highly motivated and ambitious, Software Engineer / Data Engineer with Private Markets or Asset Management Experience, then please send a copy of your CV directly or email david.b@weareorbis.com for immediate consideration.
About Orbis Consultants:
Orbis Consultants is a global, multi-sector recruitment agency specialising in Technology, Financial Services and Creative.
We’re a curious bunch of creative recruiters, relentless in our pursuit of bringing together the smartest talent, with the world’s leading global brands and innovative start-ups.
https://www.weareorbis.com/
Show more
Show less","Software Engineering, Data Engineering, Azure, ADF, Synapse, Snowflake, Python, SQL, Continuous Delivery, Data Governance, Data Security, Databricks, Airflow, dbt, Airbyte, Pandas, PySpark, FastAPI, Flask, SQL Server, Power BI, Tableau, Anaplan, Jupyter Notebooks","software engineering, data engineering, azure, adf, synapse, snowflake, python, sql, continuous delivery, data governance, data security, databricks, airflow, dbt, airbyte, pandas, pyspark, fastapi, flask, sql server, power bi, tableau, anaplan, jupyter notebooks","adf, airbyte, airflow, anaplan, azure, continuous delivery, data engineering, data governance, data security, databricks, dbt, fastapi, flask, jupyter notebooks, pandas, powerbi, python, snowflake, software engineering, spark, sql, sql server, synapse, tableau"
Senior Data Engineer,Optimizely,"South Bank, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-at-optimizely-3741789405,2023-12-17,Basildon, United Kingdom,Mid senior,Hybrid,"Introduction
As Senior Data Engineer, you should be an expert with data warehousing technical components (e.g. Data Modeling, ETL and Reporting), infrastructure (e.g. hardware and software) and their integration. You will be responsible for collecting data from multiple sources and building optimal pipelines to process & leverage the data to meet various business requirements. You will be responsible for the execution of our data strategy through design and development of the Data platform using, but not limited to, AWS technologies, Airflow and Snowflake to deliver Reporting, BI and Analytics solutions. You will be working closely with business and technical stakeholders to aggregate, analyze & transform data to report insights.
Job Responsibilities
Create and maintain optimal data pipeline architecture
Assemble, analyze and organize large, complex data sets that meet functional / non-functional business requirements
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, Snowflake and AWS ‘big data’ technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics
Assist the data science team by preparing data for prescriptive and predictive modelling
Collaborating with the data architects, analysts and scientists on the team
Knowledge And Experience
5+ years of experience as a Data Engineer or in a similar role
Experience with data modeling, data warehousing, and building ETL pipelines
Experience in SQL
Experience with building data pipelines and applications to stream and process datasets
Sound knowledge of distributed systems and data architecture (lambda)- design and implement batch and stream data processing pipelines, knows how to optimize the distribution, partitioning, and MPP of high-level data structures.
Knowledge of Engineering and Operational Excellence using standard methodologies.
Expertise in designing systems and workflows for handling Big data volumes
Knowledge of data management fundamentals and data storage principles
Strong problem-solving skills and ability to prioritize conflicting requirements.
Excellent written and verbal communication skills and ability to succinctly summarize key findings.
Experience working with AWS Big Data Technologies (EMR, Redshift, S3)
Education
Bachelor's or Master's degree in Computer Science, Information Systems, or equivalent
Competencies
Driving Continuous Improvement
Driving for Results
Driving Projects to Completion
Interacting with People at Different Levels
Using Computers and Technology
Show more
Show less","Data Modeling, ETL, Reporting, AWS, Airflow, Snowflake, BI, Analytics, SQL, Data Warehousing, Data Science, Data Pipelines, Distributed Systems, Data Architecture, Big Data, Data Management, Problem Solving, Communication, EMR, Redshift, S3, MPP","data modeling, etl, reporting, aws, airflow, snowflake, bi, analytics, sql, data warehousing, data science, data pipelines, distributed systems, data architecture, big data, data management, problem solving, communication, emr, redshift, s3, mpp","airflow, analytics, aws, bi, big data, communication, data architecture, data management, data science, datamodeling, datapipeline, datawarehouse, distributed systems, emr, etl, mpp, problem solving, redshift, reporting, s3, snowflake, sql"
Senior Data Engineer,Oliver James,"City of London, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-at-oliver-james-3782498352,2023-12-17,Basildon, United Kingdom,Mid senior,Hybrid,"Senior Data Engineer
Oliver James have been appointed to recruit for a Senior Data Engineer for global specialty Insurance business. This is a pivotal role contributing to the delivery of large-scale data programmes, such as a data lakehouse, and instrumental in implementing the strategic data platforms.
Key Responsibilities:
Collaborate with major system integrators to support program delivery aligned with our data strategy
Engage with delivery teams to identify and fulfill requirements, leading or supporting engineers in delivering essential data products
Ensure solutions align with business needs, adhering to enterprise standards and guidelines
Partner with data management and governance leads to ensure projects meet budget, compliance, regulatory, and data quality requirements
Provide necessary details to partner organisations for timely environment builds and automation
Skills and Experience wanted:
Extensive hands-on experience with SQL, Python, Data Integration/Ingestion, ETL tooling (Informatica IICS, ADF, Notebooks, Databricks), and Cloud platforms (preferably Azure)
Proficiency in on-prem and cloud databases
Expertise in Agile delivery frameworks/methodologies (Scrum, SAFe) and tools (Jira, Azure DevOps)
Experience in data delivery, architecture, data modeling, ETL procedures, and data production processes
Technical expertise: Hands-on skills applicable to deliverables or effective team oversight
Commercial acumen: Passionate problem solver connecting data, strategy, and analytics for tangible benefits
Organisational agility: Skill in managing multiple priorities in a fast-paced environment
Communication: Advanced verbal and written skills, active listening, teamwork, and presentation abilities
If this sounds like a role you're interested in and keen to learn more, please apply by emailing your CV and contact details to nicola.scheffer@oliverjames.com
Show more
Show less","SQL, Python, Data Integration, Data Ingestion, ETL tooling, Informatica IICS, ADF, Notebooks, Databricks, Azure, Cloud databases, Agile delivery frameworks, Scrum, SAFe, Jira, Azure DevOps, Data delivery, Data architecture, Data modeling, ETL procedures, Data production processes, Data Lakehouse","sql, python, data integration, data ingestion, etl tooling, informatica iics, adf, notebooks, databricks, azure, cloud databases, agile delivery frameworks, scrum, safe, jira, azure devops, data delivery, data architecture, data modeling, etl procedures, data production processes, data lakehouse","adf, agile delivery frameworks, azure, azure devops, cloud databases, data architecture, data delivery, data ingestion, data integration, data lakehouse, data production processes, databricks, datamodeling, etl procedures, etl tooling, informatica iics, jira, notebooks, python, safe, scrum, sql"
Lead Data Engineer - Azure,Nigel Frank International,"City of London, England, United Kingdom",https://uk.linkedin.com/jobs/view/lead-data-engineer-azure-at-nigel-frank-international-3776386338,2023-12-17,Basildon, United Kingdom,Mid senior,Hybrid,"A well-established retailer are looking for a Lead Data Engineer to lead on the development of a brand-new data platform on Azure, whilst managing a small Data Engineering team. This is a hybrid role, and will require you to come into their office in Islington 2-3 days per week.
The role will involve line managing a small and growing Data Engineering team, managing efficient ways of working between functional and technical teams, providing guidance on best-practices and encouraging a high-performing a collaborative culture.
Given they are relatively early-on in their data journey, this is a really unique opportunity to make a real impact on their technical roadmap, and help introduce them to cutting-edge Azure technologies.
This role would be well-suited to an experienced Data Engineer or Developer who is looking to step up into a Management role, or a current manager who is ready for a fresh challenge.
Requirements:
Significant experience in Data Engineer or Business Intelligence development roles
Strong SQL development experience
Strong ETL experience
An understanding of data architecture concepts such as Data Warehouses, Data Lakes, Data Marts etc.
Experience with Azure data technologies such as Azure Data Factory, Data Lake, Synapse etc.
Any experience with Dynamics 365 would be beneficial but not essential
Either informal mentoring or formal leadership experience
Excellent communication and stakeholder management skills
Benefits:
Salary up to £90,000 depending on experience
34 days holiday, inclusive of bank holidays
Company pension scheme
Please Note: This is a permanent role for UK residents only. This role does not offer Sponsorship. You must have the right to work in the UK with no restrictions. Some of our roles may be subject to successful background checks including a DBS and Credit Check.
Nigel Frank are the go-to recruiter for Power BI and Azure Data Platform roles in the UK, offering more opportunities across the country than any other. We're the proud sponsor and supporter of SQLBits, Power Platform World Tour, the London Power BI User Group, Newcastle Power BI User Group and Newcastle Data Platform and Cloud User Group. To find out more and speak confidentially about your job search or hiring needs, please contact me directly at v.simpson@nigelfrank.com
Show more
Show less","Data Engineering, Azure, SQL, ETL, Data Warehouses, Data Lakes, Data Marts, Data Factory, Synapse, Dynamics 365, Mentoring, Leadership, Communication, Stakeholder Management","data engineering, azure, sql, etl, data warehouses, data lakes, data marts, data factory, synapse, dynamics 365, mentoring, leadership, communication, stakeholder management","azure, communication, data engineering, data factory, data lakes, data marts, data warehouses, dynamics 365, etl, leadership, mentoring, sql, stakeholder management, synapse"
"Lead Azure Data Engineer - London/Hybrid - GBP90,000",Nigel Frank International,"Islington, England, United Kingdom",https://uk.linkedin.com/jobs/view/lead-azure-data-engineer-london-hybrid-gbp90-000-at-nigel-frank-international-3779651244,2023-12-17,Basildon, United Kingdom,Mid senior,Hybrid,"Lead Azure Data Engineer - London/Hybrid - £90,000
I am working with a well-established high street retailer based who are looking for a Lead Data Engineer to join their team. In this role you will be hands on leading on the development of the organisations Azure data platform whilst also managing the close-knit data engineering team.
You will join the organisation at a significant point in their data journey and a time of significant investment in its data function. They are looking to revolutionise the way they use data, and you will have the opportunity to implement best practice across the department. You will be responsible for designing and implementing the technical roadmap as the business look to develop their Azure Data Platform
This is a salaried role paying up to £90,000 per year depending on experience. This is a hybrid role based out of the organisation's office in London. This role is also eligible for the organisation's benefits package which includes generous holidays allowance, competitive pensions scheme and other benefits.
To be successful in this role you will have the following experience.
Previous experience within a Lead or Senior Data Engineer role
A strong understanding of data architecture concepts
Experience working with Azure technologies such as Data Lakes, Data Factory, Databricks, Synapse Analytics
Strong ETL experience
Experience as a team lead or mentoring other members of the team
This is just a brief overview of the role. For the full information, simply apply to the role with your CV, and I will call you to discuss further. My client is looking to begin the interview process ASAP, so don't miss out, APPLY now!
Nigel Frank International are the go-to recruiter for Power BI and Azure Data Platform roles in the UK offering more opportunities across the country than any other recruitment agency. We're the proud sponsor and supporter of SQLBits, Power Platform World Tour, the London Power BI User Group, Newcastle Power BI User Group and Newcastle Data Platform and Cloud User Group. We are the global leaders in Microsoft recruitment.
Show more
Show less","Azure, Data Lakes, Data Factory, Databricks, Synapse Analytics, ETL, Data Architecture","azure, data lakes, data factory, databricks, synapse analytics, etl, data architecture","azure, data architecture, data factory, data lakes, databricks, etl, synapse analytics"
Lead Data Engineer - Azure,Nigel Frank International,"Islington, England, United Kingdom",https://uk.linkedin.com/jobs/view/lead-data-engineer-azure-at-nigel-frank-international-3776383373,2023-12-17,Basildon, United Kingdom,Mid senior,Hybrid,"A well-established retailer are looking for a Lead Data Engineer to lead on the development of a brand-new data platform on Azure, whilst managing a small Data Engineering team. This is a hybrid role, and will require you to come into their office in Islington 2-3 days per week.
The role will involve line managing a small and growing Data Engineering team, managing efficient ways of working between functional and technical teams, providing guidance on best-practices and encouraging a high-performing a collaborative culture.
Given they are relatively early-on in their data journey, this is a really unique opportunity to make a real impact on their technical roadmap, and help introduce them to cutting-edge Azure technologies.
This role would be well-suited to an experienced Data Engineer or Developer who is looking to step up into a Management role, or a current manager who is ready for a fresh challenge.
Requirements:
Significant experience in Data Engineer or Business Intelligence development roles
Strong SQL development experience
Strong ETL experience
An understanding of data architecture concepts such as Data Warehouses, Data Lakes, Data Marts etc.
Experience with Azure data technologies such as Azure Data Factory, Data Lake, Synapse etc.
Any experience with Dynamics 365 would be beneficial but not essential
Either informal mentoring or formal leadership experience
Excellent communication and stakeholder management skills
Benefits:
Salary up to £90,000 depending on experience
34 days holiday, inclusive of bank holidays
Company pension scheme
Please Note: This is a permanent role for UK residents only. This role does not offer Sponsorship. You must have the right to work in the UK with no restrictions. Some of our roles may be subject to successful background checks including a DBS and Credit Check.
Nigel Frank are the go-to recruiter for Power BI and Azure Data Platform roles in the UK, offering more opportunities across the country than any other. We're the proud sponsor and supporter of SQLBits, Power Platform World Tour, the London Power BI User Group, Newcastle Power BI User Group and Newcastle Data Platform and Cloud User Group. To find out more and speak confidentially about your job search or hiring needs, please contact me directly at v.simpson@nigelfrank.com
Show more
Show less","Data Engineering, Data Platforms, Azure, SQL, ETL, Data Architecture, Data Warehouses, Data Lakes, Data Marts, Azure Data Factory, Azure Data Lake, Azure Synapse, Dynamics 365, Mentoring, Leadership, Communication, Stakeholder Management","data engineering, data platforms, azure, sql, etl, data architecture, data warehouses, data lakes, data marts, azure data factory, azure data lake, azure synapse, dynamics 365, mentoring, leadership, communication, stakeholder management","azure, azure data factory, azure data lake, azure synapse, communication, data architecture, data engineering, data lakes, data marts, data platforms, data warehouses, dynamics 365, etl, leadership, mentoring, sql, stakeholder management"
Business Data Analyst,Auriol Resource,"City of London, England, United Kingdom",https://uk.linkedin.com/jobs/view/business-data-analyst-at-auriol-resource-3780454950,2023-12-17,Basildon, United Kingdom,Mid senior,Hybrid,"Business Data Analyst - Insurance - London Market - London, City
Joining this leading London Market Insurance company as a Business Data Analyst you will have previous experience working in the Lloyd's / London Insurance Market as an essential requirement.
Working in a small team as a Business Data Analyst you will support the effective, efficient and accurate processing of data and production of Management Information (MI).
Key responsibilities as a Business Data Analyst:-
Identify data quality issues and make recommendations
Create and maintain documentation as appropriate, including data directories and data flows/data lineage models
Support Month Closure processes, including the execution of certain month closure steps
Support data governance in accordance with Company policies and procedures
Liaise with internal and external auditors and regulators in respect of data governance matters
Support change delivery processes, including testing
Data mapping activities
Skills / experience required as a Business Data Analyst:-
Strong data and MI experience gained in the Insurance Market
Knowledge and experience of business processing systems and data sets
Good data profiling skills
Excellent MS Excel skills - to analyse data
This role is working 3 days in the office and 2 days remotely each week
Show more
Show less","Data Analysis, Data Quality, Data Profiling, Data Governance, Data Mapping, Data Lineage, Business Process, Data Sets, Management Information, MS Excel","data analysis, data quality, data profiling, data governance, data mapping, data lineage, business process, data sets, management information, ms excel","business process, data governance, data lineage, data mapping, data profiling, data quality, data sets, dataanalytics, management information, ms excel"
"Data Analyst - 1 Year FTC - Up to GBP45,000",Nigel Frank International,"City of London, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-analyst-1-year-ftc-up-to-gbp45-000-at-nigel-frank-international-3737169191,2023-12-17,Basildon, United Kingdom,Mid senior,Hybrid,"Data Analyst - 1 Year FTC - Up to £45,000
I'm currently working with not for profit company who are looking for an experienced Data Analyst with skills in Power BI and D365, to join their team. The role itself is a data analyst and you will work on projects across multiple departments, be involved in data gathering and cleansing, creating reports using Power BI and data analysis to provide insights and recommendations.
This is a salaried position of up to £45,000 depending on experience. They offer a hybrid working arrangement with 2 days on-site in their London office and the rest working from home.
I am looking for...
Previous experience in a similar role
Strong experience working with Power BI to create and maintain reports
Strong experience with CRM systems - ideally D365
Excellent communication skills
If this is of interest then get in touch ASAP. Send across your CV to d.moore1@nigelfrank.com or alternatively, give me a call on 0191 338 7577
Key Skills: Power BI, D365, CRM, Reports, Analysis, Insight, Data, Azure
Show more
Show less","Data Analysis, Power BI, D365, CRM, Reports, Insight, Azure","data analysis, power bi, d365, crm, reports, insight, azure","azure, crm, d365, dataanalytics, insight, powerbi, reports"
Lead Data Engineer - London/Hybrid - GBP80k,Nigel Frank International,"City of London, England, United Kingdom",https://uk.linkedin.com/jobs/view/lead-data-engineer-london-hybrid-gbp80k-at-nigel-frank-international-3781053996,2023-12-17,Basildon, United Kingdom,Mid senior,Hybrid,"Lead Data Engineer - London/Hybrid - £80k
Please note - this role requires you to be based in the UK. Unfortunately this role is not able to offer sponsorship.
I am working with on of the UK's leading public transport companies who are looking for a Data Engineer to join their team based in Central London. The organisation have invested significantly in their data function in the past 2 years with the aim for the business to make data-led decisions and generate in house data solutions.
They are currently working to develop their product roadmap and are looking to develop an in house Data Centre of Excellence. This team is the focal point for everything data related within the organisation and lead on projects to help predict future revenue, understand financial costs and pinch points and monitor operational activities. This is an exciting time to join the team and help shape the way that the organisation use their data moving forward.
As part of this role, you will be responsible for some of the following areas.
Create, develop and maintain robust data pipelines for data ingestion and transformation purposes
Work closely with other Data Engineers to create effective data models
Work collaboratively as part of the organisations data team on their data migration project
Engage with stakeholders from various departments to understand requirements and deliver them data that is required
This is a salaried role paying up to £65,000 per year depending on experience. This is a hybrid role where you will be required to work in the organisations Central London office on a weekly basis. There is also a company benefits package which includes -
Up to 10% annual salary
Subsided travel
25 days holiday
Private health insurance
To be successful in this role you will have:
Strong knowledge of SQL and Python
Commercial experience using the Azure Data Platform
Strong experience with Databricks
Experience creating, developing and maintaining ETL pipelines
To apply for this role submit your CV or contact David Airey on 0191 338 7508 or at d.airey@nigelfrank.com.
Show more
Show less","Data Engineering, Azure Data Platform, SQL, Python, Data Migration, Data Modeling, Data Pipelines, ETL Pipelines, Databricks, Stakeholder Engagement, Data Visualization, DataDriven Decision Making, Data Center of Excellence","data engineering, azure data platform, sql, python, data migration, data modeling, data pipelines, etl pipelines, databricks, stakeholder engagement, data visualization, datadriven decision making, data center of excellence","azure data platform, data center of excellence, data engineering, data migration, databricks, datadriven decision making, datamodeling, datapipeline, etl pipelines, python, sql, stakeholder engagement, visualization"
Senior Data Engineer,Mozilla,United States,https://www.linkedin.com/jobs/view/senior-data-engineer-at-mozilla-3787100084,2023-12-17,Caldwell,United States,Associate,Remote,"Why Mozilla?
Mozilla Corporation is the non-profit-backed technology company that has shaped the internet for the better over the last 25 years. We make pioneering brands like Firefox, the privacy-minded web browser, and Pocket, a service for keeping up with the best content online. Now, with more than
225
million people around the world using our products each month, we’re shaping the next 25 years of technology. Our work focuses on diverse areas including AI, social media, security and more. And we’re doing this while never losing our focus on our core mission – to make the internet better for everyone.
The Mozilla Corporation is wholly owned by the non-profit 501(c) Mozilla Foundation. This means we aren’t beholden to any shareholders — only to our mission. Along with
60,000
+ volunteer contributors and collaborators all over the world, Mozillians design, build and distribute
open-source
software that enables people to enjoy the internet on their terms.
About The Team & Role
Now more than ever, the Internet is a utility that facilitates modern life. At Mozilla, we take this to heart, striving to build products that keep the Internet open, accessible, and secure for everyone. We handle terabytes of data every day from millions of users to guide our decision-making processes. We need your help to enable the future of Mozilla in a way that makes us proud!
As a Data Engineer At Mozilla, Your Primary Area Of Focus Will Be On Our Analytics Engineering Team. This Team Focuses On Modeling Our Data So That The Rest Of Mozilla Has Access To It, In The Appropriate Format, When They Need It, To Help Them Make Data Informed Decisions. This Team Is Also Tasked With Helping To Maintain And Make Improvements To Our Data Platform. Some Recent Improvements Include Introducing a Data Catalog, Building In Data Quality Checks Among Others. Check Out The Data@Mozilla Blog For More Details On Some Of Our Work. You Will
work with other data engineers to design and maintain scalable data models and ETL pipelines.
help design, build, and improve the infrastructure for ingesting, storing, and transforming data at a scale of tens of terabytes per day.
help design and build systems to monitor and analyze data from Mozilla’s products.
work with data scientists to answer questions and guide product decisions.
General Professional Requirements
Proficiency with one or more of the programming languages used by our teams (SQL and Python).
Strong software engineering fundamentals: modularity, abstraction, data structures, and algorithms.
Ability to work collaboratively with a distributed team.
Specific Skills/Experience
Our team requires skills in a variety of domains. You should have proficiency in one or more of the areas listed below, and be interested in learning about the others.
You have used data to answer specific questions and guide company decisions.
You have experience building modular and reusable ETL/ELT pipelines in distributed databases
You are opinionated about data models and how they should be implemented. You partner with others to map out a business process, profile available data, design and build flexible data models for analysis.
You have experience recommending / implementing new data collection to help improve the quality of data models.
You have experience with data infrastructure: databases, message queues, batch and stream processing
You have experience with highly scalable distributed systems hosted on cloud providers (e.g. Google Cloud Platform)
About Mozilla
Mozilla exists to build the Internet as a public resource accessible to all because we believe that open and free is better than closed and controlled. When you work at Mozilla, you give yourself a chance to make a difference in the lives of Web users everywhere. And you give us a chance to make a difference in your life every single day. Join us to work on the Web as the platform and help create more opportunity and innovation for everyone online.
Commitment to diversity, equity, inclusion, and belonging
Mozilla understands that valuing diverse creative practices and forms of knowledge are crucial to and enrich the company’s core mission. We encourage applications from everyone, including members of all equity-seeking communities, such as (but certainly not limited to) women, racialized and Indigenous persons, persons with disabilities, persons of all sexual orientations, gender identities, and expressions.
We will ensure that qualified individuals with disabilities are provided reasonable accommodations to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment, as appropriate. Please contact us at hiringaccommodation@mozilla.com to request accommodation.
We are an equal opportunity employer. We do not discriminate on the basis of race (including hairstyle and texture), religion (including religious grooming and dress practices), gender, gender identity, gender expression, color, national origin, pregnancy, ancestry, domestic partner status, disability, sexual orientation, age, genetic predisposition, medical condition, marital status, citizenship status, military or veteran status, or any other basis covered by applicable laws. Mozilla will not tolerate discrimination or harassment based on any of these characteristics or any other unlawful behavior, conduct, or purpose.
Group: D
Req ID: R2413
To learn more about our Hiring Range System, please click this link.
Hiring Ranges
US Tier 1 Locations
$137,000—$200,000 USD
US Tier 2 Locations
$126,000—$185,000 USD
US Tier 3 Locations
$116,000—$170,000 USD
Show more
Show less","SQL, Python, data models, ETL pipelines, distributed databases, data infrastructure, databases, message queues, batch processing, stream processing, Google Cloud Platform","sql, python, data models, etl pipelines, distributed databases, data infrastructure, databases, message queues, batch processing, stream processing, google cloud platform","batch processing, data infrastructure, data models, databases, distributed databases, etl pipelines, google cloud platform, message queues, python, sql, stream processing"
Data Integrations Engineer,Lexia Learning,United States,https://www.linkedin.com/jobs/view/data-integrations-engineer-at-lexia-learning-3763820251,2023-12-17,Caldwell,United States,Associate,Remote,"Job Responsibilities
Responds to requests via phone, case/email, and conference calls for data integration and technical support with Lexia products.
Handles complex data integration cases and escalated customer requests
Champions Lexia customers by reproducing and reporting bugs and requesting features from our customers’ point of view
Collaborates with teammates and other departments to troubleshoot issues and advocate for customers
Knows all Lexia products and data integration methods, in-depth, in order to explain them to customers, both external and internal
Continually seeks to improve technical knowledge and service skills as well as Lexia’s understanding of and service to our customers
Has a direct impact in making our customers successful through ensuring the voice and perspective of the customer is heard and improving our products based on that perspective.
Creates content and updates documentation in the internal knowledgebase, templates, and help center articles
Collaborates with customers to gain a deep understanding of their intended outcomes, processes, system environments, and data infrastructure.
Job Requirements
Minimum 3 years experience in technical customer support
Education or EdTech experience preferred
Superb communication skills, both written and spoken
Excellent collaborator with a proactive mindset
Strong knowledge of Software as a Service, web browser technologies, network technologies, SQL, Excel, and basic database concepts
Solid knowledge of data integration methods; previous knowledge of Clever or OneRoster integration is a plus
Familiarity with case tracking and customer relationship management software
To learn more about our organization and the exciting work we do, visit https://www.lexialearning.com/
An Equal Opportunity Employer
We are dedicated to fostering a culture that celebrates unique backgrounds, ideas, and experiences. All qualified applicants will receive consideration for employment without discrimination on the basis of race, color, age, religion, sex, gender, gender identity/expression, sexual orientation, national origin, protected veteran status, or disability.
Show more
Show less","Data Integration, Technical Support, Case Management, Customer Service, Issue Troubleshooting, Data Warehousing, Cloud Computing, Agile Development, SQL, Excel, Data Mining, Software as a Service, Web Technologies, Network Technologies, Data Analysis, Data Visualization, DevOps, Git, Jenkins, Jira","data integration, technical support, case management, customer service, issue troubleshooting, data warehousing, cloud computing, agile development, sql, excel, data mining, software as a service, web technologies, network technologies, data analysis, data visualization, devops, git, jenkins, jira","agile development, case management, cloud computing, customer service, data integration, data mining, dataanalytics, datawarehouse, devops, excel, git, issue troubleshooting, jenkins, jira, network technologies, software as a service, sql, technical support, visualization, web technologies"
Data Engineering Consultant,Nigel Frank International,"Norwich, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineering-consultant-at-nigel-frank-international-3740004189,2023-12-17,Norwich, United Kingdom,Mid senior,Onsite,"I am working with a Microsoft Partnered consultancy who are looking for a Data Engineering Consultant to join their growing team. You will have the opportunity to work on a variety of client projects across a number of different sectors such as retail, finance and the public sector.
In this role you will utilise your experience with the latest Azure technologies to provide guidance to clients. As a hands-on consultant you use your experience as a data engineer to ensure the timely delivery on clients data driven projects.
You will be joining a people centred business, who invest heavily in its team. You will be given the opportunity to explore other areas of technologies that may be of interest to you, including data science, machine learning and AI. You will have time designated purely to training and development, with internal development schemes as well as funded Microsoft learning, training courses and certifications.
This is an exciting time to join a hugely successful and growing business who have a number of exciting roadmap of projects planned all the way into 2024!
As part of this role, you will be responsible for some of the following areas.
Offer guidance to clients on data driven projects
Take the lead on data engineering projects focused around the Microsoft and Azure tech stack
Deliver on all aspects of the project including requirements gathering, testing, implementation and maintenance
This is a salaried role paying up to £55,000 per annum depending on experience and a company benefits package. This is a home based role with occasional, fully expensed visits to client sites as and when required. You will also need to commute to company retreats once per quarter to meet other members of the team.
To be successful in the role you will have.
Experience working with the Azure tech stack including Azure Data Factory, Synapse and Azure Data Lake.
Experience designing and implementing ETL solutions.
Coding experience with languages such as SQL and Python.
Strong knowledge of Databricks for data ingestion and transformation would be beneficial
Excellent communication skills
This is just a brief overview of the role. For the full information, simply apply to the role with your CV, and I will call you to discuss further. My client is looking to begin the interview process ASAP, so don't miss out, APPLY now! To do so please email me at a.pinkerton@nigelfrank.com or call me on 0191 3387487.
Nigel Frank International are the go-to recruiter for Power BI and Azure Data Platform roles in the UK offering more opportunities across the country than any other recruitment agency. We're the proud sponsor and supporter of SQLBits, Power Platform World Tour, the London Power BI User Group, Newcastle Power BI User Group and Newcastle Data Platform and Cloud User Group. We are the global leaders in Microsoft recruitment.
Show more
Show less","Azure Data Platform, Azure Synapse, Azure Data Lake, ETL Solutions, SQL, Python, Databricks, Data Ingestion, Data Transformation","azure data platform, azure synapse, azure data lake, etl solutions, sql, python, databricks, data ingestion, data transformation","azure data lake, azure data platform, azure synapse, data ingestion, data transformation, databricks, etl solutions, python, sql"
"Data Governance Specialist - Nielsen Brandbank, Norwich",NielsenIQ,"Norwich, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-governance-specialist-nielsen-brandbank-norwich-at-nielseniq-3784375910,2023-12-17,Norwich, United Kingdom,Mid senior,Onsite,"Job Description
The Data Governance Specialist is responsible for flexibly supporting the successful delivery of improvements to our data model.
This varied role will support in all aspects of change management to our data model and systems, including: data transformation, data testing, systems User Acceptance Testing (UAT), and document creation.
You will work as part of a small, focused team alongside the Mapping and Testing Leads to ensure all change milestones are successfully met as part of our quarterly release process.
Full training will be given for this role.
You will:
Define the human readable transformation instructions needed for data flows between internal systems
Perform UAT of changes to internal and external applications
Define the scenarios needed to test data flows between internal systems
Create test data in internal and external applications
Run internal tests to prove transformation instructions are working as intended
Support external communications on the scope of changes for each release
Update external facing documentation and guides to support customer experience
Source local translations for data model elements and codelist values
Maintain internal documentation and training in line with quarterly release changes
Run end to end process tests
Contribute to the timely delivery of quarterly data model releases
Provide support to customers, helping to answer queries and resolve any data-related issues
Provide internal support, helping to answer queries relating to data transformation
Qualifications
You have:
Degree-level graduate, or equivalent experience and knowledge of data processes
Good understanding of E-Commerce or FMCG data
Ideally previous experience working with data models, data transformation
A career history that demonstrates the attributes below
You are:
Organised and meticulous
Logical, objective thinker
Analytical mindset
Self-starter
Excellent time-management
Enjoys solving complex problems
Quick to absorb new information
Accountable for delivering on time
Good collaboration with strong English skills
Additional Information
About NIQ
NIQ is the world’s leading consumer intelligence company, delivering the most complete understanding of consumer buying behavior and revealing new pathways to growth. In 2023, NIQ combined with GfK, bringing together the two industry leaders with unparalleled global reach. With a holistic retail read and the most comprehensive consumer insights—delivered with advanced analytics through state-of-the-art platforms—NIQ delivers the Full View™.
NIQ, is an Advent International portfolio company with operations in 100+ markets, covering more than 90% of the world’s population. For more information, visit NIQ.com.
Want to keep up with our latest updates?
Follow us on: LinkedIn | Instagram | Twitter | Facebook
Our commitment to Diversity, Equity, and Inclusion
NIQ is committed to reflecting the diversity of the clients, communities, and markets we measure within our own workforce. We exist to count everyone and are on a mission to systematically embed inclusion and diversity into all aspects of our workforce, measurement, and products. We enthusiastically invite candidates who share that mission to join us.
We are proud to be an Equal Opportunity/Affirmative Action-Employer, making decisions without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability status, age, marital status, protected veteran status or any other protected class. Our global non-discrimination policy covers these protected classes in every market in which we do business worldwide.
Learn more about how we are driving diversity and inclusion in everything we do by visiting the NielsenIQ News Center: https://nielseniq.com/global/en/news-center/diversity-inclusion/
NIQ or any of our subsidiaries will never ask you for money at any point of the recruitment or onboarding process.
Show more
Show less","Data Transformation, Data Testing, User Acceptance Testing (UAT), Data Modeling, Data Flows, ECommerce Data, FMCG Data, Data Processes, StateoftheArt Platforms, Agile Development, Analytical Mindset, SelfStarter, Time Management, Collaboration, Strong English Skills","data transformation, data testing, user acceptance testing uat, data modeling, data flows, ecommerce data, fmcg data, data processes, stateoftheart platforms, agile development, analytical mindset, selfstarter, time management, collaboration, strong english skills","agile development, analytical mindset, collaboration, data flows, data processes, data testing, data transformation, datamodeling, ecommerce data, fmcg data, selfstarter, stateoftheart platforms, strong english skills, time management, user acceptance testing uat"
Junior Data Engineer/Analyst,Gravity Global,"Norwich, England, United Kingdom",https://uk.linkedin.com/jobs/view/junior-data-engineer-analyst-at-gravity-global-3786559368,2023-12-17,Norwich, United Kingdom,Mid senior,Hybrid,"Gravity Global is the world’s most awarded marketing consultancy specializing in brands operating in complex markets. Led by strategy, innovation, technology, data and creativity, we create brand-and-demand marketing programmes that drive transformational growth for our clients.
With 14 wholly owned offices across the US, Europe, and APAC, you can develop a truly global career in a rapidly scaling organisation.
We are looking for a Junior Data Engineer / Analyst to join our Performance Marketing team in Norwich.
Key Responsibilities:
Maintaining cloud data warehouses, ensuring organised, available, and secure.
Hands on experience designing, building pipelines, and supporting data platforms across Google data products, e.g., Google Cloud Platform – BigQuery.
Be required to showcase your GCP Data Engineering experience when communicating with clients and their requirements, turning these into technical data solutions.
Experience working with REST API
Guarantee effective client reporting by recommending and monitoring appropriate campaign metrics and KPIs, as well as having a practical involvement in ensuring the relevance and quality of the agencies in-house reporting platform.
To provide expert insight into website and channel usage statistics, typically via an in-depth understanding of GA4 and BigQuery, and the ability to produce meaningful report summaries.
To integrate client data with campaign data where necessary, working with clients to build a data integration strategy based on their tech stack and security requirements.
To devise and implement internal research and development projects to quantify and evaluate different search marketing techniques.
To provide data, analysis and research skills across the business wherever required including monthly reporting and quarterly review documentation for retained digital marketing clients.
Experience / Skills Required:
A minimum of 1-2 years’ experience in a Data related role.
Agency experience is desirable.
Experience in building data solutions using JavaScript, HTML, Python or PHP & API scripting.
A strong knowledge of relational databases and SQL.
Practical use of cloud applications (Azure, AWS, GCP).
Must be able to use reporting platforms to visualise and present data insight.
Experience of using Microsoft Applications (Excel, PowerPoint, Teams, SharePoint, etc.)
Excellent communication skills to understand and pass on requirements internally and externally as well as identify, visualise, and explain key insights.
Confident in delivering and presenting your analysis to both colleagues and clients.
Able to work individually and collaboratively with UK and US teams.
Are clear and persuasive in both verbal and written communication.
What we can offer:
Competitive remuneration package
Global career development opportunities
Hybrid working with 2 days in the office (usual hours 9 - 5:30pm, Monday to Friday), however you may be required to work up to 4 days in the office during your probationary period
Opportunities to make a difference and get involved with our working groups focusing on areas such as wellbeing and ESG / Sustainability
Supportive and inclusive policies including, but not limited to, family friendly policies such as adoption, maternity and paternity, and equal opportunities
To work with the best and most exciting clients
To work on global accounts in the most challenging sectors
To work with the best brains and creative talent in the industry
To learn and develop with access to industry specific training, including our in-house training modules
To work on innovative and ground-breaking marketing solutions
To join a team of hard-working specialists who like to have fun along the way!
If your experience doesn't align perfectly with the job criteria, but you meet some of the criteria and are excited about the role and Gravity, please don't be put off applying! We would encourage you to apply for the role as you may be just the right person for this role or other roles.
Gravity Global is an equal opportunities employer and we do not discriminate on the basis of sex, race, religion, age, sexuality, disability and other protected status as required by applicable law.
Show more
Show less","Data Engineering, Cloud Data Warehouses, Data Platforms, Google Cloud Platform, BigQuery, REST API, GA4, SQL, JavaScript, HTML, Python, PHP, API Scripting, Microsoft Applications, Excel, PowerPoint, Teams, SharePoint","data engineering, cloud data warehouses, data platforms, google cloud platform, bigquery, rest api, ga4, sql, javascript, html, python, php, api scripting, microsoft applications, excel, powerpoint, teams, sharepoint","api scripting, bigquery, cloud data warehouses, data engineering, data platforms, excel, ga4, google cloud platform, html, javascript, microsoft applications, php, powerpoint, python, rest api, sharepoint, sql, teams"
Principal Data Scientist,Aviva,"Norwich, England, United Kingdom",https://uk.linkedin.com/jobs/view/principal-data-scientist-at-aviva-3763896736,2023-12-17,Norwich, United Kingdom,Mid senior,Hybrid,"Principal Data Scientist
Salary £50,000 – 80,000
An exciting opportunity has arisen in our high-profile data science team helping shape the future of insurance at Aviva through cutting-edge analytics.
The candidate will have several years’ experience in data science with a track record of solving problems and experience of leading a team of data scientists delivering significant business benefit is a prerequisite. The ideal candidate will also have experience of designing, building, and implementing sophisticated Fraud and Financial Crime models.
Our ideal candidate will be a confident leader, with strong stakeholder management and influencing skills.
A bit about the job:
You’ll work with one of the richest insurance datasets in the world, applying sophisticated techniques and technology to all stages of the data science process.
You’ll lead a team helping to formulate problems, extract, and prepare data, apply modelling, and then deploy the solution.
Aviva are investing in data science, machine learning and advanced analytics as a distinctive capability. The UK team is recognised as a centre of excellence in our Global Data Science Practice.
Join us, and you’ll have the chance to define a significant & measurable impact on Aviva.
Skills and experience we’re looking for:
· Highly numerate and educated to degree or postgraduate (MSc) in a meaningful field - Mathematics, Statistics, Computer Science
· You will bring several years’ experience with a track record of solving complex non-routine problems; along with expertise in some, if not all, of the following areas: Statistics, Machine Learning, Deep Learning & AI
· Excellent communication and stakeholder management skills - able to communicate complex ideas in an understandable way often to non-specialists
· Ability to deliver projects, work collaboratively with operational teams and can think and act strategically
What you’ll get for this role:
Our purpose - with you today, for a better tomorrow – is a promise we make to our colleagues too. And one of the ways we live up to that promise is by investing in you. We have so much to offer when it comes to being an Aviva colleague.
· Starting salary between £50,000 and £80,000 (depending on skills, experience, and qualifications)
· Bonus opportunity - 12% of annual salary. Actual amount depends on your performance and Aviva’s.
· Generous pension scheme - Aviva will contribute up to 14%, depending on what you put in.
· 29 days holiday plus bank holidays, and you can choose to buy or sell up to 5 days.
· Make your money go further - Up to 40% discount on Aviva products, and other retailer discounts.
· Up to £1,200 of free Aviva shares per year through our Matching Share Plan and share in the success of Aviva with our Save As You Earn scheme.
· Brilliantly supportive policies including parental and carer’s leave.
· Flexible benefits to suit you, including sustainability options such as cycle to work.
· Make a difference, be part of our Aviva Communities and use your 3 paid volunteering days to help others.
· We take your wellbeing seriously with lots of support and tools.
Take a look to learn more. Put a salary into this calculator to see what your total Aviva Reward could be.
Aviva is for everyone:
We’re inclusive and welcome everyone – we want applications from people with diverse backgrounds and experiences.
Excited but not sure you tick every box? Research tells us that women, particularly, feel this way. So, regardless of gender, why not apply. And if you’re in a job share just apply as a pair.
We flex locations, hours and working patterns to suit our customers, business, and you. Most of our people are smart working –spending around 50% of their time in our offices every week - combining the benefits of flexibility, with time together with colleagues.
We interview every disabled applicant who meets the minimum criteria for the job. Once you’ve applied, please send us an email stating that you have a disclosed disability, and we’ll interview you.
We’d love it if you could submit your application online. If you require an alternative method of applying, please give Matthew Drew a call on 0121 234 7664 or send an email to matthew.drew@aviva.com.
Show more
Show less","Data Science, Machine Learning, Deep Learning, Artificial Intelligence, Statistics, Mathematics, Computer Science, Communication, Stakeholder Management, Leadership, Project Management","data science, machine learning, deep learning, artificial intelligence, statistics, mathematics, computer science, communication, stakeholder management, leadership, project management","artificial intelligence, communication, computer science, data science, deep learning, leadership, machine learning, mathematics, project management, stakeholder management, statistics"
Data Analyst I,Commonwealth of Massachusetts,"Chelsea, MA",https://www.linkedin.com/jobs/view/data-analyst-i-at-commonwealth-of-massachusetts-3761487531,2023-12-17,Gloucester,United States,Mid senior,Onsite,"Job Description - Data Analyst I (23000BB0)
Job Description
Data Analyst I - ( 23000BB0 )
Description
Agency Overview:
The Department of Revenue's core mission is to collect the revenues required to support the business of the Commonwealth, to make a difference in the lives of children by enforcing the financial responsibilities of parenthood, and to assist Massachusetts cities and towns in sound and efficient fiscal management. Our vision is to conduct our operations in a manner that inspires public confidence in government by administering and enforcing tax, child support and municipal finance laws in ways that are innovative, customer-focused and transparent.
Division Overview:
The Tax Administration Division is responsible for fostering voluntary compliance and narrowing the tax gap by identifying, educating, and auditing noncompliant taxpayers. The Tax Administration Division audits the tax returns of businesses and individuals filed, including sales/use, meals, room’s occupancy, and special fuels are subject to examination by the Tax Administration Division staff.
Duties & Responsibilities:
The Massachusetts Department of Revenue Tax Division Business Innovation is seeking to hire Data Analyst I who will be assisting with the new application development and enhancing the current applications to meet agency needs.
Will be responsible for helping to drive key big data-based initiatives that will generate significant benefits throughout the audit division, reviewing and completing requests in the enhancement of the new and the existing applications/reports and assisting with the development and supporting any other agency applications/reports.
Will also perform unit testing by preparing test data, conducting test runs, and reviewing input and output data.
Evaluate problem statements and requests for information, gather business requirements, and creatively develop solutions, generating accurate queries and outputs to satisfy business needs.
Select appropriate data sources, develop queries to extract data from various data sources, verify the accuracy of the information and analyze the output to identify patterns and trends in the data.
Write comprehensive reports explaining results of queries and outputs. Produce standard and ad hoc reports using SQL, PLSQL, other tools and data sources.
Produce business rules and other project documentation based on DOR standards.
Conduct advanced data mining, statistical analysis, and research activities.
Manage a dynamic workload of development, testing, and research tasks, ensuring key deadlines are met.
Design and develop utility programs to automate manual operations, when needed.
Conduct data matches for the purpose of improving collections, enforcement, and data reliability.
Identify all audit and external source data files and variables needed to produce specific output requirements utilizing both data extracts and Data Warehouse.
Qualifications:
Proficient with SQL/PL SQL.
Knowledge of Microsoft applications including Excel, Teams, and Word.
Strong analytical, troubleshooting, and problem-solving skills.
Develop, maintain, and troubleshoot the existing applications.
Experience in business intelligence analysis and design.
Experience with predictive collections models and a risked based scoring process to forecast behavior, to better understand our customers, and to determine what metrics are critical to the organization.
Experience in the integration of business intelligence data into the daily operations of the organization to prioritize actions that will result in the greatest return.
Ability to design programs to gather and manipulate appropriate data to accommodate requests for reports and/or analysis, and then translate that analysis into management reports.
Ability to communicate effectively orally and in writing; and handle confidential assignments.
Ability to work independently/ with minimal supervision, handle multiple tasks simultaneously and adapt quickly to changes.
Perform unit testing by preparing test data, conducting test runs, and reviewing input and output data.
Preferred Qualifications:
Well versed with Microsoft Excel and other office applications.
Knowledge of R/R-Studio or Python.
Experience working with Jira and AWS.
Experience working with Tableau.
Expert knowledge of SQL/PL SQL programming.
Experience in design and developing of applications using .Net
Applicants must have excellent oral and written communications skills as well as excellent interpersonal skills.
Preferred candidate will have knowledge and experience with the following tax filing requirements: Individual Income tax, Corporate Excise Tax, Partnerships, Banking and Insurance and Estate Tax.
The preferred candidate will have a demonstrated record of success in team settings.
The preferred candidate must exercise sound judgement, handle confidential information with discretion.
Prior experience performing system testing and documenting results is a plus.
Demonstrated ability to meet deadlines and prioritize time sensitive assignment in a fast-paced environment is expected.
Ability to analyze and determine the applicability of electronic data processing data, to draw conclusions and make appropriate recommendations is also expected.
An external candidate recommended for a position with the Department of Revenue will be subject to a background check that includes checks for: state and federal tax compliance; child support compliance; education verification; Massachusetts CORI checks; criminal background checks in state(s) of current and previous residence/employment; validation of eligibility to work in the United States; and national fingerprint-based criminal background checks.
As part of the Future of Work initiative, the agency is adopting a hybrid work model.  Hybrid work is a mix of in-office work and telework. This position will work remotely some days and in-person on other days.
Qualifications
First consideration will be given to those applicants that apply within the first 14 days.
Minimum Entrance Requirements:
Applicants must have (A) at least two years of full-time or equivalent part-time professional or technical experience performing related duties that include business data recovery, analysis, query design, reporting and statistical analysis; or (B) any equivalent combination of the required experience and the substitutions below
Substitutions:
I. A Bachelor’s degree or higher with a major in Mathematics, Economics, Computer Science, Information Management, Statistics or a related field may be substituted for the required experience
Comprehensive Benefits
When you embark on a career with the Commonwealth, you are offered an outstanding suite of employee benefits that add to the overall value of your compensation package. We take pride in providing a work experience that supports you, your loved ones, and your future.
Want the specifics? Explore our Employee Benefits and Rewards!
An Equal Opportunity / Affirmative Action Employer. Females, minorities, veterans, and persons with disabilities are strongly encouraged to apply.
The Commonwealth is an Equal Opportunity Employer and does not discriminate on the basis of race, religion, color, sex, gender identity or expression, sexual orientation, age, disability, national origin, veteran status, or any other basis covered by appropriate law. Research suggests that qualified women, Black, Indigenous, and Persons of Color (BIPOC) may self-select out of opportunities if they don't meet 100% of the job requirements. We encourage individuals who believe they have the skills necessary to thrive to apply for this role.
Official Title : Data Analyst I
Primary Location
: United States-Massachusetts-Chelsea-200 Arlington Street
Job
: Information Systems and Technology
Agency
: Department of Revenue
Schedule
: Full-time
Shift
: Day
Job Posting
: Oct 26, 2023, 7:46:04 PM
Number of Openings
: 1
Salary
: 71,428.50 - 102,981.06 Yearly
If you have Diversity, Affirmative Action or Equal Employment Opportunity questions or need a Reasonable Accommodation, please contact Diversity Officer / ADA Coordinator : Allan Motenko - 6176263410
Bargaining Unit : 06-NAGE - Professional Admin.
Confidential : No
Potentially Eligible for a Hybrid Work Schedule : Yes
Refer a candidate for this job
Tell us about a friend who might be interested in this job. All privacy rights will be protected.
Refer a candidate
Guides
Apply for a Job
Apply for a Job for JAWS Users
Create a Profile
Create a Profile for JAWS Users
System Requirements
Show more
Show less","SQL, PLSQL, Microsoft Excel, Microsoft Teams, Microsoft Word, Data mining, Statistical analysis, R, RStudio, Python, Jira, AWS, Tableau, .Net, Electronic data processing","sql, plsql, microsoft excel, microsoft teams, microsoft word, data mining, statistical analysis, r, rstudio, python, jira, aws, tableau, net, electronic data processing","aws, data mining, electronic data processing, jira, microsoft excel, microsoft teams, microsoft word, net, plsql, python, r, rstudio, sql, statistical analysis, tableau"
Data Analyst I,Commonwealth of Massachusetts,"Chelsea, MA",https://www.linkedin.com/jobs/view/data-analyst-i-at-commonwealth-of-massachusetts-3779323229,2023-12-17,Gloucester,United States,Mid senior,Onsite,"Job Description - Data Analyst I (23000D9L)
Job Description
Data Analyst I - ( 23000D9L )
Description
Agency Overview:
The Department of Revenue's core mission is to collect the revenues required to support the business of the Commonwealth, to make a difference in the lives of children by enforcing the financial responsibilities of parenthood, and to assist Massachusetts cities and towns in sound and efficient fiscal management. Our vision is to conduct our operations in a manner that inspires public confidence in government by administering and enforcing tax, child support and municipal finance laws in ways that are innovative, customer-focused and transparent.
Division Overview:
The Tax Administration Division is responsible for fostering voluntary compliance and narrowing the tax gap by identifying, educating, and auditing noncompliant taxpayers. The Tax Administration Division audits the tax returns of businesses and individuals filed, including sales/use, meals, rooms occupancy and special fuels, are subject to examination by the Tax Administration Division staff.
Duties & Responsibilities:
The Massachusetts Department of Revenue Tax Division Data Analytics team is seeking to hire a Data Analyst I who will be assisting with the new application development and enhancing the current applications to meet agency needs. The person will be responsible for helping to drive key big data-based initiatives that will generate significant benefits throughout the audit division, reviewing and completing requests in the enhancement of the new and the existing applications/reports and assisting with the development and supporting any other agency applications/reports. The Data Analyst I will also perform unit testing by preparing test data, conducting test runs, and reviewing input and output data.This role will:
Assist in generating audit queries from within Genisys and then assign audit leads to their examiners.
Evaluate requests, gather business requirements, and creatively develop solutions, generating accurate queries and outputs to satisfy business requirements and needs.
Write comprehensive reports explaining results of queries and outputs.
Produce standard and ad hoc reports using SQL, PLSQL, other tools and data sources.
Produce business rules and other project documentation based on DOR standards.
Conduct advanced data mining, statistical analysis and research activities.
Manage a dynamic workload of development, testing and research tasks, ensuring key deadlines are met.
Design and develop utility programs to automate manual operations, when needed.
Conduct data matches for the purpose of improving collections, enforcement, and data reliability.
Identify all audit and external source data files and variables needed to produce specific output requirements utilizing both data extracts and Data Warehouse.
Qualifications:
Proficient with SQL/PL SQL.
Experience in SQL Server 2012 and up.
Proficient with at least one computer programming language such as JAVA, C#, .Net, Python, XML, etc.
Knowledge of Microsoft applications including Excel, Teams and Word.
Strong analytical, troubleshooting, and problem-solving skills.
Develop, maintain, and troubleshoot the existing applications.
Experience in business intelligence analysis and design
Experience with predictive collections models and a risked based scoring process to forecast behavior, to better understand our customers, and to determine what metrics are critical to the organization.
Experience in the integration of business intelligence data into the daily operations of the organization to prioritize actions that will result in the greatest return.
Ability to design programs to gather and manipulate appropriate data to accommodate requests for reports and/or analysis, and then translate that analysis into management reports.
Ability to communicate effectively orally and in writing; and handle confidential assignments.
Ability to work independently/ with minimal supervision, handle multiple tasks simultaneously and adapt quickly to changes.
Perform unit testing by preparing test data, conducting test runs, and reviewing input and output data.
Preferred Qualifications:
Well versed with Microsoft Excel and other office applications.
Knowledge of R/R-Studio or Python.
Experience working with Jira and AWS.
Experience working with Tableau.
Expert knowledge of SQL/PL SQL programming.
Experience in design and developing of applications using .Net
Good knowledge of MA and IRS tax laws, policies and procedures.
An external candidate recommended for a position with the Department of Revenue will be subject to a background check that includes checks for: state and federal tax compliance; child support compliance; education verification; Massachusetts CORI checks; criminal background checks in state(s) of current and previous residence/employment; validation of eligibility to work in the United States; and national fingerprint-based criminal background checks.
As part of the Future of Work initiative, the agency is adopting a hybrid work model.  Hybrid work is a mix of in-office work and telework. This position will work remotely some days and in-person on other days.
Qualifications
First consideration will be given to those applicants that apply within the first 14 days.
MINIMUM ENTRANCE REQUIREMENTS: Applicants must have (A) at least two years of full-time or equivalent part-time professional or technical experience performing related duties that include business data recovery, analysis, query design, reporting and statistical analysis; or (B) any equivalent combination of the required experience and the substitutions below
Substitutions:
I. A Bachelor’s degree or higher with a major in Mathematics, Economics, Computer Science, Information Management, Statistics or a related field may be substituted for the required experience
Comprehensive Benefits
When you embark on a career with the Commonwealth, you are offered an outstanding suite of employee benefits that add to the overall value of your compensation package. We take pride in providing a work experience that supports you, your loved ones, and your future.
Want the specifics? Explore our Employee Benefits and Rewards!
An Equal Opportunity / Affirmative Action Employer. Females, minorities, veterans, and persons with disabilities are strongly encouraged to apply.
The Commonwealth is an Equal Opportunity Employer and does not discriminate on the basis of race, religion, color, sex, gender identity or expression, sexual orientation, age, disability, national origin, veteran status, or any other basis covered by appropriate law. Research suggests that qualified women, Black, Indigenous, and Persons of Color (BIPOC) may self-select out of opportunities if they don't meet 100% of the job requirements. We encourage individuals who believe they have the skills necessary to thrive to apply for this role.
Official Title : Data Analyst I
Primary Location
: United States-Massachusetts-Chelsea-200 Arlington Street
Job
: Information Systems and Technology
Agency
: Department of Revenue
Schedule
: Full-time
Shift
: Day
Job Posting
: Dec 11, 2023, 9:57:11 PM
Number of Openings
: 1
Salary
: 71,428.50 - 102,981.06 Yearly
If you have Diversity, Affirmative Action or Equal Employment Opportunity questions or need a Reasonable Accommodation, please contact Diversity Officer / ADA Coordinator : Allan Motenko - 6176263410
Bargaining Unit : 06-NAGE - Professional Admin.
Confidential : No
Potentially Eligible for a Hybrid Work Schedule : Yes
Refer a candidate for this job
Tell us about a friend who might be interested in this job. All privacy rights will be protected.
Refer a candidate
Guides
Apply for a Job
Apply for a Job for JAWS Users
Create a Profile
Create a Profile for JAWS Users
System Requirements
Show more
Show less","SQL, PL/SQL, SQL Server 2012, JAVA, C#, .Net, Python, XML, Microsoft Office Suite (Excel Teams Word), Microsoft Excel, R/RStudio, Python, Jira, AWS, Tableau, Data Analytics, Mathematical Modeling, Statistical Analysis, Business Intelligence","sql, plsql, sql server 2012, java, c, net, python, xml, microsoft office suite excel teams word, microsoft excel, rrstudio, python, jira, aws, tableau, data analytics, mathematical modeling, statistical analysis, business intelligence","aws, business intelligence, c, dataanalytics, java, jira, mathematical modeling, microsoft excel, microsoft office suite excel teams word, net, plsql, python, rrstudio, sql, sql server 2012, statistical analysis, tableau, xml"
Entry Level Data Analyst/Management Consultant - Nationwide (US Based Candidates Only),Arcadis,"Wakefield, MA",https://www.linkedin.com/jobs/view/entry-level-data-analyst-management-consultant-nationwide-us-based-candidates-only-at-arcadis-3701467665,2023-12-17,Gloucester,United States,Mid senior,Onsite,"Arcadis is the world's leading company delivering sustainable design, engineering, and consultancy solutions for natural and built assets.
We are more than 36,000 people, in over 70 countries, dedicated to improving quality of life. Everyone has an important role to play. With the power of many curious minds, together we can solve the world’s most complex challenges and deliver more impact together.
Role description:
Note: See below regarding the nature of this position being a prospecting position.
Arcadis is currently seeking Analysts and Junior Management Consultants to join our world-class Business Advisory practice nationwide.
We are looking for candidates who want to apply technical know-how, combined with business principles, to the water, wastewater, and stormwater industry. We want dedicated, creative, and energetic candidates interested in tackling challenges and developing sustainable solutions to address water issues like renewal and replacement of aging infrastructure, funding of capital improvements, water supply, workforce retention and development, and emergency preparedness. Collaborating with our experienced consulting professionals, you will support and contribute to project outcomes; interact, and work with clients, and develop your technical capabilities.
We are a People First company, industry thought leaders, and drivers and allies of utility innovation.
Our passion: to Improve Quality of Life.
Our approach: to delight our clients by developing successful long-term partnerships and supporting them to address existing and emerging challenges.
Arcadis provides multiple onboarding and development programs created for young professionals that support professional growth and help drive creativeness, innovation, and greater integration within our local, National and global teams.
Role accountabilities:
What will you do?
Assess, develop, and support a variety of management consultant projects including performing data analytics, financial analysis, operational and organizational assessments, condition assessments, vulnerability, and mitigation assessments, as well as planning and development for utilities, municipalities, and cities’ (primarily water/wastewater/stormwater utilities).
Utilize strong analytical skills and ability to apply logic to solve problems.
Support teams in tasks ranging from general fieldwork to technical office-based analysis.
Assist in technical writing which may include preparation of technical reports, business development support, presentations, and other audiovisual materials.
Work independently and as part of a team, with the flexibility to accommodate collaboration with team members across the U.S. and internationally.
Manage multiple concurrent projects with multiple deadlines, ensuring completion per project budgets and timelines.
What skills will you need?
Reliable, client-focused, and capable of working independently under the supervision of project managers.
Exceptional analytical and problem-solving skills, strong attention to detail, organization skills, and work ethic.
Self-motivated and team-oriented, with the ability to work successfully both independently and within a team.
Ability to balance and address new challenges as they arise and an eagerness to take ownership of tasks.
Knowledge of engineering concepts, theories, and practices related to water/wastewater/stormwater.
Drive to succeed and grow a career in the utility industry
Qualifications & Experience:
Required Qualifications:
Masters of Science degree in Civil or Environmental Engineering, or closely related STEM discipline; or business analytics/MBA, MS in data science or related business discipline.
For those with engineering degrees, ability to obtain the EIT within six months of start date
Preferred Qualifications:
Previous relevant consulting or utility experience, either internship or full-time.
Experience applying programming languages and analytics to problem-solving is a plus
SharePoint, Building Information Modeling (BIM), Power BI, Excel, PowerPoint, Visio, Change Management skills, and/or Augmented Reality experience
This is a general job posting and not tied to a specific current open position. Please make sure you create a search agent to be alerted of specific opportunities of interest. Candidates who submit their resume to this posting may be considered for all future openings as they arise.
Why Arcadis?
We can only achieve our goals when everyone is empowered to be their best. We believe everyone's contribution matters. It’s why we are pioneering a skills-based approach, where you can harness your unique experience and expertise to carve your career path and maximize the impact we can make together.
You’ll do meaningful work, and no matter what role, you’ll be helping to deliver sustainable solutions for a more prosperous planet. Make your mark, on your career, your colleagues, your clients, your life and the world around you.
Together, we can create a lasting legacy.
Join Arcadis. Create a Legacy.
Our Commitment to Equality, Diversity, Inclusion & Belonging
We want you to be able to bring your best self to work every day which is why we take equality and inclusion seriously and hold ourselves to account for our actions. Our ambition is to be an employer of choice and provide a great place to work for all our people. We are an equal opportunity and affirmative action employer. Women, minorities, people with disabilities and veterans are strongly encouraged to apply. We are dedicated to a policy of non-discrimination in employment on any basis including race, creed, color, religion, national origin, sex, age, disability, marital status, sexual orientation, gender identity, citizenship status, disability, veteran status, or any other basis prohibited by law.
Arcadis offers benefits for full time and part time positions. These benefits include medical, dental, and vision, EAP, 401K, STD, LTD, AD&D, life insurance, paid parental leave, reward & recognition program and optional benefits including wellbeing benefits, adoption assistance and tuition reimbursement. We offer seven paid holidays and potentially up to two floating holidays per calendar year depending on start date, and 15 days PTO that accrue per year. The salary range for this position is $52000 - 89700 / year.
#ANACollege
Show more
Show less","Data analytics, Financial analysis, Operational and organizational assessments, Condition assessments, Vulnerability and mitigation assessments, Planning and development, Analytical skills, Problemsolving skills, Attention to detail, Organizational skills, Work ethic, Selfmotivation, Teamwork, Ability to balance and address new challenges, Ownership of tasks, Civil engineering, Environmental engineering, STEM discipline, Business analytics, MBA, Data science, EIT, Programming languages, SharePoint, Building Information Modeling (BIM), Power BI, Excel, PowerPoint, Visio, Change Management skills, Augmented Reality, Consulting, Utility experience, Internship, Fulltime","data analytics, financial analysis, operational and organizational assessments, condition assessments, vulnerability and mitigation assessments, planning and development, analytical skills, problemsolving skills, attention to detail, organizational skills, work ethic, selfmotivation, teamwork, ability to balance and address new challenges, ownership of tasks, civil engineering, environmental engineering, stem discipline, business analytics, mba, data science, eit, programming languages, sharepoint, building information modeling bim, power bi, excel, powerpoint, visio, change management skills, augmented reality, consulting, utility experience, internship, fulltime","ability to balance and address new challenges, analytical skills, attention to detail, augmented reality, building information modeling bim, business analytics, change management skills, civil engineering, condition assessments, consulting, data science, dataanalytics, eit, environmental engineering, excel, financial analysis, fulltime, internship, mba, operational and organizational assessments, organizational skills, ownership of tasks, planning and development, powerbi, powerpoint, problemsolving skills, programming languages, selfmotivation, sharepoint, stem discipline, teamwork, utility experience, visio, vulnerability and mitigation assessments, work ethic"
Python + Data Structures Engineer,iTech Solutions,"San Jose, CA",https://www.linkedin.com/jobs/view/python-%2B-data-structures-engineer-at-itech-solutions-3714397707,2023-12-17,California,United States,Associate,Onsite,"Title - Python + Data Structures Engineer
Location - San Jose, CA
We need a strong python developer with minimum 3-4 years of experience (Fresh grads are also fine but with extra ordinary python skills).
Requirements :
1.     Python - 10/10
2.     Mongodb (database)
3.     Data structures
4.     Traversal/recursion Algorithms
5.     Professional candidate with eager to work hard personality.
Nice to Have :
1.     Machine Learning
Show more
Show less","Python, MongoDB, Data Structures, Traversal Algorithms, Recursion Algorithms, Machine Learning","python, mongodb, data structures, traversal algorithms, recursion algorithms, machine learning","data structures, machine learning, mongodb, python, recursion algorithms, traversal algorithms"
Data Analyst,Zero Labs Automotive,"Gardena, CA",https://www.linkedin.com/jobs/view/data-analyst-at-zero-labs-automotive-3646118565,2023-12-17,California,United States,Associate,Onsite,"Zero Labs is on a mission to bring Premium Classic Vehicles into the modern world through Electrification.
We have an exciting opportunity for a motivated and enthusiastic Data Analyst.
You are the right fit if you are an effective communicator, analytically minded and you take pride in finding resolutions to data challenges. As a Data Analyst with Zero Labs, you will serve as a combination of a Business Intelligence Analyst and Operations Analyst.
This role is based in Gardena, CA and is primarily on-site. Please check the commute impact ahead of applying.
What you’ll do:
Manage Master Data, including creating and revising existing data models
Develop and maintain databases
Provide quality assurance of any imported data, working with various teams, as needed
Helping develop reports and analysis
Supporting the database in identifying and revising reporting requirements
Supporting initiatives for data integrity and normalization
Troubleshooting reports and data
Identifying trends and patters in complex
Requirements
Requirements:
Bachelor’s degree in computer science, finance, statistics or a related field
Three (3) years experience working in data analysis, data science or related industry
Excellent database management skills in Excel/Sheets
A positive, self-starting attitude
Excellent communication skills, including written, verbal and presentation
Solution-oriented
Proven analytic skills, including mining, evaluation and visualization
No Recruiters
Benefits
Health Care Plan (Medical, Dental & Vision)
Paid Time Off (Vacation, Sick & Public Holidays)
Show more
Show less","Data analysis, Data science, Business intelligence, Database management, SQL, Excel, Sheets, Data mining, Data evaluation, Data visualization","data analysis, data science, business intelligence, database management, sql, excel, sheets, data mining, data evaluation, data visualization","business intelligence, data evaluation, data mining, data science, dataanalytics, database management, excel, sheets, sql, visualization"
Senior Engineer - Data Management & MDM,Vuori,"Carlsbad, CA",https://www.linkedin.com/jobs/view/senior-engineer-data-management-mdm-at-vuori-3779279998,2023-12-17,California,United States,Associate,Onsite,"Vuori is re-defining what athletic apparel looks like: built to move and sweat in but designed with a casual aesthetic to transition into everyday life. We draw inspiration from an active coastal California lifestyle; an integration of fitness, creative expression and life. Our high energy fast paced office environment is reflected in the clothes we make. We aim to inspire others to take on all aspects of their lives with clarity, enthusiasm and purpose…while having a lot of fun along the way. We are proud to be an outlet for opportunity and for personal growth and success.
Job Description
We are seeking a highly skilled and experienced Senior Engineer specializing in Data Management and Master Data Management (MDM). In this role, you will be responsible for designing, implementing, and maintaining robust data management solutions, with a focus on ensuring the integrity, quality, and accessibility of organizational data. Your expertise in MDM will be crucial in establishing and maintaining a centralized and consistent view of master data across the enterprise.Experience in Retail apparel space would be valuable in driving industry specific governance and data management techniques.
Responsibilities include but are not limited to:
Data Architecture and Design:
Collaborate with cross-functional teams to understand data requirements and design scalable and efficient data architectures.
Develop and implement data models, ensuring compatibility and integration with existing systems.
Master Data Management (MDM):
Lead the implementation and maintenance of MDM solutions to create and manage a single, accurate, and consistent view of master data.
Define and enforce data governance policies and procedures related to master data.
Data Quality Assurance:
Establish and implement data quality standards and procedures to ensure the accuracy and completeness of data.
Develop and maintain data quality monitoring processes and resolve data quality issues.
Data Integration:
Design and implement data integration solutions to ensure seamless flow of data between different systems and platforms.
Work on ETL (Extract, Transform, Load) processes to integrate data from various sources.
Data Security and Compliance:
Implement data security measures to safeguard sensitive information.
Ensure compliance with relevant data protection and privacy regulations.
Collaboration and Communication:
Collaborate with business stakeholders, analysts, and other IT teams to understand data requirements and deliver effective solutions.
Communicate complex technical concepts to non-technical stakeholders.
Documentation:
Create and maintain comprehensive documentation for data management processes, data models, and MDM solutions.
Continuous Improvement:
Stay abreast of industry trends and emerging technologies in data management and MDM.
Identify opportunities for process improvement and implement best practices.
Qualifications
Bachelor’s degree in Computer Science, Information Technology, or a related field. Master’s degree is a plus.
Proven experience 7+ in data management, data architecture, and MDM.
Preferred 3+ experience in retail industry
In-depth knowledge of data modeling, data pipe development, and data warehousing.
Hands-on experience with MDM tools and technologies.
Strong programming skills in languages such as SQL, Python, or Java.
Familiarity with data quality management and ETL processes.
Excellent problem-solving and analytical skills.
Strong communication and interpersonal skills.
Additional Information
Pay Range:
From $136,000-$160,000/yr
Benefits:
Health Insurance
Paid Time Off
Employee Discount
401(k)
All your information will be kept confidential according to EEO guidelines.
Show more
Show less","Data Management, Master Data Management (MDM), Data Architecture, Data Modeling, Data Quality Assurance, Data Integration, ETL (Extract Transform Load), Data Security, Data Governance, Data Warehousing, SQL, Python, Java, Data Quality Management","data management, master data management mdm, data architecture, data modeling, data quality assurance, data integration, etl extract transform load, data security, data governance, data warehousing, sql, python, java, data quality management","data architecture, data governance, data integration, data management, data quality assurance, data quality management, data security, datamodeling, datawarehouse, etl extract transform load, java, master data management mdm, python, sql"
Data Scientist / ML Engineer (Credit Underwriting) (US Based),Grupago,"San Mateo, CA",https://www.linkedin.com/jobs/view/data-scientist-ml-engineer-credit-underwriting-us-based-at-grupago-3728901560,2023-12-17,California,United States,Associate,Onsite,"We are an early-stage fintech that focuses on bringing affordable credit and financial services to small business owners across LatAm - starting with Mexico.
More specifically, the vast majority of businesses in LatAm are micro-PYMEs that lack access to affordable credit.
We’re launching a credit card and related line of credit product specifically built for the needs of micro-PYMEs - including credit limits and rates that adjust over time as customers successfully repay.
The core of our business is:
A rural distribution model that lets us reach customers at scale and low cost, and
Technology (including AI) to underwrite risk effectively as well as product features like group lending to manage risk
We are backed by top venture investors across the US and Mexico, and our co-founding team has spent their careers working in fintech and entrepreneurship.
Requirements
We are looking for a hard-working and highly entrepreneurial data scientist / machine learning engineer to join our team and work with the co-founders to build and refine our initial machine learning model for automated credit issuance.
You will have primary ownership over managing our machine learning model to allow us to instantly approve new credit cards and lines of credit at low loss rates.
Your main responsibilities are to:
Build our automated credit model
Continuously improve and refine models focused on loss rates and profitability
Work closely with the CEO and MX country manager to apply modeling and data science to other aspects of our business
Experience
We are looking for a data scientist experienced in rapidly building machine learning models – ideally within credit underwriting.
Our ideal candidate has:
A stellar track record across least 5 years of data science experience
Ideally have work experience within micro-business credit or consumer households at the bottom of the pyramid
Ideally fluent or comfortable in Spanish
Previous work experience at a startup or other fast moving, highly entrepreneurial work environment
Depending on your level of experience and team needs, you may also be responsible for managing junior team members.
Benefits
In addition to working at an early stage fast moving startup - you will qualify for equity (stock option grants), competitive compensation and any bonuses, paid time off and holidays.
Learn more at www.grupago.mx or email us with questions at hola@grupago.mx
Show more
Show less","Machine learning, Data science, Credit underwriting, Risk management, AI, Modeling, Data analysis, Spanish, Startups, Entrepreneurship, Equity, Stock options, Compensation, Bonuses, Paid time off, Holidays","machine learning, data science, credit underwriting, risk management, ai, modeling, data analysis, spanish, startups, entrepreneurship, equity, stock options, compensation, bonuses, paid time off, holidays","ai, bonuses, compensation, credit underwriting, data science, dataanalytics, entrepreneurship, equity, holidays, machine learning, modeling, paid time off, risk management, spanish, startups, stock options"
Data Center Engineer,Cloudflare,"Los Angeles, CA",https://www.linkedin.com/jobs/view/data-center-engineer-at-cloudflare-3732384438,2023-12-17,California,United States,Associate,Onsite,"About Us
At Cloudflare, we have our eyes set on an ambitious goal: to help build a better Internet. Today the company runs one of the world’s largest networks that powers approximately 25 million Internet properties, for customers ranging from individual bloggers to SMBs to Fortune 500 companies. Cloudflare protects and accelerates any Internet application online without adding hardware, installing software, or changing a line of code. Internet properties powered by Cloudflare all have web traffic routed through its intelligent global network, which gets smarter with every request. As a result, they see significant improvement in performance and a decrease in spam and other attacks. Cloudflare was named to Entrepreneur Magazine’s Top Company Cultures list and ranked among the World’s Most Innovative Companies by Fast Company.
We realize people do not fit into neat boxes. We are looking for curious and empathetic individuals who are committed to developing themselves and learning new skills, and we are ready to help you do that. We cannot complete our mission without building a diverse and inclusive team. We hire the best people based on an evaluation of their potential and support them throughout their time at Cloudflare. Come join us!
Data Center Operations Engineer
About the department
In this role, you will be focused on maintaining the Clou dflare global network. You 'll work closely with Cloudflare’s SRE (Site Reliability Engineering) team, Network Engineering team, Network Deployment Engineering team and with various vendors and partners (including hardware vendors, datacenter and network providers, and ISPs) to maintain and improve our global infrastructure. You will further be responsible for the development and implementation of consistent processes and visibility measurements for consistent and effective management of our infrastructure. This is a highly visible position that requires deep technical understanding of datacenter infrastructure, networking (physical), and basic experience with data analysis and project management.
To be successful in this position, you should have excellent technical skills, communication skills, and be able to navigate a range of challenges and constraints (e.g. schedule adherence, time zones, and cultures). You will have the opportunity to (literally) build a faster, safer Internet for our millions of users and the billions of web surfers that visit their sites each month.
Who You Are
You will thrive in a hypergrowth engineering environment and be self driven with a keen attention to detail. You will come with a deep technical understanding of Data Center colocation environments, network architecture and server technologies. You will be used to working through partners to support infrastructure delivery to a number of remote locations. You will have had experience managing operational environments, and used to developing new approaches to improve delivery efficiency or operational stability.
What You'll Do
Collaborating with internal teams (Infrastructure, Network Engineering and SRE). Create documentation and manage remote contractors to complete datacenter tasks, working with hardware manufacturers, datacenter and network providers, logistics partners and other service providers in support of our 300+ datacenter locations
Maintain Data Center environment operational availability
Creating and maintaining documentation, plans, SOP’s, MOP’s etc.
Support and configure network infrastructure where required
Providing feedback to internal teams to support internal tools and external vendor partnerships
Required Experience
Minimum of 5 yrs of Linux systems administration
Experience with Juniper, Cisco and DWDM network equipment
Experience managing and instructing remote contractors
Familiarity with work required to stand up infrastructure in remote colocation facilities
Experience running and improving operational processes, including automation tooling, in a rapidly changing environment
Familiarity with day-to-day tasks and projects common to Data Center Operations (deployment, migration, decommissioning etc.)
Comfortable handling basic program management responsibilities (prioritization, planning, scheduling, status reporting) such as JIRA
Incident management
Other Responsibilities May Include
Aggressively seek opportunities to introduce cutting-edge technology and automation solutions that are effective, efficient and scalable in order to improve our ability to deploy and maintain our global infrastructure
Assist with the definition, documentation and implementation of consistent processes across all region
Limited travel
Examples Of Desirable Skills, Knowledge And Experience
Bachelor’s degree; technical background in engineering, computer science, or MIS
Direct experience executing on complex data center/infrastructure projects
Previous experience installing / maintaining data center (and other IT) infrastructure and DCIM tools
Experience running and improving operational processes in a rapidly changing environment
Strong verbal and written communication skills, problem-solving skills, attention to detail, and interpersonal skills
Must be proactive with proven ability to learn fast and execute on multiple tasks simultaneously
Ability to manage MS excel and Google spreadsheets
Comfortable handling basic program management responsibilities (prioritization, planning, scheduling, status reporting) such as JIRA
Must be a team player
Bonus Points
Multi-lingual; experience working with infrastructure in multiple countries
Comfortable with remote “lights-out” and out-of-band access to data center resources
Linux certifications (RHCSA etc.)
Network certifications (CCNA, JNCIA or higher)
Compensation
Compensation may be adjusted depending on work location.
For Colorado-based hires: Estimated annual salary of $ 111,000 - $ 135,000 .
For New York City, Washington, and California (excluding Bay Area) based hires: Estimated annual salary of $ 135,000 - $ 165,000
For Bay Area-based hires: Estimated annual salary of $ 142,000 - $ 174,000 .
Equity
This role is eligible to participate in Cloudflare’s equity plan.
Benefits
Cloudflare offers a complete package of benefits and programs to support you and your family. Our benefits programs can help you pay health care expenses, support caregiving, build capital for the future and make life a little easier and fun! The below is a description of our benefits for employees in the United States, and benefits may vary for employees based outside the U.S.
Health & Welfare Benefits
Medical/Rx Insurance
Dental Insurance
Vision Insurance
Flexible Spending Accounts
Commuter Spending Accounts
Fertility & Family Forming Benefits
On-demand mental health support and Employee Assistance Program
Global Travel Medical Insurance
Financial Benefits
Short and Long Term Disability Insurance
Life & Accident Insurance
401(k) Retirement Savings Plan
Employee Stock Participation Plan
Time Off
Flexible paid time off covering vacation and sick leave
Leave programs, including parental, pregnancy health, medical, and bereavement leave
What Makes Cloudflare Special?
We’re not just a highly ambitious, large-scale technology company. We’re a highly ambitious, large-scale technology company with a soul. Fundamental to our mission to help build a better Internet is protecting the free and open Internet.
Project Galileo
: We equip politically and artistically important organizations and journalists with powerful tools to defend themselves against attacks that would otherwise censor their work, technology already used by Cloudflare’s enterprise customers--at no cost.
Athenian Project
: We created Athenian Project to ensure that state and local governments have the highest level of protection and reliability for free, so that their constituents have access to election information and voter registration.
Path Forward Partnership
: Since 2016, we have partnered with Path Forward, a nonprofit organization, to create 16-week positions for mid-career professionals who want to get back to the workplace after taking time off to care for a child, parent, or loved one.
1.1.1.1
: We released 1.1.1.1 to help fix the foundation of the Internet by building a faster, more secure and privacy-centric public DNS resolver. This is available publicly for everyone to use - it is the first consumer-focused service Cloudflare has ever released. Here’s the deal - we don’t store client IP addresses never, ever. We will continue to abide by our privacy commitment and ensure that no user data is sold to advertisers or used to target consumers.
Sound like something you’d like to be a part of? We’d love to hear from you!
This position may require access to information protected under U.S. export control laws, including the U.S. Export Administration Regulations. Please note that any offer of employment may be conditioned on your authorization to receive software or technology controlled under these U.S. export laws without sponsorship for an export license.
Cloudflare is proud to be an equal opportunity employer. We are committed to providing equal employment opportunity for all people and place great value in both diversity and inclusiveness. All qualified applicants will be considered for employment without regard to their, or any other person's, perceived or actual race, color, religion, sex, gender, gender identity, gender expression, sexual orientation, national origin, ancestry, citizenship, age, physical or mental disability, medical condition, family care status, or any other basis protected by law. We are an AA/Veterans/Disabled Employer.
Cloudflare provides reasonable accommodations to qualified individuals with disabilities. Please tell us if you require a reasonable accommodation to apply for a job. Examples of reasonable accommodations include, but are not limited to, changing the application process, providing documents in an alternate format, using a sign language interpreter, or using specialized equipment. If you require a reasonable accommodation to apply for a job, please contact us via e-mail at hr@cloudflare.com or via mail at 101 Townsend St. San Francisco, CA 94107.
Show more
Show less","Linux systems administration, Juniper Cisco and DWDM network equipment, Remote contractors, Colocation facilities, Automation tooling, JIRA, Incident management, Cuttingedge technology, DCIM tools, Microsoft Excel, Google spreadsheets, RHCSA, CCNA, JNCIA","linux systems administration, juniper cisco and dwdm network equipment, remote contractors, colocation facilities, automation tooling, jira, incident management, cuttingedge technology, dcim tools, microsoft excel, google spreadsheets, rhcsa, ccna, jncia","automation tooling, ccna, colocation facilities, cuttingedge technology, dcim tools, google spreadsheets, incident management, jira, jncia, juniper cisco and dwdm network equipment, linux systems administration, microsoft excel, remote contractors, rhcsa"
Senior Software Engineer (Cloud/Data Lake ),Palo Alto Networks,"Santa Clara, CA",https://www.linkedin.com/jobs/view/senior-software-engineer-cloud-data-lake-at-palo-alto-networks-3775087143,2023-12-17,California,United States,Associate,Onsite,"Company Description
Our Mission
At Palo Alto Networks® everything starts and ends with our mission:
Being the cybersecurity partner of choice, protecting our digital way of life.
Our vision is a world where each day is safer and more secure than the one before. We are a company built on the foundation of challenging and disrupting the way things are done, and we’re looking for innovators who are as committed to shaping the future of cybersecurity as we are.
Our Approach to Work
We lead with flexibility and choice in all of our people programs. We have disrupted the traditional view that all employees have the same needs and wants. We offer personalization and offer our employees the opportunity to choose what works best for them as often as possible - from your wellbeing support to your growth and development, and beyond!
At Palo Alto Networks, we believe in the power of collaboration and value in-person interactions. This is why our employees generally work from the office three days per week, leaving two days for choice and flexibility to work where you feel most effective. This setup fosters casual conversations, problem-solving, and trusted relationships. While details may evolve, our goal is to create an environment where innovation thrives, with office-based teams coming together three days a week to collaborate and thrive, together!
Job Description
Your Career
Cortex Data Lake (CDL) Enables AI-based Innovations For Cybersecurity With The Industry’s Only Approach To Normalizing And Stitching Together An Enterprise’s Data. Cortex Data Lake Can
Radically simplify customer security operations by collecting, integrating, and normalizing an enterprise’s security data
Effortlessly run advanced AI and machine learning with cloud-scale data and compute
Constantly learns from new data sources to evolve customer defenses
CDL is built to benefit from public cloud scale and locations ready for elastic scale, eliminating the need for local compute and storage. CDL has strict privacy and security controls in place to prevent unauthorized access to sensitive or identifiable information. Its infrastructure is secured with industry-standard best practices for security and confidentiality, including rigorous technical and organizational security controls.
Your Impact
Tackle new and challenging problems by building a new generation of highly scaled data processing and analytics systems for use in AI-powered use cases
Contribute in architecture, design and development of features
Solve complex problems in pipeline scaling and data storage to facilitate AI/ML analytics and AI-powered dashboards
Suggest and implement improvements to the development processes
Work with DevOps and Technical Support teams to investigate and resolve critical customer defects
Qualifications
Your Experience
Must have
8+ years of hands-on experience in building large enterprise applications primarily in Java
3+ years of experience as a backend Java developer on a distributed systems environment
Deep understanding of design patterns
Good communication skills and ability to work in a fast-paced environment
Good to have
Strong knowledge of databases SQL, NoSQL, Time series, GraphDB etc
Experience in working on cloud environments especially GCP
Knowledge of Linux fundamentals and networked computing environment concepts
Additional Information
The Team
Our engineering team is at the core of our products – connected directly to the mission of preventing cyberattacks. We are constantly innovating – challenging the way we, and the industry, think about cybersecurity. Our engineers don’t shy away from building products to solve problems no one has pursued before.
We define the industry, instead of waiting for directions. We need individuals who feel comfortable in ambiguity, excited by the prospect of a challenge, and empowered by the unknown risks facing our everyday lives that are only enabled by a secure digital environment.
Our Commitment
We’re trailblazers that dream big, take risks, and challenge cybersecurity’s status quo. It’s simple: we can’t accomplish our mission without diverse teams innovating, together.
We are committed to providing reasonable accommodations for all qualified individuals with a disability. If you require assistance or accommodation due to a disability or special need, please contact us at accommodations@paloaltonetworks.com.
Palo Alto Networks is an equal opportunity employer. We celebrate diversity in our workplace, and all qualified applicants will receive consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or other legally protected characteristics.
All your information will be kept confidential according to EEO guidelines.
The compensation offered for this position will depend on qualifications, experience, and work location. For candidates who receive an offer at the posted level, the starting base salary (for non-sales roles) or base salary + commission target (for sales/com-missioned roles) is expected to be $175,000/yr. The offered compensation may also include restricted stock units and a bonus. A description of our employee benefits may be found here.
Show more
Show less","Java, Distributed systems, SQL, NoSQL, Time series, GraphDB, Linux, Networked computing, GCP","java, distributed systems, sql, nosql, time series, graphdb, linux, networked computing, gcp","distributed systems, gcp, graphdb, java, linux, networked computing, nosql, sql, time series"
"Senior Data Engineer, Seller Experience",Square,"San Francisco, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-seller-experience-at-square-3775850401,2023-12-17,California,United States,Associate,Onsite,"Company Description
Since we opened our doors in 2009, the world of commerce has evolved immensely, and so has Square. After enabling anyone to take payments and never miss a sale, we saw sellers stymied by disparate, outmoded products and tools that wouldn’t work together.
So we expanded into software and started building integrated, omnichannel solutions – to help sellers sell online, manage inventory, offer buy now, pay later functionality through Afterpay, book appointments, engage loyal buyers, and hire and pay staff. Across it all, we’ve embedded financial services tools at the point of sale, so merchants can access a business loan and manage their cash flow in one place. Afterpay furthers our goal to provide omnichannel tools that unlock meaningful value and growth, enabling sellers to capture the next generation shopper, increase order sizes, and compete at a larger scale.
Today, we are a partner to sellers of all sizes – large, enterprise-scale businesses with complex operations, sellers just starting, as well as merchants who began selling with Square and have grown larger over time. As our sellers grow, so do our solutions. There is a massive opportunity in front of us. We’re building a significant, meaningful, and lasting business, and we are helping sellers worldwide do the same.
Job Description
Square’s Growth team builds intelligent and scalable solutions that empower various communication channels, like Sales and Account Management, to drive seller growth and retention.
As a Data Engineer on the Growth team, you will be the expert on the data solutions that enable the team to build and roll out scalable go-to-market processes, effectively design and launch experimentation, and provide stakeholders with direct visibility into program business impact.
You will:
Partner closely with data scientists, business intelligence analysts and machine learning engineers to power GTM operations, experimentation and a suite of analytical data products
Work with channel specific teams to build a roadmap for foundational data sets and metrics that are aligned with business goals and that enable self-service
Architect, build, and launch scalable data pipelines to support the Growth team’s growing experimentation needs across multiple channels
Understand and implement data logging best practices to support our data flow
Build integrations with 3rd party data vendors
Identify opportunities to streamline, automate tasks, and build reusable components across multiple use cases and teams
Create dashboards that help our stakeholders understand the performance of the experiments and help them make decisions
Qualifications
You have:
5+ years of experience as a Data Engineer or related specialization (e.g. Business Intelligence, Data Science, etc.) with a track record of delivering large scale data solutions
Expert knowledge of data warehouse architecture and hands on experience of data modeling design, building complex, scalable ETLs using SQL and Python for a variety of different business and product use cases (batch and/or streaming).
Experience with ingesting and transforming data from application APIs preferred
Hands on experience on cloud-based computing services and data warehouses like Snowflake, Redshift, Azure, or similar
Experience using job orchestration platforms like Airflow, Prefect, or similar
Experience building reports with Looker or similar BI visualization tools
Excellent communication skills with strong business intuition and ability to articulate complex technical concepts and explain data problems in concise language; versatility and willingness to learn new technologies on the job
Salesforce (SFDC) experience a plus
Databricks/Spark experience a plus
Additional Information
Block takes a market-based approach to pay, and pay may vary depending on your location. U.S. locations are categorized into one of four zones based on a cost of labor index for that geographic area. The successful candidate’s starting pay will be determined based on job-related skills, experience, qualifications, work location, and market conditions. These ranges may be modified in the future.
Zone A: USD $152,100 - USD $185,900
Zone B: USD $144,500 - USD $176,700
Zone C: USD $136,900 - USD $167,300
Zone D: USD $129,300 - USD $158,100
To find a location’s zone designation, please refer to this resource. If a location of interest is not listed, please speak with a recruiter for additional information.
Full-time employee benefits include the following:
Healthcare coverage (Medical, Vision and Dental insurance)
Health Savings Account and Flexible Spending Account
Retirement Plans including company match
Employee Stock Purchase Program
Wellness programs, including access to mental health, 1:1 financial planners, and a monthly wellness allowance
Paid parental and caregiving leave
Paid time off (including 12 paid holidays)
Paid sick leave (1 hour per 26 hours worked (max 80 hours per calendar year to the extent legally permissible) for non-exempt employees and covered by our Flexible Time Off policy for exempt employees)
Learning and Development resources
Paid Life insurance, AD&D, and disability benefits
These benefits are further detailed in Block's policies. This role is also eligible to participate in Block's equity plan subject to the terms of the applicable plans and policies, and may be eligible for a sign-on bonus. Sales roles may be eligible to participate in a commission plan subject to the terms of the applicable plans and policies. Pay and benefits are subject to change at any time, consistent with the terms of any applicable compensation or benefit plans.
US and Canada EEOC Statement
We’re working to build a more inclusive economy where our customers have equal access to opportunity, and we strive to live by these same values in building our workplace. Block is a proud equal opportunity employer. We work hard to evaluate all employees and job applicants consistently, without regard to race, color, religion, gender, national origin, age, disability, pregnancy, gender expression or identity, sexual orientation, citizenship, or any other legally protected class.
We believe in being fair, and are committed to an inclusive interview experience, including providing reasonable accommodations to disabled applicants throughout the recruitment process. We encourage applicants to share any needed accommodations with their recruiter, who will treat these requests as confidentially as possible.
Want to learn more about what we’re doing to build a workplace that is fair and square? Check out our
I+D page
.
Additionally, we consider qualified applicants with criminal histories for employment on our team, and always assess candidates on an individualized basis.
Block, Inc. (NYSE: SQ) is a global technology company with a focus on financial services. Made up of Square, Cash App, Spiral, TIDAL, and TBD, we build tools to help more people access the economy. Square helps sellers run and grow their businesses with its integrated ecosystem of commerce solutions, business software, and banking services. With Cash App, anyone can easily send, spend, or invest their money in stocks or Bitcoin. Spiral (formerly Square Crypto) builds and funds free, open-source Bitcoin projects. Artists use TIDAL to help them succeed as entrepreneurs and connect more deeply with fans. TBD is building an open developer platform to make it easier to access Bitcoin and other blockchain technologies without having to go through an institution.
Show more
Show less","Data Engineering, Data Science, Business Intelligence, Machine Learning, Python, SQL, ETL, Snowflake, Redshift, Azure, Airflow, Looker, Salesforce, Databricks, Spark","data engineering, data science, business intelligence, machine learning, python, sql, etl, snowflake, redshift, azure, airflow, looker, salesforce, databricks, spark","airflow, azure, business intelligence, data engineering, data science, databricks, etl, looker, machine learning, python, redshift, salesforce, snowflake, spark, sql"
Azure Data Architect,VeeAR Projects Inc.,"Sacramento, CA",https://www.linkedin.com/jobs/view/azure-data-architect-at-veear-projects-inc-3719535607,2023-12-17,California,United States,Associate,Onsite,"Job Description:
Data Architect/Technical Consultant Data & Analytics who can work with customers and teams (local/remote), lead technical or business discussions, provide technical guidance to clients and team members delivering initiatives. Must have been in an Architect/Consultant/Engineer role, led the design/implementation of several complex data-oriented solutions on Azure Data Platform with multiple integrations and reporting needs. Experience in On Prem and other Cloud Data technologies will be helpful. This role is a combination of solutioning, consulting, hands-on experience. Envision, design, and implement the technical solution working with customer and internal technical team. Lead multiple Data, analytics projects providing design/implementation guidance to teams. 15 years of IT experience, 10 years of experience in architecture and design of highly scalable, performance, resilient data and analytical solutions.
Modern Data Platform:
Must have worked on several (5+) projects building solutions for Data Pipelines, Data warehousing, Data Modelling on Azure Data Platform (AWS, GCP - good to have)
Expertise in design and development of Data pipelines and ETL using ADF, Databricks to move data from relational/structured/unstructured data from source to data lake to data warehouse, on Azure
Creating Pipelines and integrations of multiple data sources
Data processing and transformation using Databricks (Scala/Python)
Expertise in optimizing cost and performance of data pipelines
Experience working with Data Lake, Synapse on Azure
Designing Data Lake storage layer (Landing/staging, raw, trusted/curated zones)
Designing data storage DB schema, data models and processing on Azure Synapse DW
Experience with Synapse Spark Pool
Experience in Data Modelling
Building Data Models from structured and semi-structured data (Data Model, Common Data Model etc.)
Performance optimization of data model for high performance analytical and reporting workloads
Expertise with SQL Server and extensive experience with SQL programming
Programming and optimizing DB objects Views, Stored Procs, Functions
Good experience in defining data access policy and security for azure data services
Experience with Oracle EDW, knowledge/experience migrating from Oracle EDW to any Cloud DW
Experience integrating AI skills/Client models with Data and reporting solutions
Experience in building API layer for downstream consumption
Reporting /Preferred:
Expertise in building scalable high performance reporting solution using Power BI, in both shared and dedicated capacity
Designing Reports against Cloud and On-Prem data sources
Performance analysis and optimization of slow performing reports/visuals
Import models and Direct Query; Experience working with Power BI Paginated reports
Expertise in integrating Power BI reports with custom applications using Power BI Embedded or Premium
Building data models in Power BI, Hands-on with DAX
Migrating reports from Tableau to Power BI
Show more
Show less","Azure Data Platform, AWS, GCP, ADF, Databricks, Scala, Python, Data Lake, Synapse, Azure Synapse DW, Synapse Spark Pool, Data Modelling, SQL Server, SQL, Oracle EDW, Power BI, Power BI Paginated reports, Power BI Embedded, Premium, DAX, Tableau","azure data platform, aws, gcp, adf, databricks, scala, python, data lake, synapse, azure synapse dw, synapse spark pool, data modelling, sql server, sql, oracle edw, power bi, power bi paginated reports, power bi embedded, premium, dax, tableau","adf, aws, azure data platform, azure synapse dw, data lake, data modelling, databricks, dax, gcp, oracle edw, power bi embedded, power bi paginated reports, powerbi, premium, python, scala, sql, sql server, synapse, synapse spark pool, tableau"
Senior Data Engineer,Kforce Inc,"Montebello, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-kforce-inc-3782229483,2023-12-17,California,United States,Associate,Onsite,"Responsibilities
Kforce has a client in Montebello, CA that is seeking a Senior Data Engineer. Under the direction of Director - Data Engineer, the Senior Data Engineer works closely with business leaders, managers, staff and vendor to accurately gather and interpret requirements, specifications. Responsibilities:
Lead on all aspects of a modern and flexible SDLC approach, applied to data engineering, analytics and data science efforts
Provide business guidance and steer efforts toward desired outcomes; Partner with internal stakeholders and external vendors to meet strategic data warehouse goals
Research, evaluate and formally recommend third party software and technology package
Responsible for new & existing integrated system & data flow enterprise architecture
Set standards for data engineering functions; Design templates for the data management which are scalable, repeatable, and simple
Stay abreast of established and industry emerging data technologies
Compliance with standards and procedures such as standard of communication, work management, change management, version control, implementation and/or consistency of coding
Recognizes code, process and/or standard inefficiencies and suggests new standards and opportunities for improvement
Build and integrate a data-driven intelligent solution into our business processes; Manage the innovation development processes, and be responsible for driving the data architecture for the company's products and IT processes
Research, evaluate and formally recommend third party software and technology package
Keep big picture concepts in mind when designing solutions - fully understand business needs
Providing proactive technical oversight and advice to application architecture and development teams fostering re-use, design for scale, stability, and operational efficiency of data/analytical solutions
Requirements
7+ years of experience in a complex data warehouse role for a mid to large size organization; Health Care industry experience highly desired
4+ years of experience with ETL/Data Integration, BI, data mining and modeling, SSIS strongly preferred
Proficiency in all facets of DW Architecture, data flow strategy, data modeling, metadata and master data management
Expertise in SQL and RDBMS systems such as Microsoft SQL Server
Experience working with reporting tools like SSRS and Tableau
Ability to produce high quality technical documentation
Experience in implementing a data architecture with separation between storage and computer preferred
Understanding of data architecture and ability to coordinate with the implementation team is highly desired
Able to consult on complex data engineering efforts and lead project teams through the solution design process
Able to teach and mentor to less-experience technical team members
Experience on database technologies, data warehouse, data validation, data quality, metadata management and data governance
Knowledge of and experienced in rolling out best practices in all facets of DW architecture, data flow strategy, data modeling, metadata and master data management
Ability to interact and develop relationships with all levels of personnel and management
Ability to sit, stand, lift (up to 20 lbs), bend, and walk, as required, for carrying out the duties of position
Work is based in an office environment, climate controlled through central air conditioning
May be required to attend off site meetings and other related functions
May require travel to sites/program and special functions
The pay range is the lowest to highest compensation we reasonably in good faith believe we would pay at posting for this role. We may ultimately pay more or less than this range. Employee pay is based on factors like relevant education, qualifications, certifications, experience, skills, seniority, location, performance, union contract and business needs. This range may be modified in the future.
We offer comprehensive benefits including medical/dental/vision insurance, HSA, FSA, 401(k), and life, disability & ADD insurance to eligible employees. Salaried personnel receive paid time off. Hourly employees are not eligible for paid time off unless required by law. Hourly employees on a Service Contract Act project are eligible for paid sick leave.
Note: Pay is not considered compensation until it is earned, vested and determinable. The amount and availability of any compensation remains in Kforce's sole discretion unless and until paid and may be modified in its discretion consistent with the law.
This job is not eligible for bonuses, incentives or commissions.
Kforce is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.
Salary: $50 - $69 per hour
Show more
Show less","Data Engineering, Data Analytics, Data Science, SDLC, Data Warehouse, ETL, Data Integration, BI, Data Mining, Data Modeling, SQL, RDBMS, Microsoft SQL Server, SSRS, Tableau, Data Architecture, Data Flow Strategy, Metadata Management, Master Data Management, Data Governance, Data Validation, Data Quality, Best Practices, Communication, Work Management, Change Management, Version Control, Implementation, Coding, Innovation Development, DataDriven Solutions, Business Processes, Application Architecture, Development, Reuse, Scalability, Stability, Operational Efficiency","data engineering, data analytics, data science, sdlc, data warehouse, etl, data integration, bi, data mining, data modeling, sql, rdbms, microsoft sql server, ssrs, tableau, data architecture, data flow strategy, metadata management, master data management, data governance, data validation, data quality, best practices, communication, work management, change management, version control, implementation, coding, innovation development, datadriven solutions, business processes, application architecture, development, reuse, scalability, stability, operational efficiency","application architecture, best practices, bi, business processes, change management, coding, communication, data architecture, data engineering, data flow strategy, data governance, data integration, data mining, data quality, data science, data validation, dataanalytics, datadriven solutions, datamodeling, datawarehouse, development, etl, implementation, innovation development, master data management, metadata management, microsoft sql server, operational efficiency, rdbms, reuse, scalability, sdlc, sql, ssrs, stability, tableau, version control, work management"
"Data Engineering Lead, Translational Genomics",Genentech,"South San Francisco, CA",https://www.linkedin.com/jobs/view/data-engineering-lead-translational-genomics-at-genentech-3609864910,2023-12-17,California,United States,Associate,Onsite,"The Position
If you are a big data engineer and want to work on something that truly can change the world, this job is for you. Biology is approaching an inflection where we can directly leverage data to understand the cellular basis of human diseases and from this generate therapeutics that can treat these diseases. Our Translational Genomics initiative is spearheading this effort and bringing together data from human genetics, functional genomics, molecular biology, disease model engineering, and tissue and cellular profiling. We need a Data Engineering Lead to help us create a next-generation data engine that scalably and rigorously ingests and transforms data generated from this initiative so they are ready for machine-driven analysis. The Data Engineering Lead will act as an architect and engineering manager tasked to oversee the construction and operation of this data engine. This data engine will be used to help assemble an exabyte scale connected and computable data universe composed of high value internally and externally generated data and results that we can build our data science efforts on top of. Your efforts will therefore directly enable computational discovery of disease targets and from these potentially life saving therapies.
A person hired in this position will
Manage a team that will architect and deliver a next generation data engine that enables scalable, flexible, and rigorous data transformations using modern data management practices.
Help architect and deliver data infrastructure that will enable machines to crawl and compute on and across all our data.
Work with a cross functional team of scientists and engineers to design and deliver these solutions.
Exert influence across the informatics organization via presentations and collaborations.
Successful Candidates Will Meet The Following Requirements
You have a BS in a computational discipline with 12 years of work experience or a Masters with 7 years of experience.
7+ years experience architecting and developing scalable pipelines, frameworks and platforms to power data science efforts in distributed cloud environments, 5 of which are on AWS.
Multiple years of experience leading a distributed team of engineers to deliver solutions.
Practical understanding of the data management practices required to power rigorous data science and enable advanced analytics like AI & ML.
Exceptional communication skills.
Experience leading projects focused on omics data.
Hands-on experience working with the following technologies, frameworks, and languages: Java, Scala, Python, Spark, Airflow, RabbitMQ, Spring.
What To Expect From Us
A highly collaborative and dynamic research environment where we aim to advance the rate of scientific discovery using purposefully built solutions.
Access to large multimodal omic datasets focused on disease biology, samples and compute resources.
Access to state-of-the-art technologies and pioneering research.
Participation in seminar series featuring academic and industry scientists.
Campus-like lifestyle with a healthy work-life balance.
Mentored opportunities to further develop professional skills.
#gCS
Who We Are
A member of the Roche Group, Genentech has been at the forefront of the biotechnology industry for more than 40 years, using human genetic information to develop novel medicines for serious and life-threatening diseases. We are a research-driven biotechnology company, whose medical innovations for cancer and other serious illnesses make a difference for patients across the globe. Please take this opportunity to learn about Genentech where we believe that our employees are our most important asset & are dedicated to remaining a great place to work.
Genentech is an equal opportunity employer & prohibits unlawful discrimination based on race, color, religion, gender, sexual orientation, gender identity/expression, national origin/ancestry, age, disability, marital & veteran status. For more information about equal employment opportunity, visit our Genentech Careers page.
The expected salary range for this position based on the primary location of California is $148,800-$276,400. Actual pay will be determined based on experience, qualifications, geographic location, and other job-related factors permitted by law. A discretionary annual bonus may be available based on individual and Company performance. This position also qualifies for the benefits detailed at the link provided below.
Benefits
#gREDinformatics
Genentech is an equal opportunity employer, and we embrace the increasingly diverse world around us. Genentech prohibits unlawful discrimination based on race, color, religion, gender, sexual orientation, gender identity or expression, national origin or ancestry, age, disability, marital status and veteran status.
Show more
Show less","Big data engineering, Data architecture, Data management, Data science, Machine learning, AI, AWS, Java, Scala, Python, Spark, Airflow, RabbitMQ, Spring, Genomics, Bioinformatics, Omics data, Exabyte scale data universe, Computational discovery, Disease targets, Therapeutics, Cloud computing, Distributed systems, Distributed cloud environments, Software engineering, Project management, Scalable pipelines, Frameworks, Platforms, Big data platforms, Data analytics, Realtime analytics, Data visualization, Data governance, Data security, Data privacy","big data engineering, data architecture, data management, data science, machine learning, ai, aws, java, scala, python, spark, airflow, rabbitmq, spring, genomics, bioinformatics, omics data, exabyte scale data universe, computational discovery, disease targets, therapeutics, cloud computing, distributed systems, distributed cloud environments, software engineering, project management, scalable pipelines, frameworks, platforms, big data platforms, data analytics, realtime analytics, data visualization, data governance, data security, data privacy","ai, airflow, aws, big data engineering, big data platforms, bioinformatics, cloud computing, computational discovery, data architecture, data governance, data management, data privacy, data science, data security, dataanalytics, disease targets, distributed cloud environments, distributed systems, exabyte scale data universe, frameworks, genomics, java, machine learning, omics data, platforms, project management, python, rabbitmq, realtime analytics, scala, scalable pipelines, software engineering, spark, spring, therapeutics, visualization"
Data Engineer - Contract W2,ClifyX,"Walnut Creek, CA",https://www.linkedin.com/jobs/view/data-engineer-contract-w2-at-clifyx-3785801910,2023-12-17,California,United States,Associate,Remote,"Job Title: Data Engineer - Contract W2
Position: Contract - W2
Location: Walnut Creek, California, United States
Job Description:
•Minimum 2 years experience with Python.
•Minimum 3 years experience SQL.
•Minimum 3 years experience Big Data Analytics.
•High School Diploma or GED.
•Experience with large enterprise data sets.
Thanks
Show more
Show less","Python, SQL, Big Data Analytics, Enterprise data sets","python, sql, big data analytics, enterprise data sets","big data analytics, enterprise data sets, python, sql"
"Senior Software Engineer, Orders Data Platform",Square,"San Francisco, CA",https://www.linkedin.com/jobs/view/senior-software-engineer-orders-data-platform-at-square-3785213809,2023-12-17,California,United States,Associate,Remote,"Company Description
Since we opened our doors in 2009, the world of commerce has evolved immensely, and so has Square. After enabling anyone to take payments and never miss a sale, we saw sellers stymied by disparate, outmoded products and tools that wouldn’t work together.
So we expanded into software and started building integrated, omnichannel solutions – to help sellers sell online, manage inventory, offer buy now, pay later functionality through Afterpay, book appointments, engage loyal buyers, and hire and pay staff. Across it all, we’ve embedded financial services tools at the point of sale, so merchants can access a business loan and manage their cash flow in one place. Afterpay furthers our goal to provide omnichannel tools that unlock meaningful value and growth, enabling sellers to capture the next generation shopper, increase order sizes, and compete at a larger scale.
Today, we are a partner to sellers of all sizes – large, enterprise-scale businesses with complex operations, sellers just starting, as well as merchants who began selling with Square and have grown larger over time. As our sellers grow, so do our solutions. There is a massive opportunity in front of us. We’re building a significant, meaningful, and lasting business, and we are helping sellers worldwide do the same.
Job Description
We are the Orders Data Platform team, a team whose mission is to help merchants of all sizes gain insights from their sales data through a variety of interfaces. We sit at the center of critical domains and data flows, and we’re building a multi-layered platform to achieve our goals with Square-wide impact. We’re looking for a senior engineer who can help us build new platform features to dramatically improve user-facing search and reporting experiences at Square.
We provide a unified view of Orders/Orders-adjacent data at Square, and power a variety of interfaces for our teams and third-party developers to access that data. You’ll work with teams to understand requirements, and ensure the platform we’re building works across several distinct use cases.
To power that view of Orders, we’re also building a general-purpose Elasticsearch- and GraphQL-based search and reporting platform that can operate at Square scale. This general-purpose platform already powers search and reporting for a variety of teams at Square in production, and we’re continuing to evolve it for our collective use cases. We build in the open, and look forward to open-sourcing this platform in 2024 for even wider impact.
You will:
Work with Product and partners across Square to identify platform requirements, and work within the engineering team to develop the corresponding features
Provide high-quality hands-on contributions across multiple code bases
Identify technical and architectural end states for the project, and influence/evolve the code case in those directions
Play a key role in choosing technical investments for the team
Qualifications
You Have:
5+ years of software development experience
Familiarity with architecting/implementing Java-/Kotlin-based backend services
Strong product intuition and interest, with platform-building experience
Strategic leadership experience on medium/large-scale software projects
Interest and experience in mentoring other engineers
Even better:
Experience with real-time data streaming platforms such as Kafka and Kinesis
Experience and familiarity with Kotlin, GraphQL, Elasticsearch, and AWS technologies
Proficiency in large-scale Ruby projects, or a history and interest in quickly learning new technologies and stacks
History of contributions to open-source projects
Familiarity with the payments-processing domain
Technologies we use within Orders Data Platform:
Java, Kotlin, Ruby
GraphQL
Elasticsearch, DynamoDb
Terraform, AWS Lambda, SQS, Cloudwatch
Additional Information
Block takes a market-based approach to pay, and pay may vary depending on your location. U.S. locations are categorized into one of four zones based on a cost of labor index for that geographic area. The successful candidate’s starting pay will be determined based on job-related skills, experience, qualifications, work location, and market conditions. These ranges may be modified in the future.
Zone A: USD $167,300 - USD $204,500
Zone B: USD $158,900 - USD $194,300
Zone C: USD $150,600 - USD $184,000
Zone D: USD $142,200 - USD $173,800
To find a location’s zone designation, please refer to this resource. If a location of interest is not listed, please speak with a recruiter for additional information.
Full-time employee benefits include the following:
Healthcare coverage (Medical, Vision and Dental insurance)
Health Savings Account and Flexible Spending Account
Retirement Plans including company match
Employee Stock Purchase Program
Wellness programs, including access to mental health, 1:1 financial planners, and a monthly wellness allowance
Paid parental and caregiving leave
Paid time off (including 12 paid holidays)
Paid sick leave (1 hour per 26 hours worked (max 80 hours per calendar year to the extent legally permissible) for non-exempt employees and covered by our Flexible Time Off policy for exempt employees)
Learning and Development resources
Paid Life insurance, AD&D, and disability benefits
These benefits are further detailed in Block's policies. This role is also eligible to participate in Block's equity plan subject to the terms of the applicable plans and policies, and may be eligible for a sign-on bonus. Sales roles may be eligible to participate in a commission plan subject to the terms of the applicable plans and policies. Pay and benefits are subject to change at any time, consistent with the terms of any applicable compensation or benefit plans.
US and Canada EEOC Statement
We’re working to build a more inclusive economy where our customers have equal access to opportunity, and we strive to live by these same values in building our workplace. Block is a proud equal opportunity employer. We work hard to evaluate all employees and job applicants consistently, without regard to race, color, religion, gender, national origin, age, disability, pregnancy, gender expression or identity, sexual orientation, citizenship, or any other legally protected class.
We believe in being fair, and are committed to an inclusive interview experience, including providing reasonable accommodations to disabled applicants throughout the recruitment process. We encourage applicants to share any needed accommodations with their recruiter, who will treat these requests as confidentially as possible.
Want to learn more about what we’re doing to build a workplace that is fair and square? Check out our
I+D page
.
Additionally, we consider qualified applicants with criminal histories for employment on our team, and always assess candidates on an individualized basis.
We’ve noticed a rise in recruiting impersonations across the industry, where individuals are sending fake job offer emails. Contact from any of our recruiters or employees will always come from an email address ending with @block.xyz, @squareup.com, @tidal.com, or @afterpay.com, @clearpay.co.uk.
Block, Inc. (NYSE: SQ) is a global technology company with a focus on financial services. Made up of Square, Cash App, Spiral, TIDAL, and TBD, we build tools to help more people access the economy. Square helps sellers run and grow their businesses with its integrated ecosystem of commerce solutions, business software, and banking services. With Cash App, anyone can easily send, spend, or invest their money in stocks or Bitcoin. Spiral (formerly Square Crypto) builds and funds free, open-source Bitcoin projects. Artists use TIDAL to help them succeed as entrepreneurs and connect more deeply with fans. TBD is building an open developer platform to make it easier to access Bitcoin and other blockchain technologies without having to go through an institution.
Show more
Show less","Java, Kotlin, Ruby, GraphQL, Elasticsearch, DynamoDB, Terraform, AWS Lambda, SQS, Cloudwatch, Kafka, Kinesis","java, kotlin, ruby, graphql, elasticsearch, dynamodb, terraform, aws lambda, sqs, cloudwatch, kafka, kinesis","aws lambda, cloudwatch, dynamodb, elasticsearch, graphql, java, kafka, kinesis, kotlin, ruby, sqs, terraform"
Data Labeling Analyst II,Facebook,Los Angeles Metropolitan Area,https://www.linkedin.com/jobs/view/data-labeling-analyst-ii-at-facebook-3785573594,2023-12-17,California,United States,Associate,Remote,"APPROVED REMOTE LOCATIONS:
Los Angeles, CA, San Fransisco Bay Area, CA, San Diego, CA, New York, NY, Denver, CO, Houston, TX, Seattle, WA, Baltimore, MD.
Summary:
The main function of a data labeling analyst is to create and manage labeling and change processes within the data management systems. The typical data labeling analyst will have experience in data quality assurance.
Job Responsibilities:
• Create and modify data labels ensuring compliance to all regulatory and legal requirements.
• Maintain batch records, room logs, product travelers, and inventory records.
• Label and analyze large data sets to inform product decisions.
• Asses data quality.
Skills:
• Ability to identify trends within large data sets.
• Excellent communication skills, verbal and written.
• Problem solving skills.
• Team oriented with attention for detail.
Education/Experience:
• Bachelors degree in related field.
Show more
Show less","Data Labeling, Data Quality Assurance, Data Analysis, Data Management, Regulatory Compliance, Batch Records, Room Logs, Product Travelers, Inventory Records, DataDriven Decision Making, Data Quality Assessment, Trend Identification, Communication Skills, Problem Solving, Teamwork, Attention to Detail, Bachelor's Degree in Related Field","data labeling, data quality assurance, data analysis, data management, regulatory compliance, batch records, room logs, product travelers, inventory records, datadriven decision making, data quality assessment, trend identification, communication skills, problem solving, teamwork, attention to detail, bachelors degree in related field","attention to detail, bachelors degree in related field, batch records, communication skills, data labeling, data management, data quality assessment, data quality assurance, dataanalytics, datadriven decision making, inventory records, problem solving, product travelers, regulatory compliance, room logs, teamwork, trend identification"
"Data Research Analyst, gt.school (Remote) - $60,000/year USD",Crossover,"Fulton, CA",https://www.linkedin.com/jobs/view/data-research-analyst-gt-school-remote-%2460-000-year-usd-at-crossover-3783190004,2023-12-17,California,United States,Associate,Remote,"Crossover is the world's #1 source of full-time remote jobs. Our clients offer top-tier pay for top-tier talent. We're recruiting this role for our client, gt.school. Have you got what it takes?
Are you ready to revolutionize education with the power of cutting-edge technology? We're seeking strategic problem solvers who thrive on developing and implementing innovative solutions and are eager to apply the power of generative AI to supercharge their work. Your consistent high performance and dedication will drive our mission to revolutionize education.
At gt.school, we're on a mission to accelerate the pace of learning by 5-10X, harnessing the incredible potential of technology, algorithms, and generative AI. We are creating a platform where students can discover, explore, and learn at an unprecedented rate. You'll play a pivotal role in bringing this vision to life. You'll have the opportunity to build new tools and operational processes, as well as streamline existing workflows for maximum efficiency.
But that's not all! At gt.school, we believe in your growth and development. You'll receive regular coaching and training from a team of global tech experts and operations professionals, ensuring your continuous improvement. We value collaboration and provide a supportive environment where you can meet with team members to work together on exciting projects.
If you're eager for a role that combines research, strategy, product development, and data science in the ever-evolving realm of education and technology, seize the moment and apply below. Let's embark on this exhilarating journey together!
What You Will Be Doing
Engineering generative AI prompts in Python/JavaScript/JSON
Analyze AI outputs and iterate on prompts to reach desired outcomes
Leverage generative AI and technology to develop new educational tools, iterate on current products, and finetune AI training models
Research topics to support the product engineering team
What You Won’t Be Doing
Collecting data or performing administrative tasks - we have a junior operations analyst team to do this type of work.
Working on unimportant projects - you'll be tasked with projects that are mission-critical
Data Research Analyst Key Responsibilities
Create technical functionalities that revolutionize the way education is delivered.
Basic Requirements
Excellent written and verbal English communication skills
Proficient in Python, R, JavaScript, or another OOP language
2+ years of work experience working in a technical field (science, technology, programming, financial services, data science, etc.)
Consistently able to work 40 hours per week, Monday through Friday, between 6AM and 6PM CST
Must be a proactive communicator who can effectively manage multiple priorities and stakeholders simultaneously
About Gt.school
GT School is an EdTech Startup that leverages technology, AI, and subject matter experts to cultivate a new way of learning. Our unique approach leverages 50+ years of learning science, cutting-edge data analytics and high-performance coaching. In doing so, we can help students learn more, learn faster, and learn better - and have fun while doing it. We are a remote-first company that hires globally via Crossover.
There is so much to cover for this exciting role, and space here is limited. Hit the Apply button if you found this interesting and want to learn more. We look forward to meeting you!
Working with Crossover
This is a full-time (40 hours per week), long-term position. The position is immediately available and requires entering into an independent contractor agreement with Crossover. The compensation level for this role is $30 USD/hour, which equates to $60,000 USD/year assuming 40 hours per week and 50 weeks per year. The payment period is weekly. Consult www.crossover.com/help-and-faqs for more details on this topic.
What to expect next:
You will receive an email with a link to start your self-paced, online job application.
Our hiring platform will guide you through a series of online “screening” assessments to check for basic job fit, job-related skills, and finally a few real-world job-specific assignments.
Important!
If you do not receive an email from us:
First, emails may take up to 15 minutes to send, refresh and check again.
Second, check your spam and junk folders for an email from Crossover.com, mark as “Not Spam” since you will receive other emails as well.
Third, we will send to whatever email account you indicated on the Apply form - by default, that is the email address you use as your LinkedIn username and it might be different than the one you have already checked.
If all else fails, just reset your password by visiting https://www.crossover.com/auth/password-recovery if you already applied using LinkedIn EasyApply.
Crossover Job Code: LJ-4880-US-Fulton-DataResearchAn.002
Show more
Show less","Python, JavaScript, JSON, OOP, Data Science, Generative AI, Machine Learning, Natural Language Processing, Cloud Computing, Agile Development, Software Engineering, Product Management, Communication, Collaboration, Problem Solving, Critical Thinking, Innovation, Research, Analysis","python, javascript, json, oop, data science, generative ai, machine learning, natural language processing, cloud computing, agile development, software engineering, product management, communication, collaboration, problem solving, critical thinking, innovation, research, analysis","agile development, analysis, cloud computing, collaboration, communication, critical thinking, data science, generative ai, innovation, javascript, json, machine learning, natural language processing, oop, problem solving, product management, python, research, software engineering"
MuleSoft Enterprise Data Engineer,Schrödinger,San Diego Metropolitan Area,https://www.linkedin.com/jobs/view/mulesoft-enterprise-data-engineer-at-schr%C3%B6dinger-3719239074,2023-12-17,California,United States,Associate,Hybrid,"We’re seeking a
MuleSoft Enterprise
Data Engineer to fill a temporary-to-permanent position
and join us in our mission to improve human health and quality of life through the development, distribution, and application of advanced computational methods!
Schrödinger is on the cutting edge of computer-aided drug discovery and materials science, collaborating with companies like Takeda, Nimbus, Pfizer, and Sanofi. We set the record for the largest and fastest cloud computing run, and our software suites continue to revolutionize the design of therapeutics and materials. WaterMap, Maestro, and LiveDesign are just a few examples of the programs we’ve created.
As a member of our Data team, you’ll play a critical role in the administration and improvement of our corporate data management system.
Who Will Love This Job
A data engineer with meaningful architecture experience
A creative problem-solver with an attention to detail
An excellent team player who wants to make a sizable impact working on a small, dedicated department
A lifelong student who’s eager to learn new and varied technologies and business processes on the job
A humanitarian who cares deeply about improving human health
What You’ll Do
Provide technical leadership towards architecting and delivering end to end solutions
Work with business partners for prioritization, impact assessment, and resolution
Collaborate with other teams for multi-functional initiatives
Design and build reusable components, frameworks and libraries at scale to support analytics products
Devise and implement product features in collaboration with business and technology partners
Develop architecture and design patterns to process and store high volume data sets
Identify and tackle issues concerning data management to improve data quality
Collaborate on the implementation of new data management projects and re-structure of the current data architecture
Build continuous integration, test-driven development and production deployment frameworks
Tackle data issues and perform root cause analysis to proactively resolve product and operational issues
What You Should Have
At least four years of relevant experience, with detailed knowledge of data warehouse technical architectures, infrastructure components, ETL/ ELT, and reporting/analytic tools
At least three years of experience in cloud environments like GCP and BigQuery
Experience developing custom features and solutions for ERP/CRM systems in similar roles, especially using Apex in Salesforce
Understanding of relational and non-relational SQL
Familiarity with building real-time streaming data pipelines
Background in pub/sub modes like Kafka
Experience in building lambda, kappa, microservice and batch architecture
Knowledge of CI/CD processes and source control tools such as GitHub and related dev processes
Experience with MuleSoft
Experience with NetSuite and/or Salesforce Lightning is a major plus
If you don’t consider yourself an expert in all of the fields
above
, that’s OK - we’re looking to hire an adaptable polymath who’s excited to learn on the job. Our team will be happy to help you get up to speed in any unfamiliar areas!
Pay And Perks
Schrödinger understands it’s people that make a company great. Because of this, we’re prepared to offer a competitive salary, stock options, and a wide range of benefits that include healthcare (with dental and vision), a 401k, pre-tax commuter benefits, a flexible work schedule, and a parental leave program. We have catered meals in the office every day, a company culture that is relaxed but engaged, and over a month of paid vacation time. Our Administrative and Human Resources departments also plan a myriad of fun company-wide events. New York is home to our largest office, but we have teams all over the world. Schrödinger is honored to have been selected as one of Crain's New York Best Places to Work in 2018 and 2019.
Sound exciting? Apply today and join us!
Estimated base salary range: $105,000 - $165,000. Actual compensation package is dependent on a number of factors, including, for example, experience, education, degrees held, market data, and business needs. If you have any questions regarding the compensation for this role, do not hesitate to reach out to a member of our Strategic Growth team.
As an equal opportunity employer, Schrödinger hires outstanding individuals into every position in the company. People who work with us have a high degree of engagement, a commitment to working effectively in teams, and a passion for the company's mission. We place the highest value on creating a safe environment where our employees can grow and contribute, and refuse to discriminate on the basis of race, color, religious belief, sex, age, disability, national origin, alienage or citizenship status, marital status, partnership status, caregiver status, sexual and reproductive health decisions, gender identity or expression, sexual orientation, or any other protected characteristic. To us, ""diversity"" isn't just a buzzword, but an important element of our core principles and key business practices. We believe that diverse companies innovate better and think more creatively than homogenous ones because they take into account a wide range of viewpoints. For us, greater diversity doesn't mean better headlines or public images - it means increased adaptability and profitability.
Show more
Show less","MuleSoft, NetSuite, Salesforce Lightning, Cloud Architectures, Data warehouse architecture, ETL/ELT, Reporting Tools, Apex, SQL, Relational Databases, NonRelational Databases, Realtime Streaming Data Pipelines, Apache Kafka, Lambda Architecture, Kappa Architecture, Microservice Architecture, Batch Architecture, CI/CD, GitHub, Source Control Tools, DevOps, Data Quality Management, Data Transformation, Data Analytics, Machine Learning, Distributed Systems, Scalable Systems, Data Integration, Data Governance","mulesoft, netsuite, salesforce lightning, cloud architectures, data warehouse architecture, etlelt, reporting tools, apex, sql, relational databases, nonrelational databases, realtime streaming data pipelines, apache kafka, lambda architecture, kappa architecture, microservice architecture, batch architecture, cicd, github, source control tools, devops, data quality management, data transformation, data analytics, machine learning, distributed systems, scalable systems, data integration, data governance","apache kafka, apex, batch architecture, cicd, cloud architectures, data governance, data integration, data quality management, data transformation, data warehouse architecture, dataanalytics, devops, distributed systems, etlelt, github, kappa architecture, lambda architecture, machine learning, microservice architecture, mulesoft, netsuite, nonrelational databases, realtime streaming data pipelines, relational databases, reporting tools, salesforce lightning, scalable systems, source control tools, sql"
"Healthcare Data Analyst or Senior Healthcare Data Analyst, Analytics Hub",ECG Management Consultants,"San Diego, CA",https://www.linkedin.com/jobs/view/healthcare-data-analyst-or-senior-healthcare-data-analyst-analytics-hub-at-ecg-management-consultants-3785922736,2023-12-17,California,United States,Associate,Hybrid,"Overview
ECG is a national management consulting firm working exclusively in the healthcare industry. At ECG, our primary emphasis is on quality—in our people as well as our services—and we’re seeking others who appreciate our high standards of excellence.
What’s in It for You: Consult with Purpose
At ECG, you can have a bigger impact than you ever imagined. The work you do will help health systems deliver care more effectively and efficiently—and that’s just the beginning. We’re looking for innovators, problem-solvers, and self-starters to collaborate across our five divisions, take on challenging projects, and find new ways to improve patient care. ECG is committed to ensuring a friendly work environment that rewards high performance and welcomes, values, and supports all people.
Join Our Analytics Hub
Our growing Analytics Hub works with our consulting teams to deliver new insights, streamlined analysis, and world-class intelligence to the healthcare organizations we serve as clients. Combined with our consulting teams, you will be helping physician and executive leadership make informed decisions that can alter the course of their enterprises in areas such as physician alignment, mergers and acquisitions, service line planning, provider compensation, financial performance, transforming care models, patient access, managed care portfolio optimization, digital health expansion, and more. ECG uses a Microsoft Azure and Microsoft 365 environment, with Microsoft Power BI as ECG’s current analytics visualization platform. The goal of the position is to further capitalize on the existing ECG infrastructure, as well as implement new features and technologies. The ideal candidate is open to new challenges, exceptional at multitasking, and proficient at implementing new solutions.
Our detailed plans incorporate qualitative findings with data-driven strategic, operational, and financial considerations that enable organizations to pursue realistic change. Our consultants bridge the gap between strategic thinking and operational implementation with sensible action plans and tactical recommendations. This approach allows us to stretch our clients’ thinking while ensuring that initiatives with the broadest and deepest impact are prioritized and implementable. Our wide-ranging engagements often focus on helping our clients:
Position themselves for value-based care delivery.
Align hospitals and physician organizations.
Strengthen financial performance.
Enhance clinical programs.
Develop strategic partnerships and/or mergers.
Your Opportunity with ECG: Data Analyst or Senior Data Analyst
As a data analyst or senior data analyst, you will support the firm’s client delivery and business development efforts, working with consultants and senior leaders to manage large data sets, aggregate internal and external data, develop scalable models and other analyses, and support data visualization and presentation sets. In addition, the analyst will work directly with our internal support teams (IT, business development, and L&D) and associated vendors for data management and IT solutions and will assist with a range of innovative analytic opportunities. Here, no two days or projects are alike, which means you’ll have a lot to learn and plenty of support to help you succeed.
Your Responsibilities May Include:
Gathering and synthesizing data from various sources (e.g., national surveys, proprietary research, client interviews, industries, markets)
Preparing analyses related to hospital inpatient databases and provider claims data (Stratasan)
Building flexible, dynamic, and scalable financial and operational models to forecast trends, preparing scenarios for consulting efforts related to delivery and business development (e.g., strategic planning, operational improvement, market assessments)
Enhancing existing models and capabilities built using SQL, primarily, with some use of Python or R in certain models or scenarios
Performing complex analyses on big data, including cleaning, preparing, and interpreting
Identifying and executing analyses in response to consultant business intelligence inquiries
Building relational databases using a Microsoft SQL Server or Fabric
Communicating market insights to pursuit teams, and supporting business development efforts
Configuring, maintaining, and building flexible reports, queries, and visuals within the Power BI environment and the production and development environments of related databases
Researching, testing, and implementing solutions in Power BI
Partnering with various consulting units (strategy, academic health, provider financial services, performance transformation, etc.) to gather requirements related to data analytics
Developing, configuring, and maintaining interfaces to exchange data among Power BI, SQL databases, and other cloud-based applications
Creating and maintaining analytical support documentation and operating procedures
Developing new and innovative methodologies and approaches
Contributing to internal education and learning opportunities for consultants and operations members for the Analytics Hub and analytics in general, and supporting education about ECG’s use cases related to analytics in work
Assisting the director of data analytics in developing analytics and data strategy to support data analytics capabilities at the firm
Our Expectations of You
A bachelor’s degree in either computer science, computer engineering, mathematics, statistics, health information management, health administration, business, or a related degree that relies heavily on critical thinking, logic, and math
Prior data analytics work experience using databases, SQL, and Power BI in a healthcare or enterprise environment (five+ years of experience for senior analyst level; one to three years for analyst)
Experience using Python, R, Fabric, and Tableau a plus
Proficiency in Microsoft Word, Excel, and PowerPoint
Comfort with manipulating and synthesizing large data sets
Strong written and verbal communication skills
Excellent organizational skills
Job Locations
St. Louis and Washington, DC, offices are preferred. Other offices may include Atlanta, Boston, Chicago, Minneapolis, San Diego, or Seattle. Able to work a hybrid schedule with days in the office and remote.
Schedule
Full time/exempt
What You Can Expect Of Us
To reward our driven, innovative, and passionate employees, we’ve built a company culture that’s centered on performance. We offer an attractive compensation package, challenging work, and an entrepreneurial environment where you can take ownership of your career—and get out as much as you put in.
About ECG
ECG is a strategic consulting firm leading healthcare forward using knowledge and expertise built over the course of five decades to help clients see clearly where the industry is going and navigate toward success. We work as trusted, professional partners with hospitals, health systems, medical groups, and academic medical centers across the country. We thrive on delivering smart counsel and pragmatic solutions to the critical challenges facing healthcare providers. Client success is our primary objective. ECG’s national presence includes offices in Atlanta, Boston, Chicago, Dallas, Minneapolis, San Diego, Seattle, St. Louis, and Washington, DC.
Apply now and make an impact for years to come.
To begin the recruitment process, please submit your resume via our career site at https://careers.ecgmc.com.
ECG provides equal employment opportunities to all employees and applicants for employment without regard to sex, race, color, religion, national origin, citizenship, ancestry, age, disability, pregnancy, medical condition (cancer and genetic characteristics), genetic information, gender, gender identity or expression, sexual orientation, marital status, military or veteran status, or any other legally protected characteristic. People of color are encouraged to apply. We participate in E-Verify as part of our onboarding process. Having the permanent legal right to work in the United States is a condition of employment. ECG is not currently able to provide assistance to candidates requiring sponsorship or a visa.
Residents of the states of California or Washington may receive salary information for this job through this link or by contacting the recruiter directly at schavez@ecgmc.com.
Show more
Show less","SQL, Python, R, Fabric, Tableau, Microsoft Power BI, Microsoft Word, Microsoft Excel, Microsoft PowerPoint, Microsoft Azure, Microsoft 365, Scrapping, Data Analysis, Data Visualization, Data Management, Data Mining, Data Modeling, Statistical Analysis, Financial Modeling, Operational Modeling, Market Research, Business Intelligence, Consulting, Problem Solving, Communication, Teamwork, Leadership, Innovation, Multitasking, Adaptability, Analytical Thinking, Critical Thinking, Logical Thinking, Strategic Thinking, Decision Making, Problem Solving, Presentation Skills, Written Communication, Verbal Communication, Project Management, Time Management, Organization Skills","sql, python, r, fabric, tableau, microsoft power bi, microsoft word, microsoft excel, microsoft powerpoint, microsoft azure, microsoft 365, scrapping, data analysis, data visualization, data management, data mining, data modeling, statistical analysis, financial modeling, operational modeling, market research, business intelligence, consulting, problem solving, communication, teamwork, leadership, innovation, multitasking, adaptability, analytical thinking, critical thinking, logical thinking, strategic thinking, decision making, problem solving, presentation skills, written communication, verbal communication, project management, time management, organization skills","adaptability, analytical thinking, business intelligence, communication, consulting, critical thinking, data management, data mining, dataanalytics, datamodeling, decision making, fabric, financial modeling, innovation, leadership, logical thinking, market research, microsoft 365, microsoft azure, microsoft excel, microsoft power bi, microsoft powerpoint, microsoft word, multitasking, operational modeling, organization skills, presentation skills, problem solving, project management, python, r, scrapping, sql, statistical analysis, strategic thinking, tableau, teamwork, time management, verbal communication, visualization, written communication"
Lead Data Platform Architect,"Vir Biotechnology, Inc.","San Francisco, CA",https://www.linkedin.com/jobs/view/lead-data-platform-architect-at-vir-biotechnology-inc-3715899529,2023-12-17,California,United States,Associate,Hybrid,"Vir Biotechnology, Inc. is an immunology company focused on combining cutting-edge technologies to treat and prevent infectious diseases and other serious conditions. Vir has assembled two technology platforms that are designed to stimulate and enhance the immune system by exploiting critical observations of natural immune processes. Its current clinical development pipeline consists of product candidates targeting hepatitis B and hepatitis delta viruses and human immunodeficiency virus. Vir has several preclinical candidates in its pipeline, including those targeting influenza A and B, COVID-19, RSV/MPV and HPV.
We believe the success of our colleagues drives the success of our mission. We are committed to creating a company passionate about equality, inclusion, and respect. When everyone feels supported and encouraged to give their best, we will collectively deliver outstanding results. We are proud to be the first company to be ranked at the top of the Deloitte Fast 500 list two years in a row (2023 & 2022)!
The Data Science and Operations (DSO) group is looking for a Lead Data Platform Architect to join the Data Engineering team in San Francisco. You will report to the Senior Director of Data Engineering and will collaborate with multiple departments across Vir. You will build data platforms and analytical capabilities to support and enhance business operations throughout the organization. This is a hybrid role and you will work a few days each week from our HQ in San Francisco.
DSO is made up of engineers, bioinformaticians, genome analysts, and machine learning specialists building data infrastructure, pipelines, and data analytics across Vir.
RESPONSIBILITIES AND LEARNING OPPORTUNITIES:
Lead the architecture, design, and deployment of an enterprise healthcare data lake.
Help design core data platforms to allow the building of bioinformatics pipelines, machine learning models, data science applications and custom applications and dashboards.
Participate as a member of the VIR data architecture forum group.
Evaluate and select AWS technologies including data governance technologies to support building an enterprise healthcare data lake.
Develop a scalable and reliable infrastructure that supports bioinformatics pipelines and machine learning model deployment.
Be a liaison between IT, Machine Learning, Bioinformatics and the Data Engineering Team
Review and enhance documentation and the Vir knowledgebase.
QUALIFICATIONS AND EXPERIENCE:
12+ years development experience in Python or similar object-oriented languages.
5+ years of experience as a full stack developer.
Experience as lead or principal data engineer.
Deep experience with AWS technologies, APIs, MLOps.
Building data lakes using AWS or similar technologies.
Designing and building data processing pipelines using Next flow, Airflow, or similar technologies.
Enterprise data governance, the machine learning lifecycle and LLMs and their use.
Code/Build/Deployment: GIT, Docker, Jenkins, Kubernetes.
Scientific pipelines, i.e. Bioinformatics (genomic processing) pipelines, on AWS or other cloud vendors.
Executive level dashboard using Tableau or Spotfire.
Strong database experience, i.e. opensource vector databases like Milvus, Pinecone.
Relevant certifications would include AWS Certified Data Analytics or AWS Certified Machine Learning.
BS in Computer Science or related discipline, or equivalent industry experience.
_AA1
The expected salary range for this position is $168,500.00 to $246,500.00. Actual pay will be determined based on experience, qualifications, geographic location, and other job-related factors.
Vir's compensation and benefits are aligned with the current market and commensurate with the person's experience and qualifications. All full-time employees receive a package that includes: compensation, bonus and equity as well as many other Vir benefits and perks such as: health, dental, vision, life and disability insurance benefits, non-accrual paid time off, company shut down for holidays, commuter benefits, child care reimbursement, education reimbursement, 401K match and lunch for all onsite employees!
Vir Biotechnology (“Vir”) is an equal opportunity employer. All employment decisions at Vir are based on legitimate, non-discriminatory business requirements, job duties and individual qualifications. Employment decisions are made without regard to race, color, religion, sex (including pregnancy), gender, gender identity, gender expression, sexual orientation, age, parental status, marital status, national origin, ancestry, disability, medical condition, genetic information (including family medical history), political affiliation, military service or any other legally protected characteristic.
This commitment extends to all management practices and decisions, including recruitment and hiring, compensation, appraisal systems, promotions, training and career development programs. Vir also strongly commits to providing employees with a work environment free of unlawful discrimination or harassment.
Vir Human Resources leads recruitment and employment for Vir. Unsolicited resumes sent to Vir from recruiters do not constitute any type of relationship between the recruiter and Vir and do not obligate Vir to pay fees should we hire from those resumes. We ask that external recruiters and/or agencies not contact or present candidates directly to our hiring manager or employees.
For hires based in the United States, Vir Biotechnology, Inc., participates in E-Verify.
Candidate Privacy Notice
Show more
Show less","Python, AWS, MLOps, Nextflow, Airflow, Kubernetes, Docker, Jenkins, Git, Data lake, Bioinformatics pipelines, Machine learning models, Tableau, Spotfire, Milvus, Pinecone, AWS Certified Data Analytics, AWS Certified Machine Learning","python, aws, mlops, nextflow, airflow, kubernetes, docker, jenkins, git, data lake, bioinformatics pipelines, machine learning models, tableau, spotfire, milvus, pinecone, aws certified data analytics, aws certified machine learning","airflow, aws, aws certified data analytics, aws certified machine learning, bioinformatics pipelines, data lake, docker, git, jenkins, kubernetes, machine learning models, milvus, mlops, nextflow, pinecone, python, spotfire, tableau"
Principal Data Analyst,Barracuda,"Campbell, CA",https://www.linkedin.com/jobs/view/principal-data-analyst-at-barracuda-3770804394,2023-12-17,California,United States,Associate,Hybrid,"J ob ID 24-453
Come Join Our Passionate Team! At Barracuda, we make the world a safer place. We believe every business deserves access to cloud-enabled, enterprise-grade security solutions that are easy to buy, deploy, and use. We protect email, networks, data and applications with innovative solutions that grow and adapt with our customers’ journey. More than 200,000 organizations worldwide trust Barracuda to protect them — in ways they may not even know they are at risk — so they can focus on taking their business to the next level.
We know a diverse workforce adds to our collective value and strength as an organization. Barracuda Networks is proud to be an Equal Opportunity Employer, committed to equal employment opportunity and equitable compensation regardless of race, gender, religion, sex, sexual orientation, national origin, or disability.
Envision yourself at Barracuda
The Global Data Analytics group at Barracuda enables business teams across the enterprise to make critical operational & strategic decisions through deep data analysis, insightful BI Reporting and advanced analytics. As we are building a world class team and capabilities across all the above areas, we are looking for a highly hands on Principal Data Analyst to join our team. The person will get a fantastic opportunity to work hand in hand with a broad range of business groups and be directly responsible for enabling them to meet & exceed their goals through data analysis & reporting
What You’ll Be Working On
Work with VP’s, C level Execs and business analysts to clearly understand their business needs and translate them into clear data projects/ requirements.
Do detailed analysis of multiple large datasets using SQL and be able to derive specific insights and inference.
Quickly create intuitive dashboards and reports based for multiple business domains and communicate the value to the business audience effectively.
Work with data engineers, data architects and other data analysts to establish data pipelines and automation for the business data layer.
Create short presentations on data projects or data analysis findings and be able to effectively communicate the business value of the work to multiple audiences.
Strong understanding and business acumen of GTM functions (Marketing, Sales Ops, Finance, Customer Success) and the associated datasets
Highly self-driven and able to lead data projects and initiatives with minimal supervision. Initiate and lead business and data projects end to end collaborating with business partners and technology teams.
What You Bring To The Role
Conversant in advanced SQL with at least 6-8 years hands on experience in data analysis, on a day-to-day basis with a Masters or 8+ years of experience with a Bachelors
At least 2 years’ experience in python scripting and skillful to automate data processes as required
3 to 5 years’ experience working in a data warehouse environment, with strong familiarity in dimensional models, facts, dimensions and aggregate tables for reporting
Hands on experience building dashboards and reports with any reporting tools preferably PowerBI
Experience working in cloud based relational DB platforms (Databricks, SQL Server, Oracle)
Demonstrate sense of urgency to set and accomplish goals with strong focus on data quality and data governance
Highly skillful in complex data analysis, ability to get to root cause of data issues quickly and implement work arounds or permanent solutions to keep the business moving with quality data
What You’ll Get From Us
A team where you can voice your opinion, make an impact, and where you and your experience are valued. Internal mobility – there are opportunities for cross training and the ability to attain your next career step within Barracuda.
Equity, in the form of non-qualifying options
High-quality health benefits
Retirement Plan with employer match
Career-growth opportunities
Flexible Time Off and Paid Time Off benefits
Volunteer opportunities
The anticipated on-target earnings range for this role is $138,000-$160,000 at Campbell. Actual compensation offered will be dependent upon the individual's skills, experience, and qualifications as they directly relate to the requirements of the position, the budget for the position, and applicable employment laws.
Show more
Show less","SQL, Python, Power BI, Databricks, SQL Server, Oracle, Cloudbased relational DB platforms, Dimensional modeling, Data governance, Data analysis, Business intelligence, Reporting, Data visualization, Data warehousing, Data pipelines, Automation, Data quality","sql, python, power bi, databricks, sql server, oracle, cloudbased relational db platforms, dimensional modeling, data governance, data analysis, business intelligence, reporting, data visualization, data warehousing, data pipelines, automation, data quality","automation, business intelligence, cloudbased relational db platforms, data governance, data quality, dataanalytics, databricks, datapipeline, datawarehouse, dimensional modeling, oracle, powerbi, python, reporting, sql, sql server, visualization"
Data Scientist,VeeAR Projects Inc.,"Sunnyvale, CA",https://www.linkedin.com/jobs/view/data-scientist-at-veear-projects-inc-3767587764,2023-12-17,California,United States,Associate,Hybrid,"Job Description
We are looking for an experienced AI Engineer.
The successful candidate will be responsible for fine-tuning existing foundation models, developing new models, and offering them as APIs.
Additionally, the candidate will also build IDE plugins (For eg, Intellij) that consumes these APIs for code generation, test case generation, document generation, and automatic code reviews.
Key Responsibilities:
Develop and fine-tune a large language model for the Java (and other) programming language.
Develop new models for specific use cases.
Offer models as APIs for consumption by other tools and services.
Build an Intellij plugin that consumes the APIs for code generation, test case generation, document generation, and automatic code reviews.
Collaborate with cross-functional teams to gather requirements and ensure seamless integration of the language model and plugin.
Stay up-to-date with the latest developments in AI and LLM space and incorporate them into the language model and plugin.
Ensure the accuracy and performance of the language model and plugin through testing and quality assurance processes (like building testbeds)
Requirements:
Strong knowledge of machine learning (Client) techniques and Large Language Models (LLMs).
Proficiency in programming languages such as Python, Java, Go etc.
Experience with deep learning frameworks such as TensorFlow, PyTorch, and Keras.
Strong communication skills and ability to work in a team environment.
Familiarity with Intellij IDEA and its plugin development framework is a plus.
Show more
Show less","Machine Learning, Client, Large Language Models (LLMs), Python, Java, Go, TensorFlow, PyTorch, Keras, Intellij IDEA, Plugin Development Framework, Code Generation, Test Case Generation, Document Generation, Automatic Code Reviews","machine learning, client, large language models llms, python, java, go, tensorflow, pytorch, keras, intellij idea, plugin development framework, code generation, test case generation, document generation, automatic code reviews","automatic code reviews, client, code generation, document generation, go, intellij idea, java, keras, large language models llms, machine learning, plugin development framework, python, pytorch, tensorflow, test case generation"
Community Programs Predictive Data Analyst,Brilliant Corners,"Los Angeles, CA",https://www.linkedin.com/jobs/view/community-programs-predictive-data-analyst-at-brilliant-corners-3765387213,2023-12-17,California,United States,Associate,Hybrid,"Salary:
$6994/month - $7916/month
Location:
Los Angeles
Organization Overview
Founded in 2004, Brilliant Corners provides innovative housing and housing-related services to California’s most vulnerable individuals, with an emphasis on those transitioning from, or at risk of, homelessness or institutionalization. We develop, own, and manage multi-family supportive housing and licensed residential care homes. We implement a broad array of scattered-site, one-on-one supportive housing, and clinical case management programs through partnerships with developmental services, homeless services, veterans’ services, and health care sectors.
In short, we do good work.
We have offices throughout California, currently have almost 500 staff members, and are growing daily. We’re looking for inspired, talented people who want to be part of a team that’s affecting profound change and is having fun doing it.
Position Summary
This Data Analyst role will be contracted with DHS as part of our partnership in the Housing for Health program.
Housing for Health was founded in 2012 to provide housing and services to patients who frequented County public hospitals and were homeless. The program has grown to now provide a full continuum of services to clients, from street outreach to interim housing to permanent housing, with case management, benefits advocacy, and clinical services layered across all service categories. Housing for Health maintains its core ethos and founding principles of improving the health and quality of life of the most vulnerable homeless individuals in L.A. County. Community Programs is a program office within Health Services Administration, a division under the Los Angeles County Department of Health Services (DHS). In addition to other responsibilities, this position will have a supportive role in the tracking, reporting, quality assurance, and billing efforts for CalAIM, a Medi-Cal waiver program.
Essential Functions
Reporting and Analysis
Assists department stakeholders to define data needs and requirements to ensure data science projects meet business needs at the community, regional, divisional, departmental, or county-wide level
Uses statistical programming languages and packages, computer scripting languages, and/or domain-specific languages to digest, manipulate, prepare, augment, evaluate, analyze, summarize, and visualize data
Conducts end-to-end predictive data modeling projects that include basic exploratory data analysis, feature engineering, model selection, deployment of models into production environments, and visualization of findings
Performs data management and manipulation, including the writing of data queries of departmental databases and data marts, data cleaning, and joining of multiple and disparate data sets
Extracts and organizes data and applies quality-control techniques to ensure the accuracy and completeness of all data; researches source documents and coordinates with others to resolve missing or conflicting data and other anomalies
Ensures regular reporting occurs within specified deadlines
Regularly creates deliverables for special projects and assignments, such as ad-hoc department reporting and program analyses
Creates documentation for the processes and procedures used for reporting and analyses
Tool Development and Maintenance
Collaborates with other department subject matter experts to understand, identify, and select available and relevant sources of data for use cases, including internal and external data sources
Helps refine program management practices through the creation of new reporting and analytical tools
Helps to design, build, and maintain analytical support tools to be used within the Department of Health Services, by housing and service providers, and by other external stakeholders
Project Management
:
Collaborates with data administrators and other IT staff to develop data pipelines and ensure the availability of quality data
Convenes and facilitates discussions with stakeholders to design and implement tools and reports
Manages the design, development, implementation and maintenance of reports, dashboards and analysis tools
Program and Provider Support
Works with program staff to understand the implications of analyses and to ensure that findings are actionable and support data-driven program, policy, and operational decision- making
Assists in coordinating with various functional teams to develop and implement products, services, tools, or business process changes resulting from the analysis
Provides regular analyses of the impact of changes in management practices and policies
Designs program reports, analyzes performance, and provides resulting analyses to stakeholders
Requirements
The ideal candidate has experience working directly with a client-centric relational database and with managing individuals. They have strong technical knowledge and are comfortable doing analyses and building reports out of a SQL database. They are comfortable with a variety of analytical tools and have taken the lead on projects where the best path forward is unclear. They have a customer service mindset. They have a passion for social services and an interest in looking beyond the data.
Education/Experience
Option I:
A Bachelor’s degree from an accredited college or university in a field of applied research such as Data Science, Machine Learning, Mathematics, Statistics, Business Analytics, Psychology, or Public Health that included 12 semester or 18 quarter units of coursework in data science, predictive analytics, quantitative research methods, or statistical analysis
-AND-
Two (2) years of experience in the application of techniques of machine learning, predictive analytics, data management, and hypothesis-driven data analysis to complex experimental designs leading to actionable findings and recommendations
-OR-
A Master’s or Doctoral degree from an accredited college or university in a field of applied research such as Data Science, Machine Learning, Mathematics, Statistics, Business Analytics, Psychology, or Public Health.
Option II:
Four (4) years of experience with responsibility for planning, designing, conducting, and evaluating research projects involving the application of techniques of machine learning, predictive analytics, data management, and/or hypothesis-driven data analysis to complex experimental designs leading to actionable findings and recommendations.
Certificates/Licenses/Clearances
Successful clearing through the Live Scan process with the County of Los Angeles
WORK ENVIRONMENT
Hybrid. Currently 1 day per month required in-office (Downton Los Angeles), may eventually increase up to 1 day per week max
Otherwise Remote Work with a set schedule
This contracted position will terminate 10/1/2025
Job Title: Community Programs Predictive Data Analyst
Program Name: DHS Housing for Health
Location: Los Angeles, CA
Position Status: Full Time Non-Exempt Salaried
Salary: $6994 - $7916/mo ($40.35 - $45.66 hourly)
Benefits
Health Care Plan (Medical, Dental & Vision)
Retirement Plan (401k, IRA)
Life Insurance (Basic, Voluntary & AD&D)
Paid Time Off (Vacation, Sick & Public Holidays)
Family Leave (Maternity, Paternity)
Short Term & Long Term Disability
Hybrid Work
Show more
Show less","Data Analysis, Predictive Analytics, Machine Learning, Statistical Programming, Data Management, Data Manipulation, SQL, Data Visualization, Tableau, Power BI, R, Python, SAS, Hadoop, Spark, Hive, Pig, NoSQL, MongoDB, Cassandra, HBase, Cloud Computing, AWS, Azure, Google Cloud Platform, Data Warehousing, Data Mining, Text Mining, Artificial Intelligence, Natural Language Processing, Business Intelligence, Reporting, Dashboards, Project Management, Agile, Scrum, Kanban, Waterfall, Communication, Teamwork, Problem Solving, Critical Thinking, Attention to Detail, Customer Service, Social Services","data analysis, predictive analytics, machine learning, statistical programming, data management, data manipulation, sql, data visualization, tableau, power bi, r, python, sas, hadoop, spark, hive, pig, nosql, mongodb, cassandra, hbase, cloud computing, aws, azure, google cloud platform, data warehousing, data mining, text mining, artificial intelligence, natural language processing, business intelligence, reporting, dashboards, project management, agile, scrum, kanban, waterfall, communication, teamwork, problem solving, critical thinking, attention to detail, customer service, social services","agile, artificial intelligence, attention to detail, aws, azure, business intelligence, cassandra, cloud computing, communication, critical thinking, customer service, dashboard, data management, data manipulation, data mining, dataanalytics, datawarehouse, google cloud platform, hadoop, hbase, hive, kanban, machine learning, mongodb, natural language processing, nosql, pig, powerbi, predictive analytics, problem solving, project management, python, r, reporting, sas, scrum, social services, spark, sql, statistical programming, tableau, teamwork, text mining, visualization, waterfall"
Privacy Data Analyst,Venable LLP,"Los Angeles, CA",https://www.linkedin.com/jobs/view/privacy-data-analyst-at-venable-llp-3725941694,2023-12-17,California,United States,Associate,Hybrid,"Venable LLP’s Technology & Innovation Group seeks a Privacy Data Analyst to join the Venable Blue team in the Washington, D.C., New York, Los Angeles, or San Francisco office. The Privacy Data Analyst, works on and supports all aspects of client-based projects related to privacy program management, privacy operations, and regulatory response. This is a non-attorney position.
This role will help bring efficiency to how clients respond to regulatory compliance requests, including data gathering, automation of regulatory responses, and measuring operational effectiveness of privacy safeguards. This will include gap analysis and actionable recommendations for improvements in quality and accuracy.
The Privacy Data Analyst will work on and support the delivery of various workstreams such as Safeguard Maintenance, Safeguard Development, Program Management, and Audit Support.
Key responsibilities include working with clients to:
Formulate responses to privacy compliance requests within SLA, perform data quality reviews, and conduct operational effectiveness testing against privacy safeguards.
Develop and communicate best practices for regulatory response.
Develop automation for compliance responses to increase efficiency and to provide consistency and quality in responses.
Articulate data and technology gaps and their compliance impact to a variety of technical and non-technical stakeholders, including product and engineering teams, risk and compliance partners, assessors and regulators.
Recommend process improvements and strategic initiatives as related to privacy compliance response.
Coordinate and drive client privacy response activities for both inbound and outbound relationships.
Support business relationships with internal and external auditors and regulators.
Qualifications:
B.A. or B.S. degree; Master’s degree (or equivalent) preferred
3 years of minimum work experience in high profile settings, such as presenting to leadership and driving cross-functional teams
3 years of minimum experience in a quantitative role related to data, reporting, and analytical problem solving
3 years of minimum experience writing complex SQL queries to drive analysis and insights
Experience coding in Python
Preferred Qualifications:
Experience coding in R, PHP and/or similar programming language
Experience with developing security/privacy reporting and recommendations that are meaningful, defensible and actionable for a variety of audiences
Experience developing and submitting audit and compliance reports to governing bodies, legal entities, and/or external authorities
Experienced in processes for assessing and designing internal controls for large scale organizations
Experience performing risk assessments or safeguard/control operational effectiveness testing
Venable offers full-service solutions to everything from routine to novel privacy and cybersecurity challenges. Our team brings to bear significant experience and industry knowledge to help clients satisfy data privacy and security laws and maximize their business potential. Fully immersed in all aspects of data privacy, cybersecurity, and information governance, Venable is unique among privacy and cybersecurity practices.
We participate in legislative advocacy, rulemakings, and development of new legal standards. Our team advises organizations with regard to industry best practices and drafting codes of conduct and standards, helping them stay compliant with federal, state, international, and self-regulatory requirements.
We strengthen the integrity of our clients’ data, ecommerce security, and customer or user records; develop internal data collection and use practices; and ensure the creation of sound privacy policies and procedures.
Venable Blue helps organizations and individuals manage and mitigate risk in the online space. Whether it’s an issue of data access, account takeover, cyber harassment, child safety, or a government or regulatory investigation, we build, operationalize, and deploy integrated programs and systems designed with people and products in mind.
For additional information about the Technology & Innovation Group, Venable Blue team see: www.venableblue.com
Venable LLP is an
American Lawyer
Global 100 law firm headquartered in Washington, D.C., with offices in California, Delaware, Florida, Illinois, Maryland, New York, and Virginia. Our lawyers and legislative advisors serve domestic and international clients in all areas of corporate and business law, complex litigation, intellectual property, regulatory matters, and government affairs. Additional information can be found at Venable.com.
The salary range for Privacy Data Analyst positions is $85,000 - $130,000 per year. This is the minimum and maximum salary that Venable in good faith believes at the time of this posting that it is willing to pay for the advertised position. Exact compensation will be determined based on individual candidate qualifications and location.
We comply with the Los Angeles Fair Chance Initiative for Hiring and the San Francisco Fair Chance Ordinance. Conviction of a crime will not necessarily be a bar to employment at the Firm. Factors such as age at the time of the offense, type of the offense, seriousness of the offense, remoteness of the offense in time, position applied for, rehabilitation, overall record, and other relevant factors will be taken into account in determining effect on suitability for employment.
Venable LLP is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, age, protected Veteran status and any other protected basis under applicable law.
Show more
Show less","SQL, Python, R, PHP, Coding, Data Analysis, Regulatory Compliance, Privacy Safeguards, Risk Assessment, Data Gathering, Data Privacy, Data Security, Automation, Program Management, Internal Controls, Legislative Advocacy, Cybersecurity, Information Governance","sql, python, r, php, coding, data analysis, regulatory compliance, privacy safeguards, risk assessment, data gathering, data privacy, data security, automation, program management, internal controls, legislative advocacy, cybersecurity, information governance","automation, coding, cybersecurity, data gathering, data privacy, data security, dataanalytics, information governance, internal controls, legislative advocacy, php, privacy safeguards, program management, python, r, regulatory compliance, risk assessment, sql"
"Senior, Data Engineer",Walmart,"Sunnyvale, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-walmart-3739239796,2023-12-17,California,United States,Mid senior,Onsite,"Position Summary...
What you'll do...
About The Team
The mission of Walmart Display Ads is to connect brand owners and shoppers with relevant display ads. We bring brand awareness and rich product information to shoppers and help brand owners grow their business with advanced targeting and optimization techniques. This is a fast-growing business never lack ing opportunities ! We are looking for an experienced tech lead to spearhead advertising data analytics related to deman d, supply, and overall marketplace healt h. You will be responsible for extracting meaningful insights about campaign performance, marketplace efficiency, and gaps in systems and products , surfacing marketplace health metrics via dashboards , and working with cross-functiona l partners to move the needle in display advertising.
What You Will Do
Lead and participate in small to large sized projects by reviewing project requirements; gather requested information; write and develop code; conduct unit testing; communicate status and issues to team members and stakeholders; collaborate with project team and cross functional teams; troubleshoot open issues and bug-fixes; and ensure on-time delivery and hand-offs.
Lead assigned projects by providing pertinent design, documents; identifying short- and long-term solutions and timeline; reviewing and providing feedback for proposed solutions; and performing design and code reviews of changes.
Provide guidance and mentorship to junior data.
Work closely with cross functional teams, including data scientists, product managers, and software engineers, to carry out various experiments across different areas of the AdTech platforms to improve customer experience and ad relevance.
Develop and maintain highly scalable and fault-tolerant data systems that process, store, and serve large volumes of data including online and offline user activities, impressions, and sales data.
Build and maintain data pipelines and ETL processes required for optimal extraction, transformation, and loading of data from a wide variety of data sources.
Ensure data ingested and processed is accurate and of high quality by implementing data quality checks, data validation, and data cleaning processes.
Implement and enhance machine learning pipelines for model evaluation and promotion.
What You'll Bring
Bachelor's degree in computer science or a related field.
5+ years of experience in the following areas in a Data Engineer role.
Expertise in one or more programming languages (Python, Java etc.)
Advanced Data Management: Proficiency in managing and manipulating huge datasets in the order of terabytes (TB) is essential. This includes knowledge of distributed file systems, data partitioning, and optimization techniques for handling large-scale data.
Big Data Technologies: Expertise in big data technologies like Hadoop, Apache Spark, Apache Hive, or similar frameworks is crucial for efficiently processing and analyzing massive volumes of data.
Data Integration: Skill in integrating data from multiple data sources is vital. This involves understanding various data formats, APIs, and protocols to effectively bring data together from diverse sources.
Data Modeling: Strong data modeling skills are necessary for designing a schema that can accommodate the evolution of data sources and facilitate seamless data joins across various datasets.
Database Management: Experience in working with large-scale databases is essential to handle the storage and retrieval of vast amounts of data efficiently.
Data Cleansing and Transformation: The ability to clean and transform data from different sources into a standardized format is crucial for accurate data analysis and effective data joins.
Query Optimization: Proficiency in optimizing queries to achieve better performance and reduce processing times is important when dealing with massive datasets.
Distributed Computing: Understanding the principles of distributed computing is vital for effectively utilizing resources across a cluster of machines to process and analyze large volumes of data.
Data Security and Privacy: Knowledge of data security and privacy best practices is essential to protect sensitive information when dealing with massive datasets from multiple sources.
Problem-Solving and Troubleshooting: Strong analytical and problem-solving skills are crucial for identifying and resolving issues that may arise during the data integration and schema evolution process.
Ability to effectively interpret technical and business objectives and challenges and articulate solutions.
Strong analytical skills and programming skills.
Technologies: Spark/Hive, Kafka, Apache Parquet, Avro, GCS, GCP, BigQuery, Python, Airflow
Nice To Have
Experience in the advertising domain
Experience in Machine Learning
About Walmart Global Tech
Imagine working in an environment where one line of code can make life easier for hundreds of millions of people. That is what we do at Walmart Global Tech. We are a team of software engineers, data scientists, cybersecurity expert's and service professionals within the world's leading retailer who make an epic impact and are at the forefront of the next retail disruption. People are why we innovate, and people power our innovations. We are people-led and tech-empowered. We train our team in the skillsets of the future and bring in experts like you to help us grow. We have roles for those chasing their first opportunity and those looking for the opportunity to define their career. Here, you can kickstart a distinguished career in tech, gain new skills and experience for virtually every industry, or leverage your expertise to innovate at scale, impact millions and reimagine the future of retail.
Flexible, Hybrid Work
We use a hybrid way of working that is primarily in the office coupled with virtual when not onsite. Our campuses serve as a hub to enhance collaboration, bring us together for purpose and deliver on business needs. This approach helps us make quicker decisions, remove location barriers across our global team and be more flexible in our personal lives.
Benefits
Benefits: Beyond our great compensation package, you can receive incentive awards for your performance. Other great perks include 401(k) match, stock purchase plan, paid maternity and parental leave, PTO , multiple health plans, and much more.
Equal Opportunity Employer
Walmart, Inc. is an Equal Opportunity Employer - By Choice. We believe we are best equipped to help our associates, customers, and the communities we serve live better when we really know them. That means understanding, respecting and valuing diversity- unique styles, experiences, identities, ideas, and opinions - while being inclusive of all people.
The above information has been designed to indicate the general nature and level of work performed in the role. It is not designed to contain or be interpreted as a comprehensive inventory of all responsibilities and qualifications required of employees assigned to this job. The full Job Description can be made available as part of the hiring process.
At Walmart, we offer competitive pay as well as performance-based incentive awards and other great benefits for a happier mind, body, and wallet. Health benefits include medical, vision and dental coverage. Financial benefits include 401(k), stock purchase and company-paid life insurance. Paid time off benefits include PTO (including sick leave), parental leave, family care leave, bereavement, jury duty, and voting. Other benefits include short-term and long-term disability, company discounts, Military Leave Pay, adoption and surrogacy expense reimbursement, and more.
You will also receive PTO and/or PPTO that can be used for vacation, sick leave, holidays, or other purposes. The amount you receive depends on your job classification and length of employment. It will meet or exceed the requirements of paid sick leave laws, where applicable. For information about PTO, see https://one.walmart.com/notices .
Live Better U is a Walmart-paid education benefit program for full-time and part-time associates in Walmart and Sam's Club facilities. Programs range from high school completion to bachelor's degrees, including English Language Learning and short-form certificates. Tuition, books, and fees are completely paid for by Walmart.
Eligibility requirements apply to some benefits and may depend on your job classification and length of employment. Benefits are subject to change and may be subject to a specific plan or program terms. For information about benefits and eligibility, see One.Walmart at https://bit.ly/3iOOb1J .
The annual salary range for this position is $117,000.00-$234,000.00
Additional Compensation Includes Annual Or Quarterly Performance Incentives.
Additional compensation for certain positions may also include:
Regional Pay Zone (RPZ) (based on location)
Stock equity incentives
Minimum Qualifications...
Outlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications.
Option 1: Bachelor's degree in Computer Science and 3 years' experience in software engineering or related field. Option 2: 5 years' experience in
software engineering or related field. Option 3: Master's degree in Computer Science and 1 year's experience in software engineering or related
field.
2 years' experience in data engineering, database engineering, business intelligence, or business analytics.
Preferred Qualifications...
Outlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications.
Data engineering, database engineering, business intelligence, or business analytics, ETL tools and working with large data sets in the cloud, Master's degree in Computer Science or related field and 3 years' experience in software engineering
Primary Location...
640 W California Avenue, Sunnyvale, CA 94086-4828, United States of America
Show more
Show less","Python, Java, Hadoop, Apache Spark, Apache Hive, Spark/Hive, Kafka, Apache Parquet, Avro, GCS, GCP, BigQuery, Airflow, ETL, Machine Learning, Data Management, Data Modeling, Database Management, Data Cleansing, Data Transformation, Query Optimization, Distributed Computing, Data Security, Data Privacy, ProblemSolving, Troubleshooting","python, java, hadoop, apache spark, apache hive, sparkhive, kafka, apache parquet, avro, gcs, gcp, bigquery, airflow, etl, machine learning, data management, data modeling, database management, data cleansing, data transformation, query optimization, distributed computing, data security, data privacy, problemsolving, troubleshooting","airflow, apache hive, apache parquet, apache spark, avro, bigquery, data management, data privacy, data security, data transformation, database management, datacleaning, datamodeling, distributed computing, etl, gcp, gcs, hadoop, java, kafka, machine learning, problemsolving, python, query optimization, sparkhive, troubleshooting"
Senior Data Engineer,"Verticalmove, Inc",Los Angeles Metropolitan Area,https://www.linkedin.com/jobs/view/senior-data-engineer-at-verticalmove-inc-3764330604,2023-12-17,California,United States,Mid senior,Onsite,"ABOUT OUR COMPANY:
Verticalmove is among the 2023 Inc. 5000 list of the fastest-growing private companies in America! We build digital transformation, product, and software engineering teams...
Verticalmove is a respected Digital Transformation and Technical Recruitment services company with more than 20 years of proven results, delivering solutions to Fortune 500 companies and start-up organizations by finding top professional talent before their competitors can.
About The Role:
As a part of our team, you will play a pivotal role in evangelizing and constructing Data Products that simplify critical Machine Learning (ML) and Analytics products. Your role will enrich the customer experience, streamline marketing operations, and contribute to the creation of high-quality data marts. Collaborating with data engineering and platform teams, you will lead the design, implementation, and operation of substantial big data pipelines and tools.
As a member of our team, your responsibilities will include:
Designing, building, and implementing data pipelines and products to support ML and Analytical use cases.
Collaborating closely with product managers, engineers, data scientists, and analysts to fulfill property data needs and create outstanding datasets.
Identifying opportunities to enhance and support existing data processes.
Contributing to shared tooling and infrastructure to facilitate self-service solutions and expedite customer onboarding.
Qualifications:
Over 5 years of software development experience using Python, Scala, Java, and demonstrated leadership in designing and implementing scalable, reliable services and workflows/pipelines using tools such as Airflow, Hive, Spark, Kafka, EMR, or equivalents.
A degree in Computer Science or a related technical field or equivalent work experience.
Expertise in establishing and upholding high standards in pipeline monitoring, data validation, testing, etc.
Extensive experience in automating data engineering processes (DataOps).
A genuine passion for data engineering, analytics, and distributed systems.
Strong interpersonal skills and a deep commitment to collaborative work across organizational boundaries.
Comfort in transforming informal customer requirements into well-defined problem definitions, resolving ambiguity, and navigating challenging objectives.
Enthusiasm for mentoring, coaching, onboarding, and leading fellow team members.
We value diverse backgrounds and experiences. If you possess transferable skills or relevant experiences, we encourage you to apply.
Required Education: Bachelor's or Master's degree in Computer Science, Software Engineering, or a related field.
Show more
Show less","Python, Scala, Java, Airflow, Hive, Spark, Kafka, EMR, Computer Science, DataOps, Apache Airflow, Apache Hive, Apache Spark, Apache Kafka, Apache EMR, Machine Learning, Data Analysis, Data Engineering, Data Scientist, Data Validation, ETL, ELT, Data Visualization, Data Warehousing, Big Data, Hadoop, NoSQL, SQL, Cloud Computing, AWS, Azure, Google Cloud Platform","python, scala, java, airflow, hive, spark, kafka, emr, computer science, dataops, apache airflow, apache hive, apache spark, apache kafka, apache emr, machine learning, data analysis, data engineering, data scientist, data validation, etl, elt, data visualization, data warehousing, big data, hadoop, nosql, sql, cloud computing, aws, azure, google cloud platform","airflow, apache airflow, apache emr, apache hive, apache kafka, apache spark, aws, azure, big data, cloud computing, computer science, data engineering, data scientist, data validation, dataanalytics, dataops, datawarehouse, elt, emr, etl, google cloud platform, hadoop, hive, java, kafka, machine learning, nosql, python, scala, spark, sql, visualization"
"Senior Data Engineer  - Sunnyvale, CA - 12+ Months",TMS,"Sunnyvale, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-sunnyvale-ca-12%2B-months-at-tms-3779827126,2023-12-17,California,United States,Mid senior,Onsite,"Position: Senior Data Engineer
Location: Sunnyvale, CA
Duration: 12+ Months
Job Description
Proficiency in managing and manipulating huge datasets in the order of terabytes (TB) is essential.
Expertise in big data technologies like Hadoop, Apache Spark (Scala preferred), Apache Hive, or similar frameworks on the cloud (GCP preferred, AWS, Azure etc.) to build batch data pipelines with strong focus on optimization, SLA adherence and fault tolerance.
Expertise in building idempotent workflows using orchestrators like Automic, Airflow, Luigi etc.
Expertise in writing SQL to analyze, optimize, profile data preferably in BigQuery or SPARK SQL
Strong data modeling skills are necessary for designing a schema that can accommodate the evolution of data sources and facilitate seamless data joins across various datasets
Ability to work directly with stakeholders to understand data requirements and translate that to pipeline development / data solution work.
Strong analytical and problem-solving skills are crucial for identifying and resolving issues that may arise during the data integration and schema evolution process.
Ability to move at rapid pace with quality and start delivering with minimal ramp up time will be crucial to succeed in this initiative.
Effective communication and collaboration skills are necessary for working in a team environment and coordinating efforts between different stakeholders involved in the project.
Nice To Have
Experience building complex near real time (NRT) streaming data pipelines using Apache Kafka, Spark streaming, Kafka Connect with a strong focus on stability, scalability, and SLA adherence.
Good understanding of REST APIs – working knowledge on Apache Druid, Redis, Elastic search, GraphQL or similar technologies. Understanding of API contracts, building telemetry, stress testing etc.
Exposure in developing reports/dashboards using Looker/Tableau
Experience in eCommerce domain.
Tech stack: Google cloud, HDFS, SPARK, Scala, Python (optional), Automic/Airflow, BigQuery, Kafka, API, Druid
Show more
Show less","Data Engineering, Hadoop, Apache Spark, Apache Hive, GCP, AWS, Azure, Automic, Airflow, Luigi, BigQuery, Spark SQL, Schema Design, Data Modeling, Data Integration, Data Analysis, SQL, Apache Kafka, Spark Streaming, Kafka Connect, REST APIs, Druid, Redis, Elastic Search, GraphQL, Telemetry, Stress Testing, API Contracts, Looker, Tableau, eCommerce, Google Cloud, HDFS, Scala, Python","data engineering, hadoop, apache spark, apache hive, gcp, aws, azure, automic, airflow, luigi, bigquery, spark sql, schema design, data modeling, data integration, data analysis, sql, apache kafka, spark streaming, kafka connect, rest apis, druid, redis, elastic search, graphql, telemetry, stress testing, api contracts, looker, tableau, ecommerce, google cloud, hdfs, scala, python","airflow, apache hive, apache kafka, apache spark, api contracts, automic, aws, azure, bigquery, data engineering, data integration, dataanalytics, datamodeling, druid, ecommerce, elastic search, gcp, google cloud, graphql, hadoop, hdfs, kafka connect, looker, luigi, python, redis, rest apis, scala, schema design, spark sql, spark streaming, sql, stress testing, tableau, telemetry"
Sr Data Engineer,Sonitalent Corp,"Burbank, CA",https://www.linkedin.com/jobs/view/sr-data-engineer-at-sonitalent-corp-3704936298,2023-12-17,California,United States,Mid senior,Onsite,"Job title: Sr Data Engineer
Location:
Seattle, Washington or Burbank, California (Must be local to one or the other location)
Interview: Phone/ Skype
End Client
Visa: USC, GC, GC-EAD, H4-EAD
Job Description
Top Skills: Datawarehousing specifically Snowflake, AWS (Step Functions, Cloudwatch, ECS, Lambdas, Glue, etc.), Intermediate Level Python
Plus Skills: Tableau for Visualization, DBT (Data Build Tools), GIT
Visa: USC, GC, GC-EAD, H4-EAD
Interview: Telephone followed by a Zoom Interview
Vendor Call Notes
Sr Data Engineer
Duration: 18-month
m-f 9am 5pm
Preferably Seattle, but open to Burbank, CA. (4 days in office 1 day at home)
Working directly with Disney's Data Platform Droid
AWS-based Data Pipeline overseeing that, making sure can integrate the new data to pull the data into their Snowflake Datawarehouse.
There are a lot of different technologies they would have to work with to make that happen.
Making sure data is reliable, clean and ready to use for analytics.
Tableau experience for Visualization is a huge plus, but not required
Team Structure
Team is led by Rick Wolnick, Lead Data Engineer/Data Architect
Rick is also supported by Travis Knock, one of their Sr Data Engineers; He is working primarily on many of their modeling, mainly financial modeling, and some of their tableau reports.
Flexing Staff of SOW contractors who support them. They are near-shore and more on the senior side as well.
They work with a DBA who does administration side, and a product owner who manages the overall product day-to-day
They all spend about 80% of their time working on their capital project work, projects that will build new features or support the business. And 20% of their time, will be part of an on-call rotation. It's 8-5pm on-call, not overtime.
From a Senior perspective, they will also be expecting the Data Engineer to make recommendations on Systems Design and get data management techniques
This role is open as they had an IQN contractor in this role who took a perm role.
Hard Skills
SQL is the BIG ONE
Datawarehousing, specifically with Snowflake
Data Pipeline Engineering, specifically in AWS ( They use Step Functions, Cloudwatch, ECS, Lambdas, Glue)
Intermediate-level Python what they use to do some light data extract logic
DBT (Data Build Tools) is a PLUS
Tableau for Visualization, would be a huge plus, but not required
GIT would be a plus
Industry experience is not necessary.
Interview
Will have Rick and Travis review resumes first. They will send the ones they like to Danny Bullis (HM). If he likes them will set up an:
Initial 15 30-minute phone screening and will provide 3 time slots for them to pick from
If he likes them, will set them up for a single 1-hour interview. The 1st 30 minutes will be a programming question or two primarily a SQL and/or a Python question; both if possible. The Python question will be more around intermediate data structures and algorithms. The SQL question will be more like some of the intermediate types of joins and/or queries.
2nd 30 minutes will be questions aimed at softskills, and make sure they are going to be someone we're going to want to work with. Rick and Travis will probably alternate which one is going to ask the question, and we're going to be pulling from a list of questions.
Hopefully, select the candidates before October, so they can start the onboarding process beginning of October.
Team Leadership is more of a trait, so if this person can share example of when they had leadership opportunities in their past, that would be very helpful for their selection process. It is not a firm requirement.
Size of Data: Up to 1 terabyte It's operational data Aggregating stuff from ticketing or personnel system
Diversity is very important. So please front-load diverse candidates.
Show more
Show less","Datawarehousing, Snowflake, AWS, Cloudwatch, ECS, Lambdas, Glue, Python, Tableau, DBT, GIT, SQL, Data Pipeline Engineering, Step Functions","datawarehousing, snowflake, aws, cloudwatch, ecs, lambdas, glue, python, tableau, dbt, git, sql, data pipeline engineering, step functions","aws, cloudwatch, data pipeline engineering, datawarehousing, dbt, ecs, git, glue, lambdas, python, snowflake, sql, step functions, tableau"
"Staff Data Engineer, Analytics Platform",Afresh,"San Francisco, CA",https://www.linkedin.com/jobs/view/staff-data-engineer-analytics-platform-at-afresh-3767563518,2023-12-17,California,United States,Mid senior,Remote,"Afresh is on a mission to eliminate food waste and make fresh food accessible to all. Our first A.I.-powered solution optimizes ordering, forecasting, and store operations for fresh food departments in brick-and-mortar grocers. With our Fresh Operating System, regional and national grocery retailers have placed $1.6 billion in produce orders across the US and we've helped our partners prevent 34 million pounds of food from going to waste. Working at Afresh represents a one-of-a-kind opportunity to have massive social impact at scale by leveraging uncommonly impactful software – we hope you'll join us!
About the Role:
The goal of our analytics architecture is to allow teams across Afresh to read and write meaningful, consistent, and reliable metrics derived from our disparate data sources, and to use those metrics to track internal performance, power new reporting products, drive decision-making through experimentation, and alert on significant changes in performance.
In the next 6 months, we are releasing new products that build on top of our analytics architecture, namely a customer facing product for reporting insights that will help our customers drive down food waste. As we venture into building more customer facing products that leverage analytics insight, we’re looking to strengthen our analytics platform foundation.
The Data Science team sits under the larger Prediction, Optimization, and Planning (POP) team at Afresh. You will regularly interact with data engineers, applied scientists, data scientists, full stack engineers, and product managers in the course of your work.
As a staff data engineer on the Data Science team, you will own the ongoing development of our analytics platform. In this role, you will evolve our data warehouse schema, solidify our transform architecture, and establish data governance patterns to serve our internal and external analytics needs. Some of your responsibilities will include:
Improve and extend our data analytics architecture to enable consistent analytic results and easy access across multiple analytics use cases
Collaborate with engineers, product managers, and data scientists to understand their data needs, and then build extensible dimensiional models and semantic layer metrics that allow for consistent and reliable insights
Evolve our existing data quality and data governance processes
Mentor and up-skill other engineers
This is a high-impact role with ownership of highly visible projects and a lot of room to grow in your scope.
Skills and Experience:
6+ years of experience as an data engineer, analytics engineer, data warehouse engineer, or a similar role.
Strong understanding of advanced concepts in SQL.
Exceptional communication and leadership skills, with a proven ability to facilitating cross-team and cross-functional collaboration and information sharing.
1+ years of experience working with SQL-driven transform libraries that support an ELT paradigm, like dbt or sqlmesh, at scale, including setting up CI/CD pipelines that ensure high quality transformations.
Expert knowledge about the differences between OLTP and OLAP database design.
Familiarity with the differences between data engineering concepts like Data Mesh, Data Lake, Data Warehouse, Data Fabric, and Data Lakehouse.
Experience with setting up a semantic layer defined with code (LookML, Cube.dev, AtScale, dbt semantic layer).
Technologies: SQL, Python, Airflow, dbt, Snowflake/Databricks/BigQuery, Spark.
Salary Band:
About Afresh
Founded in 2017, Afresh is working on the #1 solution to curb climate change: reducing food waste. By combining human insight and transformative technology, we're helping grocers provide fresher food to customers at more affordable prices.
Afresh sits at an incredible intersection of positive social impact, rocket ship financial growth, and cutting-edge technology. Our best-in-class AI research has been published in top journals including ICML, and we've raised over $148 million in funding from investors including former co-CEO of Whole Foods Market Walter Robb and Eric Schmidt's Innovation Endeavors.
Fresh is the past, present, and future of our food system – the waste we create today will impact our planet for years to come. Join us as we continue to build a vibrant, diverse, and inclusive team that embodies our company’s values of proactivity, kindness, candor, and humility.
Afresh provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability, genetics, sexual orientation, gender identity/expression, marital status, pregnancy or related condition, or any other basis protected by law.
Here at Afresh, many of our employees work remotely provided that they reside in one of the following states: AR, CA, CO, FL, GA, IL, KY, MA, MI, MT, MO, NV, NJ, NY, NC, OR, PA, TX, WA, WI. However, there may be key roles that will require a candidate/employee to be local to our San Francisco, CA office. In which case this requirement will be included in the job posting details under ""Skills and experience"" for reference.
Show more
Show less","SQL, Data Warehousing, Data Engineering, Data Analytics, Data Governance, Data Quality, OLTP, OLAP, Data Mesh, Data Lake, Data Warehouse, Data Fabric, Data Lakehouse, LookML, Cube.dev, AtScale, dbt Semantic Layer, Python, Airflow, dbt, Snowflake, Databricks, BigQuery, Spark","sql, data warehousing, data engineering, data analytics, data governance, data quality, oltp, olap, data mesh, data lake, data warehouse, data fabric, data lakehouse, lookml, cubedev, atscale, dbt semantic layer, python, airflow, dbt, snowflake, databricks, bigquery, spark","airflow, atscale, bigquery, cubedev, data engineering, data fabric, data governance, data lake, data lakehouse, data mesh, data quality, dataanalytics, databricks, datawarehouse, dbt, dbt semantic layer, lookml, olap, oltp, python, snowflake, spark, sql"
Senior Data Engineer,Second Dinner,"Irvine, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-second-dinner-3767052785,2023-12-17,California,United States,Mid senior,Remote,"Who We Are
Second Dinner is an award-winning independent game development studio that is here to make the most fun games in the world. Not super fun games. Not SUPER DUPER fun games. We mean the MOST fun games. In fact, our game MARVEL SNAP has earned multiple Mobile Game of the Year Awards (Game Awards, DICE), Best Strategy Game (IGN), and the Apple Design Award for Innovation!
Second Dinner is a remote-first studio, so while we are headquartered in Irvine, California, most of our team is fully remote across the United States. We want the most talented teammates wherever they call home. A diverse team with varied perspectives makes us a better company and will help us make better games. If you can bring something new to the table and expand our point of view, that's a huge upside.
Our Data Team
At Second Dinner, data plays a crucial role in conveying the voices of our players and informing our decisions, which leads us to great games and player experiences. Our team enables decision-making with scientific and methodological rigor for Marvel SNAP and our new projects. We partner with teams across the studio to build data-powered player experiences directly into the games. We innovate in analytics tooling and data engineering capabilities to redefine what is possible in game development and operations.
Your Role
You will report to the Data Engineering Lead. You will play a critical role in shaping data-driven insights across our organization. You will develop and operate data pipelines to empower analytics, data science, marketing, product management, and design teams to create incredible player experiences. You will build and operate large-scale cloud data infrastructure. You will collaborate with cross-functional teams to ensure the collection and serving of timely high-quality data. Moreover, you will partner with the AI team to innovate in self-service analytics tools. If you are passionate about building high-impact core capabilities to help craft world-class game experiences and are excited to influence millions of players by leveraging your expertise in data engineering, then APPLY!!!
What You'll Do:
Develop and operate data infrastructure and pipelines to enable robust data for analytics and reporting in Marvel SNAP
Empower the SNAP Marketing team with high-quality data to improve user acquisition (UA)
Enable SNAP Product Management, Analytics, Design, Engineering, and Production teams to gain insights from analytics quickly
Partner with data scientists, analysts, and software engineers to ensure high-quality and relevant data collection
Partner with the AI team to innovate, develop, and operate self-service analytics tooling
What You’ll Need:
Extensive expertise with data infrastructure and data engineering
Demonstrated experience in large-scale distributed data systems (Spark, Flink)
Deep expertise in analytical database technologies (SQL and NoSQL)
Proficiency in SQL and Python
Experience with database technologies and ETL/ELT
Experience with orchestration and automation tools (Airflow, Beam)
Experience with Looker or Tableau
Familiarity with Databricks and data/analytics solutions on AWS
Experience with marketing platforms and data tools (Braze, AppsFlyer)
Demonstrated success in a highly collaborative cross-functional work environment
Passionate player of mobile games
Mindset for serving a diverse and global player base
Nice to Have But Not Necessary:
Experience working in online video games, preferably mobile free-to-play games
Experience in .NET/C# and backend software development
Experience in building and integrating data pipelines for customer-facing live services (e.g., recommenders)
Experience in operational database and storage technologies (e.g., DynamoDB, Cassandra, and Redis)
The total compensation for this position includes a new hire offer base salary range of $130,000 - $210,000 USD + equity + comprehensive benefits + potential for discretionary performance bonuses.
Individual pay within this salary range may span multiple levels within the discipline and is determined by assessed job-related skills, experience, relevant education or training. It also factors in market demands and business needs. The disclosed range is not adjusted based on location and may be subject to change or modification based on business needs in the future. Your recruiter can answer any questions about new hire total compensation during the hiring process.
An overview of the benefits and perks at Second Dinner:
Medical, Dental, and Vision insurance plans with Second Dinner paying 100% of premiums for employees and 75% for dependents for many plans
401(k) contribution with no waiting period
16 weeks paid parental leave with no waiting period
Home office improvement bonus
Paid Vacation & Sick time
Remote-first with core overlap hours between 10AM and 4PM PT
Company Winter Holiday shutdown (Dec 25-Jan 1)
Company Summer Holiday shutdown (week of July 4)
Company Events - In-person Summer all-hands gathering, in-person holiday party, annual camping event, and virtual events throughout the year
We are an equal opportunity employer that places high value on diversity and inclusion. We do not discriminate on the basis of race, color, ancestry, national origin, religion, age, disability status, sex (including pregnancy), gender, gender identity, gender expression, sexual orientation, medical condition, genetic information, marital status, military status, or veteran status.
You must be eligible to work in the United States to be considered for this position.
Show more
Show less","Data Engineering, Cloud Data Infrastructure, Data Pipelines, Spark, Flink, SQL, NoSQL, Python, ETL/ELT, Airflow, Beam, Looker, Tableau, Databricks, AWS, Braze, AppsFlyer, .NET, C#","data engineering, cloud data infrastructure, data pipelines, spark, flink, sql, nosql, python, etlelt, airflow, beam, looker, tableau, databricks, aws, braze, appsflyer, net, c","airflow, appsflyer, aws, beam, braze, c, cloud data infrastructure, data engineering, databricks, datapipeline, etlelt, flink, looker, net, nosql, python, spark, sql, tableau"
Senior Data Engineer,Begin,"Ontario, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-begin-3767891033,2023-12-17,California,United States,Mid senior,Remote,"BEGiN
has an exciting opportunity for a
Senior Data Engineer
to join our growing team! This role will be remote in
Ontario, Canada.
BEGiN is an award-winning educational technology company with world-wide impact. With products that are as effective as they are fun, BEGiN’s family of brands builds critical skills for school and life.
We’re a diverse team of talented people passionate about creating educational content kids love. At BEGiN, we have the rare opportunity to make a dent in the universe by bringing high-quality at-home learning to kids globally!
Reporting into our
Director, Data Engineering
, the
Senior Data Engineer
will implement reliable data pipelines upholding the best practices that are pivotal to analytics, data science, and reporting across the organization as well as develop the data platform capabilities to ensure data platform performance, scalability and maintainability.
You will:
Own efficacy and quality of data pipelines and ETL processes that bring data into the enterprise data warehouse.
Develop, maintain, and improve tools to enable team members to rapidly consume and understand data.
Design and architect scalable infrastructure to build, train, and deploy machine learning models, ETL, and CI/CD with an eye on efficiency.
Be hands-on with multiple cloud technologies, tools and programming languages (Python, PySpark, SQL, AWS, GCP, Databricks, etc.)
Responsibilities:
Work closely with the other data engineers,, data scientists/analysts, and our product engineering team to translate requirements into deliverable data pipelines.
Execute the strategy for the data platform to support the business while optimizing performance and minimizing cost.
Partner with stakeholders and engineering teams to deliver solutions in an iterative and incremental manner, leveraging lean and agile principles, fostering an environment of learning and collaboration.
Ensure that our applications and operational data remain in sync and all integrations are flowing with no data errors.
Lead root cause analysis, prioritize and manage data quality and remediation, and ensure data integrity to all downstream data systems.
You will be an expert on understanding how data is collected, maintained, and interpreted and be knowledgeable on the official sources of data in scope to address use case requirements and business needs.
Must Haves:
Bachelor degree in Computer Science or related field.
Deep understanding of Spark (Databricks) and expertise on Data Warehousing approaches in the Databricks Lakehouse.
Expert in Python/PySpark/Spark SQL and follow/evolve established SDLC, coding best practices, version control etc.
Data Platform Architecture experience in AWS and/or GCP.
Previous hands-on experience with data modeling.
5+ years of experience as a data engineer.
Excellent communication skills tailored for target audience.
Experience in BI tools (i.e. Looker)
Nice-to-Haves:
Graduate degree in Computer Science or related field.
Understanding of Analytics use cases (i.e. customer360, marketing channel optimization etc).
Prior experience with AWS Infrastructure (Networking, VPCs etc).
Prior experience with tools such as Fivetran, Airflow, Metarouter, Terraform etc
We like people who:
Are open to suggestions, collaborative, and thrive in team environments.
Love and are willing to learn new technologies and styles.
Are scrappy, entrepreneurial with the ability to turnaround high-quality projects quickly without depending on a large team.
What you’ll get:
BEGiN offers competitive compensation including equity and full benefits.
Smart, passionate, and engaged co-workers.
Excellent top-tier Medical/Dental/Vision benefits
The chance to have a big impact, quickly
The rare opportunity to make a dent in the universe. We’re bringing a love of reading and learning to children globally!
BEGiN is a proud equal opportunity employer. All qualified applicants will be considered without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status.
At BEGiN, we are committed to building a diverse team of talented people who are passionate about creating educational content kids love. We believe in fostering a culture where productivity can flourish, one that is empathetic, respectful, and inclusive. At BEGiN, we know that diversity, equity, and inclusion aren’t just an idea, a one-time initiative, or phrases to throw into a job post: they’re a daily practice and an ongoing conversation. We survey our team about inclusivity, run training on DEI topics, and have a committee to ensure we are all continuing to learn and grow.
Show more
Show less","Python, PySpark, Spark SQL, SQL, AWS, GCP, Databricks, Data Warehousing, Databricks Lakehouse, SDLC, Data Platform Architecture, Data Modeling, Looker, Fivetran, Airflow, Metarouter, Terraform, Big Data, Data Science, Cloud Computing, Machine Learning, Data Pipelines, ETL, BI Tools","python, pyspark, spark sql, sql, aws, gcp, databricks, data warehousing, databricks lakehouse, sdlc, data platform architecture, data modeling, looker, fivetran, airflow, metarouter, terraform, big data, data science, cloud computing, machine learning, data pipelines, etl, bi tools","airflow, aws, bi tools, big data, cloud computing, data platform architecture, data science, databricks, databricks lakehouse, datamodeling, datapipeline, datawarehouse, etl, fivetran, gcp, looker, machine learning, metarouter, python, sdlc, spark, spark sql, sql, terraform"
Senior/Staff Data Engineer,EvenUp,"San Francisco, CA",https://www.linkedin.com/jobs/view/senior-staff-data-engineer-at-evenup-3728158599,2023-12-17,California,United States,Mid senior,Remote,"EvenUp is a venture-backed generative AI startup that ensures injury victims are awarded the full value of their claims, expanding the $100B+ in awards granted to injury victims every year.
Every year, the legal system has made it difficult for millions of ordinary people to seek justice, especially for folks without means or who come from underrepresented backgrounds
. Our vision is to help these injury victims get the justice they deserve, irrespective of their income, demographics, or the quality of their legal representation.
EvenUp operates across all types of injury cases, from police brutality and child abuse to California wildfires and motor vehicle accidents. Our ML-driven software empowers attorneys to accurately assess the value of these cases by doing a core part of their workflow (legal drafting), enabling them to secure larger settlements in record time. As EvenUp evaluates more cases, our proprietary data grows, enhancing the precision of our predictions and delivering more value to both attorneys and victims alike.
As one of the fastest growing startups ($0 to $10M in ARR in <2 years), we raised $65M in investment from some of the best investors in the world (Bessemer, Bain Capital, Signalfire, DCM, NFX, Tribe Capital), seasoned tech executives (i.e. founder of Quora, SVP at Google, former CPO at Uber), and public figures that care about our social mission (Nas, Jared Leto, Byron Jones). Our team comes from top tech, legal, and investing backgrounds including Waymo, Google, Amazon, Uber, Quora, Blizzard, Norton Rose, Warburg Pincus, Bain, and McKinsey.
Why we are hiring a Senior/Staff Data Engineer now?
We have experienced unprecedented growth and need to scale out our data warehousing, data tooling and internal analytics. We need to architect the future of our data infrastructure at EvenUp and we’re seeking engineering leaders to help drive that vision.
We will need to 10x our pipeline processing throughput over the next 12 months. We’ll need to rethink and rebuild how we extract, process and model our ingestion to enable our organization with precise and actionable data.
We need to design & build data warehousing that democratizes data for our entire organization. We will invest in identifying and integrating tools and services that empower our teams to build on top of our data and analytics.
What you’ll do:
Democratize data at EvenUp. Ensure our organization can scale with consistent, standardized access to our data stores and accelerate our ability to build and experiment with data products
Architect and build out the future of data warehousing at EvenUp
Enable and empower our Data Science team to rapidly iterate on model experimentation
Design, organize and refine data storage strategies that reduce development friction for our tech organization
Collaborate with cross functional teams to solve critical data problems
Help grow our nascent Data Insights team and define a “data first” mentality across our organization
What we are seeking:
8+ years of data engineering experience
Previous experience building out data warehousing, data pipelines, and internal analytics
Strong understanding and practical experience with data tooling, BI tools, and systems such as DBT, BigQuery, Elasticsearch
The ability to communicate cross-functionally with various stakeholders to derive requirements and architect scalable solutions
Have several years of industry experience building high-quality software, shipping production-ready code and infrastructure
You enjoy owning a project from start to finish and love to drive a project across the finish line
Interest in making the world a fairer place (we don’t get paid unless we’re helping injured victims and/or their attorneys)
Nice to haves:
Have previously built out a Data Insights team at a data-oriented startup
Have previously planned and architected data migrations at scale
Have stood up analytics tooling to enable cross-functional teams
Domain expertise in legal technology, medical records, and working with unstructured data
A successful first year may look like:
75% doing system design and contributing code, starting with shipping code within 2 weeks!
25% collaborating with stakeholders and mentoring, lunch and learns, and more
Leverage a self-starter mindset by taking a product concept and building the feature end to end (whether it’s a component of the system or a significant piece of functionality).
Collaborate with the team to scale the tech stack based on our rapidly growing user base!
Benefits & Perks:
We seek to empower all of our team members to fulfill our mission of making the world a more just place, regardless of our team’s function, geography, or experience level. To that end, we offer:
Fully remote setup - work from wherever you feel is best (Plus a stipend to upgrade your home office!)
Flexible working hours to match your style
Offsites - get to meet your coworkers on a fully-expensed trip every 6-12 months!
Choice of great medical, dental, and vision insurance plan options
Flexible paid time off
A variety of virtual team events such as game nights & happy hours
EvenUp is an equal-opportunity employer. We are committed to diversity and inclusion in our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.
Show more
Show less","Data warehousing, Data pipelines, Internal analytics, Data tooling, BI tools, Systems, DBT, BigQuery, Elasticsearch, Data Insights, Data storage, Development friction, Data migration, Analytics tooling, Legal technology, Medical records, Unstructured data, System design, Code, Stakeholder collaboration, Mentoring, Lunch and learns, Selfstarter mindset, Product concept, Feature building, User base scaling","data warehousing, data pipelines, internal analytics, data tooling, bi tools, systems, dbt, bigquery, elasticsearch, data insights, data storage, development friction, data migration, analytics tooling, legal technology, medical records, unstructured data, system design, code, stakeholder collaboration, mentoring, lunch and learns, selfstarter mindset, product concept, feature building, user base scaling","analytics tooling, bi tools, bigquery, code, data insights, data migration, data storage, data tooling, datapipeline, datawarehouse, dbt, development friction, elasticsearch, feature building, internal analytics, legal technology, lunch and learns, medical records, mentoring, product concept, selfstarter mindset, stakeholder collaboration, system design, systems, unstructured data, user base scaling"
Senior Data Engineer,hatch I.T.,"San Francisco, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-hatch-i-t-3765524716,2023-12-17,California,United States,Mid senior,Remote,"Hatch I.T. is partnering with an
AI startup
to find a
Senior Data Engineer.
See details below:
About The Role
As a
Data Engineer,
you will build and maintain data pipelines, and help us scale our data systems and run client reports. You'll manage core database systems, evaluate A/B tests, establish industry benchmarks, and uncover trends from shoppers around the world. You'll need to bring new ideas and see the big picture, communicate with the team, and ensure projects are delivered on time. Your work will have a real impact on the industry and be seen by leaders at some of the top apparel brands and retailers in the world. You'll also develop and maintain metrics, reports, analyses, dashboards, and more, and translate data-related questions into persuasive analyses for business partners. You'll create compelling narratives and refine processes for data collection and analysis, among other responsibilities.
About You
You are a humble, passionate, and experienced data engineer that is excited to write code but also comfortable working with sales, accounts, and other engineers. You have a positive outlook and are interested in working in a highly collaborative work environment. If you have a natural curiosity for data and don't mind rolling up your sleeves and digging through messy data to find interesting insights, this might be your jam.
About The Company
The company is an AI-based platform that's transforming the apparel industry. Their platform helps retailers and brands reduce returns, improve sustainability, and optimize their supply chain. With machine learning and AI at their core, they're able to predict body measurements with incredible accuracy. This enables brands and retailers to gain a better understanding of their customers' buying habits, the fit and sizing of their products, and make optimizations in a completely new way. Their focus on data-driven insights is helping some of the world's most well-known brands and retailers leverage AI to make an impact across their entire organization.
How You Will Make An Impact
You’ll directly impact client relations with full understanding of how we’re able to impact their business
You’ll work alongside our customer success team to help understand our clients’ needs and help the answer and explore their data
You’ll drive data analysis and help create analysis the entire apparel industry will adopt
Skills and Qualifications: (even if you do not 100% meet these skills, they still welcome you to apply)
2+ years experience as a Data Engineer in a professional setting
Comfortable with SQL & Python
Able to identify trends and patterns in data
You can clearly communicate acceptance criteria and work well with a data team
You love understanding the nuances of data and are excited to take command of client analysis and data
Comfortable using a VPN, a database GUI and Jupyter
Experience working on an eCommerce and/or retail site a plus
B.A. or B.S. degree required
$110,000 - $160,000 a year
Don't think you're 100% qualified for this position? Studies have shown that women and people of color are less likely to apply to jobs unless they meet every single qualification. At hatch I.T., we're dedicated to helping companies build diverse, inclusive and authentic workplaces, so if your experience doesn't perfectly align with every qualification in the job description, we encourage you to apply anyway. You may just be the right candidate for this or other roles.
If you are interested in learning more about this company or any Startups/Small Businesses in the area, please contact us and check us out
here
!!
We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.
Show more
Show less","SQL, Python, A/B testing, Analysis, Compliance, Data engineering, Data collection, Data visualization, Dashboards, eCommerce, Jupyter, Machine learning, Metrics, Pipelines, Retail, VPN","sql, python, ab testing, analysis, compliance, data engineering, data collection, data visualization, dashboards, ecommerce, jupyter, machine learning, metrics, pipelines, retail, vpn","ab testing, analysis, compliance, dashboard, data collection, data engineering, ecommerce, jupyter, machine learning, metrics, pipelines, python, retail, sql, visualization, vpn"
Epic Data Engineer,Prominence Advisors,"San Francisco, CA",https://www.linkedin.com/jobs/view/epic-data-engineer-at-prominence-advisors-3676184717,2023-12-17,California,United States,Mid senior,Remote,"Prominence is looking for a Data Engineer to assist with dimensional data modeling development for healthcare data architecture project.
Who We Are
Prominence is a healthcare technology strategy and implementation firm, focused on helping the nation’s leading healthcare organizations to do more with their data. Founded by former Epic managers, we understand the technology landscape in healthcare and provide IT staffing, advisory services, and analytics solutions to create robust data ecosystems that support clinical workflows, automate operational processes, and expedite research. Whether it’s guiding a technology implementation, establishing governance principles, or developing leading edge analytics, we help our customers make sense out of the mountain of data at their fingertips in order to deliver higher quality care at a lower cost.
Ranked as a best place to work over 27 times (and counting!), Prominence’s culture provides consultants with a supportive environment that allows you to innovate and grow your career in healthcare IT. Additional information is available on our website.
Your Role
Our consultants guide our customers through complex technology requirements to summit the challenge at hand. You will need to be able to create order out of chaos, and compile ambiguous information into tactical action plans.
Our ideal team members are humble, smart, and driven to ensure our customer’s success. This includes a passion to deliver high-quality results, while teaching our counterparts how to fish and grow the skills needed to support and expand upon the deliverables of our projects.
If this sounds like you, and you meet the requirements below, we encourage you to apply. If you know of someone else how would be a great fit, let us know!
Requirements
As a member of our Epic Consulting team, you’ll work closely with our customers to implement and optimize their Epic workflows. In addition to your Epic project work, you will help mentor and grow our customer’s teams, escalate issues, and guide projects to a successful outcome.
Key Responsibilities
Perform Epic-related consulting and advisory services, including but not limited to the following:
Apply technical expertise to implement and optimize EMR workflows and data capture
Mentor customers to up-level their system knowledge and analyst skills
Analyze operational and business requirements, and translate into system configuration
Create build documentation and workflow diagrams
Track and resolve project risks and issues
Lead meetings and participate in ongoing work-product coordination.
Transparently report on project status and deliverables.
Develop robust knowledge transfer documentation to hand-off deliverables to customer teams.
Additional duties as may be required to successfully deliver a project
May be invited to participate in corporate functions, events, and meetings
Desired Qualifications
Active Certification(s): Cogito Data Model (Clinical or Revenue Cycle), Cogito Tools, Caboodle Developer
5+ years of experience as an Epic BI Developer or Data Engineer
Dimensional data modeling experience
Caboodle development experience preferred
SSIS, Azure Data Factory, or Data Lake development preferred
Demonstrated ability to deliver successful projects remotely
Success Criteria
Successful team members at Prominence display the following:
High degree of professionalism; treats others with respect, keeps commitments, builds trust within a team, works with integrity, and upholds organizational values.
Highly organized; able to manage multi-faceted work streams
Self-motivated; able to maintain schedule, meet deadlines, and monitor your personal work product
Highly adaptable; able to acclimate quickly to new project assignments and work environments.
Creative; not paralyzed by problems and able to work collaboratively to find novel solutions
Clear communication skills; ability to clearly convey messaging that resonates with your audience, in clear and concise written and verbal communications
Can smell smoke and anticipate issues before they arise, ability to escalate effectively
Passion to mentor and guide others
Benefits
Prominence is dedicated to hiring the best and brightest minds in healthcare and maintaining a culture that rewards our employees for following their passion. We are excited to offer the following benefits for this position:
Competitive Salaried and Hybrid Compensation Plans
Health Care Plan (Medical, HSAs, Dental & Vision)
Retirement Plan (401k)
Life Insurance (Basic, Voluntary & AD&D)
Dependent & Health Savings Accounts
Short Term & Long Term Disability
Paid Time Off (Vacation/Sick & Public Holidays)
Training & Development Fund
Technology Stipends (for Qualifying Roles)
Work From Home
Charitable Giving to Causes You Believe In
Employment Eligibility
Must be legally authorized to work in the United States without sponsorship.
Commitment to Equal Opportunity
The world’s most talented professionals come from every background. All applicants will be considered for employment without attention to age, race, color, religion, gender identity and/or expression, sexual orientation, national origin, marital status, veteran or disability status, or any other characteristic protected by law. In addition, Prominence will provide reasonable accommodations for qualified individuals with disabilities.
If you are smart and good at what you do, come as you are. All qualified candidates are encouraged to apply.
Partnership Eligibility
Our partnerships are extremely important to us. This online application is not intended for anyone who is currently under a non-compete agreement or has an arrangement that precludes employment at Prominence. We appreciate your help in respecting our partners.
Interested in learning more? Apply below to connect with our Talent team about immediate openings and future consulting projects.
Show more
Show less","Data Engineering, Dimensional Data Modeling, Healthcare Data Architecture, Cogito Data Model, Cogito Tools, Caboodle Developer, Epic BI Developer, SSIS, Azure Data Factory, Data Lake, Caboodle","data engineering, dimensional data modeling, healthcare data architecture, cogito data model, cogito tools, caboodle developer, epic bi developer, ssis, azure data factory, data lake, caboodle","azure data factory, caboodle, caboodle developer, cogito data model, cogito tools, data engineering, data lake, dimensional data modeling, epic bi developer, healthcare data architecture, ssis"
Data Engineer (Contract),Aputure,"Glendale, CA",https://www.linkedin.com/jobs/view/data-engineer-contract-at-aputure-3575816598,2023-12-17,California,United States,Mid senior,Remote,"Aputure is one of the fastest-growing cinema technology companies. Our equipment has emerged a global newcomer to watch for with hundreds of thousands of Aputure lights now being used on film sets worldwide. Beyond just products, our marketing team also works in a way that is equally visionary. Electing to create communities and content for filmmakers rather than advertisements, the Aputure A-Team is composed of like-minded creatives that genuinely enjoy working with filmmakers every day.
The Data Engineer will be instrumental in delivering and maintaining the data pipelines and infrastructure, as well as building dashboards needed for business metrics as per Aputure's needs. As one of the early hires of this team, you will get to shape the project’s direction and success.
The Role:
Contribute to the data collection/data cleaning process
Validate the metrics set out by Aputure’s executive team
Architect, build, monitor and maintain data ETL pipelines that support Aputure’s business domains
Implement the data dashboards needed for business metrics delivery and interpretation.
Integrate seamlessly with the existing Aputure’s data workflow and legacy systems.
Implement deployment workflows to take prototype data systems to production.
Required Qualifications:
At least Bachelor’s in Computer Science or relevant fields
Strong knowledge of data structures, ETL pipelines and data processing/cleaning
Strong experience with data visualization libraries such as Tableau and Plotly
2+ years of experience building production grade data platforms using AWS or GCP, and be willing to ramp up quickly on Aliyun
Advanced knowledge of SQL and NoSQL database technologies (SQL, Postgres, Cassandra, Mongo etc.)
Proficiency in Python, Spark, Kafka, RabbitMQ and related data engineering tools
Proficiency in working with/implementing REST and GraphQL APIs
Working knowledge of Git, Docker, and associated deployment tools
Aware of CI/CD best practices
Aware of modern data offerings such as Databricks and Snowflake
Experience in participating in on-call rotations and dealing with hot fixes
Specifications:
6 months contract with potential for renewal or conversion to full time
40 hours/week working hours
Remote eligible (Los Angeles presence strongly preferred for some in-person sync ups)
Work authorization: US citizen or permanent resident
Direct hire only - no agencies
Our mission is to make filmmaking better, more creative and more accessible for all. Ensuring a diverse and inclusive workplace where we learn from each other is core to Aputure’s values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer and strive to be the most supportive place to work.
Salary: $50-$70/hour depending on experience
Show more
Show less","Data Engineering, Data Pipelines, Data Dashboards, Data Visualization, ETL Pipelines, Data Processing, Data Cleaning, Tableau, Plotly, AWS, GCP, Aliyun, SQL, Postgres, Cassandra, MongoDB, NoSQL, Python, Spark, Kafka, RabbitMQ, REST APIs, GraphQL APIs, Git, Docker, Deployment Tools, CI/CD, Databricks, Snowflake, OnCall Rotations, Hot Fixes","data engineering, data pipelines, data dashboards, data visualization, etl pipelines, data processing, data cleaning, tableau, plotly, aws, gcp, aliyun, sql, postgres, cassandra, mongodb, nosql, python, spark, kafka, rabbitmq, rest apis, graphql apis, git, docker, deployment tools, cicd, databricks, snowflake, oncall rotations, hot fixes","aliyun, aws, cassandra, cicd, data cleaning, data dashboards, data engineering, data processing, databricks, datapipeline, deployment tools, docker, etl pipelines, gcp, git, graphql apis, hot fixes, kafka, mongodb, nosql, oncall rotations, plotly, postgres, python, rabbitmq, rest apis, snowflake, spark, sql, tableau, visualization"
Expression of Interest: Data Engineer,Fingerprint for Success (F4S),"San Diego, CA",https://www.linkedin.com/jobs/view/expression-of-interest-data-engineer-at-fingerprint-for-success-f4s-3787773675,2023-12-17,California,United States,Mid senior,Remote,"We are inviting professionals in high-growth industries who are thinking about their next move or looking for a new opportunity to join our expanding talent pool.
The F4S Talent Pool is a pilot project designed to:
Help job seekers get discovered by our partners based on their anticipated hiring needs.
Provide optional support and resources for job seekers in their career endeavors.
Help individuals understand, and bring out the best in themselves and each other.
The F4S Talent Pool process:
Once you express your interest, you will be asked to complete the F4S work style assessment which measures 48 key attitudes and motivations in the context of work. On completion, you will be automatically added to our growing talent pool and contacted as new opportunities arise.
About Fingerprint For Success (F4S)
Backed by 20+ years of research, F4S’s revolutionary predictive analytics have achieved over 90% reliability in forecasting personal and team motivations, behaviors, and performance. Ultimately, we help people find purpose and fulfillment at work, and help build and scale high performing teams.
Keep in mind, joining our talent pool does not guarantee a job offer. We aim to balance your technical skills with the results of your F4S work style assessment to match the hiring needs of our partners.
Your feedback is a gift! Write to us via:
Powered by JazzHR
zGyQ3I4owM
Show more
Show less","Work style assessment, Predictive analytics","work style assessment, predictive analytics","predictive analytics, work style assessment"
Senior Data Engineer / Airflow / Python / Spark,Motion Recruitment Partners LLC,"Los Angeles, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-airflow-python-spark-at-motion-recruitment-partners-llc-3781706322,2023-12-17,California,United States,Mid senior,Remote,"Dice is the leading career destination for tech experts at every stage of their careers. Our client, Motion Recruitment Partners, LLC, is seeking the following. Apply via Dice today!
A software company that handles petabytes of data is hiring a Senior Data Engineer to join their growing team of 5. This company is in a growth phase so there is opportunity to grow from within as they expand. In this role you develop, optimize and maintain their ETL data pipelines that handle billions of records and has millions of monthly users. This team utilizes Python, AWS, S3, Airflow, EMR, DynamoDB, SQL, and prefer PySpark but can train someone into PySpark if you only have Spark. In this role you will help build and improve automation platform features and data pipeline orchestration tools. This is a Los Angeles based company offering a fully Remote position for SoCal residents.
Required Skills & Experience
5+ years professional Data Engineering Experience
5years of experience building ETL pipelines with Python
Experience with AWS, S3, Redshift, EMR, and Airflow
Proficient with Spark or PySpark
Experience working with large amounts of data
Desired Skills & Experience
Bachelors in STEM field
Excellent written and verbal communication skills
The Offer
You Will Receive The Following Benefits
Medical Insurance
Dental Benefits
Vision Benefits
Paid Sick Time
Paid Time Off
401(k) with match
Remote
Applicants must be currently authorized to work in the US on a full-time basis now and in the future. Senior Data Engineer / Airflow / Python / Spark
Show more
Show less","Python, PySpark, AWS, S3, Airflow, Redshift, EMR, DynamoDB, Spark, SQL, ETL, Data Engineering, Communication","python, pyspark, aws, s3, airflow, redshift, emr, dynamodb, spark, sql, etl, data engineering, communication","airflow, aws, communication, data engineering, dynamodb, emr, etl, python, redshift, s3, spark, sql"
Senior Data Engineer,Atlassian,"Mountain View, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-atlassian-3737405985,2023-12-17,California,United States,Mid senior,Remote,"Overview
Working at Atlassian
Atlassians can choose where they work – whether in an office, from home, or a combination of the two. That way, Atlassians have more control over supporting their family, personal goals, and other priorities. We can hire people in any country where we have a legal entity. Interviews and onboarding are conducted virtually, a part of being a distributed-first company.
Atlassian is looking for a Senior Data Engineer to join our Data Engineering Team and build world-class data solutions and applications that power crucial business decisions throughout the organization. We are looking for an open-minded, structured thinker who is passionate about building systems at scale. You will enable a world-class engineering practice, drive the approach with which we use data, develop backend systems and data models to serve the needs of insights, and play an active role in building Atlassian’s data-driven culture. You love thinking about the ways the business can consume data and then figuring out how to build it.
Responsibilities
Your future team
Our Data Engineering Team is comprised of data experts. We build world-class data solutions and applications that power crucial business decisions throughout the organization. We manage multiple analytical data models and pipelines across Atlassian, covering finance, growth, product analysis, customer analysis, sales and marketing, and so on. We maintain Atlassian's data lake that provide a unified way of analyzing our customers, our products, our operations, and the interactions among them.
We're hiring a Senior Data Engineer, reporting to the Data Engineering Manager who’s based in Sydney. Here, you'll enable a world-class engineering practice, drive the approach with which we use data, develop backend systems and data models to serve the needs of insights and help build Atlassian's data-driven culture. You love thinking about the ways the business can consume data and then figuring out how to build it.
Qualifications
What you'll do
You'll partner with the product analytics and data scientist team to build the data solutions that allow them to obtain more insights from our data and use that to support important business decisions.
You'll work with different stakeholders to understand their needs and architect/build the data models, data acquisition/ingestion processes and data applications to address those requirements.
You'll add new sources, code business rules, and produce new metrics that support the product analysts and data scientists.
You'll be the data domain expert who understand all the nitty-gritty of our products.
You'll own a problem end-to-end. Requirements could be vague, and iterations will be rapid
You'll improve data quality by using & improving internal tools/frameworks to automatically detect DQ issues.
Your background
A BS in Computer Science or equivalent experience with 5+ years of professional experience as a Sr. Data Engineer or in a similar role.
Strong programming skills using Python
Working knowledge of relational databases and query authoring (SQL).
Experience designing data models for optimal storage and retrieval to meet product and business requirements.
Experience building scalable data pipelines using Spark (SparkSQL) with Airflow scheduler/executor framework or similar scheduling tools.
Experience working with AWS data services or similar Apache projects (Spark, Flink, Hive, and Kafka).
Understanding of Data Engineering tools/frameworks and standards to improve the productivity and quality of output for Data Engineers across the team.
Well versed in modern software development practices (Agile, TDD, CICD)
Compensation
At Atlassian, we strive to design equitable, explainable, and competitive compensation programs. To support this goal, the baseline of our range is higher than that of the typical market range, but in turn we expect to hire most candidates near this baseline. Base pay within the range is ultimately determined by a candidate's skills, expertise, or experience. In the United States, we have three geographic pay zones. For this role, our current base pay ranges for new hires in each zone are:
Zone A: $163,300 - $217,700
Zone B: $147,500 - $196,600
Zone C: $135,600 - $180,700
This role may also be eligible for benefits, bonuses, commissions, and equity.
Please visit go.atlassian.com/payzones for more information on which locations are included in each of our geographic pay zones. However, please confirm the zone for your specific location with your recruiter.
Our Perks & Benefits
Atlassian offers a variety of perks and benefits to support you, your family and to help you engage with your local community. Our offerings include health coverage, paid volunteer days, wellness resources, and so much more. Visit
go.atlassian.com/perksandbenefits
to learn more.
About Atlassian
At Atlassian, we're motivated by a common goal: to unleash the potential of every team. Our software products help teams all over the planet and our solutions are designed for all types of work. Team collaboration through our tools makes what may be impossible alone, possible together.
We believe that the unique contributions of all Atlassians create our success. To ensure that our products and culture continue to incorporate everyone's perspectives and experience, we never discriminate based on race, religion, national origin, gender identity or expression, sexual orientation, age, or marital, veteran, or disability status. All your information will be kept confidential according to EEO guidelines.
To provide you the best experience, we can support with accommodations or adjustments at any stage of the recruitment process. Simply inform our Recruitment team during your conversation with them.
To learn more about our culture and hiring process, visit
go.atlassian.com/crh
.
Show more
Show less","Python, SQL, Spark (SparkSQL), Apache (Spark Flink Hive and Kafka), Airflow scheduler/executor framework, Agile, TDD, CICD","python, sql, spark sparksql, apache spark flink hive and kafka, airflow schedulerexecutor framework, agile, tdd, cicd","agile, airflow schedulerexecutor framework, apache spark flink hive and kafka, cicd, python, spark sparksql, sql, tdd"
Senior Data Engineer,ARC IT Recruitment,"California, United States",https://www.linkedin.com/jobs/view/senior-data-engineer-at-arc-it-recruitment-3775977747,2023-12-17,California,United States,Mid senior,Remote,"Senior Data Engineer
REMOTE (Must be based in the US)
Are you passionate about shaping the future of technology through innovative data solutions? Do you thrive in a dynamic environment, working with cutting-edge technologies?
Key Responsibilities:
We are working with a leading California-based tech company dedicated to pushing the boundaries of what's possible in the digital landscape.
Design, implement, and optimize scalable ETL processes using Python and other relevant technologies.
Develop and maintain complex data pipelines using Apache Airflow to orchestrate data workflows efficiently.
Collaborate with cross-functional teams to understand data requirements and implement solutions that meet business needs.
Ensure the reliability, scalability, and performance of data infrastructure.
Create and maintain Data Access Objects (DAOs) and Data Access Layers (DALs) for seamless data access and manipulation.
Implement and optimize Data Warehouse solutions for efficient storage and retrieval of structured and unstructured data.
Work with Data Analysts, Data Scientists, and other stakeholders to understand data insights and implement improvements.
Qualifications:
Bachelor’s degree in computer science, Information Systems, or a related field.
4+ years’ experience with Airflow
Proven experience as a Data Engineer with a focus on ETL processes and data infrastructure.
Strong proficiency in Python and experience with Apache Airflow for DAG (Directed Acyclic Graph) creation and management.
Solid understanding of data modeling, database design, and data warehousing concepts.
Experience working with cloud-based technologies (AWS, GCP, or Azure).
Familiarity with big data technologies such as Hadoop, Spark, or Flink is a plus.
Excellent problem-solving and communication skills.
Perks and Benefits:
Salary is up to $167k/annum
Bonus and equity
Remote work (must be based eligible to work in the US)
100% medical/dental/vision coverage for all employees.
Opportunities for professional development and growth.
Collaborative and inclusive work environment.
Sound like you? Please get your CV over to us ASAP
Show more
Show less","Python, Apache Airflow, ETL, Data warehousing, Data pipelines, Data infrastructure, Data access objects, Proficiency in Python, Apache Airflow for Directed Acyclic Graph (DAG) creation and management, PostgreSQL, MS SQL, Snowflake, Hive, Spark, Hadoop, Flink, AWS, GCP, Azure","python, apache airflow, etl, data warehousing, data pipelines, data infrastructure, data access objects, proficiency in python, apache airflow for directed acyclic graph dag creation and management, postgresql, ms sql, snowflake, hive, spark, hadoop, flink, aws, gcp, azure","apache airflow, apache airflow for directed acyclic graph dag creation and management, aws, azure, data access objects, data infrastructure, datapipeline, datawarehouse, etl, flink, gcp, hadoop, hive, ms sql, postgresql, proficiency in python, python, snowflake, spark"
Senior Data Engineer,Atlassian,"Mountain View, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-atlassian-3737407633,2023-12-17,California,United States,Mid senior,Remote,"Overview
Working at Atlassian
Atlassians can choose where they work – whether in an office, from home, or a combination of the two. That way, Atlassians have more control over supporting their family, personal goals, and other priorities. We can hire people in any country where we have a legal entity. Interviews and onboarding are conducted virtually, a part of being a distributed-first company.
Atlassian is looking for a Senior Data Engineer to join our Data Engineering Team. You will build top-notch data solutions and applications that inspire important decisions across the organization. You will be reporting to the Senior Data Engineering Manager.
You'll have flexibility in where you work – whether in an office, from home (remote), or a combination of the two.
Responsibilities
A typical day may involve collaborating with partners, you will design data models, acquisition processes, and applications to address needs. With experience in large-scale data processing systems (batch and streaming), you will lead business growth and enhance product experiences. And will collaborate with Technology Teams, Global Analytical Teams, and Data Scientists across programs.
You'll take ownership of problems from end-to-end: extracting/cleaning data, and understanding generating systems. Improving the quality of data by adding sources, coding rules, and producing metrics is crucial as requirements evolve. Agility and smart risk-taking are important qualities in this industry where digital innovation meets partner/customer needs over time.
Qualifications
On your first day, we'll expect you to have:
BS in Computer Science or equivalent experience with 5+ years as Data Engineer or similar role
Programming skills in Python & Java (good to have)
Design data models for storage and retrieval to meet product and requirements
Build scalable data pipelines using Spark, Airflow, AWS data services (Redshift, Athena, EMR), Apache projects (Spark, Flink, Hive, and Kafka)
Familiar with modern software development practices (Agile, TDD, CICD) applied to data engineering
Enhance data quality through internal tools/frameworks detecting DQ issues. Working knowledge of relational databases and SQL query authoring
We’d Be Super Excited If You Have
Followed a Kappa architecture with any of your previous deployments and domain knowledge of Financial and People System
Compensation
At Atlassian, we strive to design equitable, explainable, and competitive compensation programs. To support this goal, the baseline of our range is higher than that of the typical market range, but in turn we expect to hire most candidates near this baseline. Base pay within the range is ultimately determined by a candidate's skills, expertise, or experience. In the United States, we have three geographic pay zones. For this role, our current base pay ranges for new hires in each zone are:
Zone A: $163,300 - $217,700
Zone B: $147,000 - $196,000
Zone C: $135,600 - $180,700
This role may also be eligible for benefits, bonuses, commissions, and equity.
Please visit go.atlassian.com/payzones for more information on which locations are included in each of our geographic pay zones. However, please confirm the zone for your specific location with your recruiter.
Our Perks & Benefits
Atlassian offers a variety of perks and benefits to support you, your family and to help you engage with your local community. Our offerings include health coverage, paid volunteer days, wellness resources, and so much more. Visit
go.atlassian.com/perksandbenefits
to learn more.
About Atlassian
At Atlassian, we're motivated by a common goal: to unleash the potential of every team. Our software products help teams all over the planet and our solutions are designed for all types of work. Team collaboration through our tools makes what may be impossible alone, possible together.
We believe that the unique contributions of all Atlassians create our success. To ensure that our products and culture continue to incorporate everyone's perspectives and experience, we never discriminate based on race, religion, national origin, gender identity or expression, sexual orientation, age, or marital, veteran, or disability status. All your information will be kept confidential according to EEO guidelines.
To provide you the best experience, we can support with accommodations or adjustments at any stage of the recruitment process. Simply inform our Recruitment team during your conversation with them.
To learn more about our culture and hiring process, visit
go.atlassian.com/crh
.
Show more
Show less","Data Engineering, Python, Java, Spark, Airflow, AWS data services, Apache projects, Kappa architecture, SQL, Agile, TDD, CICD, Data quality","data engineering, python, java, spark, airflow, aws data services, apache projects, kappa architecture, sql, agile, tdd, cicd, data quality","agile, airflow, apache projects, aws data services, cicd, data engineering, data quality, java, kappa architecture, python, spark, sql, tdd"
Sr. Data Engineer,Diligente Technologies,"California, United States",https://www.linkedin.com/jobs/view/sr-data-engineer-at-diligente-technologies-3783652130,2023-12-17,California,United States,Mid senior,Remote,"Title- Sr. Data Engineer
Duration: 12+ Month
Location- Remote (100%)
Skill set Required
Data engineer with 10+ years of experience
Big Data: Scala, Databricks, Spark SQL, Spark Streaming, Python, Pyspark
Cloud: Azure , ADF
System Design: API Design, Event driven architecture.
Microservice: Core Java 8 or above, SpringBoot, Azure Functions
Front end ( Nice to Have ) : JavaScripting, React
Show more
Show less","Data engineering, Scala, Databricks, Spark SQL, Spark Streaming, Python, PySpark, Azure, ADF, API Design, Event driven architecture, Core Java 8, SpringBoot, Azure Functions, JavaScripting, React","data engineering, scala, databricks, spark sql, spark streaming, python, pyspark, azure, adf, api design, event driven architecture, core java 8, springboot, azure functions, javascripting, react","adf, api design, azure, azure functions, core java 8, data engineering, databricks, event driven architecture, javascripting, python, react, scala, spark, spark sql, spark streaming, springboot"
Data Engineer (Remote Option),Balsam Brands,"Redwood City, CA",https://www.linkedin.com/jobs/view/data-engineer-remote-option-at-balsam-brands-3777336191,2023-12-17,California,United States,Mid senior,Remote,"Job Description
As Data Engineer, you will be responsible for designing and developing robust and scalable data warehousing solutions. The Data Engineer will be responsible for building data solutions based on the business requirements. Data solutions may involve retrieval, transformation, storage, and delivery of the data. The Data Engineer must follow standards and implement best practices while writing code and provide production support for the enterprise data warehouse. Our ideal candidate is a skillful data wrangler who enjoys building data solutions from the ground up and optimizing their performance.
This full-time position reports to the Manager of Data Engineering and can work remote from any U.S. state where Balsam Brands is currently setup as an employer, which includes: CA, CO, FL, GA, ID, IL, IN, KS, KY, MD, MA, MO, NJ, NC, OH, OR, PA, TN, TX, VA, and WA. This role can also work locally in our Redwood City, CA or Boise, ID office location. Our local teams work in a hybrid model, which currently includes Tuesday and Wednesday in-office.
To ensure sufficient overlap with functional and cross-functional team members globally, some flexibility with this role's regular work schedule will be required. Most of our teams have overlap with early morning and/or early evening PST. Specific scheduling needs for this role will be discussed in the initial interview.
What You’ll Do
Be accountable for building and maintaining the data infrastructure for the organization
Collaborate with systems analysts and cross functional partners to understand data requirements
Champion data warehouse, create denormalized data foundation layer and normalized data marts
Define strategies to capture all data sources and impact of business process changes on data coming from those sources
Work on all aspects of the data warehouse/BI environment including architecture, design, development, automation, caching and performance tuning
Continually explore new technologies like Big Data, Artificial Intelligence, Generative AI, Machine Learning, and Predictive Data Modeling
What You Bring To The Table
5+ years of professional experience in the data engineering field
Demonstrated history of designing and building schemas, tables, views, and data pipelines
Experience in cloud technologies like Azure, AWS
Experience in Azure Data Factory (ADF) or equivalent ETL tool
Knowledge and experience of working with SQL and relational databases like SQL Server, Oracle, Postgres and MySQL
Ability to understand and tell the story embedded in the data at the core of our business
Ability to communicate with non-technical audience from a variety of business functions
Strong knowledge of coding standards, best practices and data governance
Travel for remote team members:
At Balsam Brands, we believe that time spent together, in-person, collaborating and building relationships is important to who we are. For our newest remote Brandits, we will arrange travel to one of our local offices within your first three months of employment so you can meet and train with your new team in-person. You may also get to travel an additional 1 – 2 times a year for events such as team retreats, offsites, or learning and development opportunities.
Notes:
This is a full-time, permanent position with benefits. Please only apply if you are able to live and work full-time in one of the states listed in this posting. State locations and specifics are subject to change as our hiring requirements shift.
About Us:
Balsam Brands is a global, eCommerce retailer with roots in holiday and home décor. We strive for excellence in everything we do and present a unique opportunity for those seeking to have a meaningful impact in a people-first company that values relationship building, authenticity, and doing the right thing. We have steadily growing teams in Boise, the Bay Area, Dublin, and the Philippines.
The company's mission is to create joy together. We empower our team and partners to love what they do, provide products and experiences that inspire meaningful moments with family and friends, and give back to our families and communities in impactful ways. When you join Balsam Brands, you'll find a culture of caring people doing challenging work and building a welcoming workplace.
Check out our flagship brand, Balsam Hill: www.balsamhill.com
Balsam Brands in Forbes: https://bit.ly/balsambrandsforbes
Balsam Brands on LinkedIn: http://www.linkedin.com/company/balsam-brands/
Glassdoor: https://bit.ly/balsambrands-glassdoor
Benefits
At Balsam Brands, we strive to offer a competitive compensation and benefits package. For permanent, full-time team members, our current package includes:
Competitive compensation, including a cash-based incentive plan; salary is reviewed yearly and may be adjusted as part of the normal compensation review process
Comprehensive Medical, Dental, and Vision coverage, with 100% of monthly premiums covered for team members, and 85%+ employer-paid premiums for other coverage tiers that include dependents
Up to $2,000 annual funding toward HSA accounts
Medical, transit, dependent care FSA
Infertility coverage offered on all medical plans
Generous parental leave program and flexible return options
Company-paid life and AD&D insurance
Company-paid short and long-term disability insurance
401(k) with dollar-for-dollar company match up to $4,000 per calendar year
Employee Assistance Program (EAP) and other mental health and wellness perks
Paid holidays, annual shutdown week, PTO, and volunteer time-off (VTO) packages
Paid 5-week sabbatical leave after 10 years of employment
Annual continuous learning benefit up to $1,000 per person, per fiscal year
Up to $300 flexible reimbursement to support setup of new team member's work-from-home environment
Generous team member merchandise discount
Valuable extras: identity theft protection, subsidized parking, monthly wellness, pet insurance, accident & critical illness insurance
The base pay range for this position is: $129,000 to $162,000. Where an individual falls within that range will vary based on several factors including geographic location and may vary depending on candidate qualifications and experience, applicable skills, and other job-related factors. We benchmark our pay ranges against current external data sources and regularly review compensation for our team members. Balsam Brands is committed to providing our team members with an internally fair, externally competitive, and fiscally prudent total compensation package administered in a simple and consistent manner.
At Balsam Brands, we strive to build a diverse, equitable, and inclusive team to fulfill our purpose to create joy together. Balsam Brands is proud to be an equal opportunity employer. We encourage people from all backgrounds, ages, abilities, and experiences to apply. We do not discriminate on the basis of race, ethnicity, religion, national origin, citizenship, marital or family status, disability, sexual orientation, gender identity or expression, pregnancy or caregiver status, veteran status, or any other legally protected status. We will ensure that individuals with disabilities are provided reasonable accommodations to participate in the job application and interview process, to perform essential job functions, and to receive other benefits and privileges of employment.
#DICE
Additional Information
All your information will be kept confidential according to EEO guidelines.
Show more
Show less","Data Warehousing, Data Engineering, Schema Design, Table Design, View Design, Data Pipeline Design, Azure, AWS, Azure Data Factory, ETL, SQL, SQL Server, Oracle, Postgres, MySQL, Data Governance, Coding Standards, Best Practices, Big Data, Artificial Intelligence, Generative AI, Machine Learning, Predictive Data Modeling","data warehousing, data engineering, schema design, table design, view design, data pipeline design, azure, aws, azure data factory, etl, sql, sql server, oracle, postgres, mysql, data governance, coding standards, best practices, big data, artificial intelligence, generative ai, machine learning, predictive data modeling","artificial intelligence, aws, azure, azure data factory, best practices, big data, coding standards, data engineering, data governance, data pipeline design, datawarehouse, etl, generative ai, machine learning, mysql, oracle, postgres, predictive data modeling, schema design, sql, sql server, table design, view design"
Senior Azure Data Engineer,BayOne Solutions,"San Francisco, CA",https://www.linkedin.com/jobs/view/senior-azure-data-engineer-at-bayone-solutions-3715844315,2023-12-17,California,United States,Mid senior,Remote,"Senior Data Engineer
Location: Remote work (candidate willing to work as per PST)
Duration: 12+ months
Skills:
10+ years of experience.
Multi-cloud computing experience (Azure & GCP).
Must have done different types of ingestion on GCP or Azure Databricks.
Experience working with Lambda functions.
Need someone who has marketing domain experience.
Should understand marketing terms and terminologies, like segmentations and campaigns.
Must have real-time ingestion experience like Kafka and Kafka queues.
Good to have (not a must) experience or knowledge of Machine Learning and Artificial Intelligence. The data engineering team is planning to integrate their orchestration tool with AI, ML related tools in future.
Soft skills:
Cross-team collaboration
Collaborate, Go getter.
Good team player.
Show more
Show less","Data Engineering, MultiCloud Computing, Azure, GCP, Databricks, Lambda Functions, Marketing Domain Experience, RealTime Ingestion, Kafka, Machine Learning, Artificial Intelligence, CrossTeam Collaboration, Collaboration, Teamwork","data engineering, multicloud computing, azure, gcp, databricks, lambda functions, marketing domain experience, realtime ingestion, kafka, machine learning, artificial intelligence, crossteam collaboration, collaboration, teamwork","artificial intelligence, azure, collaboration, crossteam collaboration, data engineering, databricks, gcp, kafka, lambda functions, machine learning, marketing domain experience, multicloud computing, realtime ingestion, teamwork"
Sr Martech Data Engineer,Diligente Technologies,"California, United States",https://www.linkedin.com/jobs/view/sr-martech-data-engineer-at-diligente-technologies-3785814154,2023-12-17,California,United States,Mid senior,Remote,"Job Title:
Sr Martech Data Engineer
Duration: Contract
Location: San Francisco, CA (Remote or hybrid to SF, both are fine.)
Big Data: Scala, Databricks, Spark SQL, Spark Streaming, Python, Pyspark
Cloud: Azure , ADF
System Design: API Design, Event-Driven Architecture.
Microservice: Core Java 8 or above, Spring Boot, Azure Functions
Front end ( Nice to have): JavaScripting, React
Show more
Show less","Scala, Databricks, Spark SQL, Spark Streaming, Python, Pyspark, Azure, ADF, API Design, EventDriven Architecture, Core Java 8, Spring Boot, Azure Functions, JavaScripting, React","scala, databricks, spark sql, spark streaming, python, pyspark, azure, adf, api design, eventdriven architecture, core java 8, spring boot, azure functions, javascripting, react","adf, api design, azure, azure functions, core java 8, databricks, eventdriven architecture, javascripting, python, react, scala, spark, spark sql, spark streaming, spring boot"
Senior Data Engineer - W2 / 1099 only,Enexus Global Inc.,"Sunnyvale, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-w2-1099-only-at-enexus-global-inc-3747901038,2023-12-17,California,United States,Mid senior,Hybrid,"Role - Senior Data Engineer
Location - Sunnyvale, CA (hybrid)
Contract Type - W2/1099 only
Minimum Experience - 11+ Years
Responsibilities
Develop and enhance data-processing, orchestration, monitoring, and more by leveraging popular open-source software, AWS, and GitLab automation.
Collaborate with product and technology teams to design and validate the capabilities of the data platform
Identify, design, and implement process improvements: automating manual processes, optimizing for usability, re-designing for greater scalability
Provide technical support and usage guidance to the users of our platform’s services.
Drive the creation and refinement of metrics, monitoring, and alerting mechanisms to give us the visibility we need into our production services.
Qualifications
Experience building and optimizing data pipelines in a distributed environment
Experience supporting and working with cross-functional teams
Proficiency working in Linux environment
8+ years of advanced working knowledge of SQL, Python, and PySpark
5+ years of experience with using a broad range of AWS technologies
Experience using tools such as: Git/Bitbucket, Jenkins/CodeBuild, CodePipeline
Experience with platform monitoring and alerts tools
Show more
Show less","Data Engineering, Data Processing, Data Orchestration, Data Monitoring, AWS, GitLab, SQL, Python, PySpark, Linux, Git/Bitbucket, Jenkins/CodeBuild, CodePipeline, Platform Monitoring, Alerting Tools","data engineering, data processing, data orchestration, data monitoring, aws, gitlab, sql, python, pyspark, linux, gitbitbucket, jenkinscodebuild, codepipeline, platform monitoring, alerting tools","alerting tools, aws, codepipeline, data engineering, data monitoring, data orchestration, data processing, gitbitbucket, gitlab, jenkinscodebuild, linux, platform monitoring, python, spark, sql"
Data visualization engineer,Fladger Associates,"Foster City, CA",https://www.linkedin.com/jobs/view/data-visualization-engineer-at-fladger-associates-3781748307,2023-12-17,California,United States,Mid senior,Hybrid,"Responsibilities:
Excellent employment opportunity for a Visualization Developer in the Foster City, CA area. (Offsite)
The individual will be joining the Supply Chain Analytics organization and will be supporting the creation and design of BI dashboards.
Selected candidate will work heavily with Tableau and ThoughtSpot to create clear, comprehensive, and detailed dashboards that are used by internal leadership teams across the organization.
They'll work cross functionally between technical and non-technical teams to understand what needs to display and created.
Area of focus is on true design of dashboards and understand all intricacies of the BI tool.
Experience with SQL to help manipulate data will be a plus, as well as experience with AWS.
Candidates should be prepared to share previous works or portfolios upon submittal.
Experience:
Bachelor’s Degree and more than 3 + years of experience required.
Top 3 Required Skill Sets: ThoughtSpot, Tableau, strong in generating visualizations with insights, Supply Chain experience
Top 3 Nice to Have Skill Sets: Data Modeling, Supply Chain planning data visualization
Ability to generate insights with predictive modeling experience
Show more
Show less","Tableau, ThoughtSpot, BI dashboards, SQL, AWS, Data Modeling, Supply Chain planning, Predictive modeling","tableau, thoughtspot, bi dashboards, sql, aws, data modeling, supply chain planning, predictive modeling","aws, bi dashboards, datamodeling, predictive modeling, sql, supply chain planning, tableau, thoughtspot"
Data QA Engineer,InterEx Group,"California, United States",https://www.linkedin.com/jobs/view/data-qa-engineer-at-interex-group-3732276824,2023-12-17,California,United States,Mid senior,Hybrid,"A key client of mine is seeking a highly skilled and experienced Data QA Engineer to join their team. In this role, you will be responsible for performing data quality analysis and validation, writing test plans, test cases, and test scripts, and validating solutions built on REST APIs, Snowflake, and data pipelines. The successful candidate will be able to demonstrate a deep conceptual understanding of data analytics architectural approaches and data models (Data Vault experience is a plus).
This role is Hybrid in either one of three locations
Miami, Florida, Dallas, Texas, or Irvine, California
Key Qualifications:
- At least 4 years of experience as a Data Quality Engineer
- At least 4 years of experience performing data quality analysis and validation
- At least 2 years of experience writing test plans, test cases, and test scripts
- At least 1 year of experience validating solutions with Snowflake and Azure
- At least 1 year of experience validating data pipelines
- Strong adherence to core software testing principles, including code modularization, versioning, git, testing, Agile, etc.
Nice-to-haves:
- Experience performing validation on:
Prefect/workflow tool
Dbt
Hasura & GraphQL
Show more
Show less","Data Quality Analysis, Data Validation, Test Plans, Test Cases, Test Scripts, REST APIs, Snowflake, Data Pipelines, Data Analytics, Data Models, Data Vault, Agile, Code Modularization, Versioning, Git, Testing, Prefect, Workflow Tool, Dbt, Hasura, GraphQL","data quality analysis, data validation, test plans, test cases, test scripts, rest apis, snowflake, data pipelines, data analytics, data models, data vault, agile, code modularization, versioning, git, testing, prefect, workflow tool, dbt, hasura, graphql","agile, code modularization, data models, data quality analysis, data validation, data vault, dataanalytics, datapipeline, dbt, git, graphql, hasura, prefect, rest apis, snowflake, test cases, test plans, test scripts, testing, versioning, workflow tool"
SENIOR DATABASE ENGINEER (CASSANDRA),Software Technology Inc.,"Concord, CA",https://www.linkedin.com/jobs/view/senior-database-engineer-cassandra-at-software-technology-inc-3661798871,2023-12-17,California,United States,Mid senior,Hybrid,"Role :
SENIOR DATABASE ENGINEER (CASSANDRA)
Location : Concord CA
Position : Hybrid
Your Roles
Leads the design planning, impact analysis, administration, implementation and maintenance of the organization's application Cassandra databases. Consults with and advises senior management and multiple clients on high impact data or database management issues, influencing strategic direction.
Handles and leads various large-scale or highly complex data/database management activities including one or more of the following: designing highly complex logical and/or physical database data model; large volume data transformation and migration; capacity planning; developing database design policies, procedures and standards; and security requirements identification, analysis and development.
Provides mentoring, guidance and general oversight to lesser experienced staff in a variety of database design, performance tuning and/or administration activities.
Candidate Will Be Required To
Lead or participate in Cassandra database management activities including designing highly complex logical and physical databases
Installation and administration of Cassandra databases
Documentation of guides for production database administrators
Defining standards for installation, deployment, security, authentication and authorization, management policies and best practices
Defining and implementing backup and restoration strategies
Defining and implementing monitoring and alarming strategies
Perform the planning, research, design, implementation, maintenance, and control of server class databases
Consult with and advise management and multiple clients on high impact data or database management issues, influencing strategic direction
Minimum Qualifications
4+ years of experience with implementing and administrating Cassandra database
Experience with Data tax Cassandra
Experience with Data tax Ops Centre backup/restoration and monitoring/alarm implementation
Experience with Ansible automation tool or equivalent
4+ years of Shell or Python or Perl experience
Preferred Skills
Proven experience with Mongo dB or other SQL and NoSQL databases a plus
Experience with Agile methodology
Demonstrated experience with UNIX and Shell Scripting
Demonstrated experience in designing for high volume OLTP applications
Demonstrated experience in Change Management and SDLC
Experience with Version Control System such as Git
Experience with Issue and Tracking software such as Jira
Thanks & Regards
Mohan sai
Technical recruiter
Phone# +1-619-605-0324|mohan.sai@stiorg.com
Software Technology, Inc
www.stiorg.com
#linkedin #w2requirements #c2crequirement
#c2c #usrecruitment #usrecruiters #commentforbetterreach #comment #requirement #w2requirements #W2 #c2chotlist #C2C #vendorlist #vendorempanelment #shortlisted #hotlist#jolttek #c2crequirements #contractual #primevendors #c2c #c2cvendors #directclient #javadeveloper #javafullstackdeveloper #salesforcedeveloper #sqldba #sapfico #oraclejobs #oracledba #servicenow #LeadServiceNow #uideveloper #bi #reportsdeveloper #bianalyst #bi #upwork #upworksuccess #upworkfreelancer #upwork #thankyou #talent #like #freelancers #aws #javafullstackdeveloper #awscloudexperience #migration #migrationservices #spring #springboot #springsecurity #springframework #awsservices #awsglue #javabackenddeveloper #javabackend #remoteopportunity #microservices #urgentopening #urgentrequirement #urgenthiring #newcareeropportunities #w2jobs #w2 #h1bjobs #c2c #c2crequirements #c2cusajobs #angular #UIDeveloper #DevOps #DevOpsengineer #Angularfrontenddeveloper.#fullstackdeveloper #datanalyst #DA #salesforcedevelopers #offerroles #interviewroles #hotlist #job #python #developer #jobs#hotlists #c2chotlist #benchlist #c2c #c2crequirements #c2cjobs #c2cvendors #c2cusajobs#share #recruiters #hotlist #email #comment #recruiters #share #linkedinconnection #technicalrecruiters #databaseengineer #database #cassandra #dataengineer #dataadministrater #data
Show more
Show less","Cassandra, DataStax Cassandra, DataStax Ops Centre, Ansible, Shell, Python, Perl, MongoDB, SQL, NoSQL, Agile, UNIX, Shell Scripting, OLTP, Change Management, SDLC, Git, Jira","cassandra, datastax cassandra, datastax ops centre, ansible, shell, python, perl, mongodb, sql, nosql, agile, unix, shell scripting, oltp, change management, sdlc, git, jira","agile, ansible, cassandra, change management, datastax cassandra, datastax ops centre, git, jira, mongodb, nosql, oltp, perl, python, sdlc, shell, shell scripting, sql, unix"
Sr. Database Engineer,GIA (Gemological Institute of America),"Carlsbad, CA",https://www.linkedin.com/jobs/view/sr-database-engineer-at-gia-gemological-institute-of-america-3775266141,2023-12-17,California,United States,Mid senior,Hybrid,"Job Title: Sr. Database Engineer
Location: Carlsbad, CA
Gemological Institute of America (GIA) has immediate opportunity for a
Sr. Database Engineer in Carlsbad, CA. This is a hybrid role (combination of onsite and remote / work from home).
This is an exciting opportunity to play a pivotal role in managing multiple databases that hold business critical data and support business operations. The ideal candidate will have knowledge of modern database technologies and data structure design best practices. In this role, you will provide strategic guidance and implementation of the corporate Business Intelligence solution and the corresponding data feeds.
Responsibilities:
Analyze the needs of an organization and produce an effective database system according to their needs and specifications.
Participate in the development and maintenance of strategic, architectural and tactical plans for the database and data warehouse environments; be conversant with, and cognizant of, the future direction of systems, how that direction impacts the company and how future and current database systems can be utilized for the company’s benefit.
Implement and maintain database code in the form of stored procedures, scripts, queries, views, triggers, etc.
Ensure the efficiency of database code, the integrity of data structures, and the quality of data content.
Provide technical leadership on database systems and solutions.
Technical Competencies:
Bachelor’s Degree in Computer Science, Engineering or equivalent combination of education and related work experience.
5+ years of experience working with Oracle databases, query tuning and troubleshooting
Expert-level knowledge of SQL, PL/SQL, including stored procedures, functions, triggers, and views.
Experience with version control systems, DevOps, continuous delivery, and continuous integration practices required.
Experience of managing, owning and delivering high quality projects to objectives and within time scales required.
About GIA:
Gemological Institute of America (GIA) is the world’s foremost authority in gemology. GIA is a global organization with headquarters located in Carlsbad, CA. GIA in Carlsbad boasts a 17-acre ocean view campus that accommodates approximately 800 of its 3,000 total employees worldwide.
Whether you are passionate about science, history, design, education, cutting-edge technology or bringing your corporate expertise to advance our mission, we have a place for you. GIA offers competitive salaries and exceptional benefits as well as training opportunities to employees all over the globe.
Show more
Show less","Oracle, SQL, PL/SQL, Stored procedures, Functions, Triggers, Views, Version control systems, DevOps, Continuous delivery, Continuous integration","oracle, sql, plsql, stored procedures, functions, triggers, views, version control systems, devops, continuous delivery, continuous integration","continuous delivery, continuous integration, devops, functions, oracle, plsql, sql, stored procedures, triggers, version control systems, views"
Senior Data Security Engineer,Glocomms,San Francisco Bay Area,https://www.linkedin.com/jobs/view/senior-data-security-engineer-at-glocomms-3775085552,2023-12-17,California,United States,Mid senior,Hybrid,"Title:
Senior Data Security Engineer.
Location:
Bay Area, CA – hybrid!
Compensation:
$160,000 - $200,000 base + bonus + equity + benefits.
Glocomms are partnered with a FinTech firm in the Bay Area who are searching for a Senior Data Security Engineer who is fluent in Mandarin at a professional level to focus on security incident management, forensic testing, DLP, monitoring, threat/risk/vulnerability management, automation, and security compliance.
Key responsibilities:
Perform security incident event, threat, and vulnerability research including triage, remediation, and documentation.
Collaborate with corporate stakeholders, conduct forensic testing, evaluate test results, and create long-term resolution strategies for gaps, vulnerabilities, and control shortcomings.
Further develop the organization's overall security posture by evaluating, validating, reviewing, and auditing security as well as privacy controls.
Secure the privacy, availability, and integrity of information assets.
Cultivate data security awareness whilst keeping up with emerging technologies and security trends, and ensure security best practices are carried out.
Enhance overall operations, strategy, and risk management for Data Security.
Continuously strengthen data governance, participate in the establishment of security policies, security standards, and risk governance reporting procedures in conjunction with cross-functional teams.
Required experience:
In depth knowledge of regulatory frameworks such as ISO27001, NYDFS 500, PCI DSS, GDPR, etc.
Hands-on experience working with threat intelligence platforms, SIEM, SOAR, IDS/IPS, DLP, FIM, and networking monitoring tools.
Strong understanding of emerging security threats, risks, vulnerabilities, TTP’s, and how to defend against them.
Experience with various vulnerabilities and carrying out remediations.
Scripting experience (Python, Java, Perl, Bash, SQL, etc.) for automation.
Knowledge and experience with DLP, Security Assurance, Data Security.
Ability to assess, design, implement and configure security tools/products.
Fluent in Mandarin and English.
Show more
Show less","Data Security, Security Incident Management, Forensic Testing, DLP, Monitoring, Threat/Risk/Vulnerability Management, Security Compliance, ISO27001, NYDFS 500, PCI DSS, GDPR, Threat Intelligence Platforms, SIEM, SOAR, IDS/IPS, DLP, FIM, Networking Monitoring Tools, Security Threats, Risks, Vulnerabilities, TTPs, Remediation, Python, Java, Perl, Bash, SQL, Data Security Assurance, Security Tools/Products, Mandarin, English","data security, security incident management, forensic testing, dlp, monitoring, threatriskvulnerability management, security compliance, iso27001, nydfs 500, pci dss, gdpr, threat intelligence platforms, siem, soar, idsips, dlp, fim, networking monitoring tools, security threats, risks, vulnerabilities, ttps, remediation, python, java, perl, bash, sql, data security assurance, security toolsproducts, mandarin, english","bash, data security, data security assurance, dlp, english, fim, forensic testing, gdpr, idsips, iso27001, java, mandarin, monitoring, networking monitoring tools, nydfs 500, pci dss, perl, python, remediation, risks, security compliance, security incident management, security threats, security toolsproducts, siem, soar, sql, threat intelligence platforms, threatriskvulnerability management, ttps, vulnerabilities"
Principal Data Engineer,Harnham,"Los Angeles County, CA",https://www.linkedin.com/jobs/view/principal-data-engineer-at-harnham-3784645899,2023-12-17,California,United States,Mid senior,Hybrid,"Principal Data Engineer
Los Angeles, CA- HYBRID
$200,000 - 230,000
An impressive opportunity for a Principal Data Engineer, to join a growing media startup that is a leader in its industry. If you want to make a big impact through your drive in data and leadership skills, this is the job for you!
THE COMPANY
This company is a media startup working to enhance the creator community and looking for a Principal Data Engineer to drive scalability and performance within data. This is a critical role for building out their big data pipelines and scalability for their high growth.
THE ROLE
This Senior Data Engineer will be responsible for building out scalable data pipelines, distributed computing, and working on very large data sets.
Work on large data set
Build scalable and performant pipelines
Software Engineering background
STRONG experience with Spark
Distributed computing
YOUR SKILLS & EXPERIENCE
High level experience with Spark (4-6 years)
Must have experience with Python or Scala
Must have experience with AWS, GCP, or Azure
Experience with Redshift or any data lake related technology
Experience with Media data is a +++
BENEFITS
200,000- 230,000 base salary
Bonus
Equity
Medical, Dental, Vision Insurance
HOW TO APPLY
Please register your interest by sending your resume to Alexandra Oechsle via the apply link on this page.
Show more
Show less","Apache Spark, Scala, Python, AWS, Google Cloud Platform, Azure, Redshift, Data lake, Media data, Data pipelines, Distributed computing, Big data, Software engineering, Scalability, Performance","apache spark, scala, python, aws, google cloud platform, azure, redshift, data lake, media data, data pipelines, distributed computing, big data, software engineering, scalability, performance","apache spark, aws, azure, big data, data lake, datapipeline, distributed computing, google cloud platform, media data, performance, python, redshift, scala, scalability, software engineering"
Senior Data Engineer | Series D Video Analytics Start-Up (200-250 TC),Coda Search│Staffing,"Culver City, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-series-d-video-analytics-start-up-200-250-tc-at-coda-search%E2%94%82staffing-3705726681,2023-12-17,California,United States,Mid senior,Hybrid,"I'm recruiting a Senior Data Engineer for my client, a Series D start-up that has been rated one of TIME100’s most influential companies. They empower YouTube creators by providing capital, insights, and partnerships to scale their businesses and offer creator-friendly growth capital tailored to each creator's needs, allowing reinvestment such as hiring teams or building production studios, while creators maintain control over content and earnings. They also provide creators with detailed content performance insights, helping them maximize their library and future upload value.
This company has been featured in publications like Forbes, Fast Company, Variety, and Axios, has deployed over $740 million to YouTube creators, licensed a vast catalog of 600,000+ videos generating 88 billion monthly watch-time minutes, and offers a unique media solution to Advertisers and Ad Agencies, known for its transparency and efficiency.
As a Senior Data Engineer, you'll use Spark and your software/data engineering expertise to scale their big data platform. Responsibilities include building scalable ETL pipelines, data quality checks, creating augmented datasets, utilizing analytics tools, troubleshooting, automating pipelines, setting up analytics tables, and enhancing AWS data quality. Additionally, you'll have the opportunity to mentor junior team members.
Required:
5+ years of software engineering experience
3+ years of data engineering with Apache Spark (including the last 2 years)
3+ years of experience with cloud services
Proficiency in DataFrame APIs (Pandas and Spark) for parallel and single-node processing
Proficiency in languages like Python or Scala
Proficiency in SQL, particularly with Redshift
My client offers a comprehensive benefits package, including 100% medical, dental, and vision coverage, 401(k) matching, stock options, and complimentary gym access.
Apply now to learn more!
Show more
Show less","Apache Spark, Software Engineering, Data Engineering, ETL Pipelines, Data Quality Checks, Augmented Datasets, Analytics Tools, Troubleshooting, Automation, Analytics Tables, AWS, Python, Scala, SQL, Redshift, DataFrame APIs, Pandas","apache spark, software engineering, data engineering, etl pipelines, data quality checks, augmented datasets, analytics tools, troubleshooting, automation, analytics tables, aws, python, scala, sql, redshift, dataframe apis, pandas","analytics tables, analytics tools, apache spark, augmented datasets, automation, aws, data engineering, data quality checks, dataframe apis, etl pipelines, pandas, python, redshift, scala, software engineering, sql, troubleshooting"
Senior Data Engineer,Spotter,"Culver City, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-spotter-3550960182,2023-12-17,California,United States,Mid senior,Hybrid,"Overview
Spotter, named one of TIME100's Most Influential Companies this year, empowers top YouTube creators to accelerate their business and unleash their full creative potential by giving them access to the capital, knowledge, and community they need to succeed at scale. As the top provider of creator-friendly growth capital, Spotter tailors our investments to meet the unique needs of each creator we partner with, giving them the freedom to create without compromise.
Creators are free to reinvest their funds however they choose, from hiring a team, to building their own production studios, and everything in between, all while maintaining total control over their catalogs, their channels, and their future earnings. In addition to funding, Spotter provides creators with in-depth data insights into the performance of their existing content, enabling them to leverage the full value of their library, as well as the value of future uploads and how they can improve performance in the future.
Featured in Forbes, Fast Company, Variety, Axios, and more, Spotter has already deployed over
$850 million
to YouTube creators to reinvest in themselves and accelerate their growth. Spotter has licensed content that consists of over
725,000 videos
, which generate
88 billion
monthly watch-time minutes. With our curated premium video catalog, we deliver a unique scaled media solution to Advertisers and Ad Agencies that is transparent, efficient, and 100% brand safe.
Are you ready to become a pivotal member of Spotter's outstanding data engineering team? We are in search of a talented Senior Data Engineer, specializing in scalable data processing, working with big data sets from acquisition to transformation to serving. This position will play a pivotal role in designing, building, and optimizing data infrastructure. You will be a key player empowering us to make data-informed decisions that will fuel our innovation and growth.
What You’ll Do
Develop and maintain scalable data pipelines, including:
ETL pipelines, both single and multi-node solutions
Build data quality assurance steps for new and existing pipelines
Create derived datasets with augmented properties
Work on analytics ready datasets to power internal and creator facing tools
Troubleshoot issues when they arise, working directly with internal data consumers
Automate pipeline runs with scheduling and orchestration tools
Work with large scale datasets
Work with/use various external APIs to enhance data
Setup database tables for analytics users to consume the data collected by the Data Engineering team
Work with big data technologies to improve data availability and data quality in the cloud (AWS)
Mentor members of the team
Who You Are
Bachelor’s degree, preferably in Computer Science or Computer Information Systems
4+ years of software engineering experience
3+ years of data engineering experience with Apache Spark or Apache Flink
3+ years of experience running software and services in the cloud
Proficiency in working with DataFrame APIs (Pandas and Spark) for parallel and single node processing
Proficiency using advanced languages and techniques with Python, Scala, etc. with modern data optimized file formats such as Parquet and Avro
Proficiency with SQL on RDBMS and data warehouse solutions like Redshift
Additional Valued Skills
Experience with YouTube APIs
Experience with data acquisition from external APIs at large scale / in parallel processing
Experience with Data-Lake technologies
Experience with AWS Glue metastore
Experience with Data-Mesh approaches
Experience with data cataloging, data lineage and data governance tools and approaches
Experience with vector databases
Why Spotter
Medical and vision insurance covered up to 100%
Dental insurance
401(k) matching
Stock options
Complimentary gym access
Autonomy and upward mobility
Diverse, equitable, and inclusive culture, where your voice matters.
In compliance with local law, we are disclosing the compensation, or a range thereof, for roles that will be performed in Culver City. Actual salaries will vary and may be above or below the range based on various factors including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs. The overall market range for roles in this area of Spotter are typically: $100-$500K salary per year. The range listed is just one component of Spotter’s total compensation package for employees. Other rewards may include annual discretionary bonus and equity.
COVID-19 Vaccination Policy
Spotter requires proof of being fully vaccinated for COVID-19 as a condition of commencing employment.
Spotter is an equal opportunity employer. Spotter does not discriminate in employment on the basis of race, religion, creed, color, national origin, ancestry, citizenship, physical or mental disability, medical condition, genetic characteristics or information, marital status, sex (including pregnancy, childbirth, breastfeeding, and related medical conditions), gender, gender identity, gender expression, age, sexual orientation, military status, veteran status, use of or request for family or medical leave, political affiliation, or any other status protected under applicable federal, state or local laws.
Equal access to programs, services and employment is available to all persons. Those applicants requiring reasonable accommodations as part of the application and/or interview process should notify a representative of the Human Resources Department.
Show more
Show less","Apache Spark, Apache Flink, SQL, Python, Scala, Pandas, Parquet, Avro, Redshift, AWS Glue metastore, DataLake technologies, DataMesh approaches, Data cataloging, Data lineage, Data governance, Vector databases, YouTube APIs, Data acquisition, ETL pipelines","apache spark, apache flink, sql, python, scala, pandas, parquet, avro, redshift, aws glue metastore, datalake technologies, datamesh approaches, data cataloging, data lineage, data governance, vector databases, youtube apis, data acquisition, etl pipelines","apache flink, apache spark, avro, aws glue metastore, data acquisition, data cataloging, data governance, data lineage, datalake technologies, datamesh approaches, etl pipelines, pandas, parquet, python, redshift, scala, sql, vector databases, youtube apis"
Senior AWS Data Engineer,KPI Partners,"Santa Monica, CA",https://www.linkedin.com/jobs/view/senior-aws-data-engineer-at-kpi-partners-3620307494,2023-12-17,California,United States,Mid senior,Hybrid,"KPI Partners, A global consulting firm focused on strategy, technology, and digital transformation. We help companies tackle their most ambitious projects and build new capabilities. We provide solutions in Cloud, Data, Application Development & BI spaces.
Title: Sr AWS Data Engineer – Long-term Contract
Location: Dublin, CA / Santa Monica, CA (Hybrid)
Description
Senior Hands-on Data Engineer and Architect - Designs, develops and implements AWS eco-system-based data pipeline/engineering applications to support business requirements. Follows approved life cycle methodologies, creates design documents, and performs program coding and testing. Resolves technical issues through debugging, research, and investigation. Experience/Skills Required:
Expert in Cloud Big Data Solutions using AWS, Snowflake, and SQL. DBT preferred and highly desirable.
Experience in GitLab and CI/CD implementation in Data Engineering
Expert in Data modeling and Data Warehousing
Understand and convert business requirements into Data Models & Schema Designs
Ability to communicate well with business users to understand needs and requirements, IT Stakeholders to understand data sources and access, and manage client expectations and delivery
Show more
Show less","AWS, Snowflake, SQL, GitLab, CI/CD, Data modeling, Data warehousing, Data pipeline, DBT","aws, snowflake, sql, gitlab, cicd, data modeling, data warehousing, data pipeline, dbt","aws, cicd, data pipeline, datamodeling, datawarehouse, dbt, gitlab, snowflake, sql"
Senior Data Engineer,Electric Hydrogen,"San Carlos, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-electric-hydrogen-3744945444,2023-12-17,California,United States,Mid senior,Hybrid,"Electric Hydrogen
’s mission is to make molecules to decarbonize our world! Our outstanding people are our most important asset and will allow us to deliver hydrogen from renewable electrolysis for heavy industry, at prices below fossil fuels.
We are searching for an accomplished and motivated Senior Data Engineer to build and maintain mission-critical data infrastructure for the world's most powerful electrolyzer. As a Data Engineer, you will develop services to ingest, analyze, and store plant data and key performance metrics. Tools you build will monitor our electrolyzer fleet, assessing their operation and detecting issues before they lead to unplanned downtime.
You will be based in San Carlos, CA [and can work on a hybrid basis], reporting to the Director of Software Engineering.
Role And Responsibilities
Ensure data availability to internal and external stakeholders from internal devices and deployed plants
Architect streaming pipelines, analysis triggers, and notifications
Building data models and develop schemas for relational databases (PostgreSQL, MySQL, MS SQL Server)
Work with data from industrial PLCs and other connected devices
Coalesce data from manufacturing and deployed plant data for use in analysis
Design data systems for machine learning applications
Qualifications
Bachelor's degree in Computer Science, Engineering, or related field
6+ years working with data pipelines and industrial networks
Significant experience with relational databases (PostgreSQL, MySQL, MS SQL Server)
Familiarity with industrial streaming protocols such as MQTT and Kafka
Familiarity with OPC servers, OPC UA, and PLC communication
Experience using git and git-based workflows for version control.
Experience working with time series data and time series databases
Knowledge of process historians (OSI PI, FactoryTalk Historian)
Experience building data pipelines in AWS, using tools such as IoT Core and Kinesis
Knowledge of OSI PI ecosystem including asset framework
Experience with machine learning data pipelines
Compensation & Benefits
| Senior Data Engineer (P4)
San Carlos Zone
$173,000—$190,000 USD
Actual base salary offered to the hired applicant will be determined based on their work location, level, qualifications, job-related skills, as well as relevant education or training and experience.
Base salary is just one part of Electric Hydrogen’s total rewards package. We feel strongly that our team should not have to worry about having quality healthcare. In addition to the base salary offered, the hired applicant may receive:
an equity grant
time off programs
a $75/month cell phone allowance
a 4% employer 401(k) match
100% fully paid premiums for employees and their families: medical, dental, vision, life insurance, short-term & long-term disability coverage
a discretionary bonus
Electric Hydrogen’s benefits programs are subject to eligibility requirements.
COVID-19 vaccination required for all employees and contingent workers, unless a reasonable accommodation is approved. All prospective hires will be expected to provide proof of vaccination to attend an onsite interview or at their first day of employment.
About Electric Hydrogen
Electric Hydrogen is a team of the world's experts in scaling technologies for the post-carbon world, with a proven record in transforming the grid and transportation sectors. Backed by some of the world's top venture capital firms, we design and manufacture electrolytic hydrogen systems matched to renewable power sources to create green hydrogen by splitting water. We are building a cost-effective and transformative path between renewable energy and multiple large industrial sectors. Abundant and low-cost renewable energy sources will power the world, and Electric Hydrogen technology will use this energy to decarbonize industry through sustainable materials. We were founded in 2020 and are based in California and Massachusetts.
Electric Hydrogen is proud to be an equal opportunity employer. We are dedicated to building a diverse, inclusive, and authentic workplace for all to belong. We are aware that people from historically underrepresented groups are less likely to apply if they don't meet 100% of the job requirements. We are actively working on efforts to change this social norm. If you are excited about this role,
we encourage you to apply
!
Show more
Show less","Data engineering, Data pipelining, SQL, AWS, Machine learning, OPC, Git, Python, PostGRES, MySQL, Microsoft SQL Server, MQTT, Kafka","data engineering, data pipelining, sql, aws, machine learning, opc, git, python, postgres, mysql, microsoft sql server, mqtt, kafka","aws, data engineering, datapipeline, git, kafka, machine learning, microsoft sql server, mqtt, mysql, opc, postgres, python, sql"
Senior Data Engineer (AWS & Data Brick),Allnessjobs,"Pleasanton, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-aws-data-brick-at-allnessjobs-3674255567,2023-12-17,California,United States,Mid senior,Hybrid,"Senior Data Engineer for Pleasanton, CA
Required SKILL SET and Level of Skill set
Required 10+ years of experience
AWS & Data Brick Exp is must
Airflow - expert
Python - expert
Spark - expert
SQL - expert
AWS - redshift, S3
Show more
Show less","AWS, Apache Spark, Data Brick, Airflow, Python, SQL, AWS Redshift, AWS S3","aws, apache spark, data brick, airflow, python, sql, aws redshift, aws s3","airflow, apache spark, aws, aws redshift, aws s3, data brick, python, sql"
Senior AWS Data Engineer,KPI Partners,"Dublin, CA",https://www.linkedin.com/jobs/view/senior-aws-data-engineer-at-kpi-partners-3648719677,2023-12-17,California,United States,Mid senior,Hybrid,"KPI Partners, A global consulting firm focused on strategy, technology, and digital transformation. We help companies tackle their most ambitious projects and build new capabilities. We provide solutions in Cloud, Data, Application Development & BI spaces.
Title: Sr AWS Data Engineer – Long-term Contract
Location: Dublin, CA / Santa Monica, CA (Hybrid)
Description
Senior Hands-on Data Engineer and Architect - Designs, develops and implements AWS eco-system-based data pipeline/engineering applications to support business requirements. Follows approved life cycle methodologies, creates design documents, and performs program coding and testing. Resolves technical issues through debugging, research, and investigation. Experience/Skills Required:
Expert in Cloud Big Data Solutions using AWS, Snowflake, and SQL. DBT preferred and highly desirable.
Experience in GitLab and CI/CD implementation in Data Engineering
Expert in Data modeling and Data Warehousing
Understand and convert business requirements into Data Models & Schema Designs
Ability to communicate well with business users to understand needs and requirements, IT Stakeholders to understand data sources and access, and manage client expectations and delivery
Show more
Show less","AWS, Snowflake, SQL, DBT, GitLab, CI/CD, Data modeling, Data Warehousing, Data Models, Schema Designs","aws, snowflake, sql, dbt, gitlab, cicd, data modeling, data warehousing, data models, schema designs","aws, cicd, data models, datamodeling, datawarehouse, dbt, gitlab, schema designs, snowflake, sql"
Sr. Data Engineer,Theorem,"San Mateo, CA",https://www.linkedin.com/jobs/view/sr-data-engineer-at-theorem-3778902139,2023-12-17,California,United States,Mid senior,Hybrid,"About Us
Pursuit of truth in credit.
By using machine learning to anticipate and manage risk in credit, we’re empowering our partners and lenders to unlock opportunity and access for more borrowers, everywhere.
We strive to be the preferred partner to lending platforms, providing not only access to capital but also underwriting technology capabilities to allow innovative lending platforms to grow their business.
Our firm is made up of 60+ professionals working in San Mateo (HQ) and New York, working in-office on Tuesdays and Thursdays. We are passionate, hard-working, relentlessly-resourceful, impact-focused individuals. We deeply value intellectual curiosity, independence of thought, creative idea generation, empathy, and close collaboration.
The Role
As a Data Engineer, you will work alongside quantitative researchers, finance & operations, investor relations & sales, and capital markets & partnerships.
Your role is to develop the systems and data pipelines that enable the shared data assets informing every decision of the firm.
What You'll Do
Partner with stakeholders and senior leaders across Theorem to understand needs and set priorities for data-driven workflows
Scope new infrastructure capabilities and partner with engineers to get those delivered
Develop robust software to support new and existing data initiatives across the firm
Contribute to the creation of an industry-leading loan modeling platform that will generate data around which the business will coalesce
Build complex data pipelines with high demands for correctness, speed, robustness, and availability
Grow as an engineer by working with and receiving mentorship from senior engineers at the firm
You Will Own
Availability and freshness of data through clear ETL ownership and automated alerting strategies
Integration of system operational data sources into the Theorem data warehouse
Quantification and tracking of data quality across all pipelines and build technology
End-user discovery and accessibility of data used across the firm by normalizing, standardizing, and cataloging all assets
What We're Looking For
A minimum of 2 years of Data Engineering experience at a technology company
Track record of being able to effectively communicate technical concepts and scope solutions
Experience building automated reports, dashboards, and visualizations of curated data
Experience assessing, implementing, and monitoring data validation and quality (correctness, completeness, availability, etc.)
Fluency in SQL and Python
Deep expertise in relational data modeling, schema design, and normalization
Bonus
Previous experience in the financial industry
Characteristics To Thrive
Hardworking and gritty
Ethical, intellectually honest, and transparent
Detail-oriented
Proactive with communication
Collaborative and team success-oriented
Excited to learn and grow from feedback and experience
At home on small, high-impact teams
Thorough in your end-to-end ownership of outcomes
Biased towards action to solve problems
Additional Information
Expected full-time salary range between $150,000 to $220,000 + bonus + equity + benefits
Advertised and actual salary ranges may differ by geographic area, work experience, education, and/or skill level
Our Commitment
We foster an environment that welcomes professionals with a diversity of backgrounds and ideas. We value professionals who are thoughtful, innovative, tenacious, and mission-driven. Every member of the team has a major impact on the company's success with visible contributions to the business. We encourage and reward growth, learning, and a solutions-seeking mindset. We offer a competitive salary and opportunity for equity ownership, generous benefits, and an inclusive and collaborative work environment. If you’re excited by the opportunities to create outsized impact as part of a world-class team, we strongly encourage you to apply.
We provide reasonable accommodation for qualified individuals with disabilities and disabled veterans in job application procedures. If you have any difficulty using our online system and you need an accommodation due to a disability, you may use the following alternative email address to contact us about your interest in employment: careers@theoremlp.com. Alternatively, you can contact us at 415-489-0457.
Theorem does not accept unsolicited agency resumes and is not responsible for any fees related to unsolicited resumes.
Show more
Show less","Data Engineering, Machine Learning, Python, SQL, ETL, Data Warehousing, Schema Design, Normalization, Data Validation, Data Quality, Data Visualization, Data Modeling, Data Pipelines, Relational Databases","data engineering, machine learning, python, sql, etl, data warehousing, schema design, normalization, data validation, data quality, data visualization, data modeling, data pipelines, relational databases","data engineering, data quality, data validation, datamodeling, datapipeline, datawarehouse, etl, machine learning, normalization, python, relational databases, schema design, sql, visualization"
Palantir Application Developer /Data engineer,Cognizant,Dallas-Fort Worth Metroplex,https://www.linkedin.com/jobs/view/palantir-application-developer-data-engineer-at-cognizant-3783729049,2023-12-17,Fort Worth,United States,Mid senior,Onsite,"We are Cognizant AI & Analytics !! We are seeking a Palantir Application Developer /Data engineer
Job Title - Palantir Application Developer /Data engineer
Primary Skill Set: Palantir, Python. (Analytical background)
Secondary Skill set: SQL, Spark.
Domain: Network domain (Layer 1 – preferred)
Location: Dallas, TX.
Job Description:
As a Lead developer who has 8-10+years of experience in implementing analytical solutions by leveraging Networking data using Palantir foundry.
Integrate advanced analytics capabilities within the Palantir platform to enhance data-driven decision-making.
Conduct thorough system analysis, document processes, and ensure seamless solution integration.
Expertise in Palantir technologies, including hands-on experience in analytics components.
Apply extensive expertise in Fiber and 5G, covering Layer 1/2/3, IP, and associated Network Elements, including operational processes.
Utilize in-depth understanding of Network Service design, plan, and build, incorporating Activation, Capacity Management, Service Provisioning, Design, Network/Equipment Inventory and Topology, Network Configuration, etc., to guide effective solutions.
Work effectively in a DevOps/Agile environment, contributing to streamlined development processes.
Working experience in managing large data sets
Excellent team player able to work with virtual and global across functional teams at all levels.
Palantir Developer and Code workbook
certification is must
Show more
Show less","Palantir, Python, SQL, Spark, Network domain, Fiber, 5G, Layer 1/2/3, IP, Network Elements, Network Service design, Network/Equipment Inventory, Topology, Network Configuration, DevOps/Agile, Data sets, Palantir Developer and Code workbook, Certification","palantir, python, sql, spark, network domain, fiber, 5g, layer 123, ip, network elements, network service design, networkequipment inventory, topology, network configuration, devopsagile, data sets, palantir developer and code workbook, certification","5g, certification, data sets, devopsagile, fiber, ip, layer 123, network configuration, network domain, network elements, network service design, networkequipment inventory, palantir, palantir developer and code workbook, python, spark, sql, topology"
Lead Data Engineer,Jobs for Humanity,"Fort Worth, TX",https://www.linkedin.com/jobs/view/lead-data-engineer-at-jobs-for-humanity-3769292786,2023-12-17,Fort Worth,United States,Mid senior,Onsite,"Company Description
Jobs for Humanity is partnering with Capital One to build an inclusive and just employment ecosystem. Therefore, we prioritize individuals coming from the following communities: Refugee, Neurodivergent, Single Parent, Blind or Low Vision, Deaf or Hard of Hearing, Black, Hispanic, Asian, Military Veterans, the Elderly, the LGBTQ, and Justice Impacted individuals. This position is open to candidates who reside in and have the legal right to work in the country where the job is located.
Company Name: Capital One
Job Description
Job Title: Lead Data Engineer Location: Plano, Texas, United States of America Are you passionate about technology and problem-solving? Do you enjoy working in a fast-paced and collaborative environment? At Capital One, we are a diverse group of individuals who love to innovate and make a positive impact on our customers' lives. We are looking for Data Engineers who are excited about combining data with emerging technologies. As a Lead Data Engineer, you will be at the forefront of driving transformation within Capital One. What You'll Do: - Collaborate with agile teams to design, develop, test, implement, and support technical solutions using full-stack development tools and technologies. - Work with a team of experienced developers in machine learning, distributed microservices, and full stack systems. - Utilize programming languages like Java, Scala, Python, and open source databases to build data infrastructure. - Stay updated with the latest tech trends, experiment with new technologies, and participate in technology communities. - Collaborate with digital product managers to deliver cloud-based solutions that empower millions of Americans financially. - Conduct unit tests and code reviews to ensure high-quality, performant code. Basic Qualifications: - Bachelor's Degree. - At least 6 years of experience in application development. - At least 2 years of experience in big data technologies. - At least 1 year of experience with cloud computing (AWS, Microsoft Azure, Google Cloud). Preferred Qualifications: - 7+ years of experience in application development including Python, SQL, Scala, or Java. - 4+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud). - 4+ years of experience with distributed data/computing tools (MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, or MySQL). - 4+ years of experience working on real-time data and streaming applications. - 4+ years of experience with NoSQL implementation (MongoDB, Cassandra). - 4+ years of data warehousing experience (Redshift or Snowflake). - 4+ years of experience with UNIX/Linux. - 2+ years of experience with Agile engineering practices. At Capital One, we offer comprehensive and competitive benefits that support your well-being. Visit our website to learn more about our benefits. Eligibility varies based on employment status. Note: Capital One does not sponsor employment authorization for this position. We are an equal opportunity employer committed to diversity and inclusion in the workplace. We encourage all qualified applicants to apply, regardless of their gender, race, age, religion, disability, sexual orientation, or any other protected status. If you need accommodation during the application process, please contact Capital One Recruiting. Any information shared will be kept confidential and used only for providing necessary accommodations. For technical support or questions about our recruiting process, please email Careers@capitalone.com. Please note that Capital One Financial is comprised of different entities. Positions posted in specific countries are for those respective entities. --- Response: Subject: Lead Data Engineer Application Dear Hiring Team, I am writing to express my interest in the position of Lead Data Engineer at Capital One. I am passionate about technology and problem-solving, and I believe my skills and experience make me a strong fit for this role. Please find my application details below: - Name: [Your Name] - Contact Information: [Phone Number, Email Address] - Location: [City, State, Country] - Availability: [Full-time, Part-time] - Preferred Salary: [Salary Expectations] [Include a brief introduction about yourself, highlighting your relevant experience and skills.] I am excited about the opportunity to work with the Capital One team in driving a major transformation within the company. I believe my expertise in [mention relevant technologies or tools] and my experience in [mention any industry-specific experience] make me a valuable addition to the team. In my previous roles, I have collaborated closely with agile teams, designed and developed technical solutions, and implemented cloud-based solutions. I am skilled in programming languages like [list relevant programming languages], and I have experience working with [list relevant technologies or tools]. I am also committed to staying up-to-date with the latest tech trends and continually expanding my knowledge. I am confident in my ability to contribute to the success of Capital One's data infrastructure projects. I am a dedicated team player and am always eager to learn and grow in my career. I believe that my passion for technology and problem-solving, combined with my strong interpersonal and communication skills, will enable me to thrive in this role. Thank you for considering my application. I look forward to the opportunity to discuss how I can contribute to the success of Capital One as a Lead Data Engineer. Please find my resume attached for your review. If you require any additional information, please do not hesitate to contact me. Sincerely, [Your Name] --- Bullet points: - Passionate about technology and problem-solving - Collaborate with agile teams to design, develop, and implement technical solutions - Skilled in programming languages like Java, Scala, Python, and open source databases - Stay updated with the latest tech trends and participate in technology communities - Experience with cloud computing (AWS, Microsoft Azure, Google Cloud) - Deliver cloud-based solutions that empower millions of Americans financially - Conduct unit tests and code reviews to ensure high-quality, performant code - Bachelor's Degree required - At least 6 years of experience in application development - Experience with big data technologies - Preferred experience with distributed data/computing tools, real-time data and streaming applications, NoSQL implementation, data warehousing, UNIX/Linux, and Agile engineering practices.
Show more
Show less","Java, Scala, Python, SQL, R, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, MySQL, MongoDB, Cassandra, Redshift, Snowflake, UNIX/Linux, AWS, Microsoft Azure, Google Cloud, Agile","java, scala, python, sql, r, hadoop, hive, emr, kafka, spark, gurobi, mysql, mongodb, cassandra, redshift, snowflake, unixlinux, aws, microsoft azure, google cloud, agile","agile, aws, cassandra, emr, google cloud, gurobi, hadoop, hive, java, kafka, microsoft azure, mongodb, mysql, python, r, redshift, scala, snowflake, spark, sql, unixlinux"
SR. Data Engineer (1008352),The Judge Group,"Arlington, TX",https://www.linkedin.com/jobs/view/sr-data-engineer-1008352-at-the-judge-group-3747091845,2023-12-17,Fort Worth,United States,Mid senior,Onsite,"Location:
Arlington, TX
Salary:
Negotiable
Description:
Our client is currently seeking a SR. Data Engineer
Overview
We are expanding our efforts into complementary data technologies for decision support in areas of ingesting and processing large data sets including data commonly referred to as semi-structured or unstructured data. Our interests are in enabling data science and search based applications on large and low latent data sets in both a batch and streaming context for processing. To that end, this role will engage with team counterparts in exploring and deploying technologies for creating data sets using a combination of batch and streaming transformation processes. These data sets support both off-line and in-line machine learning training and model execution. Other data sets support search engine based analytics. Exploration and deployment of technologies activities include identifying opportunities that impact business strategy, collaborating on the selection of data solutions software, and contributing to the identification of hardware requirements based on business requirements. Responsibility also includes coding, testing, and documentation of new or modified scalable analytic data systems including automation for deployment and monitoring. This role participates along with team counterparts to develop solutions in an end-to-end framework on a group of core data technologies. Other aspects of the role include developing standards and processes for data engineering projects and cloud initiatives.
Responsibilities
Code, test, deploy, Orchestrate, monitor, document and troubleshoot cloud-based data engineering processing and associated automation in accordance with best practices and security standards throughout the development lifecycle
Work closely with data scientists, data architects, ETL developers, other IT counterparts, and business partners to identify, capture, collect, and format data from the external sources, internal systems and the data warehouse to extract features of interest
Significantly contribute to the evaluation, research, experimentation efforts with batch and streaming data engineering technologies to keep pace with industry innovation while assessing business impact and viability for use cases associated with efforts in hand
Work with data engineering related groups to inform on and showcase capabilities of emerging technologies and to enable the adoption of these new technologies and associated techniques
Significantly contribute to the definition and refinement of processes and procedures for the data engineering practice
Educate and develop ETL developers on data engineering cloud-bases initiatives so as to enable transition to data engineer and practice
Perform other duties as assigned
Conform with all company policies and procedures
Qualifications
Experience with processing large data sets using Hadoop, HDFS, Spark, Kafka, Flume or similar distributed systems
Experience with ingesting various source data formats such as JSON, Parquet, SequenceFile, Cloud Databases, MQ, Relational Databases such as Oracle
Experience with Cloud technologies (such as Azure, AWS, GCP) and native toolsets such as Azure ARM Templates, Hashicorp Terraform, AWS Cloud Formation
Understanding of cloud computing technologies, business drivers and emerging computing trends
Thorough understanding of Hybrid Cloud Computing: virtualization technologies, Infrastructure as a Service, Platform as a Service and Software as a Service Cloud delivery models and the current competitive landscape
Working knowledge of Object Storage technologies to include but not limited to Data Lake Storage Gen2, S3, Minio, Ceph, ADLS etc
Experience with containerization to include but not limited to Dockers, Kubernetes, Spark on Kubernetes, Spark Operator
Working knowledge of Agile development /SAFe, Scrum and Application Lifecycle Management
Strong background with source control management systems (GIT or Subversion); Build Systems (Maven, Gradle, Webpack); Code Quality (Sonar); Artifact Repository Managers (Artifactory), Continuous Integration/ Continuous Deployment (Azure DevOps)
Experience with NoSQL data stores such as CosmosDB, MongoDB, Cassandra, Redis, Riak or other technologies that embed NoSQL with search such as MarkLogic or Lily Enterprise
Creating and maintaining ETL processes
Knowledgeable of best practices in information technology governance and privacy compliance
Experience with Adobe solutions (ideally Adobe Experience Platform, DTM/Launch) and REST APIs
Skills
Troubleshoot complex problems and works across teams to meet commitments
Excellent computer skills and proficiency in digital data collection
Ability to work in an Agile/Scrum team environment
Strong interpersonal, verbal, and writing skills
Digital technology solutions (DMPs, CDPs, Tag Management Platforms, Cross-Device Tracking, SDKs, etc)
Knowledge of Real Time-CDP and Journey Analytics solutions
Understanding of big data platforms and architectures, data stream processing pipeline/platform, data lake and data lake houses
SQL experience: querying data and sharing what insights can be derived
Understanding of cloud solutions such as Google Cloud Platform, Microsoft Azure & Amazon AWS cloud architecture & services
Understanding of GDPR, privacy & security topics
Education
Bachelor’s Degree in related field or equivalent work experience required
Experience
5-7 years of hands-on experience with data engineering required
4-6 years of hands-on experience with processing large data sets required
4-6 years of hands-on experience with SQL, data modeling, relational databases and/or no SQL databases required
Contact:
mkozekwa@judge.com
This job and many more are available through The Judge Group. Find us on the web at www.judge.com
Show more
Show less","Hadoop, HDFS, Spark, Kafka, Flume, JSON, Parquet, SequenceFile, Cloud Databases, MQ, Oracle, Azure, AWS, GCP, Azure ARM Templates, Hashicorp Terraform, AWS Cloud Formation, Data Lake Storage Gen2, S3, Minio, Ceph, ADLS, Docker, Kubernetes, Spark on Kubernetes, Spark Operator, Agile, SAFe, Scrum, Application Lifecycle Management, GIT, Subversion, Maven, Gradle, Webpack, Sonar, Artifactory, Azure DevOps, CosmosDB, MongoDB, Cassandra, Redis, Riak, MarkLogic, Lily Enterprise, ETL, Adobe Experience Platform, DTM/Launch, REST APIs, DMPs, CDPs, Tag Management Platforms, CrossDevice Tracking, SDKs, Real TimeCDP, Journey Analytics, Data stream processing, Data lake, Data lake houses, SQL, Google Cloud Platform, Microsoft Azure, Amazon AWS, GDPR, Data modeling, Relational databases, No SQL databases","hadoop, hdfs, spark, kafka, flume, json, parquet, sequencefile, cloud databases, mq, oracle, azure, aws, gcp, azure arm templates, hashicorp terraform, aws cloud formation, data lake storage gen2, s3, minio, ceph, adls, docker, kubernetes, spark on kubernetes, spark operator, agile, safe, scrum, application lifecycle management, git, subversion, maven, gradle, webpack, sonar, artifactory, azure devops, cosmosdb, mongodb, cassandra, redis, riak, marklogic, lily enterprise, etl, adobe experience platform, dtmlaunch, rest apis, dmps, cdps, tag management platforms, crossdevice tracking, sdks, real timecdp, journey analytics, data stream processing, data lake, data lake houses, sql, google cloud platform, microsoft azure, amazon aws, gdpr, data modeling, relational databases, no sql databases","adls, adobe experience platform, agile, amazon aws, application lifecycle management, artifactory, aws, aws cloud formation, azure, azure arm templates, azure devops, cassandra, cdps, ceph, cloud databases, cosmosdb, crossdevice tracking, data lake, data lake houses, data lake storage gen2, data stream processing, datamodeling, dmps, docker, dtmlaunch, etl, flume, gcp, gdpr, git, google cloud platform, gradle, hadoop, hashicorp terraform, hdfs, journey analytics, json, kafka, kubernetes, lily enterprise, marklogic, maven, microsoft azure, minio, mongodb, mq, no sql databases, oracle, parquet, real timecdp, redis, relational databases, rest apis, riak, s3, safe, scrum, sdks, sequencefile, sonar, spark, spark on kubernetes, spark operator, sql, subversion, tag management platforms, webpack"
Big Data Developer - III (827754),The Judge Group,"Grand Prairie, TX",https://www.linkedin.com/jobs/view/big-data-developer-iii-827754-at-the-judge-group-3729749269,2023-12-17,Fort Worth,United States,Mid senior,Onsite,"Location:
Grand Prairie, TX
Description:
Our client is currently seeking a Big Data Developer - III
JOB TITLE: GCP Senior Big Data Engineer/Architect
Looking for a Senior Big Data Engineer/Architect on Google Cloud Platform to help strategize, architect and implement various solutions to migrate data hosted on our on-prem platform to Google cloud Platform (GCP).
The architect will design and implement enterprise infrastructure and platforms required for setting up data engineering pipelines utilizing the tools available on the GCP Platform.
As a GCP Platform Architect - You will work on Advanced Data Engineering products using Google Big Data technologies such as GCS, Data Proc, Airflow, Data Store and Big Query.
Very strong leadership and communication skills exhibiting right negotiating posture with customer and program teams to make the right decisions.
Experience leading one or more of the following areas of a Cloud transformation journey: strategy, design, application migration planning and implementation for any private and public cloud. Cloud foundation design and build/implement Cloud Transformation & Migration Cloud Managed service (IaaS and PaaS) Cloud foundation design and build/implement
MUST HAVE SKILLS (Most Important)
Google Cloud Certified Professional Cloud Architect Certification Bachelor?s degree with 3-5 years? experience on Google cloud with deep understanding, design and development experience with GCP products on Infrastructure, Data management, Application Development, Smart Analytics, Artificial Intelligence, Security and DevOps Extract, Transform and Load (ETL) & Big Data Tools: BigQuery, Cloud Dataflow, Cloud Proc, Cloud Pub/Sub, Cloud Composer, Google Data Studio, Google Cloud Storage. NoSQL databases: Cloud Bigtable, Cloud Fire store, Firebase Realtime Database, Cloud Memory store. Search Technologies: Lucene and Elasticsearch Relational Databases: Cloud Spanner, Cloud SQL DESIRED SKILLS: Strong knowledge on Google cloud storage Data lifecycle management Strong knowledge on BIGQuery Slots management Cost optimization for Dataproc workload management Experience of designing, building, and deploying production-level data pipelines using tools from Hadoop stack (HDFS, Hive, Spark, HBase, Kafka, NiFi, Oozie, Splunk etc). Development and deployment technologies (e.g. JIRA, GitHub, Jenkins, Nexus, Artifactory) Software development background with solid understanding of and experience in Software development life cycle (SDLC), DevOps, CI/CD. At least 2-year experience in architecting in enterprises using Agile methodologies - Experience in data visualization tools like Kibana, Grafana, Tableau and associated architectures. JOB DUTIES: Provide Subject Matter Expertise in cloud and hybrid-cloud computing with Google Cloud and related products; thereby becoming a trusted advisor to influential decision makers. Provide end-to-end technical guidance and expertise on how to effectively use Google Cloud to build solutions; creatively applying cloud infrastructure and platform services to help solve business problems; and communicating these approaches to different business users Design and implement Google solution architecture with the different products like Google App Engine, BigQuery, Kubernetes Engine, AutoML, assess architecture needs for projects, work with different development leads and managers to scope and craft proposals. Work to harvest best practices and document lessons learned as part of continuous improvement and aid in company-wide data governance. Periodically update senior management with the status of the project with excellent written and verbal communication skills. EDUCATION/CERTIFICATIONS: Bachelor?s degree with 3-5+ years of experience on Google cloud with deep understanding, design and development experience with GCP products on Infrastructure, Data management, Application Development, Smart Analytics, Artificial Intelligence, Security and DevOps?
Contact:
spandey02@judge.com
This job and many more are available through The Judge Group. Find us on the web at www.judge.com
Show more
Show less","Google Cloud Certified Professional Cloud Architect Certification, Google Cloud Platform, Extract Transform and Load (ETL), Big Data Tools: BigQuery Cloud Dataflow Cloud Proc Cloud Pub/Sub Cloud Composer Google Data Studio Google Cloud Storage., NoSQL databases: Cloud Bigtable Cloud Fire store Firebase Realtime Database Cloud Memory store., Search Technologies: Lucene and Elasticsearch, Relational Databases: Cloud Spanner Cloud SQL, Hadoop stack (HDFS Hive Spark HBase Kafka NiFi Oozie Splunk etc)., Software development life cycle (SDLC) DevOps CI/CD., Kibana Grafana Tableau","google cloud certified professional cloud architect certification, google cloud platform, extract transform and load etl, big data tools bigquery cloud dataflow cloud proc cloud pubsub cloud composer google data studio google cloud storage, nosql databases cloud bigtable cloud fire store firebase realtime database cloud memory store, search technologies lucene and elasticsearch, relational databases cloud spanner cloud sql, hadoop stack hdfs hive spark hbase kafka nifi oozie splunk etc, software development life cycle sdlc devops cicd, kibana grafana tableau","big data tools bigquery cloud dataflow cloud proc cloud pubsub cloud composer google data studio google cloud storage, extract transform and load etl, google cloud certified professional cloud architect certification, google cloud platform, hadoop stack hdfs hive spark hbase kafka nifi oozie splunk etc, kibana grafana tableau, nosql databases cloud bigtable cloud fire store firebase realtime database cloud memory store, relational databases cloud spanner cloud sql, search technologies lucene and elasticsearch, software development life cycle sdlc devops cicd"
"W2 Contract- Long Term Hybrid Contract- Data Engineer Position at Dalla/Fort Worth, TX.",Software Technology Inc.,"Fort Worth, TX",https://www.linkedin.com/jobs/view/w2-contract-long-term-hybrid-contract-data-engineer-position-at-dalla-fort-worth-tx-at-software-technology-inc-3659214344,2023-12-17,Fort Worth,United States,Mid senior,Onsite,"Hi All,
My name is Poornachandra Rao and I am a Technical Recruiter at STI Org. I am reaching out to you on an exciting job opportunity with one of our clients
Hybrid Role- 5 Days a Months
Position: Data Engineer
Location: Hybrid - Dalla/Fort Worth, TX
Duration: Long Term
Top 4 Requirements
Database SQL, Oracle or NoSQL
ETL Tools Informatica, DataStage etc.
Python/Spark/PySpark
AWS
Responsibilities
Deliver high performing scalable, flexible and cost-effective data solutions that conform to architectural designs and Fidelity technology strategy
Build and own a portfolio of policies, procedures and best practices to provide operational and engineering disciplines to evolving data protection & security technologies
Evaluate emerging technologies and market trends through research and POCs to improve and innovate on data protection & security technologies offerings
Own and continuously optimize tools, process and capabilities to support operational activities
Qualification
Strong technology background in two or more database technologies Relational (Oracle, SQL Server, MySQL), NoSQL (MongoDB, Cassandra, Graph DB) and Cloud Databases (Aurora, Dynamo, Elastic Cache etc)
Knowledge of Data Modelling Principles and solid understanding of database internals
Strong Python experience with building ETL workflows and data-driven solutions
Possess the ability to employ design patterns and generalize code to address common use cases. Capable of authoring high quality & reusable code to contribute to broader repositories
Expertise in one or more of cloud computing platform (AWS or Azure)
Experience using tools for infrastructure as a code (e.g., Terraform, Docker, CloudFormation, etc.) and automation workflows (i.e. GitLab, Jenkins, Artifactory, CI/CD)
7+ years in Technology with 3 years in developing and deploying data solutions
Note : Need genuine consultant who can give Code assessment and last 5 Years of w2 employment details once get selected
Warm Regards,
B. Poorna Chandra Rao,
Technical Recruiter,
Software Technology Inc (STI).
Email:
poornab@stiorg.com
Phone no: 609 447 3342.
www.stiorg.com
Show more
Show less","Data Engineer, SQL, Oracle, NoSQL, Informatica, DataStage, Python, Spark, PySpark, AWS, Data Modelling Principles, ETL workflows, Terraform, Docker, CloudFormation, GitLab, Jenkins, Artifactory, CI/CD","data engineer, sql, oracle, nosql, informatica, datastage, python, spark, pyspark, aws, data modelling principles, etl workflows, terraform, docker, cloudformation, gitlab, jenkins, artifactory, cicd","artifactory, aws, cicd, cloudformation, data modelling principles, dataengineering, datastage, docker, etl workflows, gitlab, informatica, jenkins, nosql, oracle, python, spark, sql, terraform"
Senior Marketing Data Analyst - Remote,Signet Jewelers,"Irving, TX",https://www.linkedin.com/jobs/view/senior-marketing-data-analyst-remote-at-signet-jewelers-3771698451,2023-12-17,Fort Worth,United States,Mid senior,Remote,"We have many opportunities available on our other career site pages. Click here to link to our careers page!
Signet Jewelers is the world's largest retailer of diamond jewelry, operating more than 2,800 stores worldwide under the iconic brands: Kay Jewelers, Zales, Jared, H.Samuel, Ernest Jones, Peoples, Banter by Piercing Pagoda, Rocksbox, JamesAllen.com and Diamonds Direct. We are a people-first company and this core value is at the heart of everything we do, from empowering our valued team members, to collaborating with our customers, to fostering the communities in which we live and serve. People – and the love their actions inspire – are what drive us. We’re not only proud of the love we inspire outside our walls, we’re especially proud of the diversity, inclusion and equity we’re inspiring inside. There are dynamic career paths awaiting you – rewarding opportunities to impact the lives of others and inspire love. Join us!
POSITION SUMMARY
:
To provide Signet Marketing with visibility and insights into customer, marketing, and business health and across all banners. The Sr. Analyst accomplishes this by utilizing the power of analytics tools (SQL, Excel, Alteryx & Tableau) across the Signet Enterprise Data Lake, Marketing platforms (Meta, Google, Adobe, etc.), and 3rd party datasets to pull actionable insight out of disparate datasets. While an Analyst surfaces what happened, the Sr. Analyst sheds light into why it may have happened, the potential implications of what happened, and what we can do about it to have a material impact on the business. The Sr. Analyst can work both independently and in a collaborative environment with data scientists, data engineers, strategy, and marketing teams. He or she has a passion for analyzing complex datasets and converting them into insights that drive decisions that create value.
MAJOR RESPONSIBILITIES/ESSENTIAL FUNCTIONS
:
Build data extracts using SQL, Excel, Alteryx, Tableau, and data analysis skills to efficiently curate data sets from multiple data sources necessary to support Marketing Mix Modeling (MMM) data collection, QA, planning, and performance analysis.
Support the development and maintenance of recurring reporting and insight development related to MMM, marketing campaign performance, competitive intelligence, and other marketing team activities.
Design and build visualizations that present complex data in an easily digestible way, surfacing learnings that drive action. (e.g. Marketing campaign trends and anomalies, share-of-voice measurement, and audience performance dashboards)
Present findings/recommendations to all levels of management based on data and analysis of results. Communicate data findings to technical and non-technical partners.
Partner with business and marketing teams to understand the full context of ad hoc data/analysis requests. Align on scope, approach, and timeline in conjunction with management team.
Identify opportunities for process improvements and automation to standardize workflows and improve efficiency.
POSITION QUALIFICATIONS
:
Education Required
: Bachelor’s degree in related field
Required or Acceptable Job-Related Experience
: Previous agency analytics experience and Familiarity with marketing mix modeling in a Saas capacity
Years of Job-Related Experience Required
: 4+ years
Technical/Other Skills Required
:
Data manipulation and visualization experience via Tableau / Power BI, SQL, Advanced Excel
Strong knowledge of marketing platforms (Meta, Google, TradeDesk, etc.) and datasets supplied to MMM models, including understanding of important marketing tactics/objectives for business owners
Experience with marketing platform campaign data, web analytics tools (Adobe), and sales/financial data (Business Objects)
Familiarity with cross-channel marketing measurement, MMM, attribution and test design
Proven ability to derive actionable insights from data and present recommendations
Ability to estimate and communicate timelines
Ability to project manage multiple workstreams
Ability to work cross-functionally with many departments/disciplines throughout the company and with its partners
Intellectual and analytical curiosity
Willingness/ability to learn new concepts, tools, and processes quickly
BENEFITS & PERKS
:
Competitive healthcare, dental & vision insurance
401(k) matching after one year of employment
Generous time off + company holidays
Merchandise discount
Learning & Development programs
Much more!
The salary range for this opportunity is $70,000.00 – $85,000.00 Base pay offered may vary depending on geographic region, internal equity, job related knowledge, skills and experience, among other factors.
Show more
Show less","SQL, Excel, Alteryx, Tableau, Marketing mix modeling, Power BI, Meta, Google, TradeDesk, Adobe, Business Objects, Crosschannel marketing measurement, Attribution, Test design, Data visualization, Data analysis, Data manipulation, Project management, Communication, Problem solving, Critical thinking, Analytical skills, Intellectual curiosity, Ability to learn new concepts quickly","sql, excel, alteryx, tableau, marketing mix modeling, power bi, meta, google, tradedesk, adobe, business objects, crosschannel marketing measurement, attribution, test design, data visualization, data analysis, data manipulation, project management, communication, problem solving, critical thinking, analytical skills, intellectual curiosity, ability to learn new concepts quickly","ability to learn new concepts quickly, adobe, alteryx, analytical skills, attribution, business objects, communication, critical thinking, crosschannel marketing measurement, data manipulation, dataanalytics, excel, google, intellectual curiosity, marketing mix modeling, meta, powerbi, problem solving, project management, sql, tableau, test design, tradedesk, visualization"
Licensed Civil Engineer - Data Center (Remote),Olsson,"Fort Worth, TX",https://www.linkedin.com/jobs/view/licensed-civil-engineer-data-center-remote-at-olsson-3784202748,2023-12-17,Fort Worth,United States,Mid senior,Remote,"Company Description
We are Olsson, a team-based, purpose-driven engineering and design firm. Our solutions improve communities and our people make it possible.
Our most meaningful asset is our people, and we are dedicated to providing an environment where they can continue to learn, grow, and thrive. Our entrepreneurial spirit is what has allowed us — and will continue to allow us — to grow. The result? Inspired people, amazing designs, and projects with purpose.
Job Description
Olsson provides multidisciplinary design services for some of the largest and most forward-thinking and desirable companies in the world to work for. The large hyperscale data center campuses we design throughout the U.S. will give you the opportunity to work on some of the largest and most complex engineering-driven projects being built today. Our clients are relationship based and truly value the work we do for them, affording us the opportunity to contribute to society’s technological and connected community through the design of the critical infrastructure that is the foundation of these projects.
As a Licensed Civil Engineer on our Data Center Civil Team, you will be a part of the firm’s largest and most complex projects. You will serve as a project manager on some projects and lead design engineer on others. Prepare planning and design documents, process design calculations, and develop and maintain team and client standards. You may lead quality assurance/quality control and act as an advisor on complex projects. You will also coordinate with other Olsson teams, professional staff, technical staff, clients, and other consultants.
You may travel to job sites for observation and attend client meetings.
Olsson currently has one opportunity for an Experienced Engineer. This role offers flexible work options, including remote and hybrid opportunities, to accommodate diverse working preferences and promote work-life balance. Candidates can live in Lincoln, Omaha, Phoenix, Chandler, or Dallas-Fort Worth area and work remotely, or work out of any Olsson office location in these regions/areas.
Qualifications
You are passionate about:
Working collaboratively with others
Having ownership in the work you do
Using your talents to positively affect communities
Solving problems
Providing excellence in client service
You bring to the team:
Strong communication skills
Ability to contribute and work well on a team
Bachelor's Degree in civil engineering
At least 6 years of related civil engineering experience
Proficient in Civil 3D software
Must be a registered professional engineer
Additional Information
Olsson is a nationally recognized, employee-owned firm specializing in planning and design, engineering, field services, environmental, and technology. Founded in 1956 on the very mindset that drives us today, we’re here to improve communities by making them more sustainable, better connected, and more efficient. Simply put, we work to leave the world better than we found it.
As an Olsson employee, you’ll receive our traditional benefits package (health care, vision, dental, paid time off, etc.), plus you’ll:
Become an owner in the company after your first year through our Employee Stock Ownership Plan (ESOP)
Engage in work that has a positive impact in communities
Receive an excellent 401(k) match
Participate in a wellness program promoting balanced lifestyles
Benefit from a bonus system that rewards performance
Have the possibility for flexible work arrangements
Olsson is an EEO employer. We encourage qualified minority, female, veteran and disabled candidates to apply and be considered for open positions. We do not discriminate against any applicant for employment, or any employee because of race, color, religion, national origin, age, sex, sexual orientation, gender identity, gender, disability, age, or military status.
Show more
Show less","Civil Engineering, Civil 3D, Project Management, Design Engineering, Planning, Design Documents, Design Calculations, Team and Client Standards, Quality Assurance/Quality Control, Client Service, Communication, Teamwork, AutoCAD, Microsoft Office Suite","civil engineering, civil 3d, project management, design engineering, planning, design documents, design calculations, team and client standards, quality assurancequality control, client service, communication, teamwork, autocad, microsoft office suite","autocad, civil 3d, civil engineering, client service, communication, design calculations, design documents, design engineering, microsoft office suite, planning, project management, quality assurancequality control, team and client standards, teamwork"
Data Analyst,Aquent,Dallas-Fort Worth Metroplex,https://www.linkedin.com/jobs/view/data-analyst-at-aquent-3785840445,2023-12-17,Fort Worth,United States,Mid senior,Hybrid,"We are looking for an experienced Data Analyst join Aquent's team supporting a global automotive manufacturer’s Digital Products Department. We are seeking a passionate and highly motivated Analyst. The responsibilities of this role are to support, design, and co-create multiple projects within the Supply and fulfillment transformation program to modernize our supply & fulfillment operations and work to proactively identify areas of opportunity to support customers and stakeholders better.
What you’ll be doing:
Support the development and delivery of multiple digital transformation projects to transform the current Supply and fulfillment operations.
Utilize business insight and data analytics to ensure the projects deliver value for customers and stakeholders.
Work cross-functionally with other client departments to ensure products align with enterprise strategy.
Leverage new technologies to improve analytical capabilities to support operations.
What you bring
Bachelor's degree (or higher) or equivalent relevant experience (Analytics, Strategy and Project Management 2 years)
Advanced experience working with Tableau, Alteryx, Power BI, or other Business Intelligence Tools
Experience using SQL, Python, and/or R (beginner)
Experience working in large projects and utilizing scrum methodology
Excellent problem-solving skills and customer service orientation
Track record leveraging tech to automate and enhance business processes
Excellent problem-solving skills and customer service orientation
Excellent analytic, presentation, interpersonal and organization skill
Ability to perform in a fast paced, deadline and detail driven environment
Ability to use business intelligence tools to effectively analyze large datasets
Added Bonus If You Have:
Supply Chain Management Experience
MBA / CPA
Understanding of the automotive industry
Experience using SQL, Python, and/or R
Working with AQUENT provides you access to some pretty awesome benefits and employee perks:
Aquent Talent: https://aquenttalent.com/
Subsidized Health, Vision and Dental Benefits, and Access to Fidelity 401(k) with Matching! https://aquent.com/talent/talent-benefits
Resume / Portfolio Review + Interview Prep. You'll be set-up for success!
Free Continued Learning: https://aquent.com/talent/training
The target hiring compensation range for this role is the equivalent of $85,000-100,000 yearly DOE Compensation is based on several factors including, but not limited to education, relevant work experience, relevant certifications, and location.
Show more
Show less","Data Analyst, Business Intelligence, Tableau, Alteryx, Power BI, SQL, Python, R, Supply Chain Management, Scrum, Business Analytics, Machine Learning, Data Visualization, Data Mining, Data Warehousing, Cloud Computing, Big Data, Agile Development, Project Management, Communication, Teamwork, Problem Solving","data analyst, business intelligence, tableau, alteryx, power bi, sql, python, r, supply chain management, scrum, business analytics, machine learning, data visualization, data mining, data warehousing, cloud computing, big data, agile development, project management, communication, teamwork, problem solving","agile development, alteryx, big data, business analytics, business intelligence, cloud computing, communication, data mining, dataanalytics, datawarehouse, machine learning, powerbi, problem solving, project management, python, r, scrum, sql, supply chain management, tableau, teamwork, visualization"
Senior Cloud Data Engineer,BDO USA,"Fort Worth, TX",https://www.linkedin.com/jobs/view/senior-cloud-data-engineer-at-bdo-usa-3765474001,2023-12-17,Fort Worth,United States,Mid senior,Hybrid,"Job Description
Job Summary:
This position will work with cutting edge technology, deliver high quality solutions across various industries, and oversee team(s) on engagements that range in size and scope. This position will receive continuous career development opportunities, given the size and potential of client engagements. This role will perform hands-on delivery of data analytics projects, contributing to the development and unit testing of solutions.
Job Duties
Designs and implements best in class data ingestion strategies, data warehouse and data mart structures, semantic layers and models, visualizations, streaming processes, API integrations, and automation (RPA) solutions for end-to-end data analytics solutions on primarily, but not limited to, cloud analytics platforms such as Azure and AWS
Listens to client needs to align solution with business requirements and delivery schedule
Creates written functional and technical designs
Participates in project status and stand meetings, and assists with providing aggregated project status for project and program managers
Assists with SLA compliance of solutions, and performs performance tuning and optimization efforts of end-to-end solutions
Writes code using multiple languages and correctly applies frameworks, architectural patterns, and software development principles
Delivers high-performance, scalable, repeatable, and secure deliverables with broad impact (high throughput and low latency)
Assists with implementation of data governance programs and best practices
Performs the cleaning and transforming of data from source systems into analytics models
Implements models to support data visualizations and integrations
Assists with implementing DevOps, DataOps and MLOps methodologies on projects
Writes custom integration logic in applicable programming languages
Assists project managers with work breakdown structure creation, project estimation, resource staffing, workload planning and adjustments throughout the project lifecycle
Assists clients with licensing, security, and cost estimation of solutions
Performs code reviews to ensure adherence to standards
Works directly with clients and team members to establish secure data analytics platforms and infrastructure
Contributes to successful deployments of developed solutions and integration of DevOps tools
Maintains a broad and current understanding of data analytics and business intelligence strategies, cloud platforms, methodologies, and tools
Builds client relationships during project execution, effectively becoming a trusted advisor of the client
Participates in support activities for existing software solutions
Other duties as assigned
Supervisory Responsibilities
Supervises the day-to-day workload of Associates on assigned engagements to ensure that timelines and deliverables are met, and reviews work product
Education
Qualifications, Knowledge, Skills and Abilities:
High School Diploma or GED equivalent, required
Bachelor’s degree, preferred; focus in Information Systems, Data Science or Computer Science, preferred
Experience
Five (5) or more years of experience within Data Analytics, Business Intelligence, Artificial Intelligence, or Application Development, required
One (1) or more years of experience technically leading development projects, preferred
One (1) or more years of consulting experience or implementation of cloud-based data analytics solutions, preferred
Software
Strong SQL skills including Data Definition Language (DDL), Data Manipulation Language (DML), views, functions, stored procedures, or performance tuning, required
Experience with Data Warehousing, Data Modeling, Semantic Model Definition or Star Schema Construction, required
Hands on delivery experience of end-to-end cloud data analytics solutions within Azure or AWS, preferred
Experience with one (1) or more of the following computer languages, preferred:
C#
Python
Java
Scala
Experience with tabular modeling within Microsoft Fabric, Power BI, or Azure Analysis Services, preferred
Experience with Git and DevOps deployment technologies, preferred
Experience with Linux, preferred
Experience with one (1) or more of the following, preferred:
Data Lake Medallion Architecture
Batch and/or streaming data ingestion into a data lake
AI Algorithms/Machine Learning
Automation tools such as UiPath, Alteryx, etc.
Computer Vision based AI technologies
Other Knowledge, Skills & Abilities
Ability to work with a high degree of professionalism and autonomy
Excellent verbal and written communication skills
Solid organizational skills, especially the ability to meet project deadlines with a focus on details
Ability to successfully multi-task while working independently or within a group environment
Ability to work in a deadline-driven environment, and handle multiple projects simultaneously
Ability to interact effectively with people at all organizational levels of the Firm
Ability to effectively interact with a team of professionals and delegating work assignments, as needed
Ability to build and maintain strong relationships with internal and client personnel
Ability to encourage a team environment on engagements, and contribute to the professional development of assigned personnel
Keywords:
Data Analytics, Business Intelligence, BI, Synapse, IoT, Machine Learning, Data Lake, Stream, Cube, Microsoft, SQL Server, Tableau, .Net, C#, Qlik, Power BI, Machine Learning, Azure Data Factory, RedShift, UiPath, Cloud, RPA, AWS, Redshift, Kinesis, QuickSight, SageMaker, S3, Databricks, AWS Lake Formation, Snowflake, Python, Qlik, Athena, Data Pipeline, Glue, Star Schema, Data Modeling, SQL, SSIS, SSAS, SSRS, PySpark, Microsoft Fabric, dbt, Linux, Terraform, Bicep, Data Ops, Purview, Git, Delta, Pandas, Spark SQL
Individual salaries that are offered to a candidate are determined after consideration of numerous factors including but not limited to the candidate’s qualifications, experience, skills, and geography.
California Range: $111,000 - $152,000
Colorado Range: $111,000 - $152,000
New York City/ Valhalla Range: $111,000 - $152,000
Washington Range: $111,000 - $152,000
About Us
BDO delivers assurance, tax, digital technology solutions and financial advisory services to clients throughout the country and around the globe. We offer numerous industry-specific practices, world-class resources, and an unparalleled commitment to meeting our clients’ needs. We currently serve more than 400 publicly traded domestic and international clients.
Unparalleled partner-involvement
Deep industry knowledge and participation
Geographic coverage across the U.S.
Cohesive global network
Focused capabilities across disciplines
BDO brings world-class resources and exceptional service to each and every one of our clients. BDO USA is a member of BDO International, the world’s fifth largest accounting network.
BDO offers a competitive Total Rewards package that encompass so much more than – “traditional benefits”. Our wide range of rewards and our employees’ ability to customize rewards to their individual needs are two of the reasons why BDO has been honored with so many workplace awards, including 100 Best Companies for Working Parents, Working Mother 100 Best Companies, Top Entry Level Employer, 2022 National Best & Brightest Companies to Work For and more.
Some Examples Of Our Total Rewards Offerings Include
Competitive pay and eligibility for an annual performance bonus.
A 401k plan plus an employer match
Comprehensive, medical, dental, vision, FSA, and prescription insurance from day one
Competitive Paid Time Off with daily accrual from day one of employment, plus paid holidays
Paid Parental Leave
Adoption Assistance
Firm paid life insurance
Wellness programs
Additional offerings include BDO Flex, Group Legal insurance, Pet insurance and Long-Term Care Insurance
Above offerings may be subject to eligibility requirements.
Click here to find out more!
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability or protected veteran status.
""BDO USA, P.A. is an EO employer M/F/Veteran/Disability""
Show more
Show less","Data Analytics, Business Intelligence, Artificial Intelligence, Application Development, Data Warehousing, Data Modeling, Semantic Model Definition, Star Schema Construction, Cloud Data Analytics, SQL, Data Definition Language (DDL), Data Manipulation Language (DML), Views, Functions, Stored Procedures, Performance Tuning, Azure, AWS, C#, Python, Java, Scala, Tabular Modeling, Microsoft Fabric, Power BI, Azure Analysis Services, Git, DevOps, Linux, Data Lake Medallion Architecture, Batch Data Ingestion, Streaming Data Ingestion, Data Lake, AI Algorithms, Machine Learning, Automation Tools, UiPath, Alteryx, Computer Vision, Professionalism, Autonomy, Verbal Communication, Written Communication, Organizational Skills, Project Deadlines, MultiTasking, Teamwork, DeadlineDriven Environment, Multiple Projects, Interpersonal Skills, Team Environment, Professional Development, Relationship Building","data analytics, business intelligence, artificial intelligence, application development, data warehousing, data modeling, semantic model definition, star schema construction, cloud data analytics, sql, data definition language ddl, data manipulation language dml, views, functions, stored procedures, performance tuning, azure, aws, c, python, java, scala, tabular modeling, microsoft fabric, power bi, azure analysis services, git, devops, linux, data lake medallion architecture, batch data ingestion, streaming data ingestion, data lake, ai algorithms, machine learning, automation tools, uipath, alteryx, computer vision, professionalism, autonomy, verbal communication, written communication, organizational skills, project deadlines, multitasking, teamwork, deadlinedriven environment, multiple projects, interpersonal skills, team environment, professional development, relationship building","ai algorithms, alteryx, application development, artificial intelligence, automation tools, autonomy, aws, azure, azure analysis services, batch data ingestion, business intelligence, c, cloud data analytics, computer vision, data definition language ddl, data lake, data lake medallion architecture, data manipulation language dml, dataanalytics, datamodeling, datawarehouse, deadlinedriven environment, devops, functions, git, interpersonal skills, java, linux, machine learning, microsoft fabric, multiple projects, multitasking, organizational skills, performance tuning, powerbi, professional development, professionalism, project deadlines, python, relationship building, scala, semantic model definition, sql, star schema construction, stored procedures, streaming data ingestion, tabular modeling, team environment, teamwork, uipath, verbal communication, views, written communication"
Cloud Big Data Engineer Lead,Elevance Health,"Grand Prairie, TX",https://www.linkedin.com/jobs/view/cloud-big-data-engineer-lead-at-elevance-health-3780068084,2023-12-17,Fort Worth,United States,Mid senior,Hybrid,"Description
Cloud Big Data Engineer Lead
Location:
This position will work a hybrid model (remote and office). Ideal candidates will live in the state of Georgia or within 50 miles of one of our Pulse Point locations.
Preferred Location: Atlanta, GA.
The
Cloud Big Data Engineer Lead
is responsible for full delivery of end to end system development and maintenance on medium Enterprise wide technology platforms. A proud member of the Elevance Health family of companies, CarelonRx (formerly IngenioRx) leverages the power of new technologies and a strong, clinical first lens, to deliver member centered, lasting pharmacy care.
How You Will Make An Impact
Maintains active relationships with customers to determine business requirements and leads requirements gathering meetings.
Leads requirements gathering meetings and reviews designs with the business.
Leads efforts with Web and marketing team to increase the presence of web products.
May implement improvements in stability, performance, and scalability across major business-critical systems.
May implement process to reduce barriers and roadblocks in projects, services, and processes in order to operate more efficiently.
Owns the change request process and coordinates with other teams as necessary.
Develops and owns list of final enhancements.
Develops and defines application scope and objectives and supervises the preparation of technical and/or functional specifications from with programs will be written.
Performs technical design reviews and code reviews.
Ensures unit test is completed and meets the test plan requirements, system testing is completed and system is implemented according to plan.
Responsible for delivery of application technology solutions and data information planning effort.
Coordinates and manages on-call support and owns the system monitoring process.
Owns the technical development environment and works on the Enterprise team.
Leads multiple or large projects and facilitates large group JAD sessions for requirements, modeling in several disciplines.
Leads vendor evaluation and analysis.
Minimum Qualifications
Requires an BA/BS degree in Information Technology, Computer Science or related field of study and a minimum of 7 years related experience; multi platform, multi-dimensional experience, and expert level experience with business and technical applications; or any combination of education and experience, which would provide an equivalent background.
Preferred Skills, Capabilities And Experiences
Experience mentoring others and provide troubleshooting support strongly preferred.
Multi database and/or multi language strongly preferred.
Please be advised that Elevance Health only accepts resumes for compensation from agencies that have a signed agreement with Elevance Health. Any unsolicited resumes, including those submitted to hiring managers, are deemed to be the property of Elevance Health.
Who We Are
Elevance Health is a health company dedicated to improving lives and communities – and making healthcare simpler. We are a Fortune 25 company with a longstanding history in the healthcare industry, looking for leaders at all levels of the organization who are passionate about making an impact on our members and the communities we serve.
How We Work
At Elevance Health, we are creating a culture that is designed to advance our strategy but will also lead to personal and professional growth for our associates. Our values and behaviors are the root of our culture. They are how we achieve our strategy, power our business outcomes and drive our shared success - for our consumers, our associates, our communities and our business.
We offer a range of market-competitive total rewards that include merit increases, paid holidays, Paid Time Off, and incentive bonus programs (unless covered by a collective bargaining agreement), medical, dental, vision, short and long term disability benefits, 401(k) +match, stock purchase plan, life insurance, wellness programs and financial education resources, to name a few.
Elevance Health operates in a Hybrid Workforce Strategy. Unless specified as primarily virtual by the hiring manager, associates are required to work at an Elevance Health location at least once per week, and potentially several times per week. Specific requirements and expectations for time onsite will be discussed as part of the hiring process. Candidates must reside within 50 miles or 1-hour commute each way of a relevant Elevance Health location.
The health of our associates and communities is a top priority for Elevance Health. We require all new candidates in certain patient/member-facing roles to become vaccinated against COVID-19. If you are not vaccinated, your offer will be rescinded unless you provide an acceptable explanation. Elevance Health will also follow all relevant federal, state and local laws.
Elevance Health is an Equal Employment Opportunity employer and all qualified applicants will receive consideration for employment without regard to age, citizenship status, color, creed, disability, ethnicity, genetic information, gender (including gender identity and gender expression), marital status, national origin, race, religion, sex, sexual orientation, veteran status or any other status or condition protected by applicable federal, state, or local laws. Applicants who require accommodation to participate in the job application process may contact elevancehealthjobssupport@elevancehealth.com for assistance.
Show more
Show less","Information Technology, Computer Science, Multi platform experience, Multidimensional experience, Business applications, Technical applications, Web products, Stability, Performance, Scalability, Process improvement, Change request process, Application scope definition, Technical design review, Code review, Unit testing, System testing, Application technology solutions, Data information planning, Vendor evaluation, Multidatabase, Multilanguage","information technology, computer science, multi platform experience, multidimensional experience, business applications, technical applications, web products, stability, performance, scalability, process improvement, change request process, application scope definition, technical design review, code review, unit testing, system testing, application technology solutions, data information planning, vendor evaluation, multidatabase, multilanguage","application scope definition, application technology solutions, business applications, change request process, code review, computer science, data information planning, information technology, multi platform experience, multidatabase, multidimensional experience, multilanguage, performance, process improvement, scalability, stability, system testing, technical applications, technical design review, unit testing, vendor evaluation, web products"
Data Analyst III,Trinus Corporation,"Chicago, IL",https://www.linkedin.com/jobs/view/data-analyst-iii-at-trinus-corporation-3761240050,2023-12-17,La Grange,United States,Associate,Onsite,"Description
Senior Data Analyst -
Responsibilities
Part of a cross-disciplinary team, working closely with other data analysts, product managers, data scientists, data engineers, software engineers, data managers and business partners.
Provide actionable, data-driven business and product insights by combining deep statistical skills, data manipulation capabilities and business acumen.
Define, instrument and maintain metrics and build dashboards.
Proactively identify impactful opportunities and autonomously execute data analysis.
Be a bridge between business and digital organizations, combining in-depth understanding of both areas.
If needed, build and maintain data pipelines to unblock your analysis.
Adhere to and advocate for data analytics best practices
Present results to peers and senior management, influencing decision making
Mentor others.
Qualifications
Essential
MSc in a quantitative field, preferably statistics.
Hands-on experience (typically 5+ years) carrying out data analytics, data mining and product analytics in complex, fast-paced environments.
Applied knowledge of data analytics and data pipelining tools and approaches across all data lifecycle stages.
Thorough understanding of underlying mathematical foundations of statistics, including knowledge of experimental design and analysis and causal inference from observational data.
Expert SQL knowledge
Advanced scripting experience in R or python.
Ability to write and maintain moderately complex data pipelines
Strong business acumen.
Customer-centric and pragmatic mindset. Focus on value delivery and swift execution, while maintaining attention to detail.
Strong communication and stakeholder management skills. Ability to lead large organizations through influence.
Continuous learning and improvement mindset.
Desired
Advanced analytics degree
Experience with big data technologies (e.g. Hadoop, Hive, and Spark) is a plus.
No prior experience in the energy industry required.
Additional Details
What attributes would make this candidate stand out? : NA
Additional Requirements : Exploratory data analysis, using scripting to provide business insights. Must be good at statistics, python and SQL. Finance background useful.
Job Posting Type : Unique
Indicate Where Contractor Will Carry Out Work : On Shore Only
Working Pattern : Non-rotational|N.A
Trinus Corporation, a leading provider of technology solutions and services with over 25 years of experience, is a certified WBE/MBE/SBE/SDB firm accredited by WBENC, NMSDC, and SBA.
Our mission is to shape the future of work by aligning the right mix of people, process, technology, and innovation to efficiently meet our clients' business objectives.
At Trinus, we understand that finding the right opportunity is pivotal in your career journey. Our staffing services go beyond mere placements; they are about matching your skills and aspirations with the perfect fit.
To learn more about us, please visit our website www.trinus.com
Show more
Show less","Data Analytics, Data Mining, Product Analytics, Statistics, SQL, R, Python, Data Pipelining, Hadoop, Hive, Spark, Exploratory Data Analysis, Business Acumen, Communication, Stakeholder Management, Leadership, Continuous Learning","data analytics, data mining, product analytics, statistics, sql, r, python, data pipelining, hadoop, hive, spark, exploratory data analysis, business acumen, communication, stakeholder management, leadership, continuous learning","business acumen, communication, continuous learning, data mining, dataanalytics, datapipeline, exploratory data analysis, hadoop, hive, leadership, product analytics, python, r, spark, sql, stakeholder management, statistics"
"Urgent Hiring for the role Business Data Analyst (Power BI) in Chicago, IL",Intellectt Inc,"Chicago, IL",https://www.linkedin.com/jobs/view/urgent-hiring-for-the-role-business-data-analyst-power-bi-in-chicago-il-at-intellectt-inc-3718988042,2023-12-17,La Grange,United States,Associate,Onsite,"Hello, this is
Kiran
from Intellect Inc. One of our Medical clients is hiring for the Role
Business Data Analyst (Power BI)/ Data Visualization Analyst (Power BI).
If you are interested then please share me your Updated Resume to my mail
kiran.o@intellectt.com
or you can reach me at (732)-398-6563.
Role: Business Data Analyst (Power BI)/ Data Visualization Analyst (Power BI)
Location: Chicago, IL
Duration: 4 Months on W2
Shift Timings
9:00 AM to 5:00 PM
100% Onsite role
We are seeking a temporary
Power BI Data Visualization Specialist
to assist our team in creating customized dashboards using Microsoft Power BI. As the Data Visualization analyst, you will work closely with our demand forecasting team to design and build visualizations, reports, and dashboards that provide insights into our data, trends, forecast changes, variance to benchmarks and other KPIs.
Responsibilities
Collaborate with internal stakeholders to identify key metrics and data sources for the dashboard
Build and design customized dashboards using Microsoft Power BI
Develop visualizations and reports that effectively communicate insights from the data
Integrate data from various sources and ensure the accuracy of the data
Troubleshoot and resolve issues with the dashboard as needed
Test and validate the dashboard to ensure it meets user requirements
Provide training and support to end-users on how to use and interpret the dashboard
Communicate progress and issues to the project manager and stakeholders
Ensure adherence to project timelines and deadlines
Requirements
Bachelor's degree in Computer Science, Information Systems, or related field
Strong experience building customized dashboards using Microsoft Power BI
Proficiency with DAX, SQL, and other data manipulation languages welcome
Strong Excel capabilities, including pivot tables and advanced functions
Experience with Alteryx or other ETL tools a plus
Experience integrating data from various sources and ensuring data accuracy
Ability to effectively communicate complex data insights to both technical and non-technical stakeholders
Excellent analytical and problem-solving skills
Strong attention to detail and ability to deliver quality work on time
Ability to work independently and in a team environment
Excellent time management skills
Thanks & Regards
Kiran Kumar | Senior Recruiter
Email:
kiran.o@intellectt.com
Desk Number:
732-412-6999
Ext
259
Direct Number:
732-398-6563
Website: www.intellectt.com
Address:
517 Route 1 South, Suite 1115 Iselin, NJ 08830
In my absence, you may contact Kowshik | Lead |
732-627-1210 | kowshik@intellectt.com
Show more
Show less","Power BI, DAX, SQL, Data manipulation languages, Excel, Pivot tables, Alteryx, ETL tools, Data integration, Data accuracy, Analytical skills, Problemsolving skills, Attention to detail, Time management skills, Teamwork skills, Communication skills","power bi, dax, sql, data manipulation languages, excel, pivot tables, alteryx, etl tools, data integration, data accuracy, analytical skills, problemsolving skills, attention to detail, time management skills, teamwork skills, communication skills","alteryx, analytical skills, attention to detail, communication skills, data accuracy, data integration, data manipulation languages, dax, etl tools, excel, pivot tables, powerbi, problemsolving skills, sql, teamwork skills, time management skills"
Senior Data Analyst (onsite),Bronson Technical Search,"Chicago, IL",https://www.linkedin.com/jobs/view/senior-data-analyst-onsite-at-bronson-technical-search-3758429951,2023-12-17,La Grange,United States,Associate,Onsite,"Job Title:
Senior Data Analyst (onsite)
Start Date:
As soon as possible
Duration:
Full-Time
Location:
Chicago, IL
Job Type:
Full-Time, Onsite, (No Visa Sponsorship Available)
We are seeking a Mid-Level Senior Data Analyst for our Philanthropic Analytics client in Chicago, IL. This role involves collecting, standardizing, and analyzing primary data for benchmarking in the nonprofit sector.
Applicants Must Have The Following
A Bachelor's degree in a quantitative field, social science research, public policy, or related field.
At least 2 years of work experience with a minimum of 1 year in analyzing quantitative data.
Proficiency in MS Excel, Tableau, R, or similar statistical software.
Strong communication skills and the ability to work collaboratively within a team.
A robust work ethic and a meticulous, detail-oriented approach.
Job Description
The Senior Data Analyst will handle complex data collection, analysis, and visualization tasks to support our nonprofit clients.
Responsibilities Include
Extracting and analyzing both internal and external data.
Developing detailed reports and presentations using advanced data software.
Ensuring quality assurance of all data and analysis.
Effectively managing project support and meeting all deadlines.
Occasionally traveling out of town as required by the project needs.
Preferred candidates will demonstrate strong analytic skills, an ability to work independently, excellent reasoning abilities, and strong communication skills and ability to meeting tight deadlines. Flexible hours may be required to accommodate global clients.
For those passionate about making a positive impact through data analytics in the nonprofit sector, we encourage you to apply.
Show more
Show less","Data Analysis, Data Visualization, Data Collection, Data Standardization, Data Benchmarking, Quantitative Data Analysis, MS Excel, Tableau, R, Statistical Software, Project Management, Communication Skills, Team Collaboration, Work Ethic, Attention to Detail","data analysis, data visualization, data collection, data standardization, data benchmarking, quantitative data analysis, ms excel, tableau, r, statistical software, project management, communication skills, team collaboration, work ethic, attention to detail","attention to detail, communication skills, data benchmarking, data collection, data standardization, dataanalytics, ms excel, project management, quantitative data analysis, r, statistical software, tableau, team collaboration, visualization, work ethic"
Custodial Quality Data Analyst,JLL,"Chicago, IL",https://www.linkedin.com/jobs/view/custodial-quality-data-analyst-at-jll-3778684847,2023-12-17,La Grange,United States,Associate,Onsite,"Pay: $80,000-100,000
Shift: Monday through Friday 9-5
The Custodial Quality Data Analyst will assist with data collection (equipment & supplies), monitoring trends, and Corrigo work order hygiene. The ideal candidate will be a self-starter with minimal coaching, unconventionally curious, and an excellent communicator able to work with technical and non-technical people at various levels of the organization.
This is a hybrid position.
What This Job Involves
Support operational analysis and performance to ensure flawless execution against KPIs and other associated metrics
Initiate conversations with key stakeholders to understand business problems, situations and opportunities where data analytics and reporting can be utilized to improve decision making and operational results
Work closely with and support Account Leadership and Client’s (internal & external) in implementing reporting routines to ensure seamless and timely service delivery
Ensure adherence to JLL and Client branding for presentations
Coordinate production of Monthly and Quarterly operational reviews and KPI tracking
Promote and document continuous process improvement with timely update the baseline Operations Playbook that fits the needs of the account
Implement and support governance policies and processes for consistent and accurate source data
Develop and maintain positive client relationship
Conflict resolution and solution analysis
Equipment Management- support obtaining equipment information for pick ups and repairs, communication with equipment repair and rental vendors
Supply Management - Assist with data collection on supply usage and purchases in quality regions 1-6 while supply programs are being developed throughout the district
Contractor Transition - assist with validating supply removal, supply pick up, and installation of new dispensers during transition
Corrigo work order hygiene - assist with equipment and supply requests as well as creating and validating Corrigo work orders
Assist with vendor KPI performance management.
Leverage BI Solutions (data warehouse) in managing data storage.
Manage and transform high volumes of data.
Develop and maintain trend analyses.
Increase efficacy and efficiency of workflow processing
Sound like you? To apply you need to be:
College degree preferred; High School diploma or equivalent required.
2-3 years of relevant experience
Excellent Communications – written and verbal, ability to present data.
Proven ability to sift through data, perform analysis, identify critical information, and develop solution options is essential
Demonstrated ability to grow and learn new systems.
Organized, proactive, and customer focused
Solid data analysis and research skills and experience extracting, tracking, and organizing data from various databases for metrics, reports preferred
Technical knowledge of building systems, data center, UPS, electrical, mechanical, HVAC preferred
Advanced experience with MS Office suite, and CMMS databases, Visio, Sharepoint preferred
SQL and VBA code experience preferred
Show more
Show less","Data Analysis, Data Collection, Data Management, Data Storage, Data Visualization, Data Warehousing, Trend Analysis, KPI Tracking, MS Office Suite, CMMS Databases, Visio, Sharepoint, SQL, VBA, Data Extraction, Data organization, Data reporting, Problemsolving, Communication, Teamwork, Conflict Resolution","data analysis, data collection, data management, data storage, data visualization, data warehousing, trend analysis, kpi tracking, ms office suite, cmms databases, visio, sharepoint, sql, vba, data extraction, data organization, data reporting, problemsolving, communication, teamwork, conflict resolution","cmms databases, communication, conflict resolution, data collection, data extraction, data management, data organization, data reporting, data storage, dataanalytics, datawarehouse, kpi tracking, ms office suite, problemsolving, sharepoint, sql, teamwork, trend analysis, vba, visio, visualization"
Data Engineer Intern Summer 2024,CCC Intelligent Solutions,"Chicago, IL",https://www.linkedin.com/jobs/view/data-engineer-intern-summer-2024-at-ccc-intelligent-solutions-3738618803,2023-12-17,La Grange,United States,Associate,Onsite,"CCC Intelligent Solutions is a leading technology company helping to improve the insurance claims process for millions of people. Our award-winning SaaS platform connects more than 35,000 businesses, including insurance carriers, repair facilities, automakers, part suppliers, lenders, and others to streamline the process from start to finish.
Our advanced capabilities in AI, IoT, telematics, data, and analytics drive continual innovation across our platform, as we work to advance the multi-trillion-dollar P&C insurance economy’s digital transformation.
At CCC, our mission is to keep people’s lives moving forward when it matters most. Diversity of experience and perspective is key to our pursuit so we can deliver a future of possibilities for our customers.
The Role
We're looking for a Data Engineer to join us for the Summer 2024 semester. At CCC, you will work and learn alongside innovative and inspiring leaders and gain valuable experience working on real business solutions in a corporate setting.
Ideal candidate will be located in Chicago area.
Key
Responsibilities:
Build streaming and batch data pipelines using Python, Kafka, Spark, Cloud (and more) which will provide insights to CCC's clients within auto property damage and repair, medical claims, and telematics IoT industries.
Build and maintain complex SQL queries using Hive, Oracle, and SQL Server. Turn raw data into valuable insights using AI techniques.
Work closely with product owners, information engineers, data scientists, data modelers, infrastructure support, and data governance positions.
Requirements:
Pursuing a Masters Degree in Computer Science, Analytics, MIS, or some related field
About the company’s commitment to its employees:
CCC Intelligent Solutions employees are part of an inclusive culture that brings together diverse backgrounds and perspectives. Our team is defined by our values of: Integrity, Customer-Focus, Innovation, Diversity & Inclusion, and Tenacity. Together, we help our clients and each other achieve new goals.
CCC is committed to providing employees with opportunities to advance their careers and skillsets. CCC team members receive access to training and education reimbursement is available.
CCC offers competitive compensation and generous benefits. Health insurance, PTO, 401K, are just some of the benefits available to team members.
Each team member plays an important role in the company’s success and each team member has a voice. CCC employee engagement and job satisfaction ratings consistently exceed industry norms – underscoring the value CCC places on its employees.
Explore the Employee Experience at CCC.
Show more
Show less","Python, Kafka, Spark, Cloud, SQL, Hive, Oracle, SQL Server, AI, Computer Science, Analytics, MIS","python, kafka, spark, cloud, sql, hive, oracle, sql server, ai, computer science, analytics, mis","ai, analytics, cloud, computer science, hive, kafka, mis, oracle, python, spark, sql, sql server"
Data Analyst - Transaction Cards,WestRock Company,"Woodridge, IL",https://www.linkedin.com/jobs/view/data-analyst-transaction-cards-at-westrock-company-3779390765,2023-12-17,La Grange,United States,Associate,Onsite,"WestRock (NYSE: WRK) partners with our customers to provide differentiated paper and packaging solutions that help them win in the marketplace. WestRock’s 50,000 team members support customers around the world from more than 300 operating and business locations spanning North America, South America, Europe and Asia. Learn more at www.westrock.com.
THERE IS NO RELOCATION FOR THIS POSITION.
WestRock Woodridge.
Data Analyst The opportunity:
The Data Analyst will provide technical and functional support to our transaction card (Tcard) market through creating, transforming, and implementing solutions to specific business performance goals and needs. The Data Analyst is responsible for learning our customer’s goals and priorities to develop and transform data for production. The Data Analyst will have business acumen and will need working knowledge of general end to end Tcard processes and how processes are or can be enabled by technology.
How you will impact WestRock:
Ensure data integrity and program accuracy through quality assurance activities, troubleshooting, testing, and reporting of system maintenance. • Use processes and systems in place to transform and prepare data for production activities. • Utilize systems to maintain and update T-SQL database as necessary for production and reporting • Maintains confidentiality of data and programs • Work with WestRock’s Tcard business segment to develop business cases for technology solutions. • Gather and document business functional requirements for technology solutions. • Addresses internal data processing issues as required • Research and understand industry best practices in technical and functional business areas in order to recommend & enable business processes, driving improvements in workflow and increasing business productivity.
What you need to succeed:
Must have high school diploma or G.E.D. • 3-5 years of IT and/or business-related analyst/data experience • Experience facilitating requirements gathering sessions • Internal client-facing engagement/working relationships • Strong process and analytical skills • Defining and designing business processes and workflows across various stakeholders in the organization • Strong interpersonal skills, collaborative problem-solving skills, ability to listen to and understand needs across diverse stakeholders at various levels of the organization • Possess an understanding of T-SQL • Thorough understanding of business processes. • Proficient in documenting work flows • Exceptional written and verbal communication skills • Preferred • Bachelor’s degree in business, Information Systems, or related area of study; • Experience in manufacturing environment • Industry experience: Transaction cards (Tcards) What we offer: • Corporate culture based on integrity, respect, accountability and excellence • Comprehensive training with numerous learning and development opportunities • An attractive salary reflecting skills, competencies and potential
A career with a global packaging company where Sustainability, Safety and Inclusion are business drivers and foundational elements of the daily work WestRock Company is an Equal Opportunity Employer committed to creating and maintaining a diverse workforce: Minorities/Females/Disabled/Veterans
Show more
Show less","Data Analysis, TSQL, Business Acumen, Troubleshooting, Data Integrity, Program Accuracy, Systems Analysis, Business Case Development, Business Requirements Gathering, Business Process Definition, Workflow Design, Interpersonal Skills, Collaborative ProblemSolving, Communication Skills, Bachelor's Degree in Business or Information Systems, Manufacturing Environment Experience, Transaction Card Experience","data analysis, tsql, business acumen, troubleshooting, data integrity, program accuracy, systems analysis, business case development, business requirements gathering, business process definition, workflow design, interpersonal skills, collaborative problemsolving, communication skills, bachelors degree in business or information systems, manufacturing environment experience, transaction card experience","bachelors degree in business or information systems, business acumen, business case development, business process definition, business requirements gathering, collaborative problemsolving, communication skills, data integrity, dataanalytics, interpersonal skills, manufacturing environment experience, program accuracy, systems analysis, transaction card experience, troubleshooting, tsql, workflow design"
A&MPLIFY Data Science Analyst,Alvarez & Marsal,"Chicago, IL",https://www.linkedin.com/jobs/view/a-mplify-data-science-analyst-at-alvarez-marsal-3787301252,2023-12-17,La Grange,United States,Associate,Onsite,"Description
Data Science – Analyst Job Description
Summary
A&MPLIFY is a professional service consultancy combining the power of design, digital technology, and artificial intelligence to help clients supercharge value creation, agility and customer loyalty. The A&MPLIFY team is hand-picked from the top global consulting firms and Fortune 500 customer-focused companies. Individually, we are experience designers, commerce strategists, technologists, data scientists, and operations leaders with deep industry experience. Together we are the unique team clients need to drive growth and build relevance with their customers. With a commitment to positive social impact, we foster a vibrant, inclusive workplace of diverse voices and talents who enjoy what they do, and with whom they do it.
The Intelligence Analyst sits within our Data Science & Analytics community of practice. This is a hybrid role spanning remote, client, and in-office work locations and requires occasional travel.
Job Description
Work in a collaborative environment with teams across A&M and within Digital to drive clients’ Digital Transformation in a broad range of industries via scalable Analytics and AI / ML solutions which solve business problems and create value by leveraging client and external data.
Participate in client discussions, interact with client organization to articulate the value of data science approaches, different AI service offerings and guide them on implementation of the same.
Quickly understand client needs, scope of work effort, assemble teams, manage delivery, and articulate findings to client executives
Review and analyze developed models for complete, robustness and alignment to client needs
Qualifications
0-3 years professional work experience as a data scientist or on advanced analytics / statistics projects.
Strong mathematics / econometric / statistical skills, bachelor’s degree in related field
Ability to design databases and run mathematical models in the database to calculate relationship strengths
Excellent team-oriented and interpersonal skills, with a strong interest in consulting
Outstanding communication skills with the ability to clearly articulate findings and present solutions to business partners and customers
Ability to analyze information and identify second-order implications to policy, processes, communications and training
Experience with common data science tools such as Python, R, Pytorch, TensorFlow, Keras
Experience with BI tools (Tableau, Power BI, Looker)
Preferred Qualifications
Experience working in a consultative capacity with internal or external clients.
Experience with building and deploying predictive and prescriptive analytics model
Experience designing and implementing organizational or programmatic performance measurement strategies
Experience in building intelligent apps and custom analytics platforms
Experience working with big data distributed programming languages, and ecosystems: Spark, Hadoop, Snowflake, Kafka
Graph Theory work, potentially Neo4J, TigerGraph, Neptune
Good understanding of modeling platforms (Azure AutoML, SageMaker, DataBricks, DataRobot and H2O.ai)
Familiarity with Cloud-based environments such as AWS (S3/EC2), Azure, Google Cloud
Experience with Cognitive Analytics including NLP / NLG, Computer Vision, Sentiment analysis
Knowledge of web frameworks to build web applications (R Shiny, Dash)
Knowledge of other coding languages such as Java, Matlab, SAS, C++
Diversity & Inclusion
A&M’s entrepreneurial culture celebrates independent thinkers and doers who can positively impact our clients and shape our industry. The collaborative environment and engaging work—guided by A&M’s core values of Integrity, Quality, Objectivity, Fun, Personal Reward, and Inclusive Diversity—are the main reasons our people love working at A&M. Inclusive Diversity means we embrace diversity, and we foster inclusiveness, encouraging everyone to bring their whole self to work each day. It runs through how we recruit, develop employees, conduct business, support clients, and partner with vendors. It is the A&M way.
Equal Opportunity Employer
It is Alvarez & Marsal’s practice to provide and promote equal opportunity in employment, compensation, and other terms and conditions of employment without discrimination because of race, color, creed, religion, national origin, ancestry, citizenship status, sex or gender, gender identity or gender expression (including transgender status), sexual orientation, marital status, military service and veteran status, physical or mental disability, family medical history, genetic information or other protected medical condition, political affiliation, or any other characteristic protected by and in accordance with applicable laws. Employees and Applicants can find A&M policy statements and additional information by region here.
Unsolicited Resumes from Third-Party Recruiters
Please note that as per A&M policy, we do not accept unsolicited resumes from third-party recruiters unless such recruiters are engaged to provide candidates for a specified opening and in alignment with our Inclusive Diversity values. Any employment agency, person or entity that submits an unsolicited resume does so with the understanding that A&M will have the right to hire that applicant at its discretion without any fee owed to the submitting employment agency, person or entity.
Show more
Show less","Python, R, Pytorch, TensorFlow, Keras, Tableau, Power BI, Looker, Spark, Hadoop, Snowflake, Kafka, Neo4J, TigerGraph, Neptune, Azure AutoML, SageMaker, DataBricks, DataRobot, H2O.ai, AWS, Azure, Google Cloud, NLP, NLG, Computer Vision, Sentiment analysis, R Shiny, Dash, Java, Matlab, SAS, C++","python, r, pytorch, tensorflow, keras, tableau, power bi, looker, spark, hadoop, snowflake, kafka, neo4j, tigergraph, neptune, azure automl, sagemaker, databricks, datarobot, h2oai, aws, azure, google cloud, nlp, nlg, computer vision, sentiment analysis, r shiny, dash, java, matlab, sas, c","aws, azure, azure automl, c, computer vision, dash, databricks, datarobot, google cloud, h2oai, hadoop, java, kafka, keras, looker, matlab, neo4j, neptune, nlg, nlp, powerbi, python, pytorch, r, r shiny, sagemaker, sas, sentiment analysis, snowflake, spark, tableau, tensorflow, tigergraph"
Health & Benefits Data Analyst Intern,Aon,"Chicago, IL",https://www.linkedin.com/jobs/view/health-benefits-data-analyst-intern-at-aon-3735877862,2023-12-17,La Grange,United States,Associate,Onsite,"Aon is looking for Health & Benefits Data Analyst Interns in:
Illinois: Chicago, Lincolnshire
Georgia: Atlanta
Colorado: Denver
Texas: Dallas, Austin
Are you intellectually curious with a passion for solving problems? Do you enjoy a flexible and collaborative, team-based environment where you are always learning and growing? If yes, Aon is the place for you!
This is a hybrid role with the flexibility to work both from home and your aligned office.
Aon is in the business of better decisions
At Aon, we shape decisions for the better to protect and enrich the lives of people around the world.
As an organization, we are united through trust as one inclusive, diverse team, and we are passionate about helping our colleagues and clients succeed.
What the day will look like
As part of an industry-leading team, you will help empower results for our clients by delivering innovative and effective solutions.
This position is part of our broader Health Analytics Intelligence team, which falls under Aon's national Health Analytics practice; however, our team infuses with many of the other high-visibility Aon business units, as well.
Our team of individuals excel at assessing customers’ data-based needs and ensuring customers (internal and external) receive a high value return. The team is a mix of data managers, data scientists, reporting analysts, SQL/MS Office/Tableau/PowerBI/Python/R programming experts, statisticians, actuaries, and custom solution architects. Our group delivers or supports a wide array of reporting tools, special projects, and methodology development to address the complex analytic needs of our data-driven clients.
Your Impact As A Health Analytics Data Intern
Health And Benefits Consulting
Employer-sponsored health and welfare benefit programs form the foundation of the U.S. health care system. These programs have evolved over time as employers have attempted to handle the changing needs of employees, rising program costs, and the impact of new legislation while supporting their human resource and business strategies.
How this opportunity is different
You will have the opportunity to work on a variety of assignments during your internship. Health Analytics data analysts support consultants to develop short and longer-term health and welfare benefit strategies. This process involves strategic assessments of the health care market and key trends and the identification of new and innovative solutions to employer problems. The technical and analytical skills health & benefits data analysts bring to this process are critical to our health & benefits consulting.
Skills And Experience That Will Lead To Success
Collaborators, Critical Thinkers, Problem Solvers, Team Players, Effective Communicators, Relationship Builders and Future Leaders.
We aim to recruit a diverse group of professionals who offer varying experiences, backgrounds and strengths to ensure we can continue to offer the most innovative solutions to our clients. Additionally, we take into consideration the following:
Relevant work or internship experience
Involvement in extracurricular activities
Proficiency in Microsoft office products such as Word, PowerPoint, Excel and Outlook
Experience with SQL and scripting languages like R/Python
Experience with data visualization tools: Tableau/Power BI, ggplot2
Experience with health claims datasets
Outstanding quantitative and analytical skills
Strong written and verbal communication skills
Aon Summer Internship Program
Aon’s Summer Internship Program provides a unique opportunity for rising college seniors (and above) to learn, contribute, and gain real world work experience in a collaborative environment. Interns have the chance to learn more about Aon’s culture and businesses while developing practical skills necessary for future success and postgraduate opportunities. The 10-week summer program includes weekly spotlights delivered by Aon business leaders, a community outreach event and networking opportunities with fellow interns and colleagues. The program equips interns with an understanding of working in an environment dedicated to exceeding client expectations and delivering products and services distinct to unique client needs.
If you graduate between December 2024 and June 2025 with a GPA of 3.0+,this opportunity is for you!
A Value Adding Program Development Plan (Curriculum)
In addition to on-the-job training, our interns actively participate, along with their fellow intern colleagues, in a formal Development Plan. This provides interns:
An understanding of the basic functions and types of clients within Aon’s businesses
An understanding of Aon’s ethical standards
Training geared toward enabling interns to demonstrate an ability to work in a professional business environment
Opportunities to network with fellow peers, Aon colleagues and executive leadership at program social events and interactive speaker forums
Opportunities to learn and use Aon’s internal systems
Opportunities to demonstrate interpersonal and team building skills through many networking events
An Opportunity to Explore Post Graduation Career Path Opportunities
The goal of the Aon Summer Internship Program is to give students significant exposure to Aon through project deliverables, continuing education and a training curriculum. The program also provides students with an excellent opportunity to learn and prepare for post‐graduate career opportunities at Aon. Upon successful completion of the Aon Summer Internship Program, highly qualified interns may be offered post-graduation full‐time positions within the Launch Program, Aon’s Early Career Development Program.
How we support our colleagues
In addition to our comprehensive benefits package, we encourage a diverse workforce. Plus, our agile, inclusive environment allows you to manage your wellbeing and work/life balance, ensuring you can be your best self at Aon. Furthermore, all colleagues enjoy two “Global Wellbeing Days” each year, encouraging you to take time to focus on yourself. We offer a variety of working style solutions, but we also recognize that flexibility goes beyond just the place of work... and we are all for it. We call this Smart Working!
The target salary range for this position is $22.50 to $25.50 per hour. The actual salary will vary based on applicant’s education, experience, skills, abilities, as well as internal equity and alignment with market data. The salary may also be adjusted based on applicant’s geographic location.
Aon offers a benefits package for temporary and seasonal employees who will work 20 or more hours per week including: medical, dental, and vision insurance at the employee’s expense, critical illness insurance, 10 Company holidays per calendar year, and paid sick leave as provided under state and local law. Eligibility for benefits are governed by the applicable plan documents and policies.
Our continuous learning culture inspires and equips you to learn, share and grow, helping you achieve your fullest potential. As a result, at Aon, you are more connected, more relevant, and more valued.
Aon values an innovative, diverse workplace where all colleagues feel empowered to be their authentic selves. Aon is proud to be an equal opportunity workplace.
Aon provides equal employment opportunities to all employees and applicants for employment without regard to race, color, religion, creed, sex, sexual orientation, gender identity, national origin, age, disability, veteran, marital, domestic partner status, or other legally protected status.
We welcome applications from all and provide individuals with disabilities with reasonable adjustments to participate in the job application, interview process and to perform essential job functions once onboard. If you would like to learn more about the reasonable accommodations we provide, email ReasonableAccommodations@Aon.com
Nothing in this job description restricts management's right to assign or reassign duties and responsibilities to this job at any time.
#AonInternUS
2023-70765
Show more
Show less","SQL, R, Python, Tableau, Power BI, ggplot2, Microsoft Office Suite, Data visualization, Health claims, Quantitative analysis, Analytical skills, Communication skills, Problem solving, Critical thinking, Collaboration","sql, r, python, tableau, power bi, ggplot2, microsoft office suite, data visualization, health claims, quantitative analysis, analytical skills, communication skills, problem solving, critical thinking, collaboration","analytical skills, collaboration, communication skills, critical thinking, ggplot2, health claims, microsoft office suite, powerbi, problem solving, python, quantitative analysis, r, sql, tableau, visualization"
"Data Analyst, HR Operations",Alvarez & Marsal,"Chicago, IL",https://www.linkedin.com/jobs/view/data-analyst-hr-operations-at-alvarez-marsal-3770544401,2023-12-17,La Grange,United States,Associate,Onsite,"Description
Overview
Alvarez & Marsal (A&M) is a leading independent global professional services firm, specializing in providing turnaround management and performance improvement for corporations and government entities. With almost 9,000 professionals based in locations across North America, Europe, Asia, and Latin America, our firm excels in problem-solving and value creation. Drawing on a strong operational heritage and hands-on approach, our professionals work closely with organizations and stakeholders to help tackle complex issues.
A&M is regularly recognized by leading publications as a premier consulting firm, including placement in the top 25 of the “Vault Consulting 50” in each of the last ten years. A&M is seeking a highly organized team member to support our HR Operations Team. We are looking for someone with an eye for detail, who enjoys administrative work and creating and supporting efficiencies. The ideal candidate will be proactive and looking for a role in an entrepreneurial and complex organization with experience working in administration in a fast-paced office environment.
Main Roles & Responsibilities:
Accountable for the delivery of all HR integrity audits/metrics weekly, biweekly, monthly, quarterly and/or annually.
Delivers a range of analysis activities – data extraction, synthesis of data, and recommendations.
Analyzes trends and metrics in partnership with HR Operations team and HR Business Partner group to develop solutions, programs, and policies.
Creates persuasive presentations that tell a story using visual representations of data through graphs, charts and models.
Design new processes, identify and escalate gaps/risks, and provide recommendations for routine high-quality reporting.
Performs audits to determine defects, data anomalies, gaps or completeness and work with HR Business Partners to find a resolution.
Assists in ad hoc projects that require the analysis of data and metrics, and other assignments.
Compiles and maintains reports.
Other duties as assigned
Ideal candidate profile:
Associate or bachelor’s degree in business administration, Computer Science, Human Resources or other relevant fields.
1-2 years of relevant experience with handling large data set management
Experience in Tableau or PowerBi is preferred, but not required
Excellent communication skills, both oral and written.
Advanced computer skills using Microsoft Office Suite (PowerPoint, Outlook, Excel).
High ethical standards, integrity, and adherence to confidentiality.
Must be detail orientated.
Excellent time management: ability to handle and prioritize multiple tasks in a fast-paced environment.
Enjoys designing new processes and procedures.
Willingness to take initiative and ownership of projects.
Previous experience in a corporate environment.
Previous experience in Workday (HRIS) is a plus.
The salary range is $70,000
- $80,000
annually, dependent on several variables including but not limited to education, experience, skills, and geography. In addition, A&M offers a discretionary bonus program which is based on a number of factors, including individual and firm performance. Please ask your recruiter for details.
Diversity & Inclusion
A&M’s entrepreneurial culture celebrates independent thinkers and doers who can positively impact our clients and shape our industry. The collaborative environment and engaging work—guided by A&M’s core values of Integrity, Quality, Objectivity, Fun, Personal Reward, and Inclusive Diversity—are the main reasons our people love working at A&M. Inclusive Diversity means we embrace diversity, and we foster inclusiveness, encouraging everyone to bring their whole self to work each day. It runs through how we recruit, develop employees, conduct business, support clients, and partner with vendors. It is the A&M way.
Equal Opportunity Employer
It is Alvarez & Marsal’s practice to provide and promote equal opportunity in employment, compensation, and other terms and conditions of employment without discrimination because of race, color, creed, religion, national origin, ancestry, citizenship status, sex or gender, gender identity or gender expression (including transgender status), sexual orientation, marital status, military service and veteran status, physical or mental disability, family medical history, genetic information or other protected medical condition, political affiliation, or any other characteristic protected by and in accordance with applicable laws. Employees and Applicants can find A&M policy statements and additional information by region here.
Unsolicited Resumes from Third-Party Recruiters
Please note that as per A&M policy, we do not accept unsolicited resumes from third-party recruiters unless such recruiters are engaged to provide candidates for a specified opening and in alignment with our Inclusive Diversity values. Any employment agency, person or entity that submits an unsolicited resume does so with the understanding that A&M will have the right to hire that applicant at its discretion without any fee owed to the submitting employment agency, person or entity.
Show more
Show less","Tableau, PowerBI, Microsoft Office Suite, Workday, HRIS, Data Analysis, Data Extraction, Data Synthesis, Data Visualization, Process Design, Data Mining, Report Compilation, Communication, Presentation Skills, Time Management, Proactiveness, DetailOriented, Initiative, Ownership","tableau, powerbi, microsoft office suite, workday, hris, data analysis, data extraction, data synthesis, data visualization, process design, data mining, report compilation, communication, presentation skills, time management, proactiveness, detailoriented, initiative, ownership","communication, data extraction, data mining, data synthesis, dataanalytics, detailoriented, hris, initiative, microsoft office suite, ownership, powerbi, presentation skills, proactiveness, process design, report compilation, tableau, time management, visualization, workday"
Sr. Staff Data Engineer(Remote),Stryker,"Chicago, IL",https://www.linkedin.com/jobs/view/sr-staff-data-engineer-remote-at-stryker-3764793854,2023-12-17,La Grange,United States,Associate,Remote,"Why engineering at Stryker?
At Stryker we are dedicated to improving lives, with a passion for researching and developing new medical device products. As an engineer at Stryker, you will be proud of the work that you will be doing, using cutting-edge technologies to make healthcare better. Here, you will work in a supportive culture with other incredibly talented and intelligent people, creating industry-leading medical technology products. You will also have growth opportunities as we have a culture that supports your personal and professional development.
Need another reason to apply?
Check out these 8 reasons to join Stryker's engineering team:
https://www.strykercareersblog.com/post/8-reasons-to-join-strykers-engineering-team
We are proud to be named one of the World’s Best Workplaces and a Best Workplace for Diversity by Fortune Magazine! Learn more about our award-winning organization by visiting stryker.com
Position Summary
The Senior Staff Data Engineer is responsible for supporting and advancing data intelligence projects by building and managing data ingestion, transformation, and storage to deliver accurate and reliable data to stakeholders. This position will be part of the Data Engineering team and will bring strong quantitative skills to the team. In this position, you will be engaged on projects that will ultimately use the data for various analytics techniques, such as optimization, forecasting, machine learning, predictive maintenance, visualization, and statistical analysis to develop solutions that help deliver significant value to customers.
Who We Need
Analytical problem solvers. People who go beyond just fixing to identify root causes, evaluate optimal solutions, and recommend comprehensive upgrades to prevent future issues.
Dedicated achievers. Relentless about quality, people who thrive in a fast-paced environment and will stop at nothing to ensure a project is complete and meets regulations and expectations.
Goal-oriented developers. Keeping the customer and system requirements squarely in focus, people who deliver safe and robust solutions.
What You Will Do
Technical Skills
Skilled and proficient in data engineering languages including Python and SQL
Experience building data pipelines and data storage solutions in a cloud environment
Proficient working knowledge of native cloud-based tools on Microsoft Azure or AWS, including SQL/NoSQL databases, data lakes & blob storage as well as various cloud-based compute solutions such as Databricks or Snowflake and Kubernetes/Docker
Skilled in data modeling, including relational and dimensional modeling, and storage of structured, semi-structured, and unstructured data
Experience with various data types (structured, semi-structured, unstructured) and analytics needs from reporting to machine learning
Expert/Master in defining and developing robust ETL pipelines and data orchestration solutions
Expert in using version control (Gitlab preferred)
Identify and use the most appropriate tool for particular project or use case
Continuously seek out industry best practices and develop skills to create new capabilities for data engineering
Develop talent and provide project leadership in the team's day-to-day project portfolio. Experience with scrum and agile frameworks expected
Provide a standard common business vocabulary, express strategic requirements, outline high-level integrated designs to meet those requirements, and align with enterprise strategy and related business architecture
Familiar with data security & data privacy best practices
Business Skills
Demonstrate financial acumen to understand financial impact for existing projects
Translate stakeholder requirements to data solutions independently
Lead discussions with Stryker enterprise across functions to leverage domain expertise and capabilities
Lead and deliver presentations and communications that build data engineering credibility and rapport to a medium size group with some guidance
Provides helpful insights in discussions on identifying opportunities where data engineering can be applied to real-world business opportunities with key stakeholder
General Skills:
Ability to manage individual tasks independently and provide leadership for owned projects
Experience leading end-to-end data engineering projects: from problem and requirements definition to model/algorithm validation and deployment
Work cooperatively with all stakeholders to ensure project success
Apply a strong understanding of Agile/Scrum procedures to enhance digital product development
Lead and mentor others' root cause/problem-solving efforts, including advanced troubleshooting
Hold self and others accountable to deliver high-quality results with passion, energy, and drive to meet business priorities
Ability to communicate complex plans and technical information to team members
Collaborate and influence others on cross-functional teams advancing partnerships to achieve business objectives
What You Will Need
Must Haves:
6+ years of experience & Bachelor's Degree in computer science, data analytics, mathematics, statistics, data science or related field with applicable data engineering & architecture work experience
4+ years of experience in developing and optimizing ETLs and pipelines with cloud-based solutions in Azure (preferred), Amazon Web Services, or Google Cloud Platform
Coding experience with Python
Database experience.(SQL Preferred)
Nice to Haves:
Azure Data Engineer Associate a plus
Strong problem-solving skills with the ability to evaluate and implement the most efficient and effective data engineering solution
Ability to understand complex and ambiguous business needs
Strong interpersonal and communication skills
Strong analytical skills and ability to derive value from complex clinical, med device, and wearables data
$
109,500
- $
232,900
salary plus bonus eligible + Benefits (Health, Vision, Dental, 401K, Tuition Reimbursement, Employee Assistance Program, Wellbeing Program, Employee Stock Purchase Program). This information reflects the anticipated salary range for this position based on current national data. Actual minimum and maximum may vary based on location. Individual pay is based on skills, experience, and other relevant factors.
About Stryker
Our benefits:
12 paid holidays annually
Health benefits include: Medical and prescription drug insurance, dental insurance, vision insurance, critical illness insurance, accident insurance, hospital indemnity insurance, personalized healthcare support, wellbeing program and tobacco cessation program.
Financial benefits include Health Savings Account (HSA), Flexible Spending Accounts (FSAs), 401(k) plan, Employee Stock Purchase Plan (ESPP), basic life and AD&D insurance, and short-term disability insurance.
For a more detailed overview of our benefits or time off, please follow this link to learn more: US Stryker employee benefits
About Stryker
Stryker is one of the world’s leading medical technology companies and, together with its customers, is driven to make healthcare better. The company offers innovative products and services in Medical and Surgical, Neurotechnology, Orthopaedics and Spine that help improve patient and healthcare outcomes. Alongside its customers around the world, Stryker impacts more than 130 million patients annually. More information is available at stryker.com.
Know someone at Stryker?
Be sure to have them submit you as a referral prior to applying for this position. Learn more about our employee referral program on our referral page
Stryker is driven to work together with our customers to make healthcare better. Employees and new hires in sales and field roles that require access to customer accounts as a function of the job may be required, depending on customer requirements, to obtain various vaccinations as an essential function of their role.
R506188
Show more
Show less","Python, SQL, Azure, AWS, Snowflake, Databricks, Kubernetes, Docker, GitLab, Scrum, Agile, Data pipelines, Data storage, Data modeling, Data types, ETL pipelines, Data orchestration, Version control, Data security, Data privacy, Financial acumen, Data solutions, Presentations, Communication, Troubleshooting, Business priorities, Team collaboration, Partnerships, Data engineering experience, ETL development, Cloudbased solutions, Database experience, Azure Data Engineer Associate, Problemsolving skills, Interpersonal skills, Communication skills, Analytical skills, Clinical data, Medical device data, Wearables data","python, sql, azure, aws, snowflake, databricks, kubernetes, docker, gitlab, scrum, agile, data pipelines, data storage, data modeling, data types, etl pipelines, data orchestration, version control, data security, data privacy, financial acumen, data solutions, presentations, communication, troubleshooting, business priorities, team collaboration, partnerships, data engineering experience, etl development, cloudbased solutions, database experience, azure data engineer associate, problemsolving skills, interpersonal skills, communication skills, analytical skills, clinical data, medical device data, wearables data","agile, analytical skills, aws, azure, azure data engineer associate, business priorities, clinical data, cloudbased solutions, communication, communication skills, data engineering experience, data orchestration, data privacy, data security, data solutions, data storage, data types, database experience, databricks, datamodeling, datapipeline, docker, etl development, etl pipelines, financial acumen, gitlab, interpersonal skills, kubernetes, medical device data, partnerships, presentations, problemsolving skills, python, scrum, snowflake, sql, team collaboration, troubleshooting, version control, wearables data"
Data Integration Engineer,Sears Home Services,"Hoffman Estates, IL",https://www.linkedin.com/jobs/view/data-integration-engineer-at-sears-home-services-3773782162,2023-12-17,La Grange,United States,Associate,Remote,"JOB SCOPE:
This position will report directly to the Sr Dir, Technology for Sears Home Services and will support financial platform technology integrations across the Sears Home Services organization.
JOB SUMMARY:
The Data Integration Engineer is responsible for designing, developing, testing, and deploying data integrations across multiple platforms both within middleware and through API connections. They will work with stakeholders to understand data requirements and develop data models. They will also select and implement the appropriate data integration tools and technologies.
Responsibilities/Skills/Experience Requirements
JOB DUTIES/RESPONSIBILITIES:
Designs, develops, tests, and deploys data integration solutions with a focus on financial data accuracy and integrity
Collaborates with stakeholders and cross-functional teams in the Finance Business Unit to understand financial data requirements and develops financial data models that can be used for further analysis
Evaluates, selects, and implements appropriate data integration tools and technologies, with a focus on financial data security and efficiency
Develops and executes comprehensive testing plans to ensure the accuracy and reliability of financial data integration solutions;
Continuously monitors and troubleshoots data integrations if issues arise
Creates and maintains documentation for financial data integration processes, ensuring compliance with regulatory requirements and facilitating knowledge transfer within the team
Stays updated on industry best practices and emerging technologies in financial data integration
Ensures compliance with financial data regulations and industry standards
Communicates effectively with both technical and non-technical team members to ensure successful implementation of financial data integration projects
REQUIRED SKILLS:
Bachelor's degree in Computer Science, Information Systems, or a related field
3+ years of experience in data integration
Experience with SQL, NoSQL, and data warehousing technologies
Experience with data modeling and data quality
Experience with cloud computing platforms (AWS, Azure, GCP)
Strong programming skills (Python, Java, etc.)
Excellent problem-solving and analytical skills
Ability to work independently and as part of a team
Experience with cloud-based data integration platforms such as AWS Glue or Azure Data Factory
Experience with machine learning and artificial intelligence
PREFERRED SKILLS:
Experience with data integration tools such as Boomi, Informatica, or MuleSoft
Experience with big data technologies such as Hadoop and Spark
Years Experience
2 - 5 Years Experience
Travel Requirements
On Occasion (Less than 5%)
Country
United States
Work-In Address 1
REMOTE
Business
Transformco Home Services - Support
Job Function
Information Technology
Employment Category
Regular, Full-time
Show more
Show less","Data Integration, Data Modeling, SQL, NoSQL, Data Warehousing, Cloud Computing, Python, Java, AWS Glue, Azure Data Factory, Machine Learning, Artificial Intelligence, Boomi, Informatica, MuleSoft, Hadoop, Spark","data integration, data modeling, sql, nosql, data warehousing, cloud computing, python, java, aws glue, azure data factory, machine learning, artificial intelligence, boomi, informatica, mulesoft, hadoop, spark","artificial intelligence, aws glue, azure data factory, boomi, cloud computing, data integration, datamodeling, datawarehouse, hadoop, informatica, java, machine learning, mulesoft, nosql, python, spark, sql"
"Data Research Analyst, gt.school (Remote) - $60,000/year USD",Crossover,"Chicago, IL",https://www.linkedin.com/jobs/view/data-research-analyst-gt-school-remote-%2460-000-year-usd-at-crossover-3783190127,2023-12-17,La Grange,United States,Associate,Remote,"Crossover is the world's #1 source of full-time remote jobs. Our clients offer top-tier pay for top-tier talent. We're recruiting this role for our client, gt.school. Have you got what it takes?
Are you ready to revolutionize education with the power of cutting-edge technology? We're seeking strategic problem solvers who thrive on developing and implementing innovative solutions and are eager to apply the power of generative AI to supercharge their work. Your consistent high performance and dedication will drive our mission to revolutionize education.
At gt.school, we're on a mission to accelerate the pace of learning by 5-10X, harnessing the incredible potential of technology, algorithms, and generative AI. We are creating a platform where students can discover, explore, and learn at an unprecedented rate. You'll play a pivotal role in bringing this vision to life. You'll have the opportunity to build new tools and operational processes, as well as streamline existing workflows for maximum efficiency.
But that's not all! At gt.school, we believe in your growth and development. You'll receive regular coaching and training from a team of global tech experts and operations professionals, ensuring your continuous improvement. We value collaboration and provide a supportive environment where you can meet with team members to work together on exciting projects.
If you're eager for a role that combines research, strategy, product development, and data science in the ever-evolving realm of education and technology, seize the moment and apply below. Let's embark on this exhilarating journey together!
What You Will Be Doing
Engineering generative AI prompts in Python/JavaScript/JSON
Analyze AI outputs and iterate on prompts to reach desired outcomes
Leverage generative AI and technology to develop new educational tools, iterate on current products, and finetune AI training models
Research topics to support the product engineering team
What You Won’t Be Doing
Collecting data or performing administrative tasks - we have a junior operations analyst team to do this type of work.
Working on unimportant projects - you'll be tasked with projects that are mission-critical
Data Research Analyst Key Responsibilities
Create technical functionalities that revolutionize the way education is delivered.
Basic Requirements
Excellent written and verbal English communication skills
Proficient in Python, R, JavaScript, or another OOP language
2+ years of work experience working in a technical field (science, technology, programming, financial services, data science, etc.)
Consistently able to work 40 hours per week, Monday through Friday, between 6AM and 6PM CST
Must be a proactive communicator who can effectively manage multiple priorities and stakeholders simultaneously
About Gt.school
GT School is an EdTech Startup that leverages technology, AI, and subject matter experts to cultivate a new way of learning. Our unique approach leverages 50+ years of learning science, cutting-edge data analytics and high-performance coaching. In doing so, we can help students learn more, learn faster, and learn better - and have fun while doing it. We are a remote-first company that hires globally via Crossover.
There is so much to cover for this exciting role, and space here is limited. Hit the Apply button if you found this interesting and want to learn more. We look forward to meeting you!
Working with Crossover
This is a full-time (40 hours per week), long-term position. The position is immediately available and requires entering into an independent contractor agreement with Crossover. The compensation level for this role is $30 USD/hour, which equates to $60,000 USD/year assuming 40 hours per week and 50 weeks per year. The payment period is weekly. Consult www.crossover.com/help-and-faqs for more details on this topic.
What to expect next:
You will receive an email with a link to start your self-paced, online job application.
Our hiring platform will guide you through a series of online “screening” assessments to check for basic job fit, job-related skills, and finally a few real-world job-specific assignments.
Important!
If you do not receive an email from us:
First, emails may take up to 15 minutes to send, refresh and check again.
Second, check your spam and junk folders for an email from Crossover.com, mark as “Not Spam” since you will receive other emails as well.
Third, we will send to whatever email account you indicated on the Apply form - by default, that is the email address you use as your LinkedIn username and it might be different than the one you have already checked.
If all else fails, just reset your password by visiting https://www.crossover.com/auth/password-recovery if you already applied using LinkedIn EasyApply.
Crossover Job Code: LJ-4880-US-Chicago-DataResearchAn.021
Show more
Show less","Generative AI, Python, JavaScript, JSON, OOP, R, Technical Skills, AI Prompts, English Communication, Data Research, Algorithms, Educational Tools, Product Development, Data Science, Independent Contractor Agreement, Data Analytics","generative ai, python, javascript, json, oop, r, technical skills, ai prompts, english communication, data research, algorithms, educational tools, product development, data science, independent contractor agreement, data analytics","ai prompts, algorithms, data research, data science, dataanalytics, educational tools, english communication, generative ai, independent contractor agreement, javascript, json, oop, product development, python, r, technical skills"
"Data Scientist, gt.school (Remote) - $60,000/year USD",Crossover,"Chicago, IL",https://www.linkedin.com/jobs/view/data-scientist-gt-school-remote-%2460-000-year-usd-at-crossover-3783185660,2023-12-17,La Grange,United States,Associate,Remote,"Crossover is the world's #1 source of full-time remote jobs. Our clients offer top-tier pay for top-tier talent. We're recruiting this role for our client, gt.school. Have you got what it takes?
Are you ready to revolutionize education with the power of cutting-edge technology? We're seeking strategic problem solvers who thrive on developing and implementing innovative solutions and are eager to apply the power of generative AI to supercharge their work. Your consistent high performance and dedication will drive our mission to revolutionize education.
At gt.school, we're on a mission to accelerate the pace of learning by 5-10X, harnessing the incredible potential of technology, algorithms, and generative AI. We are creating a platform where students can discover, explore, and learn at an unprecedented rate. You'll play a pivotal role in bringing this vision to life. You'll have the opportunity to build new tools and operational processes, as well as streamline existing workflows for maximum efficiency.
But that's not all! At gt.school, we believe in your growth and development. You'll receive regular coaching and training from a team of global tech experts and operations professionals, ensuring your continuous improvement. We value collaboration and provide a supportive environment where you can meet with team members to work together on exciting projects.
If you're eager for a role that combines research, strategy, product development, and data science in the ever-evolving realm of education and technology, seize the moment and apply below. Let's embark on this exhilarating journey together!
What You Will Be Doing
Engineering generative AI prompts in Python/JavaScript/JSON
Analyze AI outputs and iterate on prompts to reach desired outcomes
Leverage generative AI and technology to develop new educational tools, iterate on current products, and finetune AI training models
Research topics to support the product engineering team
What You Won’t Be Doing
Collecting data or performing administrative tasks - we have a junior operations analyst team to do this type of work.
Working on unimportant projects - you'll be tasked with projects that are mission-critical
Data Scientist Key Responsibilities
Create technical functionalities that revolutionize the way education is delivered.
Basic Requirements
Excellent written and verbal English communication skills
Proficient in Python, R, JavaScript, or another OOP language
2+ years of work experience working in a technical field (science, technology, programming, financial services, data science, etc.)
Consistently able to work 40 hours per week, Monday through Friday, between 6AM and 6PM CST
Must be a proactive communicator who can effectively manage multiple priorities and stakeholders simultaneously
About Gt.school
GT School is an EdTech Startup that leverages technology, AI, and subject matter experts to cultivate a new way of learning. Our unique approach leverages 50+ years of learning science, cutting-edge data analytics and high-performance coaching. In doing so, we can help students learn more, learn faster, and learn better - and have fun while doing it. We are a remote-first company that hires globally via Crossover.
There is so much to cover for this exciting role, and space here is limited. Hit the Apply button if you found this interesting and want to learn more. We look forward to meeting you!
Working with Crossover
This is a full-time (40 hours per week), long-term position. The position is immediately available and requires entering into an independent contractor agreement with Crossover. The compensation level for this role is $30 USD/hour, which equates to $60,000 USD/year assuming 40 hours per week and 50 weeks per year. The payment period is weekly. Consult www.crossover.com/help-and-faqs for more details on this topic.
What to expect next:
You will receive an email with a link to start your self-paced, online job application.
Our hiring platform will guide you through a series of online “screening” assessments to check for basic job fit, job-related skills, and finally a few real-world job-specific assignments.
Important!
If you do not receive an email from us:
First, emails may take up to 15 minutes to send, refresh and check again.
Second, check your spam and junk folders for an email from Crossover.com, mark as “Not Spam” since you will receive other emails as well.
Third, we will send to whatever email account you indicated on the Apply form - by default, that is the email address you use as your LinkedIn username and it might be different than the one you have already checked.
If all else fails, just reset your password by visiting https://www.crossover.com/auth/password-recovery if you already applied using LinkedIn EasyApply.
Crossover Job Code: LJ-4880-US-Chicago-DataScientist.012
Show more
Show less","Python, JavaScript, JSON, Generative AI, AI Outputs, R, OOP, Data Science, Data Analytics, English Communication, HighPerformance Coaching, AI Training Models, Technology, Subject Matter Experts, LinkedIn, Crossover","python, javascript, json, generative ai, ai outputs, r, oop, data science, data analytics, english communication, highperformance coaching, ai training models, technology, subject matter experts, linkedin, crossover","ai outputs, ai training models, crossover, data science, dataanalytics, english communication, generative ai, highperformance coaching, javascript, json, linkedin, oop, python, r, subject matter experts, technology"
Sustainability Data Analyst,JLL,"Chicago, IL",https://www.linkedin.com/jobs/view/sustainability-data-analyst-at-jll-3741673024,2023-12-17,La Grange,United States,Associate,Remote,"Duties & Responsibilities
The Sustainability Data Analyst role will join JLL’s Sustainability Data and Reporting team to support our data management, platform, compliance and reporting functions. The role will work across our global client portfolio delivering data management services, reporting and advice in a high performance, outcomes driven team. This is a remote and/or hybrid role that can be located anywhere in the U.S.
The position is required to work collaboratively across internal global business lines including JLL’s Client Account, Technology and Operations teams to help manage stakeholder expectations and maintain high quality service delivery.
The candidate will have experience in delivering multiple programs of works in parallel. The role will be responsible for several tasks including:
Assist in delivery of client projects by assisting them to meet their sustainability reporting objectives e.g. DJSI, GRESB, Modern Slavery, TCFD, GRI, RE100, NGER, SASB, CDP and other reporting frameworks.
Develop a detailed understanding of JLL’s sustainability reporting application, Canopy, and how we support our clients in measuring their sustainability performance.
Participate in new client transition and delivery activities as required, to ensure projects are delivered on time and to a high standard.
Work with client delivery and development teams to ensure program milestones are being met.
Develop a detailed understanding of data structures within client’s data.
Performance objectives
The role requires the ability to actively manage concurrent projects and a strong talent for project coordination.
Regularly communicate in a clear and non-technical way to internal JLL and client users.
Be an integral part of the data and reporting team, contributing to the efficient monitoring and management of support requests, following internal processes and on-going performance improvement of the team.
Contribute the successful migration of existing client base to new application within agreed timelines.
Identify areas of the platform for improvement to enhance user experience.
Identify support, training and management processes that can be improved to increase scalability and efficiency.
Key skills
The ability to manage multiple deliverables with inter-related dependencies
Strong organisational skills and process-driven, with an orientation toward continuous process improvement
Strong emotional intelligence and capacity to deliver an excellent client experience.
Strong ability to clearly identify issues with data and raise them to the appropriate JLL team member.
Ability to work to a defined milestone dates and raise any concerns early and often.
Candidate specification
1-3 years’ experience in similar analyst role.
Basic understanding of HVAC and building energy systems and/or Green House Gas Emission calculations.
Familiarity with sustainability related certification and benchmarking tool
High proficiency in Microsoft Excel and data management.
Proficient in using MS Excel, Access and SQL to combine and analyze data from various sources, and generate reports and graphics. Must be able to combine large volumes of data for consolidated reporting.
Ability to perform data analysis and identify the gaps and trends in data.
Excellent communication skills including the ability to identify and describe data anomalies and provide solutions accordingly.
Lateral thinking and problem-solving skills.
Ability to multi-task and manage priorities to meet deadlines.
A strong desire to deliver a quality outcome for the client and end users.
Experience and understanding of sustainability and carbon emissions reporting will be a strong advantage.
Project management experience would be an advantage.
This role requires a high attention to detail and a strong process-driven approach.
Show more
Show less","Sustainability, Reporting, Data Management, Compliance, Canopy, DJSI, GRESB, Modern Slavery, TCFD, GRI, RE100, NGER, SASB, CDP, HVAC, Building Energy Systems, Greenhouse Gas Emission calculations, Microsoft Excel, MS Access, SQL, Data Analysis, Communication Skills, Lateral Thinking, ProblemSolving Skills, MultiTasking, Project Management","sustainability, reporting, data management, compliance, canopy, djsi, gresb, modern slavery, tcfd, gri, re100, nger, sasb, cdp, hvac, building energy systems, greenhouse gas emission calculations, microsoft excel, ms access, sql, data analysis, communication skills, lateral thinking, problemsolving skills, multitasking, project management","building energy systems, canopy, cdp, communication skills, compliance, data management, dataanalytics, djsi, greenhouse gas emission calculations, gresb, gri, hvac, lateral thinking, microsoft excel, modern slavery, ms access, multitasking, nger, problemsolving skills, project management, re100, reporting, sasb, sql, sustainability, tcfd"
Senior Data Engineer,University of Chicago,"Chicago, IL",https://www.linkedin.com/jobs/view/senior-data-engineer-at-university-of-chicago-3773582084,2023-12-17,La Grange,United States,Associate,Hybrid,"About the Department
IT Services collaborates with campus partners to support the mission of the University of Chicago through the consistent delivery of high-quality solutions and services.
• We provide secure, stable, and reliable infrastructure and applications to support the mission of the University.
• We support and enable faculty research and teaching with the effective use of technology.
• We simplify the technology experience for faculty, students, alumni, and staff, and we ensure technology is mobile-friendly and accessible.
• We identify, manage, and mitigate the technology risks of the University.
Job Summary
The Senior Data Engineer is the technical lead on projects to design, develop, and QA Data Warehouse (DW) and extract/transform/load (ETL) environments, and other database environments, to enable analytics. This role follows the prescribed project and development methodology. Under the direction of a Project Manager, this role will work closely with the Lead Architect, BI Lead, DW developers, source application developers, and subject matter experts (SMEs). The Senior Data Engineer will support, maintain, and enhance these environments to achieve a cross-functional, integrated reporting and analysis environment for University users.
Responsibilities
Works closely with business analysts and subject matter experts to understand data analysis requirements, data definitions, and to develop data mappings, data rules, and data transformations for reporting related to a particular data warehouse and/or analytic subject area.
Finalizes the conceptual data models developed by the BI Lead, most often a dimensional model with facts and dimensions. Documents and communicates model design to project team.
Leads development of logical data models. Documents, communicates, and maintains model design for the project team.
Leads design and implementation of physical databases including index definitions, partitioning, parallelism, tuning, and space management.
Utilizes database design and documentation tools to document, analyze, and communicate database information.
Designs DW security.
Implements test and QA strategies and prepares for rollout, including assuring data validation and integrity.
Coordinates development resources to ensure that project timelines are met.
Leads the design of ETL processing modules based on transformation rules, mapping rules, processing requirements and database model.
Leads the design of ETL auditing steps to ensure data integrity and completeness of the ETL process.
Leads the design of ETL process flow including module dependencies and parallel processing opportunities.
Provides documentation for and works with job scheduling team to automate ETL processing.
Leads the design of programming specifications for each ETL component.
Leads the design of testing plans for each phase of the development process. Plans must ensure accuracy and quality of program code as well as thorough data validation procedures.
Develops efficient code that meets analysis and design criteria.
Coordinates ETL development resources to ensure that project timelines are met.
Provides break/fix support and communicates outages as defined by IT Services standards.
Troubleshoots and tunes DW and analytic systems.
Coordinates testing of software patches and upgrades.
Works closely with BIS colleagues to implement DW best practices, supporting integration of information from different subject areas.
Promulgates DW, ETL and related development standards, processes, and best practices throughout BIS.
Participates in the selection of DW, ETL and related enabling tools, including software and consulting.
Deep expertise in DW, ETL and analytic systems best practices and emerging trends is imperative
Works with users to ensure deliverables are meeting or exceeding expectations. Provides support and communicates issues to users as defined by IT standards.
Plans, tests and guides roll-out of patches and upgrades to the BI and analytic software. Solves problems in the development and interpretation of BI and analytics system standards and procedures.
Performs other related work as needed.
Minimum Qualifications
Education:
Minimum requirements include a college or university degree in related field.
Work Experience:
Minimum requirements include knowledge and skills developed through 5-7 years of work experience in a related job discipline.
Preferred Qualifications
Education:
Bachelor's degree.
Advanced study or professional certifications in database, data management, DW, or ETL disciplines.
Experience:
Four years experience designing and building ETL using ODI.
Four years experience designing and building Data Warehouse systems using Oracle and VPD.
One year of experience building MuleSoft applications.
Five to seven years of database and application development experience with a formal software development life cycle.
Three years Hadoop experience with Cloudera Apache Hadoop Ecosystem (Nifi, Zeppelin, Oozie, Spark, Impala, Kafka, et al).
A minimum of five years of data warehouse development experience including expert knowledge of data warehousing methodologies (e.g. Kimball).
A minimum of four years of experience designing and building dimensional data warehouses with Oracle db, Oracle Stored Procedures and Packages.
A minimum of four years or more designing and developing ETL processes for data warehouse implementations using Oracle OWB or equivalent ETL tools.
Experience as lead DW/ETL designer on two successful enterprise data warehouse initiatives, using both formal software lifecycle development and project management.
Two years experience working within the ITIL framework including incident, service, problem, and change management processes and procedures.
Technical Skills or Knowledge:
Programming environments, languages, and systems built using (Java, Python, R).
AI/ML and predictive analytic algorithms.
Knowledge of reporting and analytic systems based on current-generation ERP systems.
Familiarity with Business Objects, Tableau or OBIA/OBIEE.
Oracle Cloud Financials.
Familiarity with higher education administrative systems and data.
Working knowledge of Java Applications.
Expertise with data query, analysis, reporting techniques, and OLAP.
Expertise in Database Design and tools (e.g. Power Designer or Erwin).
Expertise to develop and administer UNIX-based relational database architectures (ODBC, JDBC, Perl DBI, shell scripting, PL/SQL, SQL Developer, SQL Plus, SQL Loader, TOAD, Java).
Expert skills in data profiling, source-target mapping, implementation of transformations and business rules.
Superior ETL design and development skills including maps, process flows, auditing, and scheduling using vendor tools (OWB, ODI, Data Stage, or Informatica).
Expertise in SQL coding techniques, performance monitoring and tuning.
Working with Unix Server, and Windows Workstation and Server environments.
Microsoft Office application suite, especially Excel.
Preferred Competencies
Excellent analytic skills to quickly synthesize technical, functional, and user inputs, and manage problem-solving to issue resolution.
Leads joint design sessions, documenting the results using business process design tools.
Strong oral and written communication skills, including developing standards and procedures, and presenting technical concepts to non-technical audiences.
Commitment to working and sharing expertise in a collaborative team environment.
Strong sense of accountability for completing work within defined scope and timeline, with ability to multi-task effectively.
Very high attention to detail and accuracy.
Working Conditions
Standard office environment.
On-call responsibilities.
Application Documents
Resume/CV (required)
Cover Letter (required)
When applying, the document(s)
MUST
be uploaded via the
My Experience
page, in the section titled
Application Documents
of the application.
Posting Statement
The University of Chicago is an Affirmative Action/Equal Opportunity/Disabled/Veterans and does not discriminate on the basis of race, color, religion, sex, sexual orientation, gender, gender identity, national or ethnic origin, age, status as an individual with a disability, military or veteran status, genetic information, or other protected classes under the law. For additional information please see the University's Notice of Nondiscrimination.
Staff Job seekers in need of a reasonable accommodation to complete the application process should call 773-702-5800 or submit a request via Applicant Inquiry Form.
We seek a diverse pool of applicants who wish to join an academic community that places the highest value on rigorous inquiry and encourages a diversity of perspectives, experiences, groups of individuals, and ideas to inform and stimulate intellectual challenge, engagement, and exchange.
All offers of employment are contingent upon a background check that includes a review of conviction history. A conviction does not automatically preclude University employment. Rather, the University considers conviction information on a case-by-case basis and assesses the nature of the offense, the circumstances surrounding it, the proximity in time of the conviction, and its relevance to the position.
Show more
Show less","Data Warehouse (DW), ETL (Extraction Transform Load), Dimensional Modeling, Database Design, SQL, Java, Python, R, AI/ML Algorithms, Hadoop, Cloudera Apache Hadoop Ecosystem, Oracle, VPD, MuleSoft, Business Objects, Tableau, OBIA/OBIEE, Oracle Cloud Financials, Java Applications, OLAP, Power Designer, Erwin, Unix, ODBC, JDBC, Perl DBI, Shell Scripting, PL/SQL, SQL Developer, SQL Plus, SQL Loader, TOAD, Data Profiling, SourceTarget Mapping, Transformations, Business Rules, Informatica, SQL Coding, Performance Monitoring, Tuning, Unix Server, Windows Workstation, Microsoft Office, Excel","data warehouse dw, etl extraction transform load, dimensional modeling, database design, sql, java, python, r, aiml algorithms, hadoop, cloudera apache hadoop ecosystem, oracle, vpd, mulesoft, business objects, tableau, obiaobiee, oracle cloud financials, java applications, olap, power designer, erwin, unix, odbc, jdbc, perl dbi, shell scripting, plsql, sql developer, sql plus, sql loader, toad, data profiling, sourcetarget mapping, transformations, business rules, informatica, sql coding, performance monitoring, tuning, unix server, windows workstation, microsoft office, excel","aiml algorithms, business objects, business rules, cloudera apache hadoop ecosystem, data profiling, data warehouse dw, database design, dimensional modeling, erwin, etl extraction transform load, excel, hadoop, informatica, java, java applications, jdbc, microsoft office, mulesoft, obiaobiee, odbc, olap, oracle, oracle cloud financials, performance monitoring, perl dbi, plsql, power designer, python, r, shell scripting, sourcetarget mapping, sql, sql coding, sql developer, sql loader, sql plus, tableau, toad, transformations, tuning, unix, unix server, vpd, windows workstation"
Real Estate Data Analyst,Remedy Medical Properties,"Chicago, IL",https://www.linkedin.com/jobs/view/real-estate-data-analyst-at-remedy-medical-properties-3785836394,2023-12-17,La Grange,United States,Associate,Hybrid,"Real Estate Data Analyst/Specialist
Chicago, IL.
Remedy is an independent, full-service healthcare real estate company providing acquisition, development, leasing, management and strategy consulting services across the United States. Headquartered in Chicago, we are the largest private owner of healthcare properties in the country. Our national presence enables us to offer the right solutions in the right place. Our team is always thinking ahead. Real estate minds trained on where healthcare is going, with bright ideas on how to get our clients there. If you’re one of those bright minds, we’d like to meet you.
Position Overview
: In today's swiftly evolving medical industry, the efficient management of real estate assets plays a crucial role in the success of healthcare real estate companies. To this end, a Data Analyst position has emerged as an essential role in coordination with the Leasing, Asset Management, Marketing, and Acquisition departments. This job aims to attract qualified candidates with a strong background in data analysis and an understanding of the unique requirements of medical real estate.
Primary responsibilities will be to analyze, interpret, and present data related to Healthcare Real Estate. This person will work closely with Leasing, Asset Management, Marketing, and the Acquisition departments to provide insights and recommendations in day-to-day operations and deal closings.
This role presents an exciting opportunity for professionals with a passion for leveraging data to drive strategic decision-making within the healthcare industry. The successful candidate will play a vital role in optimizing lease agreements, maximizing occupancy rates, and contributing to the financial success of the organization. If you possess the necessary skills and have a deep understanding of medical real estate leasing, we invite you to apply and be part of a dynamic team dedicated to improving leasing initiatives through effective real estate.
Responsibilities:
Data Collection and Management:
Gather, interpret, and validate relevant data related to medical real estate leasing, including occupancy rates, lease terms, rental rates, and market conditions.
Research and compile customer target lists for marketing campaigns and healthcare provider location lists for marketing collateral.
Utilize various sources and databases to ensure accurate and up-to-date information.
Develop and maintain a comprehensive database for the organization's medical real estate holdings.
Research, import, update, and clean customer data within CRM database.
Data Analysis and Reporting:
Perform complex data analysis to identify patterns, trends, and opportunities in the medical leasing market.
Perform analysis of Remedy’s existing medical office portfolio to measure performance, track market-specific trends, and predict outcomes.
Create insightful reports and dashboards to assist various teams in strategic decision-making.
Analyze CRM data to identify trends, opportunities, and areas for improvement.
Lease Optimization and Strategy:
Analyze and evaluate existing lease agreements to identify opportunities for improvement or renegotiation.
Conduct market research to understand trends and leasing benchmarks within healthcare real estate.
Forecasting and Modeling:
Utilize statistical models and forecasting techniques to predict future market conditions and leasing trends.
Assist in the development of financial models to evaluate potential lease scenarios and assess their impact on the organization's bottom line.
Qualifications:
The ideal candidate must possess the following qualifications:
Bachelor's degree in a relevant field (e.g., finance, real estate, business analytics, economics, or similar).
Proven experience in data analysis, preferably in the real estate or healthcare sector.
Proficiency in Excel is required.
Competency using Argus, VTS, Hubspot, Definitive Healthcare, ZoomInfo, ESRI, Prophia, Yardi, and data visualization software is preferred.
Experience in business analytics, statistical analysis and predictive analytics preferred
Excellent communication and presentation skills, with the ability to translate complex data into actionable insights.
Detail-oriented mindset, with an aptitude for accuracy and precision in data analysis.
Ability to work effectively in a team environment and collaborate with cross-functional departments.
Now is a great time to join our growing team at Remedy! You will work side-by-side with firm leadership to serve clients and build the business, having a direct impact on the firm’s success. We offer competitive health benefits: Medical, Dental, Vision, Life (including spouse & child), 401K, STD/LTD, Parental leave, AD&D, Tuition Reimbursement, and Commuter benefits. We offer flexibility in how and where you get your work done and meaningful relationships with clients, teammates and leadership who truly care about you and your development.
Remedy Medical Properties is committed to providing equal employment opportunities to all employees and applicants and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, marital status, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.
Show more
Show less","Data Analysis, Data Management, Real Estate Leasing, Argus, VTS, Hubspot, Definitive Healthcare, ZoomInfo, ESRI, Prophia, Yardi, Data Visualization, Excel, Predictive Analytics, Business Analytics, Statistical Analysis, CRM, Lease Optimization, Forecasting, Financial Modeling","data analysis, data management, real estate leasing, argus, vts, hubspot, definitive healthcare, zoominfo, esri, prophia, yardi, data visualization, excel, predictive analytics, business analytics, statistical analysis, crm, lease optimization, forecasting, financial modeling","argus, business analytics, crm, data management, dataanalytics, definitive healthcare, esri, excel, financial modeling, forecasting, hubspot, lease optimization, predictive analytics, prophia, real estate leasing, statistical analysis, visualization, vts, yardi, zoominfo"
Business Data Analyst with DEX tools,Steneral Consulting,"Chicago, IL",https://www.linkedin.com/jobs/view/business-data-analyst-with-dex-tools-at-steneral-consulting-3743457192,2023-12-17,La Grange,United States,Associate,Hybrid,"Share only 2 profiles
Local candidates only
Linkedin is must
This role will start off 2 days a week in office and then will transition into 5 days a week onsite.
Role: Business Data Analyst with DEX tools - Tachyon 1E and SysTrak
Duration: 6 months with extension
Location: Chicago, IL
Interview: two video rounds
This person will be working with the end user team on tools that identify issues, allow us to leverage automation to detect issues on end user devices.
This person will be working with the end user team on tools that identify issues, allow us to leverage automation to detect issues on end user devices.
Gathering requirements, work with IT business partners to understand issues and what they are seeing, and taking what they are saying to help assist in creating automation for these teams. So this person will be provide these reporting on the end user devices and the automation that we are implementing. Including what models of printers are involved, computers, and other end user equipment in this space.
Must have-
Dashboarding (power BI or excel etc..)
Reporting / Requirements gathering and organized in this space.
A mix of the software and equipment experience
IT Business Analyst Responsibilities
Liaising between End User Engineering, IT partners, and the various lines of business throughout the enterprise.
Understanding strategic business needs and plans for growth.
Enhancing the quality of IT products and services.
Analyzing the design of technical systems and business models.
Utilizing IT data for business insights.
Gathering business requirements and analyzing business needs.
Sourcing and implementing new business technology.
Finding technological solutions to business requirements.
Coordinating UAT testing with the business.
Analyzing data to inform business decisions.
Collaborate with other business analysts and/or project managers to assure a consistent implementation of project management disciplines and methodologies.
Coordinate project activities leveraging team members across areas of the enterprise, vendors, external agencies, and partners.
Document and actively manage project dependencies, issues, and risks to resolution.
Prepare and provide regular updates on progress, risks, issues, etc. to department leadership/sponsors.
Scope, estimate, and plan new IT/Infrastructure projects
Prioritize requirements from various stakeholders.
Communicate, translate, and simplify business requirements to ensure buy-in from all stakeholders.
Discover, organize, and clarify business needs and review/produce specifications for change.
Work with End User Engineering and IT partners to ensure that they understand business requirements and specifications.
IT Business Analyst Requirements
Degree in computer engineering, business administration, or related field.
5 or more years of relevant technical project management experience, especially for IT infrastructure related projects.
Candidates with technical backgrounds a plus.
Experience with DEX tools such as Tachyon 1E and SysTrak (Strongly desired).
Successful management of multiple concurrent IT projects
Previous experience working on projects that focus on automations, creating efficiencies, and improving the end user experience.
Demonstrated advanced use of Microsoft Project, Microsoft Word, Microsoft Excel, Visio, and PPM.
Demonstrated experience in creating reports and dashboards using Workspace ONE Intelligence, PowerBi, Spotfire, or other similar tools.
Proven ability to extract data from Microsoft Endpoint Configuration Manager and analyze data and create reports with findings.
Proficient at creating detailed technical documentation.
Excellent problem-solving skills.
Analytical mindset.
Exceptional interpersonal skills.
Excellent written and verbal communication skills.
Attention to detail.
Show more
Show less","DEX Tools, Tachyon 1E, SysTrak, Power BI, Excel, Microsoft Project, Microsoft Word, Microsoft Excel, Visio, PPM, Workspace ONE Intelligence, Spotfire, Microsoft Endpoint Configuration Manager","dex tools, tachyon 1e, systrak, power bi, excel, microsoft project, microsoft word, microsoft excel, visio, ppm, workspace one intelligence, spotfire, microsoft endpoint configuration manager","dex tools, excel, microsoft endpoint configuration manager, microsoft excel, microsoft project, microsoft word, powerbi, ppm, spotfire, systrak, tachyon 1e, visio, workspace one intelligence"
Senior Data Engineer,Adyen,"Chicago, IL",https://www.linkedin.com/jobs/view/senior-data-engineer-at-adyen-3737475864,2023-12-17,La Grange,United States,Mid senior,Onsite,"This is Adyen
Adyen provides payments, data, and financial products in a single solution for customers like Meta, Uber, H&M, and Microsoft - making us the financial technology platform of choice. At Adyen, everything we do is engineered for ambition.
For our teams, we create an environment with opportunities for our people to succeed, backed by the culture and support to ensure they are enabled to truly own their careers. The people of Adyen are motivated individuals who tackle unique technical challenges at scale and solve them as a team. Together, we deliver innovative and ethical solutions that help businesses achieve their ambitions faster.
Senior Data Engineer
Responsibilities
At Adyen, we treat data and data artifacts as first-class citizens. They form our backbone and drive the business and product insights we generate. To this end, Adyen is looking for a Senior Data Engineer to join our development team in Chicago, a person that understands the business context and the data needs behind it, and knows how to implement quality data pipelines on our Big Data Platform. Your pipelines will fuel stunning visuals, reports, and raw or refined datasets for internal or external use. You will be responsible for creating performant and high-quality ETL/ELTs and ensuring users have access to correct, accurate and performant data and insights. In this role, you will-
Work with product teams, product specialists and data analysts, analyzing their data and transcribing their data needs that render into data pipelines.
Design, develop, deploy and operate production ETL/ELT pipelines in PySpark.
Build data products and tooling according to quality principles such as performance, code quality, data validation, data governance and discoverability.
Establish and spread data analytics best practices across the organization.
Organize data analytics related training sessions.
Develop linters and other tooling bespoke to Adyen to help ensure a high-quality pipeline codebase
Who You Are
You have experience building ETLs/ELTs, ideally in PySpark and Airflow.
You have interest in data engineering practices and product knowledge, specifically in how to leverage the power of analytics into business.
You have a good understanding of Software Engineering practices and Data Engineering principles.
You are able to communicate complex outcomes with clarity over a wide range of audiences.
Technology- Python and SQL are a must.
Mentality- An experimental mindset with a launch fast and iterate mentality. A strong statistics/mathematics background is a plus.
Our Diversity, Equity and Inclusion commitments
Our unique approach is a product of our diverse perspectives. This diversity of backgrounds and cultures is essential in helping us maintain our momentum. Our business and technical challenges are unique, and we need as many different voices as possible to join us in solving them - voices like yours. No matter who you are or where you’re from, we welcome you to be your true self at Adyen.
Studies show that women and members of underrepresented communities apply for jobs only if they meet 100% of the qualifications. Does this sound like you? If so, Adyen encourages you to reconsider and apply. We look forward to your application!
What’s next?
Ensuring a smooth and enjoyable candidate experience is critical for us. We aim to get back to you regarding your application within 5 business days. Our interview process tends to take about 4 weeks to complete, but may fluctuate depending on the role. Learn more about our hiring process here. Don’t be afraid to let us know if you need more flexibility.
Adyen is an equal opportunity employer. We do not discriminate based on race, color, ethnicity, ancestry, national origin, religion, sex, gender, gender identity, gender expression, sexual orientation, age, disability, veteran status, genetic information, marital status or any legally protected status.
All your information will be kept confidential according to EEO guidelines.
This role is based out of our Chicago office. We have a hybrid workplace and value in-person collaboration; we do not offer remote-only roles.
Show more
Show less","Data Analytics, ETL/ELT, PySpark, Airflow, Python, SQL, Software Engineering, Data Engineering, Data Governance, Machine Learning, Statistics, Mathematics, Data Products, Data Visualization, Data Pipelines, Data Quality, Data Validation, Data Modeling, Data Transformation, Data Integration, Data Warehousing, Data Mining, Data Mining, Data Mining, Big Data","data analytics, etlelt, pyspark, airflow, python, sql, software engineering, data engineering, data governance, machine learning, statistics, mathematics, data products, data visualization, data pipelines, data quality, data validation, data modeling, data transformation, data integration, data warehousing, data mining, data mining, data mining, big data","airflow, big data, data engineering, data governance, data integration, data mining, data products, data quality, data transformation, data validation, dataanalytics, datamodeling, datapipeline, datawarehouse, etlelt, machine learning, mathematics, python, software engineering, spark, sql, statistics, visualization"
Data Engineer,EBM Software,Greater Chicago Area,https://www.linkedin.com/jobs/view/data-engineer-at-ebm-software-3778888190,2023-12-17,La Grange,United States,Mid senior,Onsite,"THE ROLE
You will join the data engineering teams daily Agile ceremonies to design, build, test, and release data ingestion pipelines and delivery frameworks consisting of ADF pipelines, MS SQL Server databases, and SSIS jobs. Our team works directly alongside our Customer Success Team and Catalyst Application development team to ingest, refine, and persist client data onto the EBM Catalyst data platform, as well as being responsible for the design and evolution of the EBM Catalyst ingestion framework.
RESPONSIBILITIES INCLUDE
Working with IT leadership and Customer Success team to intake new data delivery projects, define requirements, and refine development user stories, tasks, and bugs.
Executing data ingestion and data framework projects on the EBM Catalyst data platform end-to-end including SQL and pipeline development, code testing, data validation, release planning, and release execution.
Building configuration based modular data pipeline framework components supporting the ingestion, refinement, and delivery of customer data into EBM Catalyst, EBM Data Lake, and EBM Data Science platforms.
Actively contribute and evolve the EBM Data Engineering frameworks including building of re-usable code, automated data validation processes, and continual learning and adoption of industry best practices.
Provide excellent internal customer service through building and supporting applications with robust monitoring, documentation, and incident resolution capabilities.
WHAT YOUR RESUME SHOWS
3+ years of experience using Azure DevOps (CI/CD) / Artifactory / Jenkins / Git or comparable delivery stacks (required)
3+ years of experience in MS SQL Server development including DML and DDL, stored procedures, SSIS packages, and/or SQL performance tuning. (required)
3+ years of experience in MS SQL Server development including DML and DDL, stored procedures, SSIS packages, and/or SQL performance tuning. (required)
3+ years of experience planning, building, and delivering modular data ingestion pipeline applications and frameworks in Azure ADF/ADO or equivalent cloud platforms (required)
3+ years of experience in Python development for ELT/ETL frameworks including working with data science packages (preferred)
1+ years of experience working with SQL Version Management tools such as Visual Server SQL management, FlywayDB, or equivalent.
Experience with enterprise data ingestion/distribution/processing technologies such as Databricks, Snowflake, Informatica, or other cloud-based data platforms or frameworks (required)
Experience working with large enterprise data warehouse, data lake, or data lake-house architectures. (Preferred)
Experience developing enterprise internal or external data products (preferred)
Experience with financial industry practices and/or financial data domains (preferred)
THE OPPORTUNITY
Xerxes Global Holdings is comprised of the following businesses:
MGMT3D
specializes in strategic M&A advisory and business growth in private equity, investment banking, portfolio companies and more.
EBM Software
delivers performance software solutions that drive growth for private equity funds, investment banks and middle market to large corporations.
The Association of Retail and Consumer Professionals (ARC)
is the parent organization of the Category Management Association (CMA) and the Shopper Insights Management Association (SIMA).
Employees here are an elite group of bright, strategic thinkers. They are encouraged to take the ball and run with it, and there’s no question your individual impact can be felt. Our employees are highly valued on an individual level and are always pushed to grow and continuously move onward and upward in their careers. Here, you’ll be working side-by-side with top-level executives and thought-leaders, making connections and constantly learning. We do everything possible to ensure you’re able to grow and shine in your role.
Note: EBM Software is unable to provide sponsorship for employment at this time.
Xerxes Global, LLC provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.
This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.
Show more
Show less","Python, SQL, Azure DevOps (CI/CD), Artifactory, Jenkins, Git, MS SQL Server, DML, DDL, Stored procedures, SSIS packages, SQL performance tuning, ADF, ADO, Databricks, Snowflake, Informatica, Azure ADF/ADO, Visual SQL Management, FlywayDB, ELT/ETL frameworks, Data science packages, Data ingestion, Data distribution, Data processing, Enterprise data warehouse, Data lake, Data lakehouse architectures, Financial industry practices, Financial data domains","python, sql, azure devops cicd, artifactory, jenkins, git, ms sql server, dml, ddl, stored procedures, ssis packages, sql performance tuning, adf, ado, databricks, snowflake, informatica, azure adfado, visual sql management, flywaydb, eltetl frameworks, data science packages, data ingestion, data distribution, data processing, enterprise data warehouse, data lake, data lakehouse architectures, financial industry practices, financial data domains","adf, ado, artifactory, azure adfado, azure devops cicd, data distribution, data ingestion, data lake, data lakehouse architectures, data processing, data science packages, databricks, ddl, dml, eltetl frameworks, enterprise data warehouse, financial data domains, financial industry practices, flywaydb, git, informatica, jenkins, ms sql server, python, snowflake, sql, sql performance tuning, ssis packages, stored procedures, visual sql management"
Data Engineer III -locals,Steneral Consulting,"Chicago, IL",https://www.linkedin.com/jobs/view/data-engineer-iii-locals-at-steneral-consulting-3747733575,2023-12-17,La Grange,United States,Mid senior,Onsite,"Data Engineer III
Location: Chicago IL
Duration: 8 Months
Required
Bachelor's degree in quantitative field (statistics, software engineering, business analytics, information systems, aviation management or related degree)
5+ years of experience in data engineering or ETL development role
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with structure, semi- structure, and unstructured datasets.
Experience with Teradata, SQL server, etc.
Experience with AWS cloud services: Redshift, S3, Athena, etc.
Experience with Databricks and lakehouse architecture
Passionate about solving problems through data and analytics, and creating data products including data models
Strong initiative to take ownership of data-focused projects, get involved in the details of validation and testing, as well as provide a business user perspective to their work
Ability to communicate complex quantitative concepts in a clear, precise, and actionable manner
Proven proficiency with Microsoft Excel and PowerPoint
Strong problem-solving skills, using data to tackle problems
Outstanding writing, communication, and presentation skills
Preferred
Master's degree
Experience with Quantum Metrics and Akamai
Experience with languages: Python, R, etc.
Strong experience with continuous integration & delivery using Agile methodologies
Data engineering experience with transportation/airline industry
Strong problem-solving skills
Overview/Summary
The Product Analytics team at United Airlines is on a transformational journey to unlock the full potential of enterprise data, build a dynamic, diverse, and inclusive culture and develop a modern cloud-based data lake architecture to scale our applications, and drive growth using data and machine learning.
Our objective is to enable the enterprise to unleash the potential of data through innovation and agile thinking, and to execute on an effective data strategy to transform business processes, rapidly accelerate time to market and enable insightful decision making.
Job Overview And Responsibilities
In this role you will partner with various teams to define and execute data acquisition, storage, transformation, processing and make data actionable for operational and analytics initiatives that create sustainable revenue and share growth.
This role requires expertise in United’s data sources and technology business intuition, and a working knowledge of data transformation and analytical tools.
Manage data integration and technology projects across the various platforms that comprise United’s data strategy
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, storage, transformation, and loading (ETL) of data from a wide variety of data sources
Work with stakeholders including the Senior management, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Work with data and analytics experts to strive for greater functionality in our data systems.
Build and maintain databases, data pipelines, and data warehouses that provide a comprehensive understanding of the business.
Execute unit tests and validating expected results to ensure accuracy & integrity of data and applications through analysis, coding, writing clear documentation and problem resolution.
Leverage technical and analytical skills to understand and solve business centric questions
Coordinate and guide cross-functional projects that involve team members across all areas of the enterprise, vendors, external agencies, and partners
Ability to manage multiple deliverables both short and long-term in a busy and aggressive environment, while staying flexible to dynamic needs and priority levels
Manage agile development and delivery by collaborating with project manager, product owner and development leads
Show more
Show less","Data Engineering, ETL Development, Root Cause Analysis, AWS Cloud Services, Redshift, S3, Athena, Databricks, Lakehouse Architecture, Data Models, Microsoft Excel, PowerPoint, Python, R, Agile Methodologies, Data Integration, Data Transformation, Data Analysis, Data Warehousing, Unit Testing, Data Validation, Data Documentation, Problem Solving, CrossFunctional Projects, Agile Development, Delivery Management, Priority Management","data engineering, etl development, root cause analysis, aws cloud services, redshift, s3, athena, databricks, lakehouse architecture, data models, microsoft excel, powerpoint, python, r, agile methodologies, data integration, data transformation, data analysis, data warehousing, unit testing, data validation, data documentation, problem solving, crossfunctional projects, agile development, delivery management, priority management","agile development, agile methodologies, athena, aws cloud services, crossfunctional projects, data documentation, data engineering, data integration, data models, data transformation, data validation, dataanalytics, databricks, datawarehouse, delivery management, etl development, lakehouse architecture, microsoft excel, powerpoint, priority management, problem solving, python, r, redshift, root cause analysis, s3, unit testing"
Lead Snowflake Data Engineer,Zortech Solutions,"Rosemont, IL",https://www.linkedin.com/jobs/view/lead-snowflake-data-engineer-at-zortech-solutions-3674818449,2023-12-17,La Grange,United States,Mid senior,Onsite,"Role: Lead Snowflake Data Engineer
Location: Rosemont, IL (Initial Remote)
Mandatory skills: AWS, Snowflake, Informatica, Python
Experience working on Snowflake.
Experience working on Informatica Cloud (IICS).
Experience working on AWS data environment S3, EMR, Lambda, SQS etc.
Strong background in developing technical solutions for Cloud data warehouse and data lakes.
Exposure to Python is a plus.
Experience designing and troubleshooting complicated data extraction routines and real time / batch integrations.
Experience extracting and processing data from multiple data sources such as: Databases (Exadata, Oracle, Snowflake), Applications, Web Services (SOAP/REST), flat files (*.csv, *.txt, *.xml, *.json) and streaming data.
Experience on Migration of Data Warehouse from on premises to cloud(Snowflake preferred).
Exposure to version control (GitLab, GitHub, BitBucket) and CI/CD (Bamboo) tools and methodologies
Experience writing and troubleshooting complex SQL queries.
Experience working with Fivetran.
Best Regards,
Role Lead Snowflake Data Engineer
Location: Rosemont, IL (Initial Remote)
Mandatory Skills AWS, Snowflake, Informatica, Python
Experience working on Snowflake.
Experience working on Informatica Cloud (IICS).
Experience working on AWS data environment S3, EMR, Lambda, SQS etc.
Strong background in developing technical solutions for Cloud data warehouse and data lakes.
Exposure to Python is a plus.
Experience designing and troubleshooting complicated data extraction routines and real time / batch integrations.
Experience extracting and processing data from multiple data sources such as: Databases (Exadata, Oracle, Snowflake), Applications, Web Services (SOAP/REST), flat files (*.csv, *.txt, *.xml, *.json) and streaming data.
Experience on Migration of Data Warehouse from on premises to cloud(Snowflake preferred).
Exposure to version control (GitLab, GitHub, BitBucket) and CI/CD (Bamboo) tools and methodologies
Experience writing and troubleshooting complex SQL queries.
Experience working with Fivetran.
Show more
Show less","Snowflake, AWS, Informatica, Python, SQL, GitLab, GitHub, BitBucket, Bamboo, ETL, data warehouse, data lake, cloud computing, data extraction, data processing, data migration, version control, CI/CD","snowflake, aws, informatica, python, sql, gitlab, github, bitbucket, bamboo, etl, data warehouse, data lake, cloud computing, data extraction, data processing, data migration, version control, cicd","aws, bamboo, bitbucket, cicd, cloud computing, data extraction, data lake, data migration, data processing, datawarehouse, etl, github, gitlab, informatica, python, snowflake, sql, version control"
Sr. Data Engineer,Cardio Diagnostics Holdings Inc.,"Chicago, IL",https://www.linkedin.com/jobs/view/sr-data-engineer-at-cardio-diagnostics-holdings-inc-3745288489,2023-12-17,La Grange,United States,Mid senior,Onsite,"Sr. Data Engineer
About Cardio Diagnostics:
Cardio Diagnostics is a precision cardiovascular medicine company dedicated to making cardiovascular disease prevention and early detection more accessible, personalized, and precise. Our mission is to further develop and commercialize our proprietary Artificial Intelligence (AI)-driven Integrated Epigenetic-Genetic Engine™ (“Core Technology”) for cardiovascular disease, positioning ourselves as one of the leading medical technology companies enabling improved prevention, early detection, diagnostics and assisting in the treatment of cardiovascular disease.
Key Responsibilities:
Design, build and implement scalable data pipelines solutions to bolster research initiatives and product development.
Collaborate with machine learning engineers, data scientists, and other stakeholders to understand data needs and deliver comprehensive technical solutions.
Optimize data retrieval processes for end-users, enabling efficient access and manipulation of multi-model data for dashboards and reports.
Implement data quality checks, ensuring the accuracy and consistency of data sets used for genetic analysis.
Stay abreast of new developments in genomics and bioinformatics, incorporating novel data formats and sources as appropriate.
Ensure compliance with regulatory standards regarding data privacy and security, particularly relating to patient information.
Requirements:
Bachelor’s degree from an accredited college/university in Computer Science, Engineering, or related fields
Proven experience as a data engineer or in a similar role, with direct involvement in managing extensive and complex data sets.
Proficiency with big data technologies (e.g., Hadoop, Spark), database frameworks (e.g., SQL, NoSQL), and cloud services (e.g., AWS, GCP).
Experience in data pipeline and workflow management tools (e.g., Apache Airflow, MLflow, Databrikcs, SageMaker).
Proficiency in data engineering and data visualization tools/languages such as Python, SQL, Data Lake, Tableau
Experience working in a cross-functional agile team with end-to-end responsibility for product development and delivery.
Strong communication skills, capable of elaborating on complex technicalities to a diverse audience.
Highly self-motivated and directed individual with a strong work ethic.
Commitment to Continuous Learning: Embrace a growth mindset, consistently seeking knowledge and staying current with industry trends and best practices.
Show more
Show less","Python, SQL, Hadoop, Spark, NoSQL, AWS, GCP, Apache Airflow, MLflow, Databrikcs, Data Lake, Tableau, Data pipeline management, Workflow management, Genomics, Bioinformatics, Genetic analysis, Data visualization, Data quality checks, Data privacy, Data security","python, sql, hadoop, spark, nosql, aws, gcp, apache airflow, mlflow, databrikcs, data lake, tableau, data pipeline management, workflow management, genomics, bioinformatics, genetic analysis, data visualization, data quality checks, data privacy, data security","apache airflow, aws, bioinformatics, data lake, data pipeline management, data privacy, data quality checks, data security, databrikcs, gcp, genetic analysis, genomics, hadoop, mlflow, nosql, python, spark, sql, tableau, visualization, workflow management"
Snowflake Data Engineer,Surge Technology Solutions Inc,"Chicago, IL",https://www.linkedin.com/jobs/view/snowflake-data-engineer-at-surge-technology-solutions-inc-3730154261,2023-12-17,La Grange,United States,Mid senior,Onsite,"Emp Type:
W2 or 1099........ (No C2C)
Visa:
H1B, H4EAD, GCEAD, L2, Green Card, US Citizens (Only USA Applicants)
Workplace Type:
Remote
Experience:
7+ Yrs
Position’s Contributions To Work Group
This candidate will focus on data integrations and mapping between Snowflake & ThoughtSpot.
Candidate will be responsible for evaluating existing dashboards & data, mapping/remapping new data to fit those dashboards, and creating/optimizing views in Snowflake.
In addition, this role may interface with Dealers or Business
Typical Task Breakdown
This position can be remote, but expected to work in US time zone.
Candidate will work closely with the DataOps team lead to understand the current use-cases, establish a plan for migration to Snowflake data sources, and be autonomous in delivery of that work.
15% requirements gathering, 15% meetings & collaborative work, 70% technical work in Snowflake & ThoughtSpot
Education & Experience Required
4-year degree in Computer Science or equivalent experience plus 5-7 years exp. (additional experience acceptable in lieu of education)
Technical Skills
Snowflake (creating/modifying/optimizing views & procs)
ThoughtSpot (Data Visualization)
Data Mapping/Modeling
(Desired)
Familiarity with Python
Experience in handling dealer/customer data
Soft Skills (Required)
Will communicate with business unit partners, Dealers, and internal technical teams so solid communication & presentation skills are important.
Please forward your resume and contact details to vahini_b@surgetechinc.com / sahithi_s@surgetechinc.com or can call on 832-990-6448.
Show more
Show less","Snowflake, ThoughtSpot, Data Visualization, Data Mapping, Data Modeling, Python, Data Integration","snowflake, thoughtspot, data visualization, data mapping, data modeling, python, data integration","data integration, data mapping, datamodeling, python, snowflake, thoughtspot, visualization"
Senior Data Engineer,Zortech Solutions,"Chicago, IL",https://www.linkedin.com/jobs/view/senior-data-engineer-at-zortech-solutions-3667477823,2023-12-17,La Grange,United States,Mid senior,Onsite,"Senior Data Engineer(Sample Resume Attached)
Chicago, IL
ONSITE FROM DAY 1
We need STRONG Database candidate who has good experience in Databricks + Python Programming + Azure Devops Pipeline (CI/CD)
Senior-level software engineer or data engineer with experience developing and deploying big data analytics applications using Databricks on the AZURE platform
Duties
Contributing member of a team developing a Data Analytics system using Databricks
Contribute to the design and optimization of a Data Analytics system using Databricks on the Azure Platform.
Contribute to the development of the system, including coding algorithms developed by the data scientists using Apache Spark, and participating in the team's code reviews
Contribute to the design and implementation of the CI/CD system
Skills
Experience in Databricks on Azure Platform
Experience in Apache SPARK
Experience with PostgreSQL a plus
Show more
Show less","Databricks, Python Programming, Azure DevOps Pipeline, Apache Spark, PostgreSQL","databricks, python programming, azure devops pipeline, apache spark, postgresql","apache spark, azure devops pipeline, databricks, postgresql, python programming"
Sr. Data Engineer (FT),"Request Technology, LLC","Chicago, IL",https://www.linkedin.com/jobs/view/sr-data-engineer-ft-at-request-technology-llc-3783143172,2023-12-17,La Grange,United States,Mid senior,Onsite,"***Hybrid, 3 days onsite, 2 days remote***
***We are unable to sponsor as this is a permanent full-time role***
A prestigious company is looking for a Senior Data Engineer. This engineer will focus on data motels, ETL, ELT, and data engineering. They will focus on implementing data warehouse, data mart, and data lakes while working with Azure cloud data tools.
Responsibilities
This engineer will focus on data integration solutions in terms of design, build and deployment, DevOps with best-in-class data models, data quality and data architecture standards
Possesses strong data capabilities in terms of data analysis, data models, and hands on expertise in crafting and deploying data pipes using Azure data platform and tools, as well as enterprise ETL tool Talend, leveraging its DQ, DI and Data Catalogue features.
Accountable for the technical leadership regarding the data integration solutions and delivery. Ensuring a sound and best in class design, with enterprise implementation, deployment and operational meets the technical quality standards
Responsible for planning and coordinating in carving out the needed dev/test environments, as well as defining and managing code branching/config strategies supporting concurrent releases
Works with Data and Enterprise architecture team to define the data integration design/coding/deployment/operational standards and technology stack
Responsible for data operations, in terms of scheduling, successful execution, and reconciliation of the data pipes in production
Qualifications & Requirements
Bachelor’s degree in data, computer science or relevant discipline.
8+ years of experience in ETL, ELT and data engineering
At least 3+ years of working experience on Azure data platforms
Experience working in agile delivery, Jira usage and other agile delivery best practices
Data architecture, Data Modeling, and data visualization experience is a plus
Ability to interact with business, other teams to create data mapping documents, ETL architecture/design artifacts, performance improvements, improve delivery & operational excellence.
8+ years of end-to-end implementation experience of deploying enterprise data warehouse, data mart and data lake solutions
3+ years of working experience with Azure data solutions including but not limited to ADLS, Data Bricks, ADF, Synapse etc
Azure ADLS/Databricks administration experience
5+ years of Implementation and maintenance experience with Talend DI, DQ capabilities
Demonstrable understanding of Data Governance, and enabling technical tools and technologies
Show more
Show less","Data Engineering, ETL, ELT, Data Lakes, Data Warehouse, Data Mart, Azure, Cloud Data Tools, Data Integration, Data Models, Data Quality, Data Architecture, Data Analysis, Azure Data Platform, Talend, Jira, Data Visualization, Data Mapping, ETL Architecture, Performance Improvements, Operational Excellence, Enterprise Data Warehouse, Data Bricks, ADF, Synapse, ADLS, Data Governance","data engineering, etl, elt, data lakes, data warehouse, data mart, azure, cloud data tools, data integration, data models, data quality, data architecture, data analysis, azure data platform, talend, jira, data visualization, data mapping, etl architecture, performance improvements, operational excellence, enterprise data warehouse, data bricks, adf, synapse, adls, data governance","adf, adls, azure, azure data platform, cloud data tools, data architecture, data bricks, data engineering, data governance, data integration, data lakes, data mapping, data mart, data models, data quality, dataanalytics, datawarehouse, elt, enterprise data warehouse, etl, etl architecture, jira, operational excellence, performance improvements, synapse, talend, visualization"
Staff Data Engineer,Kin Insurance,"Chicago, IL",https://www.linkedin.com/jobs/view/staff-data-engineer-at-kin-insurance-3763328086,2023-12-17,La Grange,United States,Mid senior,Onsite,"The world has changed. Why hasn’t insurance?
Kin’s mission is to reimagine home insurance
For Every New Normal.
While other insurers struggle to handle a fast-changing world, Kin is built for the future and is prepared to meet its challenges head on while helping our customers do the same.
Kin is proud to be one of BuiltIn Chicago’s 2021 and 2022 Best Mid Sized Companies to work for, and Forbes 2021 Best Startup Employers in North America. Simply put, our people are what make us great, and we need forward-thinking, inspired game-changers like you to join us in our mission.
So, what’s the role?
Data is central to Kin’s operations and success. As a staff data engineer, you will take technical lead of a data management team that supports and enables our product, operations, analytics, and data science teams, amongst others. You will be responsible for your team’s effective technical operation and successful delivery of projects. As we scale, you will be integral in how we manage, structure, and store our data, as well as develop new solutions related to data architecture and ETL pipelines.
A day in the life could include:
Creating, designing, and maintaining ETL pipelines
Working with data science and BI teams to create data sets to be used in various projects
Owning the more advanced design and architectural decisions in the lifecycle of our data processing
Participating in recurring scrum events
Collaborating with cross-functional team members
Providing subject matter expertise and support
I’ve got the skills… but do I have the necessary ones?
7+ years of data engineering experience
Strong coding skills in one or more of the following: Bash, Python, and/or Ruby
Experience with the entire ETL pipeline: Data Integration tools, Databases, Big Data Platforms, and cloud based data platforms
Supporting data visualization tools, such as Looker, Tableau and Microstrategy
Experience in building from the ground up a modern next generation data warehouse platform
Advanced proficiency with SQL or PL/SQL optimization and development
Experience working with AWS-based Data Platforms is preferred
Deep understanding of data architecture as it relates to business goals and objectives
Oh, and don’t worry, we’ve got you covered!
Medical, Dental, Vision, Disability and Life Insurance
Flexible PTO policy
Remote work
Generous equity package
401K with company match
Parental leave
Continuing education and professional development
The excitement of joining a high-growth Insurtech company and seeing your work make an impact
About Kin
In an industry that hasn't budged in more than 100 years, our technology transforms the user experience, cuts inefficiencies that waste billions of consumer dollars, and customizes coverage homeowners want. We believe insurance was always meant to be a digital product – we’re making that a reality.
Our approach to the industry makes us unique, and the people at Kin help us excel. We’re a team of problem solvers, collaborators, builders, and dreamers who are passionate about creating positive change in the lives of our customers and in our industry. Kin is more than just our name – it’s how we treat each other. That’s one of the many reasons we’ve been recognized as a great place to work by Built In, Forbes, and Fast Company.
EEOC Statement
Kin is proud to be an Equal Employment Opportunity and Affirmative Action Employer. We don't just accept difference – we honor it, nurture it, and celebrate it. We don’t discriminate based on race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.
Kin encourages applications from all backgrounds, communities and industries, and are committed to having a team that is made up of diverse skills, experiences and abilities.
Remote
Show more
Show less","Data engineering, Bash, Python, Ruby, ETL pipelines, Data integration tools, Databases, Big data platforms, Cloudbased data platforms, Data visualization tools, Looker, Tableau, Microstrategy, Modern nextgeneration data warehouse platform, SQL, PL/SQL, AWSbased data platforms, Data architecture, Scrum events, AWS","data engineering, bash, python, ruby, etl pipelines, data integration tools, databases, big data platforms, cloudbased data platforms, data visualization tools, looker, tableau, microstrategy, modern nextgeneration data warehouse platform, sql, plsql, awsbased data platforms, data architecture, scrum events, aws","aws, awsbased data platforms, bash, big data platforms, cloudbased data platforms, data architecture, data engineering, data integration tools, data visualization tools, databases, etl pipelines, looker, microstrategy, modern nextgeneration data warehouse platform, plsql, python, ruby, scrum events, sql, tableau"
Entry level / Junior Level Data Analyst(Remote),SynergisticIT,"Chicago, IL",https://www.linkedin.com/jobs/view/entry-level-junior-level-data-analyst-remote-at-synergisticit-3767595033,2023-12-17,La Grange,United States,Mid senior,Onsite,"SYNERGISTICIT is aware that the Job Market is Challenging because of Tech Layoffs. The Job market is flooded with hundreds and thousands of laid-off Jobseekers competing with existing Jobseekers. For entry-level Job seekers to get client interviews and jobs they need to differentiate themselves by ensuring they have exceptional skills and technologies to be noticed by clients.
Since 2010 we have helped Jobseekers differentiate themselves by providing clients with candidates who have the requisite skills and experience to outperform at interviews and clients. Here at SynergisticIT We just don't focus on getting you a Job we make careers. All Positions are open for all visas and US citizens We at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped thousands of candidates get jobs at technology clients like Apple, Google, and Paypal, western union, Client, visa, Walmart labs, etc., to name a few. We have an excellent reputation with the clients. Currently, we are looking for (Remote) Entry/ Junior Level Data Analyst
Who Should Apply Recent Computer Science/Engineering /Mathematics/Statistics or Science Graduates looking to make their careers in IT Industry We welcome candidates with all visas and citizens to apply. We assist in filing for STEM extension and also for H1b and Green card filing to Candidates
We also offer Skill enhancement programs for candidates who are either missing skills or lacking Industry/Client experience with Projects and skills. Candidates have difficulty finding jobs or cracking interviews or want to improve their skill portfolio. If they are qualified with enough skills and have hands-on project work with clients then no need for skill enhancement.
Candidates can benefit from skill enhancement if they fall into the below categories.
Candidates who Lack Experience or are fresher with No actual Job experience with projects with clients who Have had a break in careers Lack Technical Competency or skills being demanded by clients Different visa candidates (Like OPT/H4EAD/L2EAD) who want to get employed and settle down in the USA please check the below links to see success outcomes of our candidates https://www.synergisticit.com/candidate-outcomes/
Watch the below videos of us participating in Industry events with the Top companies in Technology at Oracle Cloud World/Oracle Java One (Las Vegas) and at Gartner Data Analytics Summit (Florida)
Oracle Cloud World Event (OCW) Las Vegas 2022 | SynergisticIT - YouTube https://www.youtube.com/watch?v=OAFOhcGy9Z8
https://www.youtube.com/watch?v=EmO7NrWHkLM https://www.youtube.com/watch?v=NVBU9RYZ6UI
https://www.youtube.com/watch?v=Yy74yvjatVg SynergisticIT at Gartner Data and Analytics Summit 2023 - YouTube
https://www.synergisticit.com/java-track/ https://www.synergisticit.com/data-science-track/ https://www.synergisticit.com/contact-us/
If you have relevant skills and industry experience, please apply if not then candidates can opt for Skill enhancement.
For getting help with interviews please visit
https://www.synergisticit.com/interview-questions/
REQUIRED SKILLS For Java /Software Programmers
Bachelor's degree or Master's degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT
Highly motivated, self-learner, and technically inquisitive
Experience in programming language Java and understanding of the software development life cycle
Project work on the skills
Knowledge of Core Java, java script, C++ or software programming
Spring boot, Micro-services, Docker, Jenkins and REST API's experience
Excellent written and verbal communication skills
For data Science/Machine learning
Required Skills
Bachelor's degree or Master's degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT
Project work on the technologies needed
Highly motivated, self-learner, and technically inquisitive
Experience in the programming language Java and understanding of the software development life cycle
Knowledge of Statistics, SAS, Python, Computer Vision, and data visualization tools
Excellent written and verbal communication skills
Preferred skills: NLP, Text mining, Tableau, Power BI, Time series analysis
Please understand skills and relevant experience on real-world projects are required by clients for selection even if it's a Junior or entry-level position the additional skills and Project work with hands-on experience building projects at the client site are the only way a candidate can be picked by clients. If not having the skills or hands-on project work at the client site, candidates can opt for skill enhancement to gain the required skills and project work. No third-party candidates or c2c candidates
please only apply to the posting
Show more
Show less","Java, Spring Boot, Docker, Jenkins, REST API, Data Visualization, Statistics, SAS, Python, Computer Vision, NLP, Text Mining, Tableau, Power BI, Time Series Analysis","java, spring boot, docker, jenkins, rest api, data visualization, statistics, sas, python, computer vision, nlp, text mining, tableau, power bi, time series analysis","computer vision, docker, java, jenkins, nlp, powerbi, python, rest api, sas, spring boot, statistics, tableau, text mining, time series analysis, visualization"
Senior Cloud Data Engineer DWH & BI (m/f/t),Energy Jobline,"Chicago, IL",https://www.linkedin.com/jobs/view/senior-cloud-data-engineer-dwh-bi-m-f-t-at-energy-jobline-3773709633,2023-12-17,La Grange,United States,Mid senior,Onsite,"RWE Supply & Trading Americas, LLC
To start as soon as possible, full time, permanent
Your Future Plans
We are supporting our Risk business with all reporting requirements they have. To ensure proper reporting we are using the Microstrategy BI Suite as well as making use of Power BI for our Credit Risk reporting. Besides the risk reporting focus we also develop our own applications to support special requirements like PnL or Counterparty risk calculations, using a wide range of interesting technologies, like .Net core, Snowflake, AWS Cloud services, Python and more.
Design, develop, and maintain ELT/ETL processing pipelines for batch and near-real time data in Snowflake/AWS
Design and set up accurate data solutions to combine all required data and calculate risk reporting metrics as per customer requirements
Strong experience with professional Python developments
Work experience in BI development and testing of new requirements within the Microstrategy BI tool
Experience in providing support to business colleagues on questions regarding various IT platforms
Work closely within an agile international IT team (Germany, UK, Slovakia)
Ensure implementation and adherence to coding standards and governance guidelines
Be able to explain, and demonstrate experience of applying various problem solving and decision making techniques
Your powerful skills
University Degree preferably in Information Systems or Computer Science
Extensive working knowledge in Snowflake, AWS and general application development
Good communication techniques and self-organising skills
Working calm under high pressure and taking ownership on managing multiple user requests
Open to travel at least once a year to Europe strengthen collaboration within the team and cross stakeholders
Advantageous, but not a must
The ability to challenge team and stakeholders
Working experience with Microsoft .Net Environment and SQL-Server is beneficial
Benefits you can rely on
Working within a highly motivated international team where continues development and feedback culture are actively lived.
RWE is massively expanding in the area of renewable energy and offers a great variety of career opportunities.
At RWE, innovation takes place every day. Therefore we want you to create your own ideas to support our company goals.
We offer enormous development opportunities via training on the job, with pair-programming, mentoring, coaching and by sending you on conferences.
You will find a modern work place with flexible working hours, an attractive salary and great social benefits, which are really valuing your personal engagement.
rwe.com/career
Apply now with just a few clicks
Any questions? Patrycja Bartela (Recruiting), +1 312 316 9486 ad code 83309
We look forward to meeting you!
We value diversity and therefore welcome all applications, irrespective of gender, disability, nationality, ethnic and social background, religion and beliefs, age or sexual orientation and identity. #inclusionmatters
RWE Supply & Trading is the interface between RWE and the energy markets around the world. Approximately 1,700 employees from over 50 different countries trade (renewable) electricity, (green) gas, commodities and CO2 emission allowances. The trading entity also ensures the commercial optimisation of RWE’s power plant dispatch and markets electricity from renewables
Show more
Show less","Microstrategy BI Suite, Power BI, .Net Core, Snowflake, AWS Cloud services, Python, ELT/ETL, SQL, Microstrategy, Cloud services, Python developments, BI development, agile, coding standards, problem solving, decision making, Information Systems, Computer Science, AWS, Microsoft .Net Environment, SQLServer, pairprogramming, mentoring, coaching, conferences","microstrategy bi suite, power bi, net core, snowflake, aws cloud services, python, eltetl, sql, microstrategy, cloud services, python developments, bi development, agile, coding standards, problem solving, decision making, information systems, computer science, aws, microsoft net environment, sqlserver, pairprogramming, mentoring, coaching, conferences","agile, aws, aws cloud services, bi development, cloud services, coaching, coding standards, computer science, conferences, decision making, eltetl, information systems, mentoring, microsoft net environment, microstrategy, microstrategy bi suite, net core, pairprogramming, powerbi, problem solving, python, python developments, snowflake, sql, sqlserver"
Big Data/Machine Learning Engineer - Sr,TekWissen ®,"Chicago, IL",https://www.linkedin.com/jobs/view/big-data-machine-learning-engineer-sr-at-tekwissen-%C2%AE-3617896224,2023-12-17,La Grange,United States,Mid senior,Onsite,"Overview
Tek Wissen Group is a workforce management provider throughout the USA and many other countries in the world Our client is a Financial Corporation is a diversified financial service holding company specializing in credit cards, home loans, auto loans, and banking and savings products The client operates in 3 segments: Credit Card, Consumer Banking, and Commercial Banking.)
Position: Big Data/Machine Learning Engineer - Sr
Location: Chicago IL, 60610
Duration: 3 Months
Job Type: Contract
Work Type: Remote till covid
Job Description
Work on 5 domains
ETL - data storage, sharing, vending, ingesting and processing
Cyber security - IAM policies, access and encryption
Governance
Regulations and initiatives
Stocks auditing
Eg: CCPA, CPRA
Data collection from external vendors
DataLake, data streaming and warehousing
SQL queries, processing, data modeling, develop arrays
Provide day-to-day support to the team
Tech Stack
Spark
Scala
AWS most services, specifically security
SQL
DataLake
Data Warehousing
Nice to have
Python
Streaming
People Skills
Communication
Independent contributor
Design, guide and mentor other resources
TekWissen Group is an equal opportunity/affirmative action Employer (m/f/d/v) supporting workforce diversity.
Show more
Show less","Big Data, Machine Learning, ETL, Data Storage, Data Sharing, Data Vending, Data Ingesting, Data Processing, Cyber Security, IAM, Access Control, Encryption, Governance, Regulations, Initiatives, Data Collection, DataLake, Data Streaming, Data Warehousing, SQL Queries, Data Modeling, Array Development, AWS Services, Python, Streaming, Communication, Independent Contribution, Design, Guidance, Mentoring","big data, machine learning, etl, data storage, data sharing, data vending, data ingesting, data processing, cyber security, iam, access control, encryption, governance, regulations, initiatives, data collection, datalake, data streaming, data warehousing, sql queries, data modeling, array development, aws services, python, streaming, communication, independent contribution, design, guidance, mentoring","access control, array development, aws services, big data, communication, cyber security, data collection, data ingesting, data processing, data sharing, data storage, data streaming, data vending, datalake, datamodeling, datawarehouse, design, encryption, etl, governance, guidance, iam, independent contribution, initiatives, machine learning, mentoring, python, regulations, sql queries, streaming"
"SR. Scala Engineer, Database Engineering",Experfy,"Chicago, IL",https://www.linkedin.com/jobs/view/sr-scala-engineer-database-engineering-at-experfy-3646112617,2023-12-17,La Grange,United States,Mid senior,Onsite,"As a Sr. Software Engineer for our Data Platform Engineering team you will join skilled Scala engineers and core database developers responsible for developing hosted cloud analytics infrastructure (Apache Spark-based), distributed SQL processing frameworks, proprietary data science platforms, and core database optimization. This team is responsible for building the automated, intelligent, and highly performant query planner and execution engines, RPC calls between data warehouse clusters, shared secondary cold storage, etc. This includes building new SQL features and customer-facing functionality, developing novel query optimization techniques for industry-leading performance, and building a database system that's highly parallel, efficient and fault-tolerant. This is a vital role reporting to exec leadership and senior engineering leadership.
Requirements
Writing Scala code with tools like Apache Spark + Apache Arrow to build a hosted, multi-cluster data warehouse for Web3
Developing database optimizers, query planners, query and data routing mechanisms, cluster-to-cluster communication, and workload management techniques
Scaling up from proof of concept to ""cluster scale"" (and eventually hundreds of clusters with hundreds of terabytes each), in terms of both infrastructure/architecture and problem structure
Codifying best practices for future reuse in the form of accessible, reusable patterns, templates, and code bases to facilitate meta data capturing and management
Managing a team of software engineers writing new code to build a bigger, better, faster, more optimized HTAP database (using Apache Spark, Apache Arrow and a wealth of other open source data tools)
Interacting with exec team and senior engineering leadership to define, prioritize, and ensure smooth deployments with other operational components
Highly engaged with industry trends within analytics domain from a data acquisition processing, engineering, management perspective
Understand data and analytics use cases across Web3 / blockchains
Skills & Qualifications
Bachelor's degree in computer science or related technical field. Masters or PhD a plus
6+ years experience engineering software and data platforms / enterprise-scale data warehouses, preferably with knowledge of open source Apache stack (especially Apache Spark, Apache Arrow, and others)
3+ years experience with Scala and Apache Spark
A track record of recruiting and leading technical teams in a demanding talent market
Rock solid engineering fundamentals; query planning, optimizing and distributed data warehouse systems experience is preferred but not required
Nice to have: Knowledge of blockchain indexing, web3 compute paradigms, Proofs and consensus mechanisms... is a strong plus but not required
Experience with rapid development cycles in a web-based environment
Strong scripting and test automation knowledge
Nice to have: Passionate about Web3, blockchain, decentralization, and a base understanding of how data/analytics plays into this
Show more
Show less","Scala, Apache Spark, Apache Arrow, SQL, Database optimization, Query planning, Query optimization, Distributed data warehouse systems, RPC calls, Shared secondary cold storage, Faulttolerant systems, Web3, Blockchain indexing, Web3 compute paradigms, Proofs, Consensus mechanisms, Rapid development cycles, Scripting, Test automation","scala, apache spark, apache arrow, sql, database optimization, query planning, query optimization, distributed data warehouse systems, rpc calls, shared secondary cold storage, faulttolerant systems, web3, blockchain indexing, web3 compute paradigms, proofs, consensus mechanisms, rapid development cycles, scripting, test automation","apache arrow, apache spark, blockchain indexing, consensus mechanisms, database optimization, distributed data warehouse systems, faulttolerant systems, proofs, query optimization, query planning, rapid development cycles, rpc calls, scala, scripting, shared secondary cold storage, sql, test automation, web3, web3 compute paradigms"
Senior Data Engineer,Forsta,"Chicago, IL",https://www.linkedin.com/jobs/view/senior-data-engineer-at-forsta-3775906159,2023-12-17,La Grange,United States,Mid senior,Remote,"Company Description
Press Ganey
, the leading Human Experience (HX) healthcare performance improvement company, offers an integrated suite of solutions that address safety, clinical excellence, patient experience and workforce engagement. The company works with more than 41,000 healthcare facilities in its mission to reduce patient suffering and enhance caregiver resilience to improve the overall safety, quality, and experience of care. Press Ganey is a PG Forsta company.
Job Description
Position Description/Overview
Do you have deep healthcare experience? Do you love understanding strategic direction, creating design and direction, then getting fingers on the keyboard? This role is for a Senior Data Engineer. The primary responsibilities of this position are to design, develop, support, and maintain HL7/FHiR messaging applications. Other responsibilities include communicating effectively with product management to determine the software application requirements, create and effectively execute the development plan.
Duties & Responsibilities
General
Gather requirements and create solutions healthcare data transformation using Corepoint, Rhapsody, Redox or other interface engine technologies.
Create data architecture, pipelines, and analytical solutions to meet software and data science requirements for various PG Healthcare Products
Work with Product, Technical Program Manager, Clients, and other tech leads to architect solutions.
Create and execute Proofs of Concept and Proofs of Technology
Perform Systems Analysis and Design, prepare functional specifications and technical specifications for small applications or systems.
Ensures compliance and recommends change, if necessary, in standards, policies or processes.
Develops unit test plans for software applications.
Data Storage Design
Collaborate with software engineering, business teams, and data scientists to establish data storage, pipeline, and structure requirements.
Identify and plan for data storage performance requirements.
Application Interface and Data Storage Implementation
Collaborate with software engineering, business teams, and data scientists to create and execute implementations.
Identify impact of implementation on other applications and databases
Lead and mentor data engineers on data projects
Support Software Applications
Troubleshoots problems identified by users.
Creates installation, troubleshooting, and end user documentation.
Provides support and follow-up for issues.
Supports the full cycle of software applications including design, build, package, deploy, troubleshoot, and bug fix.
Follows an Agile Software Development Life Cycle that delivers incremental value to the business owner.
Qualifications
10 or more years’ experience as an Engineer in a healthcare environment.
5+ years of recent experience building software solutions with Corepoint, Rhapsody, Redox or other interface engines.
5+ years of recent experience building solutions in cloud technologies (Azure, AWS or GCP)
5+ years working with HL7, FHiR, and other EHR integration technologies.
Ability to learn quickly and work independently or as part of a team.
Strong problem solving and communication skills.
Provide technical guidance and mentorship to the team of software developers.
Experience with:
REST APIs
Continuous Integration/Continuous Deployment (CI/CD)
Docker and Terraform
Python
HAPI or other FHiR server
Minimum Qualifications
Minimum of 5 years Data Engineering experience in an enterprise environment.
Minimum of 5 years solution design/build experience using Corepoint, Rhapsody, Redox or other interface engines.
Bachelor’s degree in technology or like field required.
Additional Information
All positions at Press Ganey require an applicant who has accepted an offer to undergo a background check. The specific checks are based on the nature of the position. Background checks may include some or all of the following: SSN/SIN validation, education verification, employment verification, and criminal check, search against global sanctions and government watch lists, fingerprint verification, credit check, and/or drug test. By applying for a position with Press Ganey, you understand that you will be required to undergo a background check should you be made an offer. You also understand that the offer is contingent upon successful completion of the background check and results consistent with Press Ganey's employment policies. You will be notified during the hiring process which checks are required for the position.
Press Ganey Associates LLC is an Equal Employment Opportunity/Affirmative Action employer and well committed to a diverse workforce. We do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity, veteran status, and basis of disability or any other federal, state or local protected class.
Pay Transparency Non-Discrimination Notice – Press Ganey will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the contractor's legal duty to furnish information.
The expected base salary for this position ranges from $130,000 to $150,000. It is not typical for offers to be made at or near the top of the range. Salary offers are based on a wide range of factors including relevant skills, training, experience, education, and, where applicable, licensure or certifications obtained. Market and organizational factors are also considered. In addition to base salary and a competitive benefits package, successful candidates are eligible to receive a discretionary bonus or commission tied to achieved results.
All your information will be kept confidential according to EEO guidelines.
Our privacy policy can be found here: https://www.pressganey.com/legal-privacy/
Show more
Show less","Healthcare, HL7/FHiR, Corepoint, Rhapsody, Redox, Data architecture, Data science, Cloud technologies, Azure, AWS, GCP, EHR, REST APIs, Continuous Integration/Continuous Deployment (CI/CD), Docker, Terraform, Python, HAPI, Data Engineering, Solution design/build, Bachelor's degree in technology, Agile Software Development Life Cycle","healthcare, hl7fhir, corepoint, rhapsody, redox, data architecture, data science, cloud technologies, azure, aws, gcp, ehr, rest apis, continuous integrationcontinuous deployment cicd, docker, terraform, python, hapi, data engineering, solution designbuild, bachelors degree in technology, agile software development life cycle","agile software development life cycle, aws, azure, bachelors degree in technology, cloud technologies, continuous integrationcontinuous deployment cicd, corepoint, data architecture, data engineering, data science, docker, ehr, gcp, hapi, healthcare, hl7fhir, python, redox, rest apis, rhapsody, solution designbuild, terraform"
Senior Big Data Engineer (Remote),Cambridge Technology (CT),"Chicago, IL",https://www.linkedin.com/jobs/view/senior-big-data-engineer-remote-at-cambridge-technology-ct-3598176935,2023-12-17,La Grange,United States,Mid senior,Remote,"Join the Cambridge Technology team and grow your career. We are solving real-world problems with creative innovation at some of the most recognizable corporations in the world.
From building entire infrastructures or platforms to solving complex IT challenges, we help businesses accelerate their digital transformation and become AI-first businesses. With over 20 years of expertise as a technology services company, we enable our customers to stay ahead of the curve by helping them figure out the perfect approach, solutions, and ecosystem for their business. Our experts help customers leverage the right AI, big data, cloud solutions, and intelligent platforms that will help them become and stay relevant in a rapidly changing world.
We are seeking a skilled and experienced Big Data Engineer to join our team. As a Big Data engineer, you will be expected to work closely with our Business and ETL team to implement all Data processing procedures for all new projects and maintain effective awareness of all production activities according to required standards and provide support to all existing applications. Also, you will be expected to work with the Data Architect(s) to help drive architecture and design approaches that result in implemented business solutions.
Big Data Engineer Duties & Responsibilities:
Work with Big data development team, business stakeholders, DBAs, system administrators, data
modeling teams for Data pipeline build and design.
Translate business requirements into technical requirements.
Understanding of data quality methodologies and data governance to ensure
standardization of data is maintained.
Contribute to the requirements analysis process, associate in creating architecture and design
documents.
Design, develop and test data flows using spark/ pyspark.
Perform data science work for analytics.
Determine optimal approach for obtaining data from diverse source systems.
Be a key contributor to initiatives that require technical expertise.
Work closely with the team responsible for maintaining the data model, including
data dictionary/metadata registry.
Possess expertise in project migration and release coordination activities.
Interface with business stakeholders to understand requirements and offer solutions.
Skill Set and Qualification:
At least six (6) years of experience in working with Big data tools and technology.
Experience with Hadoop, Python, Spark/Pyspark, Hive and Tableau/Heavy.ai.
Strong exposure working with web service sources/Targets, XML Sources and Restful API’s.
Experience with Linux / shell scripting complementary with ETL tool.
At least two (2) years of experience in performance tuning.
Exposure working with relation databases Oracle, My SQL & SQL Server including complex SQL constructs and DDL generation.
Experience in managing and deploying applications in Cloud Platform.
Strong knowledge of relational databases and experience with SQL scripting / stored procedures in PL/SQL.
Experience with data modeling and data mapping design.
Bachelor’s degree in Computer Science, Information Systems, or equivalent education or work experience.
Only considering US based applicants.
Show more
Show less","Big Data, AI, Cloud Solutions, Python, Spark, Pyspark, Hive, Tableau, Heavy.ai, Hadoop, XML, Restful API, Linux, Shell Scripting, ETL, SQL, PL/SQL, Data Modeling, Data Mapping","big data, ai, cloud solutions, python, spark, pyspark, hive, tableau, heavyai, hadoop, xml, restful api, linux, shell scripting, etl, sql, plsql, data modeling, data mapping","ai, big data, cloud solutions, data mapping, datamodeling, etl, hadoop, heavyai, hive, linux, plsql, python, restful api, shell scripting, spark, sql, tableau, xml"
BI Data Engineer - REMOTE,Motion Recruitment,"Rolling Meadows, IL",https://www.linkedin.com/jobs/view/bi-data-engineer-remote-at-motion-recruitment-3762299212,2023-12-17,La Grange,United States,Mid senior,Remote,"Motion has partnered with a premier client in filling a full-time, fully REMOTE employee position for a BI Data Engineer. This is a great opportunity to expand your career and work with a well-known company in the greater Chicago area. Do you get excited working on Azure cloud platforms specifically ingesting data using Azure Data Factory (ADF)? Are you experienced with Snowflake, Databricks, SQL, Python, within the enterprise data warehouse environments? This position may be for you.
Required Skills & Experience
A relevant technical BS Degree in Information Technology and 5 years of relevant professional experience implementing well-architected data pipelines that are dynamically scalable, highly available, fault-tolerant, and reliable for analytics and platform solutions
3+ years of data engineering experience leveraging technologies such as Snowflake, Azure Data Factory, ADLS Gen 2, Logic Apps, Azure Functions, Databricks, Apache Spark, Scala, Synapse, SQL Server
Understanding the pros and cons, and best practices of implementing Data Lake, using Microsoft Azure Data Lake Storage
Experience structuring Data Lake for the reliability, security and performance
5 years writing SQL, TSQL queries against any RDBMS with query optimization and performance tuning
Experience implementing ETL for Data Warehouse and Business intelligence solutions
Working experience with Python, and Power Shell Scripting
Skills to read and write effective, modular, dynamic, parameterized and robust code, establish and follow already established code standards, and ETL framework
Strong analytical, problem solving, and troubleshooting abilities, experience performing root cause analysis
Good understanding of unit testing, software change management, and software release management
Experience working within an agile team, In-depth knowledge of agile process and principles
What You Will Be Doing
Build the infrastructure required for optimal ETL/ELT pipelines to ingest data from a wide variety of data sources using Microsoft Azure technologies such as Azure Data Factory and Databricks.
Construct and maintain of enterprise level integrations using the Snowflake platform, Azure Synapse, Azure SQL and SQL Server.
Design ETL pipelines and reusable components to implement specified business requirements Troubleshoot and optimize ETL code; interpret ETL logs, perform data validation, understand the benefits and drawbacks of parallelism, proper use of expressions, scoping of variables, commonly used transforms, event handlers and logging providers, understand and optimize the surrogate key generation and inconsistent data type handling
Create data tools for data analytics and data science team members to deliver actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Conduct code reviews, performance analysis and participate in technical design
Orchestrate large, complex data sets that meet functional/non-functional business requirements.
Seek out, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability.
Partner with data and analytics talent to strive for greater functionality in our data systems.
Applicants must be currently authorized to work in the US on a full-time basis now and in the future.
Posted By:
Aaron Rontal
Show more
Show less","Azure, Azure Data Factory (ADF), Snowflake, Databricks, Apache Spark, Scala, Synapse, SQL Server, Azure Data Lake Storage, Python, Power Shell Scripting, ETL, Agile, Unit Testing, Software Change Management, Software Release Management","azure, azure data factory adf, snowflake, databricks, apache spark, scala, synapse, sql server, azure data lake storage, python, power shell scripting, etl, agile, unit testing, software change management, software release management","agile, apache spark, azure, azure data factory adf, azure data lake storage, databricks, etl, power shell scripting, python, scala, snowflake, software change management, software release management, sql server, synapse, unit testing"
Senior Data Engineer,IHI Terrasun Solutions,"Chicago, IL",https://www.linkedin.com/jobs/view/senior-data-engineer-at-ihi-terrasun-solutions-3673090223,2023-12-17,La Grange,United States,Mid senior,Remote,"At IHI Terrasun, we are at the forefront of changing the world with green energy solutions. To build on our incredible success, we need a driven, curious, and collaborative people to join our growing team.
We know our most important assets are our people, and your role will be critical to our future success.
Position Summary
This position is responsible for the development, implementation, and maintenance of data systems for both data in motion and data at rest to monitor and control battery storage systems. In this role, you'll work with setting up systems to capture hardware metrics and track their performance for outage tracking and compliance for one of the largest collections of industrial automation data worldwide.
What You'll Be Doing
Create services to receive, translate, and send data in motion to data stores via APIs and query languages.
Developing and deploying data pipelines using docker/containerization, Jenkins and Kubernetes.
Participate in the entire software development lifecycle of a product from ideation, wireframing/prototyping, development, testing, deployment, commissioning, and long-term support.
Own automated testing and deployment for products that you create.
The ideal candidate would have the following experience:
Developing in Python including libraries like pandas, the SciPy stack, request, and data tools in the context of data processing workflows, scripts, and services.
Working in every stage of the data lifecycle from converting business specifications/requirements to technical solutions through deployment, and maintenance.
Integrating software and services, including developing adaptors between different data protocols (e.g., MQTT to Prometheus exposition format)
Creating scalable and performant systems.
Analyzing messaging protocols using tools like tcpdump, WireShark, and custom data producers and consumers.
Reading, writing, and converting data formats like json, yaml, csv, pandas dataframes, and relational databases.
Working with Prometheus, PromQL, and Grafana.
Developing and deploying Docker workflows.
Working in a Linux environment including basic shell scripting and system monitoring.
Participating in Agile development including scrum ceremonies, ticket management, GIT version control, unit testing, code reviews, and documentation.
Experience in compiled languages like Go, Rust, or C++ are a plus but not required.
Qualifications
5+ years experience in data engineering
BS, MS or PhD in CS, Engineering, Math, Physical Sciences or equivalent real-world experience.
Ability to work independently, with a development team, and within large multi-team projects.
Excited to learn and grow with new technologies and adaptability to handle evolving requirements.
Has a shared sense of responsibility and ownership both in your code and the larger system that it operates in.
Ability to communicate issues openly and honestly even when difficult.
Previous Energy Storage System experience is preferred, power industry or military systems experience is a plus
Work Environment
IHI has its main office in Chicago, IL, however this position can be remotely located.  Those in the Chicago-land area have the flexible option of working from the office or remotely from home.
Limited travel may be required for company All-Hands or other meetings. Travel expectations for this position are up to 5% within the US.
The above job description identifies the essential job functions and skills needed by the person or persons assigned to this position. These job functions and skills are not intended to be a complete and exhaustive list of all responsibilities, duties and skills required. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential job functions. The information contained herein is subject to change at the company's discretion.
About IHI Terrasun Solutions:
IHI Terrasun Solutions is a subsidiary of IHI Corporation, a 165-year-old, $15 Billion organization with deep energy industry experience. IHI Terrasun Solutions is a solar + storage systems integration and lifecycle services provider with highly integrated hardware and software capabilities.
The robust software and top tier energy storage solutions are developed by the expert team at IHI Terrasun Solutions. Employees have extensive industry knowledge and experience, and enthusiastically seek to build on IHI's advanced product offerings.
To design systems, IHI Terrasun Solutions uses proprietary software that operates on the same algorithm later used to deploy the system in real-time. This end-to-end algorithm structure coupled with the support offered by a well-established parent organization enables IHI Terrasun Solutions to provide an advanced warranty to customers, reducing project risk and increasing clarity on system scheduling and deployment.
With solar + storage expertise, robust service offerings, and technology-agnostic solutions, IHI Terrasun Solutions develops efficient and streamlined systems to achieve your energy storage goals.
IHI Terrasun has over 480MWh of projects currently installed, contracted, and in construction with over 1GWh of projects in advanced phase of contracting.
Benefits:
Not only do our employees get the chance to work in a rapidly growing energy business with global impact, they also have access to some of the best benefits in the industry, including:
100% employer paid health, dental, and vision insurance for our premium Anthem Blue Cross PPO plan
401(k) plan contribution matching
Employer sponsored Life, AD&D, Short-Term and Long-Term Disability Insurance
Tuition and continuing education stipend
Fantastic employee culture
Show more
Show less","Python, Pandas, SciPy stack, Request, Data tools, Docker, Containerization, Jenkins, Kubernetes, APIs, Query languages, Data pipelines, Agile development, Scrum ceremonies, Ticket management, Git version control, Unit testing, Code reviews, Documentation, PromQL, Grafana, Prometheus, MQTT, Prometheus exposition format, Json, Yaml, CSV, Pandas dataframes, Relational databases, WireShark, Tcpdump, Go, Rust, C++","python, pandas, scipy stack, request, data tools, docker, containerization, jenkins, kubernetes, apis, query languages, data pipelines, agile development, scrum ceremonies, ticket management, git version control, unit testing, code reviews, documentation, promql, grafana, prometheus, mqtt, prometheus exposition format, json, yaml, csv, pandas dataframes, relational databases, wireshark, tcpdump, go, rust, c","agile development, apis, c, code reviews, containerization, csv, data tools, datapipeline, docker, documentation, git version control, go, grafana, jenkins, json, kubernetes, mqtt, pandas, pandas dataframes, prometheus, prometheus exposition format, promql, python, query languages, relational databases, request, rust, scipy stack, scrum ceremonies, tcpdump, ticket management, unit testing, wireshark, yaml"
Senior Azure Data Engineer - Remote | WFH,Get It Recruit - Information Technology,"Elk Grove Village, IL",https://www.linkedin.com/jobs/view/senior-azure-data-engineer-remote-wfh-at-get-it-recruit-information-technology-3774100198,2023-12-17,La Grange,United States,Mid senior,Remote,"As a skilled Azure Data Engineer, you will play a crucial role in designing, implementing, and managing data architecture on the Azure cloud platform. You will collaborate closely with diverse teams to ensure that the data infrastructure aligns with business needs and adheres to industry best practices. If you are a strategic thinker with strong technical expertise, this role offers an exciting opportunity to shape the data landscape and make a significant impact.
Responsibilities
Design, develop, and maintain data pipelines using Azure Data Factory and other relevant Azure technologies
Optimize ETL processes for performance and scalability, leveraging Azure services such as Azure Data Factory Mapping Data Flows, Azure Data Lake Analytics, or Azure Synapse Pipelines
Extract data from various sources, transform it into a usable format, and load it into Azure data storage solutions such as Azure SQL Database, or Azure Synapse Analytics
Collaborate with cross-functional teams to understand data requirements and design scalable and efficient ETL processes using Azure services
Identify, design, and implement ETL solutions for extraction and integration of data to and from data warehouses and data marts for the purposes of reporting, decision support, and analysis
Lead the design, development, and deployment of data solutions on the Azure cloud platform
Demonstrate expert-level understanding of Azure Data Factory, Azure Synapse, Azure SQL, Azure Data Lake, and Azure App Service
Design and build data pipelines using API ingestion and Streaming ingestion methods
Possess knowledge of Dev-Ops processes (including CI/CD) and Infrastructure as code
Demonstrate knowledge of Azure Databricks, Azure IoT, Azure HDInsight + Spark, Azure Stream Analytics, and Power BI (preferable)
Qualifications
Bachelor's degree in Computer Science, Information Technology, or a related field
A minimum of 2 years' experience in Azure Data Factory
A minimum of 7 years' experience with the Microsoft Azure platform and services
Azure certifications (e.g., Azure Data Engineer, Azure Solutions Architect) are preferred
Strong expertise in data modeling, ETL processes, and database management
Proficiency in SQL, Azure SQL, and Azure Data Lake Storage
Familiarity with data warehousing concepts and tools
Excellent problem-solving and communication skills
Strong project management and leadership abilities
Ability to work collaboratively in cross-functional teams
Location
This position can be located in Chicago, IL, Pittsburgh, PA, or remotely.
Employment Type: Full-Time
Show more
Show less","Azure Data Factory, Azure Synapse, Azure SQL, Azure Data Lake, Azure App Service, API ingestion, Streaming ingestion, Azure Databricks, Azure IoT, Azure HDInsight + Spark, Azure Stream Analytics, Power BI, ETL, Data modeling, SQL, Azure SQL, Azure Data Lake Storage, Data warehousing","azure data factory, azure synapse, azure sql, azure data lake, azure app service, api ingestion, streaming ingestion, azure databricks, azure iot, azure hdinsight spark, azure stream analytics, power bi, etl, data modeling, sql, azure sql, azure data lake storage, data warehousing","api ingestion, azure app service, azure data factory, azure data lake, azure data lake storage, azure databricks, azure hdinsight spark, azure iot, azure sql, azure stream analytics, azure synapse, datamodeling, datawarehouse, etl, powerbi, sql, streaming ingestion"
Healthcare Claims Data Engineer (Contract),Vivante Health,"Chicago, IL",https://www.linkedin.com/jobs/view/healthcare-claims-data-engineer-contract-at-vivante-health-3780555436,2023-12-17,La Grange,United States,Mid senior,Remote,"Job Title:
Healthcare Claims Data Engineer (Contract)
Remote – US and British Columbia Only
About Vivante
Vivante Health is an innovative startup reinventing the way chronic conditions are managed. We’re filling the unmet needs of people with chronic conditions that are invisible, neglected or stigmatized, starting with digestive disease.
Why digestive? Because an astonishing 70 million people in the US are affected—that’s twice the number with diabetes and more than many other chronic conditions combined. Unlike other chronic conditions, though, digestive diseases often go untreated or misdiagnosed…because of stigma.
At Vivante, we think it’s time to bring digestive health to the forefront while providing REAL solutions to the millions who are struggling and don’t know where to turn. Our health management ecosystem, GIThrive, empowers people to spend less time worrying about their digestive symptoms and more time living life.
GIThrive works with our members (patients) to help identify and manage their digestive health conditions with personalized insights & recommendations. Our platform also facilitates interactions with our world-class clinical team to blend our technology with real person-to-person relationships that support each patient throughout their individual journeys.
With a remote-first workforce, backed by leading digital health investors, we’re changing the way healthcare is delivered.
Position Summary
We're looking for an experienced, solution-focused Claims Data Engineer to support our data insights and analytics efforts. If you like designing and building scalable data solutions while collaborating with other great engineers in a test-driven environment, please read on!
This engineer would help us integrate various external and internal data sources, with a special focus on medical and prescription claims data ingestion. This work is pivotal to enabling data analytics and data science efforts to assess the impact of our program on clinical outcomes, as well as personalize the program experience for our members. There will also be close collaboration with our Strategy and Operations team, who will be among the end consumers of the claims data products built.
The ideal candidate is someone who can reason through the interactions of a distributed system and deliver solutions that emphasize simplicity, reliability and supportability.
Why is this a great opportunity for a data engineer?
The problem domain and our approach to solving it is super compelling. We're bringing together data sources that have never been joined, building models of a GI patient that have never been built, and discovering new GI health insights that help real people to manage and support their real medical conditions.
It's still early enough that there’s a lot of decisions and new discoveries to be made, so this is your chance to get in early and help shape our future.
Other things about our environment you should be aware of:
We're still a small company, which means that everyone still wears a lot of hats and we need engineers who are comfortable with ambiguity and working across a wide variety of challenges. That said, our existing engineering team has built an extremely efficient working environment based on best-in-class hosted SaaS frameworks that greatly minimize the overhead in deploying and supporting software in production.
We're all really passionate about improving clinical outcomes for our members and truly advancing the GI health space. Most of us also have personal (direct/family/friend) connections to GI health problems. Whatever it is that fuels you, we're on a mission to build something much larger than ourselves here and looking for another partner in driving that mission forward.
Responsibilities Include:
Expand our capabilities for ingesting and organizing medical and pharmacy claims data, along with other forms of healthcare and/or population health data
Develop the kind of software and pipelines that you’d want to inherit from another developer (documentation, test coverage, logging, metrics, etc.)
Collaborate with the rest of your engineering team on design, planning and code reviews
Partner with product stakeholders on ideation, feedback and refinement of solutions that meet our business needs
Keep learning
You'll be a good fit here if you are:
A team player. You like collaborating closely with other engineers, often through pair programming, design & code reviews
Empathetic to our members’ GI health conditions and are driven to improve their outcomes
Security oriented. We take the stewardship of our customer's healthcare data seriously, and take no shortcuts to protect it
Comfortable with a distributed workforce. We interact with each other via video chat, Slack and PR comments
A self motivated, creative problem solver
At least partially obsessed with automating everything
We are proud of the team culture that we foster today, which is extremely friendly and supportive while constantly reaching to raise our own standards of engineering. We're really excited about what we're building, and usually having fun building it together. If all of that sounds fun to you too, we'd love to meet you.
Desired Qualifications
Degree in Computer Science or a related field, or equivalent experience
Experience in medical, healthcare and/or population health data, including expertise in medical and pharmacy claims data (e.g., medical coding systems, nuances specific to interpreting claims data)
3+ years of demonstrated experience building and supporting data pipelines,SaaS solutions or other related technologies
Proficiency with SQL, Python and some form of ELT/ETL solutions
Outstanding communication and interpersonal abilities with colleagues, business partners, and vendors alike
Bonus:
Familiarity with Google Cloud Platform (GCP) frameworks, including, BigQuery, IAM (fundamentals), Pub/Sub, Dataflow, Cloud Composer, DLP, or any others
Experience with Terraform and/or other infrastructure-as-code frameworks
Experience working in a HIPAA regulated environment (or other regulated industry) and supporting data security and privacy controls
Experience analyzing and optimizing system performance
Vivante Health is an equal opportunity employer.
We believe safe spaces where everyone can be their authentic selves is the key to a successful team so we welcome and embrace all identities, cultures, and backgrounds.
Interested? Apply here
Show more
Show less","Data Engineering, Python, SQL, ELT/ETL, GCP frameworks, BigQuery, IAM, Pub/Sub, Dataflow, Cloud Composer, DLP, Terraform, HIPAA regulations, Infrastructureascode frameworks, System performance optimization","data engineering, python, sql, eltetl, gcp frameworks, bigquery, iam, pubsub, dataflow, cloud composer, dlp, terraform, hipaa regulations, infrastructureascode frameworks, system performance optimization","bigquery, cloud composer, data engineering, dataflow, dlp, eltetl, gcp frameworks, hipaa regulations, iam, infrastructureascode frameworks, pubsub, python, sql, system performance optimization, terraform"
Sr. Data Analyst,Launch Consulting Group,"Chicago, IL",https://www.linkedin.com/jobs/view/sr-data-analyst-at-launch-consulting-group-3769546975,2023-12-17,La Grange,United States,Mid senior,Remote,"The Role
Launch is seeking a skilled and experienced Sr. Data Visualization Consultant to join our dynamic and growing
Data & AI Studio
! Our
Data Visualization Discipline
comprises seasoned experts dedicated to crafting and deploying advanced data visualizations, designed with a user-centric approach to empower individuals with data-driven insights. The ideal candidate should exude enthusiasm for leveraging a diverse set of tools and collaborating closely with clients to uncover, outline, and execute solutions.
Responsibilities:
Cultivate the skills that are needed to be a strong data analyst, including reporting and dashboarding.
Perform reporting and analysis on a wide range of data, including data aggregations, slicing and dicing, statistics, pivoting, charting, and other analytics techniques to derive business insights from raw data.
Develop and engineer business intelligence solutions, including source-target mapping, data ingestion, data transformation, data modelling, visualizations, and the creation of reports and dashboards.
Data storytelling and providing compelling insights using data.
Write SQL queries to pull data needed for reports and analysis.
Build and maintain data flows and automated data processes.
Provide high quality data through data hygiene and validation.
Track work items using project management tools such as Azure DevOps (ADO) or JIRA
Build data models and data flow diagrams using Visio, LucidChart, Miro, or similar tools.
Develop ingestion apps and automation flows using Power Platform tools.
Apply human experience design to the creation of analytics solutions
Effectively communicates with stakeholders, tailoring explanations to the appropriate audience level.
Lead project teams, offering training, and acting as a mentor to fellow team members.
Transforms business requirements into actionable features.
Delivers high-quality code resulting in minimal critical defects, reduced redo cycles, and sustainable maintenance and performance, mentors’ jr.-mid level engineers and consultants.
Establishes code design patterns and templates.
Produces well-documented, easily comprehensible code while championing advanced agile methodologies and SDLC knowledge.
Demonstrates advanced proficiency in business analysis and presentation skills.
Required Skills & Knowledge:
5+ years of relevant experience
IT Consulting experience is highly preferred, ideal candidate(s) will demonstrate a track record of successfully advising and providing solutions to strategic clients.
Experience performing data analytics and deriving insights from raw data using statistics, data mining, visualizations, and business intelligence techniques using Excel, Power BI, or other common analytics tools.
Experience developing business intelligence solutions using Power BI or Tableau, including data ingestion, data integration, data modelling, and dashboard creation, including the use of DAX for data ingestion and measure creation
Experience using data to tell a story and communicate deep insights.
Ability to write complex SQL queries, stored procedures, DDL, and an understanding of database schema design.
Experience building and maintaining data flows and automated data processes
Experience ensuring data quality and performing data validation
Working with project tracking tools, such Azure DevOps (ADO) or JIRA
Working with diagramming tools such as Visio, LucidChart, or Miro
Experience with Power Platform, including Power Apps and Power Automate
Proficiency in Sigma Computing, including experience in creating and maintaining data models, dashboards, and reports within the platform.
Preferred/Relevant Certifications:
Microsoft Certified: Power BI Data Analyst Associate
Microsoft Certified: Power Platform Fundamentals
Tableau Certified Data Analyst
AWS Certified Data Analytics – Specialty
Snowflake Certified Data Analyst
Benefits & Compensation:
As an employee at Launch, you will grow your skills and experience through a variety of exciting project work (across industries and technologies) with some of the top companies in the world! Our employees receive full benefits—medical, dental, vision, short-term disability, long-term disability, life insurance, and matched 401k. We also have an uncapped, take-what-you-need PTO policy. The anticipated wage range for this role is $120,000-$140,000. Education and experience will be highly considered, and we are happy to discuss your wage expectations in more detail throughout our internal interview process.
Don’t Be Shy!
Our Corporate Recruiting team is excited to talk about our open positions and the opportunities to join the Launch team. Even if you are still in the exploratory stage of your job search, we are always looking to network! Connect with our team today and let’s talk more.
Show more
Show less","SQL, DAX, Power BI, Tableau, Azure DevOps, Jira, Visio, Lucidchart, Miro, Power Apps, Power Automate, Sigma Computing, Power BI Data Analyst Associate, Power Platform Fundamentals, Tableau Certified Data Analyst, AWS Certified Data Analytics – Specialty, Snowflake Certified Data Analyst","sql, dax, power bi, tableau, azure devops, jira, visio, lucidchart, miro, power apps, power automate, sigma computing, power bi data analyst associate, power platform fundamentals, tableau certified data analyst, aws certified data analytics specialty, snowflake certified data analyst","aws certified data analytics specialty, azure devops, dax, jira, lucidchart, miro, power apps, power automate, power bi data analyst associate, power platform fundamentals, powerbi, sigma computing, snowflake certified data analyst, sql, tableau, tableau certified data analyst, visio"
Lead Data/ML Engineer - REMOTE,Sierra Solutions,"Chicago, IL",https://www.linkedin.com/jobs/view/lead-data-ml-engineer-remote-at-sierra-solutions-3760865756,2023-12-17,La Grange,United States,Mid senior,Remote,"Job Summary
We are looking for a Data/Machine Learning Engineer for our technology client. The ideal candidate will be passionate about building large scale AL/Client systems, building data pipelines and working with modern technology to help manage the ever-growing data needs of clients.
Primary Responsibilities
Partner with teammates to create complex data and Client processing pipelines in order to solve challenges
Collaborate with Data Scientists in order to design scalable implementations of their models
Help to design and develop end-to-end MLOps/CD4ML practices as part of a diverse team
Communicate with MLOps and stakeholders on project insights and enable the team to make informed decisions
Pair to write clean and iterative code using TDD
Leverage various continuous delivery practices to deploy, support and operate data pipelines
Advise and educate clients on how to use different distributed storage and computing technologies from the plethora of options available
Develop and operate modern data architecture approaches to meet key business objectives and provide end-to-end data solutions
Create data models and speak to the tradeoffs of different modeling approaches
Seamlessly incorporate data quality into your day-to-day work as well as into the delivery process
Assure effective collaboration between colleagues and the client's teams, encouraging open communication and advocating for shared outcomes
Education And Experience
You are equally happy coding and leading a team to implement a solution
Possess a track record in Data Engineering or Machine Learning Engineering
Experience with popular machine learning packages like scikit-learn, tensor flow and pytorch
Experience with machine learning techniques and knowledge of statistical models
You have a deep understanding of data modeling and experience with data engineering tools and platforms such as Kafka, Spark, and Hadoop
Experienced in deploying and operating machine learning systems in production, including in cloud environments
Understanding of data infrastructure and operations needs for machine learning, including automation and how to operate them on premise and in cloud environments
Ability to assess and speak to the tradeoffs of using different approaches to Model Serving, Model Deployment, and Model Observability and the tools associated with them
You have built large-scale data pipelines and data-centric applications using any of the distributed storage platforms such as HDFS, S3, ADLS, NoSQL databases (Hbase, Cassandra, etc.) and any of the distributed processing platforms like Hadoop, Spark, Hive, Oozie, and Airflow in a production setting
Understand and design suitable end-to-end CD4ML practices, depending on project settings.
Hands on experience in Databricks, Cloudera, Hortonworks and/or cloud (AWS EMR, Azure HDInsights, Qubole etc.) based Hadoop distributions
You are comfortable taking data-driven approaches and applying data security strategy to solve business problems
You're genuinely excited about data infrastructure and operations with a familiarity working in cloud environments
Experience in Big data architecture build and operate data pipelines, and maintain data storage, all within distributed systems
Show more
Show less","Data Engineering, Machine Learning, Cloud Computing, Data Pipelines, Data Architecture, Data Modeling, Data Security, Statistical Models, Data Infrastructure, Model Serving, Model Deployment, Model Observability, Continuous Delivery, TestDriven Development, Scala, Python, Java, Spark, Hadoop, Kafka, Hive, Oozie, Airflow, HDFS, S3, NoSQL, Hbase, Cassandra, Databricks, Cloudera, Hortonworks, AWS EMR, Azure HDInsights, Qubole","data engineering, machine learning, cloud computing, data pipelines, data architecture, data modeling, data security, statistical models, data infrastructure, model serving, model deployment, model observability, continuous delivery, testdriven development, scala, python, java, spark, hadoop, kafka, hive, oozie, airflow, hdfs, s3, nosql, hbase, cassandra, databricks, cloudera, hortonworks, aws emr, azure hdinsights, qubole","airflow, aws emr, azure hdinsights, cassandra, cloud computing, cloudera, continuous delivery, data architecture, data engineering, data infrastructure, data security, databricks, datamodeling, datapipeline, hadoop, hbase, hdfs, hive, hortonworks, java, kafka, machine learning, model deployment, model observability, model serving, nosql, oozie, python, qubole, s3, scala, spark, statistical models, testdriven development"
"Senior/Staff Software Engineer, Data",EvenUp,"Chicago, IL",https://www.linkedin.com/jobs/view/senior-staff-software-engineer-data-at-evenup-3782664480,2023-12-17,La Grange,United States,Mid senior,Remote,"EvenUp is a venture-backed generative AI startup that ensures injury victims are awarded the full value of their claims, expanding the $100B+ in awards granted to injury victims every year.
Every year, the legal system has made it difficult for millions of ordinary people to seek justice, especially for folks without means or who come from underrepresented backgrounds
. Our vision is to help these injury victims get the justice they deserve, irrespective of their income, demographics, or the quality of their legal representation.
EvenUp operates across all types of injury cases, from police brutality and child abuse to California wildfires and motor vehicle accidents. Our ML-driven software empowers attorneys to accurately assess the value of these cases by doing a core part of their workflow (legal drafting), enabling them to secure larger settlements in record time. As EvenUp evaluates more cases, our proprietary data grows, enhancing the precision of our predictions and delivering more value to both attorneys and victims alike.
As one of the fastest growing startups ($0 to $10M in ARR in <2 years), we raised $65M in investment from some of the best investors in the world (Bessemer, Bain Capital, Signalfire, DCM, NFX, Tribe Capital), seasoned tech executives (i.e. founder of Quora, SVP at Google, former CPO at Uber), and public figures that care about our social mission (Nas, Jared Leto, Byron Jones). Our team comes from top tech, legal, and investing backgrounds including Waymo, Google, Amazon, Uber, Quora, Blizzard, Norton Rose, Warburg Pincus, Bain, and McKinsey.
The role:
We’re looking to bring on board
Senior/Staff Software Engineers
focused on our
Data Pipelines
as we’ve experienced unprecedented growth and need to build & scale out our data pipelines and infrastructure. We’re looking for strong team members to help architect and drive forward the vision of our ideal data infrastructure at EvenUp. We will need to 10x our pipeline processing throughput over the next 12 months. We’ll need to rethink and rebuild how we extract, process and model our ingestion to enable our organization with precise and actionable data.
What you'll do:
Build fault tolerant data pipelines to process diverse datasets at EvenUp
Design and develop modularized services to increase the capabilities and scope of our data infrastructure
Collaborate with our DS team to Integrate ML models into our production workflows and simplify ML deployment and observability
Implement event driven, low latency systems to empower our stakeholders with accurate and reliable data
Analyze and solve key performance bottlenecks, scaling challenges, and high availability issues.
Mentor and coach junior team members
Help grow our engineering team and define a “data first” mentality across our organization.
What we are seeking:
8+ years of industry experience designing and building distributed data systems
Previous experience architecting and scaling event driven architectures
Strong understanding and practical experience with data pipeline tooling and storage systems such as Dagster, DBT, BigQuery, Elasticsearch
The ability to communicate cross-functionally with various stakeholders to derive requirements and architect scalable solutions
Have several years of industry experience building high-quality software, shipping production-ready code and infrastructure
You enjoy owning a project from start to finish and love to drive a project across the finish line.
Interest in making the world a fairer place (we don’t get paid unless we’re helping injured victims and/or their attorneys)
Nice to haves:
Fluency in Python, SQL and GraphQL
Previous experience integrating ML models and LLMs into data services
Domain expertise in legal technology, medical records, and working with unstructured data
Benefits & Perks:
We seek to empower all of our team members to fulfill our mission of making the world a more just place, regardless of our team’s function, geography, or experience level. To that end, we offer:
Fully remote setup - work from wherever you feel is best (Plus a stipend to upgrade your home office!)
Flexible working hours to match your style
Offsites - get to meet your coworkers on a fully-expensed trip every 6-12 months!
Choice of great medical, dental, and vision insurance plan options
Flexible paid time off
A variety of virtual team events such as game nights & happy hours
EvenUp is an equal-opportunity employer. We are committed to diversity and inclusion in our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.
Show more
Show less","Data Pipelines, Senior/Staff Software Engineers, Dagster, DBT, BigQuery, Elasticsearch, GraphQL, Python, SQL, ML, LLMs, Legal technology, Medical records","data pipelines, seniorstaff software engineers, dagster, dbt, bigquery, elasticsearch, graphql, python, sql, ml, llms, legal technology, medical records","bigquery, dagster, datapipeline, dbt, elasticsearch, graphql, legal technology, llms, medical records, ml, python, seniorstaff software engineers, sql"
"Data Engineer at Phoenix, AZ",IVY TECH SOLUTIONS INC,"Chicago, IL",https://www.linkedin.com/jobs/view/data-engineer-at-phoenix-az-at-ivy-tech-solutions-inc-3787775459,2023-12-17,La Grange,United States,Mid senior,Remote,"HI,
Kindly let me know if you have a suitable fit for the following position
Thanks
Data Engineer
Location: Phoenix, AZ
Duration: 12+Months
Initially Remote
ONLY W2
Please send the resume to
or 847- 350-1008
The data engineer is a critical role that will provide data engineering design, ETL development and technical expertise to an ETL scrum team. They will work on a team with other ETL developers, QA engineers and analysts to support data streams for payment, claim and customer service operations. The team is responsible for a large catalog of jobs that use a mixture of batch ETL architecture and real-time data streaming with API-integrated data services. The data engineer will spend their time doing hands on development, designing future data processes, conducting data analysis, consulting with other teams and interacting throughout the Agile process in a stable scrum team environment.
Essential Functions / Principal Responsibilities
Develops data pipelines in both batch ETL and real-time streaming architectures.
Develops data models to define new or modify existing data structures in support of data integration initiatives.
Provides expert technical knowledge of data solutions for business projects.
Provides source system analysis, data discovery, complex transformation assessment and target system exploration to understand information data requirements and anticipate user needs.
Contributes to data pipeline design, coding, and technical / functional reviews while collaborating with source system developers, data engineers and functional subject matter experts.
Develops effective data pipeline solutions to deliver business features.
Adheres to best practices for data movement, data quality, data profiling, data cleansing and other data pipeline related activities.
Applies tuning and optimization for continuous improvement.
Presents technical information in easily understood terms (written, verbal and visual).
Communicates effectively within the Agile team and to external stakeholders and management.
Follows Agile best practices and adheres to internal IT processes like change management and problem management.
Skills that will Ensure Success:
Specialist in ETL development with a demonstrated understanding of transactional data processing, streaming data and data pipeline best practices.
Experience in build, unit test, and deployment of Informatica ETL processes.
Knowledgeable in making REST API calls within data processes.
Familiar with real-time data pipeline platforms, preferably StreamSets, AWS Glue or similar platform.
Hands on experience with data streaming in Apache Kafka.
Able to interpret business needs and turn them into a technical plan of attack with pros and cons of various approaches to the data processing options.
Demonstrates a solid understanding of technical standards and processes related to batch and real-time data pipeline development.
Excellent team player, able to work with product owners, technical developers, DBAs, system administrators, BI professional services, data warehouse operations and functional experts.
Expertise in SQL query transactions and optimization, especially T-SQL.
Understand nulls, cardinality, joins, data types to develop technical ETL specifications and technical metadata.
Ability to integrate an application solution into the broader business and IT ecosystem in which it will operate.
Firm understanding of quality assurance activities and automation in data pipeline and ETL processing.
Desire experience working with financial and/or claims data requiring compliance, balancing and integrity checks, especially payment-related data, PCI compliant data and banking industry formats such as NACHA.
Desire a firm understanding of cloud data processing and data streaming architectures, especially in AWS.
Charan Kumar
| IVY Tech Sols Inc.
3403 N Kennecott Avenue, Suite B&C Arlington Heights, IL 60004
( Direct:
(847) 350-1008
|Gtalk : charan.ivytech|
Powered by JazzHR
19EMA7VJlz
Show more
Show less","ETL development, ETL architecture, Data modeling, Data integration, Data analysis, Data pipelines, Data quality, Data profiling, Data cleansing, Data streaming, Apache Kafka, REST API, SQL, TSQL, Cloud data processing, AWS, Informatica, StreamSets, AWS Glue","etl development, etl architecture, data modeling, data integration, data analysis, data pipelines, data quality, data profiling, data cleansing, data streaming, apache kafka, rest api, sql, tsql, cloud data processing, aws, informatica, streamsets, aws glue","apache kafka, aws, aws glue, cloud data processing, data integration, data profiling, data quality, data streaming, dataanalytics, datacleaning, datamodeling, datapipeline, etl architecture, etl development, informatica, rest api, sql, streamsets, tsql"
Expression of Interest: Data Engineer,Fingerprint for Success (F4S),"Chicago, IL",https://www.linkedin.com/jobs/view/expression-of-interest-data-engineer-at-fingerprint-for-success-f4s-3787774958,2023-12-17,La Grange,United States,Mid senior,Remote,"We are inviting professionals in high-growth industries who are thinking about their next move or looking for a new opportunity to join our expanding talent pool.
The F4S Talent Pool is a pilot project designed to:
Help job seekers get discovered by our partners based on their anticipated hiring needs.
Provide optional support and resources for job seekers in their career endeavors.
Help individuals understand, and bring out the best in themselves and each other.
The F4S Talent Pool process:
Once you express your interest, you will be asked to complete the F4S work style assessment which measures 48 key attitudes and motivations in the context of work. On completion, you will be automatically added to our growing talent pool and contacted as new opportunities arise.
About Fingerprint For Success (F4S)
Backed by 20+ years of research, F4S’s revolutionary predictive analytics have achieved over 90% reliability in forecasting personal and team motivations, behaviors, and performance. Ultimately, we help people find purpose and fulfillment at work, and help build and scale high performing teams.
Keep in mind, joining our talent pool does not guarantee a job offer. We aim to balance your technical skills with the results of your F4S work style assessment to match the hiring needs of our partners.
Your feedback is a gift! Write to us via:
Powered by JazzHR
vZYkG83TOg
Show more
Show less","F4S work style assessment, Predictive analytics, Talent pool, Motivations, Behaviors, Performance, JazzHR","f4s work style assessment, predictive analytics, talent pool, motivations, behaviors, performance, jazzhr","behaviors, f4s work style assessment, jazzhr, motivations, performance, predictive analytics, talent pool"
DataStage Data Engineer,IVY TECH SOLUTIONS INC,"Chicago, IL",https://www.linkedin.com/jobs/view/datastage-data-engineer-at-ivy-tech-solutions-inc-3787770694,2023-12-17,La Grange,United States,Mid senior,Remote,"Remote
DataStage Data Engineer
Duration: 6+Months
Only w2 or 1099
Please send the resume to
or 847- 350-1008
together we are building a culture that values diversity and creates a space of belonging for all our team members. We believe that investing in your success is an investment in our customers and our business. Our people are what sets us apart and make us great. As a Data Engineer, you’ll provide your talents in contributing to the success of the client’s team by delivering the following:
Serve in the goalie rotation to support the Production environment.
Responsible for maintaining enterprise-grade platforms that enable data-driven solutions.
Search for ways to automate and maintain scalable infrastructure.
Ensure delivery of highly available and scalable systems.
Monitor all systems and applications and ensure optimal performance.
Analyzes and designs technical solutions to address production problems.
Participate in troubleshooting applications and systems issues.
Identifies, investigates, and proposes solutions to technical problems.
While providing technical support for issues, develop, test, and modify software to improve efficiency of data platforms and applications.
Monitors system performance to maintain consistent up time.
Prepares and maintains necessary documentation.
Participate in daily standups, team backlog grooming, and iteration retrospectives.
Coordinate with data operations teams to deploy changes into production.
Highest level may function as a lead.
Other duties as assigned.
Qualifications:
Requires a Bachelor's in Computer Science, Computer Engineering or related field and experience with ETL development, SQL, UNIX/Linux scripting, Big Data distributed systems. Prefer experience with IBM DataStage.
Various programming languages like Java and Python, orchestration tools and processes or other directly related experience.
A combination of education and experience may meet qualifications.
Excellent analytical, organizational, and problem-solving skills.
Ability and desire to learn new technologies quickly.
Ability to work independently and collaborate with others at all levels of technical understanding.
Able to meet deadlines.
Good judgment and project management skills.
Ability to communicate both verbally and in writing with both technical and naon-technical staff.
Ability to work in a team environment and have good interpersonal skills.
Ability to adapt to changing technology and priorities.
Must be able to work independently, handle multiple concurrent tasks, with an ability to prioritize and manage tasks effectively.
Warm Regards,
Charan Kumar
| IVY Tech Sols Inc.
3403 N Kennecott Avenue, Suite B&C Arlington Heights, IL 60004
PH.
( Direct:
(847) 350-1008
|Gtalk : charan.ivytech|
Powered by JazzHR
f1Hgh1ZRAW
Show more
Show less","DataStage, SQL, UNIX/Linux scripting, Big Data, Java, Python, ETL, Orchestration tools","datastage, sql, unixlinux scripting, big data, java, python, etl, orchestration tools","big data, datastage, etl, java, orchestration tools, python, sql, unixlinux scripting"
Sr. Azure Data Engineers.,IVY TECH SOLUTIONS INC,"Roselle, IL",https://www.linkedin.com/jobs/view/sr-azure-data-engineers-at-ivy-tech-solutions-inc-3787780021,2023-12-17,La Grange,United States,Mid senior,Remote,"Hi,
Kindly let me know if you have a suitable fit for the following position
Thanks
Sr. Azure Data Engineers.
Location: Remote
Duration: 12+Months
Please send the resume to
or 847- 350-1008
Skills:
building Data Pipeline using Azure technologies
Charan Kumar
| IVY Tech Sols Inc.
3403 N Kennecott Avenue, Suite B&C Arlington Heights, IL 60004
( Direct:
(847) 350-1008
|Gtalk : charan.ivytech|
Powered by JazzHR
2zrGBlnKYa
Show more
Show less","Azure Data Pipeline, Data Pipeline, Azure","azure data pipeline, data pipeline, azure","azure, azure data pipeline, data pipeline"
Data Engineer- Costco Logistics BI,Costco IT,"Chicago, IL",https://www.linkedin.com/jobs/view/data-engineer-costco-logistics-bi-at-costco-it-3633075907,2023-12-17,La Grange,United States,Mid senior,Hybrid,"Costco IT is responsible for the
technical future of Costco Wholesale
, the third largest retailer in the world with wholesale operations in fourteen countries. Despite our size and explosive international expansion, we continue to provide a family, employee centric atmosphere in which our employees thrive and succeed. As proof, Costco ranks
seventh in Forbes “World’s Best Employers”
.
This is an environment
unlike anything in the high-tech world
and the secret of Costco’s success is its culture. The value Costco puts on its employees is well documented in articles from a variety of publishers including Bloomberg and Forbes. Our employees and our members come FIRST. Costco is well known for its generosity and community service and has won many awards for its philanthropy. The company joins with its employees to take an active role in volunteering by sponsoring many opportunities to help others.
Come join the
Costco Wholesale IT family
. Costco IT is a dynamic, fast-paced environment, working through exciting transformation efforts. We are building the next generation retail environment where you will be surrounded by dedicated and highly professional employees.
The
Data Engineer - Costco Logistics BI
is responsible for the end to end data pipelines to power Costco Logistics reporting. This role is focused on data engineering to build and deliver automated data pipelines from a plethora of internal and external data sources. The Data Engineer will partner with product owners, engineering and data platform teams to design, build, test, and automate data pipelines that are relied upon across the company as the single source of truth.
If you want to be a part of one of the worldwide
BEST companies “to work for”,
simply apply and let your career be reimagined.
ROLE
Develops and operationalizes data pipelines to make data available for consumption (Costco Logistics BI).
Works with data architects and data/BI engineers to design data pipelines and recommends ongoing optimization of data storage, data ingestion, data quality, and orchestration.
Designs, develops, and implements ETL/ELT processes using IICS (Informatica Cloud).
Uses MySQL to improve and speed up delivery of our data products and services.
Uses Azure services such as Azure SQL DW (Synapse), ADLS, Azure Event Hub, Azure Data Factory to improve and speed up delivery of our data products and services.
Implements big data and NoSQL solutions by developing scalable data processing platforms to drive high-value insights to the organization.
Identifies, designs, and implements internal process improvements: automating manual processes, optimizing data delivery.
Identifies ways to improve data reliability, efficiency, and quality of data management.
Communicates technical concepts to non-technical audiences both written and verbal.
Performs peer reviews for other data engineer’s work.
Required
3+ years’ experience engineering and operationalizing data pipelines with large and complex datasets.
2+ years’ experience with Informatica PowerCenter.
2+ years’ experience with Informatica IICS.
2+ years’ experience working with Cloud technologies; such as ADLS, Azure Databricks, Spark, Azure Synapse, Cosmos DB, and other big data technologies.
2+ years’ experience with Data Modeling, ETL, and Data Warehousing.
2+ years’ experience implementing data integration techniques such as event / message based integration (Kafka, Azure Event Hub), ETL.
2+ years’ experience with Git / Azure DevOps.
Extensive experience working with various data sources; SQL, Sql Server database, flat files (csv, delimited), Web API, XML.
Advanced SQL skills; Understanding of relational databases, business data, and the ability to write complex SQL queries against a variety of data sources.
Strong understanding of database storage concepts; Data Lake, Relational Databases, NoSQL, Graph, Data Warehousing.
Able to work in a fast-paced agile development environment.
Recommended
Microsoft Azure/similar certifications.
Experience delivering data solutions through agile software development methodologies.
Experience with PowerShell, Python or similar scripting language.
Experience with UC4 Job Scheduler.
Exposure to the retail industry.
Excellent verbal and written communication skills.
BA/BS in Computer Science, Engineering, or equivalent software/services experience.
Required Documents
Cover Letter
Resume
California applicants, please click
Pay Ranges
Level 1 - $75,000 - $110,000
Level 2 - $100,000 - $135,000
We offer a comprehensive package of benefits including paid time off, health benefits - medical/dental/vision/hearing aid/pharmacy/behavioral health/employee assistance, health care reimbursement account, dependent care assistance plan, short-term disability and long-term disability insurance, AD&D insurance, life insurance, 401(k), stock purchase plan to eligible employees.
Costco is committed to a diverse and inclusive workplace. Costco is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or any other legally protected status. If you need assistance and/or a reasonable accommodation due to a disability during the application or the recruiting process, please send a request to IT-Recruiting@costco.com
If hired, you will be required to provide proof of authorization to work in the United States. Applicants and employees for this position will not be sponsored for work authorization, including, but not limited to H1-B visas.
Show more
Show less","Data engineering, Data pipelines, Informatica PowerCenter, Informatica IICS, Cloud technologies, Data modeling, ETL, Data warehousing, Data integration, SQL, Azure DevOps, Git, Data sources, Database storage, Agile development, Python, PowerShell, UC4 Job Scheduler, Retail industry, Verbal communication, Written communication, Computer Science, Engineering, Software development","data engineering, data pipelines, informatica powercenter, informatica iics, cloud technologies, data modeling, etl, data warehousing, data integration, sql, azure devops, git, data sources, database storage, agile development, python, powershell, uc4 job scheduler, retail industry, verbal communication, written communication, computer science, engineering, software development","agile development, azure devops, cloud technologies, computer science, data engineering, data integration, data sources, database storage, datamodeling, datapipeline, datawarehouse, engineering, etl, git, informatica iics, informatica powercenter, powershell, python, retail industry, software development, sql, uc4 job scheduler, verbal communication, written communication"
Senior Database Engineer,Mastercard,"Chicago, IL",https://www.linkedin.com/jobs/view/senior-database-engineer-at-mastercard-3785141571,2023-12-17,La Grange,United States,Mid senior,Hybrid,"Our Purpose
We work to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments and businesses realize their greatest potential. Our decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. We cultivate a culture of inclusion for all employees that respects their individual strengths, views, and experiences. We believe that our differences enable us to be a better team – one that makes better decisions, drives innovation and delivers better business results.
Title And Summary
Senior Database Engineer
The database engineering team is looking for a Senior Database Engineer to help set direction for MasterCard’s use and best practices for our application database solutions. You will work with application teams to implement sound RDMS data solutions that follow industry best practices and methods.
You will also be responsible for migrating existing databases to AWS. You will design, automate, and manage databases as a service leveraging deep experience in Open-Source database technologies like MySQL, Aurora, and other Cloud Native database technologies.
Role
Optimize and maintain legacy systems in MySQL & MongoDB
Implement differing approaches to data management & administration in AWS
Partner with dev teams to create DevSecOps constructed databases in AWS
Develop plans to migrate legacy on-premises databases into AWS data stores
Engineer various database services across multiple AWS services and regions
Be THE escalation point for solving database usage issues and malfunction
All About You
Deep experience with database engineering
Experience sharding, clustering, HA, scaling, monitoring, deploying, tuning, etc.
Experience being the escalation point for any and all DBE/DBA issues
Sr. experience in cloud databases in production, development, and QA environments
Work Conditions
Rotational pager for Production Support
Tech Stack
AWS, Aurora MySQL, MongoDB
In the US, Mastercard is an inclusive Equal Employment Opportunity employer that considers applicants without regard to gender, gender identity, sexual orientation, race, ethnicity, disabled or veteran status, or any other characteristic protected by law. If you require accommodations or assistance to complete the online application process, please contact reasonable_accommodation@mastercard.com and identify the type of accommodation or assistance you are requesting. Do not include any medical or health information in this email. The Reasonable Accommodations team will respond to your email promptly.
Corporate Security Responsibility
All activities involving access to Mastercard assets, information, and networks comes with an inherent risk to the organization and, therefore, it is expected that every person working for, or on behalf of, Mastercard is responsible for information security and must:
Abide by Mastercard’s security policies and practices;
Ensure the confidentiality and integrity of the information being accessed;
Report any suspected information security violation or breach, and
Complete all periodic mandatory security trainings in accordance with Mastercard’s guidelines.
In line with Mastercard’s total compensation philosophy and assuming that the job will be performed in the US, the successful candidate will be offered a competitive base salary based on location, experience and other qualifications for the role and may be eligible for an annual bonus or commissions depending on the role. Mastercard benefits for full time (and certain part time) employees generally include: insurance (including medical, prescription drug, dental, vision, disability, life insurance), flexible spending account and health savings account, paid leaves (including 16 weeks new parent leave, up to 20 paid days bereavement leave), 10 annual paid sick days, 10 or more annual paid vacation days based on level, 5 personal days, 10 annual paid U.S. observed holidays, 401k with a best-in-class company match, deferred compensation for eligible roles, fitness reimbursement or on-site fitness facilities, eligibility for tuition reimbursement, gender-inclusive benefits and many more.
Show more
Show less","AWS, Aurora MySQL, MongoDB, MySQL, Cloud Native database technologies, OpenSource database technologies, DevSecOps, Cloud databases, HA, Scaling, Monitoring, Deploying, Tuning, Sharding, Clustering","aws, aurora mysql, mongodb, mysql, cloud native database technologies, opensource database technologies, devsecops, cloud databases, ha, scaling, monitoring, deploying, tuning, sharding, clustering","aurora mysql, aws, cloud databases, cloud native database technologies, clustering, deploying, devsecops, ha, mongodb, monitoring, mysql, opensource database technologies, scaling, sharding, tuning"
Data Engineer,"Jet Support Services, Inc. (JSSI)","Chicago, IL",https://www.linkedin.com/jobs/view/data-engineer-at-jet-support-services-inc-jssi-3779281729,2023-12-17,La Grange,United States,Mid senior,Hybrid,"About JSSI
For more than 30 years, Jet Support Services, Inc. (JSSI) has been the leading independent provider of maintenance support and financial tools to the business aviation industry. With 6,000+ aircraft supported by maintenance programs and software platforms, JSSI leverages this wealth of data and purchasing power to drive cost savings and provide custom solutions that align to the interests of each client, regardless of make or model. Learn more at jetsupport.com.
JSSI products and services include
:
Maintenance Programs.
Hourly Cost Maintenance Programs to stabilize maintenance budgets, maximize aircraft availability and enhance residual value.
Parts & Leasing.
Procurement professionals who leverage JSSI’s scale and go beyond parts sourcing to find optimal customer solutions.
Maintenance Software.
Innovative digital tools that deliver actionable maintenance intelligence to enable better maintenance decisions.
Conklin & de Decker.
Impartial and accurate data to deliver full transparency into real-world aircraft operating costs.
Advisory Services.
Objective insights and independent technical advice from a global team of technical advisors and ASA-accredited appraisers for virtually any business jet, turboprop or helicopter.
Aviation Capital.
Customized asset-based finance solutions for business aviation.
Position Summary:
In this role, the Data Engineer will collaborate with stakeholders across the organization, including product management, operations, finance, marketing, and engineering, to identify, define and execute data engineering projects and initiatives. As a data engineer, you help stakeholders understand the data through exploration, and build and maintain secure and compliant data processing pipelines by using different tools and techniques. You use various Azure data services and frameworks to store and produce cleansed and enhanced datasets for analysis.
The ideal candidate will be naturally analytical and empathetic, able to naturally explain technical or complex concepts to stakeholders and committed to staying up to date with industry trends, emerging technologies, and best practices in data engineering, data science and aviation, and leverage this knowledge to drive innovation and improve existing processes.
Duties and Responsibilities:
Design, build and maintain the architecture to collect, analyze, manage and convert raw data into usable information for data scientists and business analysts to interpret.
Design and implement data storage solutions using Azure services such as Azure SQL Database, Azure Cosmos DB, and Azure Data Lake Storage
Develop and maintain data pipelines using Azure Data Factory and Azure Databricks
Create and manage data processing jobs using Azure HDInsight and Azure Stream Analytics
Perform data modeling and schema design for efficient data storage and retrieval.
Optimize data processing and storage for performance and cost efficiency.
Implement security and compliance measures for data storage and processing.
Collaborate with data scientists and analysts to provide data insights and support data-driven decision making.
Troubleshoot and resolve data processing and storage issues.
Develop and maintain documentation for data storage and processing solutions.
Stay up to date with new Azure services and technologies and evaluate their potential for improving data storage and processing solutions.
Desired Credentials:
8+ Years of experience in Database technologies like Oracle/DB2/MS SQL.
Hands on experience with Azure Data Factory, Azure Data Lake, Azure Synapse and Power BI
Strong experience in developing and maintaining data pipelines, data lakes, and data warehouses, as well as a strong understanding of data modeling and ETL (extract, transform, load) processes.
Thoroughly understand data computation languages like SQL, Python, or Scala and parallel processing and data architecture concepts.
Experience in data storage management, backup and archival processes.
Strong knowledge in data-warehousing concepts
Knowledge in big data technologies
Knowledge of Salesforce is preferred.
Experience in Master Data Management (MDM) is preferred.
Knowledge of Data Catalog and Data Governance tools, frameworks and processes is preferred.
Strong communication and collaboration skills
Prior experience in the aviation industry is preferred.
Microsoft Certified: Azure Data Fundamentals
Strongly Preferred: Microsoft Certified: Azure Data Engineer Associate
Microsoft Certified: Power BI Data Analyst Associate
Microsoft Certified: Azure AI Fundamentals
JSSI is an Equal Opportunity / Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, protected veteran status, or other characteristic protected by law.
JSSI does not accept unsolicited resumes from individual recruiters or third-party recruiting agencies in response to job postings. No fee will be paid to third parties who submit unsolicited candidates directly to our hiring managers or Recruiting Team.
Show more
Show less","Azure Data Factory, Azure Data Lake, Azure Synapse, Power BI, Azure SQL Database, Azure Cosmos DB, Azure Data Lake Storage, Azure HDInsight, Azure Stream Analytics, Oracle, DB2, MS SQL, Data modeling, ETL, SQL, Python, Scala, Parallel processing, Data architecture, Data storage management, Backup and archival processes, Data warehousing, Big data, Salesforce, Master Data Management (MDM), Data Catalog, Data Governance, Aviation","azure data factory, azure data lake, azure synapse, power bi, azure sql database, azure cosmos db, azure data lake storage, azure hdinsight, azure stream analytics, oracle, db2, ms sql, data modeling, etl, sql, python, scala, parallel processing, data architecture, data storage management, backup and archival processes, data warehousing, big data, salesforce, master data management mdm, data catalog, data governance, aviation","aviation, azure cosmos db, azure data factory, azure data lake, azure data lake storage, azure hdinsight, azure sql database, azure stream analytics, azure synapse, backup and archival processes, big data, data architecture, data catalog, data governance, data storage management, datamodeling, datawarehouse, db2, etl, master data management mdm, ms sql, oracle, parallel processing, powerbi, python, salesforce, scala, sql"
Senior Data Warehouse Engineer (Hybrid),Enova International,"Chicago, IL",https://www.linkedin.com/jobs/view/senior-data-warehouse-engineer-hybrid-at-enova-international-3759692241,2023-12-17,La Grange,United States,Mid senior,Hybrid,"We are interested in every qualified candidate who is eligible to work in the United States. However, we are not able to sponsor visas or take over sponsorship at this time.
About the role:
The Data Engineering, Warehouse, and Operations Team effectively and sustainably builds data strategy and provides data solutions and tools across the organization. We integrate, transform, and improve volumes of data at the project or enterprise level for streamlined processes, greater efficiencies, and smarter, more informed decision-making. This team is high-energy, dynamic and in a business-critical domain space. This role is an opportunity to make a real difference in the data space, and we need confident, experienced people eager to bring in solutions, with a demonstrated ability to learn fast and make that happen. #BI-Hybrid
Requirements:
Experience designing, developing, and working with dimensional models is a must.
8+ years of strong databases and SQL experience
Strong experience in data warehousing methodologies
Experience leading projects/teams from conception to completion, in fast paced dynamic environments
6+ years of experience in the design and implementation of ETL/ELT frameworks for complex data mart projects
Hands-on experience in architecting, designing, implementing, and maintaining multi-layered SQL and Python processes
Experience working with Relational Database Management Systems, including PostgreSQL, MS SQL Server, MySQL, RDS, and Cloud Data Warehouses such as Snowflake and AWS Redshift
A Bachelor’s or Master’s degree in Engineering, Computer Science, IT or related study is preferred
Nice to have: AWS and/or Snowflake Certifications
Responsibilities:
Opportunity to lead technical initiatives by architecting the solution and collaborating with team members and peers to execute the solution
Act diligently to respond to urgent projects and tasks
Troubleshooting discrepancies in existing databases, data pipelines, warehouses, and reporting
Collaborating with principals, peers, leadership, and the business
Work as a “full stack” Data Engineer contributing to each phase of the SDLC, building a new pipeline between two data sources or working with the business to design and develop a new dashboard
Advise on best practices and innovative designs/solutions
Perform other functions as assigned by management to support the operation of the business
#BI-Hybrid
Benefits & Perks:
Flexible work schedule (In-office T/W/Th and remote M/F for hybrid-eligible roles)
Health, dental, and vision insurance including mental health benefits
401(k) matching plus a roth option (U.S. Based employees only)
PTO & paid holidays off
Sabbatical program (for eligible roles)
Summer hours (for eligible roles)
Paid parental leave
DEI groups (B.L.A.C.K. @ Enova, HOLA @ Enova, Women @ Enova, Pride @ Enova, South Asians @ Enova, APEX @ Enova, and Parents @ Enova)
Employee recognition and rewards program
Charitable matching and a paid volunteer day…Plus so much more!
Full-Time Employees working 30+ hours per week are eligible for benefits; interns are not eligible.
About Enova
Enova International is a leading financial technology company that provides online financial services through our AI and machine learning-powered Colossus™platform. We serve non-prime consumers and businesses alike, while offering world-class technology and services to traditional banks—in order to create accessible credit for millions.
Being a values-driven organization is at the core of Enova’s success. We live our values by listening to our customers, challenging assumptions, thinking big, setting high expectations, and hiring and developing the best. Through our values and our commitment to making Enova an awesome place to work, we maintain an environment of inclusion and culture where our employees can thrive. You can learn more about Enova’s values and culture here.
It is our policy to provide equal employment opportunity for all persons and not discriminate in employment decisions by placing the most qualified person in each job, without regard to any other classification protected by federal, state, or local law. California Applicants: Click here to review our California Privacy Policy for Job Applicants.
Show more
Show less","Dimensional modeling, Data warehousing, ETL/ELT, Data pipelines, Python, SQL, PostgreSQL, MS SQL Server, MySQL, RDS, Cloud Data Warehouses, Snowflake, AWS Redshift, Tableau, SDLC, Agile, Jira, Confluence","dimensional modeling, data warehousing, etlelt, data pipelines, python, sql, postgresql, ms sql server, mysql, rds, cloud data warehouses, snowflake, aws redshift, tableau, sdlc, agile, jira, confluence","agile, aws redshift, cloud data warehouses, confluence, datapipeline, datawarehouse, dimensional modeling, etlelt, jira, ms sql server, mysql, postgresql, python, rds, sdlc, snowflake, sql, tableau"
Lead Data Engineer,Burtch Works,"Chicago, IL",https://www.linkedin.com/jobs/view/lead-data-engineer-at-burtch-works-3740458573,2023-12-17,La Grange,United States,Mid senior,Hybrid,"Our client, a leader in the hospitality industry, is looking for a
Lead Data Engineer
to provide technical expertise and leadership in delivering end to end data pipelines. This role will impact the teams advanced analytics capabilities and drive innovation and decision-making across the organization.
Responsibilities:
Build and maintain real-time data pipelines
Collaborate with the greater analytics organization to prepare data for modeling
Lead a team of Data Engineers and delegate tasks
Architect, implement, and maintain data warehouse and database systems for efficient data storage, retrieval, and analysis using Snowflake.
Act as a subject matter expert on data-related projects and communicate effectively with non-technical stakeholders.
Qualifications:
Bachelors degree in a STEM field preferred (Masters is a plus)
5+ Years experience in Data Engineering, 2+ years with leadership responsibilities a plus
Experience working with Snowflake as well as ETL tools like Matillion or DBT
Proven experience working with Cloud Architectures (Azure is a plus)
Expertise with Python and SQL
Work Environment: Hybrid Tuesday-Thursday
Compensation: $130,000-$150,000
KeyWords:
Matillion, DBT, Snowflake, Data Engineer, Azure, AWS, Cloud, SQL, Python, ETL
Show more
Show less","Matillion, DBT, Snowflake, Data Engineering, Data Warehousing, Data Analytics, Data Pipelines, Python, SQL, ETL, Cloud Architectures, Azure, AWS","matillion, dbt, snowflake, data engineering, data warehousing, data analytics, data pipelines, python, sql, etl, cloud architectures, azure, aws","aws, azure, cloud architectures, data engineering, dataanalytics, datapipeline, datawarehouse, dbt, etl, matillion, python, snowflake, sql"
Sr. Data Engineer,Alliant Credit Union,"Chicago, IL",https://www.linkedin.com/jobs/view/sr-data-engineer-at-alliant-credit-union-3772773871,2023-12-17,La Grange,United States,Mid senior,Hybrid,"Start a Rewarding Career with Alliant
In this role, you will provide continuous build-up and operation of an enterprise-class modern data environment, which may include various components within the Azure, Hadoop, SQL server, and Informatic stack. Works with AI/ML, LLM technologies encompassed within the cloud and on-prem data management technology stack while also having knowledge and capabilities as a systems developer. Coordinates, designs, builds, and integrates complex application technology solutions, aligned to architectural standards and definitions and ensures IT services are delivered effectively and efficiently.
Responsibilities
Development, operation and support of modern data environments (AI/ML, LLM) such as new concepts, practices, and approaches.
Design, build, deploy and maintain data pipelines using modern data tools on batch and streaming data in relational and non-relational databases in cloud and on-prem.
Collaborate with different teams on infrastructure setup, testing, monitoring, tuning/optimizing, troubleshooting and maintenance.
Collaborate with development and strategy teams on component and software vendor services in the cloud and on-prem, recommendation, installation and management of cloud-native and on prem jobs.
Collaborate with the data teams in technical investigations, development, and prototypes and corporate IT function around integrating data management ecosystem(s) with critical enterprise systems.
Develop and manage data management testing activities and create roadmaps for ongoing data management technology and growth.
Perform capacity monitoring and capacity planning on infrastructure and cloud resources.
Manage activities through the implementation and maintenance of security and governance components across various data protocols.
Participate in the design and implementation of a disaster recovery strategy for modern data components and the alignment activities with pertinent audit and compliance activities.
Provide input and develop new processes and standards in support of the organization's business/functional short-term strategies.
Address data-related problems in regard to systems integration, compatibility, and multiple-platform integration.
Support existing data pipelines on-prem, cloud and business engineering extract.
Provide technical support to meet application service level agreements.
Qualifications
Education:
Bachelors Degree - Computer Science or Related - Minimum
Years Of Experience
5 Years - Data warehouse, data lake, cloud technology or related - Minimum
3 Years - AI/ML, LLM - Preferred
In Lieu Of Education
8 years - Data warehouse, data lake, cloud technology
You Will Benefit From
Competitive medical, dental, and free vision benefits
Paid parental leave
Competitive compensation plan
Gym memberships discounts
Generous PTO and banking holidays off
Tuition reimbursement
401k with immediate employer match and vesting
Adhere to and ensure compliance of all business transactions with policy and process of the Bank Secrecy Act. Ensures compliance with all applicable state and federal laws, company procedures and policies. Maintains integrity and ethics in all actions and conversations with or regarding credit union members and their accounts; complies with Privacy Act directives.
The responsibilities listed do not contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this position. Duties, responsibilities and activities may change at any time with or without notice.
Show more
Show less","Azure, Hadoop, SQL, Informatica, AI/ML, LLM, Relational Databases, NonRelational Databases, Data Pipelines, Cloud Computing, OnPrem, Data Management, Systems Development, Data Warehousing, Data Lakes, Data Engineering, Data Governance, Security, Disaster Recovery, Capacity Planning, Data Integration, Data Migration, Data Quality, Data Visualization, Data Analytics, Data Mining, Business Intelligence, Machine Learning, Deep Learning, Natural Language Processing, Computer Vision, Robotics, Automation, Internet of Things (IoT), Edge Computing, Blockchain, Cryptocurrency, Augmented Reality (AR), Virtual Reality (VR), Mixed Reality (MR)","azure, hadoop, sql, informatica, aiml, llm, relational databases, nonrelational databases, data pipelines, cloud computing, onprem, data management, systems development, data warehousing, data lakes, data engineering, data governance, security, disaster recovery, capacity planning, data integration, data migration, data quality, data visualization, data analytics, data mining, business intelligence, machine learning, deep learning, natural language processing, computer vision, robotics, automation, internet of things iot, edge computing, blockchain, cryptocurrency, augmented reality ar, virtual reality vr, mixed reality mr","aiml, augmented reality ar, automation, azure, blockchain, business intelligence, capacity planning, cloud computing, computer vision, cryptocurrency, data engineering, data governance, data integration, data lakes, data management, data migration, data mining, data quality, dataanalytics, datapipeline, datawarehouse, deep learning, disaster recovery, edge computing, hadoop, informatica, internet of things iot, llm, machine learning, mixed reality mr, natural language processing, nonrelational databases, onprem, relational databases, robotics, security, sql, systems development, virtual reality vr, visualization"
Senior Data Engineer,Michael Page,"Chicago, IL",https://www.linkedin.com/jobs/view/senior-data-engineer-at-michael-page-3763376522,2023-12-17,La Grange,United States,Mid senior,Hybrid,"Work with the latest big data & cloud technologies in a high visibility role!|Collaborate with business stakeholders and enhance analytics for the company!
About Our Client
This client is a nationally recognized company in the food & beverage industry!
Job Description
Collaborate with business units to understand their analytics needs and gather requirements for analytics solutions.
Engineer and optimize physical and logical data models for staging, warehousing, and semantic layers in the enterprise data environment.
Develop and fine-tune ETL processes using Python, Google BigQuery Dataprocs, and Informatica to extract and transform data from various sources into dimensional data models.
Utilize advanced SQL within Google BigQuery, Informatica ETLs, or Oracle SQL Views to perform metric calculations and derive dimension attributes as needed.
Schedule and orchestrate batch and near-real-time data loading into the enterprise data warehouse while maintaining and resolving issues for existing business unit solutions.
MPI does not discriminate on the basis of race, color, religion, sex, sexual orientation, gender identity or expression, national origin, age, disability, veteran status, marital status, or based on an individual's status in any group or class protected by applicable federal, state or local law. MPI encourages applications from minorities, women, the disabled, protected veterans and all other qualified applicants.
The Successful Applicant
Must be authorized to work in the US for any employer (Visa Sponsorship not available)
Able to work Hybrid out of Chicago or Naperville, IL or Northern New Jersey
Bachelor's degree in Computer Science or a related field, and significant experience in business intelligence design and development.
7+ years of experience in reading and writing SQL for data manipulation and reporting.
7+ years of experience in designing and developing with business intelligence/reporting tools such as Oracle Business Intelligence, Tableau, or Google Cloud Platform.
5+ years of experience in data warehousing or related expertise with a focus on dimensional data modeling.
5+ years of experience in designing and developing ETL processes using Python, Google BigQuery Dataprocs, and/or Informatica ETL, as well as familiarity with performance optimization.
What's on Offer
Competitive base salary plus bonus
Annual merit increase performance reviews
Full medical, dental, vision benefits
Non-contributory pension
Profit sharing
401(k)
Stock purchase plan
Relocation assistance
PTO
Contact: Michael Convery
Quote job ref: JN-112023-6240061
Show more
Show less","Big Data, Cloud Computing, SQL, ETL, Python, Google BigQuery, Dataprocs, Informatica, Oracle, Tableau, Google Cloud Platform, Dimensional Data Modeling, Machine Learning, Data Visualization, Data Warehousing, Business Intelligence, Data Mining, Data Analytics, Hadoop, Spark, Hive, Pig, Oozie","big data, cloud computing, sql, etl, python, google bigquery, dataprocs, informatica, oracle, tableau, google cloud platform, dimensional data modeling, machine learning, data visualization, data warehousing, business intelligence, data mining, data analytics, hadoop, spark, hive, pig, oozie","big data, business intelligence, cloud computing, data mining, dataanalytics, dataprocs, datawarehouse, dimensional data modeling, etl, google bigquery, google cloud platform, hadoop, hive, informatica, machine learning, oozie, oracle, pig, python, spark, sql, tableau, visualization"
Sr Data Engineer,Pella Corporation,"Chicago, IL",https://www.linkedin.com/jobs/view/sr-data-engineer-at-pella-corporation-3724373250,2023-12-17,La Grange,United States,Mid senior,Hybrid,"JOB SUMMARY:
This role will be embedded on a cross-functional product Agile scrum team as the primary data engineer. Work with Data Architect(s) and other DAI personnel and takes the lead to develop and maintain data flows, data workflows and other code/logic to gather, create and deliver high quality reliable data to meet the needs of Pella’s business. These efforts will support Pella’s analytics and transactional needs. Additionally, this position will support Pella’s Data Enablement Technologies and Data Users with on-call responsibilities, direct end-user tickets and addressing performance or quality issues.
ESSENTIAL RESPONSIBILITIES:
Design and develop data pipelines to manage how data flows between disparate systems
Build data pipelines to feed analytics use cases, KPI or enterprise apps.
Develop data quality metrics and performs QC tests (system and visual) to verify data integrity.
Interface with architects, product managers/SMEs and product analysts to understand data needs and support the implementation of the business rules into transformation.
Document the data blending process along with the specifications and workflow/data lineage.
Perform continuous integration to ensure that every step of the pipeline is testable and automated
Lead cloud data migration, transformation and modeling projects, developing project plans and communicating project status through Agile process and in Jira for the cross-functional team
Collaborate with the business to understand backlog and refine use cases related to data management, BI reporting and data science deliverables. Research source system data, architecture and transactions.
Takes lead to perform detailed design (the Physical Data Model and transformations), based on understanding of the Logical Data Model (the business requirements)
Create design documents for data integration or data reporting projects
Develop new and improve existing processes to ensure service levels are being met
Support development of new or modify existing analytical reports
Analyze data integration problems, provide solutions and recommend corrective actions.
Analyze source system data structures and map them to target data warehouse schemas.
Must have excellent skills in requirements analysis, logical/physical modeling, data transformation and data modeling and technical governance design concepts.
Serve as a technical expert to data warehouse project teams and key business individuals for support of applications, tools, data integration, and ad-hoc analytics.
Participate in design and code reviews, documentation of design, and implementation of methodologies to ensure high quality deployments
Analyze application and data integration problems, provide solutions and recommend corrective actions.
Education/Experience
BS degree in Computer Science, Data Engineering, Software Engineering, or a related field. MBA beneficial.
5+ years' experience in data engineering / software development
QUALIFICATIONS:
Technical / functional skills (includes computer skills):
Expertise in Azure cloud technologies specifically Synapse, ADF, Delta Lake, Databricks or comparable technology experience within AWS, Snowflake and/or GCP
Experience working with architectural fabric of Salesforce or comparable CRM applications
Understanding of architecture of Data Quality, Metadata Management and Master Data Management
Understanding of Data Governance and Data Stewardship concepts
Understanding of dimensional data modeling and design as well as data population techniques for target structures such as Star Schemas.
Skilled in Python, SPARK and SQL to build production-grade data pipelines and tools
Experience navigating a modern data environment and working between on-prem & cloud technologies
Knowledge of the data science process and understanding of/experience with Data Engineering support for Data Science
Strong grasp of CI/CD operating practices
Experience operating within a Product Scrum Agile team
Experience with MS Office, Outlook, Jira
Leadership Skills:
Strong communication and collaboration skills, good project management methodology
Certifications or licenses: None
Travel Expected:
5-10% of time
Show more
Show less","Data Engineering, Data Architect, Agile, Data flows, Data workflows, Data quality metrics, SQL, Python, SPARK, Data Blending, Data Lineage, Continuous Integration, Jira, Cloud Data Migration, Data Modeling, Logical Data Model, Physical Data Model, Data Transformation, Data Governance, Data Stewardship, Dimensional Data Modeling, Star Schemas, Azure, Synapse, ADF, Delta Lake, Databricks, AWS, Snowflake, GCP, Salesforce, Data Quality, Metadata Management, Master Data Management, CI/CD, Product Scrum Agile, MS Office, Outlook","data engineering, data architect, agile, data flows, data workflows, data quality metrics, sql, python, spark, data blending, data lineage, continuous integration, jira, cloud data migration, data modeling, logical data model, physical data model, data transformation, data governance, data stewardship, dimensional data modeling, star schemas, azure, synapse, adf, delta lake, databricks, aws, snowflake, gcp, salesforce, data quality, metadata management, master data management, cicd, product scrum agile, ms office, outlook","adf, agile, aws, azure, cicd, cloud data migration, continuous integration, data architect, data blending, data engineering, data flows, data governance, data lineage, data quality, data quality metrics, data stewardship, data transformation, data workflows, databricks, datamodeling, delta lake, dimensional data modeling, gcp, jira, logical data model, master data management, metadata management, ms office, outlook, physical data model, product scrum agile, python, salesforce, snowflake, spark, sql, star schemas, synapse"
Lead Data Warehouse Engineer (Hybrid),Enova International,"Chicago, IL",https://www.linkedin.com/jobs/view/lead-data-warehouse-engineer-hybrid-at-enova-international-3759687647,2023-12-17,La Grange,United States,Mid senior,Hybrid,"We are interested in every qualified candidate who is eligible to work in the United States. However, we are not able to sponsor visas or take over sponsorship at this time.
About the role:
The Data Engineering, Warehouse, and Operations Team effectively and sustainably builds data strategy and provides data solutions and tools across the organization. We integrate, transform, and improve volumes of data at the project or enterprise level for streamlined processes, greater efficiencies, and smarter, more informed decision-making. This team is high-energy, dynamic and in a business-critical domain space. This role is an opportunity to make a real difference in the data space, and we need confident, experienced people eager to bring in solutions, with a demonstrated ability to learn fast and make that happen.
Requirements:
Experience designing, developing, and working with dimensional models is a must.
10+ years of strong database and SQL experience
7+ years of experience leading projects/teams from conception to completion in fast-paced dynamic environments
5+ years of Data Warehousing experience/methodologies
6+ years of experience in designing and implementing ETL/ELT frameworks for complex data mart projects
Proven experience in architecting, designing, implementing, and maintaining multi-layered SQL and Python processes
Experience working with Relational Database Management Systems, including PostgreSQL, MS SQL Server, MySQL, RDS, and Cloud Data Warehouses such as Snowflake and AWS Redshift
A Bachelor’s or Master’s degree in Engineering, Computer Science, IT or related study is preferred
Nice to have: AWS and/or Snowflake Certifications
Responsibilities:
Opportunity to lead technical initiatives by designing and architecting solutions that meet the business needs, ensuring data accuracy, reliability, and performance
Collaborating with principals, peers, leadership, and the business to execute solutions
Act diligently to respond to urgent projects and tasks
Troubleshooting discrepancies in existing databases, data pipelines, warehouses, and reporting
Work as a “full stack” Data Engineer, contributing to each phase of the SDLC, building a new pipeline between two data sources or working with the business to design and develop a new dashboard
Advise on best practices and innovative designs/solutions
Perform other functions as assigned by management to support the operation of the business
#BI-Hybrid
Benefits & Perks:
Flexible work schedule (In-office T/W/Th and remote M/F for hybrid-eligible roles)
Health, dental, and vision insurance including mental health benefits
401(k) matching plus a roth option (U.S. Based employees only)
PTO & paid holidays off
Sabbatical program (for eligible roles)
Summer hours (for eligible roles)
Paid parental leave
DEI groups (B.L.A.C.K. @ Enova, HOLA @ Enova, Women @ Enova, Pride @ Enova, South Asians @ Enova, APEX @ Enova, and Parents @ Enova)
Employee recognition and rewards program
Charitable matching and a paid volunteer day…Plus so much more!
Full-Time Employees working 30+ hours per week are eligible for benefits; interns are not eligible.
About Enova
Enova International is a leading financial technology company that provides online financial services through our AI and machine learning-powered Colossus™platform. We serve non-prime consumers and businesses alike, while offering world-class technology and services to traditional banks—in order to create accessible credit for millions.
Being a values-driven organization is at the core of Enova’s success. We live our values by listening to our customers, challenging assumptions, thinking big, setting high expectations, and hiring and developing the best. Through our values and our commitment to making Enova an awesome place to work, we maintain an environment of inclusion and culture where our employees can thrive. You can learn more about Enova’s values and culture here.
It is our policy to provide equal employment opportunity for all persons and not discriminate in employment decisions by placing the most qualified person in each job, without regard to any other classification protected by federal, state, or local law. California Applicants: Click here to review our California Privacy Policy for Job Applicants.
Show more
Show less","Data Engineering, Data Warehousing, SQL, PostgreSQL, MS SQL Server, MySQL, RDS, Snowflake, AWS Redshift, Python, ETL/ELT, Data Mart, Relational Database Management Systems, SDLC","data engineering, data warehousing, sql, postgresql, ms sql server, mysql, rds, snowflake, aws redshift, python, etlelt, data mart, relational database management systems, sdlc","aws redshift, data engineering, data mart, datawarehouse, etlelt, ms sql server, mysql, postgresql, python, rds, relational database management systems, sdlc, snowflake, sql"
Senior Data Engineer,Aspen Dental,"Chicago, IL",https://www.linkedin.com/jobs/view/senior-data-engineer-at-aspen-dental-3764793121,2023-12-17,La Grange,United States,Mid senior,Hybrid,"The Aspen Group (TAG) is one of the largest and most trusted retail healthcare business support organizations in the U.S., supporting 15,000 healthcare professionals and team members at more than 1,000 health and wellness offices across 46 states in three distinct categories: Dental care, urgent care, and medical aesthetics. Working in partnership with independent practice owners and clinicians, the team is united by a single purpose: to prove that healthcare can be better and smarter for everyone. ADMI provides a comprehensive suite of centralized business support services that power the impact of four consumer-facing businesses: Aspen Dental, ClearChoice Dental Implant Centers, WellNow Urgent Care, and Chapter Aesthetic Studio. Each brand has access to a deep community of experts, tools, and resources to grow its practices and an unwavering commitment to delivering high-quality consumer healthcare experiences at scale.
As a part of the data engineering team, you will join one of our teams focused on a core business domain – clinical operation that generates revenue and claim. You will work alongside developers modernizing our legacy systems by developing new software and services using cloud-native GCP technologies. Our stack uses MS-SQL, SSIS, GCP Big Query, and Python, using advanced SQL and NoSQL data layers on Kubernetes operating as an active-active multi-region solution.
R2023-017821
Show more
Show less","Cloud Computing, GCP, MSSQL, SSIS, GCP Big Query, Python, SQL, NoSQL, Kubernetes, ActiveActive MultiRegion Solution","cloud computing, gcp, mssql, ssis, gcp big query, python, sql, nosql, kubernetes, activeactive multiregion solution","activeactive multiregion solution, cloud computing, gcp, gcp big query, kubernetes, mssql, nosql, python, sql, ssis"
Sr. Data Analyst: Sales Engineer,EDGE,"Chicago, IL",https://www.linkedin.com/jobs/view/sr-data-analyst-sales-engineer-at-edge-3780723266,2023-12-17,La Grange,United States,Mid senior,Hybrid,"Edge (www.edgescore.com) is a data and analytics platform that harnesses bank transaction data to measure consumer credit risk. Founded in 2021, Edge is part of a larger family of FinTech brands that sits underneath our parent company, NinjaHoldings (www.ninjaholding.com). NinjaHoldings’ brands also include CreditNinja, a high velocity online consumer lending business, and NinjaCard, a neobanking platform focused on consumers in the emerging credit space.
At our core, Edge is a cash flow bureau (versus a credit bureau). We have proprietary technology and data sets that allow us to extract predictive features from granular bank transaction data and measure the risk of a consumer. Our solutions are built on a massive data lake that uniquely combines performance data from over $500 million in loans with over 3 billion consumer-permissioned bank transactions, an “edge” that can only be replicated at considerable time and cost. We believe that we provide a benefit to lenders and consumers by providing a more accurate financial picture of consumers who are overlooked, or incorrectly scored by the traditional bureaus, ultimately opening up access to fairer, more accurately priced credit.
With offices in Chicago and employees working remotely from around the world, we are a lean and innovative team always seeking like-minded talent to join us in our fight to disrupt consumer finance.
Key Responsibilities:
Serve as the lead technical consultant for both prospective and existing customers, translating intricate data insights into actionable strategies and solutions.
Independently conduct in-depth data analysis on financial datasets, unveiling insights to bolster customer growth and drive product adoption.
Partner with the sales and customer success teams, spearheading technical discussions and ensuring our offerings align seamlessly with customer objectives.
Develop and deliver high-impact presentations and reports for a diverse set of customers, underscoring the unparalleled value of our data-driven propositions.
Take the initiative in educating and mentoring customer technical and business resources, ensuring they derive the utmost value from our suite of solutions.
Requirements:
3-5 years of experience in a senior role within sales engineering, risk analysis, analytics consulting, or lending.
Medium to advanced proficiency in data analysis tools, particularly Python and SQL.
Exceptional communication and presentation skills, with a talent for elucidating complex concepts to a varied audience.
Proven capability to function independently, owning customer engagements from inception to completion.
Familiarity with AWS will be considered a strong advantage.
Prior experience in the lending sector or an in-depth understanding of its landscape will be given preference.
A foundational grasp of data science and machine learning methodologies is highly desirable.
Benefits:
Competitive salary and benefits package
Fun, fast-paced work environment
Dynamic start-up culture
Ability to make an immediate impact in a growth stage company
Convenient downtown Chicago office located in the heart of the city
Equal opportunity employer
Show more
Show less","Data Analysis, Python, SQL, AWS, Data Science, Machine Learning, Tableau, Sales Engineering, Risk Analysis, Analytics Consulting, Lending, Communication, Presentation Skills, Customer Engagement","data analysis, python, sql, aws, data science, machine learning, tableau, sales engineering, risk analysis, analytics consulting, lending, communication, presentation skills, customer engagement","analytics consulting, aws, communication, customer engagement, data science, dataanalytics, lending, machine learning, presentation skills, python, risk analysis, sales engineering, sql, tableau"
Software/Data Engineer,EDGE,"Chicago, IL",https://www.linkedin.com/jobs/view/software-data-engineer-at-edge-3780723208,2023-12-17,La Grange,United States,Mid senior,Hybrid,"Edge (www.edgescore.com) is a data and analytics platform that harnesses bank transaction data to measure consumer credit risk. Founded in 2021, Edge is part of a larger family of FinTech brands that sits underneath our parent company, NinjaHoldings (www.ninjaholding.com). NinjaHoldings’ brands also include CreditNinja, a high velocity online consumer lending business, and NinjaCard, a neobanking platform focused on consumers in the emerging credit space.
At our core, Edge is a cash flow bureau (versus a credit bureau). We have proprietary technology and data sets that allow us to extract predictive features from granular bank transaction data and measure the risk of a consumer. Our solutions are built on a massive data lake that uniquely combines performance data from over $500 million in loans with over 3 billion consumer-permissioned bank transactions, an “edge” that can only be replicated at considerable time and cost. We believe that we provide a benefit to lenders and consumers by providing a more accurate financial picture of consumers who are overlooked, or incorrectly scored by the traditional bureaus, ultimately opening up access to fairer, more accurately priced credit.
With offices in Chicago and employees working remotely from around the world, we are a lean and innovative team always seeking like-minded talent to join us in our fight to disrupt consumer finance.
Key Responsibilities:
Work collaboratively with data scientists and engineers on the team to scope, design, and develop high-performance data analysis services
Ensure the deployment and maintenance of analytics services, making sure they align with business objectives and industry best practices
Coordinate seamlessly with the platform and product teams, contributing to the deployment and iterative improvement of our analytics services
Participate in multiple projects simultaneously, adapting to priorities and thriving in a dynamic, fast-paced environment
Advocate for best engineering practices, including coding standards, code reviews, and continuous integration
Provide technical expertise and recommendations in assessing new software projects and initiatives
Qualifications:
A minimum of 2 years of professional experience in software engineering, data engineering, or analytics engineering
Strong proficiency with Python and SQL
Familiarity with AWS and its suite of data and cloud services
An understanding of data science and machine learning workflows is a bonus
Strong organizational and multitasking skills, with a capacity to handle multiple projects and tasks concurrently
Strong interpersonal and communication skills, with the ability to collaborate effectively across diverse teams
Experience in lending, banking, financial services, or risk analysis will be considered a significant plus.
Benefits:
Competitive salary and benefits package
Flexible, remote work
Fun, fast-paced work environment
Dynamic start-up culture
Ability to make an immediate impact in a growth stage company
Convenient downtown Chicago office located in the heart of the city
Equal opportunity employer
Show more
Show less","Python, SQL, AWS, Data science, Machine learning, Data engineering, Software engineering, Analytics engineering","python, sql, aws, data science, machine learning, data engineering, software engineering, analytics engineering","analytics engineering, aws, data engineering, data science, machine learning, python, software engineering, sql"
Data Analytics Developer,Alpine Solutions Group,"Chicago, IL",https://www.linkedin.com/jobs/view/data-analytics-developer-at-alpine-solutions-group-3769021357,2023-12-17,La Grange,United States,Mid senior,Hybrid,"MUSTS:
3+ as a data analytics developer working on the entire data pipeline
Very strong programming and querying with: SAS and SQL
Strong experience using SQL to write ETL code
Experience digging through millions of records of data to find insights
3+ years of both SQL+ PowerBI– querying data all day long. Ability to write SQL queries with complexity every day then converting into PowerBI reports
Powerbi- actually developing powerbi reports and must have actually developed and deployed powerbi dashboards in a corporate environment – published reports in the enterprise used by decision makers
Worked with power bi desktop
Executive presence- ability to engage directly with C-level executives when sharing data, reports and presentations
PLUS:
Knowledge of credit portfolio data
Knowledge of consumer underwriting platforms, etc
Azure data factory is a plus Microsoft fabric is a plus. Delta lake house, databricks is plus.
Bachelor’s degree in related field; advanced coursework/training related to computer science, management information systems, and mathematics/statistics
Comes from the cloud
Python programming
DAY TO DAY:
Our fast growing banking client is looking to hire a Data Analytics Developer to focus on their retail credit data. This role will sit within the chief data office of the bank and partner with other team members to monitor and optimize the risk, profitability, performance, and composition o the retail loan portfolios. You will help the business to grow the retail loan portfolio while also taking into account the bank’s appetite for credit risk. You will also maintain the data need for credit reporting, optimizing the overall credit strategy, and optimize the capital and risk- weighted assets.
The two main parts to this position are: developing and implementing data services and analysis and optimization of retail credit. You will create ETL artifacts, help to develop a data mart, and implement and develop reporting and dashboards for performance monitoring. You will also work to forecast portfolio performances and use statistical data techniques for the optimization of retail credit as a whole.
Overall, this role is both deep analysis and reporting both, and eventually this role will work to build predictive models and auto decisioning for our consumer loans. You will work up and down the entire data pipeline and truly won all aspects of data analytics.
Show more
Show less","Data Analytics, SAS, SQL, ETL, PowerBI, Python, Azure Data Factory, Microsoft Fabric, Delta Lake House, Databricks, Credit Portfolio Data, Consumer Underwriting Platforms, Cloud Computing, Data Mart, Reporting, Dashboards, Performance Monitoring, Portfolio Performance, Statistical Data Techniques, Predictive Modeling, Auto Decisioning","data analytics, sas, sql, etl, powerbi, python, azure data factory, microsoft fabric, delta lake house, databricks, credit portfolio data, consumer underwriting platforms, cloud computing, data mart, reporting, dashboards, performance monitoring, portfolio performance, statistical data techniques, predictive modeling, auto decisioning","auto decisioning, azure data factory, cloud computing, consumer underwriting platforms, credit portfolio data, dashboard, data mart, dataanalytics, databricks, delta lake house, etl, microsoft fabric, performance monitoring, portfolio performance, powerbi, predictive modeling, python, reporting, sas, sql, statistical data techniques"
Senior Enterprise Data Engineer,UScellular,"Chicago, IL",https://www.linkedin.com/jobs/view/senior-enterprise-data-engineer-at-uscellular-3770790363,2023-12-17,La Grange,United States,Mid senior,Hybrid,"Responsible for developing and maintaining the interfaces and data extracts for business processes. This person will design and write programs and scripts required to extract, transform, clean, and move data from the business systems into the enterprise data warehouse, data mart and operational data stores. Other related responsibilities include ongoing support, maintenance, and other ETL related information technology endeavors such as data migration, data cleansing and data modeling. Establish and maintain working relationships with technical and architectural team members across the technology organization. This person will participate in requirement meetings with Business Analysts to understand analytical content needs.
Location Note
This role is a hybrid work arrangement where the candidate can perform this role in-office or remote. In-office expectations within our operating footprint will be limited and based on an on purpose, for a purpose model should a meeting or collaborative session be scheduled. For this role, we prefer candidates that either live in the Chicagoland area or can reasonably commute to our office in Chicago should a meeting or collaborative session be scheduled.
Essential Functions
Provide strategic thinking leadership pertaining to new ways of leveraging information to improve business processes.
Design and document detailed ETL specifications based on business requirements.
Design and develop using procedural and record-based patterns. Analyze functional specifications and assist in designing potential technical solutions.
Assist in documenting the requirements, resolving ambiguities and conflicts, and ensuring requirements are complete.
Define, document, develop, enhance, and maintain Software application modules in Informatica, Talend, ksh, Perl, SQL.
Develop high performance application using tuning ETL methods related to Informatica/Talend and SQL.
Develop test plans and tests all code following standards and best practices.
Perform design validation, reconciliation, and error handling for all code.
Understand and execute release management and change management processes for changes to applications.
Identifies, troubleshoots, and resolves application issues and code defects.
Production support activities involving the identification, troubleshooting and resolution of application issues and code defects.
Work with source system developers and business owners to define data extraction methodologies.
Work with Data Modelers to develop data warehouse models, design specifications, metadata process and documentation.
Understand SDLC concepts and complies with the process specifically any iteration of an Agile process.
Demonstrate success working in a team-based environment by following standardized processes.
Participate and share expertise in peer reviews of documentation and code.
Manage individual workload and deliver to the agreed upon project milestones.
Coach and mentor junior team members to facilitate their professional development.
Demonstrate ability to work in team environment and help resolve issues.
Communicate status / risks to manager regarding delivering solution on time.
Required Experience
Bachelor’s degree in the following areas of study: Computer Science, Information Systems, Electrical Engineering or related field or equivalent work experience required.
Understanding of BI/ETL development in the IT industry with recent development, system administration, application tuning and debugging experience.
5+ years development experience with database engines including Oracle, Mongo dB and Hadoop.
5+ years of experience in ETL development, Informatica Programming, Talend Programming, Strong Database (Modeling, SQL), Oracle, Mongo dB, Hadoop, SQL, and PL/SQL.
Knowledge of database performance factors, monitoring tools and tuning procedures
Familiarity with Data Integration/Warehousing methodology and techniques.
Knowledge of the company's business practices, and familiarity with the industry's products and services is preferred.
Understanding of SDLC and production environments.
Familiarity with SCM tools specifically, GIT, Jenkins and JFrog.
Strong verbal and written communication skills.
Participate in 24/7 on-call support rotation.
Technical Skills -
skills that will be involved in this environment:
Business Applications: Billing & Rating, Activation & Provisioning, Inventory Management, Sales & Marketing, Campaign Management, Dealer Compensation/Sales Commission, CRM/Customer Service
Office Automation: Word, Excel, PowerPoint, Outlook
Planning & Design: Microsoft Project, Visio
Software Engineering: PL-SQL, SQL Plus, K Shell, C Shell, Perl
Integration Technologies: Service Oriented Architecture (SOA), Informatica, Talend, or other leading ETL tools
Databases: Oracle, Mongo dB, SQL, Data Warehousing, OLAP Systems, and/or Hadoop
O/S & Network: IBM AIX, Sun Solaris, HP/UX, LINUX
Job
Information Technology
Location(s)
Illinois-CHICAGO_IL
UScellular® is an EEO employer and gives consideration to qualified applicants without regard to race/color/age/religion/sex/sexual orientation/gender identity/national origin/disability/veteran status, pregnancy or genetic information.
Show more
Show less","Data Warehousing, Data Mining, Data Transformation, Data Integration, ETL, Informatica, Talend, SQL, Modeling, Data Migration, Data Cleansing, KSH, Perl, PL/SQL, Hadoop, SDLC, Agile, Git, Jenkins, JFrog, Business Intelligence, Data Extraction, Data Migration, Data Modeling, Data cleansing, Database performance, Monitoring tools, Tuning procedures, Service Oriented Architecture (SOA), Oracle, Mongo DB, Microsoft Project, Visio, PL/SQL, SQL Plus, K Shell, C Shell, Perl","data warehousing, data mining, data transformation, data integration, etl, informatica, talend, sql, modeling, data migration, data cleansing, ksh, perl, plsql, hadoop, sdlc, agile, git, jenkins, jfrog, business intelligence, data extraction, data migration, data modeling, data cleansing, database performance, monitoring tools, tuning procedures, service oriented architecture soa, oracle, mongo db, microsoft project, visio, plsql, sql plus, k shell, c shell, perl","agile, business intelligence, c shell, data extraction, data integration, data migration, data mining, data transformation, database performance, datacleaning, datamodeling, datawarehouse, etl, git, hadoop, informatica, jenkins, jfrog, k shell, ksh, microsoft project, modeling, mongo db, monitoring tools, oracle, perl, plsql, sdlc, service oriented architecture soa, sql, sql plus, talend, tuning procedures, visio"
Software Engineer (Python or Scala) - FinTech Data Platform,Huxley,"Chicago, IL",https://www.linkedin.com/jobs/view/software-engineer-python-or-scala-fintech-data-platform-at-huxley-3771751611,2023-12-17,La Grange,United States,Mid senior,Hybrid,"A top tier prop shop is hiring for Software Engineer on their Data Engineering / Data Platform team.
In this role, you will play a integral role in ensuring that data is available, reliable, and accessible across the Quant, Trading, and Compliance business units.
Requirements:
3+ years of professional engineering experience required
Design, develop and test batch and real time integration with data sources - internal and external
Experience with a variety of protocols, API's, programming languages -
Python (or Scala)
and AWS Cloud technologies
Build automated data validation tests to ensure data accuracy and that data is captured/ingested on time
Experience with distributed systems - strong nice to have
Local to Chicago
*Note: Full time position, hybrid (2 days in) to Chicago loop office
Desired Skills and Experience
python, data ingestion, object oriented python, backend python, scala
EOE Statement:
Specialist Staffing Group is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status.
To find out more about Huxley please visit www.huxley.com
Show more
Show less","Python, Scala, AWS Cloud, Data Ingestion, ObjectOriented Python, Backend Python","python, scala, aws cloud, data ingestion, objectoriented python, backend python","aws cloud, backend python, data ingestion, objectoriented python, python, scala"
Sr. Cloud Data Engineer (AWS),MMD Services,"Chicago, IL",https://www.linkedin.com/jobs/view/sr-cloud-data-engineer-aws-at-mmd-services-3779373515,2023-12-17,La Grange,United States,Mid senior,Hybrid,"We’re representing a modern FinTech spin-off founded by a successful publicly traded consumer financial services company.
They've been underwriting with cash flow data for over six years—a practice only now catching on with other businesses. Their knack for attracting top industry talent and developing
cutting-edge engineering products, coupled with their innovative business model, positions them ahead of the curve compared to the rest of the industry.
They are seeking a
Senior Data Engineer
with proficiency in AWS data warehouse and Python development to elevate their Analytics Platform. Collaborating directly with c-level executives and the Data Science team, you'll ensure clients receive actionable data in order to make
well-informed decisions.
Must-Have Qualifications:
Proficiency in designing, building, and managing data infrastructure within an AWS data warehouse.
Expertise on the AWS suite of products: Amazon Redshift, Amazon S3, AWS Data Pipeline, AWS Lambda, Amazon Kinesis, etc.
6+ years of strong Python development skills.
Skilled in data preparation, administration, and designing scalable architectures to efficiently handle large volumes of data crucial for ML model processing.
Previous experience collaborating closely with senior Data Science teams.
Strong background in data modeling, data quality, data governance and security
Proven track record of taking ownership of complex projects from inception to completion.
Exceptional communication and presentation skills to effectively convey complex findings to various stakeholders.
Desirable Qualifications (Plusses):
Previous experience in the fintech industry, demonstrating an understanding of financial data and industry trends.
Experience collaborating directly with sales teams to secure and win business.
Perks!
Competitive salary with bonus
Unlimited PTO
Equity
Health benefits & 401(k) vesting first day of employment
onvenient downtown Chicago office in the Loop
Dynamic start-up culture
Show more
Show less","AWS, Python, Amazon Redshift, Amazon S3, AWS Data Pipeline, AWS Lambda, Amazon Kinesis, Data preparation, Data administration, Scalable architectures, ML model processing, Data modeling, Data quality, Data governance, Data security, Complex projects, Communication skills, Presentation skills, Financial data, Industry trends, Sales teams","aws, python, amazon redshift, amazon s3, aws data pipeline, aws lambda, amazon kinesis, data preparation, data administration, scalable architectures, ml model processing, data modeling, data quality, data governance, data security, complex projects, communication skills, presentation skills, financial data, industry trends, sales teams","amazon kinesis, amazon redshift, amazon s3, aws, aws data pipeline, aws lambda, communication skills, complex projects, data administration, data governance, data preparation, data quality, data security, datamodeling, financial data, industry trends, ml model processing, presentation skills, python, sales teams, scalable architectures"
Hybrid Work - Need Azure Data Engineer in Rosemont IL or Milwaukee WI,Steneral Consulting,"Rosemont, IL",https://www.linkedin.com/jobs/view/hybrid-work-need-azure-data-engineer-in-rosemont-il-or-milwaukee-wi-at-steneral-consulting-3773319960,2023-12-17,La Grange,United States,Mid senior,Hybrid,"2-3 days onsite each week, must be local
2 Locations: Rosemont, IL and Milwaukee, WI
Need Valid LinkedIn
Need 5 years of data engineering, great at scalability, repeatability, not just writing efficient ETL code. Must have Azure.
We are currently seeking a Data Engineer responsible for the development and implementation of technical solutions. This role leverages technical expertise in database applications, ETL processes, and automation in support of internal and external customers’ data exchange and integration needs. Help move our bank to a modern data platform, including the integration work for our Salesforce platform and our digital banking platform.
Key Competencies for Position
Ability to work independently and be able to collaborate and guide other team members.
Critical thinking and problem solving skills to ensure the right-sized solution is developed for the task at hand.
Technical passion to move the bank towards modern data platform principles.
Required Education, Experience And Skills
Over 5 years of experience data engineering experience, including developing ETL/ELT. \
3 years of experience with SSIS/Azure Data Factory, and some knowledge of Python.
Expertise in building scalable data products and platforms focused on metadata driven approaches towards ingestion and transformation.
Experience working with stakeholders on gathering requirements and translating to data solutions with a focus on developing repeatable patterns.
Experience in orchestration tools or job schedulers, building dependency and event driven approaches towards loading and moving data.
Experience with scripting tools such as Python and/or C#.
Very good working experience of T-SQL. Proficiency in complex stored procedures, User Defined Functions (UDF), and query optimization.
Knowledge of CDC and experience preserving historical data in a variety of patterns from different source systems.
Excellent written and oral communication, including experience presenting designs and solutions to senior leadership.
Experience working in an Agile environment, leveraging Azure DevOps or JIRA for work management.
Exposure to code repositories such as Azure DevOps, Git, etc.
Working knowledge of MS SQL Server and SSMS
Working knowledge of analytical and data visualization technology such as Power BI
Working knowledge of cloud-based data architectures using the Azure stack and/or Snowflake
Duties/Responsibilities
Design, development, and maintenance (enhancements and maintenance) of ETL/ELT processes.
Design and development of repeatable frameworks and functions to be leveraged by other data engineers within the bank.
Focus on Operational Excellence- ensuring we are monitoring, validating, and communicating the health and status of our data products.
Collaborate with Enterprise Architecture, Info Security and Data Governance organizations.
Confer with relevant team members where necessary. Study systems flow, data usage, and work processes.
Deliver functional data products that have been thoroughly tested.
Follow the development team’s SDLC process. Accurately estimate time required to complete projects and tasks.
Meet mutually agreed upon deadlines for completion of modules throughout the program development.
Work closely with the existing DBAs to design, model, develop, and maintain existing and new database objects required for all business solutions
Show more
Show less","Data engineering, ETL/ELT development, SSIS/Azure Data Factory, Azure, Python, Scalability, Metadatadriven approaches, Gathering requirements, Repeatable patterns, Orchestration tools, Job schedulers, Dependencydriven approaches, Scripting tools, TSQL, Complex stored procedures, User Defined Functions (UDF), Query optimization, CDC, Historical data preservation, Agile, Azure DevOps, JIRA, Code repositories, MS SQL Server, SSMS, Power BI, Cloudbased data architectures, Azure stack, Snowflake, DevOps, SDLC, Database design, Database modeling, Database development, Database maintenance, Database objects","data engineering, etlelt development, ssisazure data factory, azure, python, scalability, metadatadriven approaches, gathering requirements, repeatable patterns, orchestration tools, job schedulers, dependencydriven approaches, scripting tools, tsql, complex stored procedures, user defined functions udf, query optimization, cdc, historical data preservation, agile, azure devops, jira, code repositories, ms sql server, ssms, power bi, cloudbased data architectures, azure stack, snowflake, devops, sdlc, database design, database modeling, database development, database maintenance, database objects","agile, azure, azure devops, azure stack, cdc, cloudbased data architectures, code repositories, complex stored procedures, data engineering, database design, database development, database maintenance, database modeling, database objects, dependencydriven approaches, devops, etlelt development, gathering requirements, historical data preservation, jira, job schedulers, metadatadriven approaches, ms sql server, orchestration tools, powerbi, python, query optimization, repeatable patterns, scalability, scripting tools, sdlc, snowflake, ssisazure data factory, ssms, tsql, user defined functions udf"
Big Data Engineer,Accuro,"Deerfield, IL",https://www.linkedin.com/jobs/view/big-data-engineer-at-accuro-3667474887,2023-12-17,La Grange,United States,Mid senior,Hybrid,"3+ years of development experience with big data technologies
3+ years using Apache Spark (mainly with the SQL module)
Advanced knowledge in programming languages such as Scala or Java
Advanced knowledge of Scala unit testing
Advanced knowledge in writing SQL (Hive Syntax)
Strong analytic skills related to working with unstructured datasets
Exposure to Cloud Service Providers (AWS)
Experience with Software Development Life Cycle
Eager to learn, adapt and perfect your work; you seek out help and put it to good use
Experience in designing and building large scale enterprise data solutions
Creative, attention to detail, multitasking and organizational capacity
Impeccable written and verbal communication skills.
Strong technical, process and problem-solving proficiency
Demonstrated business acumen, and cross-collaboration capabilities
Ability to set and deliver on priorities and deal with a degree of ambiguity
Exceptional interpersonal and communications capabilities
Display organizational and emotional intelligence
Able to hit the ground running with a can-do attitude
Additional Role Desirables
Big Data Certification(s)
Certified Scrum Master or Certified Product Owner
Experience with JIRA and Confluence Team Collaboration Software
Exposure to Medical / Pharmaceutical digital marketing industry
Show more
Show less","Apache Spark, Scala, Java, SQL, Hive, Cloud Service Providers (AWS), Software Development Life Cycle, Unit Testing, Enterprise Data Solutions, Jira, Confluence","apache spark, scala, java, sql, hive, cloud service providers aws, software development life cycle, unit testing, enterprise data solutions, jira, confluence","apache spark, cloud service providers aws, confluence, enterprise data solutions, hive, java, jira, scala, software development life cycle, sql, unit testing"
"Data Analyst, HR Operations",Alvarez & Marsal,"Tampa, FL",https://www.linkedin.com/jobs/view/data-analyst-hr-operations-at-alvarez-marsal-3770545197,2023-12-17,Clearwater,United States,Associate,Onsite,"Description
Overview
Alvarez & Marsal (A&M) is a leading independent global professional services firm, specializing in providing turnaround management and performance improvement for corporations and government entities. With almost 9,000 professionals based in locations across North America, Europe, Asia, and Latin America, our firm excels in problem-solving and value creation. Drawing on a strong operational heritage and hands-on approach, our professionals work closely with organizations and stakeholders to help tackle complex issues.
A&M is regularly recognized by leading publications as a premier consulting firm, including placement in the top 25 of the “Vault Consulting 50” in each of the last ten years. A&M is seeking a highly organized team member to support our HR Operations Team. We are looking for someone with an eye for detail, who enjoys administrative work and creating and supporting efficiencies. The ideal candidate will be proactive and looking for a role in an entrepreneurial and complex organization with experience working in administration in a fast-paced office environment.
Main Roles & Responsibilities:
Accountable for the delivery of all HR integrity audits/metrics weekly, biweekly, monthly, quarterly and/or annually.
Delivers a range of analysis activities – data extraction, synthesis of data, and recommendations.
Analyzes trends and metrics in partnership with HR Operations team and HR Business Partner group to develop solutions, programs, and policies.
Creates persuasive presentations that tell a story using visual representations of data through graphs, charts and models.
Design new processes, identify and escalate gaps/risks, and provide recommendations for routine high-quality reporting.
Performs audits to determine defects, data anomalies, gaps or completeness and work with HR Business Partners to find a resolution.
Assists in ad hoc projects that require the analysis of data and metrics, and other assignments.
Compiles and maintains reports.
Other duties as assigned
Ideal candidate profile:
Associate or bachelor’s degree in business administration, Computer Science, Human Resources or other relevant fields.
1-2 years of relevant experience with handling large data set management
Experience in Tableau or PowerBi is preferred, but not required
Excellent communication skills, both oral and written.
Advanced computer skills using Microsoft Office Suite (PowerPoint, Outlook, Excel).
High ethical standards, integrity, and adherence to confidentiality.
Must be detail orientated.
Excellent time management: ability to handle and prioritize multiple tasks in a fast-paced environment.
Enjoys designing new processes and procedures.
Willingness to take initiative and ownership of projects.
Previous experience in a corporate environment.
Previous experience in Workday (HRIS) is a plus.
The salary range is $70,000
- $80,000
annually, dependent on several variables including but not limited to education, experience, skills, and geography. In addition, A&M offers a discretionary bonus program which is based on a number of factors, including individual and firm performance. Please ask your recruiter for details.
Diversity & Inclusion
A&M’s entrepreneurial culture celebrates independent thinkers and doers who can positively impact our clients and shape our industry. The collaborative environment and engaging work—guided by A&M’s core values of Integrity, Quality, Objectivity, Fun, Personal Reward, and Inclusive Diversity—are the main reasons our people love working at A&M. Inclusive Diversity means we embrace diversity, and we foster inclusiveness, encouraging everyone to bring their whole self to work each day. It runs through how we recruit, develop employees, conduct business, support clients, and partner with vendors. It is the A&M way.
Equal Opportunity Employer
It is Alvarez & Marsal’s practice to provide and promote equal opportunity in employment, compensation, and other terms and conditions of employment without discrimination because of race, color, creed, religion, national origin, ancestry, citizenship status, sex or gender, gender identity or gender expression (including transgender status), sexual orientation, marital status, military service and veteran status, physical or mental disability, family medical history, genetic information or other protected medical condition, political affiliation, or any other characteristic protected by and in accordance with applicable laws. Employees and Applicants can find A&M policy statements and additional information by region here.
Unsolicited Resumes from Third-Party Recruiters
Please note that as per A&M policy, we do not accept unsolicited resumes from third-party recruiters unless such recruiters are engaged to provide candidates for a specified opening and in alignment with our Inclusive Diversity values. Any employment agency, person or entity that submits an unsolicited resume does so with the understanding that A&M will have the right to hire that applicant at its discretion without any fee owed to the submitting employment agency, person or entity.
Show more
Show less","Data analysis, Data extraction, Data synthesis, Data visualization, Microsoft Office Suite, Tableau (Software), PowerBI (Software), Workday (Software), HRIS (Software), PowerPoint, Outlook, Excel, Business administration, Computer science, Human resources","data analysis, data extraction, data synthesis, data visualization, microsoft office suite, tableau software, powerbi software, workday software, hris software, powerpoint, outlook, excel, business administration, computer science, human resources","business administration, computer science, data extraction, data synthesis, dataanalytics, excel, hris software, human resources, microsoft office suite, outlook, powerbi software, powerpoint, tableau, visualization, workday software"
Sr. Data Base Developer,Synergy Equipment,"Tampa, FL",https://www.linkedin.com/jobs/view/sr-data-base-developer-at-synergy-equipment-3705772862,2023-12-17,Clearwater,United States,Mid senior,Onsite,"The Senior Database Developer on the Information Technologies team will work closely with business partners and Stakeholders, helping to establish requirements with an analytical mind and a problem-solving aptitude. You will communicate complex key insights in written and verbal form to advise business leaders regarding their reporting requirements. As a developer, you will design and map data models to shift raw data into meaningful insights. You will study, analyze, and understand business requirements in context to business intelligence. This position will create, test, and deploy Power BI scripts, as well as execute efficient deep analysis. The Database Developer will utilize Power BI to build interactive and visually appealing dashboards and reports. Using Warehouse data, you will generate reports to support business decision-making and business performance monitoring.
Essential Functions
Study, analyze and understand business requirements in context of business intelligence
Utilize Power BI to build interactive and visually appealing dashboards and reports
Use established report standards for Synergy Rents Power BI reports
Analyze previous and present data for better decision making
Develop tabular and multidimensional models that are compatible with data warehouse standards
Connect to data sources (e.g., SQL Server, Excel, Salesforce)
Translate data requirements into informative reports and visuals
Create/Modify DAX queries in Power BI desktop
Use M code for data transformation in Power BI desktop
Creating and maintaining data models, data flows and ETL processes to ensure accurate and efficient data extraction and transformation
Maintain data warehouse
Document reports and business logic, help maintain enterprise reporting documentation
Work with Data Warehouse architect to design procedures for report data
Manage and report on data from multiple data sources, providing reporting solutions to drive business decisions
Create, maintain, and improve performance of corporate Power BI models and reports
Ability to analyze and interpret business requirements and designs into Power BI Solutions
Applying data visualization best practices to create compelling and user-friendly reports that effectively convey insights and drive decision making
Ability to produce status reports and communicate status of projects on a regular basis
Documentation and knowledge sharing: Creating and maintaining documentation for data models, ETL processes and report development methodologies. Sharing knowledge and best practices with team members
Perform other duties as assigned.
Reasonable accommodations may be made to enable individuals with disabilities to perform these essential functions.
Required Education And Experience
Bachelor’s degree in computer science, Information Systems, or related field
4+ years of experience in Power Bi and SSRS report development
Strong Proficiency in SQL programming, database design and data modeling
Experience using stored procedures and views
Experience with BI Tools and systems such as PowerBI, Tableau, SSRS, Qlik
Knowledge in Microsoft BI Stack
Communicate effectively with stakeholders and business leaders
Self-motivated, driven, and able to deliver results on a proactive and independent basis
Ability to work under due dates, timelines, and numerous tickets
Understanding of development within desktop and mobile environments
Work well in team environment
Strong multitasking skills
Strong working knowledge of Data Analytics
Strong data documentation skills about algorithms, parameters, and models
Military service will be considered in lieu of education/certification experience as applicable .
Supervisory Responsibilities
Yes
Work Environmen
t
The environment is consistent with that of an office environment, using a computer, telephone, and other office equipment. Frequent interruptions throughout the workday
Physical Demands
While performing the duties of this job, the employee is regularly required to sit, use hands to finger, handle, or feel; reach with hands and arms; and talk or hear
The employee is occasionally required to stand and walk. The employee must frequently lift and/or move up to 10 pounds
Other Duties
Please be aware that while this job description provides an overview of the main activities, duties, and responsibilities expected of the employee in this role, it may not encompass every task or requirement. Additionally, the nature of the position necessitates the potential for changes in duties, responsibilities, and activities, which may occur with or without prior notice. The employer reserves the right to modify and adapt these aspects as necessary to meet evolving business needs and organizational objectives.
Additional Eligibility Requirements
At Synergy Equipment, we prioritize the safety and well-being of our employees. To ensure a secure work environment, we require all successful candidates to undergo a drug test and background check before their start date. This policy is in place to maintain the highest standards of safety and security for our team members.
What are the Benefits?
Synergy Equipment values its employees and their families. Because of this we offer many benefits to our employees. These include medical, dental, and vision insurance; health savings accounts; 401(k) and Roth retirement options with company match; Life and Disability Insurance; and Paid Time Off that includes vacation and paid holidays. This position is eligible for profit sharing opportunities.
At Synergy, we are committed to fostering professional growth and development. We provide comprehensive training programs to equip our employees with the skills they need to excel in their roles. Moreover, we prioritize offering career growth opportunities, allowing individuals to progress and advance within the company.
By offering these extensive benefits, fostering a supportive work environment, and investing in our employees' development, Synergy Equipment demonstrates its dedication to the overall well-being and long-term success of our employees.
Synergy is an equal opportunity employer, and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law.
Show more
Show less","Power BI, SQL, Data modeling, Data visualization, Data analysis, Business intelligence, ETL, DAX, M code, Data warehousing, SSRS, Tableau, Qlik, BI Stack, Microsoft BI Stack, Data Analytics, Algorithms, Parameters, Models","power bi, sql, data modeling, data visualization, data analysis, business intelligence, etl, dax, m code, data warehousing, ssrs, tableau, qlik, bi stack, microsoft bi stack, data analytics, algorithms, parameters, models","algorithms, bi stack, business intelligence, dataanalytics, datamodeling, datawarehouse, dax, etl, m code, microsoft bi stack, models, parameters, powerbi, qlik, sql, ssrs, tableau, visualization"
Sr. Data Engineer,Tential Solutions,"Tampa, FL",https://www.linkedin.com/jobs/view/sr-data-engineer-at-tential-solutions-3783160018,2023-12-17,Clearwater,United States,Mid senior,Onsite,"Job Purpose:
Provide an overview of the responsibilities of the job by providing a summary of the most important aspects and duties of the job (1-3 sentences). The Data & Analytics Team is seeking a passionate Senior Data Engineer to lead the technical design and development of the enterprise data eco-system. This person will be responsible for scoping new architecture, leading and guiding data engineers in development and implementation of robust and scalable enterprise data solutions to support current & future initiatives. This individual will not only work hands on with the data engineering team, but will also be responsible for the designing and implementation of solutions that proactively manage and monitor the data ecosystem of analytics, data lakes, warehouses, MDM platforms, and other tools.
Direct Reporting Relationships:
List other jobs in the company that report into this position.
Direct Reports? Yes ? No ?
This position will not have direct report reports under their supervisory organization. This is expected to grow as we build out the data and analytics practice.
Main Responsibilities and Accountabilities:
List the 4-8 essential functions of the job.
Review requirements and design data architectures based on the standards set by the manager which drive data strategy implementation and transform business strategies into information architectures.
Design, implement, and train the data engineering team members on strong and measurable processes, tools and engineered data management solutions to support all data platforms – such as data warehouse, lakes, visualization tools, master data management, and advanced analytics tools.
Developing, deploying, scheduling, and maintaining the data pipeline workflows to move data from multiple source systems to the cloud data warehouse.
Review business requirements and provide appropriate levels of effort to leadership that provide guidance on capacity planning and concise tasks to the team on how to implement appropriate schema and data models.
Creating the data ingestion framework for Snowflake for both full and incremental data loading from different source systems.
Further develop current cloud data warehouse (Snowflake) with both structured and un-structured data.
Developing dbt models and macros using SQL and Jinja languages with best practices to extract, load, and transform the data for medium to large enterprises using Snowflake.
Tune ETL for maximum performance.
Data Profiling, Data Cleansing, and Data Auditing.
Loading large volumes of data.
Decoding and writing complex SQL queries.
Performance tuning of queries and data loading process.
Modeling normalized and de-normalized data structures.
Ensure best practices are established and built into SOPS.
Minimum Qualifications and Experience Requirements:
Provide hiring requirements necessary to successfully perform this job; indicate required and preferred levels for each category below.
Education
A bachelor’s or master’s degree in business administration, computer science, data science, information science or related field, or equivalent work experience.
Experience
Programming (SQL) – 8 or more years
Data Management Concepts (Data Extraction, Cleansing, Transformation, loading, Validation, Migration, Modelling, RDBMS concepts) – 8 or more years
Snowflake – 4 or more years
ETL (dbt, SSIS) – 4 or more years
Azure Architecture & Services (Blob Storage, Data Factory, Data Lake, DevOps) – 4 or more years
Git Hub – 4 or more years
Data Visualization Tools (Power BI, Tableau) – 4 or more years
Certifications/Designations
Manufacturing industry experience, a plus
Experience pulling and merging data from multi-ERPs/source systems, required
SSIS Experience, required
Extensive design and development experience in Data warehouse and data marts
Snowflake SnowPro Core Certified required
DBT experience required
GIT Hub experience required
Azure Data Factory Experience required
Microsoft Azure Data Fundamentals Certified
Experience making data usable to the end users
Experience working in a complex data environment.
Agile Delivery Experience, a plus
Show more
Show less","Data Engineering, Data Architecture, Data Management, Data Warehousing, Data Lake, Master Data Management, Advanced Analytics, Data Pipelines, Data Profiling, Data Cleansing, Data Auditing, Data Loading, SQL, Jinja, Snowflake, dbt, ETL, SSIS, Azure Architecture & Services, Git Hub, Data Visualization Tools, Power BI, Tableau, Manufacturing Industry Experience, MultiERPs/Source Systems, Agile Delivery Experience","data engineering, data architecture, data management, data warehousing, data lake, master data management, advanced analytics, data pipelines, data profiling, data cleansing, data auditing, data loading, sql, jinja, snowflake, dbt, etl, ssis, azure architecture services, git hub, data visualization tools, power bi, tableau, manufacturing industry experience, multierpssource systems, agile delivery experience","advanced analytics, agile delivery experience, azure architecture services, data architecture, data auditing, data engineering, data lake, data loading, data management, data profiling, data visualization tools, datacleaning, datapipeline, datawarehouse, dbt, etl, git hub, jinja, manufacturing industry experience, master data management, multierpssource systems, powerbi, snowflake, sql, ssis, tableau"
Database Engineer (TS/SCI Required),"Paragon Cyber Solutions (8a, EDWOSB, VOSB, CMMC-C3PAO)","Tampa, FL",https://www.linkedin.com/jobs/view/database-engineer-ts-sci-required-at-paragon-cyber-solutions-8a-edwosb-vosb-cmmc-c3pao-3590311292,2023-12-17,Clearwater,United States,Mid senior,Onsite,"Paragon Cyber Solutions has an immediate opening for a
Database Engineer
.
Clearance
Active TS/SCI Security Clearance Required
Essential Duties and Responsibilities
The Database Engineer will provide onsite support at HQ USSOCOM to maintain and sustain all instances of the DefenseReady Enterprise Server Application and NIPR to SIPR one way guard. The database engineer will be responsible for the enterprise server maintenance and database administration support.
Responsible for developing and translating computer algorithms into prototype code and maintaining, organizing, and identifying trends in large data sets.
Design SQL databases.
Create process documentation.
Code in Python, Java, Kafka, Hive, R, or Storm.
Oversee real-time business metric aggregation, data warehousing and querying, schema and data management, and related duties.
Design, develop, and test state-of-the-art Cloud-based database platforms. Implementation and maintain complex databases.
Analyze procedures to control the access and allocation of data.
Maintain security controls in supported databases
Requirements
Experience as a Database Administrator and/or Database engineer.
Experience with design, development, and implementation of structured and unstructured databases. Experience with DDL scripts to create, maintain, and upgrade database schemas.
Experience with SQL and PL/SQL to perform complex queries, data manipulations and data migrations.
Experience writing and maintaining shell scripts; additional experience with R, Python, Java is helpful.
Recommended Experience Database design and implementation using cloud-based technologies (AWS, GCP, Azure) and respective database services.
DoD 8570 Information Assurance Technical (IAT) II Certification including Security+.
Note: Some travel to TSOCs and Components may be required to ensure task execution and reporting.
Benefits
Health Care Plan (Medical, Dental & Vision)
Retirement Plan (401K w/ employer matching)
Paid Time Off & 10 Paid Public Holidays
Short and Long-Term Disability
Healthy Work-Life Balance
Training & Development
Why Work For Paragon Cyber Solutions?
You want to make a difference. You want to be recognized by name versus being a number. You want to work with a company where you can grow and work in different areas to increase your knowledge/experience. You have an entrepreneurial spirit and need to live it.
As a small business, each team member rolls up their sleeves and helps where needed (including our CEO). Our ideal candidate will be comfortable with multi-tasking and prefers working in a fast-paced, dynamic environment. You must be adaptable to the needs of a growing business.
A Model of Excellence
That’s our motto in all we do and what we seek in our team members.
Are you a hardworking professional seeking a new opportunity that fosters growth? Look no further!
We are a minority, woman, veteran-owned, 8(a), EDWOSB, VOSB, HUBZone certified Tampa, FL-based small business. We pride ourselves on delivering high-quality solutions that help our clients protect the integrity of their business operations.
Show more
Show less","Database Administration, Database Engineering, SQL, PL/SQL, Python, Java, Kafka, Hive, R, Storm, Cloudbased Technologies, AWS, GCP, Azure, DoD 8570 Information Assurance Technical (IAT) II Certification, Security+","database administration, database engineering, sql, plsql, python, java, kafka, hive, r, storm, cloudbased technologies, aws, gcp, azure, dod 8570 information assurance technical iat ii certification, security","aws, azure, cloudbased technologies, database administration, database engineering, dod 8570 information assurance technical iat ii certification, gcp, hive, java, kafka, plsql, python, r, security, sql, storm"
Senior Data Engineer,"Staffing Resource Group, Inc","Tampa, FL",https://www.linkedin.com/jobs/view/senior-data-engineer-at-staffing-resource-group-inc-3765371353,2023-12-17,Clearwater,United States,Mid senior,Onsite,"Job Description
Senior Data Engineer
Salary:
$130k - $150k/yr.
Location:
Tampa, FL (Onsite)
Industry:
Government/Defense
Employment Type:
Permanent Placement
Clearance:
Active TS/SCI
Qualifications:
Bachelor s degree
10 years of experience supporting DoD at CCMD or similar level
5 years of experience in application development
5 years of experience designing, developing, operationalizing, and maintaining complex data applications at enterprise scale
3 years of experience creating software for retrieving, parsing, and processing structured and unstructured data
3 years of experience building scalable ETL/ELT workflows for reporting and analytics
Experience with Python, SQL, Scala, or Java
Experience creating solutions within a collaborative, cross-functional team environment
Ability to develop scripts and programs for converting various types of data into usable formats and support project team to scale, monitor and operate data platforms
Active TS/SCI clearance
Preferred Qualifications:
Experience with UNIX/Linux, including basic commands and Shell scripting
Experience with a public cloud, including AWS, Microsoft Azure, or Google Cloud
Experience with distributed data and computing tools including Spark, Databricks, Hadoop, Hive, AWS EMR, or Kafka
Responsibilities:
Assist clients to find answers in their big data to impact important missions from fraud detection to cancer research to national intelligence.
Implement data engineering activities on some of the most mission-driven projects in the industry.
Deploy and develop pipelines and platforms that organize and make disparate data meaningful.
Work with and guide a multi-disciplinary team of analysts, data engineers, developers, and data consumers in a fast-paced, agile environment.
Utilize your experience in analytical exploration and data examination while you manage the assessment, design, building, and maintenance of scalable platforms for your clients.
EOE/ADA
#clearance
Show more
Show less","Python, SQL, Scala, Java, UNIX/Linux, Shell scripting, AWS, Microsoft Azure, Google Cloud, Spark, Databricks, Hadoop, Hive, AWS EMR, Kafka","python, sql, scala, java, unixlinux, shell scripting, aws, microsoft azure, google cloud, spark, databricks, hadoop, hive, aws emr, kafka","aws, aws emr, databricks, google cloud, hadoop, hive, java, kafka, microsoft azure, python, scala, shell scripting, spark, sql, unixlinux"
Data Analyst (USSOCOM),SimIS Inc.,"Tampa, FL",https://www.linkedin.com/jobs/view/data-analyst-ussocom-at-simis-inc-3787738902,2023-12-17,Clearwater,United States,Mid senior,Onsite,"Data Analyst for USSOCOM IDST
LOCATIONS: Hawaii, Stuttgart Germany, and Tampa FL.
Position Description
. Support the USSOCOM J2 Intelligence Data Science Team (IDST)’s ongoing data analytic programs using technology to mine complex, voluminous, and different varieties of data from various sources and platforms, in order to, collect, analyze, and compile data to meet customer needs; collaborate with the Digital and Artificial Intelligence personnel, Knowledge Management teams, and other activities supporting data analysis for the intelligence community to include planning events, customer requirement discussions, and client intelligence capability development.
Knowledge.
Expert knowledge using Python.
Working knowledge using AGILE software development methodology.
Preferred working knowledge of:
The intelligence cycle, Federal intelligence missions, and use of data to support intelligence priorities.
Data integration processes and supporting documentation requirements.
Special Operations Force (SOF) community intelligence process and analysis tradecraft to compile, collate, analyze, produce, and evaluate all-source intelligence and support the Ops-Intel fusion process.
The various tools that support SOF intelligence analysis, data sources relevant to the needs of the analyst, and the tradecraft associated with SOF intelligence analysis.
Skills.
Analyzing local, national, and global trends to identify new sources of data and methods to improve data collection, analysis, and reporting.
Determining technical issues and removing corrupted data & fixing coding errors and related issues.
Working collaboratively with Intelligence and Data analysis teams to produce qualitative and quantitative data analysis that supports Intelligence products and processes.
Building data solutions, tools, and capabilities to enable self-service frameworks and/or automation for data consumers to monitor, extract, and report on data.
Working with client leadership to identify process improvement opportunities, propose system modifications, and devise data management strategies for the enterprise to improve the quality and usability of data.
Using statistical tools to identify, analyze, and interpret patterns and trends in complex data sets
Preferred - using JavaScript, R, C/C++, SQL, Go, Flask, React, SQLAlchemy, CI/CD, Gitlab, and JIRA.
Providing written and verbal products and supporting business process documentation.
Working independently and as part of many teams.
Applying excellent time management and work prioritization to complex and integrated program requirements.
Executing complex projects within government defined timelines across a geographically dispersed workforce and user base.
Excellent communication, organizational, and problem-solving skills.
Experience.
Required:
Documented experience providing data analyst services similar in required tasks, scope, and complexity.
Preferred:
Using SOFNET-U, SOFNET-S, JIANT, SOIS, SOCRATES, NSANet and Commercial internet
USSOCOM military or civil service experience.
Currently residing in the Tamps, FL area.
Education.
Bachelor’s degree in a computer science discipline, or equivalent. AND 2 years of experience.
Certifications.
Preferred: Current certification for completion of the USSOCOM training and deployment requirements at the CONUS Replacement Center (CRC) as mandated by Principal Assistant for contracting (PARC) Policy Alert 12-01 dated 24 October 2011, SUBJECT: Contractor Deployment and Redeployment Requirements in Support of the U.S. Central Command Area of Responsibility.
Security.
Required:
Current DoD Top Secret clearance and eligible for SCI access and ACCM read-on and associated duty activities for access to SCI, FGI, and NATO material.
Travel.
Some travel (estimate less than 10%) both within and outside the Continental United States. Current active US passport required.
Place of Performance.
On-site at USSOCOM Headquarters, McDIll AFB, Florida; office space with utilities and Government-provided office equipment.
Full-Time employment
. 11 Federal Holidays; core work hours are 0900-1500 hrs with military team is Monday-Friday; contractors can report as early as 0630 hrs and latest to depart time is 1730 hrs and adjusted as necessary to meet installation network support requirements.
Period of Performance.
1 year Base period (starting Aug 2023); option periods under consideration.
SimIS Offers:
Flexible Spending Account (FSA)
Medical, Dental, and Vision
Short Term Disability (SimIS provides Short-Term Disability benefits at no cost to you)
LTD
Life Insurance
401(k) Savings Plan
Tuition Assistance Program
Paid Time Off (PTO)
10 Holidays each year
SimIS, Inc. is an AA / EOE / M / F / Disability / Vet / V3 certified / Drug Free Employer
Powered by JazzHR
Q59mfg6S0o
Show more
Show less","Python, AGILE software development, Intelligence cycle, Data integration, SOF community intelligence process, SOF intelligence analysis, JavaScript, R, C/C++, SQL, Go, Flask, React, SQLAlchemy, CI/CD, Gitlab, JIRA, SIFNETU, SIFNETS, JIANT, SOIS, SOCRATES, NSANet, USSOCOM military, DoD Top Secret clearance, SCI access, ACCM","python, agile software development, intelligence cycle, data integration, sof community intelligence process, sof intelligence analysis, javascript, r, cc, sql, go, flask, react, sqlalchemy, cicd, gitlab, jira, sifnetu, sifnets, jiant, sois, socrates, nsanet, ussocom military, dod top secret clearance, sci access, accm","accm, agile software development, cc, cicd, data integration, dod top secret clearance, flask, gitlab, go, intelligence cycle, javascript, jiant, jira, nsanet, python, r, react, sci access, sifnets, sifnetu, socrates, sof community intelligence process, sof intelligence analysis, sois, sql, sqlalchemy, ussocom military"
Principal Data Engineer,Reperio Human Capital,"Tampa, FL",https://www.linkedin.com/jobs/view/principal-data-engineer-at-reperio-human-capital-3742776198,2023-12-17,Clearwater,United States,Mid senior,Onsite,"Apply Now
Salary
$70-85
Location
Tampa, Florida, USA
Type
Contract
Start
ASAP
Principal Data Engineer
87799
Desired skills:
Data Engineer, AWS, GCP, Azure, Apache, SnowFlake
Are you a passionate and experienced Cloud Data Engineer looking for an exciting opportunity to shape the future of data-driven decision making? Join my clients dynamic team and play a pivotal role in designing, implementing, and optimizing their cutting-edge data solutions.
My client is a leading software company dedicated to leveraging data to drive innovation and enhance their services. They are at the forefront of technology adoption, and they are seeking an AWS Big Data Engineer to help them revolutionize the way they manage, process, and analyze vast amounts of data.
Key Responsibilities:
Design, implement, and maintain scalable data pipelines and ETL processes using AWS services such as Glue, EMR, Lambda, and Step Functions.
Collaborate with cross-functional teams to understand data requirements and translate them into efficient data processing solutions.
Optimize data storage, processing, and retrieval techniques to ensure high performance and cost-effectiveness.
Develop and manage data models, data sets, and data catalogs, ensuring data accuracy and integrity.
Monitor data processing activities, troubleshoot issues, and ensure data quality throughout the pipeline.
Stay up-to-date with the latest AWS services and data engineering best practices to continuously enhance our data infrastructure.
Qualifications:
Bachelor's or Master's degree in Computer Science, Data Science, or related field.
Proven experience as a Big Data Engineer with a strong focus on AWS services and technologies.
Proficiency in designing and implementing data pipelines using AWS tools such as Glue, EMR, S3, Lambda, and Redshift.
Strong programming skills in languages such as Python, Java, or Scala for data processing and ETL tasks.
Solid understanding of data modeling, database design, and data warehousing concepts.
Experience with real-time data streaming technologies (Kinesis, Kafka) is a plus.
AWS certification(s) in relevant domains is highly desirable.
Excellent problem-solving skills and ability to work collaboratively in a team environment.
Strong communication skills to effectively convey technical concepts to both technical and non-technical stakeholders.
If you are interested in this role, apply below or contact Gareth for more information.
Reperio Human Capital acts as an Employment Agency and an Employment Business.
Apply Now
Gareth Irvine
is recruiting for this role.
Get in touch with Gareth Irvine for more information: +1 813 902 2716
View profile
Show more
Show less","Data Engineering, AWS, GCP, Azure, Apache, SnowFlake, Big Data Engineer, Glue, EMR, Lambda, Step Functions, ETL, Data Processing, Data Modeling, Data Sets, Data Catalogs, Data Accuracy, Data Quality, Data Warehousing, Kinesis, Kafka, Python, Java, Scala, NoSQL, Redshift, Communication","data engineering, aws, gcp, azure, apache, snowflake, big data engineer, glue, emr, lambda, step functions, etl, data processing, data modeling, data sets, data catalogs, data accuracy, data quality, data warehousing, kinesis, kafka, python, java, scala, nosql, redshift, communication","apache, aws, azure, big data engineer, communication, data accuracy, data catalogs, data engineering, data processing, data quality, data sets, datamodeling, datawarehouse, emr, etl, gcp, glue, java, kafka, kinesis, lambda, nosql, python, redshift, scala, snowflake, step functions"
Data Scientist,Women in Data®,"Reigate, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-scientist-at-women-in-data%C2%AE-3784557294,2023-12-17,Crawley, United Kingdom,Mid senior,Hybrid,"APPLY HERE:
womenindata.co.uk/job/data-scientist-2-reigate/
Working Hours: 35 hours per week
Salary: Competitive
Department: Data
Here at esure, we’re no strangers to change. As one of the industry leaders in the insurance business, striving to become a world class digital insurer, we’re getting ready for more. It’s creating great new opportunities for innovative and talented industry professionals to join us at a pivotal point in our development.
esure Group are currently recruiting for a Data Scientist. You will play a key part in improving the profitability of our company through new data sources and advanced modelling techniques. This role is an end-to-end data science role spanning the entire AI lifecycle from initial concept, through to production deployment of models.
At esure Group we are continuing to build an outstanding environment for long-term ambitious research to thrive in. We are harnessing data science approaches to build end-to-end solutions which will help our business drive further value from data and build personalised experiences for our customers. We apply innovative machine learning techniques to solve a variety of problems such as customer risk, lifetime value and retail pricing across diverse product portfolios. The team is central to our growth strategy with great support from senior colleagues who are keen to drive a ‘start-up mentality’ for this function.
What you'll do:
Apply statistical and sophisticated analytical techniques to optimise and improve business performance for specific business problems and use cases
Build and maintain the algorithms required to drive value for customers. Research and development to continuously improve our services and products
Provide regular support on data science projects – both large and small – that drives business benefit and gives us a competitive advantage
Actively seek out new sources of data, making esure best in class in utilising the most expansive data sets available
Work closely with DevOps and Data Engineers to productionise and deploy machine learning models
Produce high-quality code that allows us to put solutions into production
Refactor code into reusable libraries, APIs, and tools
What are we looking for?:
Experience working as a Data Scientist with a passion for technology
R/Python toolkits, preferably working with data pipelines
Proficient with SQL and NoSQL databases
Experience with Jupyter Notebooks and version control (eg. Git)
Expert in mining large and sophisticated data sets - both structured and unstructured extraction of data, transformation and application
Desirable:
Experience with Spark/Databricks
Experience deploying ML at scale via APIs (eg. Flask, Keras) advantageous
Why choose us?
This is your opportunity to be at the forefront of our game-changing journey and be part of something truly special! And to top it off, here are some perks to life at esure…
A competitive salary that recognises your skills and potential
A bonus scheme that celebrates your contribution to esure’s success
Discounts on our insurance products, for you and your family
25 days annual leave, plus 8 flex days to be taken as and when suits you
Benefits just for you: our hub – My Benefits Box – is loaded with perks to choose from, so you can build a personal toolkit to support your health, wellbeing, lifestyle, and finances.
Grow your career with us: whatever your goals, we’ll support you with hands-on training, mentoring, a LinkedIn Learning licence, access to our exclusive Academies, regular career conversations, and expert partner resources from the likes of Women in Data and Women in Tech.
Join our communities: our networks give you the chance to connect, learn and share with like-minded colleagues across the business – for work and play. So, it’s no surprise our people consistently rate ‘making friends at work’ one of the highest scorers in our colleague engagement survey
More flexibility for you: we’re a proud supporter of the ABI’s Make Flexible Work campaign and welcome you to ask about the flexibility you need, whether it’s part time, job sharing, or compressed hours. Our hybrid working approach also puts you in the driving seat of how and where you do your best work.
Live a healthy lifestyle: we offer lots of support, so you feel like the best version of yourself – like specialist advice through our employee assistance programme, wellbeing classes, access to the My Health Advantage app, our Big Team Challenge, and learning sessions on topics like menopause.
A helping hand to do your bit for a greener and safer world: driving good in the world couldn’t be more important to us. That’s why we encourage colleagues to use volunteering days to support their local communities and have lots of initiatives to help you live a greener lifestyle.
Everyday appreciation: praise from your colleagues means the world! Our social recognition tool makes it easy to give colleagues the praise they deserve, and you could even be shortlisted for a company-wide ACE Award.
We understand some people may not apply for jobs unless they tick every box. If you are excited about joining us and think you have some of what we are looking for, even if you’re not 100% sure we would love to hear from you.
We are proud supporters of Women in Data®. Connect, engage and belong to the largest free female data community in the UK – visit: www.womenindata.co.uk to join our community.
Show more
Show less","Data Science, Machine Learning, SQL, NoSQL, Git, Jupyter Notebooks, R, Python, Spark, Databricks, Flask, Keras, APIs","data science, machine learning, sql, nosql, git, jupyter notebooks, r, python, spark, databricks, flask, keras, apis","apis, data science, databricks, flask, git, jupyter notebooks, keras, machine learning, nosql, python, r, spark, sql"
Data Analyst,Harnham,"Brighton, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-analyst-at-harnham-3776909007,2023-12-17,Crawley, United Kingdom,Mid senior,Hybrid,"DATA ANALYST
BRIGHTON - HYBRID (2 DAYS IN OFFICE)
£45,000 - £55,000
Please note that this role can't sponsor employees now or in the future.
THE COMPANY
Join a dynamic Second-hand Marketplace and E-commerce platform specializing in used cameras and photography equipment. Revolutionizing the industry, we provide a streamlined platform for buying and selling, creating a seamless experience for photography enthusiasts.
THE ROLE AND RESPONSIBILITIES
As a Data Analyst, you will play a critical role in:
Collaborating closely with the finance and ops teams to enhance operational efficiency.
Be an instrumental participant in advancing the ongoing development of the analytics function and strategy, with a dedicated emphasis on implementing best practices.
Identifying trends and blockers in raw data within the warehouse and implementing corrective measures.
You will use: AWS, GCP, Snowflake, and Tableau.
YOUR EXPERIENCE
To qualify for this role, you will require:
Proven experience in data analysis, with a focus on identifying trends and operational blockers.
Familiarity with tech stack: AWS, GCP, Snowflake, and Tableau.
Ability to work closely with cross-functional teams, linking data insights to financial and operational strategies.
HOW TO APPLY
Please apply to this role via the link to this page.
Show more
Show less","Data Analysis, AWS, GCP, Snowflake, Tableau","data analysis, aws, gcp, snowflake, tableau","aws, dataanalytics, gcp, snowflake, tableau"
Sr Data Engineer,Nike,"Beaverton, OR",https://www.linkedin.com/jobs/view/sr-data-engineer-at-nike-3758750275,2023-12-17,Red Deer, Canada,Mid senior,Hybrid,"Work options:
Hybrid
Title:
Sr Data Engineer
Location:
Beaverton, OR
Duration:
7 Month Contract
Become a Part of the Nike Team
Nike does more than outfit the world’s best athletes. We are a place to explore potential, obliterate boundaries, and push out the edges of what can be. We’re looking for people who can grow, think, dream and create. We thrive in a culture that embraces diversity and rewards imagination. We seek achievers, leaders and visionaries. At Nike, it’s about bringing what you have to a challenging and constantly evolving game.
In this role you will
The Senior Data Engineer is a professional responsible for designing, developing, and maintaining the infrastructure and systems that enable the collection, storage, processing, and analysis of data within an organization.
Data engineers play a crucial role in ensuring that data is available & accurate, accessible, and properly structured for use by data analysts, data scientists, visualization tools and other stakeholders.
Skills
PySprak
SQL
Airflow
Show more
Show less","Python, PySpark, SQL, Airflow, Data Engineering, Data Infrastructure, Data Collection, Data Storage, Data Processing, Data Analysis, Data Visualization, Data Warehousing, Data Quality, Data Governance","python, pyspark, sql, airflow, data engineering, data infrastructure, data collection, data storage, data processing, data analysis, data visualization, data warehousing, data quality, data governance","airflow, data collection, data engineering, data governance, data infrastructure, data processing, data quality, data storage, dataanalytics, datawarehouse, python, spark, sql, visualization"
Data Engineer (DE),Zortech Solutions,"Pittsburgh, PA",https://www.linkedin.com/jobs/view/data-engineer-de-at-zortech-solutions-3732623571,2023-12-17,Red Deer, Canada,Mid senior,Hybrid,"Role: Data Engineer(DE)
Location: NY, Lake Mary, Pittsburgh (Hybrid)
Duration: Fulltime
Job Description
In person Client interview
Primary skills : Python, Spark, Distributed computing
Secondary Skills : Machine Learning, Data Science, Cloud computing
Experience (min) : 7 years +
Show more
Show less","Python, Spark, Distributed computing, Machine Learning, Data Science, Cloud computing","python, spark, distributed computing, machine learning, data science, cloud computing","cloud computing, data science, distributed computing, machine learning, python, spark"
SSIS Data Engineer,Steneral Consulting,"Appleton, WI",https://www.linkedin.com/jobs/view/ssis-data-engineer-at-steneral-consulting-3745150072,2023-12-17,Red Deer, Canada,Mid senior,Hybrid,"Onsite once every 2 weeks, must be local to WI/MI/MN/IL who can travel or relocate to Appleton, WI
Need valid LinkedIn
This is a pure SSIS ETL dev/support person working with the business.
PERSONALITY IS MOST IMPORTANT - good team, very interactive and this person will be presenting to high levels of the business
Good-natured and personable team members. Looking for a data engineer who can build strong relationships with the business.
Need to know data warehousing, dims, and facts (SSIS)
Strong in SQL Queries, joins, triggers, etc.
Experience with SQL Server Agent to set up the jobs to run, maintain jobs, and review job execution and failures in SSIS jobs.
SSAS would be a plus, not a requirement
The batch process runs overnight, and sometimes needs on-call action, as well as day support.
Look into requests against warehouse tables that cause issues. Want to know lineage in a warehouse table to see the root error cause.
Show more
Show less","SSIS, SQL, SQL Server Agent, Data warehousing, Data lineage","ssis, sql, sql server agent, data warehousing, data lineage","data lineage, datawarehouse, sql, sql server agent, ssis"
Senior Data Engineer,Nike,"Beaverton, OR",https://www.linkedin.com/jobs/view/senior-data-engineer-at-nike-3693914347,2023-12-17,Red Deer, Canada,Mid senior,Hybrid,"Work options:
Hybrid
Hybrid
Title:
Business System Analyst (Tech)
Location:
Hybrid - Beaverton, OR
Duration
: 10 Month Contract
Become a Part of the NIKE, Inc. Team
NIKE, Inc. does more than outfit the world's best athletes. It is a place to explore potential, obliterate boundaries and push out the edges of what can be. The company looks for people who can grow, think, dream and create. Its culture thrives by embracing diversity and rewarding imagination. The brand seeks achievers, leaders and visionaries. At Nike, it’s about each person bringing skills and passion to a challenging and constantly evolving game.
Nike has embraced big data technologies to enable data-driven decisions. We’re looking to expand our Data Engineering team to keep pace. As a Data Engineer, you will work with a variety of talented Nike teammates and be a driving force for building first-class solutions for Nike Technology and its business partners, working on development projects related to supply chain, commerce, consumer behavior and web analytics among others.
Role Responsibilities
Contribute to Design and implement data products and features in collaboration with product owners, data analysts, and business partners using Agile / Scrum methodology
Contribute to overall architecture, frameworks, and patterns for processing and storing large data volumes
Contribute to the evaluation of new technologies/tools/frameworks centered around high-volume data processing
Translate product backlog items into logical units of work in engineering
Implement distributed data processing pipelines using tools and languages prevalent in the big data ecosystem
Build utilities, user-defined functions, libraries, and frameworks to better enable data flow patterns
Work with engineering leads and other teams to ensure quality solutions are implemented, and engineering best practices are defined and followed
Build and incorporate automated unit tests and participate in integration testing efforts
Utilize and advance continuous integration and deployment frameworks
Troubleshoot data issues and perform root cause analysis
Work across teams to resolve operational & performance issues
The following qualifications and technical skills will position you well for this role:
Bachelor’s degree in Computer Science, or related technical discipline
2+ years of experience in large-scale software development, 1+ years of big data experience
Programming experience, Python or Scala preferred.
Experience working with Hadoop and related processing frameworks such as Spark, Hive, etc.
Experience with messaging/streaming/complex event processing tooling and frameworks
Experience with data warehousing concepts, SQL and SQL Analytical functions
Experience with workflow orchestration tools like Apache Airflow
Experience with source code control tools like Github or Bitbucket
Ability to communicate effectively, both verbally and written, with team members
Interest in and ability to quickly pick up new languages, technologies, and frameworks
Experience in Agile/Scrum application development
The following skills and experience are also relevant to our overall environment, and nice to have:
Experience with Java
Experience working in a public cloud environment, particularly AWS
Experience with cloud warehouse tools like Snowflake
Experience working with NoSQL data stores such as HBase, DynamoDB, etc.
Experience building RESTful API’s to enable data consumption
Experience with build tools such as Terraform or CloudFormation and automation tools such as Jenkins or Circle CI
Experience with practices like Continuous Development, Continuous Integration and Automated Testing
These Are The Characteristics That We Strive For In Our Own Work. We Would Love To Hear From Candidates Who Embody The Same
Desire to work collaboratively with your teammates to come up with the best solution to a problem
Demonstrated experience and ability to deliver results on multiple projects in a fast-paced, agile environment
Excellent problem-solving and interpersonal communication skills
Strong desire to learn and share knowledge with others
NIKE, Inc. is a growth company that looks for team members to grow with it. Nike offers a generous total rewards package, casual work environment, a diverse and inclusive culture, and an electric atmosphere for professional development. No matter the
location, or the role, every Nike employee shares one galvanizing mission: To bring inspiration and innovation to every athlete* in the world. NIKE, Inc. is committed to employing a diverse workforce. Qualified applicants will receive consideration without regard to race, color, religion, sex, national origin, age, sexual orientation, gender identity, gender expression, veteran status, or disability.
Show more
Show less","Python, Scala, Hadoop, Spark, Hive, SQL, Apache Airflow, Github, Bitbucket, Java, AWS, Snowflake, HBase, DynamoDB, RESTful API, Terraform, CloudFormation, Jenkins, Circle CI, Continuous Development, Continuous Integration, Automated Testing","python, scala, hadoop, spark, hive, sql, apache airflow, github, bitbucket, java, aws, snowflake, hbase, dynamodb, restful api, terraform, cloudformation, jenkins, circle ci, continuous development, continuous integration, automated testing","apache airflow, automated testing, aws, bitbucket, circle ci, cloudformation, continuous development, continuous integration, dynamodb, github, hadoop, hbase, hive, java, jenkins, python, restful api, scala, snowflake, spark, sql, terraform"
Data Engineer,Steneral Consulting,"Chicago, IL",https://www.linkedin.com/jobs/view/data-engineer-at-steneral-consulting-3748441510,2023-12-17,Red Deer, Canada,Mid senior,Hybrid,"Title- Data Engineer
Location- Chicago, IL – 1 day per week onsite – hybrid- local only
Linkedin must.
JD-
Overview/Summary
The Product Analytics team at United Airlines is on a transformational journey to unlock the full potential of enterprise data, build a dynamic, diverse, and inclusive culture and develop a modern cloud-based data lake architecture to scale our applications, and drive growth using data and machine learning. Our objective is to enable the enterprise to unleash the potential of data through innovation and agile thinking, and to execute on an effective data strategy to transform business processes, rapidly accelerate time to market and enable insightful decision making.
Job Overview And Responsibilities
In this role you will partner with various teams to define and execute data acquisition, storage, transformation, processing and make data actionable for operational and analytics initiatives that create sustainable revenue and share growth. This role requires expertise in Uniteds data sources and technology business intuition, and a working knowledge of data transformation and analytical tools.
Manage data integration and technology projects across the various platforms that comprise Uniteds data strategy
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, storage, transformation, and loading (ETL) of data from a wide variety of data sources
Work with stakeholders including the Senior management, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Work with data and analytics experts to strive for greater functionality in our data systems.
Build and maintain databases, data pipelines, and data warehouses that provide a comprehensive understanding of the business.
Execute unit tests and validating expected results to ensure accuracy & integrity of data and applications through analysis, coding, writing clear documentation and problem resolution.
Leverage technical and analytical skills to understand and solve business centric questions
Coordinate and guide cross-functional projects that involve team members across all areas of the enterprise, vendors, external agencies, and partners
Ability to manage multiple deliverables both short and long-term in a busy and aggressive environment, while staying flexible to dynamic needs and priority levels
Manage agile development and delivery by collaborating with project manager, product owner and development leads
Required
Bachelor's degree in quantitative field (statistics, software engineering, business analytics, information systems, aviation management or related degree)
5+ years of experience in data engineering or ETL development role
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with structure, semi- structure, and unstructured datasets.
Experience with Teradata, SQL server, etc.
Experience with AWS cloud services: Redshift, S3, Athena, etc.
Experience with Databricks and lakehouse architecture
Passionate about solving problems through data and analytics, and creating data products including data models
Strong initiative to take ownership of data-focused projects, get involved in the details of validation and testing, as well as provide a business user perspective to their work
Ability to communicate complex quantitative concepts in a clear, precise, and actionable manner
Proven proficiency with Microsoft Excel and PowerPoint
Strong problem-solving skills, using data to tackle problems
Outstanding writing, communication, and presentation skills
Preferred
Master's degree
Experience with Quantum Metrics and Akamai
Experience with languages: Python, R, etc.
Strong experience with continuous integration & delivery using Agile methodologies
Data engineering experience with transportation/airline industry
Strong problem-solving skills
Show more
Show less","Data Engineering, SQL, Teradata, AWS, Cloud Services, Redshift, S3, Athena, Databricks, Lakehouse Architecture, Python, R, Agile Methodologies, Continuous Integration & Delivery, Transportation/Airline Industry, Quantum Metrics, Akamai, Microsoft Powerpoint, Microsoft Excel","data engineering, sql, teradata, aws, cloud services, redshift, s3, athena, databricks, lakehouse architecture, python, r, agile methodologies, continuous integration delivery, transportationairline industry, quantum metrics, akamai, microsoft powerpoint, microsoft excel","agile methodologies, akamai, athena, aws, cloud services, continuous integration delivery, data engineering, databricks, lakehouse architecture, microsoft excel, microsoft powerpoint, python, quantum metrics, r, redshift, s3, sql, teradata, transportationairline industry"
"Data Engineer, AWS, Python",Steneral Consulting,"New York, NY",https://www.linkedin.com/jobs/view/data-engineer-aws-python-at-steneral-consulting-3783585888,2023-12-17,Red Deer, Canada,Mid senior,Hybrid,"Hybrid role in NYC (2 days onsite each week), must be local
Need Valid LinkedIn
Need GC Copy (Front + Back) and Photo ID for USC ( no info must be hidden/blacked out as they wont be considered)
Need 1 or 2 strong candidates on this
Candidates need to take a video screen with prime vendor prior to end client submittal
Required
AWS, Python expert
Data pipeline creation expert
Data Warehousing, DW concepts. Star Schema, Snowflake schema, model it or build it. Not ONLY was a user of it.
ETL Background
AWS Glue required
Cloud Integration required
Requirements Include
Strong understanding of ETL processes and data warehousing concepts
Familiarity with AWS Services:
AWS Glue
AWS S3
AWS RDB
AWS Lambdas
Programming:
Proficient in SQL
Proficiency in Python required
Experience in processing leveraging Spark is a plus
Data Warehousing: hands-on experience implementing data warehousing best practices is a must
Version Control: Strong experience with Git for version control
Experience with CI/CD in AWS highly recommended
Strong problem-solving and analytical skills
ETL Tools: Experience with AWS Glue, AWS Lambdas, Python, Spark
Excellent communication and collaboration skills
Ability to work in a fast-paced environment and handle multiple tasks simultaneously
Attention to detail and commitment to data quality and accuracy
Tech Stack
Programming languages: SQL, Python
ETL tools: AWS Glue / AWS Lambdas
Data storage: AWS S3 (data lake) / AWS Aurora Postgres DB (data warehouse)
Version control: Git
Hands on experience with CI/CD required
Show more
Show less","AWS, Python, Data pipeline creation, Data warehousing, DW concepts, Star schema, Snowflake schema, ETL, AWS Glue, Cloud integration, SQL, Spark, Data warehousing best practices, Git, CI/CD, AWS S3, AWS RDB, AWS Lambdas","aws, python, data pipeline creation, data warehousing, dw concepts, star schema, snowflake schema, etl, aws glue, cloud integration, sql, spark, data warehousing best practices, git, cicd, aws s3, aws rdb, aws lambdas","aws, aws glue, aws lambdas, aws rdb, aws s3, cicd, cloud integration, data pipeline creation, data warehousing best practices, datawarehouse, dw concepts, etl, git, python, snowflake schema, spark, sql, star schema"
Data Engineer [NYC],CarbonChain,"New York, United States",https://www.linkedin.com/jobs/view/data-engineer-nyc-at-carbonchain-3733044947,2023-12-17,Red Deer, Canada,Mid senior,Hybrid,"Do you want to work on the most pressing problem of our generation?
We’re building the infrastructure for the net zero transition, and we’re looking for brilliant builders who want to help define a low carbon future.
Decarbonizing the economy requires a granular, real-time view of where emissions come from and how they might be reduced. We build software to automate the carbon footprinting of supply chains. Banks, traders, and manufacturers use our product to tame the complexity of international supply networks, identify the most carbon-intensive parts, and find greener alternatives. Having developed technology which is significant in advance of competitive solutions, we are now investing heavily in market adoption.
You can find out more about interviewing at CarbonChain at https://www.carbonchain.com/careers/interview-process .
About Us
Do you want to work on the most pressing problem of our generation?
We’re building the infrastructure for the net zero transition, and we’re looking for brilliant engineers, designers, and data scientists who want to help define a low carbon future.
Decarbonizing the economy requires a granular, real-time view of where emissions come from and how they might be reduced. We build software to automate the carbon footprinting of supply chains. Banks, traders, and manufacturers use our product to tame the complexity of international supply networks, identify the most carbon-intensive parts, and find greener alternatives.
To join CarbonChain, you’ll be a keen technologist who loves to learn from others. Our company is made up of 10 passionate people with expertise ranging from oil refining to deep learning. Between us we’ve run Amazon’s European supply chain, built JustEat’s corporate meal delivery platform, and monitored industrial emissions with satellites for Al Gore. We’ve got MBAs and PhDs but we know that there’s a lot we don’t know, and we’re hoping you can help fill that gap.
Working at CarbonChain
We encourage every member of our team to be the best they can be, whether that’s developing skills through their learning budget or participating in reading groups. Our work requires everybody to straddle the physical and digital worlds, and a growth mindset is essential. One week you might become an expert in the production of copper and figure out how to represent copper electrorefining in Java, whilst the next might be spent modelling the emissions of container ships and visualizing how they compare to LNG tankers in D3.js.
Our software has to be clever to represent the insane complexity of real-world supply chains, but clever software is worthless if nobody wants to use it. We’re focused on building intuitive software which minimizes cognitive load. If people love our tools, they’ll use them to make the changes the planet needs.
What will you be responsible for at Carbon Chain?
We are at the start of our journey and want to build the right things, the right way. We are cloud-first, allowing us to scale at will and experiment with adding new services quickly. As a Senior Data Engineer you’ll play a key role within the company, taking responsibility for our core data infrastructure and its evolution to support the company’s growth. You will lead the design, implementation, and maintenance of a performant and cost efficient data pipelines, as well as contribute to the overall CarbonChain data architecture, with the goal of expanding our data capabilities - feature development, analytics, and ML. You will work closely with cross-functional teams to ensure the delivery of high-quality data sets and features, that empower our key business and product decision makings, and enable CarbonChain’s growth and customer success.
You can expect to have:
Ownership of your projects
An independent path to production
The ability to make real changes with tangible business value
The core of the system relies on a Python application, but we aren’t shy to add other tools that make us ship valuable features better and faster. Other tools comprise Java (Spring Boot), Angular/Typescript, PostgreSQL, Redis, Neo4j, Google Identity, but our cloud-based infrastructure enables us to explore new technologies parallel with our existing platform. We’d love to bring on engineers who come from diverse backgrounds to help us explore new angles of solving business challenges and deliver solutions that wow our customers.
Which tools, technologies, and processes will you work with?
Containerised applications are the key to our technology vision allowing us to replicate production environments locally and scale services at will.
Object-oriented code forms the bulk of our codebase.
PostgreSQL, Neo4j and DynamoDB form the persistence layer - you’ll learn to navigate relational, graph, and document databases and appreciate their respective values.
Infrastructure automation is owned by the whole team, helping to spread the DevOps mindset across the whole tech department (and beyond!).
Testing approaches from unit testing to integration testing are key to delivering reliable and maintainable solutions.
Quality assurance is owned by the team themselves - we believe that the ownership of the full development cycle should sit tightly within the team.
Agile development, using various approaches from Scrum to Elephant Carpaccio , helps us deliver software in small iterations, learning and course-correcting in the process to make sure that what we deliver has an impact.
GitHub PRs are an integral part of our core development flow - with reviews to improve quality and share knowledge.
Continuous delivery is the approach we strive for - with metrics that help us optimise parts of the pipeline that need it the most.
Teamwork and collaboration are fundamental to the delivery of our solutions. We encourage solutionizing between tech and the wider business.
What we require from applicants
Right to work in the UK and willingness to come to London office 2+ days a week
A passion for environmental issues
Minimum of 7 years of experience in data engineering or relevant backend development, with a proven track record of building highly scalable, performant, and reliable data pipelines
Extensive experience with data modelling for large distributed data warehouses and/or lakehouses (or a similar cloud based) solutions, you’re able to discuss in-depth general principles and trade-offs of different modelling approaches
Experience in working with “modern data stack” standard data frameworks and adjacent tools – such as Fivetran, Airbyte, BigQuery, Databricks, Snowflake, Apache Spark, Kafka, Airflow, Kubernetes, dbt, etc.
Excellent collaboration and communication skills, with a demonstrated ability to work effectively with cross-functional partners, especially with teams outside of the product and engineering group
Working proficiency in one of the programming languages (Python, Go, Scala etc.)
Product mindset to embrace business needs and produce scalable data/engineering solutions
Experience leading technical design discussions and providing guidance on best practices, coding standards, and architecture principles
The grit and energy to work in an early stage startup
What we’re offering
Competitive salary + generous equity package
Flexible working hours - we encourage regular breaks and being AFK (away from keyboard) to support your wellbeing
Flexible working location (we like to meet in the office couple of times every week)
£1000 annual development allowance for you to spend on developing your current skills and learning new things
Tech equipment of your choice
Regular social activities
Generous amount of holidays
We're striving to build a diverse team and we would love to hear from applicants from backgrounds less frequently represented in technology, be that in terms of gender, race, or professional background.
If you think your skills and experience match what we’re looking for and you’d like to join a Carbon Tech industry unicorn, please get in touch!
If you think your skills and experience match what we’re looking for and you’d like to join a Carbon Tech industry unicorn, please get in touch!
Show more
Show less","Python, Java, Spring Boot, Angular, Typescript, PostgreSQL, Redis, Neo4j, Google Identity, Containerised applications, Objectoriented code, Relational databases, Graph databases, Document databases, Infrastructure automation, DevOps, Unit testing, Integration testing, Quality assurance, Agile development, Scrum, Elephant Carpaccio, GitHub PRs, Continuous delivery, Teamwork, Collaboration, Data engineering, Data pipelines, Data modelling, Data warehouses, Lakehouses, Modern data stack, Data frameworks, Fivetran, Airbyte, BigQuery, Databricks, Snowflake, Apache Spark, Kafka, Airflow, Kubernetes, dbt","python, java, spring boot, angular, typescript, postgresql, redis, neo4j, google identity, containerised applications, objectoriented code, relational databases, graph databases, document databases, infrastructure automation, devops, unit testing, integration testing, quality assurance, agile development, scrum, elephant carpaccio, github prs, continuous delivery, teamwork, collaboration, data engineering, data pipelines, data modelling, data warehouses, lakehouses, modern data stack, data frameworks, fivetran, airbyte, bigquery, databricks, snowflake, apache spark, kafka, airflow, kubernetes, dbt","agile development, airbyte, airflow, angular, apache spark, bigquery, collaboration, containerised applications, continuous delivery, data engineering, data frameworks, data modelling, data warehouses, databricks, datapipeline, dbt, devops, document databases, elephant carpaccio, fivetran, github prs, google identity, graph databases, infrastructure automation, integration testing, java, kafka, kubernetes, lakehouses, modern data stack, neo4j, objectoriented code, postgresql, python, quality assurance, redis, relational databases, scrum, snowflake, spring boot, teamwork, typescript, unit testing"
Sr. Data Engineer,CurbWaste,"Los Angeles, CA",https://www.linkedin.com/jobs/view/sr-data-engineer-at-curbwaste-3776466282,2023-12-17,Red Deer, Canada,Mid senior,Hybrid,"About The Role
We’re looking for a Sr Data Engineer who will align with our own passion for delivering an innovative, world-class product that makes our customers successful and gives them joy along the way. The ideal candidate has an eye for performance, best practices & industry standards, and takes pride/ownership of whatever initiative they are working on.. In this position the candidate will work closely with the Director of Engineering, Product Managers, and stakeholders to execute on the product roadmap in effort to address business and customer needs.
About Us
We are an early stage vertical SaaS business dedicated to bringing the most innovative technology to the historically underserved Waste Management Industry. We care deeply about the hard working, dedicated, and humble people of the Waste Management industry and will stop at nothing to ensure that they get the most value technology can offer. We lean on each other to deliver the best value to our customer and we constantly challenge each other to be the best version of ourselves every day. We settle at nothing short of being the best at what we do.
Responsibilities:
Design, develop, and maintain scalable and lightning-fast data model
Collaborate with cross-functional teams to optimize performance of systems
Troubleshoot and resolve complex technical issues and identify areas for improvement
Database administration across all environments
Lead technical team on reporting and analytics strategy
Generate meaning reports with actionable insights
Create tools for onboarding customer data
Analyze and organize data.
Interpret trends and patterns
Prepare data for prescriptive and predictive modeling
Build algorithms and prototypes
Communicate effectively with stakeholders, both technical and non-technical, to ensure project success
Requirements
What You Will Need
5+ years of professional software development experience, with a Bachelor’s degree or 3+ years with a Master’s degree
Advanced working SQL knowledge and experience working with relational databases and familiarity with Postgres
Experience with data transformation, data structures, and workload management
Track record of manipulating, processing and extracting value from large disconnected datasets
Knowledge of queuing, stream processing, and highly scalable ‘big data lakes’
Proven experience in taking initiative on large-scale projects
Excellent problem-solving, analytical, and critical thinking skills
Strong written and verbal communication skills, with the ability to effectively articulate complex technical concepts to diverse audiences
Familiarity with Agile development methodologies and tools (e.g., Scrum, JIRA)
Benefits
What We Offer:
This is not just a job. This is a career, a chance to make a direct impact. Here’s how we help:
Flextime, recognition, and support for autonomous work: Flexible time off with ample learning and development opportunities to continue growing your career
Health benefits: Company-paid medical, dental, and vision
Our Mission:
We aim to change the way waste companies run their business. We are a software founded by haulers and built for haulers. We care about the environment and want to play a positive role in the future of the waste industry. Software helps create solutions and we are focused on being the leaders in change.
At CurbWaste we celebrate individuality and uniqueness. We believe that the convergence of fresh perspectives and experiences from all walks of life is what makes our product and culture so great. We strongly encourage people from underrepresented groups to apply. We do not discriminate against employees based on race, color, religion, sex, national origin, gender identity or expression, age, disability, pregnancy (including childbirth, breastfeeding, or related medical condition), genetic information, protected military or veteran status, sexual orientation, or any other characteristic protected by applicable federal, state or local laws.
Show more
Show less","Data Modeling, SQL, Relational Databases, Data Transformation, Data Structures, Workload Management, Big Data Lakes, Agile Development, Scrum, JIRA, Data Analytics, Reporting","data modeling, sql, relational databases, data transformation, data structures, workload management, big data lakes, agile development, scrum, jira, data analytics, reporting","agile development, big data lakes, data structures, data transformation, dataanalytics, datamodeling, jira, relational databases, reporting, scrum, sql, workload management"
Job Opportunity - Data Engineer with Python Coding.,"Donato Technologies, Inc.","Austin, TX",https://www.linkedin.com/jobs/view/job-opportunity-data-engineer-with-python-coding-at-donato-technologies-inc-3766669358,2023-12-17,Red Deer, Canada,Mid senior,Hybrid,"Job Title:
Data Engineer with Python Coding.
Location:
Austin, TX or Bay Area, CA (Hybrid 3 days onsite).
Duration:
12+ Months.
Ex-Facebook consultant is mandatory
Job Description
10+ years of professional work experience designing and implementing data pipelines in a cloud environment is required.
5+ years of experience migrating/developing data solutions in the AWS cloud is required.
5+ years of experience building/implementing data pipelines using Databricks or similar cloud database.
Expert level knowledge of using SQL to write complex, highly optimized queries across large volumes of data.
5+ years hands-on object-oriented programming experience using Python is required.
Show more
Show less","Python, Data Pipelines, AWS, Data Migration, SQL, Data Solutions, Databricks, Cloud Database, Objectoriented Programming","python, data pipelines, aws, data migration, sql, data solutions, databricks, cloud database, objectoriented programming","aws, cloud database, data migration, data solutions, databricks, datapipeline, objectoriented programming, python, sql"
Big Data Engineer,TMS,"St Louis, MO",https://www.linkedin.com/jobs/view/big-data-engineer-at-tms-3728846367,2023-12-17,Red Deer, Canada,Mid senior,Hybrid,"A Data Engineer Will
The Data Engineer will participate on data management aspects of client engagements to deliver Test & Learn solutions, as well as contribute to and foster a high performance collaborative workplace.
Independently executes projects through design, implementation, automation, and maintenance of large scale enterprise ETL processes for a global client base
Act as an expert data resource within the team
Deliver on-time, accurate, high-value, robust data solutions across multiple clients, solutions and industry sectors
Build trust-based working relationships with peers and clients across local and global teams
All About You
Good understanding of Python – Pandas, Numpy, PySpark and Impala.
Hands-On experience on visualization tools i.e. Power BI, Tableau, Qlik Sense etc..
Experience in doing data analysis and extraction on Hadoop.
Experience with Enterprise Business Intelligence Platform/Data platform.
Strong SQL and higher-level programming languages with solid knowledge of data mining, machine learning algorithms and tools
Experience with data integration tools – ETL/ELT tools (i.e. Apache NiFi, Azure Data Factory, Pentaho, Talend)
Experience with Databrick, Snowflake, Graph Database is a plus.
Experience in hands-on data modeling, programming, querying, data mining and report development using large volumes of granular data to deliver business intelligence and custom reporting solutions.
Exposure to collecting and/or working with data including standardizing, summarizing, offering initial observations and highlighting inconsistencies.
Strong understanding of the application of analytical methods and data visualization to support business decisions.
Ability to understand complex operational systems and analytics/business intelligence tools for the delivery of information products and analytical offerings to a large, global user base.
Show more
Show less","Data Engineering, ETL Processes, Python, Pandas, Numpy, PySpark, Impala, Power BI, Tableau, Qlik Sense, Hadoop, Business Intelligence Platform, SQL, Data Mining, Machine Learning Algorithms, ETL/ELT Tools, Apache NiFi, Azure Data Factory, Pentaho, Talend, Databricks, Snowflake, Graph Database, Data Modeling, Data Querying, Report Development, Analytical Methods, Data Visualization, Business Intelligence Tools","data engineering, etl processes, python, pandas, numpy, pyspark, impala, power bi, tableau, qlik sense, hadoop, business intelligence platform, sql, data mining, machine learning algorithms, etlelt tools, apache nifi, azure data factory, pentaho, talend, databricks, snowflake, graph database, data modeling, data querying, report development, analytical methods, data visualization, business intelligence tools","analytical methods, apache nifi, azure data factory, business intelligence platform, business intelligence tools, data engineering, data mining, data querying, databricks, datamodeling, etl, etlelt tools, graph database, hadoop, impala, machine learning algorithms, numpy, pandas, pentaho, powerbi, python, qlik sense, report development, snowflake, spark, sql, tableau, talend, visualization"
Data Engineer,LaSalle Network,"Chicago, IL",https://www.linkedin.com/jobs/view/data-engineer-at-lasalle-network-3730251742,2023-12-17,Red Deer, Canada,Mid senior,Hybrid,"Want to grow your career with one of the fastest growing industries? LaSalle Network is partnered with a top Chicago-based client looking to hire a
Data Engineer
their team.
The
Data Engineer
will be responsible for delivering strategic and tangible solutions through data. We are looking for self-starters that have the ability to work cross-functionally across various teams.
Data Engineer Responsibilities:
Architect, implement and maintain the information data lake and leverage various reporting platforms to enable decision making across the portfolio companies
Build ETLs to ingest data into the data warehouse and data lake, as well as end-user facing reporting applications
Work to deliver flexible, scalable, end-to-end solutions
Work with big data and emerging technologies while driving business intelligence solutions end-to-end: business requirements, data modeling, ETL, reporting and dashboarding
Close coordination with the senior leadership team on a day-to-day basis; ability to clearly communicate complicated data infrastructure concepts is a must
Support for other ad hoc strategic data projects and initiatives
Data Engineer Requirements:
4+ years of experience with Data Modeling, SQL, ETL, Data Warehousing, (Snowflake, Redshift, Azure, AWS)
4+ years of experience with enterprise-class Business Intelligence tools such as Looker, Power BI, Tableau, Oracle BI, MicroStrategy
Python knowledge or experience an added plus
Bachelor's degree in computer science, information systems, mathematics, statistics or related field
Expertise in the design, creation, management and business use of large datasets
Machine Learning experience a plus
Passionate about working with large unstructured datasets and helping set the vision for the overall data strategy across the growing portfolio
Thank you,
Kelsey Gonzalez
Team Lead
LaSalle Network
LaSalle Network is an Equal Opportunity Employer m/f/d/v.
LaSalle Network is the leading provider of direct hire and temporary staffing services. For over two decades, LaSalle has helped organizations hire faster and connect top talent with opportunities, from entry-level positions to the C-suite. With units specializing in Accounting and Finance, Administrative, Marketing, Technology, Supply chain, Healthcare Revenue Cycle, Call Center, Human Resources and Executive Search. LaSalle offers staffing and recruiting solutions to companies of all sizes and across all industries.
LaSalle Network is the premier staffing and recruiting firm, earning over 100 culture, revenue and industry-based awards from major publications and having its company experts regularly contribute insights on retention strategies, hiring trends and hiring challenges, and more to national news outlets.
Show more
Show less","Data Engineering, Data Modeling, SQL, ETL, Data Warehousing, Snowflake, Redshift, Azure, AWS, Looker, Power BI, Tableau, Oracle BI, MicroStrategy, Python, Machine Learning, Big Data, Data Analytics, Data Infrastructure, Cloud Computing, Agile Development, Collaboration","data engineering, data modeling, sql, etl, data warehousing, snowflake, redshift, azure, aws, looker, power bi, tableau, oracle bi, microstrategy, python, machine learning, big data, data analytics, data infrastructure, cloud computing, agile development, collaboration","agile development, aws, azure, big data, cloud computing, collaboration, data engineering, data infrastructure, dataanalytics, datamodeling, datawarehouse, etl, looker, machine learning, microstrategy, oracle bi, powerbi, python, redshift, snowflake, sql, tableau"
Python Developer (not a data engineer),Oakridge Staffing,New York City Metropolitan Area,https://www.linkedin.com/jobs/view/python-developer-not-a-data-engineer-at-oakridge-staffing-3737981283,2023-12-17,Red Deer, Canada,Mid senior,Hybrid,"Our alternative asset firm is looking to add a strong Python Developer. While we don't need financial services experience, a background that has demonstrated taking projects through completion is needed.
3+ year’s work experience (preferably in financial sector though not must)
ideally Computer Science
Show more
Show less","Python Developer, Python, Financial Services, Computer Science","python developer, python, financial services, computer science","computer science, financial services, python, python developer"
Homes.com - Senior Database Engineer,Homes.com,"Irvine, CA",https://www.linkedin.com/jobs/view/homes-com-senior-database-engineer-at-homes-com-3785824066,2023-12-17,Red Deer, Canada,Mid senior,Hybrid,"Overview
Job Description
CoStar Group (NASDAQ: CSGP) is a leading global provider of commercial and residential real estate information, analytics, and online marketplaces. Included in the S&P 500 Index and the NASDAQ 100, CoStar Group is on a mission to digitize the world’s real estate, empowering all people to discover properties, insights and connections that improve their businesses and lives. We have been living and breathing the world of real estate information and online marketplaces for over 35 years, giving us the perspective to create truly unique and valuable offerings to our customers. We’ve continually refined, transformed and perfected our approach to our business, creating a language that has become standard in our industry, for our customers, and even our competitors. We continue that effort today and are always working to improve and drive innovation. This is how we deliver for our customers, our employees, and investors. By equipping the brightest minds with the best resources available, we provide an invaluable edge in real estate.
Homes.com is already one of the fastest growing real estate portals in the industry, we are driven to be #1. Just ask Brad Bellflower, Chief Change Officer at Apartments.com. After its acquisition in 2014, Apartments.com quickly turned into the most popular place to find a place. Proven success at
the highest level – and we’re doing it again with the new Homes.com. Homes.com is a CoStar Group company with 20+ years' experience in leading and growing digital marketplaces. We pride ourselves on continually improving, innovating, and setting the standard for property search and marketing experiences. With Homes.com we’re building a brand on the cusp of defining the industry. We’re looking for big thinkers, brave leaders, and creative advertising wizards ready to influence a new age of homebuying within a tried-and-true, award-winning company.
Learn more about Homes.com.
As part of a data company, Homes.com plays a strategic role in the design and implementation of the data that powers our consumer websites, customer portals, and countless other data products. Primary responsibilities include the design and implementation of management, reporting, portal, and other systems that support our consumer, customer, executive, sales, marketing, and research experiences. Additionally, they perform a vital role in our corporate integration projects. They will design, implement, and maintain complex database code in T-SQL and SSIS for transaction processing systems and data warehousing.
This position is located in Irvine, CA and offers a hybrid schedule of 3 days onsite, 2 days remote.
Responsibilities
Leverage your TSQL and SSIS skills in the development of our next generation data platforms that drives the data that is displayed on Homes.com
Use your Data Warehouse experience to help design and develop Homes.com DW that’s heavily used by internal customers for Business Intelligence, KPI monitoring, and driving Product
Ensure systems are designed for scale and be able to test for load and analyze results
Participate in the quality assurance on the systems we build to keep our platform accurate and reliable
Work well in small, tight knit agile teams
Basic Qualifications
Bachelor’s Degree required from an accredited, not for profit university or college
A track record of commitment to prior employers
5+ years in SQL Server development in a large-scale enterprise application environment, including:
Stored procedure and query development
Performance tuning, indexing techniques, and materialized views
Schema design
Data integration using SQL Server Integration Services or equivalent
Experience working with RDS, Snowflake, PostgreSQL and DynamoDB
SQL Server Availability Groups
Columnstore design, loading, and analysis
Replication and bulk data propagation techniques
SQL Server Reporting Services
Expert level proficiency with T-SQL
Advanced skills and experience with ETL concepts and SSIS
Advanced skills and experience with data modeling (Logical and Physical)
Preferred Qualifications
Business Intelligence and reporting experience (e.g., Spotfire) is a plus
Experience with NoSQL databases (e.g., MongoDB) is a plus
Knowledge of cloud solutions (eg., Amazon Redshift, Teradata cloud) is a plus
What’s In It For You
When you join CoStar Group, you’ll experience a collaborative and innovative culture working alongside the best and brightest to empower our people and customers to succeed.
We offer you generous compensation and performance-based incentives. CoStar Group also invests in your professional and academic growth with internal training, tuition reimbursement, and an inter-office exchange program.
Our Benefits Package Includes (but Is Not Limited To)
Comprehensive healthcare coverage: Medical / Vision / Dental / Prescription Drug
Life, legal, and supplementary insurance
Virtual and in person mental health counseling services for individuals and family
Commuter and parking benefits
401(K) retirement plan with matching contributions
Employee stock purchase plan
Paid time off
Tuition reimbursement
On-site fitness center and/or reimbursed fitness center membership costs (location dependent), with yoga studio, Pelotons, personal training, group exercise classes
Access to CoStar Group’s Diversity, Equity, & Inclusion Employee Resource Groups
Complimentary gourmet coffee, tea, hot chocolate, fresh fruit, and other healthy snacks
We welcome all qualified candidates who are currently eligible to work full-time in the United States to apply. However, please note that CoStar Group is not able to provide visa sponsorship for this position.
This position offers a base salary range of $110,000.00 – 198,600.00, based on relevant skills and experience and includes a generous benefits plan.
CoStar Group is an Equal Employment Opportunity Employer; we maintain a drug-free workplace and perform pre-employment substance abuse testing
Show more
Show less","TSQL, SSIS, SQL Server, Data Warehouse, RDS, Snowflake, PostgreSQL, DynamoDB, SQL Server Availability Groups, Columnstore, Replication, SQL Server Reporting Services, ETL, Data Modeling, Spotfire, MongoDB, Amazon Redshift, Teradata","tsql, ssis, sql server, data warehouse, rds, snowflake, postgresql, dynamodb, sql server availability groups, columnstore, replication, sql server reporting services, etl, data modeling, spotfire, mongodb, amazon redshift, teradata","amazon redshift, columnstore, datamodeling, datawarehouse, dynamodb, etl, mongodb, postgresql, rds, replication, snowflake, spotfire, sql server, sql server availability groups, sql server reporting services, ssis, teradata, tsql"
Data Engineer,Trispoke Managed Services Pvt. Ltd.,"Wilmington, DE",https://www.linkedin.com/jobs/view/data-engineer-at-trispoke-managed-services-pvt-ltd-3784860778,2023-12-17,Red Deer, Canada,Mid senior,Hybrid,"Job Title: Data Engineer
Job Location: Wilmington, DE 19803 (Hybrid - 3 days/Week Onsite required)
Job Duration: 12 months contract Possible extension
Required Qualifications, Capabilities, And Skills
5+ years of professional experience
Strong written, verbal communication and presentation skills
Expertise in AWS – e.g. Glue, EMR, S3, Lambda, Step functions, EventBridge, Data Analytics
Software Development expertise in Java/Python/Spark/Scala
Strong understanding of Database Management Systems, expertise in SQL
Professional in SDLC Tools: Bitbucket/Git, Jenkins, Jules
Ability to maintain stakeholder expectations and being able to juggle multiple priorities
Experience in Data Lake, Data Bases and Data modeling
Familiarity with Agile methodologies such as Scrum, Kanban, SAFe
Ability to work independently and be self-motivated
Fast learner and ability to solve complex problems
Preferred Qualifications, Capabilities, And Skills
Expertise with BI Reporting Tools such as Tableau, PowerBI, Qlikview/Qliksense
Show more
Show less","AWS, Glue, EMR, S3, Lambda, Step functions, EventBridge, Data Analytics, Java, Python, Spark, Scala, SQL, Bitbucket, Git, Jenkins, Jules, Data Lake, Data Bases, Data Modeling, Agile, Scrum, Kanban, SAFe, Tableau, PowerBI, QlikView, QlikSense","aws, glue, emr, s3, lambda, step functions, eventbridge, data analytics, java, python, spark, scala, sql, bitbucket, git, jenkins, jules, data lake, data bases, data modeling, agile, scrum, kanban, safe, tableau, powerbi, qlikview, qliksense","agile, aws, bitbucket, data bases, data lake, dataanalytics, datamodeling, emr, eventbridge, git, glue, java, jenkins, jules, kanban, lambda, powerbi, python, qliksense, qlikview, s3, safe, scala, scrum, spark, sql, step functions, tableau"
Data Engineer,HP,"Spring, TX",https://www.linkedin.com/jobs/view/data-engineer-at-hp-3772977763,2023-12-17,Red Deer, Canada,Mid senior,Hybrid,"We are looking for a
Data Engineer
to join our team and help us build and maintain scalable data pipelines and systems. You will be responsible for designing, developing, testing, and deploying data solutions that meet the needs of our clients and stakeholders. You will also collaborate with data analysts, data scientists, and other data engineers to ensure data quality, reliability, and performance.
If you are interested in working on petabytes of data from millions of devices. If solving complex data issues excites you this is the opportunity for you.
Responsibilities
Leads the team to write, deploy, and maintain software to build, integrate, manage, maintain, and quality-assure data.
Architects, designs, implements, and maintains reliable and scalable data solutions in the AWS cloud environment using Scrum/Agile methodology.
Implement data ingestion, transformation, and processing workflows using ETL tools and frameworks.
Researches and promotes new tools and techniques to shape the future of the data engineering environment.
Ensure data security, privacy, and compliance with relevant regulations and policies.
Monitor, troubleshoot, and debug data issues and performance bottlenecks. Guides team to deploy secure and well-tested software that meets privacy and compliance requirements; develops, maintains, and improves CI / CD pipeline.
Document and communicate data engineering processes and solutions to stakeholders and users.
Represents the data engineering team for all phases of larger and more-complex development projects.
Works with following site-reliability engineering standard methodologies: on-call rotations for services they maintain, responsible for defining and maintaining SLAs. Design, build, deploy and maintain infrastructure as code. Containerizes server deployments.
Actively contributes to improve developer velocity.
Knowledge & Skills
Demonstrable coding expertise in one or more object-oriented programming languages (e.g., Python, Scala, Java, etc.)
Deep and hands-on experience (5+ years) designing, planning, productionizing, maintaining, and documenting reliable and scalable data infrastructure and data products in complex environments.
Hands on experience with:
Expert in AWS tools and services such as S3, Glue, Lambda, EMR, Redshift, Athena, etc.
Experience with other cloud platforms and services such as Azure, GCP, etc. is a plus.
Experience with data quality, testing, and validation tools and techniques
Experience with data visualization and reporting tools such as QuickSight, Tableau, Power BI, etc.
Strong analytical and problem-solving skills
Excellent communication and collaboration skill
Understanding Data Structures & Algorithms & their performance
Experience designing and implementing large-scale distributed systems.
Deep knowledge and hands-on experience in technologies across all data lifecycle stages
Internal client management and ability to lead large organizations via influence .
Ability to effectively communicate product architectures, design proposals and negotiate options at senior management levels.
Education & Experience
Bachelor's or Master's degree in Computer Science, Information Systems, Engineering, or equivalent.
About HP
You’re out to reimagine and reinvent what’s possible—in your career as well as the world around you.
So are we. We love taking on tough challenges, disrupting the status quo, and creating what’s next. We’re in search of talented people who are inspired by big challenges, driven to learn and grow, and dedicated to making a meaningful difference.
HP is a technology company that operates in more than 170 countries around the world united in creating technology that makes life better for everyone, everywhere.
Our history: HP’s commitment to diversity, equity and inclusion – it's just who we are.
From the boardroom to factory floor, we create a culture where everyone is respected and where people can be themselves, while being a part of something bigger than themselves. We celebrate the notion that you can belong at HP and bring your authentic self to work each and every day. When you do that, you’re more innovative and that helps grow our bottom line. Come to HP and thrive!
Show more
Show less","Python, Scala, Java, AWS, S3, Glue, Lambda, EMR, Redshift, Athena, QuickSight, Tableau, Power BI, Data Structures, Algorithms, Cloud Computing, Data Quality, Data Testing, Data Validation, Data Visualization, Data Reporting, Communication, Collaboration, Problem Solving","python, scala, java, aws, s3, glue, lambda, emr, redshift, athena, quicksight, tableau, power bi, data structures, algorithms, cloud computing, data quality, data testing, data validation, data visualization, data reporting, communication, collaboration, problem solving","algorithms, athena, aws, cloud computing, collaboration, communication, data quality, data reporting, data structures, data testing, data validation, emr, glue, java, lambda, powerbi, problem solving, python, quicksight, redshift, s3, scala, tableau, visualization"
Senior Data Engineer,Storm4,"Somerville, MA",https://www.linkedin.com/jobs/view/senior-data-engineer-at-storm4-3779390533,2023-12-17,Red Deer, Canada,Mid senior,Hybrid,"⚡ Senior Cloud Data Engineer
🌎 Somerville, Massachusetts, United States
💼 Salary: $120,000 - $170,000
We are working with a company who pioneers in cutting edge radar technology designed to make autonomous and assisted driving safer and higher performing. Their radar is used to create a map of subsurface signatures from which self-driving cars can navigate.
They are now working with leading autonomous vehicle and automotive companies and are looking to build their talented team with a
Senior Cloud Data Engineer
who will shape and implement cloud infrastructure for their radar-based map and sensor fusion-based localization algorithms.
Roles and Responsibilities:
Design, develop and deploy the data infrastructure
which includes a distributed big data platform, data lake on top of cloud storage system (S3)
Design and maintain metadata systems, data catalogs, data governance, data search and discovery and related services
Troubleshoot and test for security, performance, and availability of the production systems
Own and extend the HD-MAP data pipeline
through the collection, storage, processing, and transmission of their datasets.
Minimum Qualifications:
7+ years of experience
in Cloud Platforms: AWS, Azure, Google Cloud: within AWS: IAM, AWS, EKS VPC, S3 etc.
Experience with hands-on experience building, releasing, monitoring, and supporting mission-critical services in high traffic applications
Solid programming skills with
Golang, TypeScript, Python, shell script, terraform, packer, C++
etc.
Experience with
Docker, Kubernetes
MS or BS in Computer Science or comparable engineering degree
Benefits
Competitive Salary and Stock options
Company-supported career development opportunities
To be part of a fast-scaling company with great progression opportunities
If the above description sounds like you then we’d love to hear from you! Please click on the “Easy Apply” button at the top of this page and follow the instructions to send us your application. Alternatively, feel free to drop an email to shannon.barnes@storm4.com if you prefer.
If you have any extra requirements to support your application, please add a note along with your CV to let us know.
Show more
Show less","Cloud Platforms, AWS, Azure, Google Cloud:, IAM, AWS, EKS VPC, S3, Docker, Kubernetes, Golang, TypeScript, Python, Shell script, Terraform, Packer, C++, Data lake, Data catalog, Data governance, Data search and discovery, HDMAP data pipeline, Data collection, Data storage, Data processing, Data transmission","cloud platforms, aws, azure, google cloud, iam, aws, eks vpc, s3, docker, kubernetes, golang, typescript, python, shell script, terraform, packer, c, data lake, data catalog, data governance, data search and discovery, hdmap data pipeline, data collection, data storage, data processing, data transmission","aws, azure, c, cloud platforms, data catalog, data collection, data governance, data lake, data processing, data search and discovery, data storage, data transmission, docker, eks vpc, golang, google cloud, hdmap data pipeline, iam, kubernetes, packer, python, s3, shell script, terraform, typescript"
Senior Data Engineer,"Request Technology, LLC","Chicago, IL",https://www.linkedin.com/jobs/view/senior-data-engineer-at-request-technology-llc-3781706197,2023-12-17,Red Deer, Canada,Mid senior,Hybrid,"Senior Data Engineer
Salary: $150k-$170k + bonus
Location: Chicago, IL
Hybrid: 3 days on-site, 2 days remote
*We are unable to provide sponsorship for this role*
Qualifications
Bachelor’s degree
8+ years of experience in ETL, ELT and data engineering
8+ years of end-to-end implementation experience of deploying enterprise data warehouse, data mart and data lake solutions.
5+ years of Implementation and maintenance experience with Talend DI, DQ capabilities
3+ years of working experience with Azure data solutions including but not limited to ADLS, Data Bricks, ADF, Synapse etc.
· 3+ years of experience on Azure data platforms
Azure ADLS/Databricks administration experience
Pluses
Data architecture, Data Modeling, and Data Visualization
Relevant certifications
Responsibilities
Accountable for the technical leadership regarding the data integration solutions and delivery. Ensuring a sound and best in class design, with enterprise implementation, deployment and operational meets the technical quality standards.
Responsible for planning and coordinating in carving out the needed dev/test environments, as well as defining and managing code branching/config strategies supporting concurrent releases.
Works with Data and Enterprise architecture team to define the data integration design/coding/deployment/operational standards and technology stack.
Responsible for data operations, in terms of scheduling, successful execution, and reconciliation of the data pipes in production
Works collaboratively with other dev teams to guide and review their deliverables against the set standards.
Works collaboratively with Data Analytics, applications, DBA, and cloud operations teams to ensure end to end integrity and usage of data assets.
Show more
Show less","Data Engineering, ETL, ELT, Data Warehousing, Data Marts, Data Lakes, Talend DI, Talend DQ, Azure Data Solutions, Azure ADLS, Databricks, Azure Data Factory, Azure Synapse, Azure Data Platforms, Data Architecture, Data Modeling, Data Visualization, Software Development, Code Branching, Config Strategies, Data Operations, Data Scheduling, Data Execution, Data Reconciliation, Data Pipelines, Data Analytics, Applications, DBA, Cloud Operations","data engineering, etl, elt, data warehousing, data marts, data lakes, talend di, talend dq, azure data solutions, azure adls, databricks, azure data factory, azure synapse, azure data platforms, data architecture, data modeling, data visualization, software development, code branching, config strategies, data operations, data scheduling, data execution, data reconciliation, data pipelines, data analytics, applications, dba, cloud operations","applications, azure adls, azure data factory, azure data platforms, azure data solutions, azure synapse, cloud operations, code branching, config strategies, data architecture, data engineering, data execution, data lakes, data marts, data operations, data reconciliation, data scheduling, dataanalytics, databricks, datamodeling, datapipeline, datawarehouse, dba, elt, etl, software development, talend di, talend dq, visualization"
Senior Data Analyst/Engineer,Steneral Consulting,"Englewood, CO",https://www.linkedin.com/jobs/view/senior-data-analyst-engineer-at-steneral-consulting-3670918321,2023-12-17,Red Deer, Canada,Mid senior,Hybrid,"Senior Data Analyst/Engineer - w2 candidates only
Location: 8560 Upland Drive Englewood CO 80112 - Hybrid work environment, 3 days onsite, 2 day remote - that are willing to relocate or Local to Colorado that can go on site from day 1
Job Summary
For this senior level position, the ideal candidate is an expert in Python programming and SQL, with extensive experience in batch and real-time data processing frameworks such as Hadoop and Apache Spark. They should also have a strong working knowledge of cloud services, particularly AWS or other cloud ecosystems, and a solid understanding of ETL processes for data transformation. The candidate should have experience working with large datasets consisting of both structured and semi-structured data, as well as a strong understanding of system-level architecture to support such datasets. The candidate should also possess a willingness to learn about networking, WiFi, and wireless technologies, as well as automation frameworks related to these areas. They should be able to collaborate effectively with data analysts and business stakeholders to build the product and feature pipeline, ensuring that the data products meet the desired requirements. The candidate should have a strong ability to develop and expand their application knowledge base, staying up-to-date with the latest industry trends and best practices for delivering data products. As part of their responsibilities, the candidate should proactively identify internal and external dependencies, and effectively communicate any issues, scope changes, and progress against the project plan.
Major Duties And Responsibilities
Work with data analyst and business stakeholders to build the product and feature pipeline.
Develop and expand application knowledge base and best practices for delivering data products.
Provide leadership in architecture, design for analytics product.
Identification of internal and external dependencies, highlighting issues, scope changes and progress against the project plan.
Required Qualifications
Skills/Abilities and Knowledge
Expert level proficiency Python Programming and SQL
Experience around batch and real time data processing framework (Hadoop, Apache Spark)
Working experience in cloud services (AWS or other cloud ecosystem) and understanding of ETL processes for data transformation
Experience around processing large dataset consisting structured and semi-structured data and supporting system level architecture
Ability to assess business requirements collaborate with stakeholders
Experience in designing and developing core modules used to build real-time and batch data processing
Proven ability to balance and manage multiple competing priorities
Collaborative interpersonal skills and ability to work with cross-functional teams
Preferred Qualifications
Skills/Abilities and Knowledge
Willingness to learn networking and WiFi and Wireless technologies and automation frameworks.
Experience with data visualization/reporting tools: Tableau or anything Microsoft Power BI is preferred
Experience with Python: numpy, pandas, pyplot, seaborn (xlsxwriter, pyodbc, pymongo, and pptx)
Experience with databases: SQL and NoSQL (MongoDB)
Experience with reporting tools: Tableau
Experience with Gitlab; JIRA; Confluence
Understanding of Agile methodology
Education
Bachelor's degree in business or related field
Related Work Experience Number Of Years
Data engineering and reporting or related experience 5+
WORKING CONDITIONS
Office environment
Show more
Show less","Python, SQL, Hadoop, Apache Spark, Cloud services, AWS, ETL, Structured data, Semistructured data, Networking, WiFi, Wireless technologies, Automation frameworks, Data visualization, Tableau, Power BI, Data reporting, Pandas, Pyplot, Seaborn, MongoDB, SQL, NoSQL, Agile methodology, JIRA, Confluence, Gitlab, xlsxwriter, pyodbc, pymongo, pptx","python, sql, hadoop, apache spark, cloud services, aws, etl, structured data, semistructured data, networking, wifi, wireless technologies, automation frameworks, data visualization, tableau, power bi, data reporting, pandas, pyplot, seaborn, mongodb, sql, nosql, agile methodology, jira, confluence, gitlab, xlsxwriter, pyodbc, pymongo, pptx","agile methodology, apache spark, automation frameworks, aws, cloud services, confluence, data reporting, etl, gitlab, hadoop, jira, mongodb, networking, nosql, pandas, powerbi, pptx, pymongo, pyodbc, pyplot, python, seaborn, semistructured data, sql, structured data, tableau, visualization, wifi, wireless technologies, xlsxwriter"
Senior Data Engineer,TekIntegral,"New York, NY",https://www.linkedin.com/jobs/view/senior-data-engineer-at-tekintegral-3688709308,2023-12-17,Red Deer, Canada,Mid senior,Hybrid,"Senior Data Engineer - Python and Data Bricks
Location: New York Hybrid Report on-site on day 1 (3 days per week in the office)
Pay Rate: $60 - $65 /hour on C2C
Visa: USC GC
Duration: 8-12 Months
Job Description
We are seeking a highly skilled and experienced Senior Data Engineer to join our dynamic team. As a Senior Data Engineer, you will play a crucial role in designing, implementing, and maintaining data pipelines and infrastructure for our big data projects. Your expertise in Java, Python, Spark cluster management, data science, big data, REST API development, and knowledge of Databricks and Delta Lake will be essential in driving the success of our data initiatives.
Responsibilities
Design, develop, and implement scalable data pipelines and ETL processes using Java, Python, and Spark.
Collaborate with data scientists, analysts, and other stakeholders to understand data requirements and design efficient solutions.
Manage and optimize Spark clusters to ensure high performance and reliability.
Perform data exploration, data cleaning, and data transformation tasks to prepare data for analysis and modeling.
Develop and maintain data models and schemas to support data integration and analysis.
Implement data quality and validation checks to ensure accuracy and consistency of data.
Utilize REST API development skills to create and integrate data services and endpoints for seamless data access and consumption.
Monitor and troubleshoot data pipeline performance, identifying and resolving bottlenecks and issues.
Stay updated with the latest technologies and trends in big data, data engineering, data science, and REST API development, and provide recommendations for process improvements.
Mentor and guide junior team members, providing technical leadership and sharing best practices.
Qualifications
Master's degree in Computer Science, Data Science, or a related field.
Minimum of 6 years of professional experience in data engineering, working with Java, Python, Spark, and big data technologies.
Strong programming skills in Java and Python, with expertise in building scalable and maintainable code.
Proven experience in Spark cluster management, optimization, and performance tuning.
Solid understanding of data science concepts and experience working with data scientists and analysts.
Proficiency in SQL and experience with relational databases (e.g., Snowflake, Delta Tables).
Experience in designing and developing REST APIs using frameworks such as Flask or Spring.
Familiarity with cloud-based data platforms (e.g.Azure)
Experience with data warehousing concepts and tools (e.g., Snowflake,
BigQuery) is a plus.
Strong problem-solving and analytical skills, with the ability to tackle complex data engineering challenges.
Excellent communication and collaboration skills, with the ability to work effectively in a team-oriented environment.
Show more
Show less","Java, Python, Spark cluster management, Data science, Big data, REST API development, Databricks, Delta Lake, Data pipelines, ETL processes, Data exploration, Data cleaning, Data transformation, Data models, Data schemas, Data integration, Data quality, Data validation, Snowflake, SQL, Relational databases, Flask, Spring, Cloudbased data platforms, Data warehousing, BigQuery","java, python, spark cluster management, data science, big data, rest api development, databricks, delta lake, data pipelines, etl processes, data exploration, data cleaning, data transformation, data models, data schemas, data integration, data quality, data validation, snowflake, sql, relational databases, flask, spring, cloudbased data platforms, data warehousing, bigquery","big data, bigquery, cloudbased data platforms, data cleaning, data exploration, data integration, data models, data quality, data schemas, data science, data transformation, data validation, databricks, datapipeline, datawarehouse, delta lake, etl, flask, java, python, relational databases, rest api development, snowflake, spark cluster management, spring, sql"
"Hybrid Work - Data Engineer/Senior Data Engineering (GCP, Python, Hadoop)-locals in Irving TX",Steneral Consulting,"Irving, TX",https://www.linkedin.com/jobs/view/hybrid-work-data-engineer-senior-data-engineering-gcp-python-hadoop-locals-in-irving-tx-at-steneral-consulting-3675562418,2023-12-17,Red Deer, Canada,Mid senior,Hybrid,"Job
: Data Engineer/Senior Data Engineering (GCP, Python, Hadoop)
Location:
Hybrid- Irving, TX
Term
: to end of Year + long term requirement
Interview Process:
one and done screen Face to face/ ONSITE in Irving, TX office
Position Summary
Analyzes complex data structures from disparate data sources and design large scale data engineering pipeline
Develops large scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs
Collaborates with product business and data science team to collect user stories and translate into technical specifications
Uses knowledge in Cloud & Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelines
Uses strong programming skills in PySpark, Python, Java or any of the major languages to build robust data pipelines and dynamic systems
Builds highly scalable and extensible data marts and data models to support Data Science and other internal customers on Cloud. Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards.
Analyzes current information technology environments to identify and assess critical capabilities and recommend solutions
Experiments with available tools and advice on new tools to determine optimal solution given the requirements dictated by the model/use case
Required Qualifications
3+ years of progressively complex related experience in cloud data engineering and data analysis
2+ years GCP experience- must be working with GCP in current role
5+ years of US Based work experience
Knowledge in programing languages such as PySpark, Java, Python, Hive, SQL
Knowledge in Cloud Technology, Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environment
Strong knowledge of large-scale search applications and building high volume data pipelines, preferably using Dataproc, composer services on GCP o
Preferred Qualifications
Ability to leverage multiple tools and programming languages to analyze and manipulate datasets from disparate data sources
Ability to understand complex systems and solve challenging analytical problems
Experience with bash shell scripts, UNIX utilities & UNIX Commands
Show more
Show less","Data Engineering, Data Analysis, Cloud Computing, Hadoop, HDFS, PySpark, Java, Python, Hive, SQL, Linux, Data Marts, Data Models, Data Integration, Data Quality, Business Intelligence, Bash Shell Scripts, UNIX Utilities","data engineering, data analysis, cloud computing, hadoop, hdfs, pyspark, java, python, hive, sql, linux, data marts, data models, data integration, data quality, business intelligence, bash shell scripts, unix utilities","bash shell scripts, business intelligence, cloud computing, data engineering, data integration, data marts, data models, data quality, dataanalytics, hadoop, hdfs, hive, java, linux, python, spark, sql, unix utilities"
Data Engineer (13+ Years),LTIMindtree,"Chicago, IL",https://www.linkedin.com/jobs/view/data-engineer-13%2B-years-at-ltimindtree-3759342507,2023-12-17,Red Deer, Canada,Mid senior,Hybrid,"$120000-$130000
Dice- https://www.dice.com/employer/job/f6c37c38200e42afb6e6aa3671d441b1
About US:
LTIMindtree
is a global technology consulting and digital solutions company that enables enterprises across industries to reimagine business models, accelerate innovation, and maximize growth by harnessing digital technologies. As a digital transformation partner to more than 700+ clients, LTIMindtree brings extensive domain and technology expertise to help drive superior competitive differentiation, customer experiences, and business outcomes in a converging world. Powered by nearly 90,000 talented and entrepreneurial professionals across more than 30 countries, LTIMindtree — a Larsen & Toubro Group company — combines the industry-acclaimed strengths of erstwhile Larsen and Toubro Infotech and Mindtree in solving the most complex business challenges and delivering transformation at scale. For more information, please visit www.ltimindtree.com.
Job Description:
Data Engineer - Stored procedures, SQL etc
Experience: 13 + years
·
Extensive experience in design and development of Databases, SQL Server
,
stored procedures, Indexes, Views, and Triggers.
· Database concepts, ability to write complex stored procedures, functions, triggers etc. with
close view on database performance.
·
Strong SQL
and performance improvement background.
· Good with designing table structure considering performance in mind.
· Good understanding of building data pipelines.
· Good understanding of Snowflake and associated workflows.
Azure
data basics
Job Title: Data Engineer with strong experience with Stored procedures, SQL etc.
Work Location
Location: Chicago, IL
Benefits/perks listed below may vary depending on the nature of your employment with LTIMindtree (“LTIM”):
Benefits and Perks:
Comprehensive Medical Plan Covering Medical, Dental, Vision
Short Term and Long-Term Disability Coverage
401(k) Plan with Company match
Life Insurance
Vacation Time, Sick Leave, Paid Holidays
Paid Paternity and Maternity Leave
The range displayed on each job posting reflects the minimum and maximum salary target for the position across all US locations. Within the range, individual pay is determined by work location and job level and additional factors including job-related skills, experience, and relevant education or training. Depending on the position offered, other forms of compensation may be provided as part of overall compensation like an annual performance-based bonus, sales incentive pay and other forms of bonus or variable compensation.
Disclaimer:
The compensation and benefits information provided herein is accurate as of the date of this posting.
LTIMindtree
is an equal opportunity employer that is committed to diversity in the workplace. Our employment decisions are made without regard to race, color, creed, religion, sex (including pregnancy, childbirth or related medical conditions), gender identity or expression, national origin, ancestry, age, family-care status, veteran status, marital status, civil union status, domestic partnership status, military service, handicap or disability or history of handicap or disability, genetic information, atypical hereditary cellular or blood trait, union affiliation, affectional or sexual orientation or preference, or any other characteristic protected by applicable federal, state, or local law, except where such considerations are bona fide occupational qualifications permitted by law.
Safe return to office
:
In order to comply with LTIMindtree’ s company COVID-19 vaccine mandate, candidates must be able to provide proof of full vaccination against COVID-19 before or by the date of hire. Alternatively, one may submit a request for reasonable accommodation from LTIMindtree’s COVID-19 vaccination mandate for approval, in accordance with applicable state and federal law, by the date of hire. Any request is subject to review through LTIMindtree’s applicable processes.
Show more
Show less","Data Engineering, SQL, Stored procedures, Databases, SQL Server, Indexes, Views, Triggers, Snowflake, Azure","data engineering, sql, stored procedures, databases, sql server, indexes, views, triggers, snowflake, azure","azure, data engineering, databases, indexes, snowflake, sql, sql server, stored procedures, triggers, views"
Report Developer/Data Engineer,Steneral Consulting,"Des Moines, IA",https://www.linkedin.com/jobs/view/report-developer-data-engineer-at-steneral-consulting-3755298271,2023-12-17,Red Deer, Canada,Mid senior,Hybrid,"Des Moines, IA - Hybrid -
This position requires onsite/hybrid work 3 days a week in Des Moines, IA.
The client would prefer someone local to Iowa, but will consider relocation as long as they are from the Midwest and can be onsite to work from day one.
Banking Industry Experience Required.
Must have LinkedIn
Must Have’s
Report Creation
SSRS
Power BI
Banking industry experience
ETL
Data Warehouse concepts
Review documented report specifications, clarify requirement ambiguity, develop report output, and unit test the reports to confirm data accuracy. Report Developer may also participate in QA or UAT testing as requested.
Excellent communication and collaboration and easily understood.
Job Description
Position Purpose:
Responsible for development of SSRS and PowerBI reports in accordance with business requirements. This position will also support development, deployment and implementation of the Bank’s Data Warehouse (HUB). Data Developer role will review documented report specifications, clarify requirement ambiguity, develop report output, and unit test the reports to confirm data accuracy. Report Developer may also participate in QA or UAT testing as requested.
Responsibilities/Duties/Function/Tasks
Works with Business Intelligence Analysts, internal customers, and external consulting teams to understand reporting requirements.
Provides recommendations to the internal customers with reporting choices based on future-state reporting needs.
Develop SSRS and/or PowerBI reports as assigned.
Unit test reports to confirm data accuracy.
Work with development teams, architects, management, and business stakeholders to ensure the Hub infrastructure is aligned with business requirements and IT standards.
Implement data solutions and designs with high quality to ensure alignment with the data architecture and applicable standards.
Support QA and UAT testing as assigned.
Quickly troubleshoot and resolve incidents and problems and follow with root cause analysis and solution.
Actively seek opportunities to improve processes.
Manage administrative tasks related to this position.
Ensures reporting solution development is documented in accordance with Data and Reporting Team standards.
Qualifications
Knowledge/experience with SQL Server Reporting Services (SSRS).
Hands on experience with PowerBI strongly preferred.
Strong analytic skills.
Strong attention to detail.
Knowledge/experience with SQL queries and Business Intelligence concepts.
Able to build and maintain relationships and facilitate collaboration and communication.
Able to influence without authority.
Knowledge/experience with ETL, data models and Business Intelligence concepts.
Excellent oral and written communication and planning skills.
Able to work independently and collaborate with people at all levels.
Ability to positively engage, self-motivate and possess a strong desire to learn.
Show more
Show less","SQL Server Reporting Services (SSRS), Power BI, Report Creation, ETL, Data Warehouse concepts, Data modeling, Business Intelligence concepts, SQL queries, Communication, Collaboration, Attention to detail, Problem solving, Analytical skills","sql server reporting services ssrs, power bi, report creation, etl, data warehouse concepts, data modeling, business intelligence concepts, sql queries, communication, collaboration, attention to detail, problem solving, analytical skills","analytical skills, attention to detail, business intelligence concepts, collaboration, communication, data warehouse concepts, datamodeling, etl, powerbi, problem solving, report creation, sql queries, sql server reporting services ssrs"
Senior Data Engineer,Resource Data,"Portland, OR",https://www.linkedin.com/jobs/view/senior-data-engineer-at-resource-data-3773545190,2023-12-17,Red Deer, Canada,Mid senior,Hybrid,"As a Senior Data Engineer with Resource Data, you’ll help design and develop data and integration solutions that elevates enterprise systems efficiency, performance, and value. You’ll join a team of technical consultants to help provide solutions to our client’s most challenging enterprise IT problems. You’ll work closely with the client to understand both their business processes and analytics needs to design and build data pipelines and cloud data solutions. You will have the opportunity to guide best practices in data warehousing, data processing, and data management design.
KeyResponsibilities
Work with clients and project teams to create data pipeline architectures, data integrations, and BI solutions such as data visualizations and dashboards.
Identify, design, and implement process improvements for our clients: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading (ETL) of data from a wide variety of data sources using SQL, Azure, and AWS technologies
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics
Basic Qualifications
5+ years' experience in a role involving database development, data modeling relational database design, building data pipelines, data migration, ETL development, etc.
Preferred Qualifications
3+ years' consulting or analytics project delivery through the entire software lifecycle
Experience with online Data Warehouses (Snowflake, Redshift, etc.)
Experience with C#, Java, Python, and R
Experience with custom application development and the Software
Experience in data structures, algorithms, and strong programming skills in Python, R or Java
Show more
Show less","Data modeling, Data pipelines, Data warehousing, ETL, SQL, Azure, AWS, Data visualization, Dashboard, C#, Java, Python, R, Snowflake, Redshift, Software development, Data structures, Algorithms","data modeling, data pipelines, data warehousing, etl, sql, azure, aws, data visualization, dashboard, c, java, python, r, snowflake, redshift, software development, data structures, algorithms","algorithms, aws, azure, c, dashboard, data structures, datamodeling, datapipeline, datawarehouse, etl, java, python, r, redshift, snowflake, software development, sql, visualization"
Data Engineer (Healthcare),VRK IT Vision Inc.,"Bridgewater, NJ",https://www.linkedin.com/jobs/view/data-engineer-healthcare-at-vrk-it-vision-inc-3778585647,2023-12-17,Red Deer, Canada,Mid senior,Hybrid,"Should have
domain knowledge of Genomics, bioinformatics and life sciences, Pharma
Should be able to create S
park , EMR jobs
Should have worked on NF-Core, AirFlow, Nextflow
Should be able to create data management pipeline in Airflow
Should have good working knowledge of AWS (AWS Glue, AWS Omics, AWS Batch, Athena, EMR Serverless)
Show more
Show less","Genomics, Bioinformatics, Life Sciences, Pharma, Spark, EMR, NFCore, AirFlow, Nextflow, Airflow, AWS, AWS Glue, AWS Omics, AWS Batch, Athena, EMR Serverless","genomics, bioinformatics, life sciences, pharma, spark, emr, nfcore, airflow, nextflow, airflow, aws, aws glue, aws omics, aws batch, athena, emr serverless","airflow, athena, aws, aws batch, aws glue, aws omics, bioinformatics, emr, emr serverless, genomics, life sciences, nextflow, nfcore, pharma, spark"
"Data Engineer, Data Platform",Grammarly,"Illinois, United States",https://www.linkedin.com/jobs/view/data-engineer-data-platform-at-grammarly-3656893890,2023-12-17,Red Deer, Canada,Mid senior,Hybrid,"Grammarly is excited to offer a
remote-first hybrid working model
. Team members work primarily remotely in the United States, Canada, Ukraine, Germany, or Poland. Certain roles have specific location requirements to facilitate collaboration at a particular Grammarly hub.
All roles have an in-person component: Conditions permitting, teams meet 2–4 weeks every quarter at one of Grammarly’s hubs in San Francisco, Kyiv, New York, Vancouver, and Berlin, or in a workspace in Kraków.
This flexible approach gives team members the best of both worlds: plenty of focus time along with in-person collaboration that fosters trust and unlocks creativity.
Grammarly team members in this role must be based in the United States or Canada, and they must be able to collaborate in person 2 weeks per quarter, traveling if necessary to the hub(s) where the team is based.
The opportunity
Grammarly is the world’s leading AI writing assistance company trusted by over 30 million people and 70,000 professional teams every day. From instantly creating a first draft to perfecting every message, Grammarly’s product offerings help people at 96% of the Fortune 500 get their point across—and get results. Grammarly has been profitable for over a decade because we’ve stayed true to our values and built an enterprise-grade product that’s secure, reliable, and helps people do their best work—without selling their data. We’re proud to be one of Inc.’s best workplaces, a Glassdoor Best Place to Work, one of TIME’s 100 Most Influential Companies, and one of Fast Company’s Most Innovative Companies in AI.
To achieve our ambitious goals, we’re looking for a Data Engineer to join our Data Engineering Platform team. This person will build highly automated, low latency core datasets that will help data engineers and end users across Grammarly to work with analytical data at scale.
Grammarly’s engineers and researchers have the freedom to innovate and uncover breakthroughs—and, in turn, influence our product roadmap. The complexity of our technical challenges is growing rapidly as we scale our interfaces, algorithms, and infrastructure. Read more about our stack or hear from our team on our technical blog.
Your impact
As a Data Engineer on our Data Engineering Platform team, you will:
Drive improvements to make our analytics effortless by creating and adjusting core data models and storage structures, all while understanding the needs of our users.
Make analytical data and metrics usable within a few minutes of real world events occuring, and build streaming processes for the output derived events and aggregate data.
Model structure, storage, and access of data at very high volumes for our data lakehouse.
Improve developer productivity and self-serve solutions by contributing components to our stream data processing framework(s).
Own data engineering's infrastructure-as-code for provisioning services that allow our engineers to deploy mature software installations within a few hours.
Build a world-class process that will allow our systems to scale.
Mentor other back-end engineers on the team and help them grow.
Build and contribute to AWS high-scale distributed systems on the back-end.
We’re Looking For Someone Who
Embodies our EAGER values—is ethical, adaptable, gritty, empathetic, and remarkable.
Is inspired by our MOVE principles, which are the blueprint for how things get done at Grammarly: move fast and learn faster, obsess about creating customer value, value impact over activity, and embrace healthy disagreement rooted in trust.
Is able to collaborate in person 2 weeks per quarter, traveling if necessary to the hub where the team is based.
Has experience with Python, Scala, or Java.
Has experience with designing database objects and writing relational queries
Has experience designing and standing up APIs and services.
Has experience with system design and building internal tools.
Has experience handling applications that work with data from data lakes.
Has at least some experience building internal Admin sites.
Has good knowledge of and at least some experience with AWS (or, alternatively, has deep expertise in Azure or GCE and is willing to learn AWS in a short time frame).
Can knowledgeably choose an open source or third-party service to accomplish what they need or, alternatively, can devise a quick and simple solution on their own.
Support for you, professionally and personally
Professional growth: We believe that autonomy and trust are key to empowering our team members to do their best, most innovative work in a way that aligns with their interests, talents, and well-being. We support professional development and advancement with training, coaching, and regular feedback.
A connected team: Grammarly builds a product that helps people connect, and we apply this mindset to our own team. Our remote-first hybrid model enables a highly collaborative culture supported by our EAGER (ethical, adaptable, gritty, empathetic, and remarkable) values. We work to foster belonging among team members in a variety of ways. This includes our employee resource groups, Grammarly Circles, which promote connection among those with shared identities, such as BIPOC and LGBTQIA+ team members, women, and parents. We also celebrate our colleagues and accomplishments with global, local, and team-specific programs.
Compensation And Benefits
Grammarly offers all team members competitive pay along with a benefits package encompassing the following and more:
Excellent health care (including a wide range of medical, dental, vision, mental health, and fertility benefits)
Disability and life insurance options
401(k) and RRSP matching
Paid parental leave
Twenty days of paid time off per year, eleven days of paid holidays per year, and unlimited sick days
Home office stipends
Caregiver and pet care stipends
Wellness stipends
Admission discounts
Learning and development opportunities
Grammarly takes a market-based approach to compensation, which means base pay may vary depending on your location. Our US and Canada locations are categorized into compensation zones based on each geographic region’s cost of labor index. For more information about our compensation zones and locations where we currently support employment, please refer to this page. If a location of interest is not listed, please speak with a recruiter for additional information.
Base pay may vary considerably depending on job-related knowledge, skills, and experience. The expected salary ranges for this position are outlined below by compensation zone and may be modified in the future.
United States
Zone 1: $167,000 - $242,000/year (USD)
Zone 2: $150,000 – $218,000/year (USD)
Zone 3: $142,000 – $206,000/year (USD)
Zone 4: $134,000 – $194,000/year (USD)
We encourage you to apply
At Grammarly, we value our differences, and we encourage all—especially those whose identities are traditionally underrepresented in tech organizations—to apply. We do not discriminate on the basis of race, religion, color, gender expression or identity, sexual orientation, ancestry, national origin, citizenship, age, marital status, veteran status, disability status, political belief, or any other characteristic protected by law. Grammarly is an equal opportunity employer and a participant in the US federal E-Verify program (US). We also abide by the Employment Equity Act (Canada).
Please note that EEOC is optional and specific to US-based candidates.
#NA
All team members meeting in person for official Grammarly business or working from a hub location are strongly encouraged to be vaccinated against COVID-19.
Show more
Show less","Data Engineering, Python, Scala, Java, Database Design, Relational Queries, API Design, System Design, Internal Tools, Data Lakes, Internal Admin Sites, AWS, Azure, GCE, Open Source, ThirdParty Services, Professional Development, Coaching, Feedback, Employee Resource Groups, Paid Parental Leave, Paid Time Off, Paid Holidays, Unlimited Sick Days, Home Office Stipends, Caregiver Stipends, Pet Care Stipends, Wellness Stipends, Admission Discounts, Learning and Development Opportunities","data engineering, python, scala, java, database design, relational queries, api design, system design, internal tools, data lakes, internal admin sites, aws, azure, gce, open source, thirdparty services, professional development, coaching, feedback, employee resource groups, paid parental leave, paid time off, paid holidays, unlimited sick days, home office stipends, caregiver stipends, pet care stipends, wellness stipends, admission discounts, learning and development opportunities","admission discounts, api design, aws, azure, caregiver stipends, coaching, data engineering, data lakes, database design, employee resource groups, feedback, gce, home office stipends, internal admin sites, internal tools, java, learning and development opportunities, open source, paid holidays, paid parental leave, paid time off, pet care stipends, professional development, python, relational queries, scala, system design, thirdparty services, unlimited sick days, wellness stipends"
Founding Data Engineer,Parker,"New York, NY",https://www.linkedin.com/jobs/view/founding-data-engineer-at-parker-3625993018,2023-12-17,Red Deer, Canada,Mid senior,Hybrid,"Parker's mission is to increase the number of financially independent people. We believe we can achieve this goal by building tools that enable independent business owners to scale their businesses profitably. Our first product combines a virtual credit card system with dynamic spending limits and software tooling to help merchants grow and optimize their profitability.
We are growing very fast -- in less than five months, we grew to millions in card volume. We have a significant waitlist of customers waiting to use our product. We are looking to expand our headcount quickly to support the demand. Our investors include Solomon Hykes (founder of Docker), Paul Buchheit (founder of Gmail), Paul Graham (founder of Y Combinator), Robert Leshner (founder of compound.finance), and many more. We have raised over $30M from top-tier fintech investors.
What you'll be working on
You'll be involved in projects of varying scope and complexity: * Build credit risk models that segment merchants based on their revenue and spend patterns to offer dynamic credit limits that change with business performance given the high seasonality and fast pace of ecommerce * Use machine learning tools to build realtime credit underwriting models leveraging alternative and traditional data sources
What You'll Need
Passion for, or curiosity to learn, financial technology
Track record of high-quality shipping products and features at scale
Ability to turn business and product ideas into engineering solutions
Desire to work in a fast-paced environment, continuously grow and master your craft
What we’d like to see
Experience working with credit risk modeling
Experience working with data analytics, algorithmic decision making, and real-time data systems
Proven experience and subject matter expertise in e-commerce payments (nice to have) or financial services
Show more
Show less","Financial Technology, Credit Risk Modeling, Data Analytics, Algorithmic Decision Making, RealTime Data Systems, Ecommerce Payments, Financial Services","financial technology, credit risk modeling, data analytics, algorithmic decision making, realtime data systems, ecommerce payments, financial services","algorithmic decision making, credit risk modeling, dataanalytics, ecommerce payments, financial services, financial technology, realtime data systems"
"GCP Data Engineer || Dallas, TX",Steneral Consulting,"Dallas, TX",https://www.linkedin.com/jobs/view/gcp-data-engineer-dallas-tx-at-steneral-consulting-3710632765,2023-12-17,Red Deer, Canada,Mid senior,Hybrid,"GCP Data Engineer (must have 7 years with google cloud experience)
Dallas, TX - Hybrid / Onsite Position: Hybrid (2-3 days/week at the office) - Local candidates only under 40-50 Min commute of Dallas or Phoenix or Pleasanton
Specific Location of the resource: Dallas TX, but open to Pleasanton CA and Phoenix
Required Qualifications
7+ years proven experience in developing and deploying data pipelines in GCP or Azure.
5+ years of Snowflake, BigQuery and/or Databricks experience
5+ years proven experience in building frameworks for data ingestion, processing, and consumption using GCP Data Flow, GCP Data Composer, Big Query.
4+ years of strong experience with SQL, Python, Java, API development
2+ years of proven expertise in creating real-time pipelines using Kafka, Pub/sub.gcp
Building high quality data pipelines with monitoring and observability
2+ years of experience building dashboards and reports with PowerBI and/or Thoughtspot
Preferred Qualifications
Extensive experience in data transformations for Retail and e-commerce business use cases will be a plus
Bachelor's or Master's in computer engineering, computer science or related area.
Knowledge of Github Actions for CICD
Knowledge of building machine learning models
Mandatory Certifications Required: GCP Professional Architect or GCP Data Engineer
Show more
Show less","Python, Java, Kafka, Azure, Snowflake, BigQuery, Databricks, GCP, SQL, GCP Data Flow, ThoughtSpot, Power BI, Pub/Sub, Machine Learning, GitHub Actions, Data Engineering, Data Pipeline Development, Data Processing, Data Ingestion, Data Consumption, Data Visualization, Data Transformations, Cloud Computing, Data Architecture, Data Governance, Data Quality, Data Security","python, java, kafka, azure, snowflake, bigquery, databricks, gcp, sql, gcp data flow, thoughtspot, power bi, pubsub, machine learning, github actions, data engineering, data pipeline development, data processing, data ingestion, data consumption, data visualization, data transformations, cloud computing, data architecture, data governance, data quality, data security","azure, bigquery, cloud computing, data architecture, data consumption, data engineering, data governance, data ingestion, data pipeline development, data processing, data quality, data security, data transformations, databricks, gcp, gcp data flow, github actions, java, kafka, machine learning, powerbi, pubsub, python, snowflake, sql, thoughtspot, visualization"
Senior Data Engineer,CENTSTONE SERVICES LLC,"Brooklyn, NY",https://www.linkedin.com/jobs/view/senior-data-engineer-at-centstone-services-llc-3783598093,2023-12-17,Red Deer, Canada,Mid senior,Hybrid,"Job Title: Senior Data Engineer
Location: Brooklyn, NY
Duration: 12+ Months contract, possible extension
Contract: Only W2
Job Description
Please Note:
As of July 22, 2021, our team will require that all candidate submissions include a LinkedIn profile. Please do not submit any candidates that do not have a LinkedIn
The Data Engineer will be focused on migrating data pipelines from legacy infrastructure and frameworks to more modern infrastructure.
The ideal candidate is a Data Engineer with considerable experience in migrations and Big Data frameworks.
Primary Skills
Scala programming language expertise
Spark framework expertise
Experience working with BigQuery
Familiarity scheduling jobs in Airflow
Fluency with Google Cloud Platform, in particular GCS and Dataproc
Thanks & Regards,
Maniteja – Recruitment Lead
CENTSTONE SERVICES
Direct Contact: +1 332-456-5874
Email
: mani.teja@centstone.com
Show more
Show less","Scala, Spark, BigQuery, Airflow, Google Cloud Platform, GCS, MapReduce, Data Pipeline, Hadoop","scala, spark, bigquery, airflow, google cloud platform, gcs, mapreduce, data pipeline, hadoop","airflow, bigquery, data pipeline, gcs, google cloud platform, hadoop, mapreduce, scala, spark"
Azure Database Engineer,Extend Information Systems Inc.,"St Louis, MO",https://www.linkedin.com/jobs/view/azure-database-engineer-at-extend-information-systems-inc-3700453387,2023-12-17,Red Deer, Canada,Mid senior,Hybrid,"Job Title: Azure Database Engineer
Location: St.Louis, MO (Day-1 Onsite Hybrid)
Duration: Contract
Experience level : 9+ yrs
Responsibilities
Collaborate with data architects and other stakeholders to design scalable and optimized data warehouse solutions on the Azure platform.
Develop and maintain data warehouse architectural documentation, including schema designs, data flow diagrams, and integration patterns.
Move SSIS / ETL Processes and execute SSIS within Azure Data Factory.
Design the NextGen Data Warehouse using Azure.
Expertise in using Master Data Management strategies, design Data Warehouse structure and schema with Azure Data services..
Execute SSIS packages in Azure from SSDT to assess the cloud compatibility of SSIS packages and run them on Azure-SSIS IR within ADF.
Fix any issues found in IR testing and can migrate all SSIS packages to run in Azure.
Re-write all SSIS packages into ADF and expand analytics using Azure Synapse and Power BI
Investigate use of Azure Fabric for end to end data processes from Data Science to Analytics
Write efficient T-SQL queries for data transformation, extraction, loading, and reporting purposes.
Implement data partitioning, indexing, and distribution strategies to enhance query performance.
Integrate data from various sources, both on-premises and in the cloud, into the Azure Data Warehouse using appropriate ETL/ELT techniques.
Work with data integration tools such as Azure Data Factory or SSIS to automate data pipelines.
Monitor and analyze query performance and data warehouse health, identifying and resolving bottlenecks and performance issues.
Optimize query execution plans, indexing strategies, and data distribution to achieve optimal performance.
Implement security measures to protect sensitive data within the Azure Data Warehouse, following best practices for data encryption, access controls, and data masking.
Ensure compliance with relevant data privacy and security regulations, such as GDPR or HIPAA.
Provide training and guidance to junior team members on data warehouse best practices and technologies.
Qualifications
Bachelor's or higher degree in Computer Science, Information Technology, or a related field.
Proven experience in designing, implementing, and optimizing data warehousing solutions using Azure services (Azure Synapse Analytics, Azure Data Factory, Azure SQL Data Warehouse).
Plan the ETL process for the data warehouse design.
Choose appropriate Azure data services (Synapse, ADF, ?)
Create data warehouse operations runbook
Manage Data Quality ongoing.
Thanks & Regards
Anoop Tiwari
Extend Information Systems
Cell: - 571 - 386 - 2431
Email:
Anoop@extendinfosys.com
Show more
Show less","Azure, Azure Synapse Analytics, Azure Data Factory, Azure SQL Data Warehouse, SSIS, ETL, TSQL, Data Warehousing, Data Integration, Data Modeling, Data Security, Data Governance, Data Privacy, GDPR, HIPAA, Computer Science, Information Technology","azure, azure synapse analytics, azure data factory, azure sql data warehouse, ssis, etl, tsql, data warehousing, data integration, data modeling, data security, data governance, data privacy, gdpr, hipaa, computer science, information technology","azure, azure data factory, azure sql data warehouse, azure synapse analytics, computer science, data governance, data integration, data privacy, data security, datamodeling, datawarehouse, etl, gdpr, hipaa, information technology, ssis, tsql"
AWS Data Engineer,TMS,"Florence, NJ",https://www.linkedin.com/jobs/view/aws-data-engineer-at-tms-3742841796,2023-12-17,Red Deer, Canada,Mid senior,Hybrid,"Job Title : AWS Data Engineer
Location : Newark, NJ ( 1 Day in a week is must)
Duration : Long Term.
Job Description
Designing solutions using AWS services.
Building proof of concepts for new tools, services.
Working, coordinating with, and guiding offshore team.
Skills \ Competencies we are looking for -
Overall experience 8-12 years
3+ years of Healthcare Payer experience within last 4 years. (Must)
ETL experience.
Experience with building Data Ingestion, Pipelines for high volume data.
In depth experience with AWS services like Redshift, Glue, Athena, Pyspark, EMR, S3
Python experience.
Advanced SQL experience.
Familiarity with Dev Ops.
Show more
Show less","AWS, Redshift, Glue, Athena, PySpark, EMR, S3, Python, SQL, ETL, Data Ingestion, Pipelines, Dev Ops, Healthcare Payer","aws, redshift, glue, athena, pyspark, emr, s3, python, sql, etl, data ingestion, pipelines, dev ops, healthcare payer","athena, aws, data ingestion, dev ops, emr, etl, glue, healthcare payer, pipelines, python, redshift, s3, spark, sql"
AWS Data Engineer,Extend Information Systems Inc.,"Reston, VA",https://www.linkedin.com/jobs/view/aws-data-engineer-at-extend-information-systems-inc-3752604102,2023-12-17,Red Deer, Canada,Mid senior,Hybrid,"Job Title: AWS data engineer
Location:
Reston, VA( Once in a Month)
Duration: C2C/FTE
Experience level : 10+ yrs
Job Description
Job Description:
Please Submit only 10+yrs Profile
And try for AWS Certified Profile(Good to Have).
Essential Skills
Strong Python development experience with Python, Pandas , NumPy, Pyspark
Strong knowledge on AWS services such as (S3, RDS, EC2, Lambda, SQS, SNS, Redshift)
Having prior working experience in Fannie Mae will be added advantage
Good Knowledge on Java and Database (Oracle Postgres)
Essential Qualification
Bachelors is a must
Thanks & Regards !!
Anoop Tiwari
Extend Information Systems
Cell: -
Email: Anoop@extendinfosys.com
Show more
Show less","Python, Pandas, NumPy, Pyspark, AWS, S3, RDS, EC2, Lambda, SQS, SNS, Redshift, Java, Oracle, Postgres","python, pandas, numpy, pyspark, aws, s3, rds, ec2, lambda, sqs, sns, redshift, java, oracle, postgres","aws, ec2, java, lambda, numpy, oracle, pandas, postgres, python, rds, redshift, s3, sns, spark, sqs"
Database Engineer,Extend Information Systems Inc.,"McLean, VA",https://www.linkedin.com/jobs/view/database-engineer-at-extend-information-systems-inc-3655945719,2023-12-17,Red Deer, Canada,Mid senior,Hybrid,"Hi
Jobseekers
,
I hope you are doing well!
We have an opportunity
Database Engineer
with one of our clients for
Mclean VA(day one onsite, 3 days onsite in a week).
Please see the job details below and let me know if you would be interested in this role.
If interested, please send me a copy of your resume, contact details, availability, and a good time to connect with you.
Only local consultant
Job Title:
Database Engineer
Location: Mclean VA(day one onsite, 3 days onsite in a week)
Duration: Full Time/Long Term
Experience level :
8+
Job Description
Manage SQL Server databases
Configure and maintain database servers and processes
Monitor system's health and performance
Ensure high levels of performance, availability, sustainability and security
Analyze, solve, and correct issues in real time
Provide suggestions for solutions
Refine and automate regular processes, track issues, and document changes
Assist developers with query tuning and schema refinement
Perform scheduled maintenance and support release deployment activities after hours
Qualifications
5 years of experience as a SQL Server DBA/Sybase/DB2 or similar role
4 years of experience with SQL Server DBA/Sybase/DB2 Administration experience required
Critical thinker and problem-solving skills
Team player
Good time-management skills
Great interpersonal and communication skills
Thanks & Regards!!
Anoop Tiwari
Extend Information Systems
Cell:
- 571 - 386 - 2431
Email:
Anoop@extendinfosys.com
Address: 44258 Mercure Circle, UNIT 102 A, Sterling VA, USA 20166
Web: www.extendinfosys.com
Show more
Show less","SQL Server, Database Administration, SQL, Sybase, DB2, Performance Analysis, Critical Thinking, Problem Solving, Time Management, Interpersonal Skills, Communication Skills","sql server, database administration, sql, sybase, db2, performance analysis, critical thinking, problem solving, time management, interpersonal skills, communication skills","communication skills, critical thinking, database administration, db2, interpersonal skills, performance analysis, problem solving, sql, sql server, sybase, time management"
Senior Software Engineer (Data Analytics / Big Data Engineer ),PubMatic,"Redwood City, CA",https://www.linkedin.com/jobs/view/senior-software-engineer-data-analytics-big-data-engineer-at-pubmatic-3687493936,2023-12-17,Red Deer, Canada,Mid senior,Hybrid,"Company Description
PubMatic (Nasdaq: PUBM) is an independent technology company maximizing customer value by delivering digital advertising’s supply chain of the future.
PubMatic’s sell-side platform empowers the world’s leading digital content creators across the open internet to control access to their inventory and increase monetization by enabling marketers to drive return on investment and reach addressable audiences across ad formats and devices.
Since 2006, our infrastructure-driven approach has allowed for the efficient processing and utilization of data in real time. By delivering scalable and flexible programmatic innovation, we improve outcomes for our customers while championing a vibrant and transparent digital advertising supply chain.
Job Description
Our Advertising technology team is building a holistic advertising platform which will be key to e-commerce company’s ad revenue growth strategy. We will build cutting edge machine learning and optimization algorithms to ingest, model and analyze online and in-store data from e-commerce companies. Importantly, we will build smart systems that deliver relevant retail ads and experiences that connect customers with the brands and products they love and enable advertisers to achieve their product sales goals via campaign activation.
The
Senior Software Engineer
will be responsible for developing end-to-end product features for reporting system for advertising platform for e-commerce clients, that would have big data processing and will include the design and development of Analytics components.
We are looking for self-motivated senior software engineers who enjoy working in dynamic, agile development environments as individual contributors to the team. The ideal candidate is a self-motivated problem solver with a strong background in big data tech stack, software design, development, and e-commerce AdTech domain.
Job Location: US (RWC/NYC)
Responsibilities
Build, design and implement our highly scalable, fault-tolerant, highly available big data platform to process terabytes of data and provide customers with in-depth analytics.
Developing Big Data pipelines using modern technology stack such as Spark, Hadoop, Kafka, HBase, Hive, Presto etc.
Developing analytics application ground up using modern technology stack such as Java, Spring, Tomcat, Jenkins, REST APIs, JDBC, Amazon Web Services, Hibernate.
Building data pipeline to automate high-volume data collection and processing to provide real-time data analytics.
Customize PubMatic’s reporting and analytics platform based on customer’s requirements from customers and deliver scalable, production-ready solutions.
Lead multiple projects to develop features for data processing and reporting platform, collaborate with product managers, cross-functional teams, other stakeholders and ensure successful delivery of projects.
Use various mechanisms established to fetch data from different external data sources and reconcile them with PubMatic’s processed data.
Collaborate with functional teams to build products to deliver end-to-end products and features and fix bugs for better performance.
Develop robust & fault-tolerant systems and monitor implications of changes on data processing pipeline and performance.
Leveraging a broad range of PubMatic’s data architecture strategies and proposing both data flows and storage solutions.
Managing hadoop map reduce and spark jobs & solving any ongoing issues with operating the cluster.
Working closely with cross functional teams on improving availability and scalability of large data platform and functionality of PubMatic software.
Expertise in developing Implementation of professional software engineering best practices for the full software development life cycle, including coding standards, performing code reviews, committing to Github, preparing documents in Confluence, continuous delivery using Jenkins, automated testing, and operations.
Participate in Agile/Scrum processes such as sprint planning, sprint retrospective, backlog grooming, user story management, work item prioritization, etc.
Frequently discuss with product managers about the software features to include in PubMatic Data Analytics platform. Understand the technical aspects customer requirement from product managers.
Keep in regular touch with quality engineering team which ensure the quality of the platforms/products and performance SLAs of java based micro services and spark-based data pipeline.
Support customer issues over email or JIRA (bug tracking system), provide updates, patches to customers to fix the issues.
Discuss with technical writing team about the technical documents that are published on documentation portal.
Perform code and design reviews for code implemented by peers or as per the code review process.
Qualifications
3+ years coding experience in Java.
Experience in implementing closed loop reporting for sponsored product listing campaigns for e-commerce companies.
Building analytics pipes for product search relevance modeling and click prediction
Experience in implementing reporting for display/video/native Ad campaigns for e-commerce companies.
Solid computer science fundamentals including data structure and algorithm design, and creation of architectural specifications.
Expertise in developing Implementation of professional software engineering best practices for the full software development life cycle, including coding standards, code reviews, source control management, documentation, build processes, automated testing, and operations.
A passion for developing and maintaining a high-quality code and test base and enabling contributions from engineers across the team.
Expertise in big data technologies like Hadoop, Spark, Kafka, HBase etc would be an added advantage.
Experience in developing and delivering large-scale big-data pipelines, real-time systems & data warehouses would be preferred.
Demonstrated ability to achieve stretch goals in a very innovative and fast paced environment.
Demonstrated ability to learn new technologies quickly and independently.
Excellent verbal and written communication skills, especially in technical communications.
Strong interpersonal skills and a desire to work collaboratively.
Base Compensation Range: $130,000 - $160,000
In accordance with applicable law, the above salary range provided is PubMatic’s reasonable estimate of the base salary for this role. The actual amount may vary, based on non-discriminatory factors such as location, experience, knowledge, skills and abilities. In addition to salary PubMatic also offers a bonus, restricted stock units and a competitive benefits package.
Additional Information
Return to Office
: PubMatic employees throughout the global have returned to our offices via a hybrid work schedule (3 days “in office” and 2 days “working remotely”) that is intended to maximize collaboration, innovation, and productivity among teams and across functions. All PubMatic employees in the US and India are required to be fully vaccinated to return to our offices. Covid-19 boosters are not required at this point in time.
Benefits
: Our benefits package includes the best of what leading organizations provide such as, paid leave programs, paid holidays, healthcare, dental and vision insurance, disability and life insurance, commuter benefits, physical and financial wellness programs, unlimited DTO in the US (that we actually require you to use!), reimbursement for mobile and internet expenses and fully stocked pantries plus in-office catered lunches 3 days per week.
Diversity and Inclusion
: PubMatic is proud to be an equal opportunity employer; we don’t just value diversity, we promote and celebrate it. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.
Show more
Show less","Java, Spring, Tomcat, Jenkins, REST APIs, JDBC, Amazon Web Services, Hibernate, Hadoop, Spark, Kafka, HBase, Hive, Presto, Data structure, Algorithm design, Architectural specifications, Agile, Scrum, Confluence, Continuous delivery, Automated testing, User story management, Work item prioritization, Jira, SQL, Git, Confluence","java, spring, tomcat, jenkins, rest apis, jdbc, amazon web services, hibernate, hadoop, spark, kafka, hbase, hive, presto, data structure, algorithm design, architectural specifications, agile, scrum, confluence, continuous delivery, automated testing, user story management, work item prioritization, jira, sql, git, confluence","agile, algorithm design, amazon web services, architectural specifications, automated testing, confluence, continuous delivery, data structure, git, hadoop, hbase, hibernate, hive, java, jdbc, jenkins, jira, kafka, presto, rest apis, scrum, spark, spring, sql, tomcat, user story management, work item prioritization"
AWS Database Engineer,Steneral Consulting,"Elmhurst, IL",https://www.linkedin.com/jobs/view/aws-database-engineer-at-steneral-consulting-3747467635,2023-12-17,Red Deer, Canada,Mid senior,Hybrid,"Share only 2 profiles
Hybrid in Elmhurst, IL
local candidates only
Duration: 6+ Months C2H
Our client is a global organization, named one of the largest in the Automobile industry! They are rapidly growing and going through a robust digital transformation. The business and senior leadership have put a huge emphasis on technology initiatives and are looking for passionate team members to contribute to the growth and development of the technical environment.
Interested in being apart of a dynamic data engineering and analytics team? This role offers a unique opportunity to engage with cutting-edge technologies in the realms of data engineering and cloud computing. You will have the chance to craft high-performance, scalable data pipelines while upholding industry-leading standards in data management, data quality, and automation. You will join at a pivotal time for the organization as they are in the midst of a global AWS migration. You will collaborate with the entire data organization, providing support to legacy systems while also adapting to the new cloud platform. If you are passionate about data and eager to leverage it for meaningful insights that drive business value, then we encourage you to join this team!
Required Experience;
6+ years experience working on a enterprise data engineering team (experience building/enhancing data lakes and big data technologies).
4+ years of database experience (relational and non-relational/NoSQL databases).
Strong SQL experience (Must write complex queries from scratch).
Strong scripting experience with ability to build complex data pipelines in python.
Experience with SQL Server (SSIS, SSRS, TSQL, C#).
At least 3 years AWS experience, Lambda required (AWS IAM, S3, API Gateway, Glue, Lake Formation, Redshift, and RDS).
Must have experience with Apache Airflow.
Foundational expertise in IaC (terraform and CloudFormation).
A strong understanding of data concepts related to data warehousing, business intelligence (BI), data security, data quality, and data profiling.
Knowledge of ETL tools (Talend Open Studio, Informatica, Pentaho).
If you are familiar with any of the technologies below you are a star!
Kubernetes
DevOps (Gitlab, Bitbucket)
Spark
NoSQL database
DynamoDB
MongoDB
EMR
Presto
Show more
Show less","Data Engineering, Cloud Computing, AWS, AWS IAM, AWS S3, AWS API Gateway, AWS Glue, AWS Lake Formation, AWS Redshift, AWS RDS, Apache Airflow, Terraform, CloudFormation, Data Warehousing, Business Intelligence (BI), Data Security, Data Quality, Data Profiling, ETL Tools, Talend Open Studio, Informatica, Pentaho, Kubernetes, DevOps, Gitlab, Bitbucket, Spark, NoSQL Database, DynamoDB, MongoDB, EMR, Presto","data engineering, cloud computing, aws, aws iam, aws s3, aws api gateway, aws glue, aws lake formation, aws redshift, aws rds, apache airflow, terraform, cloudformation, data warehousing, business intelligence bi, data security, data quality, data profiling, etl tools, talend open studio, informatica, pentaho, kubernetes, devops, gitlab, bitbucket, spark, nosql database, dynamodb, mongodb, emr, presto","apache airflow, aws, aws api gateway, aws glue, aws iam, aws lake formation, aws rds, aws redshift, aws s3, bitbucket, business intelligence bi, cloud computing, cloudformation, data engineering, data profiling, data quality, data security, datawarehouse, devops, dynamodb, emr, etl tools, gitlab, informatica, kubernetes, mongodb, nosql database, pentaho, presto, spark, talend open studio, terraform"
Senior Data Engineer,Odyssey Information Services,Dallas-Fort Worth Metroplex,https://www.linkedin.com/jobs/view/senior-data-engineer-at-odyssey-information-services-3674007949,2023-12-17,Red Deer, Canada,Mid senior,Hybrid,"No sponsorship at this time.
Must be willing to work one day onsite.
Must have:
SQL, SSIS, and healthcare data experience.
Nice to have:
ADF
Summary:
The Data Engineer Senior is an experienced data professional who analyzes, architects and engineers’ data pipelines and data integration activities to power data products and support the overall Knowledge Management and Data Operations.
This role is responsible for data engineering activities including data pipeline design, data ingestion, data integration, data cleansing, data mapping, data store architecture, data transformation, tuning, testing, and data quality. This role actively drives the expansion of the data platform through data flow development, workload management, data architecture/structures/schemas, orchestration, and metadata/lineage. The Data Engineer Senior partners with Business Intelligence developers and analysts to design robust, scalable data pipelines and data architectures that power reports, dashboards, integrations, and advanced analytics products/capabilities while enabling rapid experimentation. This position interfaces with business customers and bridges the gap between business and technology solutions throughout the data product lifecycle to ensure that technical solutions meet the needs of the business.
The Data Engineer Senior has strong expertise with SQL, data modeling (relational and/or data warehouse context), and ETL/ETL development. They have familiarity with modern cloud-based data platforms, cloud data integration technologies, business intelligence tools and programming language.
Show more
Show less","SQL, SSIS, Healthcare data, ADF, Data engineering, Data analysis, Data architecture, Data integration, Data cleansing, Data mapping, Data store architecture, Data transformation, Data tuning, Data testing, Data quality, Data flow development, Workload management, Data structures, Data schemas, Orchestration, Metadata, Lineage, Business intelligence, Analytics, Cloudbased data platforms, Cloud data integration, Programming languages, ETL/ELT development","sql, ssis, healthcare data, adf, data engineering, data analysis, data architecture, data integration, data cleansing, data mapping, data store architecture, data transformation, data tuning, data testing, data quality, data flow development, workload management, data structures, data schemas, orchestration, metadata, lineage, business intelligence, analytics, cloudbased data platforms, cloud data integration, programming languages, etlelt development","adf, analytics, business intelligence, cloud data integration, cloudbased data platforms, data architecture, data engineering, data flow development, data integration, data mapping, data quality, data schemas, data store architecture, data structures, data testing, data transformation, data tuning, dataanalytics, datacleaning, etlelt development, healthcare data, lineage, metadata, orchestration, programming languages, sql, ssis, workload management"
Data Engineer-locals,Steneral Consulting,"Chicago, IL",https://www.linkedin.com/jobs/view/data-engineer-locals-at-steneral-consulting-3745369544,2023-12-17,Red Deer, Canada,Mid senior,Hybrid,"Title:
Data Engineer
Location:
Chicago IL (3 days onsite)
Duration:
6 Months
Status:
Must be USC or GC
Description
As a member of Data Engineering teams, this role will develop the enterprise data platform in Azure cloud. It will entail loading of the raw data from SQL and Mainframe data sources and processing them by developing data transformation pipelines using Azure technologies like Terraform, Data Factory, Data Bricks, Azure Functions, PySpark/Python, SQL etc.
Top Three
Experience in Data, ETL processing, SQL programming, and exposure to Azure cloud
Programming experience using Python or any object oriented language like Java, C# etc.
Ability to learn the Azure tech stack mentioned above and get certified when on the job
Additional
A Data Engineer will build and operate our enterprise data platform, prepare and deliver curated datasets for operational and analytical use cases. DE will design and provide analytical datasets to citizen data analysts, prepare training data and extract features to accelerate data science model development.
In a complex enterprise data environment with numerous systems of record and systems of engagement, the Data Engineer will ensure that the Data Engineering services are delivered in a scalable, reliable, secure, and maintainable manner to support the service levels. The Data Engineer works closely with the Data Architects, System Architects, and Application Architects to align with the Architectural direction of the enterprise and will be required to understand the Architectural landscape and possess the ability to gather and analyze business requirements. In addition, the Data Engineer promotes data sharing across the enterprise while maintaining the accuracy, consistency, integrity, and security of the data sets in the Enterprise Data Repository.
Essential Job Duties & Responsibilities
Liberate data from source data sources into the enterprise data lake and operational data stores
Map data from the source systems into curated entities and their functional views
Implement master data management strategies
Design, implement and deploy data pipeline solutions with data lineage capabilities using Azure Data Factory and Databricks.
Manage and publish curated data sets.
Work with the business to manage data quality.
Monitor availability, performance, capacity, continuity, security, and service levels of the enterprise data platform and its services
Provide subject matter expertise to other technical teams leveraging services from the enterprise data platform
Delivering and providing production support for Data Engineering services that meet performance standards and service level agreements.
Participating in continually improving processes and procedures for enhancing the efficiency and effectiveness of Data Engineering services to analytic users.
Participating in leveraging emerging innovations related to Data Engineering and work closely with Enterprise Architecture and other teams to operationalize them
Supporting the Data Engineering department in conducting User Groups and other forums as needed to evangelize and promote the adoption of Data Engineering best practices for accessing and using data sets efficiently, effectively, and securely
Supporting enterprise initiatives in building a culture of data-driven decisions to contribute to operational excellence.
Participating in the development of Data Engineering practices focusing on long-term sustainability, reuse, and technical debt reduction in the domain of data management.
Works with stakeholders including the Executive, Product, Data, and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Publishes dashboards that summarize the utilization of data sets in delivering business value impact
Other Duties
Supports enterprise architecture function in establishing Policies, Standards, Architecture Patterns, Guidelines, and Best Practices related to Data Engineering discipline. Designs, implements and identifies internal process improvements: automating manual processes, optimizing data delivery, redesigning infrastructure for greater scalability,
Supports the Enterprise Data Repository operations as needed in non-production environments
Develops tools to automate Enterprise Data Repository operations as needed and liaises with the Service Operations teams that manage production environments
Other duties as assigned.
Knowledge, Skills & Abilities
Expertise in programming, debugging, and testing utilizing various programming languages, tools and technologies related to Data Engineering.
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience working with various operating systems, RDB platforms, NoSQL platforms, Big Data platforms, and environments for managing Data over its full lifecycle.
Strong knowledge of how to compose and implement structural data models.
Strong knowledge of using data as an enterprise asset – building and managing data marts and data warehouses
Proven Experience in managing structured data (application data bases, operational data stores, data marts and data warehouses).
Experience in managing structured data, unstructured data, and big data at scale on cloud platforms, preferably on Microsoft Azure
Experience in managing large-scale storage solutions, preferably using Microsoft Azure.
Experience molding fresh environments into cost-efficient, high performance, secure, and mature data platforms.
Understanding of data security concepts including encryption at rest and in transit
Familiarity with data visualization techniques and tools.
Flexible with a strong sense of urgency.
Ability to effectively communicate technical issues both verbally and in writing.
Ability to work in an Agile Scrum and Kanban environment.
Excellent analytical and problem-solving skills.
Experience in successfully managing large-scale data ecosystems preferred.
Willingness to use of new software aids and programming techniques that are adopted within IT as needed for the role.
Education And Experience Required
Bachelor’s degree in Computer Science or equivalent education.
5-7 years of experience in information technology systems or data services delivery
3+ years of overall experience in designing and delivering Data Engineering services using programming/scripting languages such as Python, Scala, R, SQL, C#, Java, or U-SQL.
1+ year(s) experience in delivering Data Engineering services using a NoSQL platform such as Cosmos DB, Mongo DB, Hadoop, CouchBase, or HDInsights
Experience in using ETL/ELT, Data Quality Management, Meta Data Management, and Master Data Management platforms is highly preferred
Azure Data Engineer certification strongly preferred
Experience with Microsoft Azure Data Management Platform is strongly preferred
Familiarity with the financial services and/or life insurance industry is preferred.
Show more
Show less","Azure, Terraform, Data Factory, Data Bricks, Azure Functions, PySpark, Python, SQL, Java, C#, ETL, SQL programming, Data lineage, Data pipeline, Data management, Data lake, Data warehouse, Data modeling, Data security, Data visualization, Agile Scrum, Kanban, Hadoop, Couchbase, HDInsights, ETL/ELT, Data Quality Management, Meta Data Management, Master Data Management, Cosmos DB, Mongo DB","azure, terraform, data factory, data bricks, azure functions, pyspark, python, sql, java, c, etl, sql programming, data lineage, data pipeline, data management, data lake, data warehouse, data modeling, data security, data visualization, agile scrum, kanban, hadoop, couchbase, hdinsights, etlelt, data quality management, meta data management, master data management, cosmos db, mongo db","agile scrum, azure, azure functions, c, cosmos db, couchbase, data bricks, data factory, data lake, data lineage, data management, data pipeline, data quality management, data security, datamodeling, datawarehouse, etl, etlelt, hadoop, hdinsights, java, kanban, master data management, meta data management, mongo db, python, spark, sql, terraform, visualization"
IT Data Engineer | 23-00961,ESPO Corporation,"Lake Forest, IL",https://www.linkedin.com/jobs/view/it-data-engineer-23-00961-at-espo-corporation-3319254144,2023-12-17,Red Deer, Canada,Mid senior,Hybrid,"Job Title: IT Data Engineer
Location: Lake Forest, IL (Hybrid 3 days in office - Tues thru Thurs, Remote Mon & Fri)
Job Type and Duration: Permanent salaried role, full time
Global consumer goods manufacturer looking to add an IT Data Engineer to their team in their corporate offices - hybrid role! Great team culture and bonuses!
The position reports to the Director of Data Warehouse and Data Lake
Responsibilities:
Build data pipelines:
Managed data pipelines consist of a series of stages through which data flows (for example, from data sources or endpoints of acquisition to integration to consumption for specific use cases). These data pipelines must be created, maintained and optimized as workloads move from development to production for specific use cases. Architecting, creating and maintaining data pipelines will be the primary responsibility of the data engineer
Data Integration and Data Lake:
Bring data from disparate systems and applications into a centralized data lake on the Synapse platform. Design and develop robust ETL processes to ensure smooth and efficient data extraction, transformation, and loading.
Drive Automation through effective metadata management:
The data engineer will be responsible for using innovative and modern tools, techniques and architectures to partially or completely automate the most-common, repeatable and tedious data preparation and integration tasks in order to minimize manual and error-prone processes and improve productivity. The data engineer will also need to assist with renovating the data management infrastructure to drive automation in data integration and management and automate data pipelines to ensure the timely and accurate flow of data from source systems to the data lake. Implement scheduling, monitoring, and error handling mechanisms to maintain data pipeline reliability.
Data Visualization:
Collaborate with data visualization teams to ensure that the data stored in the data lake is readily available for dashboard development and reporting purposes. Optimize data structures and formats to facilitate efficient data retrieval for visualization tools such as Tableau, Power BI, and BOBJ.
Data Cataloging and Documentation
: Develop and maintain data cataloging processes to enable easy discovery and understanding of available data assets. Document data sources, data definitions, and data lineage to ensure transparency and data governance.
Synapse
and
Purview
experience highly desired.
QUALIFICATIONS
Bachelor's degree and 5+ years of management experience; or, an advanced degree and 3+ years of management experience with Data Integration and platform build; Management experience a plus
Proven experience in ETL practices using tools such as SAP Data Services, SAP SLT, etc.
Deep technical knowledge of data architecture, cloud platforms, modern data ETL tools and languages, and data governance
Experience with Azure Data Factory, and Synapse is must
Strong knowledge of data governance frameworks, practices, and industry standards.
Familiarity with the consumer packaging industry is a plus.
Experience working with multiple business units and applications such as SAP, Blue Yonder, One Stream, etc.
At least three years of experience working in cross-functional teams and collaborating with business stakeholders in support of a departmental and/or multi-departmental data management and analytics initiative
Proficiency in data visualization tools such as Tableau, Power BI, and BOBJ.
Excellent communication and interpersonal skills to collaborate effectively with business stakeholders.
Ability to adapt to a fast-paced and evolving environment.
ESPO Corporation
Willowbrook, IL 60527
(630) 789-2525
View all open jobs at: www.espocorp.com/jobs
Leaders in Technical Recruiting & Staffing since 1965
We are an Equal Opportunity Employer and value the benefits of diversity in our workforce.
All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity and expression, national origin, disability, protected Veteran status or any other attribute or protected characteristic by law. If you need assistance applying please contact us at 630-789-2525.
Show more
Show less","Data Warehousing, Data Lake, Azure Data Factory, Synapse, Purview, SAP Data Services, SAP SLT, Cloud Platforms, ETL, Data Architecture, Data Governance, Data Catalog, Data Lineage, Metadata Management, Automation, Data Visualization, Tableau, Power BI, BOBJ","data warehousing, data lake, azure data factory, synapse, purview, sap data services, sap slt, cloud platforms, etl, data architecture, data governance, data catalog, data lineage, metadata management, automation, data visualization, tableau, power bi, bobj","automation, azure data factory, bobj, cloud platforms, data architecture, data catalog, data governance, data lake, data lineage, datawarehouse, etl, metadata management, powerbi, purview, sap data services, sap slt, synapse, tableau, visualization"
Senior Data Platform Engineer-Azure for Des Moines IA,Steneral Consulting,"Des Moines, IA",https://www.linkedin.com/jobs/view/senior-data-platform-engineer-azure-for-des-moines-ia-at-steneral-consulting-3750184997,2023-12-17,Red Deer, Canada,Mid senior,Hybrid,"Senior Data Platform Engineer-Azure
Des Moines, IA - Hybrid 3 days a week - locals are highly preferred but will consider someone from Midwest who will relocate from day one to work onsite.
back end salary of $120-$150K based on experience. Please cover rate and salary with candidate before you send them to me.
Communication and collaboration are key. They must be easily understood and able to speak well to their projects and technical experience.
Must have valid LinkedIn and Photo ID required with submission
Must Have’s: Must have everything or please do not send them to me.
Azure Microsoft Fabric -- end to end lifecycle.
Azure Azure SQL Database: Provisioning, performance tuning, scaling, and security.
Azure Cosmos DB: Understanding of NoSQL databases, partitioning, consistency models.
Azure Data Factory: Data integration and ETL processes.
Azure Blob Storage and Data Lake Storage: Management, performance, security, and data lifecycle.
Azure Stream Analytics: Real-time data streaming and analytics.
Azure Databricks & HDInsight: Big data analytics solutions. (lower priority)
Azure Synapse Analytics: Knowledge of data warehousing, data integration, and analytics.
SQL: Writing, optimizing, and debugging SQL queries.
Data modeling: Normalization, star schema, snowflake schema
Familiarity with SDKs and APIs associated with Azure data services.
Integration with other Azure services or third-party applications.
Experience in one or more programming languages like C#, Python, or Java can be beneficial.
Azure Monitor, Azure Log Analytics, and Application Insights.
DP-203 certification
Financial/Investment industry experience.
Job Description
Here are the skills sets for building out the Microsoft Azure Data Platform.
Azure Fundamentals:
Understanding of Azure subscriptions, resources, and resource groups.
Familiarity with Azure regions, availability zones, and the Azure portal.
Azure Data Services Knowledge of tool set:
Azure Microsoft Fabric -- end to end lifecycle.
Azure Azure SQL Database: Provisioning, performance tuning, scaling, and security.
Azure Cosmos DB: Understanding of NoSQL databases, partitioning, consistency models.
Azure Data Factory: Data integration and ETL processes.
Azure Blob Storage and Data Lake Storage: Management, performance, security, and data lifecycle.
Azure Stream Analytics: Real-time data streaming and analytics.
Azure Databricks & HDInsight: Big data analytics solutions. (lower priority)
Azure Synapse Analytics: Knowledge of data warehousing, data integration, and analytics.
Skills:
SQL: Writing, optimizing, and debugging SQL queries.
Data modeling: Normalization, star schema, snowflake schema
Familiarity with SDKs and APIs associated with Azure data services.
Integration with other Azure services or third-party applications.
Experience in one or more programming languages like C#, Python, or Java can be beneficial.
Azure Monitor, Azure Log Analytics, and Application Insights.
DP-203 certification
Optional but helpful:
Azure Active Directory and role-based access control (RBAC)
Tools like Azure Data Migration Service, SSIS (SQL Server Integration Services).
Strategies for migrating data from on-premises or other clouds to Azure.
Show more
Show less","Azure, Azure Microsoft Fabric, Azure SQL Database, Azure Cosmos DB, Azure Data Factory, Azure Blob Storage, Azure Data Lake Storage, Azure Stream Analytics, Azure Databricks, Azure HDInsight, Azure Synapse Analytics, SQL, Data modeling, SDKs, APIs, Azure data services, Azure services, Thirdparty applications, C#, Python, Java, Azure Monitor, Azure Log Analytics, Application Insights, DP203 certification, Azure Active Directory, RBAC, Azure Data Migration Service, SSIS, Data migration","azure, azure microsoft fabric, azure sql database, azure cosmos db, azure data factory, azure blob storage, azure data lake storage, azure stream analytics, azure databricks, azure hdinsight, azure synapse analytics, sql, data modeling, sdks, apis, azure data services, azure services, thirdparty applications, c, python, java, azure monitor, azure log analytics, application insights, dp203 certification, azure active directory, rbac, azure data migration service, ssis, data migration","apis, application insights, azure, azure active directory, azure blob storage, azure cosmos db, azure data factory, azure data lake storage, azure data migration service, azure data services, azure databricks, azure hdinsight, azure log analytics, azure microsoft fabric, azure monitor, azure services, azure sql database, azure stream analytics, azure synapse analytics, c, data migration, datamodeling, dp203 certification, java, python, rbac, sdks, sql, ssis, thirdparty applications"
AWS Data Engineer,Extend Information Systems Inc.,"Newark, NJ",https://www.linkedin.com/jobs/view/aws-data-engineer-at-extend-information-systems-inc-3742890667,2023-12-17,Red Deer, Canada,Mid senior,Hybrid,"Job Title: AWS Data Engineer
Location:
Newark, NJ
(1 time in a week)
Duration: Long Term
Experience level : 9+ yrs
Job Description
Designing solutions using AWS services.
Building proof of concepts for new tools, services.
Working, coordinating with, and guiding offshore team.
Skills \ Competencies we are looking for:
Overall experience 8-12 years
3+ years of Healthcare Payer experience within last 4 years. (Must)
ETL experience.
Experience with building Data Ingestion, Pipelines for high volume data.
In depth experience with AWS services like Redshift, Glue, Athena, Pyspark, EMR, S3
Python experience.
Advanced SQL experience.
Familiarity with Dev Ops.
Thanks & Regards !!
Anoop Tiwari
Extend Information Systems
Cell: -
Email: Anoop@extendinfosys.com
Show more
Show less","AWS, Redshift, Glue, Athena, Pyspark, EMR, S3, Python, ETL, Data Ingestion, Pipelines, Advanced SQL, Dev Ops, Healthcare Payer","aws, redshift, glue, athena, pyspark, emr, s3, python, etl, data ingestion, pipelines, advanced sql, dev ops, healthcare payer","advanced sql, athena, aws, data ingestion, dev ops, emr, etl, glue, healthcare payer, pipelines, python, redshift, s3, spark"
Data Engineer,Keylent Inc,"Berkeley Heights, NJ",https://www.linkedin.com/jobs/view/data-engineer-at-keylent-inc-3768064281,2023-12-17,Red Deer, Canada,Mid senior,Hybrid,"Immediate Need - ETL Talend Developer (Data Engineer)- Berkeley Heights NJ - Client / Apex - Hybrid/Onsite
Design and implement Cloud-native architectures that will allow business requirements to be met with a minimal degree of risk to the organization.
Perform threat modelling and design reviews to assess security implications and requirements for introduction of new technologies.
Involve in all aspects of architecture, design, build, engineering, and implementation of technical solutions using in any one of the technical platforms (JAVA, Microsoft, Azure, AWS, Big data Stack etc.,).
Understand the enterprise architecture roadmap and research, identify & recommend industry best practices for solving business problems.
Provide recommendations and technical guidance to improve performance, reliability, and reusability within the constraints of budget, resource, and business dependencies.
Help program and project managers in the design, planning and governance of implementing projects of any kind.
Perform detailed analysis of business problems and technical environments and use this in designing the solution.
Work creatively and analytically in a problem-solving environment.
Requirements
Bachelor's Degree or master's degree in Computer Science, Mathematics, Statistics or equivalent.
5+ years of hands-on software engineering experience.
5+ years of experience integrating technical processes and business outcomes - specifically: data and process analysis, data quality metrics/monitoring, data architecture, developing policies/standards & supporting processes.
Designing and building data pipeline (batch & streaming), extensive experience in Apache Spark, Spark Streaming Kafka. Hands-on with coding skills to do the POCs and build prototypes
2+ years of strong ETL experience on Talend.
Strong database fundamentals including SQL, performance, and schema design.
Strong understanding of programming languages like Java, Scala.
Excellent communications skills, both written and verbal.
Experience working in an offshore/onshore team model.
Design and implement Data security and privacy controls.
Experience with Git or equivalent source code control software.
Show more
Show less","Cloudnative architectures, Threat modelling, Security implications, JAVA, Microsoft, Azure, AWS, Big data Stack, Enterprise architecture roadmap, Industry best practices, Performance, Reliability, Reusability, Budget, Resource, Business dependencies, Program and project management, Business problems, Technical environments, Data pipeline, Apache Spark, Spark Streaming Kafka, POCs, Prototypes, ETL, Talend, SQL, Schema design, Java, Scala, Communications skills, Offshore/onshore team model, Data security, Privacy controls, Git","cloudnative architectures, threat modelling, security implications, java, microsoft, azure, aws, big data stack, enterprise architecture roadmap, industry best practices, performance, reliability, reusability, budget, resource, business dependencies, program and project management, business problems, technical environments, data pipeline, apache spark, spark streaming kafka, pocs, prototypes, etl, talend, sql, schema design, java, scala, communications skills, offshoreonshore team model, data security, privacy controls, git","apache spark, aws, azure, big data stack, budget, business dependencies, business problems, cloudnative architectures, communications skills, data pipeline, data security, enterprise architecture roadmap, etl, git, industry best practices, java, microsoft, offshoreonshore team model, performance, pocs, privacy controls, program and project management, prototypes, reliability, resource, reusability, scala, schema design, security implications, spark streaming kafka, sql, talend, technical environments, threat modelling"
"Data Engineer, Data Platform",Grammarly,"Nevada, United States",https://www.linkedin.com/jobs/view/data-engineer-data-platform-at-grammarly-3656892825,2023-12-17,Red Deer, Canada,Mid senior,Hybrid,"Grammarly is excited to offer a
remote-first hybrid working model
. Team members work primarily remotely in the United States, Canada, Ukraine, Germany, or Poland. Certain roles have specific location requirements to facilitate collaboration at a particular Grammarly hub.
All roles have an in-person component: Conditions permitting, teams meet 2–4 weeks every quarter at one of Grammarly’s hubs in San Francisco, Kyiv, New York, Vancouver, and Berlin, or in a workspace in Kraków.
This flexible approach gives team members the best of both worlds: plenty of focus time along with in-person collaboration that fosters trust and unlocks creativity.
Grammarly team members in this role must be based in the United States or Canada, and they must be able to collaborate in person 2 weeks per quarter, traveling if necessary to the hub(s) where the team is based.
The opportunity
Grammarly is the world’s leading AI writing assistance company trusted by over 30 million people and 70,000 professional teams every day. From instantly creating a first draft to perfecting every message, Grammarly’s product offerings help people at 96% of the Fortune 500 get their point across—and get results. Grammarly has been profitable for over a decade because we’ve stayed true to our values and built an enterprise-grade product that’s secure, reliable, and helps people do their best work—without selling their data. We’re proud to be one of Inc.’s best workplaces, a Glassdoor Best Place to Work, one of TIME’s 100 Most Influential Companies, and one of Fast Company’s Most Innovative Companies in AI.
To achieve our ambitious goals, we’re looking for a Data Engineer to join our Data Engineering Platform team. This person will build highly automated, low latency core datasets that will help data engineers and end users across Grammarly to work with analytical data at scale.
Grammarly’s engineers and researchers have the freedom to innovate and uncover breakthroughs—and, in turn, influence our product roadmap. The complexity of our technical challenges is growing rapidly as we scale our interfaces, algorithms, and infrastructure. Read more about our stack or hear from our team on our technical blog.
Your impact
As a Data Engineer on our Data Engineering Platform team, you will:
Drive improvements to make our analytics effortless by creating and adjusting core data models and storage structures, all while understanding the needs of our users.
Make analytical data and metrics usable within a few minutes of real world events occuring, and build streaming processes for the output derived events and aggregate data.
Model structure, storage, and access of data at very high volumes for our data lakehouse.
Improve developer productivity and self-serve solutions by contributing components to our stream data processing framework(s).
Own data engineering's infrastructure-as-code for provisioning services that allow our engineers to deploy mature software installations within a few hours.
Build a world-class process that will allow our systems to scale.
Mentor other back-end engineers on the team and help them grow.
Build and contribute to AWS high-scale distributed systems on the back-end.
We’re Looking For Someone Who
Embodies our EAGER values—is ethical, adaptable, gritty, empathetic, and remarkable.
Is inspired by our MOVE principles, which are the blueprint for how things get done at Grammarly: move fast and learn faster, obsess about creating customer value, value impact over activity, and embrace healthy disagreement rooted in trust.
Is able to collaborate in person 2 weeks per quarter, traveling if necessary to the hub where the team is based.
Has experience with Python, Scala, or Java.
Has experience with designing database objects and writing relational queries
Has experience designing and standing up APIs and services.
Has experience with system design and building internal tools.
Has experience handling applications that work with data from data lakes.
Has at least some experience building internal Admin sites.
Has good knowledge of and at least some experience with AWS (or, alternatively, has deep expertise in Azure or GCE and is willing to learn AWS in a short time frame).
Can knowledgeably choose an open source or third-party service to accomplish what they need or, alternatively, can devise a quick and simple solution on their own.
Support for you, professionally and personally
Professional growth: We believe that autonomy and trust are key to empowering our team members to do their best, most innovative work in a way that aligns with their interests, talents, and well-being. We support professional development and advancement with training, coaching, and regular feedback.
A connected team: Grammarly builds a product that helps people connect, and we apply this mindset to our own team. Our remote-first hybrid model enables a highly collaborative culture supported by our EAGER (ethical, adaptable, gritty, empathetic, and remarkable) values. We work to foster belonging among team members in a variety of ways. This includes our employee resource groups, Grammarly Circles, which promote connection among those with shared identities, such as BIPOC and LGBTQIA+ team members, women, and parents. We also celebrate our colleagues and accomplishments with global, local, and team-specific programs.
Compensation And Benefits
Grammarly offers all team members competitive pay along with a benefits package encompassing the following and more:
Excellent health care (including a wide range of medical, dental, vision, mental health, and fertility benefits)
Disability and life insurance options
401(k) and RRSP matching
Paid parental leave
Twenty days of paid time off per year, eleven days of paid holidays per year, and unlimited sick days
Home office stipends
Caregiver and pet care stipends
Wellness stipends
Admission discounts
Learning and development opportunities
Grammarly takes a market-based approach to compensation, which means base pay may vary depending on your location. Our US and Canada locations are categorized into compensation zones based on each geographic region’s cost of labor index. For more information about our compensation zones and locations where we currently support employment, please refer to this page. If a location of interest is not listed, please speak with a recruiter for additional information.
Base pay may vary considerably depending on job-related knowledge, skills, and experience. The expected salary ranges for this position are outlined below by compensation zone and may be modified in the future.
United States
Zone 1: $167,000 - $242,000/year (USD)
Zone 2: $150,000 – $218,000/year (USD)
Zone 3: $142,000 – $206,000/year (USD)
Zone 4: $134,000 – $194,000/year (USD)
We encourage you to apply
At Grammarly, we value our differences, and we encourage all—especially those whose identities are traditionally underrepresented in tech organizations—to apply. We do not discriminate on the basis of race, religion, color, gender expression or identity, sexual orientation, ancestry, national origin, citizenship, age, marital status, veteran status, disability status, political belief, or any other characteristic protected by law. Grammarly is an equal opportunity employer and a participant in the US federal E-Verify program (US). We also abide by the Employment Equity Act (Canada).
Please note that EEOC is optional and specific to US-based candidates.
#NA
All team members meeting in person for official Grammarly business or working from a hub location are strongly encouraged to be vaccinated against COVID-19.
Show more
Show less","Python, Scala, Java, MySQL, PostgreSQL, AWS, Azure, GCE, Hadoop, Spark, Kafka, Airflow, Hive, Tableau, Power BI, Looker, Data modeling, Data warehousing, Data lakehouse, Data pipeline, Streaming data, Machine learning, Artificial intelligence, Natural language processing, Distributed systems, System design, API design, Microservices, Internal tools, DevOps, Agile, Scrum, Kanban, Jira, Confluence, Slack, GitHub, GitLab, Jenkins, Docker, Kubernetes","python, scala, java, mysql, postgresql, aws, azure, gce, hadoop, spark, kafka, airflow, hive, tableau, power bi, looker, data modeling, data warehousing, data lakehouse, data pipeline, streaming data, machine learning, artificial intelligence, natural language processing, distributed systems, system design, api design, microservices, internal tools, devops, agile, scrum, kanban, jira, confluence, slack, github, gitlab, jenkins, docker, kubernetes","agile, airflow, api design, artificial intelligence, aws, azure, confluence, data lakehouse, data pipeline, datamodeling, datawarehouse, devops, distributed systems, docker, gce, github, gitlab, hadoop, hive, internal tools, java, jenkins, jira, kafka, kanban, kubernetes, looker, machine learning, microservices, mysql, natural language processing, postgresql, powerbi, python, scala, scrum, slack, spark, streaming data, system design, tableau"
Data Engineer- AZURE,"The Dignify Solutions, LLC","Bellevue, WA",https://www.linkedin.com/jobs/view/data-engineer-azure-at-the-dignify-solutions-llc-3768015842,2023-12-17,Red Deer, Canada,Mid senior,Hybrid,"Data Engineering experience primarily on Spark.
Someone who has worked on Azure cloud with knowledge on Azure DEVOPS (CI/CD & Infrastructure as a code), ADF, ADW & Power BI.
Apache Nifi will be good to have.Power BI: Understand business requirements to set functional specifications for reporting applications Build automated reports and dashboards with the help of Power BI reporting tool Be experienced in tools and systems on MS SQL Server BI Stack, including SSRS and TSQL, Power Query, MDX, PowerBI, and DAX Be able to quickly shape data into reporting and analytics solutions
Spot key performance indicators with apt objectives Run DAX queries and functions in Power BI Should have an edge over making DAX queries in Power BI desktop. Developing visual reports, KPI scorecards, and dashboards using Power BI desktop. Connecting data sources, importing data, and transforming data for Business intelligence.
Show more
Show less","Spark, Azure cloud, Azure DevOps, Azure Data Factory (ADF), Azure Data Warehouse (ADW), Power BI, Apache Nifi, MS SQL Server BI Stack, SSRS, TSQL, Power Query, MDX, DAX, Data shaping, Data analytics, KPI identification, DAX queries, Power BI desktop, Data visualization, KPI scorecards, Business intelligence, Data transformation, Data import, Data integration","spark, azure cloud, azure devops, azure data factory adf, azure data warehouse adw, power bi, apache nifi, ms sql server bi stack, ssrs, tsql, power query, mdx, dax, data shaping, data analytics, kpi identification, dax queries, power bi desktop, data visualization, kpi scorecards, business intelligence, data transformation, data import, data integration","apache nifi, azure cloud, azure data factory adf, azure data warehouse adw, azure devops, business intelligence, data import, data integration, data shaping, data transformation, dataanalytics, dax, dax queries, kpi identification, kpi scorecards, mdx, ms sql server bi stack, power bi desktop, power query, powerbi, spark, ssrs, tsql, visualization"
AWS Cloud Data Engineer,DMVTEK,"Jersey City, NJ",https://www.linkedin.com/jobs/view/aws-cloud-data-engineer-at-dmvtek-3721469195,2023-12-17,Red Deer, Canada,Mid senior,Hybrid,"Job Title:
AWS Cloud Data Engineer
Location : Jersey City, NJ (Hybrid)
Duration: Long Term
Responsibilities
12+ years of professional experience as Software Engineer, Applications Developer, Data Engineer or similar experience
Engage in Event streaming design development using, Apache Flink, AWS KDA, Kafka
Develop technical specification documents and generic/reusable frameworks Public cloud-AWS.
Design and Develop stateless and stateful transformations using Apache Flink
Design and Develop the streaming pipelines with a various sink points like MSK, S3, Rest API.
Metric collection and Dash board integration with ,Prometheus, CloudWatch, datadog.
Design and develop reconciliation and reprocessing the data and ensure zero data loss.
Identify/troubleshoot application code-related issues and review and provide feedback to the final user.
BS/BA degree or equivalent experience. Expertise in application, data, and infrastructure architecture disciplines
Advanced knowledge of architecture and design across all systems. Proficiency in multiple modern programming languages
Show more
Show less","AWS, Apache Flink, Kafka, Amazon Kinesis Data Analytics, Prometheus, CloudWatch, Datadog, Apache Kafka, Amazon S3, REST API, Infrastructure architecture, Application architecture, Data architecture, Flink, Scala","aws, apache flink, kafka, amazon kinesis data analytics, prometheus, cloudwatch, datadog, apache kafka, amazon s3, rest api, infrastructure architecture, application architecture, data architecture, flink, scala","amazon kinesis data analytics, amazon s3, apache flink, apache kafka, application architecture, aws, cloudwatch, data architecture, datadog, flink, infrastructure architecture, kafka, prometheus, rest api, scala"
Cloud Data Engineer - GCP,SkilzMatrix Digital,"Bolingbrook, IL",https://www.linkedin.com/jobs/view/cloud-data-engineer-gcp-at-skilzmatrix-digital-3763902906,2023-12-17,Red Deer, Canada,Mid senior,Hybrid,"Must Haves
GCP Admin and automation.
Ability to provide technical guidance to data engineering team for Talend pipeline design and enhancements
Strong experience in ETL with Data migration, data consolidation
Hands-on experience in Talend data pipelines around event and messaging patterns, streaming data, Kafka , API
Master Proficiency in at least one cloud platform (AWS / Azure / GCP ) and related data services
Hands-on experience in services Cloud dataflow, Cloud Pub-Sub, Big Query, Cloud Storage, Cloud Dataflow, Google Composer etc.
Strong SQL knowledge Strong leadership quality and able to handle a team
Understanding of fundamentals of Git and Git workflows.
Must have used at least one of the SAAS-based Git tools like GitHub, GitLab, Bitbucket
Experience of working in agile application development environment
Ability to proactively prioritize tasks in consultation with business stakeholders, Product Owners, Product Managers
Troubleshoot issues and perform administrative tasks like setting up projects, users, authorizations, Migrations and job scheduling.
Create and manage workspace, users, roles, groups, projects, access control, native and custom connections.
Technical support to applications on trouble shooting Environment, software and application level issues Write, test programs using Unix Shell scripting, oracle PL/SQL programming ; Perform unit testing Talend server builds, version upgrade, maintenance and migration
Expertise in Talend products including Talend Studio, Talend Administration Centre, Talend Big Data, Talend Metadata Bridge, and Talend Data Quality
Managing support services and change management using ServiceNow application.
Good To Haves
Theobald, Collibra DQ Familiarity with Power BI / Tableau based BI framework
Experience in automation of the deployment of services and configuration updates
Experience in optimization of existing cloud-based services running in production environments
Awareness of data governance processes (security, lineage, catalog) and tools like Collibra
Exposure to CI/CD Infra provisioning on cloud, auto build & deployment pipelines, code quality Valid professional certification in cloud platform (AWS / Azure / GCP)
Exposure to admin activities Proficiency on the Linux command line using tools such as bash, SSH, vim
Experience in python for scripting repetitive tasks
Personal Attributes
Excellent verbal and written communication and interpersonal skills
Confidence and agility in challenging times
Ability to work collaboratively with cross-functional teams in a fast-paced, team environment
Self-starter who requires minimal oversight
Ability to prioritize and manage multiple tasks
Process orientation and the ability to define and set up processes.
Show more
Show less","GCP, Talend, ETL, Data migration, Data consolidation, Kafka, AWS, Azure, Google Cloud Platform, Cloud dataflow, Cloud PubSub, Big Query, Cloud Storage, Cloud Dataflow, Google Composer, SQL, Git, GitHub, GitLab, Bitbucket, Unix Shell scripting, Oracle PL/SQL, Talend Studio, Talend Administration Centre, Talend Big Data, Talend Metadata Bridge, Talend Data Quality, ServiceNow, Power BI, Tableau, CI/CD, Linux, Python","gcp, talend, etl, data migration, data consolidation, kafka, aws, azure, google cloud platform, cloud dataflow, cloud pubsub, big query, cloud storage, cloud dataflow, google composer, sql, git, github, gitlab, bitbucket, unix shell scripting, oracle plsql, talend studio, talend administration centre, talend big data, talend metadata bridge, talend data quality, servicenow, power bi, tableau, cicd, linux, python","aws, azure, big query, bitbucket, cicd, cloud dataflow, cloud pubsub, cloud storage, data consolidation, data migration, etl, gcp, git, github, gitlab, google cloud platform, google composer, kafka, linux, oracle plsql, powerbi, python, servicenow, sql, tableau, talend, talend administration centre, talend big data, talend data quality, talend metadata bridge, talend studio, unix shell scripting"
AWS Data Engineer (Healthcare),Extend Information Systems Inc.,"Newark, NJ",https://www.linkedin.com/jobs/view/aws-data-engineer-healthcare-at-extend-information-systems-inc-3742855592,2023-12-17,Red Deer, Canada,Mid senior,Hybrid,"Hi Jobseekers,
Hope You are doing good.
We do have a new an excellent opportunity for you as
AWS Data Engineer
to provide support to the
Client
.
This opportunity is a
Full Time/
Permanent
at
Dallas, TX
role.
Please have a look at the job description below and let me know if you or someone you know is interested in this role. You can reach me at
(571) 421-2684.
Job Title : AWS Data Engineer
Location : Newark, NJ (day 1 onsite)
Duration : Long Term contract
At least 9+ year is required
Job Description
Designing solutions using AWS services.
Building proof of concepts for new tools, services.
Working, coordinating with, and guiding offshore team.
Skills \ Competencies we are looking for -
Overall experience 8-12 years
3+ years of Healthcare Payer experience within last 4 years. (Must)
ETL experience.
Experience with building Data Ingestion, Pipelines for high volume data.
In depth experience with AWS services like Redshift, Glue, Athena, Pyspark, EMR, S3
Python experience.
Advanced SQL experience.
Familiarity with Dev Ops.
Thanks & Regards
Shankar Kr Singh
Extend Information Systems
Cell: (571) 421-2684
Email: shankar@extendinfosys.com
Address:
44258 Mercure Circle, UNIT 102 A, Sterling VA, USA – 20166
Web:
WWW.extendinfosys.com
We are E-verified company
Show more
Show less","AWS, Redshift, Glue, Athena, Pyspark, EMR, S3, Python, SQL, Dev Ops, ETL, Data Ingestion, Pipelines, Healthcare Payer","aws, redshift, glue, athena, pyspark, emr, s3, python, sql, dev ops, etl, data ingestion, pipelines, healthcare payer","athena, aws, data ingestion, dev ops, emr, etl, glue, healthcare payer, pipelines, python, redshift, s3, spark, sql"
Data Engineer,"The Dignify Solutions, LLC","Plano, TX",https://www.linkedin.com/jobs/view/data-engineer-at-the-dignify-solutions-llc-3768011389,2023-12-17,Red Deer, Canada,Mid senior,Hybrid,"Technical hands on skill and minimum 10+ years of on job experience on below tech stack:
ï»¿ï»¿ï»¿Data factory
ï»¿ï»¿ï»¿Azure
ï»¿ï»¿ï»¿Pyspark
Azure Databricks
Delta lake
ï»¿ï»¿ï»¿Python
Synpase
good to have:
ï»¿ï»¿ï»¿Ability to analyze and understand data,data flows, end to end reconciliation of data should be able to debug issues in live production environment.
ï»¿ï»¿ï»¿Should have worked on end to end datalake ingestion project where multiple sources are being injected to Datalake with medallion architecture, using ADF, Databricks, Azure devops pipelines, With Multiple environments
Show more
Show less","Data Factory, Azure, PySpark, Azure Databricks, Delta Lake, Python, Synapse, Ability to analyze and understand data, Data flows, Endtoend reconciliation of data, Debugging issues in live production environment, Endtoend datalake ingestion project, Multiple sources injected to Datalake, Medallion architecture, ADF, Azure DevOps pipelines, Multiple environments","data factory, azure, pyspark, azure databricks, delta lake, python, synapse, ability to analyze and understand data, data flows, endtoend reconciliation of data, debugging issues in live production environment, endtoend datalake ingestion project, multiple sources injected to datalake, medallion architecture, adf, azure devops pipelines, multiple environments","ability to analyze and understand data, adf, azure, azure databricks, azure devops pipelines, data factory, data flows, debugging issues in live production environment, delta lake, endtoend datalake ingestion project, endtoend reconciliation of data, medallion architecture, multiple environments, multiple sources injected to datalake, python, spark, synapse"
Senior Data Engineer (Contract),tlnt,"Dallas, TX",https://www.linkedin.com/jobs/view/senior-data-engineer-contract-at-tlnt-3756665310,2023-12-17,Red Deer, Canada,Mid senior,Hybrid,"The Role
Requirements
We are seeking a highly experienced Senior Data Engineer with 5+ years of expertise in Azure data engineering. This role is responsible for the development and management of data pipelines, data modeling, and supporting organizational data analytics and insights initiatives. The ideal candidate will be proficient in tools and technologies like Pyspark, Azure Data Factory (ADF), Databricks, big data technologies, SQL, and Python. Additionally, the Senior Data Engineer will play a critical role in stakeholder management, leading development teams or project teams, guiding design discussions, and taking ownership of deliverables for larger projects. Effective communication skills are essential for understanding and translating functional and technical requirements into practical solutions. The ideal person will:
Design, build, and maintain data pipelines in the Azure environment, ensuring smooth operation in both batch and real-time processing.
Optimize Azure data infrastructure to facilitate accurate data extraction, transformation, and loading from a variety of data sources.
Develop and automate ETL processes using tools like Pyspark, Azure Data Factory, and Databricks to facilitate data extraction and manipulation from diverse sources.
Transform raw data stored in Azure Data Warehouses into accessible datasets suitable for both technical and non-technical stakeholders.
Demonstrate expertise in Azure services and big data technologies, including but not limited to Azure Data Factory, Databricks, and other relevant tools. Proficiency in SQL and Python is also crucial.
Engage in business discussions, collaborate with stakeholders, and take ownership of deliverables, ensuring that the data engineering solutions align with business objectives.
Lead development teams or project teams, providing guidance and expertise to ensure the successful execution of data engineering projects.
Function as an independent developer who can guide design discussions and make informed decisions to drive project success.
Leverage extensive experience to handle larger projects and manage multiple teams effectively.
Exhibit excellent communication skills, translating functional and technical requirements into actionable solutions that meet business needs.
The Person
Minimum of 4 years of hands-on experience in Azure data engineering.
Proficiency in a range of key technologies is essential, including Pyspark, Azure Data Factory (ADF), Databricks, big data tools, SQL, and Python.
Should be adept at handling business discussions and be capable of taking ownership of deliverables.
Effective stakeholder management is crucial, including leading development teams or project teams to ensure successful project outcomes.
Self-sufficient developer who can provide guidance in design discussions and make informed decisions.
Demonstrated experience in successfully managing larger projects and multiple teams is a key requirement.
Excellent communication skills are necessary to understand and bridge the gap between functional and technical requirements, providing practical and effective solutions.
Note:
Preferred candidates will be in the Dallas or willing to relocate to Dallas, Texas
﻿Our client is an equal opportunity employer, and we celebrate diversity and are committed to creating an inclusive environment for all.
Note
: This job description is intended to convey information essential to understanding the scope of the position and is not an exhaustive list of skills, efforts, duties, responsibilities, or working conditions associated with it.
Skills: azure data factory (adf),azure data engineering,python,big data technologies,databricks,azure,pyspark,sql
Show more
Show less","Azure Data Factory (ADF), Azure Data Engineering, Python, Big Data Technologies, Databricks, Azure, Pyspark, SQL","azure data factory adf, azure data engineering, python, big data technologies, databricks, azure, pyspark, sql","azure, azure data engineering, azure data factory adf, big data technologies, databricks, python, spark, sql"
Data Engineer,NR Consulting,"New York, NY",https://www.linkedin.com/jobs/view/data-engineer-at-nr-consulting-3768016737,2023-12-17,Red Deer, Canada,Mid senior,Hybrid,"Data Engineer SME
Location: NYC / Charlotte (Candidate should be open for hybrid work model)
Duration: 12 months
Responsibilities
A background in financial services, investment banking, regulated sectors and large enterprise
DevOps and Agile engineering practitioner with experience of test driven development
Ability to work independently on specialized assignments within the context of project deliverables
Take ownership of providing solutions and tools that iteratively increase engineering efficiencies.
Design should help embed standard processes, systems and operational models into the BAU approach for end-to-end execution of Data Pipelines
Excellent communication skills & team player and collaborator
Expert knowledge of Industry Best Practice for ETL Design, Principles, Concepts.
Working with Process Re-engineering and Operating Models
Strong Stakeholder Management Skills And Stakeholder Governance Experience
Working with diverse, cross-functional teams located locally and globally, and the ability to quickly develop and nurture strong working relationships
Preferred
Hands-on experience with or knowledge of Axiom data platform
Have Snowflake SnowPro certified
Snowflake Advanced Training
Azure/AWS or GCP Data Engineer
Azure/AWS or GCP Data Architect
Show more
Show less","Data Pipelines, DevOps, Agile, Test Driven Development, ETL Design, Data Reengineering, Snowflake, SnowPro, Azure, AWS, GCP","data pipelines, devops, agile, test driven development, etl design, data reengineering, snowflake, snowpro, azure, aws, gcp","agile, aws, azure, data reengineering, datapipeline, devops, etl design, gcp, snowflake, snowpro, test driven development"
"Azure Data Engineer --  6+ Months contract -- Chicago, IL (Hybrid)",Lorven Technologies Inc.,"Chicago, IL",https://www.linkedin.com/jobs/view/azure-data-engineer-6%2B-months-contract-chicago-il-hybrid-at-lorven-technologies-inc-3744869797,2023-12-17,Red Deer, Canada,Mid senior,Hybrid,"Job Title: Azure Data Engineer
Location: Chicago, IL (Hybrid)
Duration: 6+ Months contract
Required Skills
9+ years of experience.
Azure, Azure Data Factory, ETL
Machine Learning, PowerBI and Tableau are nice to have
Show more
Show less","Azure, Azure Data Factory, ETL, Machine Learning, PowerBI, Tableau","azure, azure data factory, etl, machine learning, powerbi, tableau","azure, azure data factory, etl, machine learning, powerbi, tableau"
Senior Data analyst,"Resource Providers, Inc,","Birmingham, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-analyst-at-resource-providers-inc-3769607367,2023-12-17,Worcester, United Kingdom,Mid senior,Onsite,"Senior Data Analyst
Mainstay is proud the represent a global service provider that is dedicated to delivering exceptional services and innovative solutions to their clients who are seeking a highly skilled and experienced Senior Data Analyst to join the organisation.
As a Senior Data Analyst, you will play a pivotal role in leveraging data-driven insights to optimize operational efficiency, improve service delivery, and drive strategic decision-making within the companies facilities management operations.
Key Responsibilities Of a Senior Data Analyst
Develop and implement data collection methodologies tailored specifically to facilities management operations, ensuring accuracy and relevance.
Analyse facility-related datasets to uncover trends, patterns, and opportunities that can streamline operations, improve maintenance protocols, and optimize resource allocation.
Collaborate closely with various teams within the company to align data analysis with operational needs and strategic objectives.
Design and maintain intuitive dashboards, reports, and visualizations, providing stakeholders with actionable insights.
Utilize statistical methods and predictive analytics to forecast maintenance requirements, resource utilization, and service enhancements.
The Successful Senior Data Analyst Will Have
Proven experience as a Data Analyst or similar role, preferably within the facilities management or related industry.
Proficiency in data analysis tools such as SQL, Python, R, or similar languages.
Strong analytical and problem-solving skills with the ability to work with large, complex datasets.
Experience with data visualization tools (e.g., Tableau, Power BI) to create meaningful reports and dashboards.
Excellent communication and presentation skills with the ability to translate complex data insights into actionable recommendations.
Proven track record of leading data-driven initiatives and collaborating with diverse teams.
Strong understanding of CAFM tools such as Maximo or MRI evolution (Concept Evolution)
Eligible for SC clearance
This senior data analyst role is offering remote working, with the scope of going into the office 1 day a month, additional the basic salary is supported by a fantastic benefits package.
Please Apply Now for consideration!
Show more
Show less","Data Analysis, Facilities Management, Data Collection, Data Visualization, Predictive Analytics, SQL, Python, R, Tableau, Power BI, CAFM, Maximo, MRI evolution (Concept Evolution)","data analysis, facilities management, data collection, data visualization, predictive analytics, sql, python, r, tableau, power bi, cafm, maximo, mri evolution concept evolution","cafm, data collection, dataanalytics, facilities management, maximo, mri evolution concept evolution, powerbi, predictive analytics, python, r, sql, tableau, visualization"
Lead Data Engineer,Harnham,"Birmingham, England, United Kingdom",https://uk.linkedin.com/jobs/view/lead-data-engineer-at-harnham-3781943435,2023-12-17,Worcester, United Kingdom,Mid senior,Onsite,"To Apply for this Job Click Here
Lead Data Engineer
Hybrid, 2x per week in office, Leicester
Up to £90, 000
Permanent
An opportunity for a Lead Data Engineer has opened up at a veterinary organisation who is looking to develop predictive models and aid scientific research
The Company
This company is a veterinary organisation that exists as a subdivision of a major FMCG brand. They are looking for a Lead data Engineer to manage a team of 4 and proactively develop solutions.
The company is building dashboards and predictive models from biological and medical data to sell to businesses and provide to scientists to carry out research on disease.
The Role And Responsibilities
Manage a team of 4
Building data products and pipelines
Provide technical leadership
Work with stakeholders across the business to gather requirements
Your Skills And Experience
Strong cloud experience (preferable Azure)
Experience in a person or technical leadership position
Experience building data meshes
Preferably experience in life sciences
Benefits
Up to £90, 000 salary
Learning and upskilling initiatives
Healthcare
To Apply for this Job Click Here
Show more
Show less","Cloud (Azure), Data engineering, Data meshes, Building data products, Pipelines, Technical leadership, Person leadership, Life sciences","cloud azure, data engineering, data meshes, building data products, pipelines, technical leadership, person leadership, life sciences","building data products, cloud azure, data engineering, data meshes, life sciences, person leadership, pipelines, technical leadership"
IT Data Management System Developer,Connexa,"Birmingham, England, United Kingdom",https://uk.linkedin.com/jobs/view/it-data-management-system-developer-at-connexa-3762005745,2023-12-17,Worcester, United Kingdom,Mid senior,Onsite,"My Client is looking for a Data Management System Developer to join them in the housing market ion birmingham. As the IT Data Management System Developer, your role involves implementing digital solutions for housing systems, ensuring alignment with business processes. You'll collaborate with IT teams, translate requirements into technical specifications, and oversee design, testing, and user training. Additionally, you'll review upgrades, delegate testing, and provide support for efficient system operation.
Responsibilities
Responsible for developing, maintaining, and improving housing systems, applications, and integrations, aligning with IT strategies. Configures system applications based on technical specifications, builds and tests integration touchpoints, and manages data conversion challenges.
Monitors technical programs and executes enhancements to workflow processes, utilizing data extracts and reports. Reviews housing system upgrade specifications, contributes to IT strategies, and creates user acceptance testing criteria for upgrades and implementations.
Takes ownership of issues related to system implementation and day-to-day usage, providing support for efficient operations.
Experience
Proficient in housing management system administration.
Extensive background in the housing sector.
Expert in major system implementations, including UAT and understanding database structures, application layers, modules, and integration.
Successful in supporting critical business systems and managing incidents across various applications.
Thorough understanding of system and application business processes.
Skilled in extracting and presenting system data for business needs.
Experienced in leading stakeholder requirement meetings.
Capable of translating technical documentation into specifications and conducting end-user training.
Proficient in authoring system documentation and providing progress updates in project management.
Familiar with MS Office or equivalent tools.
Desirable
Proficiency in utilizing Housing Management Systems
Expertise in housing management modules including repairs, rents, arrears, and case management/workflow.
Familiarity with financial management systems and finance approval procedures.
Background in business or data analysis.
Knowledgeable in change management strategies and principles.
Experience in an Agile working environment or collaborating with remote teams.
Established in Didsbury, Connexa Technology Ltd is becoming one of the UK's fastest growing IT and Technology recruitment companies.
People. Technology. Connected.
Connexa Technology is acting as an Employment Agency in relation to this vacancy.
Show more
Show less","Data Management, System Development, Digital Solutions, IT Strategies, System Administration, Housing Sector, System Implementations, UAT, Database Structures, Application Layers, Modules, Integration, Business Systems, Incidents, Application Business Processes, System Data, Stakeholder Requirement Meetings, Technical Documentation, System Documentation, Project Management, MS Office, Housing Management Systems, Housing Management Modules, Repairs, Rents, Arrears, Case Management, Workflow, Financial Management Systems, Finance Approval Procedures, Business Analysis, Data Analysis, Change Management Strategies, Agile Working Environment, Remote Teams","data management, system development, digital solutions, it strategies, system administration, housing sector, system implementations, uat, database structures, application layers, modules, integration, business systems, incidents, application business processes, system data, stakeholder requirement meetings, technical documentation, system documentation, project management, ms office, housing management systems, housing management modules, repairs, rents, arrears, case management, workflow, financial management systems, finance approval procedures, business analysis, data analysis, change management strategies, agile working environment, remote teams","agile working environment, application business processes, application layers, arrears, business analysis, business systems, case management, change management strategies, data management, dataanalytics, database structures, digital solutions, finance approval procedures, financial management systems, housing management modules, housing management systems, housing sector, incidents, integration, it strategies, modules, ms office, project management, remote teams, rents, repairs, stakeholder requirement meetings, system administration, system data, system development, system documentation, system implementations, technical documentation, uat, workflow"
Data Analyst,People Tech Group Inc,"Redmond, WA",https://www.linkedin.com/jobs/view/data-analyst-at-people-tech-group-inc-3788334028,2023-12-17,Summit,United States,Associate,Onsite,"Position Title: Data Analyst
Client: Weyerhaeuser
Duration: Full time with People Tech Group
Job Responsibilities
With minimal supervision, completes the technical design, build, testing, and documentation of Power BI reports based on on-premise and SaaS data sources. The role is within a large and complex ERP implementation project and will primarily support the PMO in creating reporting solutions for project KPI tracking as well as data analysis and validation to support project activities
Education:
Bachelor’s degree or equivalent experience is required
Experience:
Minimum of four (4) to six (6) years of progressive, relevant information technology experience
Experienced in ingesting and data wrangling/modelling data in Power BI from on-premise (e.g. Excel, SQL Server, etc.) and SaaS (e.g. Azure Dev Ops, web services, etc.) data sources
Experienced in creating Dax measures and Power BI reports and dashboards
Experienced in deploying and managing Power BI app in the PowerBI.com service
Experienced in performing completeness and accuracy testing for Power BI report development
Experienced writing SQL statements off of relational databases (on-premise/cloud)
Experienced in moderate to complex data analysis in Excel.
Show more
Show less","Power BI, SQL, Data analysis, Data wrangling, Data modelling, DAX measures, Power BI reports, Power BI dashboards, Power BI app, Azure Dev Ops, Excel, Relational databases","power bi, sql, data analysis, data wrangling, data modelling, dax measures, power bi reports, power bi dashboards, power bi app, azure dev ops, excel, relational databases","azure dev ops, data modelling, data wrangling, dataanalytics, dax measures, excel, power bi app, power bi dashboards, power bi reports, powerbi, relational databases, sql"
Data Center Technical Operations Engineer,Kforce Inc,"Seattle, WA",https://www.linkedin.com/jobs/view/data-center-technical-operations-engineer-at-kforce-inc-3782233050,2023-12-17,Summit,United States,Associate,Onsite,"Responsibilities
Kforce has a client in Seattle, WA that is seeking a Data Center Technical Operations Engineer to own a product, platform, or set of features. Responsibilities:
Propose enhancements and accountable for execution, measurement of outcomes, and continuous improvement
Potentially fulfills the agile scrum role of Product Owner for a squad
Collaborate with, interpret, and act as the voice of the customer for product team and bring the team to the customer whenever possible
Works directly with users, UX design, other product managers & leaders, stakeholders, and the product team to define product needs, outcomes, and metrics (product KPIs) applying user centric design principles
Ensures the product/platform team has the full context and understanding of customers and business environment to create valuable solutions
Works with team to ensure all stories are supported, clear, and delivered in high quality
Develops and maintains a deep understanding of the users of the product/platform and applies this in developing proposals and in executing changes
Contribute to the Product/Platform Strategy & Roadmap, highlighting/owning risks, and dependencies
Ensure the readiness of the work (accept or reject work results) and that the work is aligned to the roadmap
Own, maintain and prioritize product/platform backlog; Facilitate feature & user story mapping, break features into stories (functional and non-functional), define acceptance criteria, ensure capacity for enablers and defects
Prioritize and make trade-offs based on understanding of strategies, budget, epics, enabler requirements, defects, dependent teams, goals and the value features and stories bring to NA, North America Supply Chain
Understand how features are performing, the level of customer satisfaction, and provide suggestions for improvements
Provides input to the resource needs and plans for the product/platform
Requirements
Prior experience in Business or Technology role with experiences in Capability Strategy, Product Management, Business Systems Analytics, Data & Analytics or Engineering
Has successfully developed and implemented a system/product/service/feature/for a customer, evaluated its performance, and proposed and executed approaches to continually improve it
Has successfully analyzed the outcome of a landscape change, feature or improvement and developed actionable recommendations
Expertise in both the business/need their product or platform serves, as well as the technology and operational landscape
Expertise in relevant development tools & technology, with the ability to apply that knowledge independently in their work
Has a broad understanding of various software development approaches, including waterfall, agile, scrum, Kanban
Relentless in understanding customer needs/requirements and synthesizing insight through evaluating data, analytics, and user input
Skilled at cross functional alignment, product road mapping and outcomes
Strategic in setting prioritization of the roadmap while being pragmatic in delivering incremental and continuous value (north star in mind while driving continuous value)
Adept at collaborating with and empowering engineering and UX design to enable the right solution for user problems & platform strategy
Ruthless in defining, tracking, and measuring adoption and value; Accountable to the business outcomes (value) over output
Inquisitive, aware, and maintains knowledge of current industry trends and innovations
Has demonstrated the ability to prioritize features and other work in consideration of roadmap, value, and strategy considerations
Experience with Manhattan WMS systems preferred
The pay range is the lowest to highest compensation we reasonably in good faith believe we would pay at posting for this role. We may ultimately pay more or less than this range. Employee pay is based on factors like relevant education, qualifications, certifications, experience, skills, seniority, location, performance, union contract and business needs. This range may be modified in the future.
We offer comprehensive benefits including medical/dental/vision insurance, HSA, FSA, 401(k), and life, disability & ADD insurance to eligible employees. Salaried personnel receive paid time off. Hourly employees are not eligible for paid time off unless required by law. Hourly employees on a Service Contract Act project are eligible for paid sick leave.
Note: Pay is not considered compensation until it is earned, vested and determinable. The amount and availability of any compensation remains in Kforce's sole discretion unless and until paid and may be modified in its discretion consistent with the law.
This job is not eligible for bonuses, incentives or commissions.
Kforce is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.
Salary: $35 - $42 per hour
Show more
Show less","Product Management, Product Road Mapping, Prioritization, Feature Mapping, UserCentric Design, Agile, Scrum, Kanban, Business Systems Analytics, Data Analytics, CrossFunctional Alignment, Waterfall, Manhattan WMS Systems","product management, product road mapping, prioritization, feature mapping, usercentric design, agile, scrum, kanban, business systems analytics, data analytics, crossfunctional alignment, waterfall, manhattan wms systems","agile, business systems analytics, crossfunctional alignment, dataanalytics, feature mapping, kanban, manhattan wms systems, prioritization, product management, product road mapping, scrum, usercentric design, waterfall"
Data Center Technical Operations Engineer I,Kforce Inc,"Seattle, WA",https://www.linkedin.com/jobs/view/data-center-technical-operations-engineer-i-at-kforce-inc-3775446704,2023-12-17,Summit,United States,Associate,Onsite,"Responsibilities
Kforce has a client in Sterling, VA seeking a Data Center Technical Operations Engineer. Responsibilities:
In this role, the Data Center Technical Operations Engineer is responsible for the on-site management of shift technicians, senior shift technicians, sub-contractors and vendors, ensuring that all work performed is in accordance with established practices and procedures
Establish performance benchmarks, conduct analyses, and prepare reports on all aspects of the critical facility operations and maintenance
Work with IT managers and other business leaders to coordinate projects, manage capacity, and optimize plant safety, performance, reliability and efficiency
The Data Center Technical Operations Engineer is operate and manage both routine and emergency services on a variety of critical systems such as: switchgear, generators, UPS systems, power distribution equipment, chillers, cooling towers, computer room air handlers, building monitoring systems, etc.
May assist in the design and build out of new facilities
May assist in projects to increase current facility efficiency
Responsible for asset and inventory management
Assist in recruiting efforts
As a Data Center Technical Operations Engineer, you will deliver quality service and ensure all customer demands are met
Requirements
2 years of experience
2 years of Mechanical and/or electrical troubleshooting experience
2 years of Mechanical and/or Electrical Operations experience
Familiar with Procedures in this field
Mechanical and/or Electrical background
Familiar with and Utilize building management systems
Able to lift up to 40 lbs
Able to walk around job sites
Self-starter
Motivated to improve
Computer experience - proficient
Experience with ticket queues
The pay range is the lowest to highest compensation we reasonably in good faith believe we would pay at posting for this role. We may ultimately pay more or less than this range. Employee pay is based on factors like relevant education, qualifications, certifications, experience, skills, seniority, location, performance, union contract and business needs. This range may be modified in the future.
We offer comprehensive benefits including medical/dental/vision insurance, HSA, FSA, 401(k), and life, disability & ADD insurance to eligible employees. Salaried personnel receive paid time off. Hourly employees are not eligible for paid time off unless required by law. Hourly employees on a Service Contract Act project are eligible for paid sick leave.
Note: Pay is not considered compensation until it is earned, vested and determinable. The amount and availability of any compensation remains in Kforce's sole discretion unless and until paid and may be modified in its discretion consistent with the law.
This job is not eligible for bonuses, incentives or commissions.
Kforce is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.
Salary: $28 - $34 per hour
Show more
Show less","Data Center Operations, Mechanical Troubleshooting, Electrical Troubleshooting, Building Management Systems, Ticket Queues, Mechanical Engineering, Electrical Engineering, Asset Management, Inventory Management, Computer Proficiency","data center operations, mechanical troubleshooting, electrical troubleshooting, building management systems, ticket queues, mechanical engineering, electrical engineering, asset management, inventory management, computer proficiency","asset management, building management systems, computer proficiency, data center operations, electrical engineering, electrical troubleshooting, inventory management, mechanical engineering, mechanical troubleshooting, ticket queues"
Data Scientist,Kforce Inc,"Redmond, WA",https://www.linkedin.com/jobs/view/data-scientist-at-kforce-inc-3775445892,2023-12-17,Summit,United States,Associate,Onsite,"Responsibilities
Kforce has a client in Redmond, WA that is seeking a Data Scientist. Typical Day in the Role:
Purpose of the Team - Part of the infrastructure team that creates the networking/storage/and internal product services are built on
To help create insights for the business organization to inform on business decisions
Key projects - Customer Segmentation, Sentiment Analysis, and Clustering Algorithms
Typical Task Breakdown And Operating Rhythm
Expert in latest AI modeling techniques, including Clustering algorithms, Natural Language Processing, etc.
Work with senior management, technical and client teams in order to determine data requirements, business data implementation approaches, best practices for advanced data manipulation, storage and analysis strategies
Write and code logical and physical database descriptions and specify identifiers of database to management system or direct others in coding descriptions
Design, implement, automate, and maintain large scale enterprise data ETL processes
Modify existing databases and database management systems and/or direct programmers and analysts to make changes
Test programs or databases, correct errors and make necessary modifications
Compelling Story & Candidate Value Proposition:
What makes this role interesting - Have a lot of visibility and opportunity to have breakthroughs and get visibility from stakeholders globally; A lot of satisfaction that comes out of doing this role well
Requirements
Bachelor's degree in a technical field such as Computer Science, Computer Engineering or related field required
8-10 years of applicable experience required
6+ years of experience with AI Machine learning as a Data Scientist, and create trends/analysis about the business to help the org
6+ years of experience with Clustering, Algorithms, Run Large Scale Models (to help create insights for the business org) to inform on business decisions
6+ years of experience with Customer segmentation, sentiment analysis, AI Machine Learning, AI Modeling
6+ years of experience with Full Stack Development
Disqualifiers - AI Experience is a must; Not just theoretical knowledge but hands on experience on Clustering, Algorithms
Best vs. Average:
Experience with database technologies
Knowledge of the ETL process
Knowledge of at least one scripting language
Strong written and oral communication skills
Strong troubleshooting and problem-solving skills
Self starter
Demonstrated history of success
Desire to be working with data and helping businesses make better data driven decisions
Performance Indicators - Drive and Output, Quality of Work
Hard Skills Assessments
Expected Dates - ASAP
Assessment Process - One 45 minute HAS
Candidate Preparation - Come prepared with work examples in the AI space, and how that has been combined with other development skills to create meaningful business impacts
The pay range is the lowest to highest compensation we reasonably in good faith believe we would pay at posting for this role. We may ultimately pay more or less than this range. Employee pay is based on factors like relevant education, qualifications, certifications, experience, skills, seniority, location, performance, union contract and business needs. This range may be modified in the future.
We offer comprehensive benefits including medical/dental/vision insurance, HSA, FSA, 401(k), and life, disability & ADD insurance to eligible employees. Salaried personnel receive paid time off. Hourly employees are not eligible for paid time off unless required by law. Hourly employees on a Service Contract Act project are eligible for paid sick leave.
Note: Pay is not considered compensation until it is earned, vested and determinable. The amount and availability of any compensation remains in Kforce's sole discretion unless and until paid and may be modified in its discretion consistent with the law.
This job is not eligible for bonuses, incentives or commissions.
Kforce is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.
Salary: $50 - $65 per hour
Show more
Show less","Data Science, AI/Machine Learning, Clustering Algorithms, Natural Language Processing, Customer Segmentation, Sentiment Analysis, ETL, Database Management Systems, Data Manipulation, Storage and Analysis Strategies, Full Stack Development, Scripting, Troubleshooting, ProblemSolving, SQL","data science, aimachine learning, clustering algorithms, natural language processing, customer segmentation, sentiment analysis, etl, database management systems, data manipulation, storage and analysis strategies, full stack development, scripting, troubleshooting, problemsolving, sql","aimachine learning, clustering algorithms, customer segmentation, data manipulation, data science, database management systems, etl, full stack development, natural language processing, problemsolving, scripting, sentiment analysis, sql, storage and analysis strategies, troubleshooting"
Data Engineer,People Tech Group Inc,"Seattle, WA",https://www.linkedin.com/jobs/view/data-engineer-at-people-tech-group-inc-3782581461,2023-12-17,Summit,United States,Mid senior,Onsite,"Title: Sr. Data Engineer
Duration: Fulltime
Location: Seattle, WA (Onsite from day one)
Job Responsibilities:
• Need Senior Data engineer
• With Dynamo, DB, Python , Bigdata, Strong on Advance sql, AWS, Apache airflow, redshift, Glue, Informatica.
• Nice to have : Data warehousing, operation.
• Experience with EMR, Pyspark, Python, Coding, AWS
Show more
Show less","Data Engineering, Dynamo, SQL, AWS, Apache Airflow, Redshift, Glue, Informatica, Data Warehousing, EMR, PySpark, Coding","data engineering, dynamo, sql, aws, apache airflow, redshift, glue, informatica, data warehousing, emr, pyspark, coding","apache airflow, aws, coding, data engineering, datawarehouse, dynamo, emr, glue, informatica, redshift, spark, sql"
Data Engineer (FullTime),People Tech Group Inc,"Redmond, WA",https://www.linkedin.com/jobs/view/data-engineer-fulltime-at-people-tech-group-inc-3784907607,2023-12-17,Summit,United States,Mid senior,Onsite,"Role: Senior Data Engineer
Location: Redmond, WA (onsite only)
Duration: Full-time with People Tech Group.
Only full-time or W2. No C2C or C2H.
Description
We are seeking a highly skilled and experienced Data Engineer to join our dynamic team in the computer software industry. As a Data Engineer, you will play a critical role in designing and implementing data systems to extract, transform, and load data from various sources, ensuring the availability and accuracy of our organization's data assets. Your expertise in data modeling, database management, and data integration will be pivotal in driving data-driven decision making and empowering our teams to deliver exceptional products and solutions.
As a Data Engineer, you will collaborate closely with cross-functional teams, including data scientists, software engineers, and business analysts, to understand data requirements, design data pipelines, and build scalable data solutions. You will be responsible for developing efficient and reliable data processes, implementing data governance practices, and optimizing data workflows. Furthermore, you will work closely with stakeholders to gather data system requirements, provide technical guidance, and ensure timely delivery of data projects.
This position offers a unique opportunity to work with cutting-edge technologies and shape our data infrastructure to drive innovation. If you are a self-motivated and detail-oriented professional with a passion for data, we would love to hear from you.
Responsibilities
Design and implement data systems to extract, transform, and load data from various sources
Collaborate with cross-functional teams to understand data requirements and design data pipelines
Build scalable and reliable data solutions to support business initiatives
Develop and optimize data workflows to ensure data integrity and efficiency
Implement data governance practices to maintain data quality and security
Provide technical guidance and support to stakeholders regarding data system requirements
Troubleshoot and resolve data-related issues to minimize downtime and improve reliability
Requirements
Bachelor's degree in Computer Science, Engineering, or a related field
Proven experience as a Data Engineer or a similar role
Strong proficiency in SQL and experience with relational and NoSQL databases
Proficiency in ETL (Extract, Transform, Load) processes and tools
Experience with data modeling and data integration techniques
Knowledge of data management best practices and data governance frameworks
Familiarity with cloud platforms such as AWS or Azure
Strong problem-solving skills and ability to work in a fast-paced environment
Show more
Show less","Data Engineer, Data Systems, Data Modeling, Database Management, Data Integration, Datadriven Decision Making, CrossFunctional Collaboration, Data Scientists, Software Engineers, Business Analysts, Data Requirements, Data Pipelines, Scalable Data Solutions, Efficient Data Processes, Data Governance Practices, Data Workflows, Stakeholder Engagement, Technical Guidance, Data System Requirements, Troubleshooting, Datarelated Issues, Downtime, Reliability, Computer Science, Engineering, SQL, Relational Databases, NoSQL Databases, ETL Processes, Data Modeling, Data Integration Techniques, Data Management Best Practices, Data Governance Frameworks, Cloud Platforms, AWS, Azure, ProblemSolving Skills, Fastpaced Environment","data engineer, data systems, data modeling, database management, data integration, datadriven decision making, crossfunctional collaboration, data scientists, software engineers, business analysts, data requirements, data pipelines, scalable data solutions, efficient data processes, data governance practices, data workflows, stakeholder engagement, technical guidance, data system requirements, troubleshooting, datarelated issues, downtime, reliability, computer science, engineering, sql, relational databases, nosql databases, etl processes, data modeling, data integration techniques, data management best practices, data governance frameworks, cloud platforms, aws, azure, problemsolving skills, fastpaced environment","aws, azure, business analysts, cloud platforms, computer science, crossfunctional collaboration, data governance frameworks, data governance practices, data integration, data integration techniques, data management best practices, data requirements, data scientists, data system requirements, data systems, data workflows, database management, datadriven decision making, dataengineering, datamodeling, datapipeline, datarelated issues, downtime, efficient data processes, engineering, etl, fastpaced environment, nosql databases, problemsolving skills, relational databases, reliability, scalable data solutions, software engineers, sql, stakeholder engagement, technical guidance, troubleshooting"
Data Engineer,People Tech Group Inc,"Seattle, WA",https://www.linkedin.com/jobs/view/data-engineer-at-people-tech-group-inc-3787158131,2023-12-17,Summit,United States,Mid senior,Onsite,"Hi,
This is Munaf Technical Recruiter with People Tech Group, wanted to connect with you regarding excellent job opportunity with one of our Client, kindly go through the below JD and let me know your interest,
If interested, please share your updated resume and best time to connect.
Title: Data Engineer
Client: Amazon / Internal client
Location: Seattle, WA (Onsite)
Duration: Fulltime Position
Overview of Position:
Job Opportunity as Data Engineer with People Tech Group
Job description:
Work closely with offsite data engineers & Business Intelligence Engineer to deliver the projects on time.
Clarify the requirements and guide the offsite team with all the technical details.
Interface with technology teams to extract, transform, and load data from a wide variety of data sources using SQL/python and AWS big data technologies.
Designing and implementing complex ETL pipelines and other BI solutions.
Basic Qualification:
5+ years of data engineering experience
Leading teams, gathering requirements
Experience with data modeling, warehousing, and building ETL pipelines.
Knowledge of distributed systems as it pertains to data storage and computing.
Show more
Show less","Data Engineering, SQL, Python, AWS Big Data Technologies, ETL Pipelines, Data Modeling, Warehousing, Distributed Systems, Data Storage, Data Computing","data engineering, sql, python, aws big data technologies, etl pipelines, data modeling, warehousing, distributed systems, data storage, data computing","aws big data technologies, data computing, data engineering, data storage, datamodeling, datawarehouse, distributed systems, etl pipelines, python, sql"
Data Engineer,SID Global Solutions,"Bellevue, WA",https://www.linkedin.com/jobs/view/data-engineer-at-sid-global-solutions-3775484131,2023-12-17,Summit,United States,Mid senior,Onsite,"Position
: Data Engineer
Exp level
: 4+ years
Location :
Bellevue, Washington
Skillset
: SQL, AWS Stack, Python, Redshift, MYSQL
Roles & Responsibilities:
Require applicant to have hands on experience of knowledge of any Database. But prefer MySQL & Redshift
Hands on Python programming.
Working knowledge on S3
AWS certification is a nice to have.
Must be punctual and follow deadlines and deliver on time.
Must be able to clearly communicate with stakeholders.
Show more
Show less","SQL, AWS Stack, Python, Redshift, MySQL, S3, AWS certification","sql, aws stack, python, redshift, mysql, s3, aws certification","aws certification, aws stack, mysql, python, redshift, s3, sql"
Data Engineer,People Tech Group Inc,"Seattle, WA",https://www.linkedin.com/jobs/view/data-engineer-at-people-tech-group-inc-3747655980,2023-12-17,Summit,United States,Mid senior,Onsite,"Sr. Data Engineer Location - Seattle, WA
Full Time
Please apply Only 8+ years and H1B transfer allows and GC & Citizens (Willing to Relocate to Seattle)
Need Skills ETL, Big data with (AWS OR Azure)
Please share your updated resume to manikanta.potla@peopletech.com
As a Data Engineer, you will be working in one of the world's largest and most complex data Lake and warehouse environments. You will design, implement, and support scalable data infrastructure solutions to integrate with multi heterogeneous data sources, aggregate and retrieve data in a fast and safe mode, curate data that can be used in reporting, analysis, machine learning models and ad-hoc data requests. You will be exposed to cutting edge AWS big data technologies. This will be a great fit if you get excited by partnering with key stakeholders to dig deep into the business challenges to understand and identify insights that will enable us to meet associate needs at a globally scale. You need excellent analytical abilities as well as business acumen and an ability to tie together from across Amazon to help enable effective decision making. An ability to communicate insight to the business via development and implementation of ongoing and subject-specific reporting artifacts is a critical skill in this role.
Key job responsibilities
Work closely with offsite data engineers & Business Intelligence Engineer to deliver the projects on time.
Clarify the requirements and guide the offsite team with all the technical details.
Interface with technology teams to extract, transform, and load data from a wide variety of data sources using SQL/python and AWS big data technologies.
Designing and implementing complex ETL pipelines and other BI solutions.
Basic Qualification
5+ years of data engineering experience
Leading teams, gathering requirements
Experience with data modeling, warehousing, and building ETL pipelines.
Knowledge of distributed systems as it pertains to data storage and computing.
Thanks & Regards
Manikanta
Show more
Show less","Data Engineering, ETL, Big Data, AWS, Azure, SQL, Python, Data Modeling, Warehousing, ETL Pipelines, Distributed Systems","data engineering, etl, big data, aws, azure, sql, python, data modeling, warehousing, etl pipelines, distributed systems","aws, azure, big data, data engineering, datamodeling, datawarehouse, distributed systems, etl, etl pipelines, python, sql"
Data Engineer,People Tech Group Inc,"Redmond, WA",https://www.linkedin.com/jobs/view/data-engineer-at-people-tech-group-inc-3775617676,2023-12-17,Summit,United States,Mid senior,Onsite,"Job Role: Data Engineer
Location: Seattle, WA (onsite)
Duration: Full time
Skills: AWS Glue / EMR, Pyspark, SQLs
Job Description:
5-10+ years’ experience in Data Engineering and SQL.
Experience working with Amazon Web Services (AWS) including Redshift and S3.
Deep experience in Python/R, Spark, and Scala.
Experience using databases in a business environment with big data technologies and large-scale, complex datasets.
Proven ability to solve complex quantitative business challenges.
Verbal/written communication & data presentation skills, including experience to effectively communicate with both business and technical teams.
Master’s degree in business, Engineering, Statistics, Computer Science, Mathematics, or related field.
Show more
Show less","AWS Glue, AWS EMR, PySpark, SQL, Python, R, Spark, Scala, Redshift, S3, Big data technologies, Complex datasets, Business intelligence, Data communication, Data presentation, Statistics, Computer science, Mathematics","aws glue, aws emr, pyspark, sql, python, r, spark, scala, redshift, s3, big data technologies, complex datasets, business intelligence, data communication, data presentation, statistics, computer science, mathematics","aws emr, aws glue, big data technologies, business intelligence, complex datasets, computer science, data communication, data presentation, mathematics, python, r, redshift, s3, scala, spark, sql, statistics"
"Software Engineer, Data Warehouse Analytics",Stripe,"Seattle, WA",https://www.linkedin.com/jobs/view/software-engineer-data-warehouse-analytics-at-stripe-3785923333,2023-12-17,Summit,United States,Mid senior,Onsite,"Who we are
About Stripe
Stripe is a financial infrastructure platform for businesses. Millions of companies—from the world’s largest enterprises to the most ambitious startups—use Stripe to accept payments, grow their revenue, and accelerate new business opportunities. Our mission is to increase the GDP of the internet, and we have a staggering amount of work ahead. That means you have an unprecedented opportunity to put the global economy within everyone’s reach while doing the most important work of your career.
About The Team
The team is responsible for offering data warehouse analytics capabilities for Stripe and the stack is supported by a collection of internally developed services and several popular open source technologies like Trino/Presto, Hive Metastore, etc. The systems we own support all of the data warehouse workloads initiated by both services and by individual Stripes across the company to support various business analytics use cases. We process hundreds of thousands of queries on petabytes of data per day and form the key data serving layer for one of the largest financial data lakes in the world. The team is distributed across the USA.
What you’ll do
You shall be a key contributor to the evolution of our platform and how Stripe analyzes its vast and ever growing data warehouse, to power ever evolving use-cases around analytics, reporting, fraud, AI/ML training, etc at scale. As a Software Engineer, you’ll be empowered to make decisions with a significant impact on Stripe while making our systems reliable, secure, and a delight to use.
Responsibilities
Scope and lead large technical projects with enormous impact for Engineers within Stripe
Build and maintain the infrastructure which powers the core of Stripe.
Directly contribute to core interface design and write code.
Work closely with the open source community to identify opportunities for adopting new open source features as well contribute back to the OSS.
Plan for the growth of Stripe’s infrastructure. Unblock, support and communicate with internal partners to achieve results.
Ensure operational excellence and enable a highly available, reliable and secure Data Warehouse Analytics platform.
Who you are
We’re looking for someone who meets the minimum requirements to be considered for the role. If you meet these requirements, you are encouraged to apply. The preferred qualifications are a bonus, not a requirement.
Minimum Requirements
BS or MS in Computer Science or equivalent field and interest in Data.
2-7 years of professional experience writing high quality production level code or software programs.
Have experience with distributed SQL query engines like Trino.
Experience developing, maintaining and debugging distributed systems built with open source tools.
Experience building infrastructure as a product centered around user needs.
Experience optimizing the end to end performance of distributed systems.
Experience with scaling distributed systems in a rapidly moving environment.
Preferred Qualifications
Experience coding in Java, Scala, Golang.
Experience working with distributed SQL query engine space.
Familiarity designing APIs or building developer platforms.
Familiarity with cloud-based environments such as AWS.
Pay and benefits
The annual US base salary range for this role is $172,800 - $233,800. For sales roles, the range provided is the role’s On Target Earnings (""OTE"") range, meaning that the range includes both the sales commissions/sales bonuses target and annual base salary for the role. This salary range may be inclusive of several career levels at Stripe and will be narrowed during the interview process based on a number of factors, including the candidate’s experience, qualifications, and location. Applicants interested in this role and who are not located in the US may request the annual salary range for their location during the interview process.
Additional benefits for this role may include: equity, company bonus or sales commissions/bonuses; 401(k) plan; medical, dental, and vision benefits; and wellness stipends.
Show more
Show less","Data Analytics, SQL, Trino, Hive, Metastore, Java, Scala, Go, Distributed SQL query engines, Open source tools, Cloudbased environments, AWS, APIs, Developer platforms","data analytics, sql, trino, hive, metastore, java, scala, go, distributed sql query engines, open source tools, cloudbased environments, aws, apis, developer platforms","apis, aws, cloudbased environments, dataanalytics, developer platforms, distributed sql query engines, go, hive, java, metastore, open source tools, scala, sql, trino"
Azure Database Engineer,Persistent Systems,"Seattle, WA",https://www.linkedin.com/jobs/view/azure-database-engineer-at-persistent-systems-3774704520,2023-12-17,Summit,United States,Mid senior,Onsite,"About Persistent
We are a trusted Digital Engineering and Enterprise Modernization partner, combining deep technical expertise and industry experience to help our clients anticipate what’s next. Our offerings and proven solutions create a unique competitive advantage for our clients by giving them the power to see beyond and rise above. We work with many industry-leading organizations across the world including 14 of the 30 most innovative US companies, 80% of the largest banks in the US and India, and numerous innovators across the healthcare ecosystem.
Our disruptor’s mindset, commitment to client success, and agility to thrive in the dynamic environment have enabled us to cross $1 billion in revenue for FY23 (+35.3% Y-o-Y). Along with our growth, we’ve onboarded close to 4000 new employees in the past year, bringing our total employee count to over 22,750 people located in 21 countries across the globe. Throughout this market-leading growth, we’ve maintained strong employee satisfaction - over 94% of our employees approve of the CEO and 84% recommend working at Persistent to a friend.
About Position
Title-Azure Database Engineer
Job Location: Seattle, WA (Work from office)
Experience: 5+ years
Job Type: Full Time
Expertise You'll Bring
5-7 years of experience in the IT industry in Backend\database development
Strong experience in data engineering, with technical skills in Python, Azure Data Factory, Azure Databricks, and Azure SQL Server.
Knowledge of data modeling, data warehousing, and ETL/ELT processes.
Experience or knowledge of Azure Synapse is an advantage.
Experience with Agile methodologies and DevOps practices.
Strong analytical and problem-solving skills, with the ability to design and implement complex data processing solutions.
Good communication and collaboration skills, with the ability to work effectively with cross-functional teams
Preferred
The candidate should be a Citizen. Seattle, WA is the preferred location.
The candidate should be ready to work from the Persistent office every day.
Benefits
Competitive salary and benefits package
A culture focused on talent development with quarterly promotion cycles and company-sponsored higher education and certifications
Opportunity to work with cutting-edge technologies
Employee engagement initiatives such as project parties, flexible work hours, and Long Service awards
Annual health check-ups
Insurance coverage: group term life, personal accident, and Mediclaim hospitalization for self, spouse, two children, and parents
Our company fosters a values-driven and people-centric work environment that enables our employees to:
Accelerate growth, both professionally and personally
Impact the world in powerful, positive ways, using the latest technologies
Enjoy collaborative innovation, with diversity and work-life wellbeing at the core
Unlock global opportunities to work and learn with the industry’s best
Let's unleash your full potential. See Beyond, Rise Above
Show more
Show less","Python, Azure Data Factory, Azure Databricks, Azure SQL Server, Data Modeling, Data Warehousing, ETL/ELT, Azure Synapse, Agile, DevOps, Data Processing, Communication, Collaboration","python, azure data factory, azure databricks, azure sql server, data modeling, data warehousing, etlelt, azure synapse, agile, devops, data processing, communication, collaboration","agile, azure data factory, azure databricks, azure sql server, azure synapse, collaboration, communication, data processing, datamodeling, datawarehouse, devops, etlelt, python"
"Software Engineer, Data Platform",Lyft,"Seattle, WA",https://www.linkedin.com/jobs/view/software-engineer-data-platform-at-lyft-3716407381,2023-12-17,Summit,United States,Mid senior,Onsite,"At Lyft, our mission is to improve people’s lives with the world’s best transportation. To do this, we start with our own community by creating an open, inclusive, and diverse organization.
The Data Orchestration team within Data Platform is responsible for managing orchestration engines including Airflow and Flyte, which power and accelerate data pipelines and machine learning processes at the scale required by Lyft products. These components support a variety of use cases, including but not limited to Core Dataset, Financial Infra, Pricing, Estimated Time of Arrivals (ETA), and Mapping. The team's clusters host and serve thousands of workflows and over 100,000 executions on a daily basis.
As a Software Engineer, with your technical expertise you will manage project priorities, deadlines, and deliverables. You will design, develop, test, deploy, maintain, and enhance the platform offerings. Your work will have a major impact on several areas of the business.
We are looking for candidates who are self starters and have a proven track record of delivering software solutions that can solve critical business needs. The candidate should be able to dive deep into any problems with lots of ambiguity and build a technical solution to solve it. They should be willing to take ownership of a project or a feature and be able to drive it from design to implementation.
Responsibilities:
Design, develop, deploy, monitor, operate and maintain existing or new elements of our platform
Help establish roadmap and architecture based on technology and our needs
Write well-crafted, well-tested, readable, maintainable code
Analyze our internal systems and processes and locate areas for improvement/automation
Collaborate with product org stakeholders to address and prioritize custom edge cases
Help lead large projects from inception to positive execution
Unblock, support and communicate with internal partners to achieve results
Experience:
3+ years of software engineering industry experience and with data structures/algorithms
2+ years of experience building and developing large-scale infrastructure, distributed systems or networks, and/or experience with data infrastructure
Experience working with kubernetes and container technologies (e.g. Docker, cri-o, etc)
Familiar with a cloud-based environments such as AWS/GCP/Azure
Benefits:
Great medical, dental, and vision insurance options
Mental health benefits
Family building benefits
In addition to 12 observed holidays, salaried team members have unlimited paid time off, hourly team members have 15 days paid time off
401(k) plan to help save for your future
18 weeks of paid parental leave. Biological, adoptive, and foster parents are all eligible
Pre-tax commuter benefits
Lyft Pink - Lyft team members get an exclusive opportunity to test new benefits of our Ridership Program
Lyft is an equal opportunity/affirmative action employer committed to an inclusive and diverse workplace. All qualified applicants will receive consideration for employment without regards to race, color, religion, sex, sexual orientation, gender identity, national origin, disability status, protected veteran status or any other basis prohibited by law. We also consider qualified applicants with criminal histories consistent with applicable federal, state and local law.
Starting in September 2023, this role will be in-office on a hybrid schedule — Team Members will be expected to work in the office 3 days per week on Mondays, Thursdays and a team-specific third day. Additionally, hybrid roles have the flexibility to work from anywhere for up to 4 weeks per year.
Candidates for this role must be based in the Seattle metro area.
The expected range of pay for this position in the Seattle area is $103,785 - $165,600. Salary ranges are dependent on a variety of factors, including qualifications, experience and geographic location. Range is not inclusive of potential equity offering, bonus or benefits. Your recruiter can share more information about the salary range specific to your working location and other factors during the hiring process.
Show more
Show less","Software Engineering, Project Management, Data Orchestration, Airflow, Flyte, Kubernetes, Docker, crio, AWS, GCP, Azure, Data Structures, Algorithms, Distributed Systems, Networks, Data Infrastructure","software engineering, project management, data orchestration, airflow, flyte, kubernetes, docker, crio, aws, gcp, azure, data structures, algorithms, distributed systems, networks, data infrastructure","airflow, algorithms, aws, azure, crio, data infrastructure, data orchestration, data structures, distributed systems, docker, flyte, gcp, kubernetes, networks, project management, software engineering"
Senior Data Platform Engineer,"Verticalmove, Inc","Kirkland, WA",https://www.linkedin.com/jobs/view/senior-data-platform-engineer-at-verticalmove-inc-3764341125,2023-12-17,Summit,United States,Mid senior,Onsite,"Verticalmove is a member of Inc Magazine 2023 fastest-growing private companies in America!
We build digital transformation, product, and software engineering teams! We help our clients achieve successful digital transformations, and talented professionals reach their optimal progression throughout their careers.
Our portfolio of clients includes start-ups financed by the most exclusive venture capital firms and established Fortune 500 companies such as Salesforce.com, American Express, CVS Health, United Healthcare, Sony Interactive, and many others.
JOB DESCRIPTION:
As a Senior Data Platform Engineer, you will take on a pivotal role in our organization, responsible for overseeing the development, maintenance, and optimization of our data infrastructure. You will work closely with Data Science and Machine Learning teams, playing a crucial role in acquiring, transforming, and centralizing raw data into structured formats. This position is tailored for individuals passionate about Data Operations and Machine Learning, bringing a strong engineering background, a desire to stay current with emerging technologies, and excelling in communication, innovation, and collaboration.
Responsibilities:
Lead the construction of data pipelines, architectures, and infrastructure.
Take charge of cleaning and processing datasets to support data science modeling.
Develop and maintain tools to enhance data engineering workflows.
Aggregate and integrate diverse datasets for specific use cases.
Provide guidance and support to Data Scientists with their data-related requirements.
Qualifications:
A minimum of 5 years of relevant experience in software engineering.
Over 3 years of experience with ETL, data modeling, and data architecture.
Proficient in coding with languages such as Python, Java, or similar.
Expertise in SQL, query authoring, and working with relational databases.
Experience with database and storage solutions, e.g., Postgres, GraphQL, Google Cloud Storage, Microsoft Azure.
Strong foundational knowledge in Computer Science and programming.
An understanding of data manipulation, transformation, and preprocessing.
Required Experience:
At least 5 years of experience in Data, BI, or Platform Engineering, Data Warehousing/ETL, or Software Engineering.
A minimum of 4 years of experience in projects involving the implementation of solutions using development life cycles (SDLC).
Required Education:
A Bachelor's Degree in Computer Science, Robotics, Computer Engineering, or a related field (or equivalent work experience in a related domain).
Preferred Education:
A Master's Degree in Computer Science, Robotics, Computer Engineering, Mechanical Engineering, or a related field.
Over 6 years of relevant experience in software engineering.
Show more
Show less","Data Platform Engineering, Data Infrastructure, Data Science, Machine Learning, Data Pipelines, Data Architecture, ETL, Data Modeling, Python, Java, SQL, Relational Databases, Postgres, GraphQL, Google Cloud Storage, Microsoft Azure, Computer Science, Data Manipulation, Data Transformation, Data Preprocessing, Data Warehousing, Development Life Cycles (SDLC)","data platform engineering, data infrastructure, data science, machine learning, data pipelines, data architecture, etl, data modeling, python, java, sql, relational databases, postgres, graphql, google cloud storage, microsoft azure, computer science, data manipulation, data transformation, data preprocessing, data warehousing, development life cycles sdlc","computer science, data architecture, data infrastructure, data manipulation, data platform engineering, data preprocessing, data science, data transformation, datamodeling, datapipeline, datawarehouse, development life cycles sdlc, etl, google cloud storage, graphql, java, machine learning, microsoft azure, postgres, python, relational databases, sql"
ETL Cloud Data Engineer,TekIntegral,"Seattle, WA",https://www.linkedin.com/jobs/view/etl-cloud-data-engineer-at-tekintegral-3676907034,2023-12-17,Summit,United States,Mid senior,Onsite,"Title:
ETL Cloud Data Engineer
Location: Seattle, WA (On-site)
Duration: 6 months
Visa: USC or GC
Rate:60-65/hr C2C
Client Domain:IT Consulting/Services
Responsibilities
Performs repeatable testing procedures and processes.
Participate as primary, co and / or contributing author on all project deliverables associated with their assigned areas of responsibility.
Working closely with client stake holders and business to get the requirement
Working on assigned tasks and delivering on time
Working on ETL Matillion flows, unit testing and Tableau reports creation
Creating snow pipes to load the data from S3 to Snowflake
Reading the data from S3 to load the data into Snowflake using Matillion
Required Qualifications
Ingesting data from various sources - Oracle / Demantra
Data modelling
Good working experience in cloud platforms - Preferably AWS services (S3, Glue, Athena, Lambdas)
Strong experience in SQL / PL / SQL
Cloud Data Warehouse - Snowflake / AWS / Redshift
Cloud ETL - Matillion
Reporting Experience - Tableau
Bachelor's Degree
Experience
Highly desirable:
Experience with Cloud ETL Tools like Matillion
Experience with Cloud Data Warehouses like Snowflake
Visualizing the data in Tableau - Build data sources in Tableau and develop Reports / Dashboards
Show more
Show less","Data Modelling, AWS Services (S3 Glue Athena Lambdas), SQL / PL / SQL, Snowflake, Redshift, Matillion, Tableau, Cloud ETL Tools, Cloud Data Warehouses, Oracle, Demantra, S3, ETL","data modelling, aws services s3 glue athena lambdas, sql pl sql, snowflake, redshift, matillion, tableau, cloud etl tools, cloud data warehouses, oracle, demantra, s3, etl","aws services s3 glue athena lambdas, cloud data warehouses, cloud etl tools, data modelling, demantra, etl, matillion, oracle, redshift, s3, snowflake, sql pl sql, tableau"
"Data Engineer II, Analytics & Data Management",Amazon,"Seattle, WA",https://www.linkedin.com/jobs/view/data-engineer-ii-analytics-data-management-at-amazon-3775430325,2023-12-17,Summit,United States,Mid senior,Onsite,"Description
Amazon Advertising is one of Amazon's fastest growing and most profitable businesses, responsible for defining and delivering a collection of advertising solutions that drive product discovery and sales. We deliver billions of ad impressions every single day on behalf of our advertisers. You'll work with us to help our Advertising teams make sense of the torrent of data produced by the advertising lifecycle.
We are looking for an individual with strong ownership, is passionate about big data technologies, experienced in designing and building data marts, and enjoy working with PB-scale data. In this role, you will work in one of the world's largest and most complex data warehouse environments, and touch data across all Ads businesses and products. With your strong business acumen, you will partner with business owners to understand requirements, design and implement pipelines to ingest data into our PB-scale datalake, and also own data curation. You will also be exposed to cutting edge AWS big data technologies while building complex, high-performing, and scalable data products.
We are open to hiring candidates to work out of one of the following locations:
Seattle, WA, USA
Basic Qualifications
3+ years of data engineering experience
Experience with data modeling, warehousing and building ETL pipelines
Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets
Experience with SQL
Experience in at least one modern scripting or programming language, such as Python, Java, Scala, or NodeJS
Preferred Qualifications
Experience with AWS technologies like Redshift, S3, AWS Glue, EMR, Kinesis, FireHose, Lambda, and IAM roles and permissions
Experience with non-relational databases / data stores (object storage, document or key-value stores, graph databases, column-family databases)
Experience with big data technologies such as: Hadoop, Hive, Spark, EMR
Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.
Our compensation reflects the cost of labor across several US geographic markets. The base pay for this position ranges from $105,700/year in our lowest geographic market up to $205,600/year in our highest geographic market. Pay is based on a number of factors including market location and may vary depending on job-related knowledge, skills, and experience. Amazon is a total compensation company. Dependent on the position offered, equity, sign-on payments, and other forms of compensation may be provided as part of a total compensation package, in addition to a full range of medical, financial, and/or other benefits. For more information, please visit https://www.aboutamazon.com/workplace/employee-benefits. Applicants should apply via our internal or external career site.
Company
- Amazon.com Services LLC
Job ID: A2512372
Show more
Show less","AWS, S3, Python, AWS Glue, EMR, SQL, Kinesis, FireHose, Lambda, IAM, Hadoop, Hive, Spark, Data modeling, Data warehousing, Big data technologies, ETL pipelines, Redshift","aws, s3, python, aws glue, emr, sql, kinesis, firehose, lambda, iam, hadoop, hive, spark, data modeling, data warehousing, big data technologies, etl pipelines, redshift","aws, aws glue, big data technologies, datamodeling, datawarehouse, emr, etl pipelines, firehose, hadoop, hive, iam, kinesis, lambda, python, redshift, s3, spark, sql"
Sr. Data Engineer,Blueprint,"Bellevue, WA",https://www.linkedin.com/jobs/view/sr-data-engineer-at-blueprint-3766037108,2023-12-17,Summit,United States,Mid senior,Remote,"Remote (U.S. Only)
Who is Blueprint?
We are a technology solutions firm headquartered in Bellevue, Washington, with a strong presence across the United States. Unified by a shared passion for solving complicated problems, our people are our greatest asset. We use technology as a tool to bridge the gap between strategy and execution, powered by the knowledge, skills, and the expertise of our teams, who all have unique perspectives and years of experience across multiple industries. We're bold, smart, agile, and fun.
What does Blueprint do?
Blueprint helps organizations unlock value from existing assets by leveraging cutting-edge technology to create additional revenue streams and new lines of business. We connect strategy, business solutions, products, and services to transform and grow companies.
Why Blueprint?
At Blueprint, we believe in the power of possibility and are passionate about bringing it to life. Whether you join our bustling product division, our multifaceted services team or you want to grow your career in human resources, your ability to make an impact is amplified when you join one of our teams. You'll focus on solving unique business problems while gaining hands-on experience with the world's best technology. We believe in unique perspectives and build teams of people with diverse skillsets and backgrounds. At Blueprint, you'll have the opportunity to work with multiple clients and teams, such as data science and product development, all while learning, growing, and developing new solutions. We guarantee you won't find a better place to work and thrive than at Blueprint.
What will I be doing?
Blueprint is looking for a
Sr. Data Engineer
to join us as we build cutting-edge technology solutions! The ideal candidate will have a solid background in consulting, with demonstrated experience leading clients through the process of building modern data estates. As a Principal Data Engineer, you will spend a majority of your time working directly with clients to develop their advanced modern data estates, warehouses, and analytical environments. You will also be responsible for overseeing and mentoring junior developers within the organization.
Responsibilities:
Develop and implement effective data architecture solutions using Databricks and Lakehouse
Optimize and tune data pipelines for performance and scalability
Monitor and troubleshoot data pipelines to ensure data availability and reliability
Implement best practices for data governance, data security, and data quality to ensure data integrity across all data sources
Create and maintain documentation related to data architecture, data pipelines, and data models
Stay up to date with emerging technologies and best practices in data engineering and big data processing
Mentor and train other data engineers on best practices for data engineering and Databricks usage
Provide thought leadership in the Databricks and Lakehouse space, both within the organization and externally
Design, build, implement and maintain our data infrastructure to power analytics and ML
Contribute to our investments into various open-source and 3rd party tools to build a system that scales with the company
Collaborate with our Data Scientists and Analytics Engineers to make pipeline implementation faster, more straightforward, and more trustworthy driven decisions that will shape our business
Qualifications:
Bachelor's or Master's degree in Computer Science, Computer Engineering, or a related field
8+ years of experience in data engineering
3+ years of experience working with Databricks and PySpark
6-8+ years of experience with SQL
Appreciation for the Lakehouse medallion data architecture – bronze, silver, gold – and how those data stages are used
Working knowledge of DLT(Delta Live Tables) and Unity Catalog a plus
Strong understanding of ETL and ELT data ingestion, acquisition, and data processing patterns
Experience with cloud-based data warehousing platforms such as Synapse, AWS Redshift, Google BigQuery, or Snowflake
Strong understanding of data engineering, data warehousing, data modeling, data governance, and data security best practices
Excellent problem-solving and troubleshooting skills
Strong communication and collaboration skills, with the ability to work effectively in a team environment
Experience mentoring and training other data engineers
Salary Range
Pay ranges vary based on multiple factors including, without limitation, skill sets, education, responsibilities, experience, and geographical market. The pay range for this position reflects geographic based ranges for Washington state: $146,400 to $175,100 USD/annually. The salary/wage and job title for this opening will be based on the selected candidate's qualifications and experience and may be outside this range.
Equal Opportunity Employer
Blueprint Technologies, LLC is an equal employment opportunity employer. Qualified applicants are considered without regard to race, color, age, disability, sex, gender identity or expression, orientation, veteran/military status, religion, national origin, ancestry, marital, or familial status, genetic information, citizenship, or any other status protected by law.
If you need assistance or a reasonable accommodation to complete the application process, please reach out to: recruiting@bpcs.com
Blueprint believe in the importance of a healthy and happy team, which is why our comprehensive benefits package includes:
Medical, dental, and vision coverage
Flexible Spending Account
401k program
Competitive PTO offerings
Parental Leave
Personal paid Volunteer time to support our community
Opportunities for professional growth and development
Location:
Remote
Show more
Show less","Databricks, Lakehouse, Data engineering, SQL, ETL, ELT, Synapse, AWS Redshift, Google BigQuery, Snowflake, Data modeling, Data governance, Data quality, Cloudbased, Hadoop, Hive, Spark, PySpark, Python, Java, Scala","databricks, lakehouse, data engineering, sql, etl, elt, synapse, aws redshift, google bigquery, snowflake, data modeling, data governance, data quality, cloudbased, hadoop, hive, spark, pyspark, python, java, scala","aws redshift, cloudbased, data engineering, data governance, data quality, databricks, datamodeling, elt, etl, google bigquery, hadoop, hive, java, lakehouse, python, scala, snowflake, spark, sql, synapse"
"Senior Search Engineer, Data Platform",Pagoda,"Seattle, WA",https://www.linkedin.com/jobs/view/senior-search-engineer-data-platform-at-pagoda-3689444365,2023-12-17,Summit,United States,Mid senior,Remote,"About Pagoda
Pagoda is shepherding a future where NEAR becomes the blockchain operating system. We believe that re-inventing how software is made and distributed is our greatest opportunity to open economic access to those who are not fully integrated into the global economy. Our products empower people to find opportunity, invent new experiences, and collaborate. Let's build an Open Web world. A world where people control their assets, data, and power of governance.
About The Role
The Data Platform team at Pagoda is building the search experience on near.org, allowing users and developers to quickly discover content, applications built on different blockchains, and search code examples. We focus on building scalable, performant, and reliable infrastructure to allow developers to add their own indexes to our search engine and increase engagement on near.org.
What You'll Be Doing
Collaborate with a fully distributed team around the world;
Develop extensible search capabilities on near.org for on-chain and off-chain content;
Develop frameworks for software engineers to build search plugins;
Build TypeScript-based indexers to power search and other APIs;
Make Performance optimizations and scalability/reliability improvements;
Follow the minimal processes, like technical roadmaps, OKRs, milestones, regular team meetings;
Participate in on-call rotations.
What We're Looking For
Hands-on experience building search experiences using ElasticSearch, Algolia or other Search-as-a-Service products is a must.
Understanding data processing and storage solutions and ability to find balance between cost, performance and scalability.
Experience building complex analytical SQL queries;
Ability to quickly learn new programming languages and technologies.
Creativity to come up with proof of concept from high level design while not knowing all the details;
Ability to write clean yet efficient code in a variety of programming languages and technologies, including TypeScript, Rust, SQL, Python;
Effective communication skills to convey your ideas and discuss with team members for technical challenges.
We'd Love If You Have
Open source contributions and previous or continued involvement in open source communities;
Experience setting up continuous integration pipelines, observability tools and metrics;
Familiarity with Ethereum development and related technologies (web3.js, ethers.js, etc)
Familiarity with other crypto or blockchain technologies;
Experience working at a startup.
Here's What Our Interview Process Looks Like
Our interviews take place via Zoom and typically consists of the following stages:
Recruiter Call
First Round:
Algorithms & Coding Interview
Systems Design Interview
Final Round
Hiring Manager Interview
SQL Interview
Pagoda Values Interview
Compensation
The base salary range for this role is $148,750 - $195,000, which represents the salary range applicable to US locations only. This does not include bonus, incentives, or benefits.
The actual base pay within the range is dependent upon many factors, including: leveling, relevant skills, and work location. If you are based outside of the US, there are other geographic considerations that may impact your final compensation. Your recruiter can share more about the compensation and benefits applicable to your preferred location during the hiring process.
Benefits & Perks
Encouraged 20 days of flexible PTO per year, plus your local holidays
Wellness weeks – 2 weeks of paid company-wide closures
100% Paid medical, dental and vision, AD&D and life insurance for US employees, including 85% coverage for dependents, and HSA + FSA options; For non-US employees, 100% Paid private medical coverage available at the highest tiered plan
Access to licensed therapists and mental health resources through Spill, 100% confidential and paid by Pagoda; plus $75 monthly reimbursement for wellness
Generous parental leave options; All employees have access to $10,000 in fertility assistance through Carrot
For US employees, 401(k) retirement plan available (no match)
Annual company retreats and team offsites (2023 was in Spain; 2022 in Portugal)
$2,000 Continued Education Reimbursement
$2,000 Home Office Reimbursement
Co-working Space Reimbursement
Our Values at Pagoda
Our values express our company culture. Learn more on our careers page.
Pagoda is an Equal Employment Opportunity (EEO) employer and welcomes all qualified applicants. Applicants will receive fair and impartial consideration without regard to race, sex, color, religion, national origin, age, disability, veteran status, genetic data, or other legally protected status.
Show more
Show less","NEAR, blockchain, ElasticSearch, Algolia, TypeScript, Rust, SQL, Python, web3.js, ethers.js, Ethereum, continuous integration, observability tools","near, blockchain, elasticsearch, algolia, typescript, rust, sql, python, web3js, ethersjs, ethereum, continuous integration, observability tools","algolia, blockchain, continuous integration, elasticsearch, ethereum, ethersjs, near, observability tools, python, rust, sql, typescript, web3js"
"SR. Scala Engineer, Database Engineering",Experfy,"Seattle, WA",https://www.linkedin.com/jobs/view/sr-scala-engineer-database-engineering-at-experfy-3637340819,2023-12-17,Summit,United States,Mid senior,Remote,"As a Sr. Software Engineer for our Data Platform Engineering team you will join skilled Scala engineers and core database developers responsible for developing hosted cloud analytics infrastructure (Apache Spark-based), distributed SQL
processing frameworks, proprietary data science platforms, and core database optimization. This team is responsible for building the automated, intelligent, and highly performant query planner and execution engines, RPC calls between data
warehouse clusters, shared secondary cold storage, etc. This includes building new SQL features and customer-facing functionality, developing novel query optimization techniques for industry-leading performance, and building a database
system that's highly parallel, efficient and fault-tolerant. This is a vital role reporting to exec leadership and senior engineering leadership.
Requirements
Responsibilities:
Writing Scala code with tools like Apache Spark + Apache Arrow to build a hosted, multi-cluster data warehouse for Web3
Developing database optimizers, query planners, query and data routing mechanisms, cluster-to-cluster communication, and workload management techniques
Scaling up from proof of concept to ""cluster scale"" (and eventually hundreds of clusters with hundreds of terabytes each), in terms of both infrastructure/architecture and problem structure
Codifying best practices for future reuse in the form of accessible, reusable patterns, templates, and code bases to facilitate meta data capturing and management
Managing a team of software engineers writing new code to build a bigger, better, faster, more optimized HTAP database (using Apache Spark, Apache Arrow and a wealth of other open source data tools)
Interacting with exec team and senior engineering leadership to define, prioritize, and ensure smooth deployments with other operational components
Highly engaged with industry trends within analytics domain from a data acquisition processing, engineering, management perspective
Understand data and analytics use cases across Web3 / blockchains
Skills & Qualifications
Bachelor's degree in computer science or related technical field. Masters or PhD a plus
6+ years experience engineering software and data platforms / enterprise-scale data warehouses, preferably with knowledge of open source Apache stack (especially Apache Spark, Apache Arrow, and others)
3+ years experience with Scala and Apache Spark
A track record of recruiting and leading technical teams in a demanding talent market
Rock solid engineering fundamentals; query planning, optimizing and distributed data warehouse systems experience is preferred but not required
Nice to have: Knowledge of blockchain indexing, web3 compute paradigms, Proofs and consensus mechanisms... is a strong plus but not required
Experience with rapid development cycles in a web-based environment
Strong scripting and test automation knowledge
Nice to have: Passionate about Web3, blockchain, decentralization, and a base understanding of how data/analytics plays into this
Show more
Show less","Apache Spark, Apache Arrow, Apache Hadoop, Big Data, Data Analytics, Data Engineering, Data Science, Data Warehousing, Database Optimization, Distributed Systems, Hadoop Ecosystem, Programming Languages (Scala Python), Query Optimization, SQL, Data Management, Data Processing, Data Warehousing, Blockchain, Web3","apache spark, apache arrow, apache hadoop, big data, data analytics, data engineering, data science, data warehousing, database optimization, distributed systems, hadoop ecosystem, programming languages scala python, query optimization, sql, data management, data processing, data warehousing, blockchain, web3","apache arrow, apache hadoop, apache spark, big data, blockchain, data engineering, data management, data processing, data science, dataanalytics, database optimization, datawarehouse, distributed systems, hadoop ecosystem, programming languages scala python, query optimization, sql, web3"
Principal Data Engineer,Blueprint,"Bellevue, WA",https://www.linkedin.com/jobs/view/principal-data-engineer-at-blueprint-3766037114,2023-12-17,Summit,United States,Mid senior,Remote,"Remote (U.S. Only)
Who is Blueprint?
We are a technology solutions firm headquartered in Bellevue, Washington, with a strong presence across the United States. Unified by a shared passion for solving complicated problems, our people are our greatest asset. We use technology as a tool to bridge the gap between strategy and execution, powered by the knowledge, skills, and the expertise of our teams, who all have unique perspectives and years of experience across multiple industries. We're bold, smart, agile, and fun.
What does Blueprint do?
Blueprint helps organizations unlock value from existing assets by leveraging cutting-edge technology to create additional revenue streams and new lines of business. We connect strategy, business solutions, products, and services to transform and grow companies.
Why Blueprint?
At Blueprint, we believe in the power of possibility and are passionate about bringing it to life. Whether you join our bustling product division, our multifaceted services team or you want to grow your career in human resources, your ability to make an impact is amplified when you join one of our teams. You'll focus on solving unique business problems while gaining hands-on experience with the world's best technology. We believe in unique perspectives and build teams of people with diverse skillsets and backgrounds. At Blueprint, you'll have the opportunity to work with multiple clients and teams, such as data science and product development, all while learning, growing, and developing new solutions. We guarantee you won't find a better place to work and thrive than at Blueprint.
What will I be doing?
Blueprint is looking for a
Principal Data Engineer
to join us as we build cutting-edge technology solutions! The ideal candidate will have a solid background in consulting, with demonstrated experience leading clients through the process of building modern data estates. As a Principal Data Engineer, you will spend a majority of your time working directly with clients to develop their advanced modern data estates, warehouses, and analytical environments. You will also be responsible for overseeing and mentoring junior developers within the organization.
Responsibilities:
Develop and implement effective data architecture solutions using Databricks and Lakehouse
Develop Real Time Data Processing using Kafka
Optimize and tune data pipelines for performance and scalability
Monitor and troubleshoot data pipelines to ensure data availability and reliability
Collaborate with data scientists, analysts, and other stakeholders to understand their data needs and build solutions that enable them to extract insights from data
Implement best practices for data governance, data security, and data quality to ensure data integrity across all data sources
Create and maintain documentation related to data architecture, data pipelines, and data models
Stay up to date with emerging technologies and best practices in data engineering and big data processing
Mentor and train other data engineers on best practices for data engineering and Databricks usage
Provide thought leadership in the Databricks and Lakehouse space, both within the organization and externally
Qualifications:
Bachelor's or Master's degree in Computer Science, Computer Engineering, or a related field
8+ years of experience in data engineering
3+ years of experience working with Databricks and PySpark
2+ Years of Batch Procesing and Real Time Procesing using Kafka
6-8+ years of experience with SQL
Appreciation for the Lakehouse medallion data architecture – bronze, silver, gold – and how those data stages are used
Knowledge of ADLS Gen1/Gen2 (Azure Data Lake Stroage)
Working knowledge of DLT(Delta Live Tables) and Unity Catalog a plus
Strong understanding of ETL and ELT data ingestion, acquisition, and data processing patterns
Experience with cloud-based data warehousing platforms such as Synapse, AWS Redshift, Google BigQuery, or Snowflake
Knowledge of Airflow a +
Strong understanding of data engineering, data warehousing, data modeling, data governance, and data security best practices
Excellent problem-solving and troubleshooting skills
Strong communication and collaboration skills, with the ability to work effectively in a team environment
Experience mentoring and training other data engineers
Salary Range
Pay ranges vary based on multiple factors including, without limitation, skill sets, education, responsibilities, experience, and geographical market. The pay range for this position reflects geographic based ranges for Washington state: $146,400 to $175,100 USD/annually. The salary/wage and job title for this opening will be based on the selected candidate's qualifications and experience and may be outside this range.
Equal Opportunity Employer
Blueprint Technologies, LLC is an equal employment opportunity employer. Qualified applicants are considered without regard to race, color, age, disability, sex, gender identity or expression, orientation, veteran/military status, religion, national origin, ancestry, marital, or familial status, genetic information, citizenship, or any other status protected by law.
If you need assistance or a reasonable accommodation to complete the application process, please reach out to: recruiting@bpcs.com
Blueprint believe in the importance of a healthy and happy team, which is why our comprehensive benefits package includes:
Medical, dental, and vision coverage
Flexible Spending Account
401k program
Competitive PTO offerings
Parental Leave
Opportunities for professional growth and development
Location:
Remote
Show more
Show less","Databricks, Lakehouse, Kafka, SQL, ADLS Gen1/Gen2 (Azure Data Lake Storage), DLT (Delta Live Tables), Unity Catalog, ETL, ELT, Data warehousing, Data modeling, Data governance, Data security, Airflow, Cloudbased data warehousing platforms, Synapse, AWS Redshift, Google BigQuery, Snowflake, Python, Spark, SQL","databricks, lakehouse, kafka, sql, adls gen1gen2 azure data lake storage, dlt delta live tables, unity catalog, etl, elt, data warehousing, data modeling, data governance, data security, airflow, cloudbased data warehousing platforms, synapse, aws redshift, google bigquery, snowflake, python, spark, sql","adls gen1gen2 azure data lake storage, airflow, aws redshift, cloudbased data warehousing platforms, data governance, data security, databricks, datamodeling, datawarehouse, dlt delta live tables, elt, etl, google bigquery, kafka, lakehouse, python, snowflake, spark, sql, synapse, unity catalog"
Principal Data Engineer,Blueprint,"Seattle, WA",https://www.linkedin.com/jobs/view/principal-data-engineer-at-blueprint-3716423540,2023-12-17,Summit,United States,Mid senior,Remote,"Principal Date Engineer Remote
Who is Blueprint?
We are a technology solutions firm headquartered in Bellevue, Washington, with a strong presence across the United States. Unified by a shared passion for solving complicated problems, our people are our greatest asset. We use technology as a tool to bridge the gap between strategy and execution, powered by the knowledge, skills, and the expertise of our teams, who all have unique perspectives and years of experience across multiple industries. We're bold, smart, agile, and fun.
What does Blueprint do?
Blueprint helps organizations unlock value from existing assets by leveraging cutting-edge technology to create additional revenue streams and new lines of business. We connect strategy, business solutions, products, and services to transform and grow companies.
Why Blueprint?
At Blueprint, we believe in the power of possibility and are passionate about bringing it to life. Whether you join our bustling product division, our multifaceted services team or you want to grow your career in human resources, your ability to make an impact is amplified when you join one of our teams. You'll focus on solving unique business problems while gaining hands-on experience with the world's best technology. We believe in unique perspectives and build teams of people with diverse skillsets and backgrounds. At Blueprint, you'll have the opportunity to work with multiple clients and teams, such as data science and product development, all while learning, growing, and developing new solutions. We guarantee you won't find a better place to work and thrive than at Blueprint.
What will I be doing?
Blueprint is looking for a
Principal Data Engineer
to join us as we build cutting-edge technology solutions! The ideal candidate will have a solid background in consulting, with demonstrated experience leading clients through the process of building modern data estates. As a Principal Data Engineer, you will spend a majority of your time working directly with clients to develop their advanced modern data estates, warehouses, and analytical environments. You will also be responsible for overseeing and mentoring junior developers within the organization.
Responsibilities:
Develop and implement effective data architecture solutions using Databricks and Lakehouse
Optimize and tune data pipelines for performance and scalability
Monitor and troubleshoot data pipelines to ensure data availability and reliability
Collaborate with data scientists, analysts, and other stakeholders to understand their data needs and build solutions that enable them to extract insights from data
Implement best practices for data governance, data security, and data quality to ensure data integrity across all data sources
Create and maintain documentation related to data architecture, data pipelines, and data models
Stay up to date with emerging technologies and best practices in data engineering and big data processing
Mentor and train other data engineers on best practices for data engineering and Databricks usage
Provide thought leadership in the Databricks and Lakehouse space, both within the organization and externally
Qualifications:
Bachelor's or Master's degree in Computer Science, Computer Engineering, or a related field
8+ years of experience in data engineering
3+ years of experience working with Databricks and PySpark
6-8+ years of experience with SQL
Appreciation for the Lakehouse medallion data architecture – bronze, silver, gold – and how those data stages are used
Working knowledge of DLT(Delta Live Tables) and Unity Catalog a plus
Strong understanding of ETL and ELT data ingestion, acquisition, and data processing patterns
Experience with cloud-based data warehousing platforms such as Synapse, AWS Redshift, Google BigQuery, or Snowflake
Strong understanding of data engineering, data warehousing, data modeling, data governance, and data security best practices
Excellent problem-solving and troubleshooting skills
Strong communication and collaboration skills, with the ability to work effectively in a team environment
Experience mentoring and training other data engineers
Salary Range
Pay ranges vary based on multiple factors including, without limitation, skill sets, education, responsibilities, experience, and geographical market. The pay range for this position reflects geographic based ranges for Washington state: $127,000 to $211,600 USD/annually. The salary/wage and job title for this opening will be based on the selected candidate's qualifications and experience and may be outside this range.
Equal Opportunity Employer
Blueprint Technologies, LLC is an equal employment opportunity employer. Qualified applicants are considered without regard to race, color, age, disability, sex, gender identity or expression, orientation, veteran/military status, religion, national origin, ancestry, marital, or familial status, genetic information, citizenship, or any other status protected by law.
If you need assistance or a reasonable accommodation to complete the application process, please reach out to: recruiting@bpcs.com
Blueprint believe in the importance of a healthy and happy team, which is why our comprehensive benefits package includes:
Medical, dental, and vision coverage
Flexible Spending Account
401k program
Competitive PTO offerings
Parental Leave
Opportunities for professional growth and development
Location:
Remote
Show more
Show less","Data engineering, Consulting, Data architecture, Databricks, Lakehouse, PySpark, SQL, Data governance, Data security, Data quality, Data ingestion, Data processing, Cloudbased data warehousing, Synapse, AWS Redshift, Google BigQuery, Snowflake, ETL, ELT, Data modeling, Problemsolving, Troubleshooting, Communication, Collaboration, Mentoring, Training","data engineering, consulting, data architecture, databricks, lakehouse, pyspark, sql, data governance, data security, data quality, data ingestion, data processing, cloudbased data warehousing, synapse, aws redshift, google bigquery, snowflake, etl, elt, data modeling, problemsolving, troubleshooting, communication, collaboration, mentoring, training","aws redshift, cloudbased data warehousing, collaboration, communication, consulting, data architecture, data engineering, data governance, data ingestion, data processing, data quality, data security, databricks, datamodeling, elt, etl, google bigquery, lakehouse, mentoring, problemsolving, snowflake, spark, sql, synapse, training, troubleshooting"
Remote Data Engineer,Motion Recruitment,"Seattle, WA",https://www.linkedin.com/jobs/view/remote-data-engineer-at-motion-recruitment-3766164589,2023-12-17,Summit,United States,Mid senior,Remote,"Our client is looking for a skilled and experienced
Data Engineer
to join their dynamic team. If you have a passion for designing and optimizing data architectures, along with a proven track record of utilizing advanced SQL proficiency and collaborating with cross-functional teams, this role offers an exciting opportunity for you.
This role is fully remote, and is a 6-month to 1-year W-2 contract slated to go longer.
Required Skills And Experience
5+ years of experience in the data engineering field
Advanced SQL proficiency with a minimum of 5 years of practical experience
2-3+ years of hands-on experience with DBT and Snowflake
2+ years of experience working with Git, and demonstrated collaboration with other engineers
Bachelor's degree in Computer Science or Engineering, or relevant work experience
Working knowledge of Agile frameworks and Jira
Excellent problem-solving skills
What You Will Be Doing
Designing, developing, and maintaining scalable data pipelines and architectures
Collaborating with cross-functional teams to understand data requirements and deliver effective solutions
Utilizing advanced SQL skills to analyze and optimize data workflows
Working with DBT and Snowflake to model and transform data efficiently
Ensuring version control and collaboration through effective use of Git
Applying Agile methodologies and contributing to project management using Jira
Solving complex problems and contributing to the continuous improvement of data engineering processes
Desired Skills & Experience
Advanced experience with Salesforce and Salesforce CPQ is a huge plus
Functional experience with AWS
Exposure to CI/CD and DevOps concepts
Applicants must be currently authorized to work in the US on a full-time basis now and in the future.
Posted By:
Megan Barno
Show more
Show less","Data Engineering, SQL, DBT, Snowflake, Git, Agile, Jira, Salesforce, Salesforce CPQ, AWS, CI/CD, DevOps","data engineering, sql, dbt, snowflake, git, agile, jira, salesforce, salesforce cpq, aws, cicd, devops","agile, aws, cicd, data engineering, dbt, devops, git, jira, salesforce, salesforce cpq, snowflake, sql"
Staff Software Engineer - Datalake,Dremio,"Seattle, WA",https://www.linkedin.com/jobs/view/staff-software-engineer-datalake-at-dremio-3783886725,2023-12-17,Summit,United States,Mid senior,Remote,"Be Part of Building the Future
Dremio is The Easy and Open Data Lakehouse, providing self-service analytics with data warehouse functionality and data lake flexibility across all of your data. Dremio increases agility with a revolutionary data-as-code approach that adopts Git concepts to enable data experimentation, version control, and governance. In addition, Dremio breaks down data silos by simplifying ingestion into the lakehouse, and also allowing queries directly on databases and data warehouses. All of this is available through a fully managed service that not only eliminates the need to maintain infrastructure and software, but also automatically optimizes the data in the lakehouse to maximize performance for every workload.
Founded in 2015, Dremio is headquartered in Santa Clara, CA. Investors include Cisco Investments, Insight Partners, Lightspeed Venture Partners, Norwest Venture Partners, Redpoint Ventures, and Sapphire Ventures. For more information, visit www.dremio.com. Connect with Dremio on GitHub, LinkedIn, Twitter, and Facebook.
If you, like us, say “bring it on” to exciting challenges that really do change the world, we have endless opportunities where you can make your mark.
About The Role
We are looking for an experienced Staff Software Engineer to enhance Dremio’s data warehouse capabilities on top of the datalakes across all major table/file formats and object stores. These capability advancements will increase our competitive position in the market and enable Dremio adoption for a larger set of customers.
What You’ll Be Doing
Develop core components for Dremio’s query engine
Deliver key features and feature enhancements for our customers in the Datalake like DML operations, time travel, schema evolution along with performance and reliability improvements
Work with open source projects like Apache Iceberg, Parquet, Arrow and Calcite
Own design, implementation, testing, and support of next-generation features related to scalability, reliability, robustness, and performance of the product
Collaborate with Product Management to innovate and deliver on customer requirements and with Support and field teams to ensure customer success
Understand and reason about concurrency and parallelization to deliver scalability and performance in a multithreaded and distributed environment
Solve complex technical problems and customer issues while improving our telemetry and instrumentation to proactively detect issues before they arise and make debugging more efficient
Work with engineering leaders to establish solid designs/architecture for upcoming features.
Develop the future leaders of Dremio by providing continuous mentorship and coaching of junior software engineers, help with hiring and onboarding
What We’re Looking For
8+ year of industry experience
B.S./M.S/Equivalent in Computer Science or a related technical field or equivalent experience
Fluency in Java, C++ or another modern language
Strong database fundamentals including SQL, performance, and schema design and background in large scale data processing systems (e.g., Hadoop, Spark, etc.)
Understanding of distributed file systems such as S3, ADLS, or HDFS
Experience with Apache Iceberg, Parquet, AVRO and/or Delta Lake
Experience with Hive and AWS Glue
Ability to solve ambiguous, unexplored, and cross-team problems effectively
Interested and motivated to be part of a fast-moving startup with a fun and accomplished team
Big picture thinking, ability to scope and plan solutions for big problems and mentor others on the same
Bonus points if you have
Hands-on experience with distributed query engines, query processing or optimization, distributed systems, concurrency control, data replication, code generation, or storage systems
Hands on experience with AWS, Azure and Google Cloud Platform
What We Offer
Medical, dental and vision insurance
401(k) Plan
Short term / long term disability and life insurance
Pre-IPO stock options
Flexible PTO
16 hours of volunteer time off
12 company paid holidays, including Juneteenth
Remote work options
Paid parental leave
Employee Assistance Program (EAP)
Biannual swag surprise
Certain benefits are only allowed to full-time Dremio employees and may not be the same across all locations.
The base salary range for this position is $154,545 to $209,091 per year. The base salary actually offered to a successful candidate will take into account various relevant and non-discriminatory business factors including, without limitation, the candidate’s geographic location, job-related experience, knowledge, and skills, and education, as well as internal equity considerations. A successful candidate may also be eligible to earn additional compensation including commissions and/or bonuses.
Return to Office Philosophy
Workplace Wednesdays - to break down silos, build relationships and improve cross-team communication, Gnarlies who live within a 1-hour commuting distance (one-way) to a Dremio office come to the office. Lunch catering / meal credits provided in the office and local socials align to Workplace Wednesdays.
In general, Dremio will remain a hybrid work environment. We will not be implementing a 100% (5 days a week) return to office policy for all roles.
What We Value
At Dremio, we hold ourselves to high standards when it comes to People, Thinking, and Action. Our Gnarlies (that's what we call our employees) communicate with clarity, drive accountability, and are respectful towards each other. We confront brutal facts and focus on results while operating with a sense of urgency and building a ""flywheel"". People who like to jump in and drive momentum will thrive in our #GnarlyLife.
Dremio is an equal opportunity employer supporting workforce diversity. We do not discriminate on the basis of race, religion, color, national origin, gender identity, sexual orientation, age, marital status, protected veteran status, disability status, or any other unlawful factor.
Dremio is committed to providing any necessary accommodations for individuals with disabilities within our application and interview process. To request accommodation due to a disability, please inform your recruiter.
Dremio has policies in place to protect the personal information that employees and applicants disclose to us. Please click
here
to review the privacy notice.
Show more
Show less","Apache Iceberg, Parquet, Arrow, Calcite, Java, C++, SQL, Hadoop, Spark, Hive, AWS Glue, Distributed query engines, Query processing, Query optimization, Distributed systems, Concurrency control, Data replication, Code generation, Storage systems, AWS, Azure, Google Cloud Platform","apache iceberg, parquet, arrow, calcite, java, c, sql, hadoop, spark, hive, aws glue, distributed query engines, query processing, query optimization, distributed systems, concurrency control, data replication, code generation, storage systems, aws, azure, google cloud platform","apache iceberg, arrow, aws, aws glue, azure, c, calcite, code generation, concurrency control, data replication, distributed query engines, distributed systems, google cloud platform, hadoop, hive, java, parquet, query optimization, query processing, spark, sql, storage systems"
Staff Software Engineer - Datalake,Dremio,"Seattle, WA",https://www.linkedin.com/jobs/view/staff-software-engineer-datalake-at-dremio-3783889029,2023-12-17,Summit,United States,Mid senior,Remote,"Be Part of Building the Future
Dremio is The Easy and Open Data Lakehouse, providing self-service analytics with data warehouse functionality and data lake flexibility across all of your data. Dremio increases agility with a revolutionary data-as-code approach that adopts Git concepts to enable data experimentation, version control, and governance. In addition, Dremio breaks down data silos by simplifying ingestion into the lakehouse, and also allowing queries directly on databases and data warehouses. All of this is available through a fully managed service that not only eliminates the need to maintain infrastructure and software, but also automatically optimizes the data in the lakehouse to maximize performance for every workload.
Founded in 2015, Dremio is headquartered in Santa Clara, CA. Investors include Cisco Investments, Insight Partners, Lightspeed Venture Partners, Norwest Venture Partners, Redpoint Ventures, and Sapphire Ventures. For more information, visit www.dremio.com. Connect with Dremio on GitHub, LinkedIn, Twitter, and Facebook.
If you, like us, say “bring it on” to exciting challenges that really do change the world, we have endless opportunities where you can make your mark.
About The Role
We are looking for an experienced Staff Software Engineer to enhance Dremio’s data warehouse capabilities on top of the datalakes across all major table/file formats and object stores. These capability advancements will increase our competitive position in the market and enable Dremio adoption for a larger set of customers.
What You’ll Be Doing
Develop core components for Dremio’s query engine
Deliver key features and feature enhancements for our customers in the Datalake like DML operations, time travel, schema evolution along with performance and reliability improvements
Work with open source projects like Apache Iceberg, Parquet, Arrow and Calcite
Own design, implementation, testing, and support of next-generation features related to scalability, reliability, robustness, and performance of the product
Collaborate with Product Management to innovate and deliver on customer requirements and with Support and field teams to ensure customer success
Understand and reason about concurrency and parallelization to deliver scalability and performance in a multithreaded and distributed environment
Solve complex technical problems and customer issues while improving our telemetry and instrumentation to proactively detect issues before they arise and make debugging more efficient
Work with engineering leaders to establish solid designs/architecture for upcoming features.
Develop the future leaders of Dremio by providing continuous mentorship and coaching of junior software engineers, help with hiring and onboarding
What We’re Looking For
8+ year of industry experience
B.S./M.S/Equivalent in Computer Science or a related technical field or equivalent experience
Fluency in Java, C++ or another modern language
Strong database fundamentals including SQL, performance, and schema design and background in large scale data processing systems (e.g., Hadoop, Spark, etc.)
Understanding of distributed file systems such as S3, ADLS, or HDFS
Experience with Apache Iceberg, Parquet, AVRO and/or Delta Lake
Experience with Hive and AWS Glue
Ability to solve ambiguous, unexplored, and cross-team problems effectively
Interested and motivated to be part of a fast-moving startup with a fun and accomplished team
Big picture thinking, ability to scope and plan solutions for big problems and mentor others on the same
Bonus points if you have
Hands-on experience with distributed query engines, query processing or optimization, distributed systems, concurrency control, data replication, code generation, or storage systems
Hands on experience with AWS, Azure and Google Cloud Platform
What We Offer
Medical, dental and vision insurance
401(k) Plan
Short term / long term disability and life insurance
Pre-IPO stock options
Flexible PTO
16 hours of volunteer time off
12 company paid holidays, including Juneteenth
Remote work options
Paid parental leave
Employee Assistance Program (EAP)
Biannual swag surprise
Certain benefits are only allowed to full-time Dremio employees and may not be the same across all locations.
The base salary range for this position is $154,545 to $209,091 per year. The base salary actually offered to a successful candidate will take into account various relevant and non-discriminatory business factors including, without limitation, the candidate’s geographic location, job-related experience, knowledge, and skills, and education, as well as internal equity considerations. A successful candidate may also be eligible to earn additional compensation including commissions and/or bonuses.
What We Value
At Dremio, we hold ourselves to high standards when it comes to People, Thinking, and Action. Our Gnarlies (that's what we call our employees) communicate with clarity, drive accountability, and are respectful towards each other. We confront brutal facts and focus on results while operating with a sense of urgency and building a ""flywheel"". People who like to jump in and drive momentum will thrive in our #GnarlyLife.
Dremio is an equal opportunity employer supporting workforce diversity. We do not discriminate on the basis of race, religion, color, national origin, gender identity, sexual orientation, age, marital status, protected veteran status, disability status, or any other unlawful factor.
Dremio is committed to providing any necessary accommodations for individuals with disabilities within our application and interview process. To request accommodation due to a disability, please inform your recruiter.
Dremio has policies in place to protect the personal information that employees and applicants disclose to us. Please click
here
to review the privacy notice.
Show more
Show less","Java, C++, SQL, Hadoop, Spark, Apache Iceberg, Parquet, Arrow, Calcite, Hive, AWS Glue, S3, ADLS, HDFS, AVRO, Delta Lake, Distributed query engines, Query processing, Query optimization, Distributed systems, Concurrency control, Data replication, Code generation, Storage systems, AWS, Azure, Google Cloud Platform","java, c, sql, hadoop, spark, apache iceberg, parquet, arrow, calcite, hive, aws glue, s3, adls, hdfs, avro, delta lake, distributed query engines, query processing, query optimization, distributed systems, concurrency control, data replication, code generation, storage systems, aws, azure, google cloud platform","adls, apache iceberg, arrow, avro, aws, aws glue, azure, c, calcite, code generation, concurrency control, data replication, delta lake, distributed query engines, distributed systems, google cloud platform, hadoop, hdfs, hive, java, parquet, query optimization, query processing, s3, spark, sql, storage systems"
Senior Geospatial Database Software Engineer,Wherobots,"Seattle, WA",https://www.linkedin.com/jobs/view/senior-geospatial-database-software-engineer-at-wherobots-3755971342,2023-12-17,Summit,United States,Mid senior,Remote,"We are looking for passionate, skilled, and experienced software engineers to join Wherobots’ dynamic team in building the distributed geospatial cloud database of the future. As a software engineer in our SedonaDB & geospatial analytics team, you will build new features, design and implement APIs to extend our geospatial query capabilities, and improve the performance, observability, and reliability of SedonaDB’s distributed query engine.
We are 100% cloud-native, and build our product using modern and reliable tools and technologies. SedonaDB and Apache Spark are built in Java and Scala, with deeply optimized Python wrappers and SDKs, and are deployed on cloud-based Kubernetes environments.
If you are passionate about building a cutting-edge, scalable geospatial database and want to be part of a fast-growing company at the forefront of geospatial technology, we would love to hear from you. Apply now and join the Wherobots team!
Responsibilities
Design and implement scalable and efficient query capabilities at the core of our geospatial cloud database
Implement new vector and raster functions to extend SedonaDB’s feature set on geospatial datasets
Collaborate with cross-functional teams to define requirements, identify technical solutions, and deliver high-quality software
Conduct code reviews and provide constructive feedback to ensure code quality and adherence to best practices
Make open-source contributions to Apache Sedona, Apache Spark, and other open-source software packages where necessary
Stay up-to-date with industry trends and advancements in geospatial and database technologies, and contribute to the continuous improvement of our development processes
Qualifications
Strong technical background, including 5+ years of hands-on experience implementing scalable, distributed spatial query algorithms and applications
Proficiency in Java and/or Scala required
A proven track record of building high-quality, maintainable, and observable software
Solid understanding of distributed systems, query engines, DSLs, data structures, and algorithms
Strong communication and collaboration abilities, with a proven track record of working effectively in a hybrid or distributed team environment
Excellent problem-solving and troubleshooting skills
Nice to have:
Hands-on experience programming in Python
Experience with large-scale geospatial data processing and applications
Compensation And Benefits
Wherobots offers competitive compensation, equity, and benefits. The base salary range for this position is $150k-$250k per year.
Although we aim to establish a primary engineering presence in the San Francisco Bay Area, we provide flexibility and choice in the working arrangement for most roles, including remote and/or in-office roles. Please note that the base pay range is a guideline and for candidates who receive an offer, the base pay will vary based on factors such as work location, seniority, skills, and experience of the candidate.
Wherobots provides a competitive benefits package to all full-time employees, including 100% coverage of medical, dental, and vision insurance, access to a 401(k) plan, and unlimited PTO.
About Wherobots
Wherobots was founded by the creators of Apache Sedona to build the first fully-managed, highly scalable geospatial cloud database and analytics platform: Wherobots Cloud. Geospatial, location-enabled, and satellite imagery data are quickly becoming a critical and valuable source of information and insights to a broad array of industries, from logistics and insurance to financial or climate tech companies. Wherobots helps those companies bring their geospatial data down to earth and drive value from it for their business and their customers through full-featured and scalable computation, querying, analytics, and visualization capabilities.
Show more
Show less","Java, Scala, Apache Spark, Python, Kubernetes, Geospatial database, Data structures, Algorithms, Distributed systems, Query engines, DSL, Query capabilities, Opensource software packages","java, scala, apache spark, python, kubernetes, geospatial database, data structures, algorithms, distributed systems, query engines, dsl, query capabilities, opensource software packages","algorithms, apache spark, data structures, distributed systems, dsl, geospatial database, java, kubernetes, opensource software packages, python, query capabilities, query engines, scala"
Data Engineer - BI and Analytics Ecom,Costco IT,"Seattle, WA",https://www.linkedin.com/jobs/view/data-engineer-bi-and-analytics-ecom-at-costco-it-3732805908,2023-12-17,Summit,United States,Mid senior,Hybrid,"Costco IT is responsible for the
technical future of Costco Wholesale
, the third largest retailer in the world with wholesale operations in fourteen countries. Despite our size and explosive international expansion, we continue to provide a family, employee centric atmosphere in which our employees thrive and succeed. As proof, Costco ranks
seventh in Forbes “World’s Best Employers”
.
This is an environment
unlike anything in the high-tech world
and the secret of Costco’s success is its culture. The value Costco puts on its employees is well documented in articles from a variety of publishers including Bloomberg and Forbes. Our employees and our members come FIRST. Costco is well known for its generosity and community service and has won many awards for its philanthropy. The company joins with its employees to take an active role in volunteering by sponsoring many opportunities to help others.
Come join the
Costco Wholesale IT family
. Costco IT is a dynamic, fast-paced environment, working through exciting transformation efforts. We are building the next generation retail environment where you will be surrounded by dedicated and highly professional employees.
The
Data Engineer – BI and Analytics
for eCommerce is responsible for the end to end data pipelines to power analytics and data services. This role is focused on data engineering to build and deliver automated data pipelines from a plethora of internal and external data sources. The Data Engineer will partner with product owners, BI engineers, and data platform teams to design, build, test, and automate data pipelines that are relied upon across the company as the single source of truth.
Responsible for developing and operationalizing data pipelines to make data available for consumption (reports and advanced analytics). This includes data ingestion, data transformation, data validation / quality, data pipeline optimization, orchestration; and engaging with DevOps Engineer during CI / CD. The role requires a grounding in programming and SQL, followed by expertise in data storage, modeling, cloud, data warehousing, and data lakes. The Data Engineer works closely with Data Architects, Data Scientists and BI Engineers to design and maintain scalable data models and pipelines.
If you want to be a part of one of the worldwide
BEST companies “to work for”,
simply apply and let your career be reimagined.
ROLE
Designs, develops, implements and operationalizes ETL/ELT processes using IICS (Informatica Cloud).
Works in tandem with Data Architects and data/BI Engineers to design data pipelines and recommends ongoing optimization of data storage, data ingestion, data quality, orchestration.
Creates and maintains optimal data pipeline architecture.
Identifies, designs, and implements internal process improvements: automating manual processes, optimizing data delivery.
Implements big data and NoSQL solutions by developing scalable data processing platforms to drive high-value insights to the organization.
Supports development of Data Dictionaries and Data Taxonomy for product solutions.
Demonstrates strong understanding with coding and programming concepts to build data pipelines (e.g. data transformation, data quality, data integration, etc.).
Builds data models with Data Architect and develops data pipelines to store data in defined data models and structures.
Demonstrates strong understanding of data integration techniques and tools (e.g. Extract, Transform, Load (ETL) / Extract, Load, Transform (ELT)) tools and database architecture.
Demonstrates strong understanding of database storage concepts (data lake, relational databases, NoSQL, Graph, data warehousing).
Identifies ways to improve data reliability, efficiency and quality of data management.
Conducts ad-hoc data retrieval for business reports and dashboards.
Assesses the integrity of data from multiple sources.
Manages database configuration including installing and upgrading software and maintaining relevant documentation.
Monitors database activity and resource usage.
Performs peer review for another Data Engineer’s work.
Designs, develops, implements, and operationalizes ETL/ELT processes using IICS (Informatica cloud).
Works in tandem with data architects and data/BI engineers to design data pipelines and recommends ongoing optimization of data storage, data ingestion, data quality, and orchestration.
Uses Azure services such as Azure SQL DW (Synapse), ADLS, Azure Event Hub, Azure Data Factory to improve and speed up delivery of our data products and services.
Required
8+ years’ experience engineering and operationalizing data pipelines with large and complex datasets.
8 + years’ hands-on experience with Informatica IICS or PowerCenter.
5 + year s’ experience with Data Modeling, ETL, and Data Warehousing.
2+ years’ experience working with Cloud technologies such as ADLS, Azure Databricks, Spark, Azure Synapse, Cosmos DB, and other big data technologies.
Experience with digital ecommerce and clickstream data
Extensive experience working with various data sources (SQL, Oracle database, flat files (csv, delimited), Web API, XML.
Advanced SQL skills. Solid understanding of relational databases and business data; ability to write complex SQL queries against a variety of data sources.
Strong understanding of database storage concepts (data lake, relational databases, NoSQL, Graph, data warehousing).
Able to work in a fast-paced agile development environment.
Scheduling flexibility to meet the needs of the business including weekends, holidays, and 24/7 on call responsibilities on a rotational basis.
Recommended
BA/BS in Computer Science, Engineering, or equivalent software/services experience.
Azure Certifications.
Experience implementing data integration techniques such as event/message-based integration (Kafka, Azure Event Hub), ETL.
Experience with Git / Azure DevOps.
Experience delivering data solutions through agile software development methodologies.
Exposure to the retail industry.
Excellent verbal and written communication skills.
Experience with UC4 Job Scheduler.
Experience with Power BI.
Required Documents
Cover Letter
Resume
California applicants, please click to review the Costco Applicant Privacy Notice.
Pay Ranges
Level 2 - $100,000 - $135,000
Level 3 - $125,000 - $165,000
Level 4 - $155,000 - $195,000, Bonus and Restricted Stock Unit (RSU) eligible
We offer a comprehensive package of benefits including paid time off, health benefits - medical/dental/vision/hearing aid/pharmacy/behavioral health/employee assistance, health care reimbursement account, dependent care assistance plan, short-term disability and long-term disability insurance, AD&D insurance, life insurance, 401(k), stock purchase plan to eligible employees.
Costco is committed to a diverse and inclusive workplace. Costco is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or any other legally protected status. If you need assistance and/or a reasonable accommodation due to a disability during the application or the recruiting process, please send a request to IT-Recruiting@costco.com
If hired, you will be required to provide proof of authorization to work in the United States.
Show more
Show less","Data Engineering, Data Pipelines, Data Warehousing, Data Lakes, SQL, Informatica Cloud (IICS), Azure SQL DW (Synapse), ADLS, Azure Event Hub, Azure Data Factory, Azure Databricks, Spark, Azure Synapse, Cosmos DB, Kafka, Git, Azure DevOps, Agile Software Development, Power BI, UC4 Job Scheduler","data engineering, data pipelines, data warehousing, data lakes, sql, informatica cloud iics, azure sql dw synapse, adls, azure event hub, azure data factory, azure databricks, spark, azure synapse, cosmos db, kafka, git, azure devops, agile software development, power bi, uc4 job scheduler","adls, agile software development, azure data factory, azure databricks, azure devops, azure event hub, azure sql dw synapse, azure synapse, cosmos db, data engineering, data lakes, datapipeline, datawarehouse, git, informatica cloud iics, kafka, powerbi, spark, sql, uc4 job scheduler"
"Direct Client – Sr. Data Platform Engineer (Python, NoSql, CI/CD, Hadoop/Spark)",The AES Group,"Seattle, WA",https://www.linkedin.com/jobs/view/direct-client-%E2%80%93-sr-data-platform-engineer-python-nosql-ci-cd-hadoop-spark-at-the-aes-group-3697323154,2023-12-17,Summit,United States,Mid senior,Hybrid,"Location:
Hybrid (3 days in a week from office/ Seattle, WA)
Independently design and develop programs and tools to support data pipelines including ingestion, curation, and provisioning of data for analytics, reporting and data science. Deliver key, complex business capabilities and platform features. Deliver high quality software using Agile/Scrum development methodologies. Identify and drive operational improvements for the team's data platform services.
Tops 3 Skills Needed
1
Software development using Python
4+ years
2
Experience with data technologies
2+ years
3
CI/ CD
2+ years
Years Of Experience
4+ years
Basic Qualifications/ Experience
Bachelor's degree in computer science or engineering. Of the required experience, must have the years of experience in each of the following:
4 years of software development experience using Python.
4 years of experience using software engineering fundamentals, including algorithm theory and data structures.
2 years of experience using CI/CD tools to build and deploy software systems.
2 years of experience using relational and non-relational, NoSQL databases.
1 years of experience using Hadoop or Spark.
1 years of experience using data hygiene routines and models
Key Responsibilities
Collaborate with development team, other Information Technology (IT) teams' developer leads.
Initiate process improvements for new and existing systems.
Design, develop, implement, document, and test changes to an application subsystem.
Code, test, debugs, documents, and implement complex software applications.
Create more complex prototypes and ensure deliverables are high quality and meet user expectations.
Support system and integration testing activities.
Perform root cause analysis to identify permanent resolutions to software or business process issues
Initiate design reviews for new applications and adhere to software development standards.
Coach, and mentor other team members.
Perform cross-training and facilitate information sharing among team members.
Show more
Show less","Python, Data technologies, CI/CD, Agile/Scrum, Software development, Software engineering fundamentals, Algorithm theory, Data structures, Hadoop, Spark, Data hygiene routines, Data models, Relational databases, Nonrelational databases, NoSQL databases","python, data technologies, cicd, agilescrum, software development, software engineering fundamentals, algorithm theory, data structures, hadoop, spark, data hygiene routines, data models, relational databases, nonrelational databases, nosql databases","agilescrum, algorithm theory, cicd, data hygiene routines, data models, data structures, data technologies, hadoop, nonrelational databases, nosql databases, python, relational databases, software development, software engineering fundamentals, spark"
Azure Data Engineer with strong Spark,"The Dignify Solutions, LLC","Bellevue, WA",https://www.linkedin.com/jobs/view/azure-data-engineer-with-strong-spark-at-the-dignify-solutions-llc-3768013489,2023-12-17,Summit,United States,Mid senior,Hybrid,"Responsibilities
Data Engineering experience primarily on Spark. Someone who has worked on Azure cloud with knowledge on Azure DEVOPS (CI/CD & Infrastructure as a code), ADF, ADW & Power BI. Apache Nifi will be good to have.Power BI: Understand business requirements to set functional specifications for reporting applications Build automated reports and dashboards with the help of Power BI reporting tool Be experienced in tools and systems on MS SQL Server BI Stack, including SSRS and TSQL, Power Query, MDX, PowerBI, and DAX Be able to quickly shape data into reporting and analytics solutions Have knowledge of database fundamentals such as multidimensional database design, relational database design, and more Create functional reporting Study, analyze and understand business requirements in context to business intelligence. Design and map data models to shift raw data into meaningful insights. Utilize Power BI to build interactive and visually appealing dashboards and reports. Spot key performance indicators with apt objectives Run DAX queries and functions in Power BI Should have an edge over making DAX queries in Power BI desktop. Developing visual reports, KPI scorecards, and dashboards using Power BI desktop. Connecting data sources, importing data, and transforming data for Business intelligence.
Required Skills
Spark ,Azure DEVOPS (CI/CD & Infrastructure as a code), ADF, ADW & Power
Show more
Show less","Spark, Azure DEVOPS, CI/CD, Infrastructure as Code, ADF, ADW, Power BI, Apache Nifi, TSQL, Power Query, MDX, DAX, Multidimensional Database Design, Relational Database Design, Data Modeling, Visual Analytics, KPI Scorecards, Dashboards, Data Import, Data Transformation, Business Intelligence","spark, azure devops, cicd, infrastructure as code, adf, adw, power bi, apache nifi, tsql, power query, mdx, dax, multidimensional database design, relational database design, data modeling, visual analytics, kpi scorecards, dashboards, data import, data transformation, business intelligence","adf, adw, apache nifi, azure devops, business intelligence, cicd, dashboard, data import, data transformation, datamodeling, dax, infrastructure as code, kpi scorecards, mdx, multidimensional database design, power query, powerbi, relational database design, spark, tsql, visual analytics"
Senior Software Engineer / Data Engineer - IDS1090,Mastercard,"Seattle, WA",https://www.linkedin.com/jobs/view/senior-software-engineer-data-engineer-ids1090-at-mastercard-3736351921,2023-12-17,Summit,United States,Mid senior,Hybrid,"Our Purpose
We work to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments and businesses realize their greatest potential. Our decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. We cultivate a culture of inclusion for all employees that respects their individual strengths, views, and experiences. We believe that our differences enable us to be a better team – one that makes better decisions, drives innovation and delivers better business results.
Title And Summary
Senior Software Engineer / Data Engineer - IDS1090
Ekata, a Mastercard company, is the global standard in identity verification, providing businesses worldwide the ability to link any digital transaction to the human behind it. Our Ekata Identity Engine, the first and only of its kind, uses complex machine learning to combine features derived from the billions of transactions within our proprietary network and the data from our graph to deliver industry leading risk assessment solutions.
We are looking for a Software Engineer who thrives on designing, coding, and maintaining large data processing systems, primarily in the Spark framework using Scala. Our ideal candidate will understand the challenges of working with data at scale and have a firm knowledge of algorithms and processing complexity.
Our team is a mix of software engineers and data analysts working to ensure our inputs, algorithms, and publishing artifacts are of the highest quality and efficiency. Our output is critical to the accuracy of our Risk Analysis APIs which power top e-commerce sites. Constant improvement and innovation is part of our process. Your ability to learn and grow will be recognized and rewarded.
As a Senior Software Engineer for the Graph and Tools Team, you will:
Develop big data processing systems using Scala, Apache Spark, Airflow and related technologies.
Increase the capabilities and efficiency of our data processing pipeline. Investigate problems when they occur and develop solutions.
Insist on highest coding standards, follow and create best practices for clean code, tests, and architecture.
Plan and document our development by delivering well-written engineering designs for review by the Product, Privacy, and Security teams.
Manage a sense of urgency and risks on project timelines and propose creative strategies for delivering constant business value.
Our Ideal Software Engineer Will Have
Bachelor's degree in Computer Science or equivalent experience
Strong data structures, algorithm design, and problem-solving skills
Aptitude to learn new languages and technologies as necessary
Ability to write well-organized, readable, and testable code that follows best practices
Experience with Scala, Java, or other strongly typed languages
Experience with functional programming
Good verbal and written communication skills
Experience using web service APIs
Familiarity with AWS services including EC2, S3, DynamoDB, and EMR
Benefits
In line with Mastercard’s total compensation philosophy, the successful candidate will be offered a competitive base salary based on location, experience and other qualifications for the role and may be eligible for an annual bonus or commissions depending on the role.  Mastercard benefits for full time (and certain part time) employees  generally include: medical, prescription drug, dental, vision, disability, life insurance, flexible spending account and health savings account, paid leaves including 16 weeks new parent leave, up to 20 paid days bereavement leave,10 annual paid sick days, 10 or more annual paid vacation days based on level, 5 personal days, 10 annual paid U.S. observed holidays, 401k with a best-in-class company match, deferred compensation for eligible roles, fitness reimbursement or on-site fitness facilities, eligibility for tuition reimbursement, gender-inclusive benefits and many more.
#IdentitiySolutions
In the US, Mastercard is an inclusive Equal Employment Opportunity employer that considers applicants without regard to gender, gender identity, sexual orientation, race, ethnicity, disabled or veteran status, or any other characteristic protected by law. If you require accommodations or assistance to complete the online application process, please contact reasonable_accommodation@mastercard.com and identify the type of accommodation or assistance you are requesting. Do not include any medical or health information in this email. The Reasonable Accommodations team will respond to your email promptly.
Corporate Security Responsibility
All activities involving access to Mastercard assets, information, and networks comes with an inherent risk to the organization and, therefore, it is expected that every person working for, or on behalf of, Mastercard is responsible for information security and must:
Abide by Mastercard’s security policies and practices;
Ensure the confidentiality and integrity of the information being accessed;
Report any suspected information security violation or breach, and
Complete all periodic mandatory security trainings in accordance with Mastercard’s guidelines.
Pay Ranges
Seattle, Washington: $129,000 - $199,000 USD
Show more
Show less","Scala, Apache Spark, Airflow, Java, AWS, EC2, S3, DynamoDB, EMR","scala, apache spark, airflow, java, aws, ec2, s3, dynamodb, emr","airflow, apache spark, aws, dynamodb, ec2, emr, java, s3, scala"
Direct Client – Lead Data Engineer/ Architect,The AES Group,"Seattle, WA",https://www.linkedin.com/jobs/view/direct-client-%E2%80%93-lead-data-engineer-architect-at-the-aes-group-3594968223,2023-12-17,Summit,United States,Mid senior,Hybrid,"Location:
Hybrid (Seattle, WA)
As a Software Engineer Tech Lead, you will be responsible for leading a team of software engineers in an initiative to migrate an on-premises large-scale data warehouse and associated data pipelines to Azure. You will have core competencies in the big data analytics platforms including Spark/ Databricks as well as Data Warehouse/Data Lakehouse architectures. You will also have very strong project management skills, specifically using Agile Methodologies such as Scrum/Kanban/Lean.
Tops 3 Skills Needed
1
Experience in big data platforms
10+ years
2
Agile / scrum
10+ years
3
Experience uplifting/migrating big data to the cloud
10+ years
Years Of Experience
10+ years
Degree Or Certifications Required
Bachelor's degree in Computer Science or related field
Required Background/ Skills
Experience leading a team of software engineers in large-scale data warehouse and data pipelines migration.
Experience designing and implementing data pipelines and workflow management
Experience with data warehouse/lake house architectures and data modeling
Strong problem-solving skills
Strong communication and leadership skills
Strong ability to drive engineering trade-off discussions with the goal of driving consensus
Experience in Agile methodologies such as Scrum, specifically in the role of Scrum Master
Technology Requirements
Experience with Azure Cloud Analytics Platforms including PaaS and IaaS services: Azure Data Lake Storage, Azure Data Factory, Kafka, Azure Synapse
Experience with Analytics Platforms and related technologies: Spark/Databricks, Apache Streaming, Delta Tables, and Airflow
Key Projects
Prod modernization NDA
Daily Responsibilities
Lead a team of software engineers in the migration of a large-scale data warehouse and associated data pipelines.
Ability to have trade-off discussions, with engineering team, around data pipeline design with the goal of driving consensus on approach.
Attend cross-functional meetings with project Technical Product Manager (TPM).
Work with the TPM to translate requirements into backlog items
Lead the engineering team in backlog grooming and refinement meetings.
Work with team to prioritize product and engineering backlog items in planning meetings
Lead daily stand-up meetings and drive action items and ownership around blocking items.
Communicate schedule/project timeline status to engineering manager and TPM, highlighting key trade-off discussions.
Mentor junior engineers and help them to grow their skills in story creation, decomposition, and estimation.
Nice-to-Haves
Migration into the cloud
Scrum master certification
Dis-qualifiers
This is not strong hands on engineering, this would be closer to driving project management. Not looking for someone who is wanting to do the engineering need to lead the projects
Show more
Show less","Software Engineering, Data Engineering, Data Warehousing, Data Lakehouse, Agile, Scrum, Kanban, Lean, Apache Spark, Apache Databricks, Apache Kafka, Apache Airflow, Delta Tables, Azure Cloud Analytics, Azure Data Lake Storage, Azure Data Factory, Azure Synapse","software engineering, data engineering, data warehousing, data lakehouse, agile, scrum, kanban, lean, apache spark, apache databricks, apache kafka, apache airflow, delta tables, azure cloud analytics, azure data lake storage, azure data factory, azure synapse","agile, apache airflow, apache databricks, apache kafka, apache spark, azure cloud analytics, azure data factory, azure data lake storage, azure synapse, data engineering, data lakehouse, datawarehouse, delta tables, kanban, lean, scrum, software engineering"
Senior Cloud Data Engineer,BDO USA,"Seattle, WA",https://www.linkedin.com/jobs/view/senior-cloud-data-engineer-at-bdo-usa-3765469445,2023-12-17,Summit,United States,Mid senior,Hybrid,"Job Description
Job Summary:
This position will work with cutting edge technology, deliver high quality solutions across various industries, and oversee team(s) on engagements that range in size and scope. This position will receive continuous career development opportunities, given the size and potential of client engagements. This role will perform hands-on delivery of data analytics projects, contributing to the development and unit testing of solutions.
Job Duties
Designs and implements best in class data ingestion strategies, data warehouse and data mart structures, semantic layers and models, visualizations, streaming processes, API integrations, and automation (RPA) solutions for end-to-end data analytics solutions on primarily, but not limited to, cloud analytics platforms such as Azure and AWS
Listens to client needs to align solution with business requirements and delivery schedule
Creates written functional and technical designs
Participates in project status and stand meetings, and assists with providing aggregated project status for project and program managers
Assists with SLA compliance of solutions, and performs performance tuning and optimization efforts of end-to-end solutions
Writes code using multiple languages and correctly applies frameworks, architectural patterns, and software development principles
Delivers high-performance, scalable, repeatable, and secure deliverables with broad impact (high throughput and low latency)
Assists with implementation of data governance programs and best practices
Performs the cleaning and transforming of data from source systems into analytics models
Implements models to support data visualizations and integrations
Assists with implementing DevOps, DataOps and MLOps methodologies on projects
Writes custom integration logic in applicable programming languages
Assists project managers with work breakdown structure creation, project estimation, resource staffing, workload planning and adjustments throughout the project lifecycle
Assists clients with licensing, security, and cost estimation of solutions
Performs code reviews to ensure adherence to standards
Works directly with clients and team members to establish secure data analytics platforms and infrastructure
Contributes to successful deployments of developed solutions and integration of DevOps tools
Maintains a broad and current understanding of data analytics and business intelligence strategies, cloud platforms, methodologies, and tools
Builds client relationships during project execution, effectively becoming a trusted advisor of the client
Participates in support activities for existing software solutions
Other duties as assigned
Supervisory Responsibilities
Supervises the day-to-day workload of Associates on assigned engagements to ensure that timelines and deliverables are met, and reviews work product
Education
Qualifications, Knowledge, Skills and Abilities:
High School Diploma or GED equivalent, required
Bachelor’s degree, preferred; focus in Information Systems, Data Science or Computer Science, preferred
Experience
Five (5) or more years of experience within Data Analytics, Business Intelligence, Artificial Intelligence, or Application Development, required
One (1) or more years of experience technically leading development projects, preferred
One (1) or more years of consulting experience or implementation of cloud-based data analytics solutions, preferred
Software
Strong SQL skills including Data Definition Language (DDL), Data Manipulation Language (DML), views, functions, stored procedures, or performance tuning, required
Experience with Data Warehousing, Data Modeling, Semantic Model Definition or Star Schema Construction, required
Hands on delivery experience of end-to-end cloud data analytics solutions within Azure or AWS, preferred
Experience with one (1) or more of the following computer languages, preferred:
C#
Python
Java
Scala
Experience with tabular modeling within Microsoft Fabric, Power BI, or Azure Analysis Services, preferred
Experience with Git and DevOps deployment technologies, preferred
Experience with Linux, preferred
Experience with one (1) or more of the following, preferred:
Data Lake Medallion Architecture
Batch and/or streaming data ingestion into a data lake
AI Algorithms/Machine Learning
Automation tools such as UiPath, Alteryx, etc.
Computer Vision based AI technologies
Other Knowledge, Skills & Abilities
Ability to work with a high degree of professionalism and autonomy
Excellent verbal and written communication skills
Solid organizational skills, especially the ability to meet project deadlines with a focus on details
Ability to successfully multi-task while working independently or within a group environment
Ability to work in a deadline-driven environment, and handle multiple projects simultaneously
Ability to interact effectively with people at all organizational levels of the Firm
Ability to effectively interact with a team of professionals and delegating work assignments, as needed
Ability to build and maintain strong relationships with internal and client personnel
Ability to encourage a team environment on engagements, and contribute to the professional development of assigned personnel
Keywords:
Data Analytics, Business Intelligence, BI, Synapse, IoT, Machine Learning, Data Lake, Stream, Cube, Microsoft, SQL Server, Tableau, .Net, C#, Qlik, Power BI, Machine Learning, Azure Data Factory, RedShift, UiPath, Cloud, RPA, AWS, Redshift, Kinesis, QuickSight, SageMaker, S3, Databricks, AWS Lake Formation, Snowflake, Python, Qlik, Athena, Data Pipeline, Glue, Star Schema, Data Modeling, SQL, SSIS, SSAS, SSRS, PySpark, Microsoft Fabric, dbt, Linux, Terraform, Bicep, Data Ops, Purview, Git, Delta, Pandas, Spark SQL
Individual salaries that are offered to a candidate are determined after consideration of numerous factors including but not limited to the candidate’s qualifications, experience, skills, and geography.
California Range: $111,000 - $152,000
Colorado Range: $111,000 - $152,000
New York City/ Valhalla Range: $111,000 - $152,000
Washington Range: $111,000 - $152,000
About Us
BDO delivers assurance, tax, digital technology solutions and financial advisory services to clients throughout the country and around the globe. We offer numerous industry-specific practices, world-class resources, and an unparalleled commitment to meeting our clients’ needs. We currently serve more than 400 publicly traded domestic and international clients.
Unparalleled partner-involvement
Deep industry knowledge and participation
Geographic coverage across the U.S.
Cohesive global network
Focused capabilities across disciplines
BDO brings world-class resources and exceptional service to each and every one of our clients. BDO USA is a member of BDO International, the world’s fifth largest accounting network.
BDO offers a competitive Total Rewards package that encompass so much more than – “traditional benefits”. Our wide range of rewards and our employees’ ability to customize rewards to their individual needs are two of the reasons why BDO has been honored with so many workplace awards, including 100 Best Companies for Working Parents, Working Mother 100 Best Companies, Top Entry Level Employer, 2022 National Best & Brightest Companies to Work For and more.
Some Examples Of Our Total Rewards Offerings Include
Competitive pay and eligibility for an annual performance bonus.
A 401k plan plus an employer match
Comprehensive, medical, dental, vision, FSA, and prescription insurance from day one
Competitive Paid Time Off with daily accrual from day one of employment, plus paid holidays
Paid Parental Leave
Adoption Assistance
Firm paid life insurance
Wellness programs
Additional offerings include BDO Flex, Group Legal insurance, Pet insurance and Long-Term Care Insurance
Above offerings may be subject to eligibility requirements.
Click here to find out more!
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability or protected veteran status.
""BDO USA, P.A. is an EO employer M/F/Veteran/Disability""
Show more
Show less","Data analytics, Business intelligence, Artificial intelligence, Application development, Cloudbased data analytics solutions, Data warehousing, Data modeling, Semantic model definition, Star schema construction, Microsoft Azure, Amazon AWS, SQL, C#, Python, Java, Scala, Microsoft Fabric, Power BI, Azure Analysis Services, Git, DevOps, Linux, Data Lake Medallion Architecture, Batch data ingestion, Streaming data ingestion, AI algorithms, Machine learning, Automation tools, UiPath, Alteryx, Computer vision, Tableau, .Net, Qlik, Azure Data Factory, RedShift, Kinesis, QuickSight, SageMaker, S3, Databricks, AWS Lake Formation, Snowflake, Athena, Data pipeline, Glue, Star schema, Data modeling, SSIS, SSAS, SSRS, PySpark, dbt, Terraform, Bicep, Data Ops, Purview, Delta, Pandas, Spark SQL","data analytics, business intelligence, artificial intelligence, application development, cloudbased data analytics solutions, data warehousing, data modeling, semantic model definition, star schema construction, microsoft azure, amazon aws, sql, c, python, java, scala, microsoft fabric, power bi, azure analysis services, git, devops, linux, data lake medallion architecture, batch data ingestion, streaming data ingestion, ai algorithms, machine learning, automation tools, uipath, alteryx, computer vision, tableau, net, qlik, azure data factory, redshift, kinesis, quicksight, sagemaker, s3, databricks, aws lake formation, snowflake, athena, data pipeline, glue, star schema, data modeling, ssis, ssas, ssrs, pyspark, dbt, terraform, bicep, data ops, purview, delta, pandas, spark sql","ai algorithms, alteryx, amazon aws, application development, artificial intelligence, athena, automation tools, aws lake formation, azure analysis services, azure data factory, batch data ingestion, bicep, business intelligence, c, cloudbased data analytics solutions, computer vision, data lake medallion architecture, data ops, data pipeline, dataanalytics, databricks, datamodeling, datawarehouse, dbt, delta, devops, git, glue, java, kinesis, linux, machine learning, microsoft azure, microsoft fabric, net, pandas, powerbi, purview, python, qlik, quicksight, redshift, s3, sagemaker, scala, semantic model definition, snowflake, spark, spark sql, sql, ssas, ssis, ssrs, star schema, star schema construction, streaming data ingestion, tableau, terraform, uipath"
Senior Data Engineer - 462,Braintrust,Greater Seattle Area,https://www.linkedin.com/jobs/view/senior-data-engineer-462-at-braintrust-3784987769,2023-12-17,Summit,United States,Mid senior,Hybrid,"About Us
Braintrust is a user-owned talent network that connects top-tier professionals with the world's leading enterprises. We prioritize transparency, eliminating middlemen and high markups, ensuring job-seekers are matched swiftly to innovative roles while clients benefit from unparalleled efficiency and quality.
About The Hiring Process
The hiring process for this role involves completing your Braintrust profile, applying directly to the role on Braintrust, and undergoing a one-time screening to ensure you meet our vetted talent specifications. After this, the hiring team will contact you directly if they believe you are a suitable match.
Our process isn't for everyone, that's intentional. If you believe that you are a top candidate for this job, please join our network to give yourself the opportunity to work with top companies.
JOB TYPE:
Freelance, Contract Position (no agencies/C2C - see notes below)
LOCATION:
Work from anywhere - EST/EDT | partial day overlap
HOURLY RANGE
Our client is looking to pay $110.00 – $113.00/hr
ESTIMATED DURATION:
40/week - long term
EXPERIENCE:
5-9 years
BRAINTRUST JOB ID:
11491
The Opportunity
The ideal candidate is a Data Engineer with considerable experience in migrations and Big Data frameworks.
Must-Haves
Scala programming language expertise
Spark framework expertise
Experience working with BigQuery
Familiarity scheduling jobs in Airflow
Fluency with Google Cloud Platform, in particular GCS and Dataproc
Nice-to-Haves
Python programming language fluency
Scalding framework fluency
Pyspark framework fluency
Dataflow(Apache Beam) framework fluency
Details/Notes
This is a one-time project-based role and is unlikely to extend or convert.
This role is open to remote candidates across the US who can work standard hours with complete overlap to Etsy core business hours (EST 11am - 2pm).
Interview process: The interview process is two interviews, a 30 minute phone screen with the hiring manager and a 90 minute technical screen with engineers on the team.
What You'll Be Working On
About The Team
Batch Engines supports the batch Big Data tools for engineering teams at Etsy. As an infrastructure team, our goal is to ensure state of the art tools are easy to use and interface well within the Etsy engineering ecosystem to enable batch data processing. We are a 6 person team with expertise in Big Data and DevOps.
About The Role
The Data Engineer contractor role will be a project based role focused on migrating data pipelines from legacy infrastructure and frameworks such as Scalding to more modern infrastructure we support such as Spark Scala. This role will be responsible for:
Analyzing existing data pipelines to understand their architecture, dependencies, and functionality.
Working closely with data engineers to develop a migration strategy for converting pipelines from the current framework to the target framework.
Designing, building, and testing new data pipelines in the target framework, ensuring they meet or exceed existing performance and reliability standards.
Debugging and troubleshooting issues that arise during the migration process, working collaboratively with cross-functional teams to resolve them quickly.
Communicating progress, challenges, and timelines to stakeholders on a regular basis.
Apply Now!
Notes
Our employers all have varying legal and geographic requirements for their roles, they trust Braintrust to find them the talent that meet their unique specifications. For that reason, this role is not available to C2C candidates working with an agency. If you are a professional contractor who has created an LLC/corp around their consulting practice, this is well aligned with Braintrust and we’d welcome your application.
Braintrust values the multitude of talents and perspectives that a diverse workforce brings. All qualified applicants will receive consideration for employment without regard to race, national origin, religion, age, color, sex, sexual orientation, gender identity, disability, or protected veteran status.
Show more
Show less","Scala, Spark, BigQuery, Airflow, Google Cloud Platform, GCS, Dataproc, Python, Scalding, Pyspark, Dataflow, Apache Beam","scala, spark, bigquery, airflow, google cloud platform, gcs, dataproc, python, scalding, pyspark, dataflow, apache beam","airflow, apache beam, bigquery, dataflow, dataproc, gcs, google cloud platform, python, scala, scalding, spark"
Lead Snowflake Azure Data Engineer,Advanced Knowledge Tech LLC,"Seattle, WA",https://www.linkedin.com/jobs/view/lead-snowflake-azure-data-engineer-at-advanced-knowledge-tech-llc-3667475548,2023-12-17,Summit,United States,Mid senior,Hybrid,"Role: Lead Snowflake Azure Data Engineer
Location: Seattle WA
Skills required
: Snowflake, Azure Data Factory, Azure Data Lake, DBT(Main, More SQL skills), Azure functions (Python)
Job Description
Thorough understanding of system design, analysis, and development required. Strong knowledge of business process flow and/or entity relationship modeling.
Must be effective in a team with strong problem solving, critical thinking skills, influencing, communication, and presentation skills. Self-starter with history of balancing multiple priorities simultaneously with the ability to adapt to the changing needs of the business while meeting deadlines
Demonstrated outstanding written and verbal communication skills. Ability to work effectively with other technology teams as well as business partners. Ability to effectively present information, interact with, and respond to questions from managers, employees, customers, and vendors
Detail-oriented team player who can consistently provide valuable suggestions and solutions in areas of analytical solutions with conceptual thinking to ensure all parts of an application function together as intended.
Strong experience in data curation/aggregation/preparation using Azure Data lake, Azure data factory, DBT and distilling data into meaningful insights with hands-on expertise on Azure and Snowflake
Work experience (with an emphasis on Data and Solution architecture, accurate data preparation, effective data visualizations and the ability to train others)
Demonstrated success analyzing data to solve business problems and identify opportunities for improving performance
Ability to prioritize and work on multiple tasks simultaneously
Devesh
Show more
Show less","Snowflake, Azure Data Factory, Azure Data Lake, DBT, Azure Functions, Python, System Design, Analysis, Development, Business Process Flow, Entity Relationship Modeling, Problem Solving, Critical Thinking, Influencing, Communication, Presentation, Written Communication, Verbal Communication, Analytical Solutions, Conceptual Thinking, Data Curation, Data Aggregation, Data Preparation, Data Visualization, Data Analysis, Business Problem Solving, Performance Improvement, Prioritization, Multitasking","snowflake, azure data factory, azure data lake, dbt, azure functions, python, system design, analysis, development, business process flow, entity relationship modeling, problem solving, critical thinking, influencing, communication, presentation, written communication, verbal communication, analytical solutions, conceptual thinking, data curation, data aggregation, data preparation, data visualization, data analysis, business problem solving, performance improvement, prioritization, multitasking","analysis, analytical solutions, azure data factory, azure data lake, azure functions, business problem solving, business process flow, communication, conceptual thinking, critical thinking, data aggregation, data curation, data preparation, dataanalytics, dbt, development, entity relationship modeling, influencing, multitasking, performance improvement, presentation, prioritization, problem solving, python, snowflake, system design, verbal communication, visualization, written communication"
Lead Data Analyst,Zwift,"Seattle, WA",https://www.linkedin.com/jobs/view/lead-data-analyst-at-zwift-3742595614,2023-12-17,Summit,United States,Mid senior,Hybrid,"About The Role And About You
The Lead Data Analyst is a key role within Zwift's multi-disciplinary Data team. The ideal person in this role will evangelize a data driven culture and deliver solutions and insights to Watopians every day.
This role will support and drive decisions for the Marketing Team at Zwift. The role is responsible for providing insights, measuring marketing initiatives and campaigns, and building innovative products to grow the business efficiently. It will drive the development of instrumentation, tools and processes that help us better understand why users come to Zwift, subscribe, engage with the product, and realize value from Zwift's offerings across the customer journey.
We are looking for a strong, analytically capable analyst who can not only identify and proactively advocate for changes that Zwift should make, but also align cross-functional partners around the delivery of those changes. This person will collaborate with senior partners mainly in Marketing but also in Product, Engineering, Sales, and Community Support, as well as partner with other Data functions (Data Engineering, Data Science, Data Governance) to deliver coordinated insights solutions.
Zwift is a fast-paced, high growth company where everyone has a chance to create a difference and point to the results of their work. If this sounds exciting to you, then please don't hesitate to apply.
What You’ll Do
Promote data and insights-based decision making at Zwift by collaborating with marketing leaders and other teams including software and hardware product, engineering, community support, web, and ecommerce, to tackle their highest priority and most complicated problems
Influence and partner with senior leaders and business owners to define success metrics, establish rollout and measurement plans, and provide analysis on the performance of new initiatives and campaigns and significant company objectives
Drive progress by guiding teams to design, launch, monitor, analyze and report on A/B, multivariate and campaign incrementality tests
Collaborate with the Data Engineering team to produce new well-governed datasets, tools and products that will enable team members and internal customers to use data with confidence
Apply your experience, curiosity, and knowledge of industry standard methodologies to up-level data, analytics and insights capabilities at Zwift
Prioritize the work of the team to balance delivery of proactive and practical insights with a timely turnaround on ad hoc requests that are aligned to key company initiatives
Develop and maintain self-service dashboards used by analytics and the business that drive decisions
Mentor, and coach junior analysts with a range of experience levels
Lead Data Team initiatives and projects
What We’re Looking For
Master's or advanced degree in a quantitative field
Outstanding track record with 7+ years' experience in marketing analytics and marketing effectiveness for a consumer facing, digital first business - technology, gaming, ecommerce, or subscription businesses an advantage
An analytics 'translator' with the ability to take an ambiguous problem, establish hypotheses, identify goals and metrics, and drive a team to deliver on those goals
An authority in storytelling with data, with the skills to influence at all levels of the organization and across multiple business units
Deep knowledge of experimentation including crafting, implementing, and analyzing tests to support new features and the optimization of existing capabilities for product and marketing applications
Deep knowledge and experience building and using multi-touch attribution and marketing mix models
Significant involvement using web and customer journey analytics to optimize web experiences and surface opportunities
Expertise in querying databases using SQL and statistical analysis using R or Python
Excellent knowledge of data visualization and analysis tools such as Tableau, Periscope/Sisense, Google Analytics, and Optimizely
Comfortable working with data engineers and QA testers to craft, instrument and test new data models
Must be organized, self-motivated, resourceful, curious and proactive
Bonus Points
Experience collaborating with consumer insights and UX researchers
Experience with online Affiliate programs
Experience with Google and Social Media Advertising platforms
Experience with DBT and Analytics Engineering best practices
For All US Based Full-Time Positions
The base salary for this position ranges between $107,000 to $145,000. The base salary will be based on a number of factors including the role offered, the individual's job-related knowledge, skills, qualifications, and geographic location. In addition to base salary, Zwift is proud to offer a comprehensive and competitive benefits package for all eligible employees which also includes performance bonuses, equity, and a full range of medical, financial, and other perks and benefits.
How To Stand Out Among The Rest
Your resume/CV is enough to show off your skills, accomplishments, and experience. However, if you choose to include a cover letter introducing us to your awesome personality, we will read that too.
We strongly believe that different backgrounds and ideas are a competitive advantage; we hire candidates of any race, color, ancestry, religion, sex, national origin, sexual orientation, gender identity, age, marital or family status, disability, Veteran status, and any other status. Zwift is proud to be an Equal Opportunity Employer. If you have a disability or special need that requires accommodation, please let us know by emailing careers@zwift.com.
Zwift, Inc. is an Equal Opportunity Employer.
Transparency In Coverage
Health plan price transparency is designed to help consumers know the cost of covered items or healthcare-related services prior to the date upon which they receive care. Transparency in Coverage (TIC) regulations require health insurers and group health plans to create machine readable files (MRFs) that contain the negotiated rates for in-network providers and allowed amounts derived from historical claims for out-of-network providers and make those files publicly available.
Here is the link to the site on which Kaiser Permanente posts its in-network and out-of-network allowed amount machine-readable files (MRFs).
Here is the link to the site on which Anthem posts its in-network and out-of-network allowed amount machine-readable files (MRFs). The link will allow you to search for your files using your Employer Identification Number (81-2798595)
Show more
Show less","Data Analytics, Data Visualization, DataDriven Decision Making, Marketing Analytics, Multivariate Testing, A/B Testing, Campaign Incrementality Testing, Data Engineering, Data Governance, Data Science, SQL, R, Python, Tableau, Periscope/Sisense, Google Analytics, Optimizely, DBT, Analytics Engineering, Consumer Insights, UX Research, Affiliate Programs, Google Advertising, Social Media Advertising","data analytics, data visualization, datadriven decision making, marketing analytics, multivariate testing, ab testing, campaign incrementality testing, data engineering, data governance, data science, sql, r, python, tableau, periscopesisense, google analytics, optimizely, dbt, analytics engineering, consumer insights, ux research, affiliate programs, google advertising, social media advertising","ab testing, affiliate programs, analytics engineering, campaign incrementality testing, consumer insights, data engineering, data governance, data science, dataanalytics, datadriven decision making, dbt, google advertising, google analytics, marketing analytics, multivariate testing, optimizely, periscopesisense, python, r, social media advertising, sql, tableau, ux research, visualization"
Senior Data Integration Engineer (MuleSoft),Bluehawk Consulting,"Renton, WA",https://www.linkedin.com/jobs/view/senior-data-integration-engineer-mulesoft-at-bluehawk-consulting-3783951997,2023-12-17,Summit,United States,Mid senior,Hybrid,"Senior Data Integration Engineer - MuleSoft
Renton, WA / Hybrid
Bluehawk Consulting is seeking a
Senior Data Integration Engineer
to support our client, an aerospace leader.
In this position you will write/manage/tune/troubleshoot data connectors building a data platform serving as the backbone for safety in space travel. Your work will service the design, procurement and measurement of space travel ultimately getting the right people, the right data making space travel safe and possible.
Responsibilities
Collaborate with product managers, business analysts, customers, data owners and engineers to develop software integration solutions with the goal of delivering value.
Dive deep into the data for source and target systems to understand how the integration will deliver business value, map the data, and develop the integration.
Make data structure, modeling, and design decisions for optimal integration for a given business problem.
Design/Develop/Engineer scalable, observable, and sustainable integrations.
Apply the latest research and industry trends to build best in class enterprise integrations.
Build solution prototypes and conduct experiments to evaluate alternatives.
Mentor and Coach fellow and junior engineers.
Required Skills & Experience
3+ years of hand on experience with MuleSoft.
5+ years of data integration design, development, scaling, and sustenance experience.
5+ years of integration/engineering design patterns, tradeoff analysis experience.
3+ years of experience in micro-services, API Management (APIM) engineering is a plus.
3+ years’ experience in developing cloud native solutions.
3+ year experience designing/building event driven and real-time integrations.
SQL skills and scripting (Python/Bash etc.) skills.
Excellent verbal and written communications skills.
Desired Skills
Manufacturing and/or Aerospace experience.
Experience collaborating with Master/Reference Data Management, Metadata Management and Data Governance.
Education
Required Education
Bachelor’s degree in Computer Science or equivalent.
Desired Education
Master's degree or higher in Computer Science, Mathematics, Statistics, or similar analytical field.
Our Diverse and Inclusive Culture
Bluehawk Consulting is a Kirkland, Washington-based management consulting firm helping Fortune 500 clients improve their performance, increase their efficiency, and enhance organizational value while reducing costs.
We are a team who love to accomplish great things together!
We excel at hiring talented people who are driven to create solutions for our client’s most challenging projects. Uniting amazing talent with meaningful work experiences is our passion; fostering an environment that embraces individuality and unique gifts embodies our values and fuels our purpose.
We Value
Our diverse team, they make us great!
Consultative, client-focused and interactive approaches
Innovation, actualizing potential, and a positive outlook towards change and accepting challenges
Learning, listening, and communicating openly with respect
Passion, energy, zeal - we have lots of it
Humor, lightness, and flexibility
We care about creating a supportive environment and positive employee experience. Employees will receive competitive pay, health insurance, including medical, dental, and vision coverage (choose between our Base Plan with HSA or Buy-up PPO plan), 401(k) with company match, 12 paid holidays, and up to 15 days of paid time off (PTO) per year. We also offer life insurance and a short-term disability benefit.
Salary Range for this position: $154,000 - $171,000
Bluehawk is committed to employing a diverse workforce. Qualified applicants will receive consideration without regard to race, color, religion, sex, national origin, age, sexual orientation, gender identity, gender expression, veteran status, or disability.
If this sounds like a place you'd like to learn more about, visit us at
www.bluehawkconsulting.com
Show more
Show less","MuleSoft, Data integration, Data connectors, Data platform, Data modeling, Data design, Microservices, API Management, Cloud native solutions, Event driven integrations, Realtime integrations, SQL, Scripting, Python, Bash, Manufacturing, Aerospace, Master Data Management, Metadata Management, Data Governance, Computer Science, Mathematics, Statistics","mulesoft, data integration, data connectors, data platform, data modeling, data design, microservices, api management, cloud native solutions, event driven integrations, realtime integrations, sql, scripting, python, bash, manufacturing, aerospace, master data management, metadata management, data governance, computer science, mathematics, statistics","aerospace, api management, bash, cloud native solutions, computer science, data connectors, data design, data governance, data integration, data platform, datamodeling, event driven integrations, manufacturing, master data management, mathematics, metadata management, microservices, mulesoft, python, realtime integrations, scripting, sql, statistics"
"Senior Full-Stack Engineer (Frontend, Backend, Database)",Orchard Robotics,"Bellevue, WA",https://www.linkedin.com/jobs/view/senior-full-stack-engineer-frontend-backend-database-at-orchard-robotics-3776682887,2023-12-17,Summit,United States,Mid senior,Hybrid,"Orchards in the U.S. produce $35B of fruit every year, but fruit farmers are losing 30-50% of their potential crop because they lack data needed to manage crops precisely.
About Us:
Orchard Robotics is a seed-stage startup backed by top VCs like General Catalyst, Contrary, Soma Capital, Humba Ventures, and angel investors like Howard Lerman (founder of Yext).
We’re building Robots & AI that enable precision crop management. Our AI-powered camera systems count, size, and measure the growth of every fruit on 1,000,000+ trees in an orchard. Our mission is to help farmers to produce more food for the world, much more efficiently.
Farmers are using our technology today in some of the largest orchards in the nation.
Background for the role:
Here at Orchard Robotics, our AI-powered camera systems collect precision data about millions of fruit every day. This includes everything from size, color, growth rate, trees, leaf canopy area, and much more – terabytes of data going through our system.
However, all this data is useless unless farmers can actually utilize it in their day-to-day operations. When a farmer is managing hundreds of millions of fruit on thousands of acres, it’s critical they can easily understand all of their data.
Your job will be to utilize your extensive breadth & depth of frontend (web, React) and backend (Flask, MongoDB) knowledge to help us invent new solutions in our web app platform that is changing the way farmers manage their orchards.
You will own the frontend + backend development of our orchard data platform, FruitScope, where our farmers view every piece of data, recommendations, and insights collected by our AI-powered camera systems.
We value development speed, technical expertise, and grit. We win by iterating fast and creating new + inventive ways for farmers to gain previously unknown insights into their orchards.
When the software you build helps a farmer utilize their data better, say, to grow even one more apple per tree, that adds up to millions of pounds of fruit across an orchard, meaning millions of more people fed!
If you're looking to help make a positive impact in the world by transforming the future of agriculture, come join us. Even if you may not meet every requirement, we'd strongly encourage you apply.
About the role:
As an early engineer, you'll receive
very
generous equity compensation
Full-time role at our WA headquarters
Flexible working hours
Comprehensive Health, Vision, and Dental coverage
We move fast, and sometimes this means staying late or working weekends
Our team is close-knit & highly driven, you’ll work directly with our CEO and entire team
We’re deeply motivated by the impact we’re making – every line of code written or new system built means one less fruit that goes to waste, and one more person who is fed.
What you’ll do:
Build new features in our web app to help farmers better understand their data
Ship scalable software that is capable of analyzing the hundreds of millions of fruit in an orchard
Iterate quickly on customer feedback
Develop for our backend, frontend, UI/UX, and database.
Optimize systems to be fast (i.e. query and calculate statistics for millions of fruit at a time)
Build comprehensive test suites to validate accuracy over real-world edge cases
What makes you a good fit:
You know how to build fast, reliable, scalable software.
You're comfortable working with cloud (Google Cloud), python / Flask backend, and databases (MongoDB).
Hands on experience building production-quality web apps at scale
You're a hard worker with relentless persistence.
You aren't afraid of moving fast, learning what you don't know, and are comfortable with a lot of autonomy.
You are a “jack of all trades” developer, comfortable working on all parts of the stack.
Bonus Points:
Prior experience building web apps around very large amounts of data
Experience building bespoke RESTful APIs
Join us today in helping foster a more fruitful future!
Show more
Show less","Python, Flask, MongoDB, React, Javascript, UI/UX, RESTful APIs, Machine learning, Computer vision, Data warehousing, Data processing, Data analytics, Databases, Cloud computing, Agile development","python, flask, mongodb, react, javascript, uiux, restful apis, machine learning, computer vision, data warehousing, data processing, data analytics, databases, cloud computing, agile development","agile development, cloud computing, computer vision, data processing, dataanalytics, databases, datawarehouse, flask, javascript, machine learning, mongodb, python, react, restful apis, uiux"
Senior Data Operations Engineer,Prudential Financial,"Roseland, NJ",https://www.linkedin.com/jobs/view/senior-data-operations-engineer-at-prudential-financial-3776774311,2023-12-17,Oak Ridge,United States,Mid senior,Onsite,"TITLE: Senior Data Operations Engineer (The Prudential Insurance Company of America)
LOCATION: Roseland, NJ
DUTIES: Provide hands-on operational support and production oversight to our data platforms. Provide operational support and provide oversight of Retirement data platforms stability, build data solution as well as lead hands-on implementation of capabilities across multiple data technology domains. Influence, differentiate, and guide our business and technology strategies during constant interaction with various teams. Prudential Retirement’s business is embarking on a multi-year program to build a customer-centric platform utilizing contemporary technologies and innovative techniques to deliver employee benefit, employer, and advisor experiences that create and protect financial wellness. Enable large scale business and technology projects to adopt cloud-based products and solutions to increase operational efficiency and enable revenue growth. Leverage cross-business capabilities within Prudential and partnering with Enterprise groups to enable common technology capabilities for global use across national boundaries. Contribute to the continued build out and modernization of our data platforms in support of our business strategies.
REQTS: Must have Master’s degree or foreign equivalent in Computer Science, Information Systems, or related field plus two (2) years of experience in the position offered, or as a Big Data Software Engineer, Full Staff Java Developer, Big Data Developer, or a related position. Employer will accept pre or post Master’s degree experience. Must have two (2) years of experience with all of the following: Big Data Based Data Warehouse systems to import, export, archive, store, and retrieve data from various sources such as SQL, Rest APIs, Online Web Systems; Full Stack Design of various customer faced big data applications that deal with ingesting data into Hadoop warehouse; Recommendation platforms and Sentimental Analysis of grievance ports using various data science initiatives; Big Data, Hadoop; Map Reduce; Hive; Impala; Sqoop; PIG; Mahout; Hbase; Java; R; Python; Shell; Pentaho QlikView; Springs; JSP; Javascripts; CSS.
HOURS: Full time; Mon- Fri (37.5 hrs/week)
APPLY: Apply by clicking “Apply” below and following instructions to submit resume.
Prudential Financial, Inc. of the United States is not affiliated with Prudential plc. which is headquartered in the United Kingdom.
Prudential is a multinational financial services leader with operations in the United States, Asia, Europe, and Latin America. Leveraging its heritage of life insurance and asset management expertise, Prudential is focused on helping individual and institutional customers grow and protect their wealth. The company's well-known Rock symbol is an icon of strength, stability, expertise and innovation that has stood the test of time. Prudential's businesses offer a variety of products and services, including life insurance, annuities, retirement-related services, mutual funds, asset management, and real estate services.
We recognize that our strength and success are directly linked to the quality and skills of our diverse associates. We are proud to be a place where talented people who want to make a difference can grow as professionals, leaders, and as individuals. Visit www.prudential.com to learn more about our values, our history and our brand.
Prudential is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, national origin, ancestry, sex, sexual orientation, gender identity, national origin, genetics, disability, marital status, age, veteran status, domestic partner status , medical condition or any other characteristic protected by law.
The Prudential Insurance Company of America, Newark, NJ and its affiliates.
Note that this posting is intended for individual applicants. Search firms or agencies should email Staffing at staffingagencies@prudential.com for more information about doing business with Prudential.
PEOPLE WITH DISABILITIES:
If you need an accommodation to complete the application process, which may include an assessment, please call (800) 433-8960, prompt 4 or email accommodation.h&w@prudential.com.
Please note that the above number and email are solely for individuals with disabilities requesting an accommodation. If you are experiencing a technical issue with your application or an assessment, please email careers@prudential.com to request assistance.
Show more
Show less","Data Operations, Data Platforms, Cloudbased Products, Hadoop, MapReduce, Hive, Impala, Sqoop, PIG, Mahout, Hbase, Java, R, Python, Shell, Pentaho QlikView, Spring, JSP, JavaScript, CSS","data operations, data platforms, cloudbased products, hadoop, mapreduce, hive, impala, sqoop, pig, mahout, hbase, java, r, python, shell, pentaho qlikview, spring, jsp, javascript, css","cloudbased products, css, data operations, data platforms, hadoop, hbase, hive, impala, java, javascript, jsp, mahout, mapreduce, pentaho qlikview, pig, python, r, shell, spring, sqoop"
Data Visualization Expert( BI Developer),Radiant Digital,"Basking Ridge, NJ",https://www.linkedin.com/jobs/view/data-visualization-expert-bi-developer-at-radiant-digital-3762432661,2023-12-17,Oak Ridge,United States,Mid senior,Onsite,"About Us
Radiant Digital delivers technology consulting and business solutions for commercial and government clients.
Our flexible delivery model allows us to provide end-to-end solution delivery, single project execution, and, or strategic resources.
CMMI Maturity Level III and ISO 9001 – 2015 certified.
Responsibilities
Optional:
Telecom Or Wireless Experience
We expect this resource to be mostly remote. On an occasional basis (1-2 times per month), the resource could be expected to be at Verizon corporate office (either Basking Ridge, NJ or Tampa, FL) to collaborate with the VZ team.
What we’re looking for... You’ll need to have:
Minimum of a Bachelor’s Degree
Minimum of 5 years experience in a similar role in the design, development and deployment of BI solutions with at least 3 years using Tableau
Creating visualization to create mock-ups by leveraging large data sets
Demonstrated expertise in utilizing data transformation tools
Exceptional analytical skills with the ability to process raw data, extract meaningful insights, and determine the most effective visualization methods
Strong SQL query knowledge and experience.
Even better if you have one or more of the following:
Experience with Google Cloud Platform
Experience with Geospatial Analytics and Visualizations (map layers)
Telecom or Wireless experience What you'll be doing
Design, develop, and maintain Tableau dashboards and reports that provide actionable insights for data-driven decision-making
Collaborate with business stakeholders, data analysts, and other team members to define requirements, gather and refine data, and develop solutions that meet business objectives
Work with the Engineering team to automate workflows and backend data feeding the dashboards
Work with stakeholders to pull custom reports from the dashboard
Education/Certifications
Minimum of a Bachelor’s Degree
Certification in Tableau or other related visualization tools
LOGISTICS:
We expect this resource to be mostly remote. On an occasional basis (1-2 times per month), the resource could be expected to be at the Verizon corporate office (either Basking Ridge, NJ or Tampa, FL) to collaborate with the VZ team.
Show more
Show less","Tableau, Data Transformation Tools, SQL, Google Cloud Platform, Geospatial Analytics, Visualization, Data Analysis, DataDriven Decision Making, Data Visualization, Report Generation, Backend Data Feeding, Business Intelligence (BI), Mockups","tableau, data transformation tools, sql, google cloud platform, geospatial analytics, visualization, data analysis, datadriven decision making, data visualization, report generation, backend data feeding, business intelligence bi, mockups","backend data feeding, business intelligence bi, data transformation tools, dataanalytics, datadriven decision making, geospatial analytics, google cloud platform, mockups, report generation, sql, tableau, visualization"
Financial Data Analyst & Developer,PromptCare,"New Providence, NJ",https://www.linkedin.com/jobs/view/financial-data-analyst-developer-at-promptcare-3781037436,2023-12-17,Oak Ridge,United States,Mid senior,Onsite,"Job Type
Full-time
Description
PromptCare is a leading provider of home respiratory and infusion services, serving pediatric and adult patients in 30+ states from 36 locations throughout the United States. Our Hi-Tech, Hi-Touch care offerings deliver superior patient care and outcomes making PromptCare a preferred partner for hospitals, physicians, and payors. PromptCare currently services 6,400+ respiratory patients, 2,500+ infusion patients, and 4,400+ enteral nutrition patients. We estimate that we are the largest provider of ventilation care to pediatric patients in the U.S. Our mission is to be the preferred choice for superior management of complex cases, providing unsurpassed clinical expertise, reliable service, and boundless compassion in transitioning patients to the home setting and reuniting them with their loved ones. Our Differentiation consists of an unrivaled team of respiratory therapists, nurses, dietitians, pharmacists, and other care professionals who provide highly sought-after expertise, advice, and patient care. We are the “go-to” provider in our markets among leading pediatric hospitals and care facilities to handle the most acute patient situations. PromptCare has an expansive payer network participation, supported by a robust billing & collections operation.
We are looking for an experienced and highly motivated Financial Data Analyst & Developer to join our team.
This is a full-time position, open to in office, hybrid or remote. 40 hours per week, Monday - Friday. The ideal candidate should reside in Central or Eastern Time zone.
Job Summary
In this role, the Financial Data Analyst & Developer will be tasked with becoming the subject matter expert in Solver (a Corporate Performance Management solution that consolidates data from our financial accounting system and other key ERP sources). Extensive in-person and online training will be provided in order to fulfill this requirement. You will be responsible for supporting the organization through the development, implementation, and support of operational and financial reporting solutions. You will also be expected to collaborate with other departments in order to leverage data and insights to drive business decisions. If you have a keen eye for detail, strong problem-solving skills, and a passion for data analysis, we want you on our team!
Requirements
Bachelor’s degree in Finance, Economics, Computer Science, or a related field. Master’s degree a plus
Proven experience in financial analysis, budgeting, and forecasting
Strong proficiency in software development with a focus on customized analytical tools
Proficiency in Microsoft 365 applications, including Power Platform, Excel, and Power BI
Experience in data management, validation, and accuracy checks
Excellent problem-solving skills and attention to detail
Healthcare services experience is a major advantage
Must have extensive experience building complex models and mining data using Excel
Microsoft Dynamics 365 Business Central with Solver Reporting experience (strongly preferred)
Tableau experience (preferred)
Job Responsibilities, Included But Not Limited To
Develop, design, and maintain Financial Statement and departmental expense reporting, corporate KPI reporting, and other operational analytics solutions using the Solver CPM
Develop permanent, automated, ongoing analytics solutions as well as conduct specific ad hoc analysis
Develop and maintain comprehensive documentation of data analysis and reporting processes
Develop data-driven solutions to business and operational problems
Transform raw data into meaningful insights and actionable information
Utilize advanced statistical software and analytics tools to develop detailed reports
Develop and implement strategies for data acquisition, storage, and maintenance
Ensure accuracy of data and maintain data integrity
Collaborate with other teams and departments to develop data-driven solutions
Provide technical expertise and guidance in the use of data analysis techniques
Physical Demands
The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this job. While performing the duties of this job, the employee is regularly required to talk and listen. The employee is required to occasionally walk, and bend or stand as necessary. This position requires the ability to occasionally open filing cabinets, lift office products and supplies of 25 pounds but no more than 50 pounds.
Benefits & Perks
Competitive Salary
Medical, Dental, Vision Package
401(k) plan (discretionary)
PTO - Vacation, Sick and Personal days, and paid holidays
Referral bonuses & Plum Benefits Discount Ticket Program
Lunch & Learn Programs
Employee Recognition Program
Equal Employment Opportunity
The PromptCare Companies is committed to Equal Employment Opportunity (EEO) and prohibits employment discrimination on the basis of race, color, age, national origin, religion, gender, gender identity, sexual orientation, pregnancy, marital status, genetic disposition, disability, veteran's status or any other characteristic or classification protected by State/Federal/Local laws. We foster a work environment in which diversity and inclusion are embraced, people are hired and advanced on their merits, and employees are treated with mutual respect and dignity.
Show more
Show less","Solver, Microsoft Dynamics 365 Business Central, Solver Reporting, Tableau, Power Platform, Excel, Power BI, Microsoft 365, Financial analysis, Budgeting, Forecasting, Data management, Data validation, Data accuracy checks, Problemsolving, Attention to detail, Healthcare services, Complex models, Data mining, Advanced statistical software, Data acquisition, Data storage, Data maintenance, Data integrity, Datadriven solutions, Data analysis techniques","solver, microsoft dynamics 365 business central, solver reporting, tableau, power platform, excel, power bi, microsoft 365, financial analysis, budgeting, forecasting, data management, data validation, data accuracy checks, problemsolving, attention to detail, healthcare services, complex models, data mining, advanced statistical software, data acquisition, data storage, data maintenance, data integrity, datadriven solutions, data analysis techniques","advanced statistical software, attention to detail, budgeting, complex models, data accuracy checks, data acquisition, data analysis techniques, data integrity, data maintenance, data management, data mining, data storage, data validation, datadriven solutions, excel, financial analysis, forecasting, healthcare services, microsoft 365, microsoft dynamics 365 business central, power platform, powerbi, problemsolving, solver, solver reporting, tableau"
Technology & Data – Senior Systems Operations Engineer,Strategic Staffing Solutions,"Summit, NJ",https://www.linkedin.com/jobs/view/technology-data-%E2%80%93-senior-systems-operations-engineer-at-strategic-staffing-solutions-3784866983,2023-12-17,Oak Ridge,United States,Mid senior,Onsite,"Sr. Production Support Engineer
Location:
Summit, NJ
– Lead or triage major incidents management.
– Perform problem management to identify root cause and preventative actions
– Monitor application health and reduce alerts and failures
– Monitor batch and feed management
– Investigate and troubleshoot failures
– Support disaster recovery exercise
– Drive solutions to reduce recovery
– Resolve troublesome trends as they develop
– Direct the daily risk and control flow of operations, focusing on policies, procedures and work standards to ensure success
– Collaborate and consult with peers, colleagues and managers to resolve issues and achieve goals
Requirements:
– 5+ years of application production support
– 5+ years of unix, sql, autosys
– 5+ years of Ab Initio experience
– 5+ years of Teradata experience
Desired Qualifications:
– Hadoop experience
– Oracle experience
– Informatica experience
Job ID:
JOB-233041
Publish Date:
14 Dec 2023
Tagged as: Technology & Data - Senior Systems Operations Engineer
Show more
Show less","Incident Management, Problem Management, Application Health Monitoring, Batch and Feed Management, Failure Investigation and Troubleshooting, Disaster Recovery, Risk and Control Flow Management, Unix, SQL, Autosys, Ab Initio, Teradata, Hadoop, Oracle, Informatica","incident management, problem management, application health monitoring, batch and feed management, failure investigation and troubleshooting, disaster recovery, risk and control flow management, unix, sql, autosys, ab initio, teradata, hadoop, oracle, informatica","ab initio, application health monitoring, autosys, batch and feed management, disaster recovery, failure investigation and troubleshooting, hadoop, incident management, informatica, oracle, problem management, risk and control flow management, sql, teradata, unix"
"Research Scientist/Engineer, Interdisciplinary Engineering and Data Science",Northeastern University,"Portland, ME",https://www.linkedin.com/jobs/view/research-scientist-engineer-interdisciplinary-engineering-and-data-science-at-northeastern-university-3531291041,2023-12-17,Brunswick,United States,Mid senior,Onsite,"About The Opportunity
Summary and Responsibilities:
Northeastern University invites applications for one or more Research Scientist positions in the field of Interdisciplinary Engineering and Data Science with a demonstrated entrepreneurial mindset, located at The Roux Institute at Northeastern University, in Portland, Maine. The successful candidate will be named as an “Institute Member” at the Roux Institute.
As an independent contributor to the Engineering research mission, the Research Scientist will contribute to research programs in data-driven approaches for interdisciplinary engineering. Successful candidates will emphasize at least one of the following domains: mechanics and physics-based models and data science in engineering in systems of fluids, solids, motions, thermodynamics, biophysics, and electrostatics. Areas of research will focus on data-driven methods in broad areas of engineering including, but not limited to, Engineering Design, Energy, Computer Engineering, Power Electronics, Bioengineering, Ergonomic, Human Factors, Engineering of health systems, Manufacturing, Robotics, Autonomy, Internet of Things, and Cybersecurity. An effective research program will execute use-inspired discovery with an entrepreneurial mindset that develops science and solutions that matter to society and impact the Maine economy. The research is positioned in the fields of interdisciplinary Engineering and Data Science.
Research Scientists will contribute to a portfolio of highly matrixed collaborations on the Institute campus in Portland, ME, and beyond and will have an opportunity to build their own research projects. Collaborators will include faculty and professional research staff at The Roux Institute, faculty across the Northeastern network, and industry researchers who may be co-located at the Institute or located at their own company. The Northeastern University research network available includes the collection of highly innovative university institutes, including the Institute for Experiential Artificial Intelligence, the Institute for Experiential Robotics, the Institute for the Wireless Internet of Things, and the Institute for Chemical Imaging of Living Systems.
The Roux Institute is designed as an engine of innovation, talent, and economic growth in Portland, the state of Maine, and the region. Partnership is what sets our education and research model apart. With leading companies at the table from day one, we are creating an agile workforce prepared to thrive in a competitive landscape powered by artificial intelligence as a force for good and an environment for innovation in the life sciences and other high-growth sectors.
Required Qualifications
Masters, Ph.D. or other terminal degrees in engineering or related fields
Demonstrated track record of scholarly achievement and scientific/engineering impact with a commensurate publication history and/or demonstrated record of translating scientific/engineering discovery to society
Experience with the application of data science to engineering applications
Excellent communication skills across diverse audiences and settings. Ability to work in a fast-paced and innovative environment. Adaptability. Experience working as a member of an interdisciplinary team.
Ability to work effectively within entrepreneurial and interdisciplinary teams or researchers
Desired Qualifications
5 years of industrial experience in engineering and science
Demonstrated experience moving technologies through Technology Readiness Levels
Post-doctoral training in a relevant engineering discipline
The Roux Institute is strongly committed to fostering excellence through diversity and recognizes the strength that diversity in all its forms brings to the research endeavor. We invite applications from all qualified candidates and enthusiastically welcome applications from women, minorities, persons with disabilities, and persons of any sexual orientation, gender identity, and gender expression.
Position Type
Research
Additional Information
Northeastern University considers factors such as candidate work experience, education and skills when extending an offer.
Northeastern has a comprehensive benefits package for benefit eligible employees. This includes medical, vision, dental, paid time off, tuition assistance, wellness & life, retirement- as well as commuting & transportation. Visit https://hr.northeastern.edu/benefits/ for more information.
Northeastern University is an equal opportunity employer, seeking to recruit and support a broadly diverse community of faculty and staff. Northeastern values and celebrates diversity in all its forms and strives to foster an inclusive culture built on respect that affirms inter-group relations and builds cohesion.
All qualified applicants are encouraged to apply and will receive consideration for employment without regard to race, religion, color, national origin, age, sex, sexual orientation, disability status, or any other characteristic protected by applicable law.
To learn more about Northeastern University’s commitment and support of diversity and inclusion, please see
www.northeastern.edu/diversity
.
Show more
Show less","Data Science, Engineering Design, Energy, Computer Engineering, Power Electronics, Bioengineering, Ergonomic, Human Factors, Engineering of health systems, Manufacturing, Robotics, Autonomy, Internet of Things, Cybersecurity, Artificial Intelligence, Machine Learning, Data Mining, Statistical Analysis, Programming Languages, Software Development, Project Management, Teamwork, Communication Skills, Adaptability, Interdisciplinary Collaboration, Technology Readiness Levels, Postdoctoral Training","data science, engineering design, energy, computer engineering, power electronics, bioengineering, ergonomic, human factors, engineering of health systems, manufacturing, robotics, autonomy, internet of things, cybersecurity, artificial intelligence, machine learning, data mining, statistical analysis, programming languages, software development, project management, teamwork, communication skills, adaptability, interdisciplinary collaboration, technology readiness levels, postdoctoral training","adaptability, artificial intelligence, autonomy, bioengineering, communication skills, computer engineering, cybersecurity, data mining, data science, energy, engineering design, engineering of health systems, ergonomic, human factors, interdisciplinary collaboration, internet of things, machine learning, manufacturing, postdoctoral training, power electronics, programming languages, project management, robotics, software development, statistical analysis, teamwork, technology readiness levels"
Business Data Analyst,Arkatechture,"Portland, ME",https://www.linkedin.com/jobs/view/business-data-analyst-at-arkatechture-3774854571,2023-12-17,Brunswick,United States,Mid senior,Remote,"Department:
Professional Services
Employment Type:
Full Time
Location:
Portland, ME, USA
Description
Why Work Here?
At Arkatechture, we have a simple shared mission: to build a sustainable organization built upon three pillars: Do something meaningful, With a great team, Earning what you deserve.
We started in 2012 with a passion for data, business, and getting things done. We are a team of data lovers and technical experts who use our skills to help businesses big and small harness, utilize, and optimize their data. As New England's Data Resource, we are a small company constantly evolving to keep up with changing landscapes in the data world.
We are proud of the community and culture that we've created at Arkatechture, and we have no intention of slowing down. We offer a competitive benefits package that includes:
A flexible work-from-home policy
Open-concept office in Portland, ME with an easy-going dress code, and fresh pots and pops all day (that's coffee and popcorn!)
Training & certificate reimbursement
A competitive benefits package that includes medical, disability, life insurance and optional dental/vision
401K Retirement planning with company matching
Generous paid time off and eleven paid holidays
Employee recognition through milestone awards including annual PTO increases, as well as a 4 day work-week at 3 years of service!
All employees share our core values: put the team first, practice humility, take pride in everything we do, stay curious, care for our community & environment, take work seriously; ourselves not so much.
The Position
Arkatechture is seeking candidates for a Business Data Analyst position to support Business Intelligence and Data warehousing initiatives focused on our Financial Services clients.
As a Business Data Analyst, responsibilities include data analysis, data profiling, data mapping, and developing SQL views. This role requires proficiency in interpreting data, trends, patterns in complex data sets, technical expertise with data models, databases and reporting tools like Tableau or PowerBI. The ability to interpret and use artifacts like business requirement documents, system requirements documents, requirement traceability matrix, workflows, data mapping, etc. is required. The ideal candidate will be able to work efficiently with the business and the technology teams collaborating with the project/program managers. This position has the opportunity to be 100% remote if desired.
How To Apply
Please send a cover letter and resume with your application.
You must submit both documents to be considered for the position
. Don't meet every single requirement? Studies have shown that women and people of color are less likely to apply to jobs unless they meet every single qualification. At Arkatechture we are dedicated to building a diverse, inclusive and authentic workplace, so if you're excited about this role but your past experience doesn't align perfectly with every qualification in the job description, we encourage you to apply anyways. You may be just the right candidate for this or other roles.
Key Responsibilities
Requirement gathering for Data pipeline development
Managing client interactions and growing client relationship during projects
Develop Source to Target Mappings
Data Profiling, analyzing source system data and metadata and collect requirements for data ingestion into a data warehouse
Developing and deconstructing complex SQL, Common Table Expressions, Stored Procedures, and Views for Business Conformance layers
Data Cleansing, Data Quality assessments and developing data quality rules
Building reference data for hierarchies, lookups and grouping purposes
Acquiring data from primary or secondary data sources and developing and maintaining Data Dictionaries
Identify, analyze, and interpret trends or patterns in complex data sets
Filter and ""clean"" data by reviewing reports and key performance indicators to locate and correct code problems
Testing using SQL and providing development sign off
Locate and define new process improvement opportunities
Documentation of processes and workflows
Additional responsibilities as assigned
Minimum Qualifications
Skills, Knowledge and Expertise
Bachelor's Degree in a relevant field or 2-5 years of experience in a similar role
SQL Experience
Domain expertise in Financial Services working with Banks and/or Credit Unions, preferably focused on full credit lifecycle for Commercial and/or Consumer lending
Enthusiastic, learner and driven with a positive attitude
Strong critical and analytical thinking and problem solving skills
Experience working on multiple databases such as Snowflake, SQL Server, Databricks, Oracle, PostgreSQL, MySQL, etc
A strong understanding of Agile software development life cycle and methodology
Experience working on Data Management projects for Data Lakes/Data Warehousing
Preferred Experience
2+ years as a Subject Matter Expert in Financial Services, especially Commercial and/or Consumer lending
Understanding of cloud platforms such as AWS
Experience developing and implementing dbt models
Show more
Show less","SQL, Tableau, PowerBI, Data Analysis, Data Profiling, Data Mapping, Agile, Snowflake, SQL Server, Databricks, Oracle, PostgreSQL, MySQL, Cloud Platforms, AWS, dbt models","sql, tableau, powerbi, data analysis, data profiling, data mapping, agile, snowflake, sql server, databricks, oracle, postgresql, mysql, cloud platforms, aws, dbt models","agile, aws, cloud platforms, data mapping, data profiling, dataanalytics, databricks, dbt models, mysql, oracle, postgresql, powerbi, snowflake, sql, sql server, tableau"
Supply Chain Analyst / Data Analyst,Michael Page,"Belrose, New South Wales, Australia",https://au.linkedin.com/jobs/view/supply-chain-analyst-data-analyst-at-michael-page-3645209950,2023-12-17,Gosford, Australia,Mid senior,Onsite,"Join a Global organisation recognised worldwide|Newly created role with scope and autonomy to make a difference
About Our Client
My client is a leading Automotive Company present in 50 countries across the globe.
Job Description
Collaborate with stakeholders to execute and implement effective data solutions across the business.
Design, deliver, and maintain multi-year data and analytic capabilities for product coverage, product gaps, car parc and other
Implement optimized, integrated data processes to support internal customers and drive improvements in data accuracy and reliability.
Ensure timely production of data and analytical assets.
Monitor, report, resolve, and coordinate data quality metrics and systems integration as a local data steward.
The Successful Applicant
Proven experience in researching and managing complex data sets within IT, Supply Chain or Engineering
Knowledge of Excel (VBA) and Reporting tools Power BI and SQL will be of great advantage.
Knowledge of data extraction and analysis tools (e.g., SAP, PSP, etc.).
Excellent communication and project management skills.
Strong stakeholder management skills
What's on Offer
Opportunity to work with a leading automotive company and contribute to its success in the industry.
Collaborative and innovative work environment.
Competitive salary and benefits package.
Career growth and development opportunities.
Contact: Thalita Benecke
Quote job ref: JN-062023-6095508
Show more
Show less","Data analytics, Data management, Data integration, Data governance, Data quality, Data visualization, Business intelligence, Power BI, SQL, Excel, SAP, PSP, Project management, Stakeholder management, Communication skills","data analytics, data management, data integration, data governance, data quality, data visualization, business intelligence, power bi, sql, excel, sap, psp, project management, stakeholder management, communication skills","business intelligence, communication skills, data governance, data integration, data management, data quality, dataanalytics, excel, powerbi, project management, psp, sap, sql, stakeholder management, visualization"
Azure Data Engineer - No 3rd party candidates,Dexian,"Lake Forest, IL",https://www.linkedin.com/jobs/view/azure-data-engineer-no-3rd-party-candidates-at-dexian-3775473504,2023-12-17,North Chicago,United States,Mid senior,Hybrid,"Job Summary:
Dexian is seeking a Principal Azure Data Engineer for an opportunity with a client located in Lake Forest, IL.
Responsibilities:
Leads the Engineering, prototypes, and builds complex data management systems that combine core data sources into data warehouses, cloud data lake, data science platforms or other accessible structures
Leads Data Modeling, collaborates with data architecture and Enterprise Integration teams
Leads enterprise data strategy, researches various tools, frameworks and processes for building scalable modern data platforms as well as help business stakeholders with establishing consumption patterns for reporting and advanced analytics
Leads Designs and implements processes to ensure data integrity and standardization
Recommends and implements data reliability, efficiency, and quality improvements
Collaborates with Data Governance and Data compliance team members
Learns and applies modern data preparation, integration and metadata management tools and techniques
Leads strategy for implementing new techniques in the enterprise by adhering to architectural standards and best practices, business stakeholder communication
Requirements
:
Bachelor's in CS or quantitative field AND 8-10 years of data engineering (Masters preferred)
Strong effective communication skills, both verbal and written
Experience in Azure cloud with Data Lake, Synapse database, Azure Data Factory and Databricks
Experience in Data Analysis using various tools such as SQL, SAS, R, Python
Experience in Data Modeling
Experience in reporting and dashboarding tools such as Tableau, Power BI etc
Working knowledge of Data Quality, Data Governance and Data compliance tools
Has a wide range of experience knowledge and/or skill in specific aspects of the job within their own discipline and is learning about the broader function. Anticipates problems and is able to recognize wider implications
Determines methods and procedures to complete new assignments
Duties and assignments require considerable judgement and initiative
Understands the duties and responsibilities of the job and their practical implications and any deviations
Analyzes complex problems and recommends solutions
Dexian is a leading provider of staffing, IT, and workforce solutions with over 12,000 employees and 70 locations worldwide. As one of the largest IT staffing companies and the 2nd largest minority-owned staffing company in the U.S., Dexian was formed in 2023 through the merger of DISYS and Signature Consultants. Combining the best elements of its core companies, Dexian's platform connects talent, technology, and organizations to produce game-changing results that help everyone achieve their ambitions and goals.
Dexian's brands include Dexian DISYS, Dexian Signature Consultants, Dexian Government Solutions, Dexian Talent Development and Dexian IT Solutions. Visit https://dexian.com/ to learn more.
Dexian is an Equal Opportunity Employer that recruits and hires qualified candidates without regard to race, religion, sex, sexual orientation, gender identity, age, national origin, ancestry, citizenship, disability, or veteran status.
Show more
Show less","Azure, Data Lake, Synapse database, Azure Data Factory, Databricks, SQL, SAS, R, Python, Data Modeling, Tableau, Power BI, Data Quality, Data Governance, Data compliance","azure, data lake, synapse database, azure data factory, databricks, sql, sas, r, python, data modeling, tableau, power bi, data quality, data governance, data compliance","azure, azure data factory, data compliance, data governance, data lake, data quality, databricks, datamodeling, powerbi, python, r, sas, sql, synapse database, tableau"
Research Data Analyst Associate,Northwestern University,"Evanston, IL",https://www.linkedin.com/jobs/view/research-data-analyst-associate-at-northwestern-university-3784863424,2023-12-17,North Chicago,United States,Mid senior,Hybrid,"Department:
WCAS Economics
Salary/Grade:
EXS/6
Job Summary
Utilizes multiple quantitative and qualitative datasets from a variety of sources and interprets results using various techniques, ranging from simple data aggregation via statistical analysis to complex data mining. Assists in the design, development and implementation of business solutions partnering with clients to identify analytical requirements. Compiles reports, charts, and tables based on established statistical methods.
This Research Data Analyst Associate position in the Department of Economics is a full-time position and works on-site four days per week with the option of working one day remote.
The full-time Pre-Doctoral Research Assistant starting in the Summer of 2024 will be deeply involved in policy-oriented and data-intensive research on labor markets and market power in developing countries. Responsibilities will include collecting and analyzing large administrative datasets, designing and implementing randomized controlled trials, and conducting literature reviews. There may also be opportunities for field travel to India.
The RA Will Work On Projects That Closely Combine Theory And Data, With Opportunities To Be Involved In Both. Sample Questions Studied Include
The causes and consequences of gender gaps in the labor market (for example, the effect of maternity leave policies on women’s career trajectories)
The consequences of and remedies for imperfect competition in various markets (for example, collusion among employers, and the effects of introducing competition among agricultural traders)
The Pre-Doctoral Research Assistant will be fully integrated into the research community at the Global Poverty Research Lab at Northwestern.
Specific Responsibilities
Administration
Develop standardized templates, policies, and procedures for internal and external
reporting.
Designs and prepares documentation for qualitative and quantitative data
analysis.
Development
Completes a variety of statistical analyses using appropriate data and software
Merges data from multiple sources in preparation for analysis.
Interprets results of analyses for use in reports and presentations
Performance
Completes data cleaning and organizes data files for analysis.
Reviews data and Identifies quality issues
Supervises
Other duties as assigned.
Minimum Qualifications: (Education, experience, and any other certifications or clearances)
Successful completion of a full 4-year course of study in an accredited college or university leading to a bachelor's or higher degree; OR appropriate combination of education and experience.
Stata
Minimum Competencies: (Skills, knowledge, and abilities.)
Strong quantitative background, 2. Significant programming experience 3. Natural attention to detail 4. Ability to work independently to solve problems
Preferred Qualifications: (Education and experience)
A background in economics is *not* required. I strongly encourage candidates with strong technical skills and an interest in economics to apply.
Preferred Competencies: (Skills, knowledge, and abilities)
Boundless enthusiasm for empirical research including handling and analyzing large data sets.
Strong coding skills, and
The ability to independently solve challenging problems
Coding proficiency is required. Experience in Stata is valued; familiarity with Python, R, and web scraping is a plus.
The target hiring range for this position will be between $55,128 - $ 77,525 per year. Offered salary will be determined by the applicant’s education, experience, knowledge, skills and abilities, as well as internal equity and alignment with market data.
Benefits
At Northwestern, we are proud to provide meaningful, competitive, high-quality health care plans, retirement benefits, tuition discounts and more! Visit us at
https://www.northwestern.edu/hr/benefits/index.html
to learn more.
Work-Life And Wellness
Northwestern offers comprehensive programs and services to help you and your family navigate life’s challenges and opportunities and adopt and maintain healthy lifestyles.
We support flexible work arrangements where possible and programs to help you locate and pay for quality, affordable childcare and senior/adult care. Visit us at
https://www.northwestern.edu/hr/benefits/work-life/index.html
to learn more.
Professional Growth & Development
Northwestern supports employee career development in all circumstances whether your workspace is on campus or at home. If you’re interested in developing your professional potential or continuing your formal education, we offer a variety of tools and resources. Visit us at
https://www.northwestern.edu/hr/learning/index.html
to learn more
.
Northwestern strongly recommends COVID-19 vaccinations and boosters for people who can obtain them as a critical tool for minimizing severe illness. More information can be found on the COVID-19 and Campus Updates webpage.
The Northwestern campus sits on the traditional homelands of the people of the Council of Three Fires, the Ojibwe, Potawatomi, and Odawa as well as the Menominee, Miami and Ho-Chunk nations. We acknowledge and honor the original people of the land upon which Northwestern University stands, and the Native people who remain on this land today.
Northwestern University is an Equal Opportunity, Affirmative Action Employer of all protected classes, including veterans and individuals with disabilities. Women, racial and ethnic minorities, individuals with disabilities, and veterans are encouraged to apply. Click for information on
EEO is the Law.
Show more
Show less","Data Analysis, Statistical Analysis, Data Mining, Business Solutions, Reporting, Charts, Tables, Research, Policy, DataIntensive, Randomized Controlled Trials, Literature Reviews, Field Travel, Quantitative Background, Programming, Attention to Detail, Independent Problem Solving, Empirical Research, Coding, Stata, Python, R, Web Scraping","data analysis, statistical analysis, data mining, business solutions, reporting, charts, tables, research, policy, dataintensive, randomized controlled trials, literature reviews, field travel, quantitative background, programming, attention to detail, independent problem solving, empirical research, coding, stata, python, r, web scraping","attention to detail, business solutions, charts, coding, data mining, dataanalytics, dataintensive, empirical research, field travel, independent problem solving, literature reviews, policy, programming, python, quantitative background, r, randomized controlled trials, reporting, research, stata, statistical analysis, tables, web scraping"
Data Analyst,CyberCoders,"Pico Rivera, CA",https://www.linkedin.com/jobs/view/data-analyst-at-cybercoders-3695999682,2023-12-17,Pomona,United States,Mid senior,Onsite,"If you are a Data Analyst with experience, please read on!
What You Need for this Position
Data Analysis
Data Collection
Data Validation
Data Mining
Tableau
Power BI
SQL
Python
R
So, if you are a Data Analyst with experience, please apply today!
Email Your Resume In Word To
Looking forward to receiving your resume through our website and going over the position with you. Clicking apply is the best way to apply, but you may also:
Derek.Ceperley@CyberCoders.com
Please do NOT change the email subject line in any way. You must keep the JobID: linkedin : DC12-1760041 -- in the email subject line for your application to be considered.***
Derek Ceperley - Executive Recruiter - CyberCoders
Applicants must be authorized to work in the U.S.
CyberCoders is proud to be an Equal Opportunity Employer
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, protected veteran status, or any other characteristic protected by law.
Your Right to Work
– In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire.
CyberCoders will consider for Employment in the City of Los Angeles qualified Applicants with Criminal Histories in a manner consistent with the requirements of the Los Angeles Fair Chance Initiative for Hiring (Ban the Box) Ordinance.
Show more
Show less","Data Analysis, Data Collection, Data Validation, Data Mining, Tableau, Power BI, SQL, Python, R","data analysis, data collection, data validation, data mining, tableau, power bi, sql, python, r","data collection, data mining, data validation, dataanalytics, powerbi, python, r, sql, tableau"
Business Data Analyst,"Virtualitics, Inc.","Pasadena, CA",https://www.linkedin.com/jobs/view/business-data-analyst-at-virtualitics-inc-3705746161,2023-12-17,Pomona,United States,Mid senior,Onsite,"Virtualitics, Inc. is an advanced analytics company that helps enterprises and governments make smarter business decisions faster with ready-to-use AI that can be understood–by analysts and business leaders alike. Our AI platform allows organizations to rapidly process complex data into powerful multi-dimensional graph visualizations and predict future business outcomes with clear, explainable, no-code AI modeling. Virtualitics puts AI into use across the enterprise with enhanced analytics that is easily integrated into the flow of work. Our patented technology is based on over 11 years of research at the California Institute of Technology and NASA Jet Propulsion Laboratory.
The platform has been deployed across Department of Defense agencies, Federal Civilian agencies, and Fortune 500 companies. We're proud to have been recently named as one of the Air Force's 'big bets' on emerging technology and Built In’s top companies to work for in 2022!
Our fast-growing team seeks a skilled, motivated, and reliable
Business Data Analyst
to help grow our company.
What You’ll Do
Select and implement a business intelligence tool to allow the executive team to gain a better understanding of company operations. You will design, develop, and maintain business dashboards to provide actionable insights.
Reporting to the CFO, you will own the entire business intelligence function: you will be the expert on company data and use that knowledge to answer questions, provide insight, and improve the business.
Perform ad hoc analysis to solve time-sensitive operational issues and business cases quickly.
Monitor day-to-day business trends, market conditions, competitor strategies, and client needs, focusing on short- and long-term impacts on key KPIs and business strategies.
Create and deliver compelling data analysis presentations that explain in-depth insights to an experienced and diverse group of leaders in simple terms.
Preparing and cleansing large data sets, as required for reporting and analysis.
Work closely with colleagues across the organization and build strong relationships to drive improvements across the business. Collaborate with GTM and Technical teams to develop metrics and dashboards that support company objectives and key results (OKRs).
Proactively provide updates on progress and/or blockers when working on tasks and seek out feedback for continued self-improvement.
Consulting: Provide guidance to stakeholders on how to use data in decision-making
Your Qualifications
5+ years of experience delivering business analytics and a proven track record of collaborating with a cross-functional team to deliver results.
1-2 years of SaaS industry experience required.
Exemplary analytical problem-solving skills and knowledge of business intelligence tools (I.e., Domo, Looker, Tableau).
Advanced knowledge of SQL.
Familiarity with a Modern Data Stack (I.e., ETL tools like dbt, Airflow, Fivetran, and cloud data warehouses like BigQuery, Snowflake, Azure, or Redshift).
Excellent communication and collaboration skills, with the ability to influence stakeholders at all levels of the organization.
Strong attention to detail and excellent written/verbal communication skills.
Detail-oriented: A strong attention to detail and a well-reasoned sense of skepticism toward the results of analyses.
What We Offer You
We are a team of ambitious individuals who are passionate about creating a revolutionary software company. At Virtualitics, you will have a significant career development opportunity and a chance to contribute to one of the fastest-growing startups in Southern California. Our benefits include highly competitive pay, equity, and fully paid health/vision / dental insurance for you + your dependents, and unlimited PTO.
Virtualitics is committed to building a company where everyone can bring their full impact and reach their fullest potential. Our mission is to build a diverse and inclusive environment where talented people of all cultures, ages, perspectives, opinions, education, backgrounds, races, gender identities, religions, orientations, abilities, and beliefs can grow and thrive with Virtualitics.
Show more
Show less","Business intelligence, Data analysis, Data visualization, Data modeling, SQL, BigQuery, Snowflake, Azure, Redshift, Domo, Looker, Tableau, dbt, Airflow, Fivetran","business intelligence, data analysis, data visualization, data modeling, sql, bigquery, snowflake, azure, redshift, domo, looker, tableau, dbt, airflow, fivetran","airflow, azure, bigquery, business intelligence, dataanalytics, datamodeling, dbt, domo, fivetran, looker, redshift, snowflake, sql, tableau, visualization"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Cypress, CA",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3744901100,2023-12-17,Pomona,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
They are a profitable dating application.
Location: This is a hybrid role based in their Chicago office and will require you to be in office Tuesdays and Thursdays.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location
: This role is twice a week in a hybrid role minimum.
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, Kafka, Storm, SparkStreaming, Data Warehouses, ETL, SQL, TDD, Pair Programming, Continuous Integration, automated testing, deployment","python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, kafka, storm, sparkstreaming, data warehouses, etl, sql, tdd, pair programming, continuous integration, automated testing, deployment","airflow, automated testing, continuous integration, data warehouses, deployment, docker, etl, helm, kafka, kubernetes, pair programming, python, snowflake, spark, sparkstreaming, sql, storm, tdd"
Data Engineer IV,Navy Federal Credit Union,"Vienna, VA",https://www.linkedin.com/jobs/view/data-engineer-iv-at-navy-federal-credit-union-3768699032,2023-12-17,Manassas,United States,Mid senior,Hybrid,"Overview
Develop strategies for data acquisition, archive recovery, and database implementation. Responsible for designing, building, integrating data from various resources, and managing big data. Develop data consumption patterns to share data with internal/external channels, while ensuring they are easily accessible, work smoothly, with the goal of optimizing the performance of Navy Federal’s big data ecosystem. Recognized as an expert with a specialized depth and/or breadth of expertise in discipline. Solves highly complex problems; takes a broad perspective to identify solutions. Leads functional teams or projects. Works independently.
Responsibilities
Define and build data integration processes to be used across the organization
Build channel contracts and data consumption patterns for customer facing (On-line/Mobile) channels
Analyze and validate data sharing requirements within and outside data partners
Recognize potential issues and risks during the project implementation and suggest mitigation strategies
Communicate and own the processes related to contracts and data consumption patterns
Expert and key point of contact between the operational data hubs and the channel contracts for On-line/Mobile
Apply engineering principles into the design and enhancement of new and existing data management systems
Coach and mentor project team members in carrying out project implementation activities
Work directly with business leadership to understand data requirements; propose and develop solutions that enable effective decision-making and drives business objectives
Prepare advanced project implementation plans which highlight major milestones and deliverables, leveraging standard methods and work planning tools
Lead the preparation of high-quality project deliverables that are valued by the business and present them in such a manner that they are easily understood by project stakeholders
Ensure the security and integrity of system and product solutions including compliance with Navy Federal, industry engineering and Information Security principles and practices
Present clear, organized and concise information to all audiences through a variety of media to enable effective business decisions
Perform other duties as assigned
Qualifications
Master’s degree in Information Systems, Computer Science, Engineering, or related field, or the equivalent combination of education, training and experience
Advanced skills in systems and application integration in a large, distributed architecture environments
Proficient skill level in .Net/C#, Python, Agile Frameworks (SAFE), Microsoft Databricks, Azure Data Factory
Proficient skills in developing and operationalizing various data distribution patterns like, APIs, event based, pub/sub models
Proficient in Data Architecture, Web Services, REST APIs, Event and Pub/Sub messaging architecture
Proficient in Mobile and Web application technologies
Ability to understand the business problem and determine what aspects of it require optimization; articulate those aspects in a clear and concise manner
Proficient skills in understanding SQL and NoSQL and JSON structure
Ability to understand other projects or functional areas to consolidate analytical and operational needs and processes
Demonstrates change management and/or excellent communication skills
Working knowledge of various data structures and the ability to extract data from various data sources
Understands the concepts and application of data mapping and building requirements
Understands data models, large datasets, business/technical requirements
Skilled in managing the process between updating and maintaining data source systems and implementing data related requirements
Desired Qualifications
Knowledge of Navy Federal Credit Union instructions, standards, and procedures
Hours:
Monday - Friday, 8:00AM - 4:30PM
Location:
820 Follin Lane, Vienna, VA 22180 | 5550 Heritage Oaks Dr. Pensacola, FL 32526 | 141 Security Dr. Winchester, VA 22602
About Us
You have goals, dreams, hobbies, and things you're passionate about—what's important to you is important to us. We're looking for people who not only want to do meaningful, challenging work, keep their skills sharp and move ahead, but who also take time for the things that matter to them—friends, family, and passions. And we're looking for team members who are passionate about our mission—making a difference in military members' and their families' lives. Together, we can make it happen. Don't take our word for it:
Military Times 2022 Best for Vets Employers
WayUp Top 100 Internship Programs
Forbes® 2022 The Best Employers for New Grads
Fortune Best Workplaces for Women
Fortune 100 Best Companies to Work For®
Computerworld® Best Places to Work in IT
Ripplematch Campus Forward Award - Excellence in Early Career Hiring
Fortune Best Place to Work for Financial and Insurance Services
Equal Employment Opportunity: Navy Federal values, celebrates, and enacts diversity in the workplace. Navy Federal takes affirmative action to employ and advance in employment qualified individuals with disabilities, disabled veterans, Armed Forces service medal veterans, recently separated veterans, and other protected veterans. EOE/AA/M/F/Veteran/Disability EOE/AA/M/F/Veteran/Disability
Disclaimers: Navy Federal reserves the right to fill this role at a higher/lower grade level based on business need. An assessment may be required to compete for this position. Job postings are subject to close early or extend out longer than the anticipated closing date at the hiring team’s discretion based on qualified applicant volume. Navy Federal Credit Union assesses market data to establish salary ranges that enable us to remain competitive. You are paid within the salary range, based on your experience, location and market position
Bank Secrecy Act: Remains cognizant of and adheres to Navy Federal policies and procedures, and regulations pertaining to the Bank Secrecy Act.
Show more
Show less","Systems Integration, Application Integration, .Net/C#, Python, Agile Frameworks (SAFE), Microsoft Databricks, Azure Data Factory, Data Distribution Patterns, APIs, EventBased Systems, Pub/Sub Models, Data Architecture, Web Services, REST APIs, Event and Pub/Sub Messaging Architecture, Mobile Technologies, Web Technologies, SQL, NoSQL, JSON, Data Mapping, Data Requirements, Data Models, Large Datasets, Data Source Management, Navy Federal Credit Union Instructions Standards and Procedures","systems integration, application integration, netc, python, agile frameworks safe, microsoft databricks, azure data factory, data distribution patterns, apis, eventbased systems, pubsub models, data architecture, web services, rest apis, event and pubsub messaging architecture, mobile technologies, web technologies, sql, nosql, json, data mapping, data requirements, data models, large datasets, data source management, navy federal credit union instructions standards and procedures","agile frameworks safe, apis, application integration, azure data factory, data architecture, data distribution patterns, data mapping, data models, data requirements, data source management, event and pubsub messaging architecture, eventbased systems, json, large datasets, microsoft databricks, mobile technologies, navy federal credit union instructions standards and procedures, netc, nosql, pubsub models, python, rest apis, sql, systems integration, web services, web technologies"
Data Analyst Part Time,Voxmediallc,"Vernon, British Columbia, Canada",https://ca.linkedin.com/jobs/view/data-analyst-part-time-at-voxmediallc-3757240786,2023-12-17,Vernon, Canada,Mid senior,Onsite,"Summary:
The Data Analyst Part Time will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : hiring@jobsai.live
Powered by Webbtree
Show more
Show less","Data Analysis, Statistical analysis, SQL, R, Python, Data Visualization, Tableau, Power BI, Data Modeling, Hypothesis testing, A/B testing, Data management, ETL","data analysis, statistical analysis, sql, r, python, data visualization, tableau, power bi, data modeling, hypothesis testing, ab testing, data management, etl","ab testing, data management, dataanalytics, datamodeling, etl, hypothesis testing, powerbi, python, r, sql, statistical analysis, tableau, visualization"
Senior Data Analyst,Study Group,"East Sussex, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-analyst-at-study-group-3763349012,2023-12-17,Brighton and Hove, United Kingdom,Mid senior,Onsite,"Contract type:
Full Time (37.5 hours per week) - Permanent
Location:
Brighton Head Office (Remote working)
Join our team as an Analytics Support Specialist, dedicated to providing crucial support to key business areas. Your role will involve delivering insightful management information to a diverse range of internal and external stakeholders. A pivotal aspect of this position is the ability to collaborate effectively with stakeholders at all organizational levels, translating their requirements into both technical and non-technical deliverables. As an integral team member, you will play a key role in advancing Study Group's transition to a more data-driven culture, all while supporting data governance initiatives.
ABOUT THE ROLE
Undertake simple and complex statistical analysis of a variety of different datasets whilst producing (and monitoring) performance measures, modelling scenarios and outcomes as necessary, for a variety of stakeholders across the organisation. This will include internal and external data and the use of quantitative and qualitative analytics techniques. Data quality checks and screening are often necessary as is the ability to provide quality assurance regarding: imported data.
Derive insights and explain/interpret trends identified, and their implications which can include presenting findings to stakeholders. This will involve preparing and leading on reports and producing suitable metrics which are appropriately benchmarked. An integral component of the role is ensuring that data is fully understood and utilised appropriately, providing further information or context as required, and training end users on new reports and dashboards.
Manage and design the reporting environment, working to identify areas to increase efficiency and automation of processes, whilst establishing and maintaining automated data processes. This will involve working with the Data Architecture Team around the data lake by identifying and revising reporting requirements. Will need to develop and maintain MI/data reports and dashboards. Reports often must be generated from multiple systems and some data integration work will be required. Also, there is a responsibility to maintain documentation relating to reports and dashboards.
Create appropriate data visualisations, utilising PowerBI, Tableau and other appropriate tools like R, all in service of providing an accessible way to see and understand trends, outliers, and patterns in data.
Manage complex data-related projects and the production of data products from their inception to conclusion, including those involving complex data modelling and forecasting. This will include supporting initiatives for data integrity and normalisation.
Support ad hoc requests for data.
Process confidential data and information according to guidelines, with a complete understanding of GDPR and the Data Protection Act and the implications for the work – advising stakeholders as appropriate.
Work collaboratively with other colleagues to share knowledge and understanding of data analysis tools and techniques to ensure that data is effectively used. This will also involve informing discussions on appropriate systems, databases, platforms, software and data tools – especially in relation to data and analytics requirements for new systems (or upgrades to existing systems).
Play a central role in encouraging better data literacy within the Study Group and help foster a data culture across the organisation. This includes advocating for better use and management of data at Study Group.
ABOUT YOU
Undergraduate degree from a good university with a 2:1 or above, or equivalent experience.
Ideally a master’s level and/or PhD qualification.
Extensive experience working in a Management Information/Analyst role using Cognos Business Analytics or PowerBI, Tableau, advanced Excel, and SQL
Proven experience of working effectively with stakeholders in specialist and technical roles, including IT infrastructure and other data and analytics specialists as well as senior executives across the organisation.
Proven experience using sound judgement and initiative to manage competing priorities and requirements, whilst ensuring the delivery of key objectives.
Ideally, experience using SQL servers, Microsoft Azure Data Lake, or Oracle databases and data warehouses.
Highly desirable if there is experience with Alteryx.
Highly desirable if there is experience with R and/or Python programming language.
ABOUT US
Study Group is a leading international education provider that is dedicated and passionate about helping students around the world, creating a brighter future and reaching their full potential. With university partnerships and a variety of face-to-face, online, and hybrid study programmes, we can provide students with the resources and guidance they need to succeed. Our global network of offices and university partnerships ensure that students receive the best educational services available. Additionally, our Insendi platform enables us to deliver innovative, accessible, and intuitive digital learning experiences with the most advanced digital tools available.
ORGANISATIONAL COMPLIANCE
Study Group is proud of its high standards in safeguardingand recognisesits importance in enabling the best possible student experience and outcomes. Forsuccessful applicants the following checks will be conducted- Criminal Background Check (country specific), x2 References, ID and Right to Work checks as well as Education Verification.
We are proud to be an equal opportunity employer and we strive to create a diverse and inclusive workplace. We are committed to providing equal opportunities for all individuals, regardless of race, gender, religion, nationality, or any other factor. We believe that the best person for the job should be selected based on skills and experience alone. Your information will be kept confidential and stored securely. Please see ourPrivacy Policy.
We adopt a rolling recruitment process meaning that we review applications as they are submitted. To avoid disappointment, please ensure that you apply ASAP.
Show more
Show less","Analytics, PowerBI, Tableau, R, Data Visualization, Data Analysis, SQL, Management Information, Business Intelligence, Data Warehousing, Data Architecture, Data Modeling, Data Quality, Data Governance, Data Integration, Project Management, Alteryx, Python, Reporting, Dashboards, Cognos Business Analytics, Advanced Excel, GDPR, Data Protection Act, SQL Server, Azure Data Lake, Oracle","analytics, powerbi, tableau, r, data visualization, data analysis, sql, management information, business intelligence, data warehousing, data architecture, data modeling, data quality, data governance, data integration, project management, alteryx, python, reporting, dashboards, cognos business analytics, advanced excel, gdpr, data protection act, sql server, azure data lake, oracle","advanced excel, alteryx, analytics, azure data lake, business intelligence, cognos business analytics, dashboard, data architecture, data governance, data integration, data protection act, data quality, dataanalytics, datamodeling, datawarehouse, gdpr, management information, oracle, powerbi, project management, python, r, reporting, sql, sql server, tableau, visualization"
Data Engineering Consultant,Nigel Frank International,"Brighton, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-engineering-consultant-at-nigel-frank-international-3740002676,2023-12-17,Brighton and Hove, United Kingdom,Mid senior,Onsite,"I am working with a Microsoft Partnered consultancy who are looking for a Data Engineering Consultant to join their growing team. You will have the opportunity to work on a variety of client projects across a number of different sectors such as retail, finance and the public sector.
In this role you will utilise your experience with the latest Azure technologies to provide guidance to clients. As a hands-on consultant you use your experience as a data engineer to ensure the timely delivery on clients data driven projects.
You will be joining a people centred business, who invest heavily in its team. You will be given the opportunity to explore other areas of technologies that may be of interest to you, including data science, machine learning and AI. You will have time designated purely to training and development, with internal development schemes as well as funded Microsoft learning, training courses and certifications.
This is an exciting time to join a hugely successful and growing business who have a number of exciting roadmap of projects planned all the way into 2024!
As part of this role, you will be responsible for some of the following areas.
Offer guidance to clients on data driven projects
Take the lead on data engineering projects focused around the Microsoft and Azure tech stack
Deliver on all aspects of the project including requirements gathering, testing, implementation and maintenance
This is a salaried role paying up to £55,000 per annum depending on experience and a company benefits package. This is a home based role with occasional, fully expensed visits to client sites as and when required. You will also need to commute to company retreats once per quarter to meet other members of the team.
To be successful in the role you will have.
Experience working with the Azure tech stack including Azure Data Factory, Synapse and Azure Data Lake.
Experience designing and implementing ETL solutions.
Coding experience with languages such as SQL and Python.
Strong knowledge of Databricks for data ingestion and transformation would be beneficial
Excellent communication skills
This is just a brief overview of the role. For the full information, simply apply to the role with your CV, and I will call you to discuss further. My client is looking to begin the interview process ASAP, so don't miss out, APPLY now! To do so please email me at a.pinkerton@nigelfrank.com or call me on 0191 3387487.
Nigel Frank International are the go-to recruiter for Power BI and Azure Data Platform roles in the UK offering more opportunities across the country than any other recruitment agency. We're the proud sponsor and supporter of SQLBits, Power Platform World Tour, the London Power BI User Group, Newcastle Power BI User Group and Newcastle Data Platform and Cloud User Group. We are the global leaders in Microsoft recruitment.
Show more
Show less","Azure, Data Factory, Synapse, Azure Data Lake, ETL, SQL, Python, Databricks, Data engineering, Machine learning, Data science, AI","azure, data factory, synapse, azure data lake, etl, sql, python, databricks, data engineering, machine learning, data science, ai","ai, azure, azure data lake, data engineering, data factory, data science, databricks, etl, machine learning, python, sql, synapse"
Senior Data Analyst,"Studio Two One Two, LLC","Little London, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-analyst-at-studio-two-one-two-llc-3767660863,2023-12-17,Brighton and Hove, United Kingdom,Mid senior,Onsite,"Our Story
From its inception in 1994, Chrissie Ruckers vision was to build a company that specialised in stylish, white, designer-quality items for the home that were not only exceptional quality, but also outstanding value for money. In addition to this devotion to simplicity, it was imperative the customer was put at the heart of everything and provided with a second-to-none shopping experience - and so The White Company was born.
Today, the company that began as a 12-page mail-order brochure has become one of the UKs fast-growing multi-channel retailers and a leading lifestyle brand with 68 stores across the UK and a highly successful online business.
Our Role
Reporting to the Analytics Manager, they will be responsible for ensuring all customer, marketing and wider business analytics are delivered accurately, on time and to brief. The Senior Marketing Analyst will provide strategic support and actionable insight to all areas of the business: they will need to work closely with channel owners and business stakeholders to advise where data and analytics can help drive strategy, improve campaign performance, and deliver more efficient marketing. The Senior Marketing Analyst is expected to be comfortable both being hands-on, executing analytics, as well as communicating with the wider business and cascading findings and recommendations.
What Youll Be Doing
Construct and execute customer data analytics to make recommendations towards measurable business change across customer behaviour and customer experience.
Develop, maintain and evolve current Power BI reporting models and dashboards to ensure all reporting directs insight driven, strategic decisions across stakeholder
Analyse complex customer behaviour trends and sentiment using modelling tools; input to customer experience and research projects and deliver customer behaviour data driven insights.
Design, implement and interpret tests (using statistical techniques) based on a wide array hypotheses around customer behaviour, then provide opportunities for further analysis.
Advise and mentor other analysts in the team on analytical best practice and effective dissemination of results to most effectively support change
Manage relationships with senior stakeholders across the business. Proactively defining and recommending new marketing & customer analytics to both improve performance & support ad-hoc project activities, ensuring the accuracy of data before circulation.
The Skills & Experience That Youll Need
Desired/Required skills or experience:
Strong academic background in Mathematics, Economics or technical discipline (Essential)
Excellent knowledge of Excel, Power BI, data mining and data analytical tools (Essential)
Highly numerate with strong analytical skills (Essential)
Direct marketing or customer analytics experience in the retail sector (Essential)
Campaign planning and data selection execution experience (Preferred)
Experience using SQL and/or Alteryx (Preferred)
Other desirable experience: Digital/Product/Category Analytics; Online/Offline Attribution; Marketing Segmentation; Propensity Modelling.
Personal Qualities
Strong organisation skills and able to work to tight deadlines
Commercial awareness and strategic thinking
Quick-thinking and perceptive, even under pressure
Excellent communication skills: ability to effectively communicate in writing and deliver presentations to senior stakeholders.
Highly numerate with a desire to consistently streamline and improve
Ability to learn new tools quickly and thoroughly
A self-starter with the ability to work with minimal supervision who excels at both working alone and collaboratively
What Well Offer You
At The White Company, we value our employees for always going the extra mile for every one of our customers; we reward this with great benefits and competitive salaries.
Discount -Up to 50% discount (dependent on contract type)
Holiday 25 days, increasing to 28 days with length of service
Holiday Buy opportunity to buy up to an additional 5 days holiday
Bonus Potential - In addition to our competitive salaries, all our permanent employees are entitled to join a discretionary bonus scheme. (dependent on contract type)
Perkplace Benefits Platform offering a variety of discounts across wellbeing and lifestyle
Wagestream Money Management app - access to Wagestream gives you power over your pay and supports financial wellbeing
Continued Development we offer structured support from our own in house Learning and Development Hub
Pension Scheme
Life Assurance
Private Medical
Additional Benefits
Fruit basket daily
Tea and coffee provided
Working from Home - option to work from home on Mondays and Fridays
Social - Christmas party/social events throughout the year
Seasonal Sample Sales
Volunteer Day - with a charity of your choice
Great Location - Close to transport links - over ground Shepherds Bush station /underground White City & Wood Lane stations. Westfield shopping centre with shops restaurants/bars/cinema and gym
Our Equality Diversity and Inclusion statement of commitment
At The White Company we are committed to creating an inclusive culture that welcomes and celebrates a diversity of backgrounds and identities.
We are working together to ensure our environment is one where people can bring their authentic selves to work, where their contribution is valued, ability enhanced, and perspective appreciated. Where difference is respected, encouraged, and celebrated. Where you can feel you belong.
We are committed to an active Equality Diversity and Inclusion Policy, which starts with our recruitment and selection process.
We'd love you to join us on our journey.
Show more
Show less","SQL, Alteryx, Data Mining, Data Analytics, Power BI, Excel, Statistics, Marketing Analytics, Customer Analytics, Business Analytics, Propensity Modeling, Campaign Planning, Data Selection, Online/Offline Attribution, Digital Analytics, Product Analytics, Category Analytics, Marketing Segmentation","sql, alteryx, data mining, data analytics, power bi, excel, statistics, marketing analytics, customer analytics, business analytics, propensity modeling, campaign planning, data selection, onlineoffline attribution, digital analytics, product analytics, category analytics, marketing segmentation","alteryx, business analytics, campaign planning, category analytics, customer analytics, data mining, data selection, dataanalytics, digital analytics, excel, marketing analytics, marketing segmentation, onlineoffline attribution, powerbi, product analytics, propensity modeling, sql, statistics"
Senior Data Analyst,"Your 25th Hour, LLC","Little London, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-analyst-at-your-25th-hour-llc-3765064570,2023-12-17,Brighton and Hove, United Kingdom,Mid senior,Onsite,"Are you an ambitious, highly analytical and data competent individual with an expert eye for detail?
Here at Media Concierge
,
data analys
is
underpins everything we do,as we strive to measure our success and adapt to meet future marketing challenges. For the right person this
Senior Data Analyst
rolewill provide the scope to work across our business teams and to develop a long-term career in the exciting world of media and marketing.
Were a fast-paced media business group that focuses on leading edge technological marketing and innovative localised direct marketing, a sector in which we lead the field. Join our data team and youll provide analytical support to a team delivering solutions to many blue chip brands, in partnership with the best players in the advertising agency world.
About The Role
As
Senior Data Analyst
youll be responsible for the collation and interrogation of data from multiple sources and for liaising with third party data suppliers, while also maintaining databases and data systems. Youll create and maintain business reporting dashboards, plus develop and support reporting processes.
Identifying, analysing, and interpreting trends or patterns in complex data sets is a vital component of this position, as is using your first-class analytical skills to see through the data and analyse it to find conclusions.
What are we looking for?
To secure this interesting role, you will have 3+ years experience in a similar role, must possess expert knowledge of Microsoft Excel, T-SQL and SQL Server Management Studio, plus be able to write both complex SQL queries and reports. Python is a desirable skill the applicant should demonstrate growing experience or actively learning along with any ETL experience is desirable.
Proactive, meticulous and an outstanding communicator, both verbal and written, you will also be able to work effectively in a team and individually. Excellent problem-solving, time-management and prioritisation skills are also vital.
About Us
We are not your average employer. Independently owned and highly profitable, over the last 35 years we have developed the careers of some of the UKs brightest talent by trusting them to succeed on their own merits.
What makes us different is the recognition that advertising is a sector where success requires a mixture of ability, knowledge, hard work, confidence, tenacity, resilience and a will to win. Our unique way is to recognise this and to reward it accordingly.
So, i
f you would like to know more about this exciting opportunity, send your CV with a covering letter telling us why you fit the bill.
Show more
Show less","Data Analysis, SQL, Microsoft Excel, TSQL, SQL Server Management Studio, Python, ETL, ProblemSolving, TimeManagement, Prioritization, Communication, Teamwork","data analysis, sql, microsoft excel, tsql, sql server management studio, python, etl, problemsolving, timemanagement, prioritization, communication, teamwork","communication, dataanalytics, etl, microsoft excel, prioritization, problemsolving, python, sql, sql server management studio, teamwork, timemanagement, tsql"
Data Centre Facility Engineer,CGG,"Haywards Heath, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-centre-facility-engineer-at-cgg-3765982914,2023-12-17,Brighton and Hove, United Kingdom,Mid senior,Onsite,"Company Description
CGG (www.cgg.com) is a global technology and HPC leader that provides data, products, services and solutions in Earth science, data science, sensing and monitoring. Our unique portfolio supports our clients in efficiently and responsibly solving complex digital, energy transition, natural resource, environmental, and infrastructure challenges for a more sustainable future.
Our expertise, specialized HPC and digital technologies support advanced and secure cloud-based workflows, expert geoscience data transformation and AI, machine learning and data science services. Today, our HPC supports over 700 users with 300 petaflops of compute power.
Job Description
We are looking for an experienced Data Center Facility Engineer to join our HPC team!
The successful candidate will work with the Data Center Facility Manager and will be responsible to support the maintenance of data center equipment and all related infrastructure systems.
Principle Accountabilities
Carry out specialist technical and non-technical work to support the ongoing maintenance of the data center.
Ensure all work is carried out in accordance with safe systems of work and permit-to-work procedures.
Conduct risk assessments for ad-hoc activities and monitor the day-to-day implementation of safe working procedures and safe systems of work.
Facilitate subcontractor visits.
Provide technical support and backup as required.
Ensure all personnel and contractors working in hazardous areas take all necessary safety measures to prevent danger, avoid injury, and prevent damage to equipment.
Use all relevant tools and equipment within safety guidelines to complete work.
Perform basic handyman duties, such as touching up paintwork, tightening hinges, installing/repairing blinds, changing locks, making minor repairs to cabinets/desks, and assembling small equipment.
Perform plumbing activities, such as clearing blockages, attending to burst pipes, replacing components, and servicing systems.
Liaise with the contract management team.
Carry out and/or adhere to any reasonable requests made by CGG Management.
Comply with CGG policies on Health & Safety, COSHH, and other statutory legislation.
Qualifications
This role will be required to be on-call
NVQ level 2 Electrical (or equivalent)
17th Edition 7671
2392 testing and inspection (or higher) is an advantage but not required
Good communication skills
Health and safety experience
CAFM systems experience
Good knowledge of general building services
Basic computer literacy (Microsoft Word, Excel, Internet, and smartphone use)
Ability to deliver excellent customer service
Self-motivation with the ability to manage tight deadlines and prioritize workload
Additional Information
Why work at CGG?
Competitive salary commensurate with experience
Highly attractive bonus scheme
Initial 22 days annual leave with future increases
Company pension with generous employer contribution
Wellbeing Unmind app – puts you in control of your mental health
A flexible benefits platform with numerous discount schemes - gym membership, restaurants, cinema tickets, and much more!
Regular social club events, spontaneous reward events throughout the year
Cycle purchase scheme
Flexible Private Medical & Dental care programmes
Bank Holiday Swap - our holiday swap program allows you to change it for another day of your choice!
Buy & Sell Holiday Programme - you can buy extra holiday, sell unused holiday or even save it up!
Relaxed dress code policy
Learning and Development
At CGG, you will have the space to grow at the pace of your passion and explore the opportunities available to those who dare and deliver.
Our culture of learning and complementary approach to supported or self-guided career development, enables the design of tailored courses to suit specific needs our employees to aid personal growth in areas related to technical, commercial and personal skills, via an extensive suite of CGG developed courses, managed through our own Learning Hub.
We Care About The Environment
We encourage and actively support a strong sense of community, through volunteering and various company initiatives, as well as a strong company commitment to protecting our environment through sustainable solutions, energy saving and waste reduction enterprises.
We see things differently. Diversity fuels our innovation, we value the unique ways in which we differ, and we are committed to equal employment opportunities for all professionals.
Show more
Show less","NVQ level 2 Electrical, 17th Edition 7671, 2392 testing and inspection, Microsoft Word, Microsoft Excel, Internet, Smartphone, CAFM systems, General building services","nvq level 2 electrical, 17th edition 7671, 2392 testing and inspection, microsoft word, microsoft excel, internet, smartphone, cafm systems, general building services","17th edition 7671, 2392 testing and inspection, cafm systems, general building services, internet, microsoft excel, microsoft word, nvq level 2 electrical, smartphone"
Senior Data Engineer - London - GBP65k + bonus,Nigel Frank International,"Crawley, England, United Kingdom",https://uk.linkedin.com/jobs/view/senior-data-engineer-london-gbp65k-%2B-bonus-at-nigel-frank-international-3723023914,2023-12-17,Brighton and Hove, United Kingdom,Mid senior,Hybrid,"Are you an experienced Data Engineer who is looking to work with the latest tech?
My client a leading transport organisation, and they are currently looking for a Senior Data Engineer to join their growing team. Within this role you will be working alongside other Data Engineers and BI professionals, you will be developing Azure Data Factory Pipelines, working with Databricks (Notebooks, pipelines, DLT, Spark, SQL), design solutions and governance and also experience with Data LakeHouse design, build and maintenance will be beneficial as well.
This is a permanent salaried position paying between £60,000 - £65,000 depending on the strength of your skill-set, working 1-2x/week in the office in London.
Minimum qualifications required:
Strong experience with Databricks
Strong experience with SQL and Python (Pyspark)
Experience with Azure DevOps, Snowflake Data Warehouse and CI/CD pipelines
Excellent communication skills
This is a fantastic opportunity to work with an established team and up-skill with exciting technologies. Do not miss this exciting and unique opportunity and reach out to have a chat.
To discuss in more detail, please send your CV s.fokas@nigelfrank.com or alternatively, call Spiros Fokas on 191 243 5445
Nigel Frank International is the leading Microsoft Business Intelligence recruitment firm in the UK, advertising more Business Intelligence jobs than any other agency. We deal with both Microsoft Partners & End Users throughout the UK and Europe and we have never had more live requirements jobs for Microsoft Business Intelligence professionals. By specialising solely in placing candidates in the market I have built relationships with key employers in the UK and have an unrivalled understanding of where the best opportunities & Business Intelligence jobs are.
Show more
Show less","Data Engineering, Azure Data Factory, Databricks, Delta Lake, Spark SQL, Data Lakehouse, SQL, Python, Pyspark, Azure DevOps, Snowflake Data Warehouse, CI/CD pipelines","data engineering, azure data factory, databricks, delta lake, spark sql, data lakehouse, sql, python, pyspark, azure devops, snowflake data warehouse, cicd pipelines","azure data factory, azure devops, cicd pipelines, data engineering, data lakehouse, databricks, delta lake, python, snowflake data warehouse, spark, spark sql, sql"
Lead Data Tester,TalentFish,"Chicago, IL",https://www.linkedin.com/jobs/view/lead-data-tester-at-talentfish-3768018681,2023-12-17,Chicago,United States,Associate,Onsite,"Job Title: Lead Data Tester
Primary Location: Remote must reside in the Chicago land area!
Position Type: Contract - 6 months
Overview
TalentFish is casting a line for a
motivated & dynamic
Lead Data Tester. This is a remote, 6 mos. contract role with the intent for long-term (extension or conversion). The Lead Data Tester will be responsible for ensuring the successful implementation of new features as well as improvement of existing functionality for data architecture projects. This includes Integration testing, Customer Data Quality testing, Data verification testing, Regression testing etc.
This is an exciting, new position that will require hands-on testing experience and a deep knowledge & understanding of all aspects of developing and executing Back-end testing. The QA Test Engineer must be an advocate for all things Testing and Quality Assurance, while driving & implementing best practices. Seeking a candidate that possess strong analytical and critical thinking abilities with strong Data testing experience & skillset.
What You'll do in a Day. (Skills Used in this Position)
Review business requirements, functional specifications, use cases, design, and architecture documents to create effective, value-add test plans.
Identify and close gaps, by assisting in QA discovery or analysis of a built or process, in order to facilitate requirements refinements.
Identify issues in data, application configurations and screen designs, report and oversee issue resolution.
Plan and execute Manual, Functional, and Regression test suite.
Maintain close interaction with business and technical peers during the project life cycle.
Ensuring proper traceability from requirements to test artifacts.
Actively participate in requirements and code reviews.
Actively participate and drive improvements in Agile Scrum delivery.
Creation and input of test data, either manually or by using tools, programs, or masked production data to execute test cases.
Interface with Product Owners, Business Analysts, Software Developers, End Users and Scrum Masters to deliver products with high quality and on schedule.
What You Bring to the Role. (Ideal Experience)
Bachelor's degree in Computer Science or related area.
5-8+ years of experience as a Data Tester. Hands-on experience on a variety of projects in Test Strategy, Test Design, Test Execution analysis & reporting.
3+ years as a Lead Data Tester with in-depth experience building out QA processes & structures.
Hands-on experience using SQL queries for results validation and data mining.
Hands-on experience in Planning and Manual test execution.
Experience in Regression Test Strategies, Regression suite maintenance and execution.
Hands-on experience with Jira, ALM, TFS/ADO or similar systems of record.
Understanding of Development and Test cycles, including respective best practices.
Hands-on experience in Agile and Waterfall methodologies.
Ability to troubleshoot issues, identifying root causes and support development team in development of resolutions.
Strong communication and Client/Business interfacing and interpersonal skills are a must.
TalentFish is an employee-owned company pioneering a new realm in talent acquisition. We are redefining IT staffing by evolving AI, video screening and our unique platform. TalentFish focuses on providing the best employee, consultant, and client experience possible.
At TalentFish we are an Equal Opportunity Employer we embrace and encourage diversity!
Show more
Show less","Data Architecture, Data Testing, Integration Testing, Customer Data Quality Testing, Data Verification Testing, Regression Testing, Backend Testing, Quality Assurance, Analytical Thinking, Test Planning, Test Execution, Test Suite Maintenance, Jira, ALM, TFS/ADO, SQL, Agile, Waterfall, Troubleshooting, Client/Business Interfacing","data architecture, data testing, integration testing, customer data quality testing, data verification testing, regression testing, backend testing, quality assurance, analytical thinking, test planning, test execution, test suite maintenance, jira, alm, tfsado, sql, agile, waterfall, troubleshooting, clientbusiness interfacing","agile, alm, analytical thinking, backend testing, clientbusiness interfacing, customer data quality testing, data architecture, data testing, data verification testing, integration testing, jira, quality assurance, regression testing, sql, test execution, test planning, test suite maintenance, tfsado, troubleshooting, waterfall"
Senior Staff Data Engineer,"SADA, An Insight company","Chicago, IL",https://www.linkedin.com/jobs/view/senior-staff-data-engineer-at-sada-an-insight-company-3743766922,2023-12-17,Chicago,United States,Mid senior,Onsite,"Join SADA as a Senior Staff Data Engineer, Corporate!
Your Mission
As a Senior Staff Data Engineer, Corporate at SADA, you will have the opportunity to work with big data and emerging Google Cloud technologies to drive corporate services. You will have an opportunity to design, develop, and maintain the best Enterprise Data Warehouse solution to fit our corporate needs. You will be interacting with all of our business units and Google Cloud subject matter experts.
From transforming business requirements, solution architecture, data modeling, architecting, ETL, metadata, and business continuity, you will have the opportunity to work collaboratively with architects and other engineers to recommend, prototype, build, and debug data infrastructures on Google Cloud Platform (GCP). You will have an opportunity to work on real-world data problems facing our customers today. Engagements vary from being purely consultative to requiring heavy hands-on work and covering a diverse array of domain areas, such as data migrations, data archival and disaster recovery, and big data analytics solutions requiring batch or streaming data pipelines, data lakes, and data warehouses.
You will be expected to run point on whole projects, end-to-end, and to mentor less experienced Data Engineers. You will be recognized as an expert within the team and will build a reputation with Google and our customers. You will demonstrate repeated delivery of project architectures and critical components that other engineers demur to you for lack of expertise. You will also participate in early-stage opportunity qualification calls, as well as guide client-facing technical discussions for established projects.
Pathway to Success
#BeOneStepAhead: At SADA, we are in the business of change. We are focused on leading-edge technology that is ever-evolving. We embrace change enthusiastically and encourage agility. This means that not only do our engineers know that change is inevitable, but they embrace this change to continuously expand their skills, preparing for future customer needs.
Your success starts by positively impacting the direction of a fast-growing practice with vision and passion. You will be measured quarterly by the breadth, magnitude, and quality of your contributions, your ability to estimate accurately, customer feedback at the close of projects, how well you collaborate with your peers and the consultative polish you bring to customer interactions.
As you continue to execute successfully, we will build a customized development plan together that takes you through the engineering or management growth tracks.
Expectations
Internal Facing - You will interact with internal customers and stakeholders regularly, sometimes daily, other times weekly/bi-weekly. Expectations will be to capture requirements and deliver solutions suitable for corporate divisions.
Onboarding/Training - The first several weeks of onboarding are dedicated to learning and will include learning materials/assignments and compliance training, and meetings with relevant individuals. Details of the timeline are shared closer to the start date.
Job Requirements
Required Credentials:
Google Professional Data Engineer Certified or able to complete within the first 45 days of employment
Required Qualifications:
Mastery in the following domain area:
Data warehouse modernization: building complete data warehouse solutions on BigQuery, including technical architectures, star/snowflake schema designs, query optimization, ETL/ELT pipelines, and reporting/analytic tools. Must have expert-level experience working with Google's batch or streaming data processing solutions (such as BigQuery, Dataform, and BI Engine)
Proficiency in the following domain areas:
Big Data: managing Hadoop clusters (all included services), troubleshooting cluster operation issues, migrating Hadoop workloads, architecting solutions on Hadoop, experience with NoSQL data stores like Cassandra and HBase, building batch/streaming ETL pipelines with frameworks such as Spark, Spark Streaming, and Apache Beam, and working with messaging systems like Pub/Sub, Kafka and RabbitMQ.
Data Catalog: Managing Data Catalogs, definitions, and data lineage.
Data Quality: Must have experience with DataForm, or other DQ solutions.
Data migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for minimizing downtime. It may involve conversion between relational and NoSQL data stores, or vice versa
Backup, restore & disaster recovery: building production-grade data backup and restore, and disaster recovery solutions. Up to petabytes in scale
4+ years of experience with Data modeling, SQL, ETL, Data Warehousing, and Data Lakes
4+ years experience in writing production-grade data solutions (relational and NoSQL)in an enterprise-class RDBMS
2+ years of experience with enterprise-class Business Intelligence tools such as Looker, PowerBI, Tableau, etc.
Mastery in writing software in Python
Experience writing software in one or more languages, such as Javascript, Java, R, or Go
Experience with systems monitoring/alerting, capacity planning, and performance tuning
Hands-on experience building frontend applications with React
Hands-on experience with CI/CD solutions (Cloud Build / Terraform)
Useful Qualifications:
Experience working with Google Cloud data products (CloudSQL, Spanner, Cloud Storage, Pub/Sub, Dataflow, Dataproc, Bigtable, BigQuery, Dataprep, Composer, etc.)
Experience with IoT architectures and building real-time data streaming pipelines
Experience operationalizing machine learning models on large datasets
Demonstrated leadership and self-direction -- a willingness to teach others and learn new techniques
Demonstrated skills in selecting the right statistical tools given a data analysis problem
Ability to balance and prioritize multiple conflicting requirements with great attention to detail
Excellent verbal/written communication & data presentation skills, including the ability to succinctly summarize key findings and effectively communicate with both business and technical teams
About SADA An Insight Company
Values:
SADA stands for inclusion, fairness, and doing the right thing. From our very beginning, we've championed a diverse workplace where we support and learn from each other, amplifying the impact we make with our customers. We're proud that our teams are composed of contributors who represent a wide array of backgrounds, experiences, abilities, and perspectives. We are an equal opportunity employer. Our five core values are the foundation of everything we do:
Make Them Rave
Be Data Driven
Think One Step Ahead
Drive Purposeful Impact
Do The Right Thing
Work with the Best
: SADA has been the largest Google Cloud partner in North America since 2016 and, for the sixth year in a row, has been named a Google Global Partner of the Year . This year, SADA was named a Google Cloud Global Partner of the year 2023. SADA has also been awarded Best Place to Work year after year by the Business Intelligence Group and Inc. Magazine, and was recognized as a Niche Player in the 2023 Gartner® Magic Quadrant™ for Public Cloud IT Transformation Services.
Benefits
: Unlimited PTO, paid parental leave, competitive and attractive compensation, performance-based bonuses, paid holidays, generous medical, dental, vision plans, life, short and long-term disability insurance, 401K/RRSP with match, as well as Google-certified training programs and a professional development stipend.
Business Performance:
SADA has been named to the INC 5000 Fastest-Growing Private Companies list for the last 10+ years in a row, garnering Honoree status. CRN has also named SADA on the Top 500 Global Solutions Providers list for the past 5 years. The overall culture continues to evolve with engineering at its core:
3200+ projects completed, 4000+ customers served, 10K+ workloads, and 30M+ users migrated to the cloud.
To request reasonable accommodation to participate in the job application or interview process, contact careers@sada.com. SADA complies with federal and state/provincial disability laws and makes reasonable accommodations for applicants and candidates with disabilities.
Show more
Show less","Data Engineering, Google Cloud Platform (GCP), BigQuery, Dataform, Spark, Spark Streaming, Apache Beam, Python, Javascript, Java, R, Go, React, Cloud Build, Terraform, SQL, ETL, Data Warehousing, Data Lakes, Data Modeling, NoSQL, Hadoop, Cassandra, HBase, Pub/Sub, Kafka, RabbitMQ, Data Catalog, Data Quality, Data Migration, Data Backup, Data Restore, Disaster Recovery, Looker, PowerBI, Tableau, IoT, Machine Learning, Leadership, SelfDirection, Statistical Tools, Data Presentation, Google Cloud data products, Google BigQuery, Google CloudSQL, Google Spanner, Google Cloud Storage, Google Pub/Sub, Google Dataflow, Google Dataproc, Google Bigtable, Google Dataprep, Google Composer","data engineering, google cloud platform gcp, bigquery, dataform, spark, spark streaming, apache beam, python, javascript, java, r, go, react, cloud build, terraform, sql, etl, data warehousing, data lakes, data modeling, nosql, hadoop, cassandra, hbase, pubsub, kafka, rabbitmq, data catalog, data quality, data migration, data backup, data restore, disaster recovery, looker, powerbi, tableau, iot, machine learning, leadership, selfdirection, statistical tools, data presentation, google cloud data products, google bigquery, google cloudsql, google spanner, google cloud storage, google pubsub, google dataflow, google dataproc, google bigtable, google dataprep, google composer","apache beam, bigquery, cassandra, cloud build, data backup, data catalog, data engineering, data lakes, data migration, data presentation, data quality, data restore, dataform, datamodeling, datawarehouse, disaster recovery, etl, go, google bigquery, google bigtable, google cloud data products, google cloud platform gcp, google cloud storage, google cloudsql, google composer, google dataflow, google dataprep, google dataproc, google pubsub, google spanner, hadoop, hbase, iot, java, javascript, kafka, leadership, looker, machine learning, nosql, powerbi, pubsub, python, r, rabbitmq, react, selfdirection, spark, spark streaming, sql, statistical tools, tableau, terraform"
Marketing Analyst Lead - GCS - Applied Data & Insight,Cabinet Office,"Bristol-London, Ontario, Canada",https://ca.linkedin.com/jobs/view/marketing-analyst-lead-gcs-applied-data-insight-at-cabinet-office-3786564146,2023-12-17,Barrie, Canada,Mid senior,Hybrid,"Location
Glasgow, London, York
About The Job
Job summary
Our mission is to deliver exceptional public service communication that makes a difference.
People work in government communications because what we do matters. We explain the objectives and policies of the government of the day. We provide clarity and reassurance to the public at times of crisis. We support businesses to grow. We protect our environment. We help those in need to access support.
It's an exciting time to join the Government Communication Service (GCS). The launch of the GCS Strategy 2022-2025 sets our vision to harness the rapid technological changes in communications for the public good, whilst developing our talent with unparalleled opportunities for learning and development. All whilst carrying out interesting, meaningful and purposeful work.
Key to our work is collaborating across boundaries and taking a genuinely diverse and inclusive approach; communicating with and being representative of our audiences in all four corners of the UK.
The Team
The Applied Data & Insight (ADI) team is a group of multi-disciplinary experts who work closely together to provide bespoke analysis and innovative solutions based on a deep understanding of data, technology, insights and behaviours to improve and evaluate government campaigns and communications. We work highly collaboratively with other teams within GCS and build proactive partnerships with other organisations to deliver impactful and innovative products and services at pace.
The Applied, Data and Insight team consists of specialists from various disciplines working together to innovate and improve the efficiency and effectiveness of government communications. Our expertise ranges from behavioural science, audience insight, evaluation, data, analytics, standards, and innovation.
Job Description
Are you a collaborative and driven professional looking for a unique opportunity to transform how Government communicates? Do you have experience of applying marketing analytics that make organisations more efficient and effective?
We are looking for a skilled and energetic candidate to work on the most visible and important communications campaigns in the country. Recent examples have included Help for Households and GREAT.
The successful candidate could be a subject-matter expert in either marketing analytics, digital audience insights and advertising technologies. They will have a keen interest in using their expertise to influence the decisions of Ministers, Directors and senior officials. They’ll advise on how data analytics and science can be applied to marketing and paid-for media to provide data-driven and actionable insights to campaign teams. Whatever your background, we are looking for someone who likes getting things done, and can collaborate with others in driving change.
As the Marketing Analyst Lead for GCS in the Cabinet Office you will play a vital role in communicating the priorities of the Prime Minister and Cabinet, supported by an energetic and fast paced team. You will have the opportunity to be the subject-matter expert in marketing analytics in terms of government communications, build strong relationships with communications teams across different Government organisations, and provide coordination from the centre of the Government Communications profession.
Your work will be instrumental in realising GCS’s data objectives and bringing innovative ideas from communicators across Government to life. You will develop and champion the highest data ethics and privacy standards. You will lead on projects regarding application of advanced analytical techniques like marketing mix modelling (econometrics), multi-touch attribution and test & learn capabilities, while keeping up-to-date on the ethical use of artificial intelligence to support campaign planning, execution and evaluation.
You will work across a range of exciting areas like cross-government benchmarking, KPI target setting, personalisation, audience insights, artificial intelligence and the forthcoming GCS Innovation & Data Strategy. Your expertise will also be required to drive forward exciting innovation projects through the Project Spark initiative. You will also play a central role in building a data-driven culture, communicating with members and working collaboratively across the organisation to promote data-first behaviours, ensuring communicators have access to the data training they need, and creating a sense of community around data.
As the Marketing Analyst Lead, you will need excellent product management skills, working collaboratively with colleagues to deliver effective and efficient campaigns, and excellent communication and networking skills. We are looking for an enthusiastic and organised candidate who will play an active role in the ADI team, and has the skills, enthusiasm and personal resilience to thrive in a fast-paced environment.
Person specification
Responsibilities & Duties
Essential Criteria
Act as a subject-matter expert in marketing analytics, digital audience insights and advertising technologies.
Advise data analysts on the creation of marketing analytics products that help campaign managers achieve their goals. Products will include dashboards using data from Google Analytics.
Proactively share knowledge with the rest of the team to increase overall capability, and take responsibility for maintaining knowledge in marketing analytics.
Lead on the implementation of data commitments published in the GCS Innovation & Data Strategy such as the application of artificial intelligence to evaluation and personalisation.
Provide support to campaign teams across government on KPI target setting, media planning and buying, reporting requirements, as well as application of advanced analytical techniques like Marketing Mix Modelling, attribution and experimentation.
Engage with providers of advertising and marketing technology to enable data integrations, innovation, horizon scanning and tech trends for government communications.
Partner with the HMG media buying agency to drive transformational change across GCS in the areas of benchmarking, experimentation and data platforms.
Engage the wider GCS and analytics community through presentations, reports, papers, and blogs to help shape the cross-government media & marketing data knowledge sharing community.
Support the Deputy Director of Insights and Data through forging new vendor relationships and engaging industry leaders.
Experience/Ability
Experience delivering robust marketing analysis on promotional or public information campaigns.
Experience in audience insights, data and analytics, with good working knowledge of digital marketing across channels and the wider digital ecosystem.
Solid understanding of paid advertising platforms, segmentation strategies, persona analysis, market and competitive analysis, and marketing and funnel concepts.
Experience using web analytics to analyse user journeys.
Strong interest in current and future trends in the media and marketing data landscape.
Good understanding of data privacy and ethical considerations for media and marketing data.
Strong written and verbal communication skills with enthusiasm for data story-telling for diverse audiences.
Enthusiasm and a proactive approach to work, maintaining active communications with colleagues and stakeholders.
Good stakeholder management skills, with the ability to work with and influence senior stakeholders across a range of organisations.
Keen team player with experience of motivating colleagues and successfully developing/delivering shared goals.
Robust organisational and time-management skills, with experience working in a fast-paced environment handling multiple priorities.
It’s desirable but not required that you have:
Expertise in the digital marketing landscape and current industry trends, including working with adtech and brand safety techniques.
Expertise in leading the development of pilots, analysis strategies, creating and setting KPIs and developing tests with strong experimental designs.
Experience from a media agency or large organisation running many concurrent campaigns.
Knowledge of Customer Relationship Management (CRM) systems and Customer Data Platforms (CDP).
Bachelor’s degree in marketing, business, or a numerate subject such as mathematics, physics, statistics, etc.
Understanding of, and experience using, data modelling tools and techniques, and the ability to apply this knowledge when developing targeted solutions to deliver business benefit. (e.g. SQL).
Experience developing interactive dashboards using data visualisation tools such as Tableau (desired), as well as application of statistical techniques using R/Python (e.g. multi-linear regression, key driver analysis).
We welcome a broad range of applicants and encourage you to apply even if you do not fully meet the listed qualifications. Strong candidates come from many different backgrounds and will not necessarily check every single box. Studies show that talented people, especially those from groups underrepresented in their field, are more likely to doubt themselves and feel like an ""imposter"". But unique perspectives enrich teams, so we urge you to have confidence in your potential contributions. If aspects of this role resonate with you, please apply rather than prematurely excluding yourself. We look forward to your application highlighting your skills and experiences.
Behaviours
We'll assess you against these behaviours during the selection process:
Working Together
Communicating and Influencing
Leadership
Benefits
Alongside your salary of £53,400, Cabinet Office contributes £14,418 towards you being a member of the Civil Service Defined Benefit Pension scheme. Find out what benefits a Civil Service Pension provides.
Learning and development tailored to your role.
An environment with flexible working options.
A culture encouraging inclusion and diversity.
A Civil Service Pension which provides an attractive pension, benefits for dependants and average employer contributions of 27%.
A minimum of 25 days of paid annual leave, increasing by one day per year up to a maximum of 30.
Things you need to know
Selection process details
This vacancy is using Success Profiles (opens in a new window) , and will assess your Behaviours and Experience.
Application process
Stage 1: Application
Your application will consist of three parts:
A Personal Details application form.
Your employment history detailing your responsibilities,skills, and accomplishments, plus your qualifications and relevant training and qualifications. Please copy this information into the box field provided.
Personal statement, up to 1000 words. When giving details in your employment history and personal statement, you should demonstrate your experience in line with essential criteria and Experience/Ability sections above.
Should a large number of applications be received, an initial sift may be undertaken using Personal Statement. Candidates who pass the initial sift may be progressed to a full sift, or progressed straight to assessment/interview.
Selection process
Stage 2: Interview
If you’re successful at the sift stage you will be invited to a video interview via Google Meet. There, you will be assessed on your experience, the essential criteria, the Experience/Ability for the role, and the following Civil Service behaviours:
Working together
Communicating & Influencing
Leadership
Further details will be provided to candidates invited to interview.
Expected timeline (subject to change)
Expected sift date – tbc
Expected interview date/s – tbc
Interview location - remote via google meet
Reasonable adjustments
If a person with disabilities is put at a substantial disadvantage compared to a non-disabled person, we have a duty to make reasonable changes to our processes.
If you need a change to be made so that you can make your application, you should:
Contact Government Recruitment Service via cabinetofficerecruitment.grs @cabinetoffice.gov.uk as soon as possible before the closing date to discuss your needs.
Complete the ‘Assistance required’ section in the ‘Additional requirements’ page of your application form to tell us what changes or help you might need further on in the recruitment process. For instance, you may need wheelchair access at interview, or if you’re deaf, a Language Service Professional.
Further information
If you are experiencing accessibility problems with any attachments on this advert, please contact the email address in the 'contact point for applicants' section.
Please note that this role requires SC clearance, which would normally need 5 years’] UK residency in the past 5 years. This is not an absolute requirement, but supplementary checks may be needed where individuals have not lived in the UK for that period. This may mean your security clearance (and therefore your appointment) will take longer or, in some cases, not be possible.
Please note terms and conditions are attached. Please take time to read the document to determine how these may affect you.
Any move to Cabinet Office from another employer will mean you can no longer access childcare vouchers. This includes moves between government departments. You may however be eligible for other government schemes, including Tax Free Childcare. Determine your eligibility at https://www.childcarechoices.gov.uk
A reserve list will be held for a period of 12 months, from which further appointments can be made.
If successful and transferring from another Government Department a criminal record check may be carried out.
However, we recognise in exceptional circumstances some candidates will want to send their completed forms direct. If you will be doing this, please advise Government Recruitment Service of your intention by emailing Pre-EmploymentChecks.grs@cabinetoffice.gov.uk stating the job reference number in the subject heading.
For further information on the Disclosure Scotland confidential checking service telephone: the Disclosure Scotland Helpline on 0870 609 6006 and ask to speak to the operations manager in confidence, or email Info@disclosurescotland.co.uk
New entrants are expected to join on the minimum of the pay band.
Applicants who are successful at interview will be, as part of pre-employment screening, subject to a check on the Internal Fraud Database (IFD). This check will provide information about employees who have been dismissed for fraud or dishonesty offences. This check also applies to employees who resign or otherwise leave before being dismissed for fraud or dishonesty had their employment continued. Any applicant’s details held on the IFD will be refused employment.
A candidate is not eligible to apply for a role within the Civil Service if the application is made within a 5 year period following a dismissal for carrying out internal fraud against government.
This role is full time only. Applicants who wish to work an alternative pattern are welcome to apply however your preferred working pattern may not be available and you should discuss this with the vacancy holder before applying.
Feedback will only be provided if you attend an interview or assessment.
Security
Successful candidates must undergo a criminal record check.
Successful candidates must meet the security requirements before they can be appointed. The level of security needed is security check (opens in a new window) . See our vetting charter (opens in a new window) .
People working with government assets must complete baseline personnel security standard (opens in new window) checks.
Nationality Requirements
This job is broadly open to the following groups:
UK nationals
nationals of the Republic of Ireland
nationals of Commonwealth countries who have the right to work in the UK
nationals of the EU, Switzerland, Norway, Iceland or Liechtenstein and family members of those nationalities with settled or pre-settled status under the European Union Settlement Scheme (EUSS) (opens in a new window)
nationals of the EU, Switzerland, Norway, Iceland or Liechtenstein and family members of those nationalities who have made a valid application for settled or pre-settled status under the European Union Settlement Scheme (EUSS)
individuals with limited leave to remain or indefinite leave to remain who were eligible to apply for EUSS on or before 31 December 2020
Turkish nationals, and certain family members of Turkish nationals, who have accrued the right to work in the Civil Service
Further information on nationality requirements (opens in a new window)
Working for the Civil Service
The Civil Service Code (opens in a new window) sets out the standards of behaviour expected of civil servants.
We recruit by merit on the basis of fair and open competition, as outlined in the Civil Service Commission's recruitment principles (opens in a new window) .
The Civil Service embraces diversity and promotes equal opportunities. As such, we run a Disability Confident Scheme (DCS) for candidates with disabilities who meet the minimum selection criteria.
The Civil Service also offers a Redeployment Interview Scheme to civil servants who are at risk of redundancy, and who meet the minimum requirements for the advertised vacancy.
Apply and further information
This vacancy is part of the Great Place to Work for Veterans (opens in a new window) initiative.
The Civil Service welcomes applications from people who have recently left prison or have an unspent conviction. Read more about prison leaver recruitment (opens in new window) .
Once this job has closed, the job advert will no longer be available. You may want to save a copy for your records.
Contact point for applicants
Job contact :
Name : Clement Yeung
Email : clement.yeung@cabinetoffice.gov.uk
Recruitment team
Email : cabinetofficerecruitment.grs@cabinetoffice.gov.uk
Further information
Appointment to the Civil Service is governed by the Civil Service Commission’s Recruitment Principles.
If you feel that your application has not been treated in accordance with the Recruitment Principles, and wish to make a complaint, then in the first instance you should contact Government Recruitment Service at: cabinetofficerecruitment.grs@cabinetoffice.gov.uk.
If you are not satisfied with the response that you receive, then you can contact the Civil Service Commission at: info@csc.gov.uk.
For further information on the Recruitment Principles, and bringing a complaint to the Civil Service Commission, please visit their website at: https://civilservicecommission.independent.gov.uk.
Show more
Show less","Marketing analytics, Digital audience insights, Advertising technologies, Dashboard creation, Google Analytics, Data ethics, Privacy standards, AI application, Test and learn capabilities, KPI target setting, Media planning and buying, Reporting requirements, Statistical techniques, Data modelling, Tableau, R/Python, Data Visualization, Stakeholder management, Leadership, Communication, Networking, Product management, Crossgovernment benchmarking, Personalization, Attribution, Experimentation, SQL, Regression analysis, Key driver analysis, Market mix modelling, Project management, Ethics","marketing analytics, digital audience insights, advertising technologies, dashboard creation, google analytics, data ethics, privacy standards, ai application, test and learn capabilities, kpi target setting, media planning and buying, reporting requirements, statistical techniques, data modelling, tableau, rpython, data visualization, stakeholder management, leadership, communication, networking, product management, crossgovernment benchmarking, personalization, attribution, experimentation, sql, regression analysis, key driver analysis, market mix modelling, project management, ethics","advertising technologies, ai application, attribution, communication, crossgovernment benchmarking, dashboard creation, data ethics, data modelling, digital audience insights, ethics, experimentation, google analytics, key driver analysis, kpi target setting, leadership, market mix modelling, marketing analytics, media planning and buying, networking, personalization, privacy standards, product management, project management, regression analysis, reporting requirements, rpython, sql, stakeholder management, statistical techniques, tableau, test and learn capabilities, visualization"
Data Scientist,Park National Bank,"Newark, OH",https://www.linkedin.com/jobs/view/data-scientist-at-park-national-bank-3776603722,2023-12-17,Hopewell,United States,Associate,Hybrid,"This position oversees the collection, storage, and interpretation of the Bank's data, including examining data points to create organized categories. Responsible for developing and deploying statistical and machine learning models that deepens customer relationship and drives bank’s growth.
RESPONSIBILITIES #Ll-Hybrid
**Flexible Work Arrangement: 3 days a week on-site in Newark, Ohio & 2 days remote**
- Collect data by analyzing business results or establishing and monitoring new studies
- Transfers data into a format more appropriate for analysis
- Build tools to automate data collection
- Examine large data sets for usable information
- Apply statistical modeling, machine learning, and visualization techniques to internal and external data to make accurate and actionable analyses in a timely fashion.
- Maintain the framework for managing data and authoritative geographical records and support analytical capabilities and decision making through data relationships, patterns and trends concepts.
- Support customer segmentation, customer attrition modeling, Natural Language Processing, Product Recommendation and Branch Optimization.
- Create reports and presentations for business use at varying management levels.
- Develop, plan, and manage multiple priorities in response to business needs, and set appropriate and attainable team goals.
- Maintain awareness of and adherence to Bank’s compliance requirements and risk management concepts, expectations, policies and procedures and apply them to daily tasks
- Deliver a consistent, high level of service within our Serving More standards
- Other duties as assigned
EDUCATION - CERTIFICATIONS - WORK EXPERIENCE
Required to be considered
Bachelor's degree in data science, economics, or related field (applied quantitative or computationally-orientated)
Deep knowledge of statistical and ML models
Data Analysis and ML: Python or other scripting language
Advanced experience with Microsoft Word, Excel, and PowerPoint
Preferred, but not a deal breaker
Master's degree in data science, statistics, economics, applied mathematics or related field
Financial services or related regulatory industry experience
Experience in geodatabase management, GIS Portal management and web-map creation
Basic experience with ESRI ArcMap or ArcPro or ModelBuilder in ArcGIS
Tableau
Show more
Show less","Data storage, Data interpretation, Machine learning modeling, Statistical modeling, Data analysis, Visualization techniques, Data management, Framework management, Geographical records, Data relationships, Data patterns, Data trends, Customer segmentation, Customer attrition modeling, Natural Language Processing, Product Recommendation, Branch Optimization, Report creation, Presentation creation, Business analysis, Priority management, Team goal setting, Compliance management, Risk management, Python, Scripting language, Microsoft Word, Microsoft Excel, Microsoft PowerPoint, Tableau, Geodatabase management, GIS Portal management, Webmap creation, ESRI ArcMap, ArcPro, ModelBuilder, ArcGIS","data storage, data interpretation, machine learning modeling, statistical modeling, data analysis, visualization techniques, data management, framework management, geographical records, data relationships, data patterns, data trends, customer segmentation, customer attrition modeling, natural language processing, product recommendation, branch optimization, report creation, presentation creation, business analysis, priority management, team goal setting, compliance management, risk management, python, scripting language, microsoft word, microsoft excel, microsoft powerpoint, tableau, geodatabase management, gis portal management, webmap creation, esri arcmap, arcpro, modelbuilder, arcgis","arcgis, arcpro, branch optimization, business analysis, compliance management, customer attrition modeling, customer segmentation, data interpretation, data management, data patterns, data relationships, data storage, data trends, dataanalytics, esri arcmap, framework management, geodatabase management, geographical records, gis portal management, machine learning modeling, microsoft excel, microsoft powerpoint, microsoft word, modelbuilder, natural language processing, presentation creation, priority management, product recommendation, python, report creation, risk management, scripting language, statistical modeling, tableau, team goal setting, visualization, webmap creation"
Customer Service Representative/Data Analyst/Data Entry Clerk Part Time,Drmartens,"Quesnel, British Columbia, Canada",https://ca.linkedin.com/jobs/view/customer-service-representative-data-analyst-data-entry-clerk-part-time-at-drmartens-3756472400,2023-12-17,Quesnel, Canada,Mid senior,Hybrid,"Summary:
The Data Analyst will be responsible for analyzing and interpreting large datasets to provide valuable insights and recommendations to the business. They will work closely with cross-functional teams to gather and analyze data, develop reports, and provide customized solutions to help the organization gain a competitive edge.
Responsibilities:
Analyze large data sets using advanced statistical techniques and tools to uncover trends, opportunities, and insights.
Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes.
Collaborate with stakeholders to identify business questions and translate them into data and analysis requirements.
Develop models and algorithms to help optimize business processes and drive efficiencies.
Design and execute A/B tests and experiments to identify opportunities for optimization.
Identify data quality issues and help to develop solutions to improve data integrity, accuracy, and completeness.
Manage data collection, cleansing, and manipulation processes to ensure data is readily accessible and easy to work with.
Prepare and present data-driven reports and insights to stakeholders, highlighting key findings and recommendations.
Qualifications:
1+ years of relevant experience in data analysis, preferably in the Internet and New Media industry.
Proven experience in analyzing large and complex datasets using SQL, R, Python, or related tools.
Strong analytical, critical thinking, and problem-solving skills.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Experience with data visualization tools such as Tableau, Power BI, or related tools.
Knowledge of statistical modeling, hypothesis testing, and A/B testing methodologies.
Familiarity with data management and ETL processes.
If you are interested in this position, please send your resume, contact information and salary requirements to : hiring@jobsai.live
Powered by Webbtree
Show more
Show less","Data Analysis, Statistical Analysis, A/B Testing, Data Visualization, Reporting, SQL, R, Python, Tableau, Power BI, ETL, Data Wrangling, Data Quality, Data Management, Hypothesis Testing","data analysis, statistical analysis, ab testing, data visualization, reporting, sql, r, python, tableau, power bi, etl, data wrangling, data quality, data management, hypothesis testing","ab testing, data management, data quality, data wrangling, dataanalytics, etl, hypothesis testing, powerbi, python, r, reporting, sql, statistical analysis, tableau, visualization"
Junior Data Engineer,Citylitics Inc.,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/junior-data-engineer-at-citylitics-inc-3782584375,2023-12-17,Delta, Canada,Associate,Onsite,"About Citylitics
Citylitics delivers predictive intelligence on local utility & public infrastructure markets
What is Infrastructure? It is the roadways you rely on to safely get to Grandma's house, it's the potable water that comes out of your kitchen tap that you wash your family's food with and it's the energy that heats our homes and powers our digital lifestyles.
Every year, trillions of dollars are spent on all areas of infrastructure to maintain our quality life and move our economy forward. However, our infrastructure is no longer equipped to meet the needs of the future. We hear about infrastructure failures, whether bridge collapses, power blackouts, or water main breaks, every day in the news. Climate change and extreme weather events are disrupting the basic infrastructure we took for granted for years.
Citylitics is solving the hardest data problems in infrastructure while building the market & opportunity intelligence platform that enables a faster, more transparent, and more efficient infrastructure marketplace. We turn millions of unstructured documents into high value intelligence feeds and datasets that are available on an intuitive user experience. Our goal is to enable solution providers to connect with cities with relevant infrastructure needs in a faster and more digital way than historic market channels. As more companies adopt our platform, cities & utilities will be able to access solutions that deliver on the promise of moving towards a more resilient, sustainable, and equitable infrastructure future.
What are we looking for?
We are looking for a Junior Data Engineer with an ownership mindset who prides themselves on not only writing efficient code, but also in creating intuitive data flows with clear and easy to read documentation. As a Junior Data Engineer, you will play a crucial role in developing and maintaining our data pipelines, working primarily with Django and SQL. You will collaborate closely with cross-functional teams to ensure efficient data extraction, transformation, and loading (ETL) processes, enabling data accessibility and reliability for our organization.
What will you accomplish?
Design, develop, and maintain data pipelines using Django and SQL to support efficient ETL processes
Collaborate with stakeholders to understand data requirements and translate them into technical solutions
Ensure data integrity, quality, and security throughout the ETL process
Assist in the optimization and performance tuning of database queries and processes
Support the integration of Airflow/Cloud Composer into existing data workflows
Contribute to the documentation of data pipelines, processes, and best practices
Stay updated with industry trends and best practices in data engineering and tools
Other duties as assigned
Technologies we use:
Backend: Python, Django, Cloud SQL and Airflow/Cloud Composer as the main language, web framework, database and orchestration tool respectively
Cloud Infrastructure: Google Cloud Platform
Other Tools: Javascript, React as the main language and framework for our frontend
Requirements
At least 1 year experience with Python and Django
At least 1 year experience with database design and development
Experience with Airflow/Cloud Composer is an asset
Experience with Google Cloud Platform and Docker is an asset
Benefits
This is a rare opportunity to influence positive change within one of the biggest societal challenges of our generation (infrastructure - from water to transport to energy and more)
Be a part of a scale up company with no corporate bureaucracy here. You will accomplish more here in a few months than what you would in a few years at a large, entrenched technology company
Comprehensive health and dental benefits with an emphasis on mental health
Annual Learning and Development Budget and Plans
Flexible hours and work-from-wherever you are with the option to work in our downtown office if interested!
Competitive salary and equity incentives to give you a stake in our future
We work hard. We play hard. From virtual trivia to exploring the food scene around our office, we like to get together!
Show more
Show less","Data Engineering, Django, SQL, Airflow/Cloud Composer, Javascript, React, Python, Cloud SQL, Google Cloud Platform, Docker, ETL, Data Integrity, Data Quality, Data Security, Data Pipelines, Cloud Infrastructure, Database Design, Database Development","data engineering, django, sql, airflowcloud composer, javascript, react, python, cloud sql, google cloud platform, docker, etl, data integrity, data quality, data security, data pipelines, cloud infrastructure, database design, database development","airflowcloud composer, cloud infrastructure, cloud sql, data engineering, data integrity, data quality, data security, database design, database development, datapipeline, django, docker, etl, google cloud platform, javascript, python, react, sql"
Data Engineer (Azure),Tiger Analytics,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/data-engineer-azure-at-tiger-analytics-3590303150,2023-12-17,Delta, Canada,Associate,Onsite,"Tiger Analytics is a fast-growing advanced analytics consulting firm. Our consultants bring deep expertise in Data Science, Machine Learning, and AI. We are the trusted analytics partner for multiple Fortune 500 companies, enabling them to generate business value from data. Our business value and leadership has been recognized by various market research firms, including Forrester and Gartner. We are looking for top-notch talent as we continue to build the best global analytics consulting team in the world.
We are looking for a Data Engineer to join our growing team of analytics experts. The right candidate will have strong analytical skills and the ability to combine data from different sources and will strive for efficiency by aligning data systems with business goals.
Requirements
Bachelor’s degree in Computer Science or similar field
4+ years of experience in IT industry
Expertise in Python and Pyspark
Experience building data pipelines using Azure stack
2+ years of experience using Apache spark
Good working experience on Delta Lake and ETL processing
Proficiency in SQL queries
Prior experience of working in a Unix environment
Experience in harmonizing raw data into a consumer-friendly format using Azure Databricks
Experience extracting/querying/joining large data sets at scale
Experience building data ingestion pipelines using Azure Data Factory to ingest structured and unstructured data
Experience in data wrangling, advanced analytic modeling is preferred
Exposure to Java is a plus
Strong communication and organizational skills
Benefits
This position offers an excellent opportunity for significant career development in a fast-growing and challenging entrepreneurial environment with a high degree of individual responsibility.
Show more
Show less","Data Science, Machine Learning, AI, Data Engineering, Analytical skills, Python, Pyspark, Azure stack, Apache spark, Delta Lake, ETL processing, SQL queries, Unix environment, Azure Databricks, Azure Data Factory, Data wrangling, Advanced analytic modeling, Java, Communication skills, Organizational skills","data science, machine learning, ai, data engineering, analytical skills, python, pyspark, azure stack, apache spark, delta lake, etl processing, sql queries, unix environment, azure databricks, azure data factory, data wrangling, advanced analytic modeling, java, communication skills, organizational skills","advanced analytic modeling, ai, analytical skills, apache spark, azure data factory, azure databricks, azure stack, communication skills, data engineering, data science, data wrangling, delta lake, etl processing, java, machine learning, organizational skills, python, spark, sql queries, unix environment"
Intermediate Data Engineer,The Driving Force Group of Companies,"Edmonton, Alberta, Canada",https://ca.linkedin.com/jobs/view/intermediate-data-engineer-at-the-driving-force-group-of-companies-3762678495,2023-12-17,Delta, Canada,Associate,Onsite,"This position is located at 17631 103 Ave NW, Edmonton AB, T5S 1N8.
The Driving Force Group of Companies is currently hiring an
Intermediate Data Engineer
to join our inclusive, energetic, and dynamic people and culture team who exhibits our company values of integrity, respect, commitment, and passion.
We at The Driving Force Group of Companies thrive on developing high-performance data pipelines and modern cloud-based applications that positively impact our organization. This position is responsible for building reliable, high-performance data pipelines and data warehouses for both our internal and external customers. Our vision is to solve the world’s transportation challenges by one person, one project, and one community at a time.
Why work with us?
You matter, we support you.
We pay competitive wages and benefits, including extended health with a spending account, and dental.
We take care of our people.
You get 50% off rentals at any DRIVING FORCE location. We offer you our company RRSP matching program, to look after your future.
Work/Life balance is important.
You get 3 weeks accrued vacation, a flexible working environment, and wellness time to focus on yourself or your loved ones.
We want you to grow.
Our fun and inclusive company culture enables our employees to grow their careers with us. You get hands-on training and a path forward in the company if you want to develop your career.
Responsibilities
Assist in developing a Data Warehouse and Data Marts on AWS utilizing tools such as AWS Redshift, AWS Glue, AWS Step Function, AWS Lambda, DBT, among others.
Play an active role in data transformation and modeling using DBT for our Data Warehouse.
Collaborate closely with Business Intelligence Analysts, delivering data marts in line with business needs.
Design and sustain multiple data integration processes, sourcing from diverse formats and origins.
Aid in the maintenance and augmentation of our CI/CD pipelines, predominantly AWS CodePipeline.
Engage with code and project management tools, notably Jira, Bitbucket, and AWS CodeCommit.
Implement a consistent AWS architecture using Infrastructure as Code, preferably AWS Serverless Application Model or AWS CDK.
Comfortable owning multiple projects and priorities in a highly dynamic environment.
Tech Stack
AWS Infrastructure: AWS Serverless Application Model (SAM) or AWS CDK (Python)
Data Storage: AWS Redshift, AWS Aurora Serverless (PostgreSQL), Data Lakehouse on S3, and Athena.
Data Extraction: ELT processes via AWS Step Functions integrated with AWS ECS Fargate containers and Lambda functions (Python).
API Development: AWS API Gateway (REST or HTTP) and AWS Appsync GraphQL, utilizing DynamoDB as backend.
Collaboration Tools: Slack, Jira, Confluence, Bitbucket.
Sample Projects that you are going to be involved
Enhance our existing OLAP Data Warehouse schemas and optimize data models for performance and reusability.
Experience in data modelling is necessary.
Support the development of our web application by developing API endpoints using API Gateway or Appsync.
Qualifications
3+ years of experience working as a Data Engineer.
3+ years of hands-on experience with a programming language (Python preferably).
Good understanding of modern data platforms including data lake and data warehouse (serverless).
Extensive understanding of relational databases and dimensional modeling.
Don’t worry if you don’t have everything listed above. Two of our four core values are Passion and Commitment. We believe in growth! If you have some of these qualities and are excited about this opportunity, then we want to hear from you!
Please note all offers of employment at The Driving Force Group of Companies are conditional upon the successful completion of a pre-hire background check. This may include anything up to and including a criminal history check, reference checks and/or credit check.
The Driving Force Group of Companies is an equal opportunity employer that embraces diversity in the workplace and encourages applications from qualified individuals, visible minorities, Aboriginal peoples, and persons with disabilities. We welcome applications from all interested individuals; however, only those candidates selected for an interview will be contacted.
Show more
Show less","AWS, AWS Redshift, AWS Glue, AWS Step Function, AWS Lambda, DBT, AWS CodePipeline, Jira, AWS CodeCommit, AWS Serverless Application Model (SAM), AWS CDK (Python), Data Lakehouse on S3, Athena, Python, PostgreSQL, DynamoDB, Slack, Confluence, Bitbucket, OLAP, API Gateway, Appsync GraphQL","aws, aws redshift, aws glue, aws step function, aws lambda, dbt, aws codepipeline, jira, aws codecommit, aws serverless application model sam, aws cdk python, data lakehouse on s3, athena, python, postgresql, dynamodb, slack, confluence, bitbucket, olap, api gateway, appsync graphql","api gateway, appsync graphql, athena, aws, aws cdk python, aws codecommit, aws codepipeline, aws glue, aws lambda, aws redshift, aws serverless application model sam, aws step function, bitbucket, confluence, data lakehouse on s3, dbt, dynamodb, jira, olap, postgresql, python, slack"
Senior AWS Data Engineer - 6-Month Contract,Hour Consulting,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/senior-aws-data-engineer-6-month-contract-at-hour-consulting-3770141485,2023-12-17,Delta, Canada,Associate,Onsite,"Our client, a rapidly growing Canadian Technology focused Consulting Agency renowned for its tech-forward approach, seeks an exceptionally skilled
Senior Data Engineer.
This is a six-month contract position with a strong potential for extension.
Are you passionate about building robust and scalable data engineering solutions in the cloud? Join our client's dynamic Data Squad as a Senior AWS Data Engineer to contribute to developing and maintaining their centralized Data Hub/Warehouse. In this role, you will play a critical part in data ingestion, system integrations, reporting, and analytics by designing and implementing data pipelines and data models.
Candidates must demonstrate proficiency in using programming languages like Python and SQL
Deep familiarity with AWS technologies and a strong data engineering background is essential.
You should have in-depth knowledge of AWS offerings, such as Amazon S3, AWS Glue, AWS Redshift, AWS Lambda, and AWS Step Function.
Responsibilities:
Maintain and enhance the data warehouse and analytics environment for stability and scalability.
Design, implement, and deploy reliable data engineering solutions and pipelines, integrating new data sources.
Build reports and data visualizations to enable data-driven decision-making.
Develop scalable and automated solutions to enhance data management efficiency.
Implement top-notch security measures to safeguard sensitive information.
Requirements:
Minimum 4 years of hands-on experience with AWS cloud services in data engineering.
In-depth knowledge of AWS offerings related to data storage, processing, and analytics.
Expertise in building scalable and secure data engineering solutions on AWS.
Proficiency in Python and SQL for data manipulation, transformation, and analysis within AWS.
Familiarity with cloud-native data architecture and best practices for data management on AWS.
Desired experience in data modelling techniques for optimizing data storage and retrieval.
About You:
Strong work ethic, technology enthusiasm, and passion for AWS ecosystem.
Agility and stress tolerance to handle tight deadlines and fast-paced environments.
Adaptability and team collaboration skills to work effectively with cross-functional teams.
A drive for continuous learning and staying updated with advancements in cloud data engineering.
What's In It For You?
Join an inclusive, collaborative work environment that celebrates success.
Access to the latest tools and technology for creating exceptional customer experiences.
Competitive compensation and additional benefits.
After-work socials.
Location:
Downtown Toronto
Work Arrangement:
Hybrid - Toronto office, 3 days from the office.
If you are a skilled AWS Data Engineer passionate about data-driven solutions and want to be part of a collaborative team shaping the future of our business through data insights, we encourage you to apply. We welcome applicants from all backgrounds to contribute to our innovative and inclusive culture.
Show more
Show less","AWS, Python, SQL, Data Engineering, Data Warehousing, Data Analytics, Data Pipelines, Data Modeling, Data Security, Amazon S3, AWS Glue, AWS Redshift, AWS Lambda, AWS Step Function","aws, python, sql, data engineering, data warehousing, data analytics, data pipelines, data modeling, data security, amazon s3, aws glue, aws redshift, aws lambda, aws step function","amazon s3, aws, aws glue, aws lambda, aws redshift, aws step function, data engineering, data security, dataanalytics, datamodeling, datapipeline, datawarehouse, python, sql"
Data Engineer,Rockwell Automation,"Delhi, Ontario, Canada",https://ca.linkedin.com/jobs/view/data-engineer-at-rockwell-automation-3783819579,2023-12-17,Delta, Canada,Associate,Onsite,"Rockwell Automation is a global technology leader focused on helping the world’s manufacturers be more productive, sustainable, and agile. With more than 28,000 employees who make the world better every day, we know we have something special. Behind our customers - amazing companies that help feed the world, provide life-saving medicine on a global scale, and focus on clean water and green mobility - our people are energized problem solvers that take pride in how the work we do changes the world for the better.
We welcome all makers, forward thinkers, and problem solvers who are looking for a place to do their best work. And if that’s you we would love to have you join us!
Job Description
Key Responsibilities Designs, codes and tests new data management solutions, including supporting applications and interfaces. Architects data structures to provision and enable “Data as a Service” . Supports cross-functional development activity in various DA&I and Connected Enterprise related projects, for internal and external customers Develops and tests infrastructure components in Cloud and Edge-level environments Proactively monitors industry trends and identifies opportunities to implement new technologies Manages the DevOps pipeline deployment model Implements software in all environments Leverages containerization models and works with other engineers and architects to keep the architecture current Assists in the support and enhancement of applications Writes high-quality code compliant with regulations Collaborates with business systems analysts and product owners to define requirements Qualifications Bachelor’s Degree in computer science, software engineering, management information systems, or related field Experience in systems development lifecycle Experience in Data management concepts and implementations Experience with Agile development methodologies and system/process documentation Experience with server-side architectures and containerization Experience with SAP Data Services, Azure ADF, ADLS, SQL, Tabular models, or other domain-specific programming languages Familiarity with business concepts and impact of data on business processes Experience managing multiple projects simultaneously Excellent interpersonal, verbal and written communication skills Ability to adapt quickly to new technologies and changing business requirements Solid problem-solving skills, attention to detail, and critical thinking abilities
Show more
Show less","Data Management, Data Structures, DevOps, Cloud Computing, Edge Computing, Agile Development, Serverside Architectures, Containerization, SAP Data Services, Azure ADF, ADLS, SQL, Tabular Models, Business Concepts, Project Management, Interpersonal Skills, Verbal Communication, Written Communication, Adaptability, ProblemSolving Skills, Attention to Detail, Critical Thinking","data management, data structures, devops, cloud computing, edge computing, agile development, serverside architectures, containerization, sap data services, azure adf, adls, sql, tabular models, business concepts, project management, interpersonal skills, verbal communication, written communication, adaptability, problemsolving skills, attention to detail, critical thinking","adaptability, adls, agile development, attention to detail, azure adf, business concepts, cloud computing, containerization, critical thinking, data management, data structures, devops, edge computing, interpersonal skills, problemsolving skills, project management, sap data services, serverside architectures, sql, tabular models, verbal communication, written communication"
Data & Analytics Specialist,Roche,"Mississauga, Ontario, Canada",https://ca.linkedin.com/jobs/view/data-analytics-specialist-at-roche-3779755155,2023-12-17,Delta, Canada,Associate,Onsite,"The Position
Data and Analytics Specialist
JOB MISSION
As part of Roche´s Business Intelligence and Data Analysis team, the Data and Analytics Specialist will build and maintain trusting, working relationships with business stakeholders in a cross functional environment, understanding the analytic needs of the domain, and working in an agile product team setting fulfilling those needs.
Responsibilities
Transforming, testing, visualizing and documenting analytic data products
Identify, analyze and interpret trends or patterns in complex data sets
Collaborate cross-functionally with data & insights peers and business stakeholders to discover high impact opportunities and deliver robust solutions to enable self-service analytics
Understand where various opportunities fit into the broader strategies and prioritize competing demands to ensure the most impactful items are delivered
Participate as a data analyst in an agile team to deliver analytics to the business stakeholders
Participate in user discovery sessions, iterative refinement cycles with technical team and sprint planning
Develop high-impact data layers, user interfaces and other tools to enable self-service analytics
Gather feedback and input on self-service implementations from end-users and peers and create a cycle of continuous improvement
Keep a close eye on usage of the self-service data products and engage in activities (socialization, documentation, etc.) designed to increase usage
Who You Are
An experienced analytics professional with a proven track record of understanding the true needs of the business stakeholder and how to leverage data / analytic skills and partnerships to fulfill those needs
A team player contributing to an agile team focused on data product delivery
You are comfortable with ambiguity, and able to thrive with minimal oversight and process
Capabilities / Experience
You have exceptional communication and collaborative skills
You have an investigative and curious mindset and easily understand business rules and application to data
Experience with agile methodology and working in agile teams is preferred
You are highly experienced working with data and analytic report interpretation
You are experienced with building and maintaining semantic layers and self-service visualization technologies (Tableau, Power BI, Looker, Qlik, SAP Analytics Cloud etc.) for non-technical end-user use
You can build ETLs for self-use in your analytic solutions (Alteryx, dbt, Talend etc.)
You have the ability to write complex SQL queries, ad-hoc data pipelines and familiarity with dimensional modeling / data warehouse concepts
Knowledge on Python and/or R programming language is a plus
You are proficient on the use of office apps (GSheets/Excel, GDocs/Word, GSlides/PowerPoint ) - GSuite is preferred
Experience in healthcare analytics and working with healthcare data in a regulated environment is a plus
Advanced English is required
Bachelor Degree in Informatics (or equivalent) with 3+ years of relevant experience
At Roche, we believe our people are key to our success in bringing novel medicines to patients, and we are fully dedicated to being a great place to work and to providing the services, tools, and the environment where everybody feels empowered and trusted and is enabled to uncover their full potential. Join us and help build a diverse and inclusive organization where patient centricity and patient access are our priority.
This position is not eligible for relocoation support.
Who we are
At Roche, more than 100,000 people across 100 countries are pushing back the frontiers of healthcare. Working together, we’ve become one of the world’s leading research-focused healthcare groups. Our success is built on innovation, curiosity and diversity.
Roche Pharma Canada has its office in Mississauga, Ontario and employs over 850 employees. The Mississauga facility is bright, vibrant, fosters collaboration and teamwork, and is reflective of Roche's truly innovative culture.
As of January 4, 2022, Roche requires all new employees who work in Canada to be fully vaccinated against COVID-19 on the date they take office. This requirement is a condition of employment at Roche that applies regardless of whether the position is on a Roche campus or remotely. If you have a valid reason for not being fully immunized, which is limited to certain specific medical reasons or other valid reasons protected by applicable human rights laws, you may request an exemption and / or adaptation measures regarding this vaccination requirement.
Roche is an Equal Opportunity Employer.
Show more
Show less","Data and Analytics, Agile development, ETLs, Office apps, SQL, Python, R, Tableau, Power BI, Looker, Qlik, SAP Analytics Cloud, GSheets, Excel, GDocs, Word, GSlides, PowerPoint, Alteryx, dbt, Talend, Dimensional modeling, Data warehouse","data and analytics, agile development, etls, office apps, sql, python, r, tableau, power bi, looker, qlik, sap analytics cloud, gsheets, excel, gdocs, word, gslides, powerpoint, alteryx, dbt, talend, dimensional modeling, data warehouse","agile development, alteryx, data and analytics, datawarehouse, dbt, dimensional modeling, etls, excel, gdocs, gsheets, gslides, looker, office apps, powerbi, powerpoint, python, qlik, r, sap analytics cloud, sql, tableau, talend, word"
Data Scientist,Ample Insight,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/data-scientist-at-ample-insight-3763991633,2023-12-17,Delta, Canada,Associate,Onsite,"Company Description
You will join a world-class team of engineers and data scientists from Facebook, Uber, Amazon and Google. We are a fast growing consulting firm based in Toronto with clients ranging from leading startups building impactful technologies to Fortune 500 companies looking to scale their engineering and data capabilities.
Job Description
We are looking for Data Scientists who are passionate about Machine Learning, and have deep knowledge in Statistics and Computer Science. You enjoy doing data science projects with structured and/or unstructured data and are motivated to productize scalable machine learning models. We will also rely on you to build data products to extract valuable business insights. Critical thinking and problem-solving skills are essential for this role.
Qualifications
BS (or higher, e.g., MS, or PhD) in Computer Science or related engineering field involving coding
Experienced implementing and scaling machine learning models in production environments
Strong understanding of machine learning theory
Hands on experience with Statistics
Capable of quickly implementing prototypes of cutting-edge research papers
Proficient in Python (i.e. Pandas, Numpy, scikit-learn, etc), R, TensorFlow, amongst other data science related tools and libraries
Analytical mind and strong business acumen
If you're passionate about data science and is hungry to learn, please apply!
Additional Information
We have competitive compensation.
Work on cool projects based on your interests and skills. We believe in accountability and NOT micro-management.
Show more
Show less","Python, Pandas, NumPy, scikitlearn, TensorFlow, Machine learning, Statistics, Data science, Data products, Computer science, R","python, pandas, numpy, scikitlearn, tensorflow, machine learning, statistics, data science, data products, computer science, r","computer science, data products, data science, machine learning, numpy, pandas, python, r, scikitlearn, statistics, tensorflow"
"Analyst, Human Resources Data",Air Canada,"Dorval, Quebec, Canada",https://ca.linkedin.com/jobs/view/analyst-human-resources-data-at-air-canada-3784213789,2023-12-17,Delta, Canada,Associate,Onsite,"Description
Job Description
Being part of Air Canada is to become part of an iconic Canadian symbol, recently ranked the best Airline in North America. Let your career take flight by joining our diverse and vibrant team at the leading edge of passenger aviation.
Reporting to the Manager, HR Data Operations, this individual is responsible for supporting internal HR clients, more specifically pertaining to time and attendance, with their business requirements is responsible for recommending and implementing solutions to HR business challenges. This entails working with internal IT stakeholders, external development and vendor partners to implement changes and/or enhancements.
Assess, coordinate and develop the business’ requirements for new HR system development in accordance with business needs and timelines. Areas of support within HR include HR Core, Time and Absence, Scheduling, etc.
Work with business owners to develop the business case and obtain appropriate approvals.
Conduct systems analysis as it pertains to HR business requirements and client support.
Communicate analysis results and make recommendations to relevant stakeholders.
Provide system specifications to IT and ensures changes are tested, implemented and communicated to user community.
Provide user support and procedures to user community when required.
Monitor critical system issues and ensure resolution.
Maintain and update foundation and configuration tables.
Monitor data integrity and validate data anomalies.
Review and streamline end-user processes ensuring accuracy and efficiency.
Qualifications
3-5 years of IT technology or HR experience in a large company, working in the HR systems environment would be an asset.
Experience working with Time and Attendance systems is an asset.
CHRA or an equivalent certification would be an asset.
A relevant University degree/technical certification, and/or relevant experience commensurate to the role.
Exceptional analytical, organizational and communication skills.
Results oriented with proactive and methodical approach to problem solving.
Able to multi-task and work under pressure against tight deadlines and changing priorities.
Must be a team player with ability to work closely with diverse groups and working styles.
Ability to establish and maintain effective business relationships.
Conditions Of Employment
Candidates must be eligible to work in the country of interest, at the time any offer of employment is made and seeking any required work permits/visas or other authorizations which may be required is the sole responsibility of the candidates applying for this position.
Linguistic Requirements
Based on equal qualifications, preference will be given to bilingual candidates.
Diversity and Inclusion
Air Canada is strongly committed to Diversity and Inclusion and aims to create a healthy, accessible and rewarding work environment which highlights employees’ unique contributions to our company’s success.
As an equal opportunity employer, we welcome applications from all to help us build a diverse workforce which reflects the diversity of our customers, and communities, in which we live and serve.
Air Canada thanks all candidates for their interest; however only those selected to continue in the process will be contacted.
Show more
Show less","IT, HR, Time and Attendance, HR Core, Scheduling, System analysis, Data integrity, User support, Problem solving, Multitasking, Team player, Communication skills, Analytical skills, Organizational skills, Proactive approach, Ability to work under pressure, Ability to work with diverse groups, Establishment of effective business relationships","it, hr, time and attendance, hr core, scheduling, system analysis, data integrity, user support, problem solving, multitasking, team player, communication skills, analytical skills, organizational skills, proactive approach, ability to work under pressure, ability to work with diverse groups, establishment of effective business relationships","ability to work under pressure, ability to work with diverse groups, analytical skills, communication skills, data integrity, establishment of effective business relationships, hr, hr core, it, multitasking, organizational skills, proactive approach, problem solving, scheduling, system analysis, team player, time and attendance, user support"
Data Analyst Developer(Remote),RailVision Analytics,"Montreal, Quebec, Canada",https://ca.linkedin.com/jobs/view/data-analyst-developer-remote-at-railvision-analytics-3690070624,2023-12-17,Delta, Canada,Associate,Remote,"Suffering from climate anxiety? So are we. That's why we developed EcoRail, a software solution for reducing Greenhouse gas emissions across the rail industry.
Like the electric car movement, we strive to improve rail with user-friendly AI technology as we drive the rail industry towards being fully autonomous.
About us:
We are a venture-backed start-up at seed stage, with a growing customer base in the passenger and freight markets. Some of our current customers are Metrolinx, ViaRail, and Genesee & Wyoming.
We’ve attracted the attention of some of the top investors in the industry, including Trucks Venture Capital, MUUS Climate Partners, Blackhorn Ventures, and Active Impact Investments.
Our goal is to become a leader in rail software within 3 years and we are already on our way to expanding operations globally! This part is the coolest…Our solution is an app, unlike other players in rail tech which attach hardware to train engines, so customer onboarding is seamless and the possibilities are truly global with respect to impact and expansion.
Why join us?
Our team is awesome. We’ve got a superb bunch of bright minds working together to create something amazing…impact and positive change
We’re big proponents of a flexible remote-work setup and have designed an amazing way to bond with our teams during quarterly meetings around the globe
We value employee health and wellbeing and embrace uncapped paid time off to ensure our teams have the tools to unplug and recharge when needed
Our high-touch hiring process enables us to create competitive and enticing compensation packages that speak to your needs and interests
Our environment fosters learning beyond strict job requirements, and empowers each team member with decision-making power to be the master of their own projects
We are committed to hiring from diverse backgrounds because we truly believe it makes our team stronger
The position and its impact:
As our first Data Analyst, you will have a tremendous impact on how our EcoRail app shapes real-world train operations.
Your aim will be to help us increase fuel efficiency by improving how our EcoRail app provides tips to locomotive drivers as they drive trains.
A big part of your role will be building new analysis scripts and using existing ones to discover how our customer trains move across various rail lines. You'll transform your insights into easy-to-understand visuals and numerical representations. Collaborating with our team, you'll effectively communicate these to both our customers and the wider team.
This position will be highly collaborative as it requires cross functional conversations to action these insights and shape how our entire app operates.
You will build a strong understanding of rail operations and train dynamics while you investigate fuel savings with current rail equipment, GHG emissions, and estimate safety of train operations by tracking unsafe driving behaviour. You will also help us innovate and produce actionable insights in lesser known areas that influence real world train operations.
Other cool things you will do:
Analyze incoming customer data using Python and SQL to understand train operations and determine potential fuel savings / greenhouse gas reductions
Design, build and update custom analytics dashboards for internal and external use
Collaborate with our Operations and Engineering teams to proactively deliver critical insights about product performance and customer operations
You're setup for success if:
You’re a pro at coding in Python, and are no stranger to libraries such as Pandas, NumPy, SciPy
You’ve been around the block and have 1-2 years of relevant work experience with Data Analysis or are a new grad
You've got a good grasp of SQL (PostgreSQL)
You’ve got strong working knowledge of statistical techniques (distributions, regressions, error bounds, etc.)
You’ve worked with git version control and agile methodologies
We are super thrilled to hear from you if..
You’ve got experience with cloud database services such as AWS, InfluxDB
You lived through a Computer Science, Engineering, Physics, or Mathematics degree
(Bonus) Experience in the rail industry….we geek out on trains…a lot :)
We vibe if you:
are a self-starter
are a concise communicator
are passionate about your work
enjoy learning
like to create impact
enjoy setting your teammates up for success by helping us create good documentation and correspondence
RailVision Analytics is proud to be an equal opportunity employer committed to fostering an inclusive, innovative environment with the best employees. We provide employment opportunities without regard to age, race, color, national origin, religion, disability, sex, gender identity or expression, sexual orientation, or any other protected status in accordance with applicable law. If you require accommodation because of a disability or special need for any part of the employment process, please let us know on the job application form how we can help ensure you have a comfortable and positive experience.
Show more
Show less","Python, Pandas, NumPy, SciPy, SQL, PostgreSQL, AWS, InfluxDB, Git, Agile, Machine learning, Data analysis, Data visualization, Statistics, Computer science, Engineering, Physics, Mathematics, Rail industry","python, pandas, numpy, scipy, sql, postgresql, aws, influxdb, git, agile, machine learning, data analysis, data visualization, statistics, computer science, engineering, physics, mathematics, rail industry","agile, aws, computer science, dataanalytics, engineering, git, influxdb, machine learning, mathematics, numpy, pandas, physics, postgresql, python, rail industry, scipy, sql, statistics, visualization"
Data Engineer,Nestlé,"North York, Ontario, Canada",https://ca.linkedin.com/jobs/view/data-engineer-at-nestl%C3%A9-3777119011,2023-12-17,Delta, Canada,Associate,Hybrid,"A Little Bit About Us
While Nestlé is known for KitKat, Gerber, Nescafe, and Häagen-Dazs, our recipe for success comes down to one thing: our people.
We strive to lead a people-focused culture that empowers employees to bring their authentic selves to work each day. There are 3,000+ members of Nestlé Canada celebrated for taking action using agility, courage, and trust to find solutions that benefit the business or greater good. We’re a team of changemakers, who are curious and challenge the status quo, that take risks that will help drive us forward.
Our focus is not only on nourishing our customers, but also about enriching you. We know that empowerment leads to strong employee engagement, a great work culture, and motivated employees.
Position Snapshot
Business area: Nestlé Canada Inc
Job title: Data Engineer
Location: North York, ON located at 25 Sheppard Ave W, North York, ON M2N 6S8
Hybrid
Benefits
Comprehensive total rewards benefits package including Health and Dental benefits that start on day one of employment
Company matched pension plan
Three weeks of Vacation and five personal days (Personal Paid Holidays)
Flexible and hybrid work arrangements
Excellent training and development programs as well as opportunities to grow within the company
Access to Educational Assistance & Tuition Reimbursement
Bonus eligibility
Up to 50% off – Nespresso Coffee Machine, Capsules and accessories
Access to the Discount Company store with Nestlé, Nespresso, and Purina products (Located across various Nestle offices/sites)
Additional discounts on a variety of products and services offered by our preferred vendors and partnerships
Position Summary
We are looking for a
Data Engineer
based at our
North York office
reporting into
the Data Engineer Manager
. This role is responsible for managing every step of the data process from requirements gathering to stakeholder engagement to ensuring that data is in the right form and quality required for analysis (e.g. through data validation and testing) by downstream users.
A day in the life of a Data Engineer
We are a team of IT professionals from many countries and diverse backgrounds, each with unique missions and challenges in the biggest health, nutrition and wellness company of the world. We innovate every day through forward-looking technologies to create opportunities for Nestlé’s digital challenges with our consumers, customers and at the workplace. We collaborate with our business partners around the world to deliver standardized, integrated technology products and services to create tangible business value.
As Data Engineer you will be working on developing data solutions to empower business users to derive actionable insights and business decisions. You will also be involved in sustaining/improving existing cloud data solutions to ensure data freshness. In addition, you will play a key role in understanding data needs of our business functions, translating their business requirements to data engineering solutions and leading development of data projects in cloud by working closely with our stakeholders.
Service Delivery
Responsible for developing and maintaining scalable data pipelines in cloud for advanced analytics.
Analyze data and data structures, as well as the design and build of all forms of database schemas (OLTP, OLAP etc..) to support a wide range of strategies for secure data acquisition, data modelling, data cleansing, blending and data storage.
Stakeholder Management
Responsible to engage with business users about their data needs/clarifications.
Responsible for communication of project updates/risks/challenges to business users.
Engage with Receivers as appropriate to ensure business understanding and receiver satisfaction.
People
Work closely with team of experts, locally and remotely to ensure service delivery.
Collaborate and work with the business users, visualization architects and data scientists to further build Nestle capabilities.
Collectively work as part of Data Engineering team to foster team cohesion and bring to table innovate approaches to address data challenges.
What will make you successful?
University Degree in Applied Statistics, Mathematics and/or Computer Science/Computer Engineer.
Minimum 2-3 years working across BIG or complex data sets in a statistical/analytical role; academic work experience may be considered/included.
Preferred Qualifications:
Strong understanding of data engineering concepts, including idempotency, data modelling, ETL process and data integration.
Demonstrated ability to articulate ideas, and effectively convey complex technical concepts to both technical and non-technical audiences.
Knowledge of designing a transactional schema, star schema and/or snowflake schema.
Experience with cloud-based platforms- Microsoft Azure, Azure Databricks and Snowflake.
Expertise in working with structured and semi-structured data; apply methods, technologies and techniques that address data architecture, integration, and governance of data.
Experience with Azure Cloud services – Azure Synapse Analytics, Data Factory, Logic Apps, Azure Data Lake Storage Gen2.
Experience with Snowflake architecture and data ingress/egress methodologies.
Well-versed with Delta Lake concept and medallion architecture in Databricks.
Knowledge of master data management principles and experience with data catalogue applications.
Possess solid skills in understanding data structures in Python or Scala, SQL concepts including but not limited to ACID compliance, slowly changing dimensions and knowledge of spark including but not limited to its use case, memory management, parallelization etc.
Experience working in Azure DevOps to adopt continuous integration and continuous delivery method.
Knowledge of Azure Compute services, uses, and methodologies.
Demonstrable experience with Agile projects and knowledge of Scrum techniques and artifacts (such as definition of sprint planning, planning poker, feature creation, user stories and backlog refinement)
Knowledge of SAP systems and technologies, including BW and HANA.
Provide thought-leadership and dependable execution on diverse projects.
We have a friendly, supportive team with a coaching and mentoring environment. There are real opportunities for future development and progression – this really could be a move towards the exciting Information Technology career you’ve always wanted.
What You Need To Know
We will be considering applicants as they apply, so please don’t delay in submitting your application.
Nestlé Canada is an equal-opportunity employer committed to diversity, equity, inclusion, and accessibility. We welcome qualified applicants to bring their diverse and unique experiences as a result of their education, perspectives, culture, ethnicity, race, sex, gender identity and expression, nation of origin, age, languages spoken, veteran’s status, colour, religion, disability, sexual orientation and beliefs.
If you are selected to participate in the recruitment process, please inform Human Resources of any accommodations you may require. Nestlé will work with you in an effort to ensure that you are able to fully participate in the process.
Show more
Show less","Data Engineering, Agile, Scrum, Python, Scala, SQL, Azure, Databricks, Snowflake, SAP, BW, HANA, ETL, ACID, Big Data, Data Modeling, Data Cleansing, Data Storage, Data Acquisition, Data Governance, Statistical Analysis, Cloud Computing, Data Integration, Data Pipelines, Master Data Management, Data Catalogue, Continuous Integration, Continuous Delivery, DevOps, Stakeholder Management, Communication","data engineering, agile, scrum, python, scala, sql, azure, databricks, snowflake, sap, bw, hana, etl, acid, big data, data modeling, data cleansing, data storage, data acquisition, data governance, statistical analysis, cloud computing, data integration, data pipelines, master data management, data catalogue, continuous integration, continuous delivery, devops, stakeholder management, communication","acid, agile, azure, big data, bw, cloud computing, communication, continuous delivery, continuous integration, data acquisition, data catalogue, data engineering, data governance, data integration, data storage, databricks, datacleaning, datamodeling, datapipeline, devops, etl, hana, master data management, python, sap, scala, scrum, snowflake, sql, stakeholder management, statistical analysis"
Data Engineer,Strategic Solutions Technology Staffing Inc.,"Greater Toronto Area, Canada",https://ca.linkedin.com/jobs/view/data-engineer-at-strategic-solutions-technology-staffing-inc-3783677082,2023-12-17,Delta, Canada,Associate,Hybrid,"Our client is a prominent global asset management company committed to delivering exceptional investment solutions. As a leader in the industry, they rely on robust data infrastructure to support its investment strategies and decision-making processes. We are seeking a talented Data Engineer to join our team, dedicated to maintaining and expanding our data infrastructure and ensuring the reliability and scalability of our data systems.
Job Description:
Our client is looking for a Data Engineer to play a crucial role in building and maintaining their data infrastructure. In this role, you will collaborate with cross-functional teams to design, implement, and manage data pipelines, databases, and data warehouses. You will also be responsible for optimizing data workflows, ensuring data availability, and supporting the data needs of our investment and analytics teams.
Key Responsibilities:
Data Pipeline Development: Design and develop robust data pipelines to extract, transform, and load (ETL) data from various sources into data repositories.
Database Management: Manage and optimize databases to ensure data integrity and availability for data analysis and reporting.
Data Warehouse Design: Contribute to the design and maintenance of data warehouses for efficient storage and retrieval of structured and unstructured data.
Data Quality Assurance: Implement data quality checks and validation processes to ensure data accuracy and consistency.
Automation: Develop and maintain automated data processes, reducing manual data handling and improving efficiency.
Scalability and Performance: Optimize data infrastructure for scalability, performance, and cost-effectiveness.
Data Security: Implement data security measures to protect sensitive data and ensure compliance with regulatory requirements.
Collaboration: Work closely with data analysts, data scientists, and other stakeholders to understand data requirements and provide support.
Documentation: Create and maintain comprehensive documentation for data processes and systems.
Qualifications:
Bachelor's degree in Computer Science, Data Engineering, or a related field.
Proven experience in data engineering, ETL processes, and database management.
Proficiency in data engineering tools and languages (e.g., Python, SQL, Spark).
Experience with data warehousing and data modeling concepts.
Strong problem-solving and troubleshooting skills.
Knowledge of data security and compliance best practices.
Excellent communication and teamwork skills.
Familiarity with cloud platforms (e.g., AWS, Azure, GCP) is a plus.
Show more
Show less","Data Engineering, ETL, Python, SQL, Apache Spark, Data Warehousing, Data Modeling, Data Quality Assurance, Data Security, Data Compliance, AWS, Azure, GCP, Data Pipelines, Databases, Data Analytics, Collaboration, Documentation, Cloud Platforms","data engineering, etl, python, sql, apache spark, data warehousing, data modeling, data quality assurance, data security, data compliance, aws, azure, gcp, data pipelines, databases, data analytics, collaboration, documentation, cloud platforms","apache spark, aws, azure, cloud platforms, collaboration, data compliance, data engineering, data quality assurance, data security, dataanalytics, databases, datamodeling, datapipeline, datawarehouse, documentation, etl, gcp, python, sql"
Senior Python Data Engineer,Hour Consulting,"North York, Ontario, Canada",https://ca.linkedin.com/jobs/view/senior-python-data-engineer-at-hour-consulting-3780079593,2023-12-17,Delta, Canada,Associate,Hybrid,"Our client, a rapidly expanding global technology partner, is looking for a highly skilled
Senior Python Data Engineer
to join their exceptional Technology and Development team. If you are passionate about demonstrating your expertise and thrive on collaborating with a group of talented engineers, then this role was made for you!
At the heart of technology innovation, our client specializes in delivering cutting-edge solutions to clients across a wide array of sectors. With a strategic focus on finance, banking, and corporate verticals, they have earned a stellar reputation for their commitment to excellence in every project they undertake.
They seek an experienced
Senior Python Data Engineer
with a strong background in building Extract, Transform, Load (ETL) processes and a deep understanding of AWS serverless cloud environments.
As a vital member of the data engineering team, you will play a critical role in designing, developing, and maintaining data pipelines that facilitate data ingestion, transformation, and storage for our organization.
Your expertise will contribute to the foundation of our data infrastructure, enabling data-driven decision-making and analytics.
Key Responsibilities:
ETL Pipeline Development: Design, develop, and maintain ETL processes using Python, AWS Glue, or other serverless technologies to ingest data from various sources (databases, APIs, files), transform it into a usable format, and load it into data warehouses or data lakes.
AWS Serverless Expertise: Leverage AWS services such as AWS Lambda, AWS Step Functions, AWS Glue, AWS S3, and AWS Redshift to build serverless data pipelines that are scalable, reliable, and cost-effective.
Data Modelling: Collaborate with data scientists and analysts to understand data requirements and design appropriate data models, ensuring data is structured optimally for analytical purposes.
Data Quality Assurance: Implement data validation and quality checks within ETL pipelines to ensure data accuracy, completeness, and consistency.
Performance Optimization: Continuously optimize ETL processes for efficiency, performance, and scalability, monitoring and troubleshooting any bottlenecks or issues that may arise.
Documentation: Maintain comprehensive documentation of ETL processes, data lineage, and system architecture to ensure knowledge sharing and compliance with best practices.
Security and Compliance: Implement data security measures, encryption, and compliance standards (e.g., GDPR, HIPAA) as required for sensitive data handling.
Monitoring and Logging: Set up monitoring, alerting, and logging systems to identify and resolve data pipeline issues proactively.
Collaboration: Work closely with cross-functional teams, including data scientists, data analysts, software engineers, and business stakeholders, to understand data requirements and deliver solutions.
Continuous Learning: Stay current with industry trends, emerging technologies, and best practices in data engineering and cloud computing and apply them to enhance existing processes.
Qualifications:
Bachelor's or Master's degree in Computer Science, Data Science, or a related field.
Proven experience as a Data Engineer with a focus on ETL pipeline development.
Strong proficiency in Python programming.
In-depth knowledge of AWS serverless technologies and services.
Familiarity with data warehousing concepts and tools (e.g., Redshift, Snowflake).
Experience with version control systems (e.g., Git).
Strong SQL skills for data extraction and transformation.
Excellent problem-solving and troubleshooting abilities.
Ability to work independently and collaboratively in a team environment.
Effective communication skills for articulating technical concepts to non-technical stakeholders.
Certifications such as AWS Certified Data Analytics - Specialty or AWS Certified DevOps Engineer are a plus.
Preferred Experience:
Knowledge of data orchestration and workflow management tools
Familiarity with data visualization tools (e.g., Tableau, Power BI).
Previous experience in industries with strict data compliance requirements (e.g., insurance, finance) is beneficial.
What's In It For You?
Join an inclusive, collaborative work environment that fosters creativity and curiosity and celebrates success!
Receive access to the latest tools and technology to create exceptional customer experiences.
Enjoy career advancement opportunities. Your talent is valued, and they want to see you succeed, not just in your current role but throughout your career.
After-work socials
We encourage you to apply today if you're ready to take the reins of your career and be a driving force behind transformative data solutions—a chance to unlock the power of data to make a real difference.
Location: North York, Onsite - 3/4 days a week
Full Time: Permanent & Contract
Show more
Show less","Python, AWS, Glue, Lambda, Step Functions, S3, Redshift, Data Modelling, Data Quality Assurance, Performance Optimization, Documentation, Security, Compliance, Monitoring, Logging, Git, SQL, Tableau, Power BI","python, aws, glue, lambda, step functions, s3, redshift, data modelling, data quality assurance, performance optimization, documentation, security, compliance, monitoring, logging, git, sql, tableau, power bi","aws, compliance, data modelling, data quality assurance, documentation, git, glue, lambda, logging, monitoring, performance optimization, powerbi, python, redshift, s3, security, sql, step functions, tableau"
Data Engineer,TouchTunes,"Montreal, Quebec, Canada",https://ca.linkedin.com/jobs/view/data-engineer-at-touchtunes-3785585776,2023-12-17,Delta, Canada,Associate,Hybrid,"*French will follow*
🗺️ Hybrid Montreal tech hub
📌Your mission in the Data Science team:
You will put your kick-ass SQL and Python skills into designing and developing scalable and complex data pipelines on Databricks to extract, move, and combine huge volumes of data from multiple data sources within the organization. You will work with Data Scientists, Data Analysts, Data Architects, and Product Managers to enable delivering amazing products and clarifying our lens of monitoring our products’ performance.
🚀Our stack:
Pipeline: Databricks
Cloud: AWS
Stack: Python & SQL (expert in both)
Container: Docker & Kubernetes
Architecture: Microservices
🔍 Your profile:
At least 5 years of experience as a Data engineer or ML engineer
Working knowledge of data streaming and big data pipeline design & and development
Experience assembling large, complex data sets that meet functional / non-functional business requirements.
Experience with data science and ML frameworks and libraries
Familiarity with data warehousing, data lakes, and data modeling concepts
💡 Your edge:
Experience with Scala or Java
Experience with PyTorch or Tensorflow
❤️Our perks:
Flexwork:
Hybrid WFH-office environment and work anywhere around the globe for up to 1 month/year
Learning:
Free e-learning access and the ability to attend conferences in your field of your choice!
Health:
Competitive health insurance, Dialogue app, and unlimited sick days
Commute:
50% off OPUS and EXO train and BIXI
Office:
Office accessible to people with disabilities
Etc.:
RRSP matching, Life/LT/ST Disability coverage, Weekly free lunch and healthy snacks, and our very own TouchTunes Culture Collective Community!
🎵 TouchTunes
Found at over 65,000 bars, restaurants, and other social venues across North America and Europe, TouchTunes brings people together with fun, interactive music, and entertainment solutions, enhancing how people interact with each other and their environment. Our native mobile apps and jukes are seen and heard by millions daily across the TouchTunes network and community.
We firmly believe that our team should be as diverse as the music we listen to.
🌈TouchTunes is a proud ally of
QueerTech.
_____________________________________________
🗺️ Pôle technologique hybride de Montréal
📌 Votre mission au sein l'équipe Data Science :
Vous concevrez et développerez des pipelines de données évolutifs et efficaces sur Databricks, en utilisant Spark, Python et SQL pour extraire et déplacer des données à partir de multiples sources de données au sein de l'organisation, tout en permettant aux scientifiques des données et aux gestionnaires de produits de livrer des produits passionants.
🚀 Notre stack :
Pipeline: Databricks
Cloud: AWS
Stack: Python & SQL (expert(e))
Container: Docker & Kubernetes
Architecture: Microservices
🔍 Ton profil :
Au moins 2 ans dans un rôle d'ingénierie de données ou d'ingénierie ML.
Expérience des plateformes big data et des plateformes cloud
Expérience dans l'assemblage de big data complexes qui répondent aux exigences fonctionnelles / non fonctionnelles de l'entreprise.
Expérience de la science des données et des cadres et bibliothèques de ML.
Familiarité avec l'entreposage de données, le ''data lake'' et les concepts de modélisation des données.
💡 Tes atouts:
Expérience avec Scala ou Java
Expérience avec PyTorch ou Tensorflow
❤️ Nos avantages :
Travail flexible :
Environnement de bureau hybride (télétravail et burau) et possibilité de travailler à partir de n'importe où dans le monde jusqu'à 1 mois/an.
Formation :
Accès gratuit à Udemy, possibilité d'assister à des conférences dans le domaine de votre choix !
Santé :
Assurance maladie compétitive, application Dialogue et jours de maladie illimités.
Déplacements domicile-travail :
50% de réduction sur le train OPUS, EXO et BIXI
Bureau :
Bureau accessible aux personnes handicapées.
Etc. :
Cotisation de contrepartie au REER, couverture vie/invalidité, déjeuner hebdomadaire gratuit et collations saines ainsi que notre Communauté Culture Collective de TouchTunes !
🎵 TouchTunes
Présent dans plus de 65 000 bars, restaurants et autres lieux de rencontre en Amérique du Nord et en Europe, TouchTunes rassemble les gens grâce à des solutions musicales et de divertissement amusant et interactif, améliorant ainsi la façon dont les gens interagissent entre eux et avec leur environnement. Nos applications mobiles natives et nos jukes sont vus et entendus par des millions de personnes chaque jour à travers le réseau et la communauté TouchTunes.
Nous sommes convaincus que notre équipe doit être aussi diversifiée que la musique que nous écoutons.
🌈 TouchTunes est un fier allié de
QueerTech
.
Show more
Show less","SQL, Python, Databricks, AWS, Docker, Kubernetes, Microservices, Data Science, Machine Learning, PyTorch, Tensorflow, Scala, Java","sql, python, databricks, aws, docker, kubernetes, microservices, data science, machine learning, pytorch, tensorflow, scala, java","aws, data science, databricks, docker, java, kubernetes, machine learning, microservices, python, pytorch, scala, sql, tensorflow"
"Senior Data Engineer, Public & Alternative Investments, Data Platforms",CPP Investments | Investissements RPC,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/senior-data-engineer-public-alternative-investments-data-platforms-at-cpp-investments-investissements-rpc-3767684924,2023-12-17,Delta, Canada,Associate,Hybrid,"Company Description
Make an impact at a global and dynamic investment organization
When you invest your career in CPP Investments, you join one of the most respected and fastest growing institutional investors in the world. With current assets under management valued in excess of $500 billion, CPP Investments is a professional investment management organization that globally invests the funds of the Canada Pension Plan (CPP) to help ensure long-term sustainability. The CPP Fund is projected to reach $3 trillion by 2050. CPP Investments invests in all major asset classes, including public equity, private equity, real estate, infrastructure and fixed-income instruments, and is headquartered in Toronto with offices in Hong Kong, London, Luxembourg, Mumbai, New York City, San Francisco, São Paulo and Sydney.
CPP Investments attracts and selects high-calibre individuals from top-tier institutions around the globe. Join our team and look forward to:
Diverse and inspiring colleagues and approachable leaders
Stimulating work in a fast-paced, intellectually challenging environment
Accelerated exposure and responsibility
Global career development opportunities
Being motivated every day by CPP Investments’ important social purpose and unshakable principles
A flexible/hybrid work environment combining in office collaboration and remote working
A deeply rooted culture of Integrity, Partnership and High Performance
If you share a passion for performance, value a collegial and collaborative culture, and approach everything with the highest integrity, here’s an opportunity for you to invest your career at CPP Investments.
Job Description
As a Senior Data Engineer, you will be working on the Data Service Applications for Public Markets and Alternative Assets. The role will involve -
Role-Specific Accountabilities
Design, implement and deploy the data pipelines to transform (structure and map) vendor data into CPPIB standard format.
Design and implement the data model and enriched strategy neutral data.
Design and implement the data service using Amazon Web Service (AWS)
Design and implement data related tools and analytics, in addition to operational processes.
Address daily data exceptions on a timely basis and provide L3 operation support.
Stay current with new data vendor product offerings and how they might be used by the quantitative investment groups.
Manage and enhance investment data infrastructure through initiatives such as improving metadata and data integrity
Work in a fast-paced environment collaborating with business users, engineers, architects, researchers, and data scientists.
Qualifications
Skills/Knowledge/Competencies/Capabilities
A Bachelor’s degree in computer science or engineering
Hands on Experience with data engineering technologies such as AWS Glue, EMR, Athena, Redshift, Lake Formation, Apache Spark, Apache Hive, Apache Airflow, S3, CloudWatch, Lambda, Apache Hudi, API Gateway, GraphQL, Elastic Search, Elastic Cache and Trino.
Hands on experience with CI/CD pipeline using Terraform, Jenkins, Github actions, Gitflow
Experience working with RESTful APIs. Familiarity with microservices architecture
Strong programming skills in Python, pyspark, SQL, Java with professional programming experience in a data context is a must (e.g. data extraction, data manipulation, joins, data analysis, data model design etc.)
Experience of building data pipelines using orchestration tool like Apache Airflow.
Experience in ML and Generative AI is a big plus.
Ability to deliver time-sensitive deadlines
Good understanding of financial data; CFA is a plus.
Passionate to learn new technology
Additional Information
Visit our
LinkedIn Career Page
or
Follow us
on
LinkedIn.
At CPP Investments, we are committed to diversity and equitable access to employment opportunities based on ability.
We thank all applicants for their interest but will only contact candidates selected to advance in the hiring process.
Our Commitment to Inclusion and Diversity:
In addition to being dedicated to building a workforce that reflects diverse talent, we are committed to fostering an inclusive and accessible experience. If you require an accommodation for any part of the recruitment process (including alternate formats of materials, accessible meeting rooms, etc.), please let us know and we will work with you to meet your needs.
Disclaimer:
CPP Investments does not accept resumes from employment placement agencies, head-hunters or recruitment suppliers that are not in a formal contractual arrangement with us. Our recruitment supplier arrangements are restricted to specific hiring needs and do not include this or other web-site job postings. Any resume or other information received from a supplier not approved by CPP Investments to provide resumes to this posting or web-site will be considered unsolicited and will not be considered. CPP Investments will not pay any referral, placement or other fee for the supply of such unsolicited resumes or information.
Mandatory Vaccine Policy
All employees in the Toronto, Sydney and Hong Kong offices will be required to be vaccinated against COVID-19. Accommodations to this policy will be made for medical or other protected grounds. Please contact us to discuss any accommodation needs.
Show more
Show less","AWS Glue, EMR, Athena, Redshift, Lake Formation, Apache Spark, Apache Hive, Apache Airflow, S3, CloudWatch, Lambda, Apache Hudi, API Gateway, GraphQL, Elastic Search, Elastic Cache, Trino, Terraform, Jenkins, Github actions, Gitflow, RESTful APIs, Microservices architecture, Python, Pyspark, SQL, Java, Apache Airflow, ML, Generative AI, CFA, Financial data","aws glue, emr, athena, redshift, lake formation, apache spark, apache hive, apache airflow, s3, cloudwatch, lambda, apache hudi, api gateway, graphql, elastic search, elastic cache, trino, terraform, jenkins, github actions, gitflow, restful apis, microservices architecture, python, pyspark, sql, java, apache airflow, ml, generative ai, cfa, financial data","apache airflow, apache hive, apache hudi, apache spark, api gateway, athena, aws glue, cfa, cloudwatch, elastic cache, elastic search, emr, financial data, generative ai, gitflow, github actions, graphql, java, jenkins, lake formation, lambda, microservices architecture, ml, python, redshift, restful apis, s3, spark, sql, terraform, trino"
Cloud Data Engineer - AWS EMR and Glue,Hays,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/cloud-data-engineer-aws-emr-and-glue-at-hays-3787324425,2023-12-17,Delta, Canada,Associate,Hybrid,"Fixed term contract till March 2026
Your new company
Our client is an independent, not-for-profit organization that provides essential information on Canada's health system and the health of Canadians.
Your new role
Reporting to the Manager, Enterprise Technology Services, the Cloud Administrator is responsible for the design, implementation, and management of data-related services and infrastructure in the AWS Cloud Infrastructure. Cloud Administrator is responsible for system benchmarking and optimization through pro-active monitoring and administrative technologies to identify potential issues before they impact the IT environment. The role advises and contributes to IT strategy, roadmap, process and project planning activities and identifies areas for improvement in operational processes and procedures and proposes innovative, cost-effective solutions within the technology services operations. The scope of this position, security will play a pivotal role in all aspects from planning to implementation and throughout the operational life cycle of the information technology assets and will work closely with Information Security staff in order to ensure security is maintained at high levels for all information assets while maintaining reliability, accessibility, service and ease of use for end-users.
Responsibilities
Support Information Security Management System (ISMS) through the implementation, management and monitoring of required Information Security controls, including but not limited to Risk Management, Audit and Assessment, Monitoring and Reporting.
Provision and manage AWS data-related services and infrastructure. Monitor and optimize AWS Services for performance and cost-efficiency.
Configure and manage Amazon big data clusters. Work with AWS technologies for data processing and analytics tasks.
Plan and Implement changes to Infrastructure and technology environments with ITSM processes (Release Management Process, Change Management, Incident Management/Service Desk, Problem Management etc.).
Execute configuration changes, patching/upgrading and day-to-day administration/ monitoring of applicable domain technologies.
Provide senior-level cross-training, support and knowledge transfer to ITS development/ maintenance teams and business areas.
Monitor the applicable domain technology platform performance and reports/responds to any degradation of service to ensure that user experience and expectations are being met. Identifies areas of improvement to admin teams and management.
Contribute technical expertise to the teams. Provide support and expertise relating to best practices for coding, report generation, performance tuning etc.
Participate in the development, maintenance, testing, and implementation of backup and recovery procedures for organization’s environments.
Develops and documents policies and procedures for technical system processes. Maintains and documents related data profiles/engineering and architecture.
Leads and coordinates the resolution of technical issues in a time-sensitive/priority-sensitive manner.
What you'll need to succeed
Computer Technology Diploma or bachelor’s degree in computer science or related field or equivalent education and experience.
Minimum of 5 years in related progressive experience.
Strong and solid technical expertise with the following within an enterprise environment (Installation, configuration, support, troubleshooting).
Python and PySpark.
EC2 instances, S3, EBS, EFS, Security Groups, IAM, AWS Airflow, AWS Redshift instances, AWS lambda functions
Amazon EMR (Elastic MapReduce) clusters,
Apache Spark, and Apache Hive
AWS Lake Formation
Identity and Access Management (IAM)
Infrastructure as Code (IaC), AWS CloudFormation
Monitoring and Alerting, CloudTrail and CloudWatch
Excellent analytical, problem solving and troubleshooting skills.
Works well under pressure and can prioritize competing priorities appropriately.
Ability to work with minimal supervision and direction.
Excellent interpersonal and communication skills – both written and oral. With the ability to communicate effectively with users, management, and technical staff.
Ability to work in fast-paced multi-disciplinary teams.
Ability to provide after-hours/weekend support and provide on-call on a rotational basis when required.
An innovative, progressive, and creative thinker who increases knowledge through formal training and research to stay current on technology and trends.
Fluency in both official languages is an asset.
What you'll get in return
A flexible and supportive work environment. Fixed long term contract opportunity
Interested?
Please share your resume in word document highlighting your expereince in AWS EMR and Lake formation
Show more
Show less","Cloud Administrator, AWS Cloud Infrastructure, System benchmarking, Optimization, Proactive monitoring, Administrative technologies, IT strategy, Roadmap, Process and project planning, AWS datarelated services, Infrastructure, Amazon big data clusters, AWS technologies, Data processing, Analytics tasks, Infrastructure as Code (IaC), AWS CloudFormation, Monitoring, Alerting, CloudTrail, CloudWatch, Python, PySpark, EC2 instances, S3, EBS, EFS, Security Groups, IAM, AWS Airflow, AWS Redshift instances, AWS lambda functions, AWS Lake Formation, Identity and Access Management, EMR (Elastic MapReduce) cluster, Apache Spark, Apache Hive","cloud administrator, aws cloud infrastructure, system benchmarking, optimization, proactive monitoring, administrative technologies, it strategy, roadmap, process and project planning, aws datarelated services, infrastructure, amazon big data clusters, aws technologies, data processing, analytics tasks, infrastructure as code iac, aws cloudformation, monitoring, alerting, cloudtrail, cloudwatch, python, pyspark, ec2 instances, s3, ebs, efs, security groups, iam, aws airflow, aws redshift instances, aws lambda functions, aws lake formation, identity and access management, emr elastic mapreduce cluster, apache spark, apache hive","administrative technologies, alerting, amazon big data clusters, analytics tasks, apache hive, apache spark, aws airflow, aws cloud infrastructure, aws cloudformation, aws datarelated services, aws lake formation, aws lambda functions, aws redshift instances, aws technologies, cloud administrator, cloudtrail, cloudwatch, data processing, ebs, ec2 instances, efs, emr elastic mapreduce cluster, iam, identity and access management, infrastructure, infrastructure as code iac, it strategy, monitoring, optimization, proactive monitoring, process and project planning, python, roadmap, s3, security groups, spark, system benchmarking"
Data Analyst,Insight Global,"Calgary, Alberta, Canada",https://ca.linkedin.com/jobs/view/data-analyst-at-insight-global-3781726440,2023-12-17,Delta, Canada,Associate,Hybrid,"6 MONTH CONTRACT
HYBRID CALGARY 2X PER WEEK
Insight Global is looking for Data Analyst to join one of the largest convenience and fuel retailing companies in Alberta. This is a 6 month contract with a high possibility of extensions. It is a hybrid role, with the expectation of a successful candidate being in office 2 days a week in Calgary, Alberta. Day-to-day, a successful candidate be reviewing and manipulating large sets of data to support load and data cleanse activities. Responsibilities include managing customer lists to support system migrations and other data related efforts including cleansing, mapping and massaging.
Must Haves:
· 3+ years' experience working as a Data Analyst
○ Ability to work across IT/ systems, finance, and business functions (with support) to source and provide data for project purposes
· Knowledge and/or working experience in large ERP systems (JDE and Oracle preferred)
· Advanced knowledge of Microsoft Excel including manipulating large sets of data, understanding equations using equations, index match and VLOOKUP's.
· Experience building dashboards using Power BI and Tableau
· Understanding of sFTP file transfer processes
· Bachelor's degree in a relevant field e.g. computer or data science
Plusses:
· Retail/industry experience
· Experience working with highly sensitive data
Show more
Show less","Data Analysis, Data Manipulation, Data Cleansing, Data Migration, Customer List Management, Microsoft Excel, Power BI, Tableau, sFTP, JDE, Oracle, IT systems, Finance, Business functions, Project management","data analysis, data manipulation, data cleansing, data migration, customer list management, microsoft excel, power bi, tableau, sftp, jde, oracle, it systems, finance, business functions, project management","business functions, customer list management, data manipulation, data migration, dataanalytics, datacleaning, finance, it systems, jde, microsoft excel, oracle, powerbi, project management, sftp, tableau"
"Specialist, Data Developer- EN",CN,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/specialist-data-developer-en-at-cn-3774813032,2023-12-17,Delta, Canada,Associate,Hybrid,"At CN, we work together to move our company—and North America—forward. Be part of our Information & Technology (I&T) team, a critical piece of the engine that keeps us in motion. From enterprise architecture to operational technology, our teams use the agile methodology to automate and digitize our railroad ensuring our operations run optimally and safely and our employees can focus on value-added tasks. You will be able to develop your skills and career in our close-knit, safety-focused culture working together as ONE TEAM. The careers we offer are meaningful because the work we do matters. Join us!
Job Summary
The Specialist, Data Developer is responsible for building, handling, and optimizing data pipelines. The role moves them effectively into production for key data and analytics consumers, shapes the enterprise Data as a Service (DaaS) model and delivers on Information and Technology (I&T) business models. Moreover, the incumbent develops best practices and optimizes data pipelines to deliver products and services aligned with business expectations. The position plays a pivotal role in operationalizing data and analytics initiatives, defining and building CN’s data integration and DaaS platform roadmap.
Main Responsibilities
Data Development
Ensure optimal data delivery architecture and processes are consistent throughout ongoing projects
Optimize CN’s data architecture to support the next generation of products and data initiatives
Build, handle, and optimize data pipelines, moving them effectively into production for key data and analytics consumers
Build data and domain event models, implement business rules, and scalable data pipelines
Ensure compliance with data governance and security requirements while creating, improving, and operationalizing integrated and reusable data pipelines
Enable faster data access, integrate data reuse, and improve time-to-solution for data and analytics initiatives
Integrate analytics and data science results with business processes
Promote effective data management practices
Collaborate with Data Science, Reporting, Analytics and other Development teams to build data pipelines, infrastructure and tooling to support business initiatives
Design and develop Exact, Transfer and Load (ETL) pipelines using multiple sources of data in various formats and deploy them to achieve a high-level of reliability, scalability, and security
Data Design
Collaborate with stakeholders and architects to model data landscape and define secure data exchange approaches
Meet with stakeholders to identify fit-for-purpose within CN’s existing data ecosystem and deliver options and agile solutions
Design and develop processing pipelines that ingest data into Data Hubs
Provide day-to-day support and technical expertise to both technical and
non-technical teams
Participate in building data development expertise and framework
Translate business needs into technical requirements
Data Operations and Quality Assurance
Use Agile methodologies and development practices to streamline project delivery aligned with goals, timelines, and budgets and for code reviews and testing to develop and deliver data pipelines
Build monitoring and debugging tools to analyze data pipelines
Help unify software development and operations seamlessly, efficiently, and cost effectively
Improve software quality, automate processes, and accelerate software releases
Develop and implement test plans and scripts for various data quality processes
Maintain manual and automated test scripts
Working Conditions
Due to the nature of the role, the incumbent must be able to meet tight deadlines, handle pressure, and stress.
Requirements
Experience
Data Development
Minimum 5 years overall work experience
Minimum 3 years of experience in a Data Development role, working in different data management disciplines including data integration, modelling, optimization, and quality
Ø Experience working in cross-functional teams and collaborating with business stakeholders in support of a departmental or multi-departmental data management and analytics initiative
Ø Experience in translating business requirements into advanced data models able to fulfill Analysts and Data Scientists’ requirements
Ø Experience working in an Agile team environment
Experience with Informatica IICS (Informatica Intelligent Cloud Services)
Experience with Enterprise Data Catalog
Education/Certification/Designation
Bachelor's Degree in Computer Science, Electrical Development or Software Development
Google or Azure Data Development certification*
Any designation for these above would be considered as an asset
Competencies
Inspires others with impactful communications and adapts to the audience through speech and writing
Applies analytical thinking
Innovates through problem solving
Knows the business and stays current with industry trends to elevate expertise and work
Demonstrates organizational abilities
Collaborates with key internal stakeholders to enable higher productivity
Works independently with little supervision
Technical Skills/Knowledge
Knowledge of Scala, Java or Python
Knowledge of software development best practices such as code reviews, testing frameworks, maintainability, and readability
Expertise with Databricks Delta Lake
Knowledge of Structured Query Language (SQL) and Non-Structured Query Language (NoSQL) technologies and fluent in writing, executing, and optimizing SQL queries
Knowledge of Big Data technologies and cloud platforms such as Databricks, Apache Spark, Azure Data Factory, Azure Data Explorer, Azure Data Lake, Google BigQuery, Google Dataproc, Google Cloud Data Fusion, Google Dataflow, Google Cloud Composer, Google, Dataprep, Google Dataplex, Google BigLake, Google Vertex
Knowledge of event-driven architecture (e.g., Pub-Sub, Kafka, Message Queuing (MQ), Message Queuing Telemetry Transport (MQTT), Advanced Message Queuing Protocol (AMQP), Event Hub, Logstash)
About CN
CN is a world-class transportation leader and trade-enabler. Essential to the economy, to the customers, and to the communities it serves, CN safely transports more than 300 million tons of natural resources, manufactured products, and finished goods throughout North America every year. As the only railroad connecting Canada’s Eastern and Western coasts with the Southern tip of the U.S. through a 19,500 mile rail network, CN and its affiliates have been contributing to community prosperity and sustainable trade since 1919. CN is committed to programs supporting social responsibility and environmental stewardship. At CN, we work as ONE TEAM, focused on safety, sustainability and our customers, providing operational and supply chain excellence to deliver results.
For internal candidates, note that the grade level of the position will depend on the employee's experience.
CN is an employment equity employer and we encourage all qualified candidates to apply. We thank all applicants for their interest, however, only candidates under consideration will be contacted. Please monitor your email on a regular basis, as communication is primarily made through email.
Show more
Show less","Apache Spark, Agile, Azure Data Explorer, Azure Data Factory, Azure Data Lake, BigQuery, Bigtable, Cloud Composer, Cloud Data Fusion, Cloud Dataprep, Cloud Dataplex, Databricks, Databricks Delta Lake, Data Development, Data Governance, Data Integration, Data Modeling, Data Pipeline, Data Quality, Data Science, ETL, Google Cloud Platform, Informatica IICS, Java, Kafka, Message Queuing, Message Queuing Telemetry Transport, MQTT, NoSQL, PubSub, Python, Scala, SQL","apache spark, agile, azure data explorer, azure data factory, azure data lake, bigquery, bigtable, cloud composer, cloud data fusion, cloud dataprep, cloud dataplex, databricks, databricks delta lake, data development, data governance, data integration, data modeling, data pipeline, data quality, data science, etl, google cloud platform, informatica iics, java, kafka, message queuing, message queuing telemetry transport, mqtt, nosql, pubsub, python, scala, sql","agile, apache spark, azure data explorer, azure data factory, azure data lake, bigquery, bigtable, cloud composer, cloud data fusion, cloud dataplex, cloud dataprep, data development, data governance, data integration, data pipeline, data quality, data science, databricks, databricks delta lake, datamodeling, etl, google cloud platform, informatica iics, java, kafka, message queuing, message queuing telemetry transport, mqtt, nosql, pubsub, python, scala, sql"
Specialist Data Developer- EN,CN,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/specialist-data-developer-en-at-cn-3780032510,2023-12-17,Delta, Canada,Associate,Hybrid,"At CN, we work together to move our company—and North America—forward. Be part of our Information & Technology (I&T) team, a critical piece of the engine that keeps us in motion. From enterprise architecture to operational technology, our teams use the agile methodology to automate and digitize our railroad ensuring our operations run optimally and safely and our employees can focus on value-added tasks. You will be able to develop your skills and career in our close-knit, safety-focused culture working together as ONE TEAM. The careers we offer are meaningful because the work we do matters. Join us!
Job Summary
The Specialist, Data Developer is responsible for building, handling, and optimizing data pipelines. The role moves these data pipelines effectively into production for key data and analytics consumers, shapes the enterprise Data as a Service (DaaS) model and delivers on Information and Technology (I&T) business models. Moreover, the incumbent develops best practices and optimizes data pipelines to deliver products and services aligned with business expectations. The position plays a pivotal role in operationalizing data and analytics initiatives, defining and building CN’s data integration and DaaS platform roadmap.
Main Responsibilities
Data Development
Ensure optimal data delivery architecture and processes are consistent throughout ongoing projects
Optimize CN’s data architecture to support the next generation of products and data initiatives
Build, handle, and optimize data pipelines, moving them effectively into production for key data and analytics consumers
Build data and domain event models, implement business rules, and scalable data pipelines
Ensure compliance with data governance and security requirements while creating, improving, and operationalizing integrated and reusable data pipelines
Enable faster data access, integrate data reuse, and improve time-to-solution for data and analytics initiatives
Integrate analytics and data science results with business processes
Promote effective data management practices
Collaborate with Data Science, Reporting, Analytics and other Development teams to build data pipelines, infrastructure and tooling to support business initiatives
Design and develop Exact, Transfer and Load (ETL) pipelines using multiple sources of data in various formats and deploy them to achieve a high-level of reliability, scalability, and security
Data Design
Collaborate with stakeholders and architects to model data landscape and define secure data exchange approaches
Meet with stakeholders to identify fit-for-purpose within CN’s existing data ecosystem and deliver options and agile solutions
Design and develop processing pipelines that ingest data into Data Hubs
Provide day-to-day support and technical expertise to both technical and
non-technical teams
Participate in building data development expertise and framework
Translate business needs into technical requirements
Data Operations and Quality Assurance
Use Agile methodologies and development practices to streamline project delivery aligned with goals, timelines, and budgets and for code reviews and testing to develop and deliver data pipelines
Build monitoring and debugging tools to analyze data pipelines
Help unify software development and operations seamlessly, efficiently, and cost effectively
Improve software quality, automate processes, and accelerate software releases
Develop and implement test plans and scripts for various data quality processes
Maintain manual and automated test scripts
Working Conditions
Due to the nature of the role, the incumbent must be able to meet tight deadlines, handle pressure, and stress.
Requirements
Experience
Data Development
Minimum 5 years overall work experience
Minimum 3 years of experience in a Data Development role, working in different data management disciplines including data integration, modelling, optimization, and quality
Experience working in cross-functional teams and collaborating with business stakeholders in support of a departmental or multi-departmental data management and analytics initiative
Experience in translating business requirements into advanced data models able to fulfill Analysts and Data Scientists’ requirements
Experience working in an Agile team environment
Education/Certification/Designation
Bachelor's Degree in Computer Science, Electrical Development or Software Development
Google or Azure Data Development certification*
Any designation for these above would be considered as an asset
Competencies
Inspires others with impactful communications and adapts to the audience through speech and writing
Applies analytical thinking
Innovates through problem solving
Knows the business and stays current with industry trends to elevate expertise and work
Demonstrates organizational abilities
Collaborates with key internal stakeholders to enable higher productivity
Works independently with little supervision
Technical Skills/Knowledge
Knowledge of Scala, Java or Python
Knowledge of software development best practices such as code reviews, testing frameworks, maintainability, and readability
Expertise with Databricks Delta Lake
Knowledge of Structured Query Language (SQL) and Non-Structured Query Language (NoSQL) technologies and fluent in writing, executing, and optimizing SQL queries
Knowledge of Big Data technologies and cloud platforms such as Databricks, Apache Spark, Azure Data Factory, Azure Data Explorer, Azure Data Lake, Google BigQuery, Google Dataproc, Google Cloud Data Fusion, Google Dataflow, Google Cloud Composer, Google, Dataprep, Google Dataplex, Google BigLake, Google Vertex
Knowledge of event-driven architecture (e.g., Pub-Sub, Kafka, Message Queuing (MQ), Message Queuing Telemetry Transport (MQTT), Advanced Message Queuing Protocol (AMQP), Event Hub, Logstash)
About CN
CN is a world-class transportation leader and trade-enabler. Essential to the economy, to the customers, and to the communities it serves, CN safely transports more than 300 million tons of natural resources, manufactured products, and finished goods throughout North America every year. As the only railroad connecting Canada’s Eastern and Western coasts with the Southern tip of the U.S. through a 19,500 mile rail network, CN and its affiliates have been contributing to community prosperity and sustainable trade since 1919. CN is committed to programs supporting social responsibility and environmental stewardship. At CN, we work as ONE TEAM, focused on safety, sustainability and our customers, providing operational and supply chain excellence to deliver results.
For internal candidates, note that the grade level of the position will depend on the employee's experience.
CN is an employment equity employer and we encourage all qualified candidates to apply. We thank all applicants for their interest, however, only candidates under consideration will be contacted. Please monitor your email on a regular basis, as communication is primarily made through email.
Show more
Show less","Data Development, Data Design, Data Operations, Quality Assurance, Scala, Java, Python, Databricks Delta Lake, SQL, NoSQL, Big Data, Databricks, Apache Spark, Azure Data Factory, Azure Data Explorer, Azure Data Lake, Google BigQuery, Google Dataproc, Google Cloud Data Fusion, Google Dataflow, Google Cloud Composer, Google Dataprep, Google Dataplex, Google BigLake, Google Vertex, PubSub, Kafka, Message Queuing (MQ), Message Queuing Telemetry Transport (MQTT), Advanced Message Queuing Protocol (AMQP), Event Hub, Logstash","data development, data design, data operations, quality assurance, scala, java, python, databricks delta lake, sql, nosql, big data, databricks, apache spark, azure data factory, azure data explorer, azure data lake, google bigquery, google dataproc, google cloud data fusion, google dataflow, google cloud composer, google dataprep, google dataplex, google biglake, google vertex, pubsub, kafka, message queuing mq, message queuing telemetry transport mqtt, advanced message queuing protocol amqp, event hub, logstash","advanced message queuing protocol amqp, apache spark, azure data explorer, azure data factory, azure data lake, big data, data design, data development, data operations, databricks, databricks delta lake, event hub, google biglake, google bigquery, google cloud composer, google cloud data fusion, google dataflow, google dataplex, google dataprep, google dataproc, google vertex, java, kafka, logstash, message queuing mq, message queuing telemetry transport mqtt, nosql, pubsub, python, quality assurance, scala, sql"
GCP Data Engineering Specialist,CGI,"Greater Toronto Area, Canada",https://ca.linkedin.com/jobs/view/gcp-data-engineering-specialist-at-cgi-3749587967,2023-12-17,Delta, Canada,Associate,Hybrid,"We are seeking a GCP Data Engineer who possesses strong expertise in both on-premises and GCP environments. The ideal candidate should have practical hands-on experience in the field of Data Engineering. This role encompasses a combination of responsibilities, including on-premises Hadoop support, data migration, and active involvement in the development of GCP features. Prior experience in on-premises Hadoop support is a valuable asset.
Location: Toronto, or Montreal
Role type: Hybrid (up to 3 days a week at client site)
The ET (Emerging Technologies) team in Global Technology and Operations (GTO) Canada at CGI is a trusted Data, Cloud and AI advisor and go-to implementation partner for our global clients' Data and Advanced Analytics needs. We're an entrepreneurial team that is on a continuous mission to position CGI as the best-in-class AI partner and develop new and exciting opportunities in latest technologies including Generative AI. In our endeavor to provide end-to-end AI capabilities, we start with learning about the clients` business and data landscape, assess possible opportunities, help shortlist and turn those opportunities into action, and then scale it across the organization unlocking a larger impact for clients and their consumers. For this, we depend on a talented and motivated team of Data Architects, Data Scientists, Data Engineers, Cloud DevOps professionals, Cloud Architects and other solutions experts.
Qualifications:
Minimum of 3-5 years hands-on Data engineering experience with GCP ( Google Cloud Platform)
Experience with other relevant DevOps and data engineering technologies is a plus such as Kubernetes, Kafka, Spark and Hadoop
Strong problem-solving skills and the ability to work in a fast-paced, collaborative environment.
Excellent communication and teamwork skills to interact effectively with cross-functional teams and stakeholders.
Core Skills:
Apache Airflow
Apache Spark
Hadoop
Kubernetes
Python
Amazon S3/Minio
Jupyter Hub/Notebook
Docker
GCP (Data Analytics/Related services)
Show more
Show less","Data Engineering, GCP (Google Cloud Platform), Hadoop, Kubernetes, Kafka, Spark, Apache Airflow, Python, Amazon S3/Minio, Jupyter Hub/Notebook, Docker, Data Analytics","data engineering, gcp google cloud platform, hadoop, kubernetes, kafka, spark, apache airflow, python, amazon s3minio, jupyter hubnotebook, docker, data analytics","amazon s3minio, apache airflow, data engineering, dataanalytics, docker, gcp google cloud platform, hadoop, jupyter hubnotebook, kafka, kubernetes, python, spark"
"Specialist, Data Developer- EN",CN,"Montreal, Quebec, Canada",https://ca.linkedin.com/jobs/view/specialist-data-developer-en-at-cn-3774809340,2023-12-17,Delta, Canada,Associate,Hybrid,"At CN, we work together to move our company—and North America—forward. Be part of our Information & Technology (I&T) team, a critical piece of the engine that keeps us in motion. From enterprise architecture to operational technology, our teams use the agile methodology to automate and digitize our railroad ensuring our operations run optimally and safely and our employees can focus on value-added tasks. You will be able to develop your skills and career in our close-knit, safety-focused culture working together as ONE TEAM. The careers we offer are meaningful because the work we do matters. Join us!
Job Summary
The Specialist, Data Developer is responsible for building, handling, and optimizing data pipelines. The role moves them effectively into production for key data and analytics consumers, shapes the enterprise Data as a Service (DaaS) model and delivers on Information and Technology (I&T) business models. Moreover, the incumbent develops best practices and optimizes data pipelines to deliver products and services aligned with business expectations. The position plays a pivotal role in operationalizing data and analytics initiatives, defining and building CN’s data integration and DaaS platform roadmap.
Main Responsibilities
Data Development
Ensure optimal data delivery architecture and processes are consistent throughout ongoing projects
Optimize CN’s data architecture to support the next generation of products and data initiatives
Build, handle, and optimize data pipelines, moving them effectively into production for key data and analytics consumers
Build data and domain event models, implement business rules, and scalable data pipelines
Ensure compliance with data governance and security requirements while creating, improving, and operationalizing integrated and reusable data pipelines
Enable faster data access, integrate data reuse, and improve time-to-solution for data and analytics initiatives
Integrate analytics and data science results with business processes
Promote effective data management practices
Collaborate with Data Science, Reporting, Analytics and other Development teams to build data pipelines, infrastructure and tooling to support business initiatives
Design and develop Exact, Transfer and Load (ETL) pipelines using multiple sources of data in various formats and deploy them to achieve a high-level of reliability, scalability, and security
Data Design
Collaborate with stakeholders and architects to model data landscape and define secure data exchange approaches
Meet with stakeholders to identify fit-for-purpose within CN’s existing data ecosystem and deliver options and agile solutions
Design and develop processing pipelines that ingest data into Data Hubs
Provide day-to-day support and technical expertise to both technical and
non-technical teams
Participate in building data development expertise and framework
Translate business needs into technical requirements
Data Operations and Quality Assurance
Use Agile methodologies and development practices to streamline project delivery aligned with goals, timelines, and budgets and for code reviews and testing to develop and deliver data pipelines
Build monitoring and debugging tools to analyze data pipelines
Help unify software development and operations seamlessly, efficiently, and cost effectively
Improve software quality, automate processes, and accelerate software releases
Develop and implement test plans and scripts for various data quality processes
Maintain manual and automated test scripts
Working Conditions
Due to the nature of the role, the incumbent must be able to meet tight deadlines, handle pressure, and stress.
Requirements
Experience
Data Development
Minimum 5 years overall work experience
Minimum 3 years of experience in a Data Development role, working in different data management disciplines including data integration, modelling, optimization, and quality
Ø Experience working in cross-functional teams and collaborating with business stakeholders in support of a departmental or multi-departmental data management and analytics initiative
Ø Experience in translating business requirements into advanced data models able to fulfill Analysts and Data Scientists’ requirements
Ø Experience working in an Agile team environment
Experience with Informatica IICS (Informatica Intelligent Cloud Services)
Experience with Enterprise Data Catalog
Education/Certification/Designation
Bachelor's Degree in Computer Science, Electrical Development or Software Development
Google or Azure Data Development certification*
Any designation for these above would be considered as an asset
Competencies
Inspires others with impactful communications and adapts to the audience through speech and writing
Applies analytical thinking
Innovates through problem solving
Knows the business and stays current with industry trends to elevate expertise and work
Demonstrates organizational abilities
Collaborates with key internal stakeholders to enable higher productivity
Works independently with little supervision
Technical Skills/Knowledge
Knowledge of Scala, Java or Python
Knowledge of software development best practices such as code reviews, testing frameworks, maintainability, and readability
Expertise with Databricks Delta Lake
Knowledge of Structured Query Language (SQL) and Non-Structured Query Language (NoSQL) technologies and fluent in writing, executing, and optimizing SQL queries
Knowledge of Big Data technologies and cloud platforms such as Databricks, Apache Spark, Azure Data Factory, Azure Data Explorer, Azure Data Lake, Google BigQuery, Google Dataproc, Google Cloud Data Fusion, Google Dataflow, Google Cloud Composer, Google, Dataprep, Google Dataplex, Google BigLake, Google Vertex
Knowledge of event-driven architecture (e.g., Pub-Sub, Kafka, Message Queuing (MQ), Message Queuing Telemetry Transport (MQTT), Advanced Message Queuing Protocol (AMQP), Event Hub, Logstash)
About CN
CN is a world-class transportation leader and trade-enabler. Essential to the economy, to the customers, and to the communities it serves, CN safely transports more than 300 million tons of natural resources, manufactured products, and finished goods throughout North America every year. As the only railroad connecting Canada’s Eastern and Western coasts with the Southern tip of the U.S. through a 19,500 mile rail network, CN and its affiliates have been contributing to community prosperity and sustainable trade since 1919. CN is committed to programs supporting social responsibility and environmental stewardship. At CN, we work as ONE TEAM, focused on safety, sustainability and our customers, providing operational and supply chain excellence to deliver results.
For internal candidates, note that the grade level of the position will depend on the employee's experience.
CN is an employment equity employer and we encourage all qualified candidates to apply. We thank all applicants for their interest, however, only candidates under consideration will be contacted. Please monitor your email on a regular basis, as communication is primarily made through email.
Show more
Show less","Data Development, Data Pipelines, Data Integration, Data Modeling, Data Optimization, Data Quality, Data Governance, Data Security, Data Architecture, Data Engineering, Business Intelligence, Data Analytics, Machine Learning, Software Development, Cloud Computing, Big Data, Agile Methodology, Informatica IICS, Enterprise Data Catalog, Scala, Java, Python, Databricks Delta Lake, SQL, NoSQL, Databricks, Apache Spark, Azure Data Factory, Azure Data Explorer, Azure Data Lake, Google BigQuery, Google Dataproc, Google Cloud Data Fusion, Google Dataflow, Google Cloud Composer, Google Dataprep, Google Dataplex, Google BigLake, Google Vertex, PubSub, Kafka, Message Queuing, Message Queuing Telemetry Transport, Advanced Message Queuing Protocol, Event Hub, Logstash","data development, data pipelines, data integration, data modeling, data optimization, data quality, data governance, data security, data architecture, data engineering, business intelligence, data analytics, machine learning, software development, cloud computing, big data, agile methodology, informatica iics, enterprise data catalog, scala, java, python, databricks delta lake, sql, nosql, databricks, apache spark, azure data factory, azure data explorer, azure data lake, google bigquery, google dataproc, google cloud data fusion, google dataflow, google cloud composer, google dataprep, google dataplex, google biglake, google vertex, pubsub, kafka, message queuing, message queuing telemetry transport, advanced message queuing protocol, event hub, logstash","advanced message queuing protocol, agile methodology, apache spark, azure data explorer, azure data factory, azure data lake, big data, business intelligence, cloud computing, data architecture, data development, data engineering, data governance, data integration, data optimization, data quality, data security, dataanalytics, databricks, databricks delta lake, datamodeling, datapipeline, enterprise data catalog, event hub, google biglake, google bigquery, google cloud composer, google cloud data fusion, google dataflow, google dataplex, google dataprep, google dataproc, google vertex, informatica iics, java, kafka, logstash, machine learning, message queuing, message queuing telemetry transport, nosql, pubsub, python, scala, software development, sql"
GCP Data Engineering Specialist,CGI,Greater Montreal Metropolitan Area,https://ca.linkedin.com/jobs/view/gcp-data-engineering-specialist-at-cgi-3772780763,2023-12-17,Delta, Canada,Associate,Hybrid,"We are seeking a GCP Data Engineer who possesses strong expertise in both on-premises and GCP environments. The ideal candidate should have practical hands-on experience in the field of Data Engineering. This role encompasses a combination of responsibilities, including on-premises Hadoop support, data migration, and active involvement in the development of GCP features. Prior experience in on-premises Hadoop support is a valuable asset.
Location: Toronto, or Montreal
Role type: Hybrid (up to 3 days a week at client site)
The ET (Emerging Technologies) team in Global Technology and Operations (GTO) Canada at CGI is a trusted Data, Cloud and AI advisor and go-to implementation partner for our global clients' Data and Advanced Analytics needs. We're an entrepreneurial team that is on a continuous mission to position CGI as the best-in-class AI partner and develop new and exciting opportunities in latest technologies including Generative AI. In our endeavor to provide end-to-end AI capabilities, we start with learning about the clients` business and data landscape, assess possible opportunities, help shortlist and turn those opportunities into action, and then scale it across the organization unlocking a larger impact for clients and their consumers. For this, we depend on a talented and motivated team of Data Architects, Data Scientists, Data Engineers, Cloud DevOps professionals, Cloud Architects and other solutions experts.
Qualifications:
Minimum of 3-5 years hands-on Data engineering experience with GCP ( Google Cloud Platform)
Experience with other relevant DevOps and data engineering technologies is a plus such as Kubernetes, Kafka, Spark and Hadoop
Strong problem-solving skills and the ability to work in a fast-paced, collaborative environment.
Excellent communication and teamwork skills to interact effectively with cross-functional teams and stakeholders.
Core Skills:
Google Cloud Platform
Kubernetes
Kafka
Spark
Hadoop
Show more
Show less","Data engineering, Google Cloud Platform, Kubernetes, Kafka, Spark, Hadoop, Data migration, DevOps, Cloud computing, Big data, Data architecture, Data analytics, Generative AI","data engineering, google cloud platform, kubernetes, kafka, spark, hadoop, data migration, devops, cloud computing, big data, data architecture, data analytics, generative ai","big data, cloud computing, data architecture, data engineering, data migration, dataanalytics, devops, generative ai, google cloud platform, hadoop, kafka, kubernetes, spark"
Specialist Data Developer- EN,CN,"Montreal, Quebec, Canada",https://ca.linkedin.com/jobs/view/specialist-data-developer-en-at-cn-3780034427,2023-12-17,Delta, Canada,Associate,Hybrid,"At CN, we work together to move our company—and North America—forward. Be part of our Information & Technology (I&T) team, a critical piece of the engine that keeps us in motion. From enterprise architecture to operational technology, our teams use the agile methodology to automate and digitize our railroad ensuring our operations run optimally and safely and our employees can focus on value-added tasks. You will be able to develop your skills and career in our close-knit, safety-focused culture working together as ONE TEAM. The careers we offer are meaningful because the work we do matters. Join us!
Job Summary
The Specialist, Data Developer is responsible for building, handling, and optimizing data pipelines. The role moves these data pipelines effectively into production for key data and analytics consumers, shapes the enterprise Data as a Service (DaaS) model and delivers on Information and Technology (I&T) business models. Moreover, the incumbent develops best practices and optimizes data pipelines to deliver products and services aligned with business expectations. The position plays a pivotal role in operationalizing data and analytics initiatives, defining and building CN’s data integration and DaaS platform roadmap.
Main Responsibilities
Data Development
Ensure optimal data delivery architecture and processes are consistent throughout ongoing projects
Optimize CN’s data architecture to support the next generation of products and data initiatives
Build, handle, and optimize data pipelines, moving them effectively into production for key data and analytics consumers
Build data and domain event models, implement business rules, and scalable data pipelines
Ensure compliance with data governance and security requirements while creating, improving, and operationalizing integrated and reusable data pipelines
Enable faster data access, integrate data reuse, and improve time-to-solution for data and analytics initiatives
Integrate analytics and data science results with business processes
Promote effective data management practices
Collaborate with Data Science, Reporting, Analytics and other Development teams to build data pipelines, infrastructure and tooling to support business initiatives
Design and develop Exact, Transfer and Load (ETL) pipelines using multiple sources of data in various formats and deploy them to achieve a high-level of reliability, scalability, and security
Data Design
Collaborate with stakeholders and architects to model data landscape and define secure data exchange approaches
Meet with stakeholders to identify fit-for-purpose within CN’s existing data ecosystem and deliver options and agile solutions
Design and develop processing pipelines that ingest data into Data Hubs
Provide day-to-day support and technical expertise to both technical and
non-technical teams
Participate in building data development expertise and framework
Translate business needs into technical requirements
Data Operations and Quality Assurance
Use Agile methodologies and development practices to streamline project delivery aligned with goals, timelines, and budgets and for code reviews and testing to develop and deliver data pipelines
Build monitoring and debugging tools to analyze data pipelines
Help unify software development and operations seamlessly, efficiently, and cost effectively
Improve software quality, automate processes, and accelerate software releases
Develop and implement test plans and scripts for various data quality processes
Maintain manual and automated test scripts
Working Conditions
Due to the nature of the role, the incumbent must be able to meet tight deadlines, handle pressure, and stress.
Requirements
Experience
Data Development
Minimum 5 years overall work experience
Minimum 3 years of experience in a Data Development role, working in different data management disciplines including data integration, modelling, optimization, and quality
Experience working in cross-functional teams and collaborating with business stakeholders in support of a departmental or multi-departmental data management and analytics initiative
Experience in translating business requirements into advanced data models able to fulfill Analysts and Data Scientists’ requirements
Experience working in an Agile team environment
Education/Certification/Designation
Bachelor's Degree in Computer Science, Electrical Development or Software Development
Google or Azure Data Development certification*
Any designation for these above would be considered as an asset
Competencies
Inspires others with impactful communications and adapts to the audience through speech and writing
Applies analytical thinking
Innovates through problem solving
Knows the business and stays current with industry trends to elevate expertise and work
Demonstrates organizational abilities
Collaborates with key internal stakeholders to enable higher productivity
Works independently with little supervision
Technical Skills/Knowledge
Knowledge of Scala, Java or Python
Knowledge of software development best practices such as code reviews, testing frameworks, maintainability, and readability
Expertise with Databricks Delta Lake
Knowledge of Structured Query Language (SQL) and Non-Structured Query Language (NoSQL) technologies and fluent in writing, executing, and optimizing SQL queries
Knowledge of Big Data technologies and cloud platforms such as Databricks, Apache Spark, Azure Data Factory, Azure Data Explorer, Azure Data Lake, Google BigQuery, Google Dataproc, Google Cloud Data Fusion, Google Dataflow, Google Cloud Composer, Google, Dataprep, Google Dataplex, Google BigLake, Google Vertex
Knowledge of event-driven architecture (e.g., Pub-Sub, Kafka, Message Queuing (MQ), Message Queuing Telemetry Transport (MQTT), Advanced Message Queuing Protocol (AMQP), Event Hub, Logstash)
About CN
CN is a world-class transportation leader and trade-enabler. Essential to the economy, to the customers, and to the communities it serves, CN safely transports more than 300 million tons of natural resources, manufactured products, and finished goods throughout North America every year. As the only railroad connecting Canada’s Eastern and Western coasts with the Southern tip of the U.S. through a 19,500 mile rail network, CN and its affiliates have been contributing to community prosperity and sustainable trade since 1919. CN is committed to programs supporting social responsibility and environmental stewardship. At CN, we work as ONE TEAM, focused on safety, sustainability and our customers, providing operational and supply chain excellence to deliver results.
For internal candidates, note that the grade level of the position will depend on the employee's experience.
CN is an employment equity employer and we encourage all qualified candidates to apply. We thank all applicants for their interest, however, only candidates under consideration will be contacted. Please monitor your email on a regular basis, as communication is primarily made through email.
Show more
Show less","Apache Spark, Scala, Java, Python, Databricks Delta Lake, Structured Query Language (SQL), NonStructured Query Language (NoSQL), Big Data, Databricks, Azure Data Factory, Azure Data Explorer, Azure Data Lake, Google BigQuery, Google Dataproc, Google Cloud Data Fusion, Google Dataflow, Google Cloud Composer, Google Dataprep, Google Dataplex, Google BigLake, Google Vertex, Eventdriven architecture, PubSub, Kafka, Message Queuing (MQ), Message Queuing Telemetry Transport (MQTT), Advanced Message Queuing Protocol (AMQP), Event Hub, Logstash","apache spark, scala, java, python, databricks delta lake, structured query language sql, nonstructured query language nosql, big data, databricks, azure data factory, azure data explorer, azure data lake, google bigquery, google dataproc, google cloud data fusion, google dataflow, google cloud composer, google dataprep, google dataplex, google biglake, google vertex, eventdriven architecture, pubsub, kafka, message queuing mq, message queuing telemetry transport mqtt, advanced message queuing protocol amqp, event hub, logstash","advanced message queuing protocol amqp, apache spark, azure data explorer, azure data factory, azure data lake, big data, databricks, databricks delta lake, event hub, eventdriven architecture, google biglake, google bigquery, google cloud composer, google cloud data fusion, google dataflow, google dataplex, google dataprep, google dataproc, google vertex, java, kafka, logstash, message queuing mq, message queuing telemetry transport mqtt, nonstructured query language nosql, pubsub, python, scala, structured query language sql"
Data Engineer,Wrmth,"North Bay, Ontario, Canada",https://ca.linkedin.com/jobs/view/data-engineer-at-wrmth-3787778051,2023-12-17,Delta, Canada,Mid senior,Onsite,"Wrmth Corp. is
seeking a skilled Data Engineer to play a pivotal role in our technology development project that involves an IoT device, mobile app, and back-end database. The Data Engineer will be responsible for designing, developing, and maintaining the data architecture, pipelines, and processes to ensure efficient and secure data collection, storage, and analysis. This role requires expertise in data integration, transformation, and visualization, as well as a strong understanding of data privacy and compliance.
Responsibilities:
Design, develop, and maintain data pipelines for collecting and processing usage data from IoT devices and mobile apps.
Collaborate with cross-functional teams to ensure seamless data flow between hardware, software, and databases.
Implement data anonymization and encryption techniques to ensure data privacy and security.
Optimize data storage and retrieval for efficient analysis and reporting.
Work with data scientists and analysts to create meaningful insights from collected data.
Monitor data quality, troubleshoot data-related issues, and ensure data accuracy.
Stay updated with industry trends and best practices in data engineering and data management.
Qualifications:
Bachelor's degree in Computer Science, Engineering, or a related field.
Proven experience in designing and implementing data pipelines and ETL processes.
Proficiency in programming languages such as Python, Java, or Scala.
Familiarity with data modeling, database systems, and cloud-based data solutions.
Strong understanding of data privacy regulations and compliance.
Excellent problem-solving and communication skills.
Experience with IoT data management is a plus.
Wrmth is an onshoring manufacturing company building systems that are committed to changing outdoor living. Software, electronic design, next generation manufacturing, 3D printing and material science meet cutting edge industrial design to create our green energy powered, intelligent outdoor furniture. Wrmth leads the way in this new category with design forward technology integrations informed by the uniquely Canadian, intimate relationship with outdoor entertainment. Every Wrmth product is designed and engineered in Canada, to ensure premium quality and craftsmanship. Join our dynamic team and contribute your expertise in embedded systems to create cutting-edge IoT-enabled furniture that revolutionizes user experience and data insights. This is an exciting opportunity to work on a project that combines hardware, software, and innovation to bring value to our users.
Want to work in a great small city with no traffic, easy access to the great outdoors where you can enjoy a collaborative and creative working environment, then this position is for you! Please apply with your resume and a cover letter explaining why you would be a great fit for this position.
Applications will be received until Dec. 23, 2023
Powered by JazzHR
XtwVlMKyQk
Show more
Show less","Data Engineering, Data Pipelines, Data Modeling, Data Analysis, Data Visualization, ETL, Python, Java, Scala, Data Privacy, Data Security, Data Quality, Data Compliance, Database Systems, Cloud Computing, IoT Data Management, Embedded Systems, Hardware, Software, Innovation","data engineering, data pipelines, data modeling, data analysis, data visualization, etl, python, java, scala, data privacy, data security, data quality, data compliance, database systems, cloud computing, iot data management, embedded systems, hardware, software, innovation","cloud computing, data compliance, data engineering, data privacy, data quality, data security, dataanalytics, database systems, datamodeling, datapipeline, embedded systems, etl, hardware, innovation, iot data management, java, python, scala, software, visualization"
Data Engineer,MayStreet (acquired by LSEG),"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/data-engineer-at-maystreet-acquired-by-lseg-3787730441,2023-12-17,Delta, Canada,Mid senior,Onsite,"We’re building the next generation of capital markets data technology. Global capital markets are an ocean of fast-moving, interrelated and complex data. We are looking for passionate and enthusiastic problem-solvers with expertise in capital markets data to join us as we continue providing technology innovation and support to the world’s leading global capital markets companies.
Current strategic pursuits
The Data Engineer will be responsible for data processing pipelines. We have the mission of ""Capturing, Storing and Transforming"" the world's market data. Our data team manages the data pipelines that flow between each of those stages. The role has a large operational component while also requiring someone to envision future growth in both utilization and technology selection for the components of the work.
Work Qualifications
Significant experience with Python and ability to write and maintain object-oriented Python applications is a must
Experience with processing job management – experience using one or more of the following: Jenkins, Rundeck, Apache Airflow or other task scheduling software
Time spent working in a batch data processing environment or other environment where large data jobs are performed using automation
Experience with ETL/ELT pipelines and Data Lakes
Experience with any Git version control systems like GitLab or GitHub
Proficiency with Linux and an understanding of Linux server architecture and operation
Should be very comfortable with data copying, scheduling tasks and Linux daemons
Nice To Haves, But Not Required
Experience with AWS data warehouse, Redshift, Athena, Dremio or Snowflake
Experience with and knowledge of US and International market data
At the London Stock Exchange Group, we believe that creating a diverse and inclusive organization is fundamental to the way we deliver on our promise of creating essential partnerships and open opportunities. Our open culture is central to how we deliver our purpose – driving financial stability, empowering economies and enabling customers to create sustainable growth – in everything we do.
Working with us means that you will be part of a dynamic organization of 25,000 people across 70 countries. However, we will value your individuality and enable you to bring your true self to work so you can help enrich our diverse workforce. You will be part of a collaborative and creative culture where we encourage new ideas and are committed to sustainability across our global business. You will experience the critical role we have in helping to re-engineer the financial ecosystem to support and drive sustainable economic growth. Together, we are aiming to achieve this growth by accelerating the just transition to net zero, enabling growth of the green economy and creating inclusive economic opportunity.
LSEG offers a range of tailored benefits and support, including healthcare, retirement planning, paid volunteering days and wellbeing initiatives.
We are proud to be an equal opportunities employer. This means that we do not discriminate on the basis of anyone’s race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital status, veteran status, pregnancy or disability, or any other basis protected under applicable law. Conforming with applicable law, we can reasonably accommodate applicants' and employees' religious practices and beliefs, as well as any mental health or physical disability needs.
Please take a moment to read this
Powered by JazzHR
ZJWktRgG8M
Show more
Show less","Python, Objectoriented programming, Data pipelines, Jenkins, Rundeck, Apache Airflow, Task scheduling software, Batch data processing, ETL/ELT pipelines, Data Lakes, Git version control systems, Linux, Linux server architecture, Linux daemons, Data copying, Scheduling tasks, AWS data warehouse, Redshift, Athena, Dremio, Snowflake, US and International market data","python, objectoriented programming, data pipelines, jenkins, rundeck, apache airflow, task scheduling software, batch data processing, etlelt pipelines, data lakes, git version control systems, linux, linux server architecture, linux daemons, data copying, scheduling tasks, aws data warehouse, redshift, athena, dremio, snowflake, us and international market data","apache airflow, athena, aws data warehouse, batch data processing, data copying, data lakes, datapipeline, dremio, etlelt pipelines, git version control systems, jenkins, linux, linux daemons, linux server architecture, objectoriented programming, python, redshift, rundeck, scheduling tasks, snowflake, task scheduling software, us and international market data"
Digital Data Engineer,Experfy,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/digital-data-engineer-at-experfy-3738075778,2023-12-17,Delta, Canada,Mid senior,Onsite,"Our client is looking for:
Work with business teams to understand requirements, and translate them into technical needs
Gather/organize large & complex data assets, and perform relevant analysis
Ensure the quality of the data in coordination with Data Analysts and Data Scientists (peer validation)
Propose and implement relevant data models for each business case
Create data models and optimize queries performance
Communicate results and findings in a structured way
Partner with Product Owner and Data Analysts to prioritize the pipeline implementation plan
Partner with Data Analysts and Data scientists to design pipelines relevant for business requirements
Leverage existing or create new ""standard data pipelines"" within the company to bring value through business use cases
Ensure best practices in data manipulation are enforced end-to-end
Actively contribute to Data governance community
Remains up to date on company's standards, industry practices and emerging technologies
Requirements
Key Functional Requirements & Qualifications:
Experience working with a variety of cross-functional teams
Good understanding of agile/scrum development processes and concepts
Able to work in a fast-paced, constantly evolving environment and manage multiple priorities
Pragmatic and capable of solving complex issues
Service-oriented, flexible team player
Attention to detail & technical intuition
Excellent written, verbal, and interpersonal skills for executive level communication and collaboration
Fluent in English (Other languages a plus)
Key Technical Requirements & Qualifications:
Bachelor's Degree or equivalent in in Computer Science, Engineering, or relevant field
Experience with AWS cloud services (Azure & GCP a plus)
Good knowledge of SQL and relational databases technologies/concepts
Experience working with data models and query tuning
Experience in Data warehousing solutions (Snowflake a plus)
Experience in Integration Services (IICS, Tibco a plus)
Working knowledge of scripting languages (Python, R a plus)
Familiarity with Source Code Management Tools (GitHub a plus)
Familiarity with Visualization Tools (PowerBI, Tableau a plus)
Familiarity with Project Management Tools (JIRA, Confluence a plus)
Familiarity with Service Management Tools (Service Now a plus)
Experience working in life sciences/pharmaceutical industry is a plus
Relevant cloud certifications (AWS, Azure, Snowflake, IICS) are a plus
Experience on working within compliance (e.g.: quality, regulatory - data privacy, GxP, SOX) and cybersecurity requirements is a plus
Mentoring and/or technology evangelism/advocacy experience
Additions For Data Engineer - Database
Strong experience in automation tools and methodologies specifically using Gitlab, Github action, Terraform, Ansible
Experience with programming languages such as Python, JSON, YAML, Shell Scripting.
Experience with backup system like Netbackup & CommVault
Good knowledge of ServiceNow and monitoring tool such as Splunk, BPPM
Additions for Data Engineer - RWE
Experience with Real World Data (e,g, EHR, Claims) and standard data models (e,g, OMOP, FHIR)
Experience using frameworks to create pipelines (e.g. Apache Airflow, Kedro)
Show more
Show less","Data engineering, Agile development, AWS cloud services, SQL, Data models, Query tuning, Data warehousing, Snowflake, Integration services, Python, R, Git, Visualization tools, PowerBI, Tableau, Project management tools, JIRA, Confluence, Service management tools, Service Now, Compliance, Cybersecurity, Automation tools, Gitlab, Github action, Terraform, Ansible, JSON, YAML, Shell Scripting, Netbackup, CommVault, Splunk, BPPM, Real World Data, OMOP, FHIR, Apache Airflow, Kedro","data engineering, agile development, aws cloud services, sql, data models, query tuning, data warehousing, snowflake, integration services, python, r, git, visualization tools, powerbi, tableau, project management tools, jira, confluence, service management tools, service now, compliance, cybersecurity, automation tools, gitlab, github action, terraform, ansible, json, yaml, shell scripting, netbackup, commvault, splunk, bppm, real world data, omop, fhir, apache airflow, kedro","agile development, ansible, apache airflow, automation tools, aws cloud services, bppm, commvault, compliance, confluence, cybersecurity, data engineering, data models, datawarehouse, fhir, git, github action, gitlab, integration services, jira, json, kedro, netbackup, omop, powerbi, project management tools, python, query tuning, r, real world data, service management tools, service now, shell scripting, snowflake, splunk, sql, tableau, terraform, visualization tools, yaml"
Data Engineer,STACK IT Recruitment,"Mississauga, Ontario, Canada",https://ca.linkedin.com/jobs/view/data-engineer-at-stack-it-recruitment-3780792548,2023-12-17,Delta, Canada,Mid senior,Onsite,"Our client in Mississauga is looking for a
Data Engineer
to join their team!
Work Type
MANDATORY: In-Office (5 days a week)
Must have valid driver’s license and access to a vehicle to travel onsite daily
Office Location: Mississauga
Company Overview
Our client is seeking a highly skilled and motivated Data Engineer to develop and manage data pipelines, warehouses, and analytics platforms, ensuring robust and efficient data operations. You will be empowered to lead autonomously and excel in your expertise, creating innovative solutions and addressing complex data challenges.
You will work in an environment that nurtures continuous learning and growth while delivering exceptional value to clients.
Responsibilities
Collaborate with cross-functional teams to understand data requirements and design data solutions.
Develop, construct, test, and maintain scalable data architectures and pipelines.
Build efficient, reliable, and high-performance data pipelines for data modeling, mining, and production purposes.
Design and build scalable data warehouses from the ground up, ensuring optimal performance, reliability, and data integrity.
Design, implement, and maintain data pipelines to extract, load, and transform (ELT) data from various sources into a data lake.
Troubleshoot data-related problems, ensuring data quality and reliability.
Design and deploy data schemas, data structures, and data storage solutions.
Conduct in-depth data analysis to identify trends, patterns, and anomalies, and translate findings into meaningful business insights.
Design, develop, and implement analytical solutions and interactive dashboards to enable stakeholders to effectively understand and interpret data.
Evaluate internal systems for efficiency, problems, and inaccuracies, developing and maintaining protocols for handling, processing, and cleaning data
Develop, implement, and maintain robust business models and predictive analytics solutions to aid decision-making and drive business performance.
Skills And Qualifications
Bachelor’s degree in Computer Science, Mathematics or a related field.
4+ years of experience using a variety of data tools to build data warehouses and pipelines from scratch.
Strong understanding of data modeling, ETL development, and data architecture principles.
Proficiency in Data warehouse tools such as; Snowflake, Matillion, Stitch, or Fivetran
Amazon Redshift, Azure Databricks, or equivalent technologies is an asset
Experience with DBT (or other equivalent technology) for data transformations
Experience with data warehousing/ETL processes and database management systems such as PostGres, MYSQL, and SQL.
Proficiency in data visualization tools, particularly Power BI and Tableau.
Strong Experience in extracting, preparing, and analyzing large data sets (>100 million records)
Strong programming skills in languages such as Python
Strong proficiency with in Excel (advanced functions, pivot tables, etc.)
Salary, Benefits & Perks
$80,000 - $100,000
Annual performance bonus
Paid-Time Off
Comprehensive health benefits + RRSP
Show more
Show less","Data Engineering, Data Warehousing, Data Pipelines, ETL Development, Data Modeling, Data Architecture, Snowflake, Matillion, Stitch, Fivetran, Data Visualization, Power BI, Tableau, Python, PostGres, MYSQL, SQL","data engineering, data warehousing, data pipelines, etl development, data modeling, data architecture, snowflake, matillion, stitch, fivetran, data visualization, power bi, tableau, python, postgres, mysql, sql","data architecture, data engineering, datamodeling, datapipeline, datawarehouse, etl development, fivetran, matillion, mysql, postgres, powerbi, python, snowflake, sql, stitch, tableau, visualization"
Big DATA Developer-Canada,Zortech Solutions,"Mississauga, Ontario, Canada",https://ca.linkedin.com/jobs/view/big-data-developer-canada-at-zortech-solutions-3672849081,2023-12-17,Delta, Canada,Mid senior,Onsite,"Role: Big DATA Developer
Location: Mississauga, ON/Canada-Onsite from Day one
Duration: Full time
Job Description
Technical & Other Skills:
7+ years of working experience in scalable data pipeline using Scala, Python, pyspark, Hadoop, Apache Spark, Spark SQL, Kafka, Nifi, ETL and Incremental Data Load with Big data technologies.
Experience of working with Databases like Oracle, Netezza and have strong SQL knowledge
Developing scalable streaming solutions based on Hadoop/ Big Data stack such as HDFS, Ozone, Hive/ Impala, Apache Spark and Kafka.
Experience in Data Engineering and implementing multiple end-to-end DW projects in Big Data environment
Experience of building data pipelines through Git, Shell, Spark with Java/Python on Hadoop or Object storage (S3/CEPH)
Strong analytical skills required for debugging production issues, providing root cause and implementing mitigation plan
Strong communication skills - both verbal and written and strong relationship, collaboration skills and organizational skills
Flexible enough to stretch and work in India time to coordinate with India team
Experiences
Nice to have development experience in Python based AI/ML workloads on Hadoop.
Experience of working on Nifi, APIs and Unix Environment will be an added advantage
Experience of working in Agile teams
Ability to multi-task across multiple projects, interface with external / internal resources and provide technical leadership to junior team members
Ability to be high-energy, detail-oriented, proactive and able to function under pressure in an independent environment along with a high degree of initiative and self-motivation to drive results
Ability to quickly learn and implement new technologies, and perform POC to explore best solution for the problem statement
Contribute ideas to help ensure that required standards and processes are in place and actively look for opportunities to enhance standards and improve process efficiency
Perform assigned tasks and production incident independently
Proven track record of delivering and willingness to roll up sleeves to get the job done.
Show more
Show less","Scala, Python, Pyspark, Hadoop, Apache Spark, Spark SQL, Kafka, Nifi, ETL, Incremental Data Load, Oracle, Netezza, SQL, HDFS, Ozone, Hive, Impala, Git, Shell, Java, S3, CEPH, Unix, Agile, POC","scala, python, pyspark, hadoop, apache spark, spark sql, kafka, nifi, etl, incremental data load, oracle, netezza, sql, hdfs, ozone, hive, impala, git, shell, java, s3, ceph, unix, agile, poc","agile, apache spark, ceph, etl, git, hadoop, hdfs, hive, impala, incremental data load, java, kafka, netezza, nifi, oracle, ozone, poc, python, s3, scala, shell, spark, spark sql, sql, unix"
Sr. Data Engineer,Lovelytics,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/sr-data-engineer-at-lovelytics-3787786996,2023-12-17,Delta, Canada,Mid senior,Onsite,"Lovelytics is seeking a Sr. Data Engineer (Lead Consultant) with experience delivering strategic Databricks client engagements to join our Data & AI practice!
This Lead Consultant will play a key role in delivering on client engagements related to data warehousing, ETL development, data integrations, and data modeling. This is a client-facing role, focused on using and migrating to our partner technologies; Databricks, AWS, and Azure to name a few. In addition to the technical capabilities for this role, we are looking for someone who wants to work in a collaborative, dynamic, and inclusive environment and has a passion for bringing meaning to data.
Role Location: Ontario, Canada with preference for candidates near one of our HQ office (Toronto, ON)
This role is not open for sponsorship at this time.
Primary Responsibilities:
Utilize consulting and technical skills to be able to work in a client-facing project environment independently.
Be responsible for your own execution and sometimes lead individual work streams on client engagements as assigned and under supervision of engagement lead.
Collaborate with other team members to successfully deliver on projects.
Work effectively and directly communicate with both internal and client and/or partner teams.
Develop full ownership of your execution on client engagements, you'll become involved in the project planning and solution stages of engagements as well.
Design and implement complex ETL/ELT pipelines with evidence of improved data processing times.
Successfully lead small data warehousing projects with measurable performance enhancements under the management of an engagement lead.
Contribute to real-time data processing solutions and managed streaming data.
Implement security and compliance measures for data pipelines.
Design and implement version control and branching strategies and integrate them into CI/CD for promoting and testing in higher environments.
Our Ideal Candidate's Skills and Experiences:
B.S. in Computer Science or equivalent
3-5 years' experience in data engineering and big data. At least 2 years working directly with clients and external stakeholders.
Extensive knowledge of data warehousing concepts and hands-on experience deploying pipelines using Databricks and/or Spark
Data modeling and database design skills and knowledge of version control
Excellent verbal and written communication skills
Is able to apply technical skills to engagement needs
Works with engagement leads and directors to gain exposure in the design and architecture of solutions
Understands and utilizes Lovelytics tools and client tools
What We Promise You:
Exciting projects with great clients in varying departments and verticals across the world
The ability to work closely with experienced data engineers and quickly grow and expand your skillset
The ability to work closely with all sizes of companies, ranging from Fortune 100 to small local businesses
A workplace where you are encouraged to challenge the status quo and develop new technologies, methodologies, and processes
A diverse team consisting of data gurus, experience seekers, and entrepreneurial minds that are always pushing to be better
Lovelytics is an Equal Opportunity Employer. This means you don’t have to worry about whether your application process will be fair. We consider all applicants without regard to race, color, religion, age, ancestry, ethnicity, gender, gender identity, gender expression, sexual orientation, veteran status, or disability.
Powered by JazzHR
l00ikPX87t
Show more
Show less","Databricks, AWS, Azure, ETL, Data warehousing, Data integrations, Data modeling, Data processing, Data pipelines, Security, Compliance, Version control, Branching strategies, CI/CD, Computer Science, Data engineering, Big data, Spark, Data modeling, Database design, Verbal communication, Written communication, Technical skills","databricks, aws, azure, etl, data warehousing, data integrations, data modeling, data processing, data pipelines, security, compliance, version control, branching strategies, cicd, computer science, data engineering, big data, spark, data modeling, database design, verbal communication, written communication, technical skills","aws, azure, big data, branching strategies, cicd, compliance, computer science, data engineering, data integrations, data processing, database design, databricks, datamodeling, datapipeline, datawarehouse, etl, security, spark, technical skills, verbal communication, version control, written communication"
Senior Azure Data Engineer,Zortech Solutions,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/senior-azure-data-engineer-at-zortech-solutions-3710656090,2023-12-17,Delta, Canada,Mid senior,Onsite,"Job Title: Senior Azure Data Engineer
Duration: Fulltime
Location: Toronto, ON
Skills And Responsibilities
Lead group of Data engineers on Sprint tasks, planning and troubleshooting issues
Good Knowledge and hands on experience in Azure Databricks, Data Factory, Synapse Analytics or SQL DB, ADLS Gen 2
Implement ETL/ELT flows via Big Data Solutions i.e., Spark, Python, Azure Data Factory, Azure Synapse, Azure, Databricks, etc.
Good knowledge in Data warehousing concepts
Good knowledge of SQL queries and concepts
Knowledge in any Python or object-oriented programming language
Knowledge in any Change Data Capture (CDC) solutions
Experience in working on Agile projects is an advantage.
Show more
Show less","Azure Databricks, Data Factory, Synapse Analytics, SQL DB, ADLS Gen 2, Spark, Python, Azure Data Factory, Azure Synapse, Azure Databricks, Data warehousing, SQL queries, Python, Objectoriented programming, Change Data Capture (CDC), Agile","azure databricks, data factory, synapse analytics, sql db, adls gen 2, spark, python, azure data factory, azure synapse, azure databricks, data warehousing, sql queries, python, objectoriented programming, change data capture cdc, agile","adls gen 2, agile, azure data factory, azure databricks, azure synapse, change data capture cdc, data factory, datawarehouse, objectoriented programming, python, spark, sql db, sql queries, synapse analytics"
Senior Data Engineer,Lyft,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/senior-data-engineer-at-lyft-3778296188,2023-12-17,Delta, Canada,Mid senior,Onsite,"TBS - Data Engineering
At Lyft, our mission is to improve people's lives with the world's best transportation. To do this, we start with our own community by creating an open, inclusive, and diverse organization.
Here at Lyft, Data is the only way we make decisions. It is the core of our business, helping us create a transportation experience for our customers and providing insights into the effectiveness of our product launch & features.
This team focuses on supporting our business by building the data transport, collection, and storage that powers our Transit, Bikes, and Scooters business. We are looking for a Data Engineer to build scalable solutions, leveraging their data expertise and our technology stack to provide timely, accurate data for our internal and external customers. You will have the opportunity to be a member of a new, growing part of our micro mobility platform. This role will involve collaborating with product managers, external stakeholders, GMs, engineers, and data scientists to gather and translate requirements into solutions, ensuring that data-driven decisions are at the core of our business.
This role reports to the Engineering Manager.
Responsibilities:
Owner of core company data pipelines, responsible for scaling up data processing flow to meet the rapid data growth at Lyft
Evolve data model and data schema based on business and engineering needs
Implement systems tracking data quality and consistency
Develop tools supporting self-service data pipeline management (ETL)
SQL and MapReduce job tuning to improve data processing performance
Write well-crafted, well-tested, readable, maintainable code
Participate in code reviews to ensure code quality and distribute knowledge
Unblock, support and communicate with internal & external partners to achieve results
Experience:
5+ years of relevant professional experience
Experience with Hadoop (or similar) Ecosystem (MapReduce, Yarn, HDFS, Hive, Spark, Presto, Pig, HBase, Parquet)
Proficient in at least one of the SQL languages (MySQL, PostgreSQL, SqlServer, Oracle)
Good understanding of SQL Engine and able to conduct advanced performance tuning
Strong skills in scripting language (Python, Ruby, Bash)
2+ years of experience with workflow management tools (Airflow, Oozie, Azkaban, UC4)
Comfortable working directly with data analytics to bridge Lyft's business goals with data engineering
Benefits:
Extended health and dental coverage options, along with life insurance and disability benefits
Mental health benefits
In addition to provincial observed holidays, team members get 15 days paid time off, with an additional day for each year of service
4 Floating Holidays each calendar year prorated based off of date of hire
10 paid sick days per year regardless of province
Family building benefits
Access to a Health Care Savings Account
18 weeks of paid parental leave. Biological, adoptive, and foster parents are all eligible
Lyft proudly pursues and hires a diverse workforce. Lyft believes that every person has a right to equal employment opportunities without discrimination because of race, ancestry, place of origin, colour, ethnic origin, citizenship, creed, sex, sexual orientation, gender identity, gender expression, age, marital status, family status, disability, pardoned record of offences, or any other basis protected by applicable law or by Company policy. Lyft also strives for a healthy and safe workplace and strictly prohibits harassment of any kind. Accommodation for persons with disabilities will be provided upon request in accordance with applicable law during the application and hiring process. Please contact your recruiter now if you wish to make such a request.
Starting in September 2023, this role will be in-office on a hybrid schedule — Team Members will be expected to work in the office 3 days per week on Mondays, Thursdays and a team-specific third day. Additionally, hybrid roles have the flexibility to work from anywhere for up to 4 weeks per year.
Show more
Show less","Hadoop Ecosystem, MapReduce, YARN, HDFS, Apache Hive, Apache Spark, Apache Presto, Pig, HBase, Parquet, SQL, MySQL, PostgreSQL, SqlServer, Oracle, Python, Ruby, Bash, Apache Airflow, Apache Oozie, Azkaban, UC4","hadoop ecosystem, mapreduce, yarn, hdfs, apache hive, apache spark, apache presto, pig, hbase, parquet, sql, mysql, postgresql, sqlserver, oracle, python, ruby, bash, apache airflow, apache oozie, azkaban, uc4","apache airflow, apache hive, apache oozie, apache presto, apache spark, azkaban, bash, hadoop ecosystem, hbase, hdfs, mapreduce, mysql, oracle, parquet, pig, postgresql, python, ruby, sql, sqlserver, uc4, yarn"
Senior Data Engineer,Synechron,"Montreal, Quebec, Canada",https://ca.linkedin.com/jobs/view/senior-data-engineer-at-synechron-3783157457,2023-12-17,Delta, Canada,Mid senior,Onsite,"Responsibilities:
Strong understanding or Snowflake on Azure Architecture, design, implementation and operationalization of large-scale data and analytics solutions on Snowflake Cloud Data Warehouse.
Hands-on development experience with Snowflake features such as Snow SQL; Snow Pipe; Python; Tasks; Streams; Time travel; Zero Copy Cloning; Optimizer; Metadata Manager; data sharing; and stored procedures.
Experience in Data warehousing - OLTP, OLAP, Dimensions, Facts, and Data modeling.
Need to have working knowledge of MS Azure configuration items with respect to Snowflake.
Developing EL pipelines in and out of data warehouse using combination of Data bricks, Python and Snow SQL.
Developing scripts UNIX, Python etc. to Extract, Load and Transform data, as well as other utility functions.
Provide production support for Data Warehouse issues such data load problems, transformation translation problems
Requirements:
You are:
Minimum 7 years of designing and implementing an operational production grade large-scale data solution on Microsoft Azure Snowflake Data Warehouse.
Including hands on experience with productionized data ingestion and processing pipelines using Python, Data bricks, Snow SQL
Excellent understanding of Snowflake Internals and integration of Snowflake with other data processing and reporting technologies
It would be great if you also had:
Detail-oriented, ability to turn deliverables around quickly with a high degree of accuracy
Strong analytical skills, ability to interpret business requirements and produce functional and technical design documents
Good time management skills – Ability to prioritize and multi-task, handling multiple efforts at once
Strong desire to understand and learn domain.
Experience in a financial services/banking industry
Ability to work in a fast paced environment; to be flexible and learn quickly.
Ability to multi-task with attention to detail/ prioritize tasks.
We can offer you:
A highly competitive compensation and benefits package
A multinational organization with 44 offices in 19 countries and the possibility to work abroad
15 days of paid annual leave (plus national holidays)
Maternity & Paternity leave plans
A comprehensive insurance plan including: medical, dental, vision, life insurance, and long-/short-term disability
RRSP with employer’s contribution
A higher education certification policy
Comprehensive Relocation Expense Coverage
Extensive training opportunities, focused on skills, substantive knowledge, and personal development
On-demand Udemy for Business for all Synechron employees with free access to more than 5000 curated courses
Coaching opportunities with experienced colleagues from our Financial Innovation Labs (FinLabs) and Center of Excellences (CoE) groups
Cutting edge projects at the world’s leading tier-one banks, financial institutions and insurance firms
A flat and approachable organization
A truly diverse, fun-loving and global work culture
SYNECHRON’S DIVERSITY & INCLUSION STATEMENT
Diversity & Inclusion are fundamental to our culture, and Synechron is proud to be an equal opportunity workplace and is an affirmative action employer. Our Diversity, Equity, and Inclusion (DEI) initiative ‘Same Difference’ is committed to fostering an inclusive culture – promoting equality, diversity and an environment that is respectful to all. We strongly believe that a diverse workforce helps build stronger, successful businesses as a global company. We encourage applicants from across diverse backgrounds, race, ethnicities, religion, age, marital status, gender, sexual orientations, or disabilities to apply. We empower our global workforce by offering flexible workplace arrangements, mentoring, internal mobility, learning and development programs, and more.
All employment decisions at Synechron are based on business needs, job requirements and individual qualifications, without regard to the applicant’s gender, gender identity, sexual orientation, race, ethnicity, disabled or veteran status, or any other characteristic protected by law.
Le rôle : Développeur Intégrateur Snowflake
Responsabilités :
Solide compréhension de Snowflake sur Azure. Architecture, conception, mise en œuvre et opérationnalisation de solutions de données et d'analyse à grande échelle sur Snowflake Cloud Data Warehouse.
Expérience pratique du développement avec les fonctionnalités de Snowflake telles que Snow SQL, Snow Pipe, Python, Tâches, Flux, Time Travel, Clonage Zero Copy, Optimiseur, Gestionnaire de métadonnées, partage de données et procédures stockées.
Expérience en entreposage de données - OLTP, OLAP, Dimensions, Faits et modélisation des données.
Il est nécessaire d'avoir une connaissance pratique des éléments de configuration de MS Azure par rapport à Snowflake.
Développement de pipelines EL entrants et sortants de l'entrepôt de données en utilisant une combinaison de Data bricks, Python et Snow SQL.
Développement de scripts UNIX, Python, etc. pour extraire, charger et transformer les données, ainsi que d'autres fonctions utilitaires.
Fournir un support de production pour les problèmes d'entrepôt de données tels que les problèmes de chargement de données, les problèmes de traduction de transformation.
Comprendre les pipelines de données et les moyens modernes d'automatiser les pipelines de données à l'aide de tests basés sur le cloud et documenter clairement les mises en œuvre, de manière à ce que d'autres puissent facilement comprendre les exigences, la mise en œuvre et les conditions de test.
Exigences :
Vous êtes :
Au minimum 7 ans d'expérience dans la conception et la mise en œuvre d'une solution de données à grande échelle en production opérationnelle sur Microsoft Azure Snowflake Data Warehouse.
Expérience pratique de l'ingestion et du traitement de données en production à l'aide de Python, Data bricks, Snow SQL.
Excellentes compétences en présentation et en communication, à la fois écrites et verbales, capacité à résoudre des problèmes et à concevoir dans un environnement aux exigences peu claires.
Ce serait génial si vous aviez également :
Souci du détail, capacité à produire rapidement des livrables avec un haut degré de précision.
Solides compétences analytiques, capacité à interpréter les besoins commerciaux et à produire des documents de conception fonctionnelle et technique.
Bonnes compétences en gestion du temps - Capacité à hiérarchiser et à effectuer plusieurs tâches en même temps.
Fort désir de comprendre et d'apprendre le domaine.
Expérience dans le secteur des services financiers/bancaires.
Nous pouvons vous offrir :
Un package de rémunération et d'avantages sociaux très compétitif.
Une organisation multinationale avec 44 bureaux dans 19 pays et la possibilité de travailler à l'étranger.
15 jours de congé annuel payé (plus les jours fériés nationaux).
Plans de congé de maternité et de paternité.
Un plan d'assurance complet comprenant : assurance maladie, dentaire, vision, assurance vie, invalidité à long terme/court terme.
REER avec contribution de l'employeur.
Une politique de certification de l'enseignement supérieur.
Une couverture complète des frais de déménagement.
De nombreuses opportunités de formation axées sur les compétences, les connaissances substantielles et le développement personnel.
Udemy à la demande pour les entreprises pour tous les employés de Synechron, avec un accès gratuit à plus de 5000 cours sélectionnés.
Possibilités de coaching avec des collègues expérimentés de nos laboratoires d'innovation financière (FinLabs) et de nos groupes de centres d'excellence (CoE).
Projets de pointe auprès des principales banques de niveau un, des institutions financières et des compagnies d'assurance au monde.
Une organisation accessible et conviviale.
Une culture de travail vraiment diversifiée, amusante et mondiale.
DÉCLARATION DE DIVERSITÉ ET D'INCLUSION DE SYNECHRON
La diversité et l'inclusion sont fondamentales pour notre culture, et Synechron est fier d'être un lieu de travail égalitaire et un employeur pratiquant l'action positive. Notre initiative de diversité, d'équité et d'inclusion (DEI) «‘Same Difference’» s'engage à favoriser une culture inclusive - promouvoir l'égalité, la diversité et un environnement respectueux envers tous. Nous croyons fermement qu'une main-d'œuvre diversifiée contribue à renforcer les entreprises avec succès en tant qu'entreprise mondiale. Nous encourageons les candidats de divers horizons, qu'il s'agisse de race, d'origine ethnique, de religion, d'âge, de statut matrimonial, de genre, d'orientation sexuelle ou de handicap, à postuler. Nous autonomisons notre main-d'œuvre mondiale en proposant des arrangements de travail flexibles, du mentorat, une mobilité interne, des programmes d'apprentissage et de développement, et bien plus encore.
Toutes les décisions d'emploi chez Synechron sont basées sur les besoins de l'entreprise, les exigences du poste et les qualifications individuelles, sans tenir compte du genre, de l'identité de genre, de l'orientation sexuelle, de la race, de l'origine ethnique, du handicap ou du statut de vétéran du candidat, ou de toute autre caractéristique protégée par la loi.
Show more
Show less","Snowflake, Azure, Snow SQL, Snow Pipe, Python, Tasks, Streams, Time travel, Zero Copy Cloning, Optimizer, Metadata Manager, data sharing, stored procedures, Data warehousing, OLTP, OLAP, Dimensions, Facts, Data modeling, MS Azure, Data bricks, UNIX, EL pipelines, Data ingestion, Data processing, Data pipelines, Cloudbased testing, Technical design documents, Functional design documents","snowflake, azure, snow sql, snow pipe, python, tasks, streams, time travel, zero copy cloning, optimizer, metadata manager, data sharing, stored procedures, data warehousing, oltp, olap, dimensions, facts, data modeling, ms azure, data bricks, unix, el pipelines, data ingestion, data processing, data pipelines, cloudbased testing, technical design documents, functional design documents","azure, cloudbased testing, data bricks, data ingestion, data processing, data sharing, datamodeling, datapipeline, datawarehouse, dimensions, el pipelines, facts, functional design documents, metadata manager, ms azure, olap, oltp, optimizer, python, snow pipe, snow sql, snowflake, stored procedures, streams, tasks, technical design documents, time travel, unix, zero copy cloning"
Database Engineer,Soho Square Solutions,"Montreal, Quebec, Canada",https://ca.linkedin.com/jobs/view/database-engineer-at-soho-square-solutions-3693328702,2023-12-17,Delta, Canada,Mid senior,Onsite,"Description
Job Title : Database Engineer
Duration: 12 Months (Contract)
Location; Montreal ( Office attendance from day one - Hybrid Mode 3x per week)
As a Database Engineer you will be a member of the LCG Technology Controls Management development team, with specific focus on sourcing data and developing data solutions for Technology Risk functions within Morgan Stanley. In this role you will be primarily responsible for the development of data workflows, views, and stored procedures, in addition to performing data analysis, and monitoring and tuning queries and data loads. You will be working closely with data providers, data scientists, data developers, and data analytics teams to facilitate the implementation of client-specific business requirements and requests.
Your Key Responsibilities
To develop stored procedures, triggers, and views on our DB2-based Data Warehouse
To perform data profiling and analysis on source system data to ensure that source system data can be integrated and represented properly in our models
To monitor the performance of queries and data loads and perform tuning as necessary
To provide assistance and guidance during the QA & UAT phases to quickly confirm the validity of potential issues and to determine the root cause and best resolution of verified issues
Skills / Qualifications
Bachelor's degree in Computer Science, Software Engineering, Information Technology, or related field required
At least 7+ years of experience in data development and solutions in highly complex data environments with large data volumes
At least 5+ years of experience with the development of data workflows in Informatica and/or Talend
At least 5+ years of SQL / PLSQL experience with the ability to write ad-hoc and complex queries and with developing complex stored procedures, triggers, MQTs and views on IBM DB2 (experience with v10.5 a plus)
At least 5+ years of experience developing complex stored procedures, triggers, MQTs and views on IBM DB2 (experience with v10.5 a plus)
Experience with developing scripts and utilities in Perl and Python
Experience with Autosys and/or Airflow
Experience with Hadoop a plus
Strong analytical skills, including a thorough understanding of how to interpret customer business requirements and translate them into technical designs and solutions
Experience with both Waterfall and Agile development methodologies
Strong communication skills both verbal and written. Capable of collaborating effectively across a variety of IT and Business groups, across regions, roles and able to interact effectively with all levels.
Self starter. Proven ability to manage multiple, concurrent projects with minimal supervision. Can manage a complex ever changing priority list and resolve conflicts to competing priorities.
Strong problem solving skills. Ability to identify where focus is needed and bring clarity to business objectives, requirements and priorities.
Additional Details
Education Level : Bachelor's Degree
Experience Level : Level 4
If Pre-Identified, please provide Candidate First Name and Last Name : (No Value)
Show more
Show less","Database Engineer, Data Warehouse, Informatica, Talend, IBM DB2, SQL, PLSQL, Perl, Python, Autosys, Airflow, Hadoop, Agile, Waterfall, Communication Skills, Analytical Skills, Problem Solving Skills, Multitasking Skills","database engineer, data warehouse, informatica, talend, ibm db2, sql, plsql, perl, python, autosys, airflow, hadoop, agile, waterfall, communication skills, analytical skills, problem solving skills, multitasking skills","agile, airflow, analytical skills, autosys, communication skills, database engineer, datawarehouse, hadoop, ibm db2, informatica, multitasking skills, perl, plsql, problem solving skills, python, sql, talend, waterfall"
Senior Data Engineer,Psycho Bunny,"Montreal, Quebec, Canada",https://ca.linkedin.com/jobs/view/senior-data-engineer-at-psycho-bunny-3728652907,2023-12-17,Delta, Canada,Mid senior,Onsite,"Can you envision your future accomplishments as a
SENIOR DATA ENGINEER
at
PSYCHO BUNNY
?
WHO WE ARE ?
Psycho Bunny is a worldwide menswear brand that had one mission when it was founded in New York back in 2005: to perfect the polo. Nearly two decades later, our quality-obsessed approach has built a devoted following of fashion-conscious individuals looking for exceptional-quality everyday staples that command attention—while remaining unconventional and daring. Today, color-driven collections extend to Pima cotton T-shirts, plush French terry sweats, versatile bottoms, elevated outerwear, need-to-have accessories, and a myriad of perfected polos. are reimagined for each drop, of which there are approximately 13 every year. Our global expansion in the fashion landscape is attributed to the eclectic personalities who have joined our ranks, making Psycho Bunny the creative community it is today—and will be tomorrow.
WHY JOIN THE TEAM ?
This isn’t your typical workplace. Your voice will be heard. You’ll matter. And you’ll be instrumental in our unique brand’s ascent to new heights. Our rapid global expansion requires fluidity in an ever-changing high-performance environment. You’ll have the opportunity to learn, grow, and constantly expand your skill set. We’re looking for forward-thinkers who want to take ownership and drive change. As the company forges to scale exception growth, you’re presented with the opportunity to get involved and make a meaningful impact. If all this sounds appealing, we’d love to meet you!
WHAT IS THE ROLE ?
Reporting to the Director, Data and Analytics, the Senior Data Engineer will be responsible for creating and managing data pipelines and for ensuring the highest level of data quality and integrity for business leaders at Psycho Bunny. Contribute to the business’ growth and development by providing strategic and mission critical data to give the organization a competitive edge. The Senior Data Engineer will provide mentorship and guidance to the data scientists and other data/analytics engineers, promoting best practices in data management and pipeline construction.
HOW WILL YOU DO IT?
Data Pipeline Management
Partner with the business unit leaders to understand all data needs and requirements.
Design, develop, and maintain reliable data pipelines that efficiently process large volumes of data according to evolving business needs.
Implement systems and practices to ensure data is accessible and usable for business intelligence tools, data analytics teams, and other stakeholders.
Manage loading and transformation of data through both technical processes and business logic.
Produce strategic data that adds value and contributes to the organization’s growth and competitiveness.
Data Quality & Integrity
Establish and enforce data quality standards, methodologies, and systems to ensure data accuracy and reliability.
Monitor data ingestion and processing, resolving any discrepancies and ensuring smooth data flows.
Collaborate with data source providers and internal stakeholders to address data quality issues.
Catalog and document the data sources needed to implement self-service analytics.
Continually improve ongoing reporting and analysis processes and practices.
Technical Collaboration & Guidance
Be the reference, subject matter expert and owner for the data pipelines at Psycho Bunny.
Provide mentorship and guidance to junior data/analytics engineers, promoting best practices in data management and pipeline construction.
Introduce novelty and leading edge technologies and practices to keep with trends and ensure efficiency and optimal impact.
WHO YOU ARE ?
Your achievements
Bachelor's or Master's degree in Computer Science, Data Science, Engineering, or a related field preferred.
7+ years of hands-on experience in data engineering with a proven track record in designing scalable, resilient data pipelines.
5+ years of experience with SQL, preferably for data transformation or analytical use cases.
Your contribution
Solid experience with Snowflake Cloud Data Platform (SnowPro Core Certification a bonus) or other cloud data warehouses (Azure Synapse, Amazon Redshift, Google BigQuery).
Experience with dbt (core and/or Cloud) and Fivetran.
Experience with Informatica Cloud a bonus.
Mastery of data warehouse methodologies and techniques from transactional databases to dimensional data modeling, to wide denormalized data marts.
Strong problem-solving skills and the ability to handle complex data challenges.
Startup mentality with strong ownership to solve problems with minimal guidance and being comfortable with ambiguities.
Detail - oriented and committed to excellence.
Bring improvement and new ideas to current processes.
Strong collaborator and listener to understand and intake business needs.
Be proactive and alert to changing needs.
Add value to the business by providing strategic data for optimal decision making.
What We Offer
You’ll have an entire team in your corner, ensuring that you are seen, heard, respected, and treated the way you want to be treated. That includes access to our employee benefits:
Group Insurance coverage, including health, dental, vision, Life, and long-term disability.
Psycho Bunny offers all Canadian employees the possibility to participate in a group RRSP/DPSP plan which includes a very generous match from Psycho Bunny!
An Employee /Family Assistance Program (EFAP).
Casual dress code.
Company events and social hours.
On-site gym.
On-site cafeteria/bistro with subsidized meals, breakfast, and lunch.
Three (3) weeks of vacation.
Six (6) wellness days.
On-site parking, including electric car chargers.
Your birthday off on us.
You can take advantage of increased work flexibility and our 40 days a year ""Work from anywhere"" policy!
Last but not least, let us not forget the swag you can get with an amazing employee discount!
We thank all applicants; however, only those selected for an interview will be contacted.
Show more
Show less","Data Engineering, Data Pipelines, Data Quality, Data Integrity, SQL, Snowflake Cloud Data Platform, Cloud Data Warehouses, dbt, Fivetran, Informatica Cloud, Data Warehouse Modeling, Dimensional Data Modeling, Wide Denormalized Data Marts","data engineering, data pipelines, data quality, data integrity, sql, snowflake cloud data platform, cloud data warehouses, dbt, fivetran, informatica cloud, data warehouse modeling, dimensional data modeling, wide denormalized data marts","cloud data warehouses, data engineering, data integrity, data quality, data warehouse modeling, datapipeline, dbt, dimensional data modeling, fivetran, informatica cloud, snowflake cloud data platform, sql, wide denormalized data marts"
Sr Data Engineer,KASAPUR IT SERVICES INC,"Montreal, Quebec, Canada",https://ca.linkedin.com/jobs/view/sr-data-engineer-at-kasapur-it-services-inc-3740507906,2023-12-17,Delta, Canada,Mid senior,Onsite,"Job Description
Business Background
Position is responsible for maintaining, distributing, and reporting on risk data (Market Risk, Credit Risk, Operational Risk, Liquidity Risk). As the authoritative source of key data sets, we are at the forefront of database technology and are heavily involved in data engineering, data science, data visualization, and machine learning efforts across the Firm.
Position Introduction
This is a data engineer role in the team responsible for developing on the Firm's Risk data stores that holds the transactional bigdata for real time and archive processing and getting it into the archives and data lake.
The global team consists of highly technical team members who are adaptable to both hands-on development and project management. We deliver multiple projects for multiple business areas in parallel. The business owners and subject matter experts will be globally distributed, making communication and pro-active to be important. You will be expected to work closely with our operations partners on requirements for projects.
The development will be performed using an agile methodology which is based on scrum (time boxing, daily scrum meetings, retrospectives, etc.) and XP (continuous integration, refactoring, unit testing, etc.) best practices. Candidates must therefore be able to work collaboratively, demonstrate good ownership and be able to work well in teams.
Primary Responsibilities Include
Translate business requirement into queries against a set of relational tables and produce reporting based on the requirements.
Design and build reporting layer from different data sources and act as a SPOC for user queries
Database and ETL development, including stored procedures, queries, performance tuning etc., using python, SQL and ETL tools such as Informatica.
Efficient and clean automation script (using Python etc.) as part of the ETL process.
The current global team members are all very skilled in domain modeling, database design, big data, Java and messaging so this is an excellent opportunity to play a key role in the growing team.
Technical Skills Requirement
Strong relational database skills especially with DB2 or/and Greenplum.
Knowledge of Hadoop/Spark/Snowflake is desirable.
Create high quality and optimized stored procedures and queries.
Experience with Power Designer or some similar modeling tool.
Strong with scripting language such as Python and Unix / K-Shell.
Strong knowledge base of relational database performance and tuning such as: proper use of indices, database statistics/reorgs, de-normalization concepts.
Experience with data mining is a big plus.
Familiar with lifecycle of a trade and flows of data in an investment banking operation.
Experienced in Agile development process.
Show more
Show less","Data Engineering, Data Science, Data Visualization, Machine Learning, Database Design, Big Data, Java, Messaging, Relational Database, DB2, Greenplum, Hadoop, Spark, Snowflake, Python, SQL, Informatica, Stored Procedures, ETL, Unix, KShell, Power Designer, Data Mining, Agile","data engineering, data science, data visualization, machine learning, database design, big data, java, messaging, relational database, db2, greenplum, hadoop, spark, snowflake, python, sql, informatica, stored procedures, etl, unix, kshell, power designer, data mining, agile","agile, big data, data engineering, data mining, data science, database design, db2, etl, greenplum, hadoop, informatica, java, kshell, machine learning, messaging, power designer, python, relational database, snowflake, spark, sql, stored procedures, unix, visualization"
Senior Data Engineer- Snowflake,Tiger Analytics,"Montreal, Quebec, Canada",https://ca.linkedin.com/jobs/view/senior-data-engineer-snowflake-at-tiger-analytics-3732384916,2023-12-17,Delta, Canada,Mid senior,Onsite,"Tiger Analytics is a fast-growing advanced analytics consulting firm. Our consultants bring deep expertise in Data Science, Machine Learning and AI. We are the trusted analytics partner for several Fortune 100 companies, enabling them to generate business value from data. Our business value and leadership has been recognized by various market research firms, including Forrester and Gartner. We are looking for top-notch talent as we continue to build the best analytics global consulting team in the world.
We are seeking an experienced Sr. Data Engineer with expertise in Snowflake to join our data team. As a Data Engineer, you will be responsible for designing, building, and maintaining data pipelines, data integration processes, and data infrastructure using Cloud Snowflake DBT. You will collaborate closely with data scientists, analysts, and other stakeholders to ensure efficient data flow and support data-driven decision making across the organization.
Requirements
9+ years of overall industry experience specifically in data engineering
5+ years of experience building and deploying large-scale data processing pipelines in a production environment
Understanding of Datawarehouse (DWH) systems, and migration from DWH to data lakes/Snowflake
Solid experience with Snowflake Cloud Data Platform (SnowPro Core Certification is a bonus) or other cloud data warehouses (AWS Services)
Experience with dbt (core and/or Cloud) and Fivetran
Experience with Informatica Cloud is a bonus
Strong problem-solving skills and the ability to handle complex data challenges
Build processes supporting data transformation, data structures, metadata, dependency and workload management
Benefits
Significant career development opportunities exist as the company grows. The position offers a unique opportunity to be part of a small, challenging, and entrepreneurial environment, with a high degree of individual responsibility.
Show more
Show less","Data Science, Machine Learning, Artificial Intelligence, Snowflake, Cloud Computing, DBT, Data Engineering, Data Integration, Data Infrastructure, Data Pipelines, Data Warehouses, Data Lakes, DWH, AWS, Fivetran, Informatica Cloud, Data Transformation, Data Structures, Metadata, Dependency Management, Workload Management, ProblemSolving, Data Challenges","data science, machine learning, artificial intelligence, snowflake, cloud computing, dbt, data engineering, data integration, data infrastructure, data pipelines, data warehouses, data lakes, dwh, aws, fivetran, informatica cloud, data transformation, data structures, metadata, dependency management, workload management, problemsolving, data challenges","artificial intelligence, aws, cloud computing, data challenges, data engineering, data infrastructure, data integration, data lakes, data science, data structures, data transformation, data warehouses, datapipeline, dbt, dependency management, dwh, fivetran, informatica cloud, machine learning, metadata, problemsolving, snowflake, workload management"
Data Analyst,Stripe,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/data-analyst-at-stripe-3756988360,2023-12-17,Delta, Canada,Mid senior,Onsite,"About Stripe
Stripe is a financial infrastructure platform for businesses. Millions of companies—from the world’s largest enterprises to the most ambitious startups—use Stripe to accept payments, grow their revenue, and accelerate new business opportunities. Our mission is to increase the GDP of the internet, and we have a staggering amount of work ahead. That means you have an unprecedented opportunity to put the global economy within everyone’s reach while doing the most important work of your career.
About The Team
Data Science at Stripe is a vibrant community where data analysts, data scientists and engineers learn and grow together. You will work with some of Stripe’s most fundamental and exciting data, and use that data to help drive company-wide initiatives. We have a variety of Data Analytics roles and teams across Stripe and will seek to align you to the most relevant team based on your background.
What you'll do
In this role, you will partner deeply with teams across Stripe to ensure that our users, our products, and our business have the models, data products, and insights needed to make decisions and grow responsibly. You will work closely with partners to extract insights from Stripe's rich and complex data. You will also work with leaders to translate business needs into data problems. You will build metrics, scalable data pipelines, dashboards, and reports to inform and run the business. You will deliver actionable business recommendations through analyses and data storytelling.
Who you are
We’re looking for someone who meets the minimum requirements to be considered for the role. If you meet these requirements, you are encouraged to apply. The preferred qualifications are a bonus, not a requirement.
Minimum Requirements
2-8+ years experience in Business Intelligence Engineering, Data Engineering, Data Analysis or Data Science roles, building data pipelines and analyzing large datasets to solve problems
Proficiency in SQL and Python
Strong statistical knowledge
Expertise in visualization and using data insights to make recommendations and achieve goals
Proven ability to manage and deliver on multiple projects with great attention to detail
Ability to clearly communicate results and drive impact
Comfortable collaborating across functions to identify data analytics problems and execute solutions with technical rigor and data-driven insights.
Preferred Qualifications
Master’s degree in Mathematics, Statistics, Economics, Engineering, or a related technical field.
Prior experience at a growth stage internet or software company.
Experience with distributed data frameworks like Hadoop and Spark to write and debug data pipelines.
Good understanding of development processes and best practices like engineering standards, code reviews, and testing.
Pay and benefits
The annual salary range for this role in the primary location is C$128,700 - C$174,100. This range may change if you are hired in another location. For sales roles, the range provided is the role’s On Target Earnings (“OTE”) range, meaning that the range includes both the sales commissions/sales bonuses target and annual base salary for the role. This salary range may be inclusive of several career levels at Stripe and will be narrowed during the interview process based on a number of factors, including the candidate’s experience, qualifications, and specific location. Applicants interested in this role and who are not located in the primary location may request the annual salary range for their location during the interview process.
Specific benefits and details about what compensation is included in the salary range listed above will vary depending on the applicant’s location and can be discussed in more detail during the interview process. Benefits/additional compensation for this role may include: equity, company bonus or sales commissions/bonuses; retirement plans; health benefits; and wellness stipends.
Show more
Show less","Data Science, Data Analytics, Data Engineering, SQL, Python, Statistics, Data Visualization, Data Storytelling, Hadoop, Spark, Engineering Standards, Code Reviews, Testing, Mathematics, Statistics, Economics, Engineering","data science, data analytics, data engineering, sql, python, statistics, data visualization, data storytelling, hadoop, spark, engineering standards, code reviews, testing, mathematics, statistics, economics, engineering","code reviews, data engineering, data science, data storytelling, dataanalytics, economics, engineering, engineering standards, hadoop, mathematics, python, spark, sql, statistics, testing, visualization"
Senior Data Engineer,FutureFit AI,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/senior-data-engineer-at-futurefit-ai-3741582582,2023-12-17,Delta, Canada,Mid senior,Onsite,"FutureFit AI is looking for a contract Senior Data Engineer to join our team. We have a culture of high trust, high impact, high velocity and a gritty determination to win. If you are passionate about development, like to have fun, do it right, and get things done we would love to hear from you!
An important note: Data shows that men on average apply for a role if they meet 3/10 requirements while women often only do so if it’s 10/10. We work hard to be clear and specific about what our roles include and demand and encourage you to apply if you see a strong (but not necessarily perfect) fit between you and the opportunity.
About You:
Technology - You are a passionate data engineer, eager to learn, solve problems and share your knowledge with others
Data - Strong architecture and design around various data paradigms (document stores, data lakes/warehouses and relational) and databases (Mongo, Elastic, Postgres, Redshift, etc.)
Data Query - Experience writing highly performant data queries for large/complex data sets across relational and NoSQL databases
Data Pipelines - Experience in architecture and implementation of ETL processes, streaming data, data process orchestration (Airflow)
BI - Adept at connecting BI platforms (Looker) to data warehouses to provide critical insights via dashboards
MLOps - Experience with model versioning, deployment, monitoring and serving models via REST APIs
Understanding - You are eager to understand the business problem and collaborate with product to build a solution that meets both technology and business requirements
Communication - You can share your perspective clearly and constructively, you are comfortable challenging ideas and collaborating to find the best approach
Grit - You like to solve challenging problems, are not afraid of the unknown, eager to learn and take a systematic approach to solving problems
AI - You are interested in integrating advances in AI into FutureFit AI products and the development process to drive improvements in velocity, quality and engineer experience
Values - You are a good person who shares our values of excellence, transparency, and humanity
The Product’s Technology Stack:
Front End - React, TypeScript, Tailwind, Storybook
Back End - Node.js, Nest.js, Python, Flask, GraphQL, Postgres, MongoDB, RedShift, Airflow
AWS - CloudFront, Cognito, ECS, RDS, S3, SQS, Elasticsearch, VPC, Lambda
Location:
Most days we work from home but everyone comes to the office at least once a week for face to face collaboration and team building. The office is located at 325 Front St West (a short walk from Union Station).
Our Company:
In looking at a job posting, it’s often hard to get a basic picture of the company profile (size, stage, structure, etc.) which is why we are sharing it with you upfront. This helps you quickly decide and helps us focus any time we spend together on going beyond the basics:
Funding
: We have raised one round of funding led by JP Morgan which fueled a significant growth trajectory for us and we have a safe financial runway to execute against.
Problem Domain:
Future of Work / Workforce Development - important that the problem domain interests you even if you haven’t worked in the space before.
Customers
: We partner with workforce development agencies, government agencies, and employers/enterprises.
Structure
: We are organized around the following key departments: Growth, Customer Success, Product, Engineering, Data, People & Culture, Finance & Operations.
Team
: We are a team of 30-50 across US and Canada with main hubs in New York and Toronto. This role will be based in Toronto.
Core Principles
: Be Curious, Drive to Outcomes, Raise the Bar, Speed Matters, Own It, Put We Over Me
About FutureFit AI
At FutureFit AI, we’re on a mission to unlock pathways between talent and opportunity using the power of AI. We focus on personalized, AI-powered career guidance for job seekers, emphasizing skills over extensive resumes, and partner with workforce development partners, governments, and employers to level access to opportunity.
FutureFit AI is a growing, venture-backed company focused on using technology to improve the lives and outcomes for people going through career transitions. We’re a small, driven team, united by our commitment to the job seekers and workforce ecosystems we serve. We're not just building a company; we're shaping the future of work.
We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, perform essential job functions, and receive other benefits and privileges of employment. Please contact us to request an accommodation.
©
FutureFit AI All rights reserved, we are proud to be an equal opportunity workplace. We celebrate diversity and are committed to creating an inclusive environment for all employees. We do not discriminate on the basis of race, religion, color, gender identity, sexual orientation, age, disability, veteran status, or other applicable legally protected characteristics. We encourage people of different backgrounds, experiences, abilities, and perspectives to apply.
Show more
Show less","Data Engineering, Data Architecture, Data Design, Data Pipelines, ETL, Data Warehousing, Data Lakes, Relational Databases, NoSQL Databases, SQL, REST APIs, Model Deployment, Model Monitoring, Business Requirements, Communication, Problem Solving, AI Integration, React, TypeScript, Tailwind, Storybook, Node.js, Nest.js, Python, Flask, GraphQL, AWS, CloudFront, Cognito, ECS, RDS, S3, SQS, Elasticsearch, VPC, Lambda","data engineering, data architecture, data design, data pipelines, etl, data warehousing, data lakes, relational databases, nosql databases, sql, rest apis, model deployment, model monitoring, business requirements, communication, problem solving, ai integration, react, typescript, tailwind, storybook, nodejs, nestjs, python, flask, graphql, aws, cloudfront, cognito, ecs, rds, s3, sqs, elasticsearch, vpc, lambda","ai integration, aws, business requirements, cloudfront, cognito, communication, data architecture, data design, data engineering, data lakes, datapipeline, datawarehouse, ecs, elasticsearch, etl, flask, graphql, lambda, model deployment, model monitoring, nestjs, nodejs, nosql databases, problem solving, python, rds, react, relational databases, rest apis, s3, sql, sqs, storybook, tailwind, typescript, vpc"
Senior Data Engineer,Vista,"Lakeshore, Ontario, Canada",https://ca.linkedin.com/jobs/view/senior-data-engineer-at-vista-3773355716,2023-12-17,Delta, Canada,Mid senior,Onsite,"Who We Are
Vista Engineering’s mission is to become industry-leading in engineering, data, and analytics to create products and services that empower small business owners across the globe in the areas of marketing and design. You can read more about our data mission on https://vista.io/
Vista Engineering is fully committed to delivering on this vision through a decentralized data mesh strategy. We are looking for an experienced Data Engineer to join our Business Performance and Manufacturing Domain to build thoughtfully architected, performant, scalable data solutions and data products to support the Windsor analytics in our journey from legacy sql server reporting to Vista’s data platform.
What You Will Do
Build production grade data solutions on legacy sql server platform
Establish, enforce, and educate team members on best practices in development and data modeling
Build production grade data products including foundational data sets using technologies such as Snowflake, S3, Databricks (Spark), Matillion, Looker and more
Work directly with plant analysts, data product analysts, data engineers, product owners and business experts
Provide feedback for new features to the Vistaprint Data Platform team
Your Qualifications:
Coding proficiency in at least one modern programming language, e.g.: Python.
Strong skills in SQL
Strong experience building complex ETL data pipelines with scheduling and monitoring (e.g., with DBT and or Airflow) for batch processing and real-time data
Experience with cloud services in AWS (Preferred), Azure or Google
Familiarity with API’s/microservices (REST)
Experience handling large volume data as well as both structured/unstructured data of various quality
Experience with data modeling and leading other team members
Degree in Computer Science, Engineering, Mathematics, or a related field
Problem solving and multi-tasking ability in a fast-paced, globally distributed environment
Nice to Have:
Familiarity with reporting tools like Looker would be a great plus
Statistics experience is a plus!
Equal Opportunity Employer:
Vistaprint, a Cimpress company, is an Equal Employment Opportunity Employer. All qualified candidates will receive consideration for employment without regard to race, color, sex, national or ethnic origin, nationality, age, religion, citizenship, disability, medical condition, sexual orientation, gender identity, gender presentation, legal or preferred name, marital status, pregnancy, family structure, veteran status or any other basis protected by human rights laws or regulations. This list is not exhaustive and, in fact, in many cases, we strive to do more than the law requires
Show more
Show less","Python, SQL, ETL pipelines, DBT, Airflow, Batch processing, Realtime data, AWS, Azure, Google, API, Microservices, REST, Data modeling, Looker, Statistics, Data mesh strategy, Snowflake, S3, Databricks, Matillion","python, sql, etl pipelines, dbt, airflow, batch processing, realtime data, aws, azure, google, api, microservices, rest, data modeling, looker, statistics, data mesh strategy, snowflake, s3, databricks, matillion","airflow, api, aws, azure, batch processing, data mesh strategy, databricks, datamodeling, dbt, etl pipelines, google, looker, matillion, microservices, python, realtime data, rest, s3, snowflake, sql, statistics"
"Principal, Data Engineer Architect",Lovelytics,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/principal-data-engineer-architect-at-lovelytics-3787795090,2023-12-17,Delta, Canada,Mid senior,Onsite,"Lovelytics is seeking a skilled consultant with experience delivering strategic Databricks and data engineering client engagements.
This Principal Consultant will play the role of a data and analytics solution architect, primarily focusing on leading the technical sales process related to data warehousing, ETL development, data integrations, and data modeling. In addition to the technical capabilities for this role, we are looking for someone who wants to work in a collaborative, dynamic, and inclusive environment and has a passion for bringing meaning to data through thought leadership and execution.
Role Location: Remote in Ontario, Canada with preference for candidates near our HQ office (Toronto, ON)
Primary Job Responsibilities:
Fill the role of data engineer solutions architect for our largest and most complex client engagements focused on developing data warehouses, optimizing back-end performance, and integrating data sources to systems using Databricks
Partner with our Technical Directors to identify emerging trends in the market, design and implement internal initiatives, etc.
Provide thought leadership in the form of blogs, technical ideation for Lovelytics' new offerings, and solutions for client's problems across various verticals.
Work hand-in-hand with our other capability teams to successfully sell, plan, and deliver engagements
Gather requirements from clients and develop creative and effective technical solutions
Partner with our sales team to play an extensive role in the presales and pursuit process, applying knowledge to potential client problems, enabling Lovelytics to expand the business
Apply your skills with Databricks, using Python, and big data streaming to pioneer client technologies and data
Manage projects to ensure project milestones are reached within the given timeline and budget allocated
Support other team members on projects, which can oftentimes mean wearing many different hats
Integrate Databricks with 3rd-party applications to support customers' architectures
Troubleshooting data issues on the fly with prospects and clients
Our Ideal Candidate's Skills and Experiences:
B.S. in Computer Science or equivalent
6+ years in data engineering working with cloud-based data analytics architectures and 4+ yrs' experience working directly with clients.
2+ years of involvement in technology pre-sales
Extensive knowledge of data warehousing and data lake concepts and hands-on experience deploying pipelines using Databricks
Experience developing Machine Learning models or ML Ops processes a plus
Excellent communication skills are a MUST, all our employees are client-facing, and this role requires both written and verbal client management skills.
Proven success in client development and partnering with Sales to develop solutions in the presales and discovery phase
Databricks Data Engineer Professional and Databricks Machine Learning Professional certifications a plus
Experience designing architectures within a public cloud (AWS or Azure)
Hands-on experience with Big Data technologies, including Spark, Hadoop, Cassandra, and others
Ability to extract and transform data via Python, deep exposure and understanding of data warehousing, ETL pipelines, etc.
Overall understanding of analytics from analytic engineering to visualization tools
What We Promise You:
Exciting projects with great clients in varying departments and verticals across the world
The ability to work closely with experienced data engineers and quickly grow and expand your skillset
The ability to work closely with all sizes of companies, ranging from Fortune 100 to small local businesses
A workplace where you are encouraged to challenge the status quo and develop new technologies, methodologies, and processes
A diverse team consisting of data gurus, experience seekers, and entrepreneurial minds that are always pushing to be better
Lovelytics is an Equal Opportunity Employer. This means you don’t have to worry about whether your application process will be fair. We consider all applicants without regard to race, color, religion, age, ancestry, ethnicity, gender, gender identity, gender expression, sexual orientation, veteran status, or disability.
Powered by JazzHR
Vc06jDrEcV
Show more
Show less","Databricks, Data engineering, Data warehousing, ETL, Data modeling, Python, Big data streaming, Machine Learning, ML Ops, Cloud computing, AWS, Azure, Spark, Hadoop, Cassandra, Data Analytics, Data visualization","databricks, data engineering, data warehousing, etl, data modeling, python, big data streaming, machine learning, ml ops, cloud computing, aws, azure, spark, hadoop, cassandra, data analytics, data visualization","aws, azure, big data streaming, cassandra, cloud computing, data engineering, dataanalytics, databricks, datamodeling, datawarehouse, etl, hadoop, machine learning, ml ops, python, spark, visualization"
Azure Data Engineer - Remote/Contract,Emtec Inc.,Canada,https://ca.linkedin.com/jobs/view/azure-data-engineer-remote-contract-at-emtec-inc-3785858775,2023-12-17,Delta, Canada,Mid senior,Onsite,"Company Overview
Emtec is a Global consulting company that provides technology-empowered business solutions for world-class organizations. Our Global Workforce of over 1000 consultants provide best in class services to our clients to realize their digital transformation journey. Our clients span the emerging, mid-market and enterprise space. With multiple offices worldwide, we are uniquely positioned to deliver digital solutions to our clients leveraging Microsoft, Java and Open Source with a focus on Mobility, Cloud, Data Engineering and Intelligent Automation. Emtec’s singular mission is to create “Clients for Life” - long-term relationships that deliver rapid, meaningful, and lasting business value.
At Emtec, we have a unique blend of Corporate and Entrepreneurial cultures. This is where you would have an opportunity to drive business value for clients while you innovate and continue to grow and have fun while doing it. You would work with team members who are vibrant, smart and passionate and they bring their passion to all that they do – whether it’s learning, giving back to our communities or always going the extra mile for our client.
Position Description
We are looking for a talented and experienced Azure Data Engineer who will support various data projects within the organization. We are looking from someone with a strong background with Azure Data Factory, Azure Databricks, Azure SQL Database, Azure Data Lake Storage,etc. Responsibilites may vary depending on the project; however, ETL will likely be a major responsibility across the board.
Core Tech Stack:
Azure Data Factory, Azure Databricks, Azure SQL Database, Azure Data Lake Storage,etc.
Location:
Fully Remote
Employment Type:
This is a 6 month contract, with potential extention.
Responsibilities Include But Are Not Limited To
Develop and implement ETL processes using Azure Data Factory, Azure Databricks, and other relevant tools.
Implement and manage Azure SQL Database, Azure Data Lake Storage, and other Azure data storage solutions.
Ensure data integrity, security, and compliance with industry best practices and organizational standards.
Utilize Azure Databricks and other tools for data transformation and processing.
Optimize queries and data processing workflows for performance and efficiency.
Must Have Skills
Bachelor's degree in Computer Science, Information Technology, or a related field.
4+ Years of Experience in Data Integration platform on cloud (Azure preferred)
In-depth knowledge of Azure Data Factory, Azure SQL Database, Azure Databricks, and other relevant Azure services.
Strong proficiency in SQL, Python, and/or other scripting languages.
Proven experience as a Data Engineer with a focus on Azure cloud technologies
Knowledge on GitHub Integration with Azure DevOps
Emtec is an Equal Opportunity Employer
Canadian citizens and those authorized to work in Canada are encouraged to apply
Show more
Show less","Azure Data Factory, Azure Databricks, Azure SQL Database, Azure Data Lake Storage, Azure DevOps, ETL, SQL, Python, GitHub, Data Engineering","azure data factory, azure databricks, azure sql database, azure data lake storage, azure devops, etl, sql, python, github, data engineering","azure data factory, azure data lake storage, azure databricks, azure devops, azure sql database, data engineering, etl, github, python, sql"
"Data Engineer, Marketplace",Lyft,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/data-engineer-marketplace-at-lyft-3780707617,2023-12-17,Delta, Canada,Mid senior,Onsite,"At Lyft, community is what we are and it’s what we do. It’s what makes us different. To create the best ride for all, we start in our own community by creating an open, inclusive, and diverse organization where all team members are recognized for what they bring.
Here at Lyft, Data is the only way we make decisions. It is the core of our business, helping us create a transportation experience for our customers, and providing insights into the effectiveness of our product launch & features.
As a Data Engineer at Lyft, you will be a part of an early stage team that builds the data transport, collection, and storage, and exposes services that make data a first-class citizen at Lyft. We are looking for a Data Engineer to build a scalable data platform. You’ll have ownership of our core data pipeline that powers Lyft’s top-line metrics; You will also use data expertise to help evolve data models in several components of the data stack; You will help architect, building, and launching scalable data pipelines to support Lyft’s growing data processing and analytics needs. Your efforts will allow access to business and user behavior insights, using huge amounts of Lyft data to fuel several teams such as Analytics, Data Science, Marketplace, and many others.
Responsibilities:
Owner of the core company data pipeline, responsible for scaling up data processing flow to meet the rapid data growth at Lyft
Evolve data model and data schema based on business and engineering needs
Implement systems tracking data quality and consistency
Develop tools supporting self-service data pipeline management (ETL)
SQL and MapReduce job tuning to improve data processing performance
Write well-crafted, well-tested, readable, maintainable code
Participate in code reviews to ensure code quality and distribute knowledge
Unblock, support and communicate with internal & external partners to achieve results
Experience:
3+ years of relevant professional experience
Experience with Hadoop (or similar) Ecosystem (MapReduce, Yarn, HDFS, Hive, Spark, Presto, Pig, HBase, Parquet)
Strong skills in a scripting language (Python, Ruby, Bash)
Good understanding of SQL Engine and able to conduct advanced performance tuning
Proficient in at least one of the SQL languages (MySQL, PostgreSQL, SqlServer, Oracle)
1+ years of experience with workflow management tools (Airflow, Oozie, Azkaban, UC4)
Comfortable working directly with data analytics to bridge Lyft’s business goals with data engineering
Benefits:
Extended health and dental coverage options, along with life insurance and disability benefits
Mental health benefits
Family building benefits
Access to a Health Care Savings Account
In addition to provincial observed holidays, team members get 15 days paid time off, with an additional day for each year of service
4 Floating Holidays each calendar year prorated based off of date of hire
10 paid sick days per year regardless of province
18 weeks of paid parental leave. Biological, adoptive, and foster parents are all eligible
Lyft proudly pursues and hires a diverse workforce. Lyft believes that every person has a right to equal employment opportunities without discrimination because of race, ancestry, place of origin, colour, ethnic origin, citizenship, creed, sex, sexual orientation, gender identity, gender expression, age, marital status, family status, disability, pardoned record of offences, or any other basis protected by applicable law or by Company policy. Lyft also strives for a healthy and safe workplace and strictly prohibits harassment of any kind. Accommodation for persons with disabilities will be provided upon request in accordance with applicable law during the application and hiring process. Please contact your recruiter now if you wish to make such a request.
This role will be in-office on a hybrid schedule following the establishment of a Lyft office in Toronto — Team Members will be expected to work in the office 3 days per week on Mondays, Thursdays and a team-specific third day. Additionally, hybrid roles have the flexibility to work from anywhere for up to 4 weeks per year.
Show more
Show less","Python, Ruby, Bash, SQL, MySQL, PostgreSQL, SqlServer, Oracle, Hadoop, MapReduce, Yarn, HDFS, Hive, Spark, Presto, Pig, HBase, Parquet, Airflow, Oozie, Azkaban, UC4","python, ruby, bash, sql, mysql, postgresql, sqlserver, oracle, hadoop, mapreduce, yarn, hdfs, hive, spark, presto, pig, hbase, parquet, airflow, oozie, azkaban, uc4","airflow, azkaban, bash, hadoop, hbase, hdfs, hive, mapreduce, mysql, oozie, oracle, parquet, pig, postgresql, presto, python, ruby, spark, sql, sqlserver, uc4, yarn"
Azure Data Engineer,Wipro,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/azure-data-engineer-at-wipro-3765457730,2023-12-17,Delta, Canada,Mid senior,Onsite,"About Wipro
Wipro Limited (NYSE: WIT, BSE: 507685, NSE: WIPRO) is a leading technology services and consulting company focused on building innovative solutions that address clients’ most complex digital transformation needs.
We leverage our holistic portfolio of capabilities in consulting, design, engineering, operations, and emerging technologies to help clients realize their boldest ambitions and build future-ready, sustainable businesses.
A company recognized globally for its comprehensive portfolio of services, strong commitment to sustainability and good corporate citizenship, we have over 250,000 dedicated employees serving clients across 66 countries.
We deliver on the promise of helping our customers, colleagues, and communities thrive in an ever-changing world.
·
A PROUD HISTORY OF OVER 75 YEARS
·
FY22 REVENUE 10.4 BN USD
·
WE’RE PRESENT IN 66 COUNTRIES
·
OVER 1,400 ACTIVE GLOBAL CLIENTS
JD:
Must-have skills
• Cloud certified in one of these categories
• Azure Data Engineer
• Azure Data Factory , Azure Data bricks Spark (PySpark or scala), SQL
• Experience in Azure ingestion from on-prem source, e.g. mainframe, SQL server, Oracle.
• Experience in Sqoop / Hadoop
• Microsoft Excel (for metadata files with requirements for ingestion)
• Any other certificate in Azure/AWS/GCP and data engineering hands-on experience in cloud
• Strong Programming skills with at least one of Python, Scala or Java
• Strong SQL skills ( T-SQL or PL-SQL)
• Data files movement via mailbox
• Source-code versioning/promotion tools, e.g. Git/Jenkins
• Orchestration tools, e.g. Autosys, Oozie
• Source-code versioning with Git.
Nice-to-have skills
• Experience working with mainframe files
• Experience in Agile environment, JIRA/Confluence tools.
Wipro is an Equal Employment Opportunity employer and makes all employment and employment-related decisions without regard to a person's race, sex, national origin, ancestry, disability, sexual orientation.
Show more
Show less","Cloud Computing, Azure, Azure Data Engineer, Azure Data Factory, Apache Spark, PySpark, Scala, SQL, Hadoop, Sqoop, Microsoft Excel, Python, Java, TSQL, PLSQL, Git, Jenkins, Autosys, Oozie","cloud computing, azure, azure data engineer, azure data factory, apache spark, pyspark, scala, sql, hadoop, sqoop, microsoft excel, python, java, tsql, plsql, git, jenkins, autosys, oozie","apache spark, autosys, azure, azure data engineer, azure data factory, cloud computing, git, hadoop, java, jenkins, microsoft excel, oozie, plsql, python, scala, spark, sql, sqoop, tsql"
Senior Data Engineer,Lyft,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/senior-data-engineer-at-lyft-3744004352,2023-12-17,Delta, Canada,Mid senior,Onsite,"TBS - Data Engineering
At Lyft, our mission is to improve people’s lives with the world’s best transportation. To do this, we start with our own community by creating an open, inclusive, and diverse organization.
Here at Lyft, Data is the only way we make decisions. It is the core of our business, helping us create a transportation experience for our customers and providing insights into the effectiveness of our product launch & features.
This team focuses on supporting our business by building the data transport, collection, and storage that powers our Transit, Bikes, and Scooters business. We are looking for a Data Engineer to build scalable solutions, leveraging their data expertise and our technology stack to provide timely, accurate data for our internal and external customers. You will have the opportunity to be a member of a new, growing part of our micro mobility platform. This role will involve collaborating with product managers, external stakeholders, GMs, engineers, and data scientists to gather and translate requirements into solutions, ensuring that data-driven decisions are at the core of our business.
This role reports to the Engineering Manager.
Responsibilities:
Owner of core company data pipelines, responsible for scaling up data processing flow to meet the rapid data growth at Lyft
Evolve data model and data schema based on business and engineering needs
Implement systems tracking data quality and consistency
Develop tools supporting self-service data pipeline management (ETL)
SQL and MapReduce job tuning to improve data processing performance
Write well-crafted, well-tested, readable, maintainable code
Participate in code reviews to ensure code quality and distribute knowledge
Unblock, support and communicate with internal & external partners to achieve results
Experience:
5+ years of relevant professional experience
Experience with Hadoop (or similar) Ecosystem (MapReduce, Yarn, HDFS, Hive, Spark, Presto, Pig, HBase, Parquet)
Proficient in at least one of the SQL languages (MySQL, PostgreSQL, SqlServer, Oracle)
Good understanding of SQL Engine and able to conduct advanced performance tuning
Strong skills in scripting language (Python, Ruby, Bash)
2+ years of experience with workflow management tools (Airflow, Oozie, Azkaban, UC4)
Comfortable working directly with data analytics to bridge Lyft's business goals with data engineering
Benefits:
Extended health and dental coverage options, along with life insurance and disability benefits
Mental health benefits
In addition to provincial observed holidays, team members get 15 days paid time off, with an additional day for each year of service
4 Floating Holidays each calendar year prorated based off of date of hire
10 paid sick days per year regardless of province
Family building benefits
Access to a Health Care Savings Account
18 weeks of paid parental leave. Biological, adoptive, and foster parents are all eligible
Lyft proudly pursues and hires a diverse workforce. Lyft believes that every person has a right to equal employment opportunities without discrimination because of race, ancestry, place of origin, colour, ethnic origin, citizenship, creed, sex, sexual orientation, gender identity, gender expression, age, marital status, family status, disability, pardoned record of offences, or any other basis protected by applicable law or by Company policy. Lyft also strives for a healthy and safe workplace and strictly prohibits harassment of any kind. Accommodation for persons with disabilities will be provided upon request in accordance with applicable law during the application and hiring process. Please contact your recruiter now if you wish to make such a request.
Starting in September 2023, this role will be in-office on a hybrid schedule — Team Members will be expected to work in the office 3 days per week on Mondays, Thursdays and a team-specific third day. Additionally, hybrid roles have the flexibility to work from anywhere for up to 4 weeks per year.
Show more
Show less","Data Engineering, Hadoop, MapReduce, Yarn, HDFS, Hive, Spark, Presto, Pig, HBase, Parquet, SQL, MySQL, PostgreSQL, SqlServer, Oracle, Python, Ruby, Bash, Airflow, Oozie, Azkaban, UC4","data engineering, hadoop, mapreduce, yarn, hdfs, hive, spark, presto, pig, hbase, parquet, sql, mysql, postgresql, sqlserver, oracle, python, ruby, bash, airflow, oozie, azkaban, uc4","airflow, azkaban, bash, data engineering, hadoop, hbase, hdfs, hive, mapreduce, mysql, oozie, oracle, parquet, pig, postgresql, presto, python, ruby, spark, sql, sqlserver, uc4, yarn"
AWS Data Engineer,Tata Consultancy Services,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/aws-data-engineer-at-tata-consultancy-services-3765122908,2023-12-17,Delta, Canada,Mid senior,Onsite,"TCS is an equal opportunity employer, and embraces diversity in race, nationality, ethnicity, gender, age, physical ability, neurodiversity, and sexual orientation, to create a workforce that reflects the societies we operate in. Our continued commitment to Culture and Diversity and is reflected in our people stories across our workforce implemented through equitable workplace policies and processes.
About TCS
TCS operates on a global scale, with a diverse talent base of more than 600,000 associates representing 153 nationalities across 55 countries. TCS has been recognized as a Global Top Employer by the Top Employers Institute - one of only eight companies worldwide to have achieved this status. Our organizational structure is domain-led and designed to offer businesses a single window into industry-specific solutions. Our agile industry units have embedded capabilities to enable rapid responses that provide a competitive edge to our customers. This, coupled with a unique Global Network Delivery Model™ (GNDM™), is recognized as the current benchmark of excellence in technology deployment. We have made significant investments in digital technology, horizontal, and vertical platforms, allowing us to successfully serve our clients for over 50 years.
Job Description
• University degree in engineering or computer science preferred.
• Hands-on experience building data exploration interfaces using notebooks (JupyterHub/Lab,
Spark/Livy, Sparkmagic, Enterprise Gateway, Matplotlib, Plotly/Dash, etc.).
• Hands-on expertise building data pipelines and applications, leveraging Kubernetes, Python,
AWS Sagemaker, PySpark, YARN, S3, Athena, Glue, Lakeformation, Step Functions, Airflow,
serverless platforms.
• Experience using cloud technology best practices enabling distribution and analysis of big data in
the cloud (formatting, partitioning, etc.).
• Experience using ETL processing chains, managing multiple datasets and providing support as
needed.
• Proficiency in Python and experience using Spark or PySpark.
• Ability to work in an entrepreneurial environment and demonstrate initiative.
• Demonstrated ability to work with abstract and concrete concepts and bring them together
based on audience and context preferred.
• Quick understanding of organizational dynamics and management priorities and ability to work
effectively within a fast-paced, results-oriented company.
• Excellent skills in facilitating, negotiating, building interpersonal relationships, communicating
and collaborating with others.
Tata Consultancy Services Canada Inc. is committed to meeting the accessibility needs of all individuals in accordance with the Accessibility for Ontarians with Disabilities Act (AODA) and the Ontario Human Rights Code (OHRC). Should you require accommodations during the recruitment and selection process, please inform Human Resource.
Thank you for your interest in TCS. Candidates that meet the qualification for this position will be contacted within a 2 week period. We invite you to continue to apply for other opportunities that match your profile.
Show more
Show less","Engineering, Computer Science, Data Exploration, JupyterHub/Lab, Spark/Livy, Sparkmagic, Enterprise Gateway, Matplotlib, Plotly/Dash, Data Pipelines, Kubernetes, Python, AWS Sagemaker, PySpark, YARN, S3, Athena, Glue, Lakeformation, Step Functions, Airflow, Serverless Platforms, Cloud Technology, Big Data, ETL Processing Chains, Data Management","engineering, computer science, data exploration, jupyterhublab, sparklivy, sparkmagic, enterprise gateway, matplotlib, plotlydash, data pipelines, kubernetes, python, aws sagemaker, pyspark, yarn, s3, athena, glue, lakeformation, step functions, airflow, serverless platforms, cloud technology, big data, etl processing chains, data management","airflow, athena, aws sagemaker, big data, cloud technology, computer science, data exploration, data management, datapipeline, engineering, enterprise gateway, etl processing chains, glue, jupyterhublab, kubernetes, lakeformation, matplotlib, plotlydash, python, s3, serverless platforms, spark, sparklivy, sparkmagic, step functions, yarn"
Data Engineer with Hands on Snowflake Development,Tech Mahindra,"Mississauga, Ontario, Canada",https://ca.linkedin.com/jobs/view/data-engineer-with-hands-on-snowflake-development-at-tech-mahindra-3773541254,2023-12-17,Delta, Canada,Mid senior,Onsite,"Primary Responsibilities
Strong understanding or Snowflake on Azure …..Architecture, design, implementation and operationalization of large-scale data and analytics solutions on Snowflake Cloud Data Warehouse.
Hands-on development experience with Snowflake features such as Snow SQL; Snow Pipe; Python; Tasks; Streams; Time travel; Zero Copy Cloning; Optimizer; Metadata Manager; data sharing; and stored procedures.
Experience in Data warehousing - OLTP, OLAP, Dimensions, Facts, and Data modeling.
Need to have working knowledge of MS Azure configuration items with respect to Snowflake.
Developing EL pipelines in and out of data warehouse using combination of Data bricks, Python and Snow SQL.
Show more
Show less","Snowflake, Azure, Data Warehousing, AWS Configuration items, Data Bricks, Python, Snow SQL, SQL, Snow Pipe, Tasks, Streams, Time Travel, Zero Copy Cloning, Optimizer, Metadata Manager, Data Sharing, Stored Procedures, OLTP, OLAP, Dimensions, Facts, Data Modeling","snowflake, azure, data warehousing, aws configuration items, data bricks, python, snow sql, sql, snow pipe, tasks, streams, time travel, zero copy cloning, optimizer, metadata manager, data sharing, stored procedures, oltp, olap, dimensions, facts, data modeling","aws configuration items, azure, data bricks, data sharing, datamodeling, datawarehouse, dimensions, facts, metadata manager, olap, oltp, optimizer, python, snow pipe, snow sql, snowflake, sql, stored procedures, streams, tasks, time travel, zero copy cloning"
Data Engineer with Hands on Snowflake Development,Tech Mahindra,"Mississauga, Ontario, Canada",https://ca.linkedin.com/jobs/view/data-engineer-with-hands-on-snowflake-development-at-tech-mahindra-3773591964,2023-12-17,Delta, Canada,Mid senior,Onsite,"Primary Responsibilities
Strong understanding or Snowflake on Azure …..Architecture, design, implementation and operationalization of large-scale data and analytics solutions on Snowflake Cloud Data Warehouse.
Hands-on development experience with Snowflake features such as Snow SQL; Snow Pipe; Python; Tasks; Streams; Time travel; Zero Copy Cloning; Optimizer; Metadata Manager; data sharing; and stored procedures.
Experience in Data warehousing - OLTP, OLAP, Dimensions, Facts, and Data modeling.
Need to have working knowledge of MS Azure configuration items with respect to Snowflake.
Developing EL pipelines in and out of data warehouse using combination of Data bricks, Python and Snow SQL.
Show more
Show less","Snowflake, Azure, Snow SQL, Snow Pipe, Python, Tasks, Streams, Time travel, Zero Copy Cloning, Optimizer, Metadata Manager, Data sharing, Stored procedures, Data warehousing, OLTP, OLAP, Dimensions, Facts, Data modeling, MS Azure, Data bricks","snowflake, azure, snow sql, snow pipe, python, tasks, streams, time travel, zero copy cloning, optimizer, metadata manager, data sharing, stored procedures, data warehousing, oltp, olap, dimensions, facts, data modeling, ms azure, data bricks","azure, data bricks, data sharing, datamodeling, datawarehouse, dimensions, facts, metadata manager, ms azure, olap, oltp, optimizer, python, snow pipe, snow sql, snowflake, stored procedures, streams, tasks, time travel, zero copy cloning"
Data Engineer,Rminds Technology,"Ontario, Canada",https://ca.linkedin.com/jobs/view/data-engineer-at-rminds-technology-3775749676,2023-12-17,Delta, Canada,Mid senior,Onsite,"Description:
Recognized Subject Matter Expert (SME) with at least 4 years of significant experience in a requirements management
Proven experience designing and driving Systems Engineering programs with solid hands-on experience and in-depth knowledge including business and technology issues related to implementation and approaches in a federated model.
Proven experience in requirements definition of achievable design characteristics.
Flow system level requirements to testable structures.
Perform detail system verification activities including definition of verification methodology.
.
Technical Skills Required:
Minimum 4 years in a Requirements Management Role
Minimum 4 years in a Technical Writing Capacity
Minimum 4 years in working with DOORS or similar tool
Knowledge or/and hands experience with PMO (Waterfall or Agile methodology) essential.
Possess strong proficiency for articulating data concepts across and between both business-focused and our technology-based partners and colleagues.
Soft Skills Required:
Keeping abreast with emerging data management or governing trends, techniques and technology and related legal and regulatory requirements.
Ability to articulate vision of transformation efforts and a sense of mission and resilient in manifold ways to achieve targets and goals.
Ability to manage senior relationships across all the business and functional areas and able to influence the way things are done with a lens for driving initiatives and behavior to become a data-driven organization.
Exceptional skills in the art of negotiation and influencing others while demonstrating the ability to influence without authority.
Position Id: R010658
Show more
Show less","Requirements Management, Technical Writing, DOORS, PMO, Waterfall, Agile, Data Management, Data Governance, Data Concepts, Legal and Regulatory Requirements, DataDriven Organization, Negotiation, Influencing","requirements management, technical writing, doors, pmo, waterfall, agile, data management, data governance, data concepts, legal and regulatory requirements, datadriven organization, negotiation, influencing","agile, data concepts, data governance, data management, datadriven organization, doors, influencing, legal and regulatory requirements, negotiation, pmo, requirements management, technical writing, waterfall"
Bigdata Developer,Tata Consultancy Services,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/bigdata-developer-at-tata-consultancy-services-3769011922,2023-12-17,Delta, Canada,Mid senior,Onsite,"About TCS
TCS operates on a global scale, with a diverse talent base of more than 600,000 associates representing 153 nationalities across 55 countries. TCS has been recognized as a Global Top Employer by the Top Employers Institute - one of only eight companies worldwide to have achieved this status. Our organizational structure is domain-led and designed to offer businesses a single window into industry-specific solutions. Our agile industry units have embedded capabilities to enable rapid responses that provide a competitive edge to our customers. This, coupled with a unique Global Network Delivery Model™ (GNDM™), is recognized as the current benchmark of excellence in technology deployment. We have made significant investments in digital technology, horizontal, and vertical platforms, allowing us to successfully serve our clients for over 50 years.
Skills Required:
• Need to be good on PROD Support experience with 6 - 7 years of experience.
• Provide L2/L3 support
• Analyze production logs and troubleshoot issues
• Highlight and escalate to L3 if bug fix is needed
• Good hands on experience with deployments and support
o Bigdata, Hadoop, HDFS, Impala, Hive, Cloudrera, Precisely, Autosys, Sqoop, Oozie
o Python, Scala
o User Access setup and Onboarding Experience will be a great add on
o ETL Experience
o UNIX, RDBMS (MSSQL, Oracle)
o Cloud Technologies: Microsoft Azure, DataFactory, DataBricks
• Good communication and coordination skill
• Well versed with ITIL processes (Incident Management, Problem Management, Change Management, Release Management)
Tata Consultancy Services Canada Inc. is committed to meeting the accessibility needs of all individuals in accordance with the Accessibility for Ontarians with Disabilities Act (AODA) and the Ontario Human Rights Code (OHRC). Should you require accommodations during the recruitment and selection process, please inform Human Resource
Thank you for your interest in TCS. Candidates that meet the qualification for this position will be contacted within a 2-week period. We invite you to continue to apply for other opportunities that match your profile.
Show more
Show less","PROD Support, L2/L3 Support, Troubleshooting, Bug Fixing, Deployments, Bigdata, Hadoop, HDFS, Impala, Hive, Cloudrera, Precisely, Autosys, Sqoop, Oozie, Python, Scala, User Access Setup, Onboarding, ETL Experience, UNIX, RDBMS, MSSQL, Oracle, Cloud Technologies, Microsoft Azure, DataFactory, DataBricks, ITIL Processes, Incident Management, Problem Management, Change Management, Release Management","prod support, l2l3 support, troubleshooting, bug fixing, deployments, bigdata, hadoop, hdfs, impala, hive, cloudrera, precisely, autosys, sqoop, oozie, python, scala, user access setup, onboarding, etl experience, unix, rdbms, mssql, oracle, cloud technologies, microsoft azure, datafactory, databricks, itil processes, incident management, problem management, change management, release management","autosys, bigdata, bug fixing, change management, cloud technologies, cloudrera, databricks, datafactory, deployments, etl experience, hadoop, hdfs, hive, impala, incident management, itil processes, l2l3 support, microsoft azure, mssql, onboarding, oozie, oracle, precisely, problem management, prod support, python, rdbms, release management, scala, sqoop, troubleshooting, unix, user access setup"
Data Engineer,Tiger Analytics,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/data-engineer-at-tiger-analytics-3784315864,2023-12-17,Delta, Canada,Mid senior,Remote,"Tiger Analytics is a fast-growing advanced analytics consulting firm. Our consultants bring deep expertise in Data Science, Machine Learning and AI. We are the trusted analytics partner for several Fortune 100 companies, enabling them to generate business value from data. Our business value and leadership has been recognized by various market research firms, including Forrester and Gartner. We are looking for top-notch talent as we continue to build the best analytics global consulting team in the world.
We are seeking an experienced Data Engineer with expertise in Dataiku to join our data team. As a Data Engineer, you will be responsible for designing, building, and maintaining data pipelines, data integration processes, and data infrastructure. You will collaborate closely with data scientists, analysts, and other stakeholders to ensure efficient data flow and support data-driven decision making across the organization.
Requirements
8+ years of overall industry experience specifically in data engineering
Strong knowledge of data engineering principles, data integration, and data warehousing concepts
Proficiency in building and maintaining data pipelines using Dataiku
Solid understanding of ETL processes and tools
Strong programming skills in Python, SQL, or Scala
Good understanding of data modeling, data architecture, and database design
Familiarity with cloud platforms like AWS, Snowflake, dbt, Azure, or GCP
Excellent problem-solving and troubleshooting skills
Strong communication and collaboration abilities
Attention to detail and a focus on delivering high-quality work
Benefits
Significant career development opportunities exist as the company grows. The position offers a unique opportunity to be part of a small, challenging, and entrepreneurial environment, with a high degree of individual responsibility.
Show more
Show less","Dataiku, Data Engineering, Data Pipelines, Data Integration, Data Warehousing, ETL, Python, SQL, Scala, Data Modeling, Data Architecture, Database Design, AWS, Snowflake, dbt, Azure, GCP, ProblemSolving, Troubleshooting, Communication, Collaboration, Attention to Detail","dataiku, data engineering, data pipelines, data integration, data warehousing, etl, python, sql, scala, data modeling, data architecture, database design, aws, snowflake, dbt, azure, gcp, problemsolving, troubleshooting, communication, collaboration, attention to detail","attention to detail, aws, azure, collaboration, communication, data architecture, data engineering, data integration, database design, dataiku, datamodeling, datapipeline, datawarehouse, dbt, etl, gcp, problemsolving, python, scala, snowflake, sql, troubleshooting"
Data Engineer,Intelliswift Software,Canada,https://ca.linkedin.com/jobs/view/data-engineer-at-intelliswift-software-3774832973,2023-12-17,Delta, Canada,Mid senior,Remote,"Must Have skills:
Snowflake, SQL, SQL server, SSIS, Informatica Cloud (IICS) , Data warehousing Data lake concepts, Data modeling understanding and Azure cloud services.
Detailed Job Description
We are seeking a skilled and experienced Data Engineer to join our team and play a pivotal role in designing, developing, and maintaining our data infrastructure. As a Data Engineer, you will be responsible for building and managing data pipelines, data warehouses, and data lakes, ensuring that our data is accessible, reliable, and secure. You will work closely with cross-functional teams to understand data requirements, translate them into technical solutions, and implement those solutions in a timely and efficient manner.
Responsibilities:
Design, develop, and implement data pipelines using Snowflake, SQL, SQL Server, SSIS, and Informatica Cloud (IICS)
Build and maintain data warehouses and data lakes
Implement data modeling techniques to ensure data integrity and consistency
Develop and maintain data quality processes
Collaborate with data scientists and analysts to ensure data is available for analysis
Automate data processes to improve efficiency and reduce manual effort
Monitor data pipelines and data quality to identify and resolve issues
Stay up-to-date on the latest data engineering technologies and best practices
Qualifications
Bachelor's degree in Computer Science, Information Systems, or a related field
Solid understanding of data warehousing, data lakes, and data modeling concepts
Proficient in Snowflake, SQL, SQL Server, SSIS, and Informatica Cloud (IICS)
Experience with Azure cloud services, such as Azure Data Factory and Azure Databricks
Strong programming skills in Python or Scala
Excellent problem-solving and analytical skills
Ability to work independently and as part of a team
Excellent communication and written skills
Show more
Show less","Snowflake, SQL, SQL Server, SSIS, Informatica Cloud (IICS), Data warehousing, Data lake, Data modeling, Azure cloud services, Data pipelines, Data quality, Python, Scala, Azure Data Factory, Azure Databricks","snowflake, sql, sql server, ssis, informatica cloud iics, data warehousing, data lake, data modeling, azure cloud services, data pipelines, data quality, python, scala, azure data factory, azure databricks","azure cloud services, azure data factory, azure databricks, data lake, data quality, datamodeling, datapipeline, datawarehouse, informatica cloud iics, python, scala, snowflake, sql, sql server, ssis"
Entry Level Software Engineer - Data Backend Engineer (Remote - Canada),Yelp,Canada,https://ca.linkedin.com/jobs/view/entry-level-software-engineer-data-backend-engineer-remote-canada-at-yelp-3761340875,2023-12-17,Delta, Canada,Mid senior,Remote,"Summary
Yelp engineering culture is driven by our values: we’re a cooperative team that values individual authenticity and encourages creative solutions to problems. All new engineers deploy working code their first week, and we strive to broaden individual impact with support from managers, mentors, and teams. At the end of the day, we’re all about helping our users, growing as engineers, and having fun in a collaborative environment.
Interested in exploring data and finding innovative ways to collect it, curate it, and make it easily accessible for others to use? How about doing that at an enormous scale? Yelp’s datasets contain billions of interactions between users and local businesses around the globe. If working with our vast amount of data sounds exciting, come and join us!
The Marketplace Data Observability team is responsible for managing, governing, and monitoring all of Yelp’s Ads data to protect our revenue stream and also support offline analysis that aids in making long term company decisions. We build tools that help us to better understand our data quality, and we strive to uncover data issues before they impact our consumers.
As a Data Backend Engineer on this team, you will be responsible for building elegant and scalable data products that serve critical, up-to-date, structured information to support different types of analytics within Yelp. As a core contributor to our growing data modeling and data warehousing engineering efforts, you will help design and own mission-critical data flow pipelines and datastores to enable decisions including effective A/B testing and company investments.
This opportunity is fully remote and does not require you to be located in any particular area in Canada. We welcome applicants from throughout Canada. We’d love to have you apply, even if you don’t feel you meet every single requirement in this posting. At Yelp, we’re looking for great people, not just those who simply check off all the boxes.
What You'll Do
Build systems that can effectively store and crunch terabytes of data.
Design and develop data models for efficient data storage, retrieval, and reporting.
Create and maintain conceptual, logical, and physical data models using industry-standard modeling tools.
Collaborate with cross-functional teams, including engineers, data analysts, business analysts, and data scientists, to understand data requirements and translate them into effective data models.
Participate in data integration efforts, including ETL processes and data migration.
Stay up-to-date with industry best practices, emerging technologies, and trends related to data warehousing.
Support on-call rotations as needed to operate the team.
What It Takes To Succeed
Understanding of high performing and scalable data systems.
Experience in building and orchestrating ETL pipelines.
Experience with Data Lake or Data Warehouse landscape.
A hunger for tracking down root causes and fixing them in systematic ways.
Ability to communicate effectively to technical and non-technical cohorts alike.
Exposure to some of the following technologies: Python, AWS Redshift, AWS Athena / Apache Presto, Big Data technologies (e.g S3, Hadoop, Hive, Spark, Flink, Kafka etc), NoSQL systems like Cassandra, DBT.
What You'll Get
This opportunity has the option to be fully remote in all locations across Canada.
You can find more information about Yelp's five star benefits here!
Closing
At Yelp, we believe that diversity is an expression of all the unique characteristics that make us human: race, age, sexual orientation, gender identity, religion, disability, and education — and those are just a few. We recognize that diverse backgrounds and perspectives strengthen our teams and our product. The foundation of our diversity efforts are closely tied to our core values, which include “Playing Well With Others” and “Authenticity.”
We’re proud to be an equal opportunity employer and consider qualified applicants without regard to race, color, religion, sex, national origin, ancestry, age, genetic information, sexual orientation, gender identity, marital or family status, veteran status, medical condition, disability, or any other protected status.
We are committed to providing reasonable accommodations for individuals with disabilities in our job application process. If you need assistance or an accommodation due to a disability, you may contact us at accommodations-recruiting@yelp.com or 1-415-969-8488.
Note: Yelp does not accept agency resumes. Please do not forward resumes to any recruiting alias or employee. Yelp is not responsible for any fees related to unsolicited resumes.
Recruiting and Applicant Privacy Notice
Show more
Show less","AWS Athena, Apache Presto, AWS Redshift, Data warehousing, Apache Flink, Apache Hive, Scalable data systems, Data modeling, Data integration, ETL pipelines, Apache Kafka, Cassandra, Hadoop, Spark, Big data, Python, NoSQL, Hive, S3","aws athena, apache presto, aws redshift, data warehousing, apache flink, apache hive, scalable data systems, data modeling, data integration, etl pipelines, apache kafka, cassandra, hadoop, spark, big data, python, nosql, hive, s3","apache flink, apache hive, apache kafka, apache presto, aws athena, aws redshift, big data, cassandra, data integration, datamodeling, datawarehouse, etl pipelines, hadoop, hive, nosql, python, s3, scalable data systems, spark"
Data Engineer,Acunor,Canada,https://ca.linkedin.com/jobs/view/data-engineer-at-acunor-3774810353,2023-12-17,Delta, Canada,Mid senior,Remote,"Acunor is hiring for Data Engineer role, remote canada interested candidates can apply your resume at rahul@acunor.com
Title: Data Engineer- customer data platform
Location: Canada (Remote)
Duration: Long Term
JD:
Job Responsibilities:
Role and Responsibilities:
Customer Data Asset Stewardship (CDA, including configuration modification/change to source(s), stitch, etc.)
Analysis and remediation within the Customer Data Assets
Coordination and tracking with Source Data Stewards to ensure source remediation is performed
Administrating access management for Customer Data Asset to Direct and Indirect Users
Ensuring the CDA is used only for approved uses
Create the data integration and data diagram documentation
Lead the data validation, UAT and regression test for new data asset creation
Create and maintain data models, including schema design and optimization
Work with stakeholders to understand the data requirements to design, develop, and maintain complex ETL processes
Create and manage data pipelines that automate the flow of data, ensuring data quality and consistency
Requirement/Competencies:
Bachelor’s or Master’s (preferred) degree in in a quantitative or technical field such as Computer Science, Information Technology, Computer Engineering or equivalent
4+ years of experience in Data Engineering, Data Governance and working with Customer Data Platforms (CDP)
Knowledge of all popular CDPs and expert working knowledge of at least one CDP
Knowledge of market trends including cloud technology
Hands on data stewardship experience
Strong working knowledge of SQL, Hadoop, Hive, Python and PySpark
Demonstrated interest in learning about emerging data platforms and Big Data technologies
Show more
Show less","Data Engineering, Data Governance, Customer Data Platform (CDP), Cloud Technology, Data Stewardship, SQL, Hadoop, Hive, Python, PySpark, Emerging Data Platforms, Big Data Technologies","data engineering, data governance, customer data platform cdp, cloud technology, data stewardship, sql, hadoop, hive, python, pyspark, emerging data platforms, big data technologies","big data technologies, cloud technology, customer data platform cdp, data engineering, data governance, data stewardship, emerging data platforms, hadoop, hive, python, spark, sql"
Senior Data Engineer,Tiger Analytics,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/senior-data-engineer-at-tiger-analytics-3782502560,2023-12-17,Delta, Canada,Mid senior,Remote,"Tiger Analytics is a fast-growing advanced analytics consulting firm. Our consultants bring deep expertise in Data Science, Machine Learning and AI. We are the trusted analytics partner for several Fortune 100 companies, enabling them to generate business value from data. Our business value and leadership has been recognized by various market research firms, including Forrester and Gartner. We are looking for top-notch talent as we continue to build the best analytics global consulting team in the world.
The Data Engineer will be responsible for architecting, designing, and implementing advanced analytics capabilities. The right candidate will have broad skills in database design, be comfortable dealing with large and complex data sets, have experience building self-service dashboards, be comfortable using visualization tools, and be able to apply your skills to generate insights that help solve business challenges. We are looking for someone who can bring their vision to the table and implement positive change in taking the company's data analytics to the next level.
Responsibilities:
A technologist and engineer at heart and be comfortable in enabling new technology and being hands on with the execution of the strategy. They must exhibit a deep understanding of modern AWS Data stack and agile delivery models, demonstrated focus on building business data pipelines
Functions as senior member of an agile team and helps drive consistent development practices, tools usage, common components, and patterns
Direct the identification and recommendation of appropriate solutions
Performs hands-on design and development with PySpark/Pandas/SQL, AWS Glue for ETL and Catalog, Datasync, Parquet files, Warehousing experience preferably with Snowflake and/or Redshift
Spends significant amount of time writing code and testing
Requirements
8+ years of overall industry experience specifically in data engineering
5+ years of experience building and deploying large-scale data processing pipelines in a production environment
Strong experience in building ETL data pipelines and analysis using Python, SQL, and PySpark
In-depth knowledge of data engineering concepts, techniques, and best practices
Creating and optimizing complex data processing and data transformation pipelines using python
Knowledge of programming languages in data engineering such as Python or PySpark
Experience with ETL tools and frameworks (e.g., Apache NiFi, Talend, AWS Glue, Azure Data Factory)
Knowledge of big data platforms like Snowflake, DBT, AWS Redshift, Postgres, MongoDB, and Hadoop
Experience with data pipeline and workflow management tools
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases
Understanding of Datawarehouse (DWH) systems, and migration from DWH to data lakes/Snowflake
Understanding of ELT and ETL patterns and when to use each. Understanding of data models and transforming data into the models
Strong analytic skills related to working with unstructured datasets
Cloud certification (AWS Certified Big Data - Specialty, Microsoft Certified: Azure Data Engineer) is a plus
Build processes supporting data transformation, data structures, metadata, dependency and workload management
Experience supporting and working with cross-functional teams in a dynamic environment
Benefits
Significant career development opportunities exist as the company grows. The position offers a unique opportunity to be part of a small, challenging, and entrepreneurial environment, with a high degree of individual responsibility.
Show more
Show less","Data Science, Machine Learning, AI, Database Design, Data Visualization, AWS Data Stack, Agile Development, PySpark, Pandas, SQL, AWS Glue, ETL, Catalog, Datasync, Parquet Files, Warehousing, Snowflake, Redshift, Python, Apache NiFi, Talend, Azure Data Factory, Big Data Platforms, DBT, Hadoop, Data Pipeline Management Tools, SQL, Relational Databases, Datawarehouse Systems, ELT, ETL, Data Models, Analytic Skills, Unstructured Datasets, AWS Certified Big Data  Specialty, Microsoft Certified: Azure Data Engineer","data science, machine learning, ai, database design, data visualization, aws data stack, agile development, pyspark, pandas, sql, aws glue, etl, catalog, datasync, parquet files, warehousing, snowflake, redshift, python, apache nifi, talend, azure data factory, big data platforms, dbt, hadoop, data pipeline management tools, sql, relational databases, datawarehouse systems, elt, etl, data models, analytic skills, unstructured datasets, aws certified big data specialty, microsoft certified azure data engineer","agile development, ai, analytic skills, apache nifi, aws certified big data specialty, aws data stack, aws glue, azure data factory, big data platforms, catalog, data models, data pipeline management tools, data science, database design, datasync, datawarehouse, datawarehouse systems, dbt, elt, etl, hadoop, machine learning, microsoft certified azure data engineer, pandas, parquet files, python, redshift, relational databases, snowflake, spark, sql, talend, unstructured datasets, visualization"
Sr Data Engineer,TEEMA,Canada,https://ca.linkedin.com/jobs/view/sr-data-engineer-at-teema-3781199444,2023-12-17,Delta, Canada,Mid senior,Remote,"Are you a Data Engineering professional that can work independently? Are you skilled with AWS and enjoy working at high-growth startups?
If yes, read on. My client, a leader in the SportsTech industry in Canada is scaling rapidly and looking for Senior Data Engineers. They are building a cloud-based platform and DW SAAS solutions for clients in Sports and Entertainment. Seeking seasoned Data Engineers with deep AWS expertise and who can hit the ground running.
This is a permanent role and can be remote anywhere in Canada.
What you'll do:
Take ownership of building and optimising products and features
Build and test data pipelines, work on data migration
Design large, complex data solutions to meet business needs
Create relevant documentation, conduct code reviews
You will need:
At least 5-8 years of experience as a Senior Data Engineer - preferably with a start up or product company
At least 3 plus years of experience with AWS and related services
Well-versed with ETL, databases and Python scripting
Ability to hit the ground running in a fast paced environment
Apply today to get the conversation started!
Show more
Show less","Data Engineering, AWS, Cloud Computing, DW SAAS, Data Pipelines, Data Migration, Data Solutions, ETL, Databases, Python, Scripting","data engineering, aws, cloud computing, dw saas, data pipelines, data migration, data solutions, etl, databases, python, scripting","aws, cloud computing, data engineering, data migration, data solutions, databases, datapipeline, dw saas, etl, python, scripting"
Data Center Engineer,Thinkpivot Talent Solutions,Canada,https://ca.linkedin.com/jobs/view/data-center-engineer-at-thinkpivot-talent-solutions-3776744737,2023-12-17,Delta, Canada,Mid senior,Remote,"We are recruiting a Veeam Certified Engineer with client-facing experience for a professional services company providing:
Infrastructure as a Service (Cloud)
Cloud Backup
Disaster Recovery as a Service
Colocation
Network Connectivity
Key Responsibilities
Data centre operations (service provisioning, customer service, ticket management, troubleshooting, monitoring, etc.)
IT infrastructure implementation, maintenance and troubleshooting (network, storage, and compute)
Assisting with the operation, monitoring and maintenance of data centre infrastructure
Participate in on-call rotation
Change management and documentation
Providing end-user support to both customers and employees
Assist with the design and implementation of new systems and services
Requirements & Skills:
Must have a current Veeam certification.
Must have managed colocation experience from within your company's own data centre facility
And practical experience in Linux Systems Administration as well as at least two of the following:
Storage administration (including FC/FCoE, iSCSI, and NAS protocols)
Virtual infrastructure (VMWare, Hyper-V, or other)
System Administration (Windows)
IP networks (including LAN, WAN, BGP, VPN, and wireless technologies)
Security (Firewall, Network)
Show more
Show less","Veeam, Cloud, Backup, Disaster Recovery, Colocation, Network Connectivity, Service Provisioning, Customer Service, Troubleshooting, Monitoring, IT Infrastructure, Storage, Compute, Linux Systems Administration, Storage Administration, Virtual Infrastructure, Windows Systems Administration, IP Networks, Security, Firewall, BGP, VPN","veeam, cloud, backup, disaster recovery, colocation, network connectivity, service provisioning, customer service, troubleshooting, monitoring, it infrastructure, storage, compute, linux systems administration, storage administration, virtual infrastructure, windows systems administration, ip networks, security, firewall, bgp, vpn","backup, bgp, cloud, colocation, compute, customer service, disaster recovery, firewall, ip networks, it infrastructure, linux systems administration, monitoring, network connectivity, security, service provisioning, storage, storage administration, troubleshooting, veeam, virtual infrastructure, vpn, windows systems administration"
Pyspark Data Engineer,Iris Software Inc.,Canada,https://ca.linkedin.com/jobs/view/pyspark-data-engineer-at-iris-software-inc-3776629850,2023-12-17,Delta, Canada,Mid senior,Remote,"Iris's client, one of the world's largest financial institutions, is currently searching for a strong
Data Engineer
with strong experience in
Pyspark
based in
Mississauga, ON
to join their team.
Job Title: Pyspark Data Engineer
Location: Hybrid (Mississauga, ON)
Duration: 18 Months
Skills: Pyspark, ETL, DataEngineer.
Job Description:
PySpark is the primary skillset
Ab initio knowledge/experience/willingness to pick up is secondary
Experience in ETL is must
Thanks and Regards,
Mohit Singh
Iris Software
200 Metroplex Drive, Suite #300
Edison, NJ 08817
Mohit.singh@irissoftware.com
www.irissoftware.com
Show more
Show less","Pyspark, ETL, Data Engineering, Ab Initio","pyspark, etl, data engineering, ab initio","ab initio, data engineering, etl, spark"
GCP Data Engineer,LTIMindtree,"Mississauga, Ontario, Canada",https://ca.linkedin.com/jobs/view/gcp-data-engineer-at-ltimindtree-3782414358,2023-12-17,Delta, Canada,Mid senior,Remote,"About Us:
LTIMindtree
is a global technology consulting and digital solutions company that enables enterprises across industries to reimagine business models, accelerate innovation, and maximize growth by harnessing digital technologies. As a digital transformation partner to more than 700+ clients, LTIMindtree brings extensive domain and technology expertise to help drive superior competitive differentiation, customer experiences, and business outcomes in a converging world. Powered by nearly 90,000 talented and entrepreneurial professionals across more than 30 countries, LTIMindtree — a Larsen & Toubro Group company — combines the industry-acclaimed strengths of erstwhile Larsen and Toubro Infotech and Mindtree in solving the most complex business challenges and delivering transformation at scale. For more information, please visit www.ltimindtree.com.
Job Title:
GCP Data Engineer
Work Location:
Remote
Job Description:
Bachelor’s degree in computer science or a related discipline.
5+ years of applicable engineering experience.
Experience in GCP, Big Query.
Strong proficiency in Python with an emphasis in building data pipelines.
Experience working with third-party APIs.
Ability to write complex SQL to perform common types of analysis and aggregations.
Experience with Apache Airflow or Google Composer.
Detail-oriented and document all the work.
Ability to work with others from diverse skill sets and backgrounds.
GCP solution architect – certified.
Benefits/perks listed below may vary depending on the nature of your employment with LTIMindtree Canada (“LTIMC”):
Benefits and Perks:
Comprehensive Medical Plan Covering Medical, Dental, Vision
Health Care Spending Account
Short Term and Long-Term Disability Coverage
Life Insurance
Annual vacation and other Paid Leaves
Maternity Leave Top Up Pay
The range displayed on each job posting reflects the minimum and maximum salary target for the position across all Canada locations. Within the range, individual pay is determined by work location and job level and additional factors including job-related skills, experience, and relevant education or training. Depending on the position offered, other forms of compensation may be provided as part of overall compensation like an annual performance-based bonus, sales incentive pay and other forms of bonus or variable compensation.
Disclaimer:
The compensation and benefits information provided herein is accurate as of the date of this posting.
LTIMindtree
is an equal opportunity employer that is committed to diversity in the workplace. Our employment decisions are made without regard to race, color, creed, religion, sex (including pregnancy, childbirth or related medical conditions), gender identity or expression, national origin, ancestry, age, family-care status, veteran status, marital status, civil union status, domestic partnership status, military service, handicap or disability or history of handicap or disability, genetic information, atypical hereditary cellular or blood trait, union affiliation, affectional or sexual orientation or preference, or any other characteristic protected by applicable federal, state, or local law, except where such considerations are bona fide occupational qualifications permitted by law.
Safe return to office
:
In order to comply with LTIMindtree’ s company COVID-19 vaccine mandate, candidates must be able to provide proof of full vaccination against COVID-19 before or by the date of hire. Alternatively, one may submit a request for reasonable accommodation from LTIMindtree’s COVID-19 vaccination mandate for approval, in accordance with applicable state and federal law, by the date of hire. Any request is subject to review through LTIMindtree’s applicable processes.
Show more
Show less","GCP, Big Query, Python, Data pipelines, Thirdparty APIs, SQL, Apache Airflow, Google Composer, GCP solution architect","gcp, big query, python, data pipelines, thirdparty apis, sql, apache airflow, google composer, gcp solution architect","apache airflow, big query, datapipeline, gcp, gcp solution architect, google composer, python, sql, thirdparty apis"
Senior Data Engineer,Zinnia,"Montreal, Quebec, Canada",https://ca.linkedin.com/jobs/view/senior-data-engineer-at-zinnia-3758743084,2023-12-17,Delta, Canada,Mid senior,Remote,"WHO WE ARE:
Zinnia Open Insurance is advanced data-driven technology that accelerates innovation, enables unrivaled speed-to-market, deepens relationships with policyholders, and empowers better risk-based outcomes. Open transforms the end-to-end experience to make everything faster, more transparent, and more collaborative for consumers, carriers, and advisors. Zinnia is a new tech company, but we're not new to insurance. We're L&A experts and technology innovators building Open Insurance to power exceptional experiences that evolve ahead of the marketplace. We're Zinnia. And we're Open. Join us, and experience Life, More Open.
WHO YOU ARE:
Zinnia is disrupting the insurance industry by delivering innovative technology-driven experiences. We are advancing our tech capabilities and learning to leverage our data to develop innovative data applications. We are relentless in our drive to reliably deliver outstanding products at scale. We are growing fast, but we can go further faster with experienced, collaborative, challenge-seeking analytics leaders like yourself.
Our data team serves Zinnia through data engineering, data analysis, and data science. Our goal is to help uncover opportunities and make decisions with data. We partner with all department stakeholders across the company to develop deeper predictors of behavior, develop insights that drive business strategy and build solutions to optimize our internal and external experiences.
WHAT YOU'LL DO:
Take complicated algorithms, code review, optimize for production and integrate into product features or process flows.
Design data architecture that is simple and maintainable while enabling Data Analysts, Data Scientists, and stakeholders to efficiently work with data.
Design data pipelines utilizing ETL tools, event driven software, and other streaming software.
Partner with both data scientists and engineers to bring our amazing concepts to reality. This requires learning to speak the language of statisticians as well as software engineers.
Ensure reliability in data pipelines and enforce data governance, security and protection of our customer's information while balancing tech debt.
Mentor data engineering team members and even data scientists in architecture and coding techniques.
Partner with product and engineering teams to design data models for downstream data maximization.
Evaluate and champion new engineering tools that help us move faster and scale our team
WHAT YOU'LL NEED:
You have 7+ years of data engineer experience coding in Python and using SQL.
Bachelor's degree in Technology or Data Science or a related filed, work experience can be substituted for education
You are comfortable with data engineering tooling such as jira, git, buildkite, terraform, airflow, dbt and containers as well as GCP tools such as kubernetes, cloud functions, and data store, and vertex ai.
You understand standard ETL patterns, modern data warehousing ideas such as data mesh or data vaulting, and data quality practices regarding test driven design and data observability.
You have some experience with ML Operations
You have experience partnering with data analysts, data scientists, and engineering stakeholders.
You have a background in data engineering, computer science, software development or another engineering field
You can communicate with a team and articulate ideas to both team members and non-technical stakeholders
You have a drive to learn and master new technologies and techniques
You have experience with relational cloud databases like MSSQL, Postgres, Oracle and BigQuery, but also comfortable working with unstructured files and datasets as well as 3rd party data sources and vendors
Technologies You Will Use
Python for data pipelining and automation.
Google Cloud Platform: Kubernetes, Cloud SQL, Cloud Functions, PubSub, BigQuery, DataStore, and more: we keep adopting new tools as we grow!
Airflow and dbt for data pipelining.
Tableau and Narrator for data visualization and consumer facing dashboards.
Many more to come!
WHAT'S IN IT FOR YOU?
The opportunity to work with a fantastic team with a great culture;
The opportunity to be part of an innovative and exciting company that is completely changing an outdated industry;
The opportunity to grow professionally in a dynamic and fun environment;
A competitive compensation plan;
Hybrid work from home and office model;
A flexible time off policy which includes unlimited wellness days, personal days and vacation time;
Two volunteer days;
A day off on your Birthday;
The company paid top-tier health benefits plan and a telemedicine service;
A stipend for your home office setup.
Show more
Show less","Python, SQL, Jira, Git, Buildkite, Terraform, Airflow, DBT, Containers, Kubernetes, Cloud Functions, Data Store, Vertex AI, Data mesh, Data vaulting, Test driven design, Data observability, ML Operations, Relational cloud databases, MSSQL, Postgres, Oracle, BigQuery, Tableau, Narrator","python, sql, jira, git, buildkite, terraform, airflow, dbt, containers, kubernetes, cloud functions, data store, vertex ai, data mesh, data vaulting, test driven design, data observability, ml operations, relational cloud databases, mssql, postgres, oracle, bigquery, tableau, narrator","airflow, bigquery, buildkite, cloud functions, containers, data mesh, data observability, data store, data vaulting, dbt, git, jira, kubernetes, ml operations, mssql, narrator, oracle, postgres, python, relational cloud databases, sql, tableau, terraform, test driven design, vertex ai"
Senior Data Engineer (AWS),Quantiphi,Canada,https://ca.linkedin.com/jobs/view/senior-data-engineer-aws-at-quantiphi-3779147019,2023-12-17,Delta, Canada,Mid senior,Remote,"About us:
Quantiphi is an award-winning Applied AI and Big Data software and services company, driven by a deep desire to solve transformational problems at the heart of businesses. Our signature approach combines groundbreaking machine-learning research with disciplined cloud and data-engineering practices to create breakthrough impact at unprecedented speed.
Some company highlights:
Delivered 2.5x growth YoY since its inception in 2013
Headquartered in Boston, with 4000+ Quantiphi professionals across the globe
Great Places to Work certified for 2 consecutive years- 2022, 2021
Recognized by Everest Group as Specialist Leader and Star Performer in Analytics and AI Services, 2022
Recognized as an AIFinTech100 Company, 2022 by InsurTech
Winner of Best in Business Award in Established Business category by INC., 2022
Winner of Competitive Strategy Leadership Award in Artificial Intelligence Services in Healthcare by Frost & Sullivan, 2022
Recognized in Gartner Hype Cycle Reports for AI Strategy, 2022
Winner of 2021 Google Cloud Breakthrough Partner of the Year- North America
Winner of 2021 AWS Canada Rising Star of the Year
Recognized as Leader in IDC MarketScape: WorldWide AI IT Services, 2021
Recognized in the Fast Company 2021 World Changing Ideas- AI and Data category
Winner of NVIDIA's Americas Service Delivery Partner of the Year, 2021
Job Description:
Experience Level: 5 years
Role & Responsibilities:
Design, develop and implement highly scalable data capture and transformation processes
Create effective ETLs/ELTs to move large volumes of data from various operational systems to dimensional data models for analytics consumption
Act as principal designer and reviewer for new data models, make data architectural decision, and provide coaching on data modeling and process design
Develop, and maintain conceptual, logical and physical data models
Work with the database engineering and DBAs to create optimal physical data models (transactions, normalized and dimensional models etc.)
Manage data modeling repository, data modeling process and support model driven development
Define and govern data modeling/design standards, tools, best practices, and related development methodologies
Champion Data Lineage, Metadata Management and Data quality Analysis processes
Collaborate with Business Data Analysts to design and model Business Intelligence Semantic layers for optimized information access
Seamlessly able to translate business requirements to technology requirements
Expand and grow data existing platform capabilities to solve new data problems and challenges
Ensure all automated processes preserve data integrity by managing the alignment of data availability and integration processes
Support quantitative analysts and data scientists with data discovery and rapid assembly of large data sets from disparate sources
Identify opportunities for new data acquisition and new uses for existing data resources
Research and make recommendations for new data management technologies and software engineering practices. Collaborate on decisions around the use of new tools and practices
Define data retention policy, establish data governance best practice, and create automated anomaly detection services
Document and update business continuity and disaster recovery procedures
Engage in ongoing collaboration with data architects, modelers and other members to achieve common goals
Provide guidance to development teams regarding best practices and design patterns for analytics solutions. Coach and provide guidance to junior team members
Produce and maintain support documentation for ongoing operations.
Act as support to troubleshoot and resolve technical issues with production data models and services
Excellent communication, influencing and facilitation skills, in particular for problem solving/troubleshooting activities
Experience of client-facing roles
Assertiveness in dealing with people at all levels within and outside of the delivery organization, including partner and client organizations
Required Skills:
●
Minimum of 5 years of experience in IT industry and Data Engineering.
● Hands on experience with Python, Pyspark
● Must have experience in AWS Cloud
● Must be completed 5 years stay in Canada.
What is in it for you:
Be part of the fastest-growing AI-first digital transformation and engineering company in the world
Be a leader of an energetic team of highly dynamic and talented individuals
Exposure to working with fortune 500 companies and innovative market disruptors
Exposure to the latest technologies related to artificial intelligence and machine learning, data and cloud
Show more
Show less","Python, Pyspark, ETL, ELT, Data Lineage, Metadata Management, Modeling, Data Analysis, AWS Cloud, Data Retention Policy, Anomaly Detection, Data Governance","python, pyspark, etl, elt, data lineage, metadata management, modeling, data analysis, aws cloud, data retention policy, anomaly detection, data governance","anomaly detection, aws cloud, data governance, data lineage, data retention policy, dataanalytics, elt, etl, metadata management, modeling, python, spark"
Database Engineer,Power Staffing Solutions,Canada,https://ca.linkedin.com/jobs/view/database-engineer-at-power-staffing-solutions-3768708469,2023-12-17,Delta, Canada,Mid senior,Remote,"Position:
Database Engineer
Location:
Remote (Canada)
Tenure:
Full-time, Contract
Position Overview:
Our healthcare industry client is looking for a contracted Database Engineer to join their team on an ongoing contract.
What you’ll be doing:
Developing and implementing IAM systems, including authentication, SSO, access controls ensuring confidentiality, integrity, and availability of IAM systems and data
Administer and maintain database security, integrity, and optimization.
Develop periodic and requested reports and new databases and applications.
Identify and resolve performance and capacity issues.
Maintain backup and recovery processes and securely store, back-up, and archive data.
Meticulously document processes related to database design and archiving data.
Must Haves:
Post-secondary education in Computer Science or related
5+ years of experience in a similar role
Experienced with MySQL database queries and creation of database views
Working experience with Java, REST APIs, SOAP, and LDAP
Experience with Radiant Logic HDAP, Industrial Control Systems, or Federated Identity is a strong asset
Experience in a large enterprise organization is an asset
Show more
Show less","Database Engineering, IAM Systems, Authentication, Single SignOn, Access Controls, Database Security, Database Optimization, Performance Tuning, Capacity Planning, Backup and Recovery, Data Archiving, Computer Science, MySQL, Java, REST APIs, SOAP, LDAP, Radiant Logic HDAP, Industrial Control Systems, Federated Identity, Enterprise Organization","database engineering, iam systems, authentication, single signon, access controls, database security, database optimization, performance tuning, capacity planning, backup and recovery, data archiving, computer science, mysql, java, rest apis, soap, ldap, radiant logic hdap, industrial control systems, federated identity, enterprise organization","access controls, authentication, backup and recovery, capacity planning, computer science, data archiving, database engineering, database optimization, database security, enterprise organization, federated identity, iam systems, industrial control systems, java, ldap, mysql, performance tuning, radiant logic hdap, rest apis, single signon, soap"
Data Engineer,Terminal,Canada,https://ca.linkedin.com/jobs/view/data-engineer-at-terminal-3775347278,2023-12-17,Delta, Canada,Mid senior,Remote,"About Plato Systems
Plato unlocks the power of digital transformation and industrial automation through its integrated Spatial Intelligence (TM) system and platform. Plato Deep Fusion perception technology tracks activity patterns of people, equipment, and assets in industrial environments – making it the world’s first tagless activity tracking system. Plato provides a reliable, frictionless, and scalable automation solution that delivers actionable KPIs so businesses can improve safety, optimize process efficiency, and increase productivity.
About The Role
Plato Systems is hiring a dedicated Data Engineer for a full-time role, emphasizing deep domain engagement to understand and address the intricacies of our business environment. The position focuses on creating and maintaining automated reporting, dashboards, and thorough analysis to drive data-driven decisions across teams. You will contribute to generating business insights through adept statistical analysis, identifying and articulating key insights. We seek an individual with technical proficiency and a keen understanding of our business, aiming to enhance our data-driven capabilities and contribute to Plato Systems' overall success.
What You'll Do
Develop and maintain ETL pipelines: Create ETL data pipelines using SQL, and PySpark, while ensuring data quality. Familiarity with Databricks is preferred.
What You'll Bring
Prior experience working with processing (normalizing, aggregating, aligning, correlating, ...) time-series data e.g. from sensors, machines, processes
Demonstrated experience with working on production of data products, including handling data quality issues, orchestration and automation, & testing
Excellent SW development practices
Show more
Show less","Data Engineering, ETL Pipelines, Data Quality, SQL, PySpark, Databricks, TimeSeries Data, Processing Data, Data Products, Software Development Practices","data engineering, etl pipelines, data quality, sql, pyspark, databricks, timeseries data, processing data, data products, software development practices","data engineering, data products, data quality, databricks, etl pipelines, processing data, software development practices, spark, sql, timeseries data"
Senior Data Engineer,Lever Middleware Test Company 2,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/senior-data-engineer-at-lever-middleware-test-company-2-3787339116,2023-12-17,Delta, Canada,Mid senior,Remote,"a Bit About Us
Lever, an Employ brand, has a vision of connecting human potential to meaningful work. Hiring is the most strategic challenge that every company faces, and Lever is one of the most recognized brands in talent acquisition software. As the innovation leader in our space, we are searching for great people to join us and push further.
With an overall gender ratio of 50:50 women and men, Lever is also fiercely committed to building a team culture that celebrates diversity and inclusion. We have been certified as a “Great Place to Work”, along with “Best Company Outlook 2022,” and “Forbes Best Startup Employers 2022,” and the list keeps growing! Our people are Lever’s biggest competitive advantage. For more information on our team culture, visit https://inside.lever.co/.
THE CHALLENGE
Lever is on a mission to provide the best analytics feature set in the world of Talent software. We’ve made good progress in the last 2 years, but we still have room to improve.
This is where you come in. You will be the technical lead for the engineering team that is evolving Lever’s analytics features. You will help Lever’s customers recruit more efficiently and more predictably by providing them with a reliable and accurate data set to analyze.
This position will focus on maintaining and improving our data pipelines while consulting with partners in product management and product design on building user facing analytics features. As part of that collaboration, you will be encouraged to use your expertise and conduct technical research to help inform the direction we take with our analytics products.
THE OPPORTUNITY
One of Lever’s top priorities is to make our analytics features a reason that people buy and stay with Lever. As the lead engineer on the Analytics team you will have the opportunity to have a big impact on the success of Lever’s product and Lever’s business. You’ll work collaboratively with partners on the analytics team to drive the implementation of features, and be a champion of best practices for writing well-tested, well-organized code.
In addition to working on a high impact product area, as part of your daily work you will have the opportunity to build expertise in multiple Business Intelligence tools. You will be able to exercise and strengthen your automation and optimization skills. You will have the opportunity to learn about state of the art data infrastructure systems like Kafka and Snowflake, and you will be able to leverage observability tools like Datadog and Kibana to observe our systems.
You will also have the opportunity to lead the development of user facing analytics features, depending on your preferences and prior experience.
THE TEAM
Lever’s reporting engineering team combines ETL engineers, product engineers, data analysts, designers, and product managers to build, define, and design analytics products for customers. You will join the team as the lead software engineer building and maintaining advanced analytics products for Lever’s customers.
You will collaborate closely with our brilliant data analysts who create dashboards in the product, and scrappy Talent Analytics Consultants provide custom analysis services for customers. You will use your data and engineering knowledge to help guide the product and design team that strives to build analytics features to connect human potential to meaningful work.
THE SKILL SET
6+ years of software engineering experience
Experience with data pipelines and both the common pitfalls and best practices for maintaining them is a plus
Professional experience with relational databases that speak SQL (Postgres, MySql, MSSql, Oracle, Snowflake, etc)
An understanding of Business Intelligence tools, like Tableau or Looker, and the problems they solve
Knowledge of Looker is a plus, a desire to learn Looker is a must
A desire to teach others with the communication skills needed to effectively share knowledge
A healthy appetite for being a technical mentor for other engineers
This role includes a lot of technical freedom, so the ability to research technical solutions beyond those that already exist in our code base is a plus
Empathy for users of the Lever product and internal users that work at Lever
WITHIN 1 MONTH, YOU'LL
Complete Ramp Camp, our onboarding program designed to get you up to speed on our business, vision, and team.
Learn about the technology behind Lever’s analytics features (Lever Talent Intelligence, Data Warehouse Sync, and Visual Insights)
Participate in the reporting team’s ceremonies and get to know your teammates
Meet internal users of Business Intelligence tools
Deploy your first code change
WITHIN 3 MONTHS, YOU'LL
Learn about Lever’s product and how the analytics feature fit into the full picture
Build familiarity with Lever’s application data model in Mongo, how it aligns with data in SQL, and how the our data pipelines perform the translation
Understand the existing observability tools and monitoring we have in place for our data pipelines
Participate in Lever’s analytics product development process and ship your first feature to customers
Improve tools that reporting engineers use to maintain data systems
WITHIN 6 MONTHS, YOU'LL
Find and fill gaps in observability and automated testing for reporting features
Engage in your first on-call rotation for the product to help diagnose and resolve production problems and customer issues
Collaborate with other product engineering teams to incorporate data generated by new features into our analytics products
Identify and fix large areas of improvement in our data pipelines and schema management tool set
Contribute new ideas to the long-term roadmap of the reporting team
WITHIN 12 MONTHS, YOU'LL
Help the data analysts improve performance of pre-built Looker dashboards with optimizations and pre-calculated aggregations
Improve the experience of data analysts by providing them with better data environments to use for development
Automate continuous data quality and end-to-end service testing of data pipelines
Incorporate custom access control rules into the data available to end users of Lever
Is this role not an exact fit?
Sign up to stay in touch, we'll let you know when we have new positions on the team.
The Lever Story
Lever, an Employ brand, is a leading Talent Acquisition Suite that makes it easy for talent teams to reach their hiring goals and to connect companies with top talent. Lever is the only platform that provides all talent acquisition leaders with complete ATS and robust CRM capabilities in one product, LeverTRM. The LeverTRM features allow leaders to scale and grow their people pipeline, build authentic and long-lasting relationships, and source the right people to hire. Lever Analytics provides customized reports with data visualization, see offers completed and interview feedback, and more, to inform strategic decisions between hiring managers and executives alike.
Our platform also enables companies to hire with inclusivity in mind, helping eliminate any hiring bias. Lever supports the hiring needs of over 5,000 companies around the globe including the teams at Netflix, Spotify, Atlassian, KPMG, and Nielsen. For more information, visit https://www.lever.co.
California residents applying for positions at Lever can see our privacy policy here.
#BI-REMOTE
Show more
Show less","Software Engineering, Data Pipelines, SQL, Business Intelligence Tools, Tableau, Looker, Technical Mentoring, Technical Research, Observability Tools, Kafka, Snowflake, Datadog, Kibana, Mongo, Data Analytics, Data Quality, EndtoEnd Service Testing, Data Access Control, ETL, Postgres, MySQL, MSSQL, Oracle","software engineering, data pipelines, sql, business intelligence tools, tableau, looker, technical mentoring, technical research, observability tools, kafka, snowflake, datadog, kibana, mongo, data analytics, data quality, endtoend service testing, data access control, etl, postgres, mysql, mssql, oracle","business intelligence tools, data access control, data quality, dataanalytics, datadog, datapipeline, endtoend service testing, etl, kafka, kibana, looker, mongo, mssql, mysql, observability tools, oracle, postgres, snowflake, software engineering, sql, tableau, technical mentoring, technical research"
Senior Data engineer,Vaco,"Montreal, Quebec, Canada",https://ca.linkedin.com/jobs/view/senior-data-engineer-at-vaco-3779362813,2023-12-17,Delta, Canada,Mid senior,Remote,"BILINGUALISM (FR & ENG) IS REQUIRED.
Senior Data Engineer Opportunity
About the Company:
Join a leading organization in the e-training and high-tech industry, boasting a global workforce of 10,000+ employees. The company, well-established in its field, prides itself on its innovative approaches in critical sectors.
About the Role:
As a Senior Data Engineer, you will play a crucial role in designing, building, and maintaining scalable and reliable data pipelines on modern cloud platforms. Your expertise will be pivotal in managing manufacturing data, transitioning it from on-premises storage to cloud-based solutions for developer consumption. This remote position offers a chance to collaborate with a team of about 20 members.
Key Responsibilities:
Design and develop data pipelines on platforms like AWS, GCP, and Azure.
Collaborate with data architects, scientists, and analysts to meet business data needs.
Create and manage automated data workflows.
Build and maintain data infrastructure, including ETL pipelines, data warehousing, and lakes.
Optimize pipelines for performance and reliability.
Troubleshoot and debug data issues.
Keep up-to-date with emerging trends in data engineering.
Requirements:
Bachelor's or Master's degree in Computer Science, Information Systems, or related.
At least 8 years of experience in data engineering or software engineering.
Proficiency in Databricks, PySpark, and SQL.
Deep understanding of AWS, GCP, and Azure.
Knowledge of big data technologies like Hadoop, Spark, Kafka.
Familiarity with software engineering practices and agile methodologies.
Strong communication and problem-solving skills.
Preferred Qualifications:
Experience with containerization technologies and data visualization tools.
Familiarity with machine learning, AI technologies, and DevOps practices.
Background in regulated industries.
Benefits:
Flexible working hours, 40 hours per week. REMOTE
Opportunity to work with cutting-edge technology and contribute to key projects in the e-training and high-tech industry.
Application Process:
The hiring process includes a 30-minute screening followed with a technical interview.
Show more
Show less","Data Engineering, Data Pipelines, AWS, GCP, Azure, Databricks, PySpark, SQL, Hadoop, Spark, Kafka, Agile, DevOps, Machine Learning, AI, Data Visualization","data engineering, data pipelines, aws, gcp, azure, databricks, pyspark, sql, hadoop, spark, kafka, agile, devops, machine learning, ai, data visualization","agile, ai, aws, azure, data engineering, databricks, datapipeline, devops, gcp, hadoop, kafka, machine learning, spark, sql, visualization"
Senior Data Acquisition Engineer,Navigator - Powered By LifeRaft,"Halifax, Nova Scotia, Canada",https://ca.linkedin.com/jobs/view/senior-data-acquisition-engineer-at-navigator-powered-by-liferaft-3774269264,2023-12-17,Delta, Canada,Mid senior,Remote,"LifeRaft is looking to fill a Senior Data Acquisition Engineer role to help us continue to grow and expand the data sources our customers use to keep their people and assets safe.
Based out of Nova Scotia, Canada, LifeRaft was incorporated in 2014 with a clear objective to advance corporate security. We pride ourselves on our innovative spirit and determination to help solve new challenges developed by the complexities of open source data.
Our SaaS-based threat intelligence platform, called Navigator, is designed to identify, track, and validate issues from open source channels (surface, deep web, and darknet) related to executive safety, fraud prevention, and asset & infrastructure protection.
This individual will play a critical role in building on the already sophisticated open source data collection capabilities we have. You will partner closely with the engineering leadership, Manager, Data Acquisition and core product engineering teams to create new data integrations and further enhance existing ones to continue improving the quality and breadth of data we provide.
Why LifeRaft?
We pride ourselves on our innovative spirit and determination to help solve new challenges developed by the complexities of open source data. LifeRaft provides a threat intelligence and investigations platform, Navigator, to corporate security teams around the world, including some of the biggest brands you've probably referenced today! Navigator is designed to identify, track, and validate issues from open source channels (surface, deep web, and darknet) related to executive safety, fraud prevention, and asset & infrastructure protection. Our technology is helping keep these companies, their people, and their operations safe – making a real impact in the world we all live in.
The diversity of our team is integral to our success. We are a team of passionate and supportive individuals and pride ourselves in fostering a collaborative, innovative, and fun culture. We offer our team:
Competitive salary & benefits
Challenging and focused responsibilities
Lifestyle subsidy
Remote and in-person options (with dogs!)
Hilarious co-workers & great office parties
Mediocre office-supplied coffee
Real DEI
What You Do As a Senior Data Acquisition Engineer
Work closely with the Director, Data Acquisition and our product engineering teams to create new data source integrations
Design and build the next generation data integration platform efficiently and reliably to support different threat detection intelligence use cases
Develop and implement new API extraction methods to boost capability and efficiency of engineering teams and activities
Continuously analyze and optimize existing data sources to provide a more complete coverage with the lowest latency possible
Evaluate and propose new technologies and methodologies as needed to ensure we are aligned with industry best practices
Leverage agile practices, encourage collaboration, prioritization, and urgency to develop at a rapid pace
What You Need To Get The Job Done
At least 5 years of experience working with Python, Ruby, Go, PHP or similar programming languages with a focus on third party API integrations
5+ years of experience with MySQL with deep understanding of database schema design, analytics, and data modeling
Experience with ElasticSearch or other similar technologies is a plus
Experience working with structured and unstructured data
Experience working with a messaging/event solution such as Kafka or RabbitMQ
You are a creative problem solver who works well with others and has a passion for lifelong learning.
You love data!
Strong presentation and communication skills, with the ability to explain complex concepts to both technical and non-technical audiences
LifeRaft values inclusion and diversity in our workplace. We welcome all qualified applicants regardless of race, national or ethnic origin, colour, religion, age, sex, sexual orientation, gender identity or expression, marital status, family status, genetic characteristics, disability, or any other protected characteristic.
Show more
Show less","Python, Ruby, Go, PHP, MySQL, ElasticSearch, Kafka, RabbitMQ, API integrations, Data collection, Data modeling, Data analysis, Data optimization, Agile development, Presentation skills, Communication skills","python, ruby, go, php, mysql, elasticsearch, kafka, rabbitmq, api integrations, data collection, data modeling, data analysis, data optimization, agile development, presentation skills, communication skills","agile development, api integrations, communication skills, data collection, data optimization, dataanalytics, datamodeling, elasticsearch, go, kafka, mysql, php, presentation skills, python, rabbitmq, ruby"
Big Data Developer,Iris Software Inc.,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/big-data-developer-at-iris-software-inc-3784565563,2023-12-17,Delta, Canada,Mid senior,Hybrid,"Our direct end client which is
one of the multinational financial services company and the largest bank in Canada by market capitalization
is urgently looking to hire
Big Data Developer
.
Big Data Developer
Location: Downtown Toronto
Nature of Contract – Contract Role
Over 8 years of hands-on development experience
Experienced in Java/J2EE
, Scala, Shell Script, C, Python, C#
Experienced in one or more of DB2, Oracle, MySQL, MS SQL Server, No-SQL (HBase, Vertica, Casandra)
Experienced in Hadoop
, Spark, MapReduce, Hive, Kafka, Knox, Oozie, Solr, Elasticsearch
Focus on Kafka, API, APGEE, Micro-services, general application build
Looking forward to hear from you..!!!
About Iris Software Inc.
With 4,000+ associates and offices in India, U.S.A. and Canada, Iris Software delivers technology services and solutions that help clients complete fast, far-reaching digital transformations and achieve their business goals. A strategic partner to Fortune 500 and other top companies in financial services and many other industries, Iris provides a value-driven approach - a unique blend of highly-skilled specialists, software engineering expertise, cutting-edge technology, and flexible engagement models. High customer satisfaction has translated into long-standing relationships and preferred-partner status with many of our clients, who rely on our 30+ years of technical and domain expertise to future-proof their enterprises. Associates of Iris work on mission-critical applications supported by a workplace culture that has won numerous awards in the last few years, including Certified Great Place to Work in India; Top 25 GPW in IT & IT-BPM; Ambition Box Best Place to Work, #3 in IT/ITES; and Top Workplace NJ-USA.
Show more
Show less","Java/J2EE, Scala, Shell Script, C, Python, C#, DB2, Oracle, MySQL, MS SQL Server, NoSQL (HBase Vertica Casandra), Hadoop, Spark, MapReduce, Hive, Kafka, Knox, Oozie, Solr, Elasticsearch, Kafka, API, APGEE, Microservices","javaj2ee, scala, shell script, c, python, c, db2, oracle, mysql, ms sql server, nosql hbase vertica casandra, hadoop, spark, mapreduce, hive, kafka, knox, oozie, solr, elasticsearch, kafka, api, apgee, microservices","apgee, api, c, db2, elasticsearch, hadoop, hive, javaj2ee, kafka, knox, mapreduce, microservices, ms sql server, mysql, nosql hbase vertica casandra, oozie, oracle, python, scala, shell script, solr, spark"
Data Engineer-Lead,Zortech Solutions,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/data-engineer-lead-at-zortech-solutions-3667476671,2023-12-17,Delta, Canada,Mid senior,Hybrid,"Role: Data Engineer-Lead
Location: Remote/Canada
Duration: Contract/Fulltime
JOB Description
We are looking for a hands-on, talented DATA ENGINEER - LEAD to join our rapidly expanding team in NA.
Essential Skills And Experience
8 to 10 years of developing data solutions Refer below Table for tech stack.
Good understanding of Data Processing Frameworks (like Spark) Orchestration Frameworks (like Airflow).
Should be able to manage & co-ordinate 6-8 Data Engineers in India Time Zone
Deliver updates to stakeholders based on analytics
Excellent problem solving skills
Expertise in full data stack: ETL/ELT, data analysis, storage, data pipelines, Orchestration
Hands on experience in relational databases (PL/SQL, Postgres) etc.
Experience with Version Control (e.g. GIT, GitHub)
Cloud Experience (AWS)
Good communication skills and ability to work as part of a Scrum team & well co-ordinate with development team in different Time Zone.
Tools
Cloud Technologies: AWS (S3, Glue, RDS),
Big Data: Hive, Spark, Kafka
Databases: Oracle, Postgres SQL
Languages/Tools: SQL, PL/SQL, Python, Windows Shell Scripting, AirFlow
Source Control Tool: GITHUB, GIT
ETL Tools: SSIS,
PM Tools & Delivery Model: JIRA, Confluence, AGILE [SCRUM]
Key Job Responsibilities
Work with DE team Mentor & guide data engineers on their deliverables.
Develop, construct, test and maintain architectures
Align architecture with business requirement
Data acquisition
Develop data set processes
Use programming language and tools
Identify ways to improve data reliability, efficiency and quality
Use large data sets to address business issues
Deploy sophisticated analytics programs, machine learning and statistical methods
Prepare data for predictive and prescriptive modeling
Find hidden patterns using data
Use data to discover tasks that can be automated
Show more
Show less","Data Engineering, Spark, Airflow, AWS, Hive, Kafka, Oracle, PostgreSQL, SQL, PL/SQL, Python, Windows Shell Scripting, GitHub, GIT, SSIS, Jira, Confluence, Agile, Scrum","data engineering, spark, airflow, aws, hive, kafka, oracle, postgresql, sql, plsql, python, windows shell scripting, github, git, ssis, jira, confluence, agile, scrum","agile, airflow, aws, confluence, data engineering, git, github, hive, jira, kafka, oracle, plsql, postgresql, python, scrum, spark, sql, ssis, windows shell scripting"
Python Data Analyst,TV2 Consulting,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/python-data-analyst-at-tv2-consulting-3787037261,2023-12-17,Delta, Canada,Mid senior,Hybrid,"python and geospatial analytics experience required
Data analytics, using geospatial predictive modeling, and traditional data analytics
Some NLP work that will need to perform some NER, summarization, and sentiment analysis
requires working with our internal datasets using geospatial manipulation libraries, dashboarding libraries (dash, flask, matplotlib) and some ML/AI libraries (numpy, pandas, scikit, etc)
Python development data processing/ETL, API development and pipeline automation
assist with on-going competitive use cases and cloud environment issues when required
Benefits
Work hours are Monday - Friday, normal 37.5 hours weeks working onsite up to 5 days a week
Location: Onsite in Toronto
Type of job: Temporary Contractor
Hourly contract rate: $70-85 per hour CAD depending on experience
Date candidate required: Immediate
Length of Contract: 12 months with the possibility of extension
Show more
Show less","Python, Geospatial analytics, NLP, NER, Summarization, Sentiment analysis, Data analytics, Predictive modeling, Geospatial manipulation libraries, Dashboarding libraries (Dash Flask Matplotlib), ML/AI libraries (NumPy Pandas Scikitlearn), Data processing/ETL, API development, Pipeline automation, Competitive use cases, Cloud environment issues","python, geospatial analytics, nlp, ner, summarization, sentiment analysis, data analytics, predictive modeling, geospatial manipulation libraries, dashboarding libraries dash flask matplotlib, mlai libraries numpy pandas scikitlearn, data processingetl, api development, pipeline automation, competitive use cases, cloud environment issues","api development, cloud environment issues, competitive use cases, dashboarding libraries dash flask matplotlib, data processingetl, dataanalytics, geospatial analytics, geospatial manipulation libraries, mlai libraries numpy pandas scikitlearn, ner, nlp, pipeline automation, predictive modeling, python, sentiment analysis, summarization"
Sr. Data Engineer,J&M Group,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/sr-data-engineer-at-j-m-group-3768019544,2023-12-17,Delta, Canada,Mid senior,Hybrid,"Develops and maintains scalable data pipelines and builds out new API integrations to support continuing increases in data volume and complexity.
Collaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization.
Implements processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business
Required Skills
Azure Data Factory, Azure Data Lake, Azure Databricks
Nice To Have Skills
Python, Hive, PySpark, Databricks
Roles & Responsibilities
At least 4 years of professional experience working as Data Engineer with focus on big data processing and/or relational databases
Experienced with working on Structured and Semi-Structured datasets.
Experience working with Microsoft Azure Data Platform, specifically Azure SQL Database, Azure Data Factory, ADLS storage account
Experienced in creating data pipelines & developing complex and optimized queries
Experience with implementing CI / CD pipeline on GitHub, Jenkins / JGP.
Experienced in any of the following programming/scripting languages (SQL, Python, pyspark)
Knowledgeable in any of the following tools: Tableau, PowerBI, QlikView/QlikSense
Knowledgeable in using collaboration tools (eg. MS Teams/Skype, Confluence, JIRA)
Ability to own and execute an initiative end to end.
Experience with any SLDC Methodologies and familiarity with different Agile methodologies
Demonstrates a commitment to delivering excellent service, balanced with appropriate risk management.
Monitor, validate, and drive continuous improvement to methods, and propose enhancements to data sources that improve usability and results
Good Communication and presentation skills
Analytical, structured, organized, and proactive Nice to Have:
Previous work experience in the financial sector (insurance or benefits industry).
Show more
Show less","Azure Data Factory, Azure Data Lake, Azure Databricks, Azure SQL Database, ADLS, Python, Hive, PySpark, Databricks, SQL, Tableau, PowerBI, QlikView, QlikSense, Jenkins, JGP, MS Teams, Skype, Confluence, JIRA, Agile, SDLC","azure data factory, azure data lake, azure databricks, azure sql database, adls, python, hive, pyspark, databricks, sql, tableau, powerbi, qlikview, qliksense, jenkins, jgp, ms teams, skype, confluence, jira, agile, sdlc","adls, agile, azure data factory, azure data lake, azure databricks, azure sql database, confluence, databricks, hive, jenkins, jgp, jira, ms teams, powerbi, python, qliksense, qlikview, sdlc, skype, spark, sql, tableau"
Lead AWS Data Engineer,First Derivative,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/lead-aws-data-engineer-at-first-derivative-3784390167,2023-12-17,Delta, Canada,Mid senior,Hybrid,"Who are we?
First Derivatives (FD) is a leading software and services company, with world-leading intellectual property in ultra-high-performance analytics (KX) across industries, and extensive domain expertise and capabilities in capital markets systems and technology (managed services and consulting).
Lead AWS Data Engineer:
Required Skills
10+ years of professional experience with strong Python & Pyspark coding skill, AWS + AWS Glue.
AWS technologies: Lambda, SNS, SQS, S3, Glue, EMR, IAM.
Proven track record working in data engineering, business intelligence, or a similar role
Experience in ETL orchestration and workflow management tools like Airflow, or Oozie
Guide in Database fundamentals, SQL and distributed computing
Excellent communication skills and experience working with technical and non-technical teams.
Experience in AWS (EC2/S2/IAM)
Experience in Databricks & Snowflake
Proficiency in programming languages such as Java
KX technology is designed to capture and analyze data to make real-time decisions in a world where data volumes generated by markets and machines are increasing exponentially, and existing technologies fail due to technological or commercial limitations.
KX is widely adopted throughout the financial industry, and is poised for accelerated growth across high-tech manufacturing, automotive, oil and gas, utilities and telecommunications.
Managed Services and Consulting
FD provides a range of services worldwide to its clients in the capital markets sector, including many of the world’s leading banks, focused on supporting mission-critical systems as well as helping them to achieve and maintain regulatory compliance.
FD provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.
Show more
Show less","Python, Pyspark, AWS, AWS Glue, Lambda, SNS, SQS, S3, EMR, IAM, Data engineering, Business intelligence, ETL orchestration, Airflow, Oozie, SQL, Distributed computing, Java, KX technology, Databricks, Snowflake","python, pyspark, aws, aws glue, lambda, sns, sqs, s3, emr, iam, data engineering, business intelligence, etl orchestration, airflow, oozie, sql, distributed computing, java, kx technology, databricks, snowflake","airflow, aws, aws glue, business intelligence, data engineering, databricks, distributed computing, emr, etl orchestration, iam, java, kx technology, lambda, oozie, python, s3, snowflake, sns, spark, sql, sqs"
Senior Data Engineer,J&M Group,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/senior-data-engineer-at-j-m-group-3768030695,2023-12-17,Delta, Canada,Mid senior,Hybrid,"Backend developers with solid ETL / Bulk data processing experience preferably in Azure Data Factory, Automation / Batch scheduling and API development experience as well .
Senior data engineer will work as a senior member of the insurance data squad that builds Azure data pipelines for various analytical use cases.
You will partner with stakeholders in understanding data and leveraging the data to build data marts ensuring adherence to Manulife's IT security and Risk guidelines.
These modern data marts will replace legacy BI and reporting tools.
Required Skills : Microsoft ADFS
Nice to have skills : Azure Data Factory, Databricks
Roles And Responsibilities
"" 6 + years of previous data engineering experience, with 3+ years of experience working in enabling azure related data technologies
"" Strong knowledge of Azure Databricks, ADF, ADLS, Synapse etc.
"" Excellent knowledge of Python, Spark, and T-SQL
"" Hands-on experience on designing and developing scripts for ETL processes and automation in Azure data factory, Azure databricks and PySpark
"" Good understanding of data modeling and ability to apply different techniques within a cloud platform
"" Knowledge of Azure networking Subnets, private endpoints, firewalls etc.
"" Knowledge of Jenkins using pipelines for deployment
"" Knowledge of implementing azure technologies and networking via terraform
Show more
Show less","ETL, Data processing, Azure Data Factory, Automation, Batch scheduling, API development, Data engineering, Azure data pipelines, Data marts, Data security, Risk management, BI tools, Reporting tools, Microsoft ADFS, Azure Databricks, Databricks, ADF, ADLS, Synapse, Python, Spark, TSQL, Azure data factory, Azure databricks, PySpark, Data modeling, Cloud platform, Azure networking, Subnets, Private endpoints, Firewalls, Jenkins, Terraform","etl, data processing, azure data factory, automation, batch scheduling, api development, data engineering, azure data pipelines, data marts, data security, risk management, bi tools, reporting tools, microsoft adfs, azure databricks, databricks, adf, adls, synapse, python, spark, tsql, azure data factory, azure databricks, pyspark, data modeling, cloud platform, azure networking, subnets, private endpoints, firewalls, jenkins, terraform","adf, adls, api development, automation, azure data factory, azure data pipelines, azure databricks, azure networking, batch scheduling, bi tools, cloud platform, data engineering, data marts, data processing, data security, databricks, datamodeling, etl, firewalls, jenkins, microsoft adfs, private endpoints, python, reporting tools, risk management, spark, subnets, synapse, terraform, tsql"
Lead Azure data Engineer,Randstad Canada,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/lead-azure-data-engineer-at-randstad-canada-3781755625,2023-12-17,Delta, Canada,Mid senior,Hybrid,"Lead Azure data Engineer contract position
Location Toronto ON
Must be eligible to work in Canada
Hybrid role 2-3 time /week onsite 40 hours /w billing
Must have skills:
Experienced Azure Data Engineer to design/lead work as well as hands on skills in the following areas:
6+yrs of experiences with:
Azure Data Factory
Databricks, ADF, Hadoop, SQL
Familiar with Rahona
ETL framework is nice to have
Great communication is a must
Should have strong leadership skills / take initiative / able to drive the migration from SQL server to Azure
Show more
Show less","Azure Data Factory, Databricks, Apache Hadoop, SQL, Rahona, ETL, Leadership, Communication, SQL Server","azure data factory, databricks, apache hadoop, sql, rahona, etl, leadership, communication, sql server","apache hadoop, azure data factory, communication, databricks, etl, leadership, rahona, sql, sql server"
Senior Software Data Engineer,Liquid Analytics,"Ontario, Canada",https://ca.linkedin.com/jobs/view/senior-software-data-engineer-at-liquid-analytics-3676304148,2023-12-17,Delta, Canada,Mid senior,Hybrid,"You are a
Senior Software Data Engineer
with production experience building high-performance data analytics solutions with Python, DuckDB SQL, Dagster, DBT, Java,  Apache Spark and related tools. Read the preparation section prior to applying.
Must have requirements and qualifications
It would be best if you had a min of
3 years of production work experience
(not school) building large data solutions
It would help if you had a min of 3 years of working knowledge of Python
Three-plus years of Python data pipeline orchestration, specifically with Dagster
A min of 1 week of DuckDB experience as our interview and demos will use DuckDB.
You must have a solution foundational developer workflow using Git, Visual Studio Code, and related tools.
As we use Rust, you must have strong experience with C, C++ (the language used to build DuckDB).
Knowledge of Typescript is required.
Excellent verbal and written communication skills so that you can work with subject matter experts.
A drive to own and complete analytical algorithmic projects on time, in scale, and that meet client SLAs.
Strong statistics background.
Visual Studio Code
Git and CI/CD are an absolute must.
Linux, Linux Networking, Cloud (preferably AWS), Containerization with tools like Docker, WASM
Responsibilities
You will understand the business requirements in sales, distribution, and finance.
You will work with clients and researchers to define, develop, test, scale, and various analytical algorithms.
Your number one focus is performance. We aim to build sub-second real-time analytics algorithms for multi-user machines and human environments.
You will work across the entire stack to deliver production-grade analytics solutions that delight users.
You own your data flow from ingestion, prep/blend, data models, test/validate, server, and refactor
Preparation
We will have a design discussion around our stack and how it maps to your current production experience. So please research the following before applying.
In your personal development environment, you will demonstrate your skills using Python, Dagster, and DuckDB.
You've researched DuckDB and can show that you're working with DuckDB as your in-process SQL engine.
Review existing published blogs, videos, and experiments on building an analytical data lake application with Python, DuckDB, and Dragster.
Check out the DuckDB blog and features of version 0.7.1 and 0.8.1 and show experiments using the features of DuckDB.
To apply, you must be coding and solving problems in the same space with Python and SQL in your current job of more than two years.
You can use the job post to apply; please don't send messages directly to my inbox.
We have other job posts for Java, Front End, and DevOps developers, so please use the right job post for the right job.
Thank you for your application, and we look forward to building production-grade software with you! Liquid Analytics is a product-driven company. Our focus is ""Fast real-time decision engines.""
Show more
Show less","Python, DuckDB SQL, Dagster, DBT, Apache Spark, Java, Rust, C, C++, Typescript, Git, Visual Studio Code, Linux, Cloud, Docker, WASM, SQL","python, duckdb sql, dagster, dbt, apache spark, java, rust, c, c, typescript, git, visual studio code, linux, cloud, docker, wasm, sql","apache spark, c, cloud, dagster, dbt, docker, duckdb sql, git, java, linux, python, rust, sql, typescript, visual studio code, wasm"
Sr. Data Engineer,Munich Re,"Greater Toronto Area, Canada",https://ca.linkedin.com/jobs/view/sr-data-engineer-at-munich-re-3712326932,2023-12-17,Delta, Canada,Mid senior,Hybrid,"Munich Re sees the use of data as instrumental in making it easier for people to buy life insurance and to expand the number of people insured. The Integrated Analytics (IA) team leads this effort in the US and Canada for the North American Life business. We are looking for a Senior Data Engineer to help build and develop our data intensive infrastructure in support of underwriting and life insurance use cases.
Because of our unique positioning and team size, you will work on projects and have significantly more ownership than you would at a larger company. We also value individual development and are happy to help you grow independently by exposing you to different functional areas within the team.
To learn more about the North American Integrated Analytics team, please visit our site:
https://www.munichre.com/us-life/en/solutions/integrated-analytics.html
THE ROLE:
Responsibilities may include, but will not be limited to the following:
Build and expand our ETL batch pipelining practices using tools such as Airflow and Databricks
Manage and secure our enterprise databases systems and Data Warehouses
Learn and implement engineering tools and techniques such as Infrastructure-as-Code (IaC) and Database-as-Code(DaC)
Support in active debugging and fixing bugs, while maintaining production versions of data & analytics pipelines
Follow a cloud-native approach and build data products with heavy reliance on cloud infrastructure.
Actively engage in the Agile methodology and work collaboratively in sprints
Provide mentorship to junior engineers on skill development.
QUALIFICATIONS:
Undergraduate degree (major/minor) or higher in Computer Science, Engineering, Physics, Bioinformatics – or equivalent program
Familiarity with Python
Ability to work in a cloud-native development environment using cloud platforms such as Azure or AWS
Experience dealing with different data formats and suitable algorithms for data wrangling
Knowledge of SQL, relational and NoSQL database systems (RDMS) such as MS SQL Server and CosmosDB and complex data modelling
Basic knowledge of Big Data tools such as Spark and Databricks
Intermediate knowledge on data model types
Intermediate knowledge of the linux Operating System
Ability to perform version control using Git and submit well-structured Pull Requests (PR)
Experience in creatively and rapidly debugging for on-the-job issues
Desire to learn new technologies and passion about integrating them into our data stack
Ability to clearly communicate in a non-technical manner with stakeholders
Preferred (but not required):
Previous exposure to insurance or financial services environment
Experience dealing with Azure-native Big Data tools
Knowledge of security best-practices for data infrastructure tools
Munich Re is one of the world’s leading reinsurance companies with approximately 40,000 employees in over 50 locations around the globe. We expanding our diverse team of Data Scientists, Machine Learning Engineers, and Software Engineers as we deploy more complex and larger products. Headquartered in midtown New York City, our Integrated Analytics team operates as a start-up within the larger corporation; we are central to innovation and pioneering new technologies in the Life Insurance space. Our solutions are driven by the bold vision and strong sponsorship of North America CEO Mary Forrest.
Show more
Show less","ETL, Airflow, Databricks, Enterprise databases, Data Warehouses, InfrastructureasCode (IaC), DatabaseasCode (DaC), Python, Cloudnative development, Azure, AWS, Spark, NoSQL database systems, RDMS, MS SQL Server, CosmosDB, Linux, Git, Pull Request (PR), Insurance, Financial services, Data infrastructure security","etl, airflow, databricks, enterprise databases, data warehouses, infrastructureascode iac, databaseascode dac, python, cloudnative development, azure, aws, spark, nosql database systems, rdms, ms sql server, cosmosdb, linux, git, pull request pr, insurance, financial services, data infrastructure security","airflow, aws, azure, cloudnative development, cosmosdb, data infrastructure security, data warehouses, databaseascode dac, databricks, enterprise databases, etl, financial services, git, infrastructureascode iac, insurance, linux, ms sql server, nosql database systems, pull request pr, python, rdms, spark"
Senior Data Engineer,Gore Mutual Insurance,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/senior-data-engineer-at-gore-mutual-insurance-3749619726,2023-12-17,Delta, Canada,Mid senior,Hybrid,"We’re now at the boldest phase of our Next Horizon journey
At Gore Mutual, we’ve completely transformed our business in under three years. By investing in top talent and leading technology, we’ve redefined what it means to be a modern mutual that does good.
Our path forward brings a sharper focus on our business’ performance that’s powered by innovation and an agile, high-performing culture – we’re built for success.
We’re well on our way to becoming a purpose-driven, digitally led national insurer. Come join us.
Are you a talented and motivated individual with a passion for creating cutting-edge data solutions? Do you thrive on challenges and have a desire to constantly improve the quality and performance of your work? If you are, this is an opportunity to be part of our team and help build an enterprise data platform from the ground up.
As a Senior Data Engineer at Gore Mutual Insurance, you will play a pivotal role in designing, building, and maintaining our data infrastructure. Your work will facilitate data accessibility, accuracy, and accountability, enabling data-driven decision-making across our organization. You will collaborate closely with cross-functional teams to ensure our data processes are robust and scalable.
What will you be doing in this role?
Take ownership of data engineering projects.
Be resourceful and adept at navigating unfamiliar challenges.
Pay meticulous attention to detail.
Communicate clearly and effectively with cross-functional teams.
Understand and mitigate security risks and vulnerabilities.
Apply performance optimization techniques where needed.
Collaborate efficiently with IT teams.
Strike a balance between rapid project completion and maintaining high-quality standards.
Embrace test-driven development practices.
Identify issues or opportunities proactively, raising them and taking appropriate actions.
Contribute to defining data engineering patterns and design while constructively challenging solutions.
Collect and analyze requirements, then develop data processes and applications that meet them.
Design, develop, and maintain robust, scalable, and efficient data pipelines for data ingestion, processing, and transformation.
Continuously monitor and optimize data pipelines and systems for peak performance and reliability.
Implement data quality checks and governance policies to maintain data accuracy, completeness, and security.
Maintain comprehensive documentation of the data platform.
Provide guidance and mentorship to junior data engineers, fostering their professional growth.
What will you need to succeed in this role?
Bachelor’s or Master’s degree in Computer Science, Data Engineering, Software Engineering or a related field.
Minimum 4 years of proven experience as a Data Engineer.
Proficiency in Python, including a strong grasp of Object Oriented and Functional programming paradigms.
Excellent SQL skills and expertise in database management and tuning.
Strong pySpark and distributed systems knowledge
Experience working with and analyzing structured, semi-structured, and unstructured datasets.
Familiarity with software design patterns.
Proficiency in data modeling, ETL processes, data lakes, and data warehousing concepts.
Ability to communicate complex technical concepts clearly and effectively.
Proficiency in Azure Data infrastructure components.
Ability to work independently and as part of a team in a fast-paced, dynamic environment.
Strong problem-solving and critical-thinking abilities.
Strong communication and collaboration skills.
Proactive and results-oriented mindset.
Demonstrated experience working in Agile development teams (Scrum or Kanban).
Understand the Software Development Life Cycle (SDLC).
Exceptional technical writing abilities.
Nice to Have
A graduate degree in a technical field with specialization in Analytics, Data Science, or a related subject.
Azure certifications (Microsoft Certified: Azure Data Engineer Associate).
Experience with Data Lakehouse architecture
Experience with building custom data tools.
Databricks certifications (Databricks Certified /Data Engineer Professional certification).
Experience with reference or master data management.
#IndHP
Gore Mutual Insurance is committed to providing accommodations for people with disabilities during all phases of the recruiting process, including the application process. If you require accommodation because of a disability, we will work with you to meet your needs. If you are selected for an interview and require accommodation, please advise the HR representative who will consult with you to determine an appropriate accommodation.
Show more
Show less","Python, Object Oriented Programming, Functional Programming, SQL, Database Management, Tuning, PySpark, Distributed Systems, Structured Data, SemiStructured Data, Unstructured Data, Software Design Patterns, Data Modeling, ETL Processes, Data Lakes, Data Warehousing, Azure Data, Agile Development, Scrum, Kanban, Software Development Life Cycle, Data Lakehouse Architecture, Custom Data Tools, Databricks Certification, Reference Data Management, Master Data Management","python, object oriented programming, functional programming, sql, database management, tuning, pyspark, distributed systems, structured data, semistructured data, unstructured data, software design patterns, data modeling, etl processes, data lakes, data warehousing, azure data, agile development, scrum, kanban, software development life cycle, data lakehouse architecture, custom data tools, databricks certification, reference data management, master data management","agile development, azure data, custom data tools, data lakehouse architecture, data lakes, database management, databricks certification, datamodeling, datawarehouse, distributed systems, etl, functional programming, kanban, master data management, object oriented programming, python, reference data management, scrum, semistructured data, software design patterns, software development life cycle, spark, sql, structured data, tuning, unstructured data"
Sr. Azure Data Engineer,J&M Group,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/sr-azure-data-engineer-at-j-m-group-3768021839,2023-12-17,Delta, Canada,Mid senior,Hybrid,"6+ years of experience as Sr. Azure data engineer
Good hands-on experience in building azure pipelines using Azure data factory and databricks
Should be able to lead a team of 5-6 people
Show more
Show less","Azure, Azure data factory, Databricks, Data engineering, Team leadership","azure, azure data factory, databricks, data engineering, team leadership","azure, azure data factory, data engineering, databricks, team leadership"
Senior Full Stack Data Engineer,Luxoft,"Toronto, Ontario, Canada",https://ca.linkedin.com/jobs/view/senior-full-stack-data-engineer-at-luxoft-3746869863,2023-12-17,Delta, Canada,Mid senior,Hybrid,"Project Description:
We are looking for an experienced Full Stack Data Engineer to join our team for building a next-generation data platform built on Data Mesh architecture/principles. The ideal candidate should have extensive hands-on experience in building a big data platform, Big Data Technologies, Data Pipelines, backend development using Python, BI/Analytics tools as well as experience with DevOps, AWS, and UI Development in Angular JS. You will be responsible for designing, developing, and maintaining our web applications and data pipelines, as well as implementing CI/CD best practices.
Responsibilities:
• Design, build, and maintain scalable and efficient data platforms using data engineering technologies such as Glue, EMR, Athena, Redshift, Lake Formation, Apache Spark, Hive, HDFS, and Trino.
• Build/manage data pipelines, and common data-related cross-cutting concerns like data catalog, data lineage, data quality, data profiling, data discovery, metadata management
• Develop and maintain web applications using AngularJS and Python.
• Build/manage BI/Analytical dashboard reducing time to insight for the business stakeholders.
• Implement CI/CD pipelines using Terraform, Jenkins, Github actions, and Gitflow.
• Collaborate with cross-functional teams to develop and implement new features.
• Write clean, reusable, and efficient code.
• Participate in code reviews and ensure code quality.
• Develop and maintain APIs using Python and ensure API security and best practices are implemented.
• Implement SSO integration with Microsoft Azure AD using oAuth, OIDC, and SAML.
• Implement integration with AWS Cognito for user authentication and authorization.
• Ensure the application is optimized for maximum speed and scalability.
• Troubleshoot and debug issues as they arise.
• Implement DevOps best practices to ensure efficient application deployment and management.
• Collaborate with data scientists and analysts to integrate data analytics solutions with web applications.
• Stay up to date with emerging trends and technologies.
Mandatory Skills Description:
• 8+ years of experience in similar positions;
• Hands-on Experience with data engineering technologies such as AWS Glue, EMR, Athena, Redshift, Lake Formation, Apache Spark, Apache Hive, Apache Airflow, S3FS, Apache Hudi, and Trino.
• Extensive experience in building data pipelines using orchestration tools like Apache Airflow. Hands-on experience in building cross-cutting concerns like data catalog, data lineage, data quality, data profiling, data discovery, metadata management
• Proven experience as a Full Stack Developer with AngularJS and Python.
• Strong understanding of web development technologies including HTML, CSS, and JavaScript.
• Experience working with RESTful APIs and JSON. Familiarity with microservices architecture.
• Experience with core AWS technologies such as EC2, ELB, Auto Scaling, S3, EFS, Lambda, API
Gateway, Step Functions, Cloudwatch, VPC, Route 53, ACM
• Hands on experience with SQL and NoSQL databases.
• Hands experience with BI tools like Tableau, AWS QuickSight
• Experience with Git or other version control systems.
• Understanding of agile development methodologies.
• Strong problem-solving skills.
• Excellent written and verbal communication skills.
• Ability to work independently and collaboratively in a team environment.
• Experience with cloud platforms such as AWS.
• Bachelor's degree in Computer Science, Engineering, or related field
Nice-to-Have Skills:
• Agile, Scrum framework 2+ years' experience on past projects
• Knowledge of containerization technologies like Docker or Kubernetes.
• Experience with front-end frameworks like React or Vue.js.
• Experience with DevOps and CI/CD best practices.
• Experience with Terraform and the AWS provider.
• Experience with API development, security, and best practices.
• Experience with SSO integration with Microsoft Azure AD using oAuth, OIDC, and SAML.
• Experience with integration with AWS Cognito for user authentication and authorization
Languages:
English: C1 Advanced
Show more
Show less","Data Engineering, Data Pipelines, Python, BI/Analytics, DevOps, AWS, AngularJS, Glue, EMR, Athena, Redshift, Lake Formation, Apache Spark, Hive, HDFS, Trino, Data Catalog, Data Lineage, Data Quality, Data Profiling, Data Discovery, Metadata Management, HTML, CSS, JavaScript, RESTful APIs, JSON, Microservices, EC2, ELB, Auto Scaling, S3, EFS, Lambda, API Gateway, Step Functions, Cloudwatch, VPC, Route 53, ACM, SQL, NoSQL, Tableau, AWS QuickSight, Git, Agile, Docker, Kubernetes, React, Vue.js, Terraform, OAuth, OIDC, SAML","data engineering, data pipelines, python, bianalytics, devops, aws, angularjs, glue, emr, athena, redshift, lake formation, apache spark, hive, hdfs, trino, data catalog, data lineage, data quality, data profiling, data discovery, metadata management, html, css, javascript, restful apis, json, microservices, ec2, elb, auto scaling, s3, efs, lambda, api gateway, step functions, cloudwatch, vpc, route 53, acm, sql, nosql, tableau, aws quicksight, git, agile, docker, kubernetes, react, vuejs, terraform, oauth, oidc, saml","acm, agile, angularjs, apache spark, api gateway, athena, auto scaling, aws, aws quicksight, bianalytics, cloudwatch, css, data catalog, data discovery, data engineering, data lineage, data profiling, data quality, datapipeline, devops, docker, ec2, efs, elb, emr, git, glue, hdfs, hive, html, javascript, json, kubernetes, lake formation, lambda, metadata management, microservices, nosql, oauth, oidc, python, react, redshift, restful apis, route 53, s3, saml, sql, step functions, tableau, terraform, trino, vpc, vuejs"
Applications Dev & Test - Data Scientist 4 Data Scientist 4 #: 23-07122,HireTalent - Diversity Staffing & Recruiting Firm,"Redmond, OR",https://www.linkedin.com/jobs/view/applications-dev-test-data-scientist-4-data-scientist-4-%23-23-07122-at-hiretalent-diversity-staffing-recruiting-firm-3778971364,2023-12-17,Bend,United States,Mid senior,Onsite,"Job Title: Applications Dev & Test - Data Scientist 4
Job Duration: 6 Months
Job Location: Remote
Job Description
Typical Day in the Role
Purpose of the Team – Part of the Azure Core Org, Specifically in the Azure Fabric Division. Bread and butter infrastructure team that creates the networking/storage/and internal product services are built on.
To help create insights for the business org) to inform on business decisions
Key projects – Customer Segmentation, Sentiment Analysis, and Clustering Algorithms
Typical task breakdown and operating rhythm -
Expert in latest AI modeling techniques, including Clustering algorithms, Natural Language Processing etc.
Work with senior management, technical and client teams in order to determine data requirements, business data implementation approaches, best practices for advanced data manipulation, storage and analysis strategies
Write and code logical and physical database descriptions and specify identifiers of database to management system or direct others in coding descriptions
Design, implement, automate and maintain large scale enterprise data ETL processes
Modify existing databases and database management systems and/or direct programmers and analysts to make changes
Test programs or databases, correct errors and make necessary modifications
Summary
The main function of a Data Scientist is to coordinate changes to computer databases, test, and implement the database applying knowledge of database management systems.
Job Responsibilities
Expert in latest AI modeling techniques, including Clustering algorithms, Natural Language Processing etc.
Work with senior management, technical and client teams in order to determine data requirements, business data implementation approaches, best practices for advanced data manipulation, storage and analysis strategies
Write and code logical and physical database descriptions and specify identifiers of database to management system or direct others in coding descriptions
Design, implement, automate and maintain large scale enterprise data ETL processes
Modify existing databases and database management systems and/or direct programmers and analysts to make changes
Test programs or databases, correct errors and make necessary modifications
Qualifications
Bachelor's degree in a technical field such as computer science, computer engineering or related field required
8-10 years applicable experience required
Experience with database technologies
Knowledge of the ETL process
Knowledge of at least one scripting language
Strong written and oral communication skills
Strong troubleshooting and problem solving skills
Demonstrated history of success
Desire to be working with data and helping businesses make better data driven decisions
Top 3 Hard Skills Required &plus; Years of Experience
6&plus; Years of Experience AI Machine learning as a Data Scientist, and create trends/analysis about the business to help the org
6&plus; Years of Experience Clustering, Algorithms, Run Large Scale Models (to help create insights for the business org) to inform on business decisions
6&plus; Years of experience Customer segmentation, sentiment analysis, AI Machine Learning, AI Modeling
6&plus; Years of Full Stack Development
Show more
Show less","AI, Machine learning, Clustering algorithms, Natural language processing, ETL, Scripting languages, Database technologies, Data manipulation, Data analysis, Data storage, Data visualization, Communication skills, Troubleshooting, Problem solving, Full stack development","ai, machine learning, clustering algorithms, natural language processing, etl, scripting languages, database technologies, data manipulation, data analysis, data storage, data visualization, communication skills, troubleshooting, problem solving, full stack development","ai, clustering algorithms, communication skills, data manipulation, data storage, dataanalytics, database technologies, etl, full stack development, machine learning, natural language processing, problem solving, scripting languages, troubleshooting, visualization"
Sr. Data Governance Engineer,Bose Corporation,"Framingham, MA",https://www.linkedin.com/jobs/view/sr-data-governance-engineer-at-bose-corporation-3777119188,2023-12-17,Attleboro,United States,Mid senior,Onsite,"Job Description
About Bose
You know the moment. It’s the first notes of that song you love, the intro to your favorite movie, or simply the sound of someone you love saying “hello.” It’s in these moments that sound matters most.
At Bose, we believe sound is the most powerful force on earth. We’ve dedicated ourselves to improving it for nearly 60 years. And we’re passionate down to our bones about making whatever you’re listening to a little more magical.
About The IT Team
The Information Technology team at Bose exists to deliver valuable and reliable business and technology solutions with an innovative, engaged, and collaborative team focused on contributing to our corporate vision.
About The Role
The Data Governance Engineer will manage organization-wide data governance activities and will have responsibility for stewardship in the identification and protection of sensitive data and information assets. The position will assist the Data Governance Lead in establishing and ensuring development and adherence to an enterprise data governance framework for data policies, standards and practices and to achieve the required level of data compliance and protection.  The position will also collaborate with the Data Strategy team to drive consistency of data across the enterprise.
The Data Governance Engineer will lead the resolution of data governance, compliance, and protection issues and will work closely with Business and Functional teams to identify and validate ownership, improve the quality and value of core data assets, respond to regulatory protection requirements as well as support the strategic requirements of the department.
Roles and Responsibilities
Establish an enterprise data governance implementation roadmap including strategic priorities for development of information-based capabilities
In collaboration with Data Strategy, roll out an enterprise-wide data governance framework, with a focus on data inventory, ownership and the protection of sensitive data through modifications to organization behavior policies and standards, principles, governance metrics, processes, related tools and data architecture
Assist in defining roles and responsibilities related to data governance and ensure clear accountability for stewardship of the company’s principal information assets
Serve as a liaison between Business and Functional areas and technology to ensure that data related business requirements for protecting sensitive data are clearly defined, communicated and well understood and considered as part of operational prioritization and planning
Develop & maintain inventory of the enterprise information maps, including authoritative systems, owners, and retention policies
Facilitate the development and implementation of data protection standards and adoption requirements across the enterprise
Resolve issues escalated from Business and Functional areas regarding various aspects of data governance
Coordinate external data sources to eliminate redundancy and streamline the expense related to those services
Qualifications
Minimum of 5 years of experience in a major services organization, with large-scale data or project management and oversight experience.
Knowledge of industry leading data quality and data protection management practices
Knowledge of data governance practices, business and technology issues related to management of enterprise information assets and approaches related to data protection
Knowledge of data related to government regulatory requirements and emerging trends and issues
Knowledge of Cloud Technologies (IaaS, PaaS, SaaS) that support functionalities to identity and protect data including Amazon Web Service (AWS)
Demonstrated consulting skills, with change management concepts and strategies, including communication, culture change and performance measurement system design
Knowledge of risk data architecture and technology solutions
Bachelors or Master’s in computer science, Data Analytics/Informatics or related fields to support Data Governance.
Bose is an equal opportunity employer that is committed to inclusion and diversity. We evaluate qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, genetic information, national origin, age, disability, veteran status, or any other legally protected characteristics. For additional information, please review: (1) the EEO is the Law Poster (http://www.dol.gov/ofccp/regs/compliance/posters/pdf/OFCCP_EEO_Supplement_Final_JRF_QA_508c.pdf); and (2) its Supplements (http://www.dol.gov/ofccp/regs/compliance/posters/ofccpost.htm). Please note, the company's pay transparency is available at http://www.dol.gov/ofccp/pdf/EO13665_PrescribedNondiscriminationPostingLanguage_JRFQA508c.pdf. Bose is committed to working with and providing reasonable accommodations to individuals with disabilities. If you need a reasonable accommodation because of a disability for any part of the application or employment process, please send an e-mail to Wellbeing@bose.com and let us know the nature of your request and your contact information.
Show more
Show less","Data Governance, Data Protection, Data Quality, Cloud Technologies, AWS, Data Analytics, Data Informatics, Data Inventory, Data Architecture, Data Compliance, Data Strategy, Data Governance Framework, Risk Data Architecture","data governance, data protection, data quality, cloud technologies, aws, data analytics, data informatics, data inventory, data architecture, data compliance, data strategy, data governance framework, risk data architecture","aws, cloud technologies, data architecture, data compliance, data governance, data governance framework, data informatics, data inventory, data protection, data quality, data strategy, dataanalytics, risk data architecture"
Sr Data Analyst - Secondary,Cengage Group,"Massachusetts, United States",https://www.linkedin.com/jobs/view/sr-data-analyst-secondary-at-cengage-group-3771468168,2023-12-17,Attleboro,United States,Mid senior,Onsite,"We believe in the power and joy of learning
At Cengage Group, our employees have a direct impact in helping students around the world discover the power and joy of learning. We are bonded by our shared purpose – driving innovation that helps millions of learners improve their lives and achieve their dreams through education.
Our culture values diversity, engagement, and discovery
Our business is driven by our strong culture, and we know that creating an inclusive and diverse workplace is absolutely essential to the success of our company and our learners, as well as our individual well-being. We recognize the value of diverse perspectives in everything we do, and strive to ensure employees of all levels and backgrounds feel empowered to voice their ideas and bring their authentic selves to work. We achieve these priorities through inclusive programs, benefits, and initiatives that are integrated into the fabric of how we work every day. To learn more, please see https://www.cengagegroup.com/about/inclusion-and-diversity/.
We are looking for a Sr Data Analyst to play a key role in the Marketing group to provide data analytics and related operational support for the Secondary Education team. Responsibilities include operationalizing pricing; update and maintain key metrics; and operational support in pursuit of strategic drivers of value.
The successful candidate will have strong interpersonal skills with a track record of establishing collaborative business relationships with cross functional teams within an organization; the ability to manage and use large volumes of data within analyses along with using existing resources to problem solve; and effective communication skills to work with all levels of management and staff!
What You'll Do Here
Partner with colleagues from other functions to implement overall pricing of print and digital products for the K12 organization.
Assist with Go-to-Market planning, growth strategy and business operations, including pricing, pricing model rules, and ensuring pricing is accurate in all systems.
Implement annual K12 price updates, new product pricing, and updates as needed/requested throughout the year.
Create/update price lists for key products, state adoptions, and other key sales opportunities.
Provide price lists to Sales, Marketing, depositories, customers, and distributors.
Continuously enhance monthly reporting to include comprehensive Key Performance Indicators, dashboards, analysis of business trends and drivers to improve predictability.
Develop analytical models to improve insight into business performance and support critical initiatives.
Prepare analysis and commentary focused on major trends, actionable insights to assist with key decision points.
Extract and manage large amounts of data in multiple formats,
Conduct analyses, summarize, and effectively communicate findings to support the decision-making process.
Support special projects and analyses on an ad-hoc basis, as required.
Skills You Will Need Here
Undergraduate or Graduate degree in Business preferred.
Minimum of 3 years related experience in reporting, planning, forecasting and analysis required.
Industry experience in publishing, software, or related content business is preferred.
Advanced experience with Microsoft Office tools, specifically Excel, Access, and PowerPoint required.
Experience with PowerPivot, Tableau, Smartsheets and other data visualization tools a plus.
Excellent detail-orientation and ability to multitask and respond to competing priorities.
Ability to contribute constructively in team-oriented environment.
Strong analytical and critical thinking skills and exhibits initiative.
Solid interpersonal and communications skills, both written and oral with ability to communicate complex business information to a range of audiences.
At National Geographic Learning, a part of Cengage Group, we are enabling opportunity, powering progress, and supporting student journeys toward college and career. Using our digital learning programs and classroom learning resources, students experience the excitement and joy of learning that National Geographic explorers, scientists, writers, and photographers experience.
Cengage Group is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, veteran status, marital status, parental status, cultural background, organizational level, work styles, tenure and life experiences. Or for any other reason. You may view Cengage’s EEO/Affirmative Action Policy signed by CEO Michael Hansen and Equal Employment Opportunity is the Law notice by visiting their corresponding links.
Cengage Group is committed to providing reasonable accommodations for qualified individuals with disabilities in our job application procedures. If you need assistance or an accommodation due to a disability, you may contact us at accommodations.ta@cengage.com or you may call us at +1 (617) 289-7917.
About Cengage Group
Cengage Group, a global education technology company serving millions of learners, provides affordable, quality digital products and services that equip students with the skills and competencies needed to be job ready. For more than 100 years, we have enabled the power and joy of learning with trusted, engaging content, and now, integrated digital platforms. We serve the higher education, workforce skills, secondary education, English language teaching and research markets worldwide. Through our scalable technology, including MindTap and Cengage Unlimited, we support all learners who seek to improve their lives and achieve their dreams through education.
Base Pay Range
The full base pay range has been provided for this position. Individual base pay will vary based on work schedule, qualifications, experience, internal equity, and geographic location. Sales roles often incorporate a significant incentive compensation program beyond this base pay range.
$63,800.00 - $102,100.00 USD
Show more
Show less","Excel, Access, PowerPoint, PowerPivot, Tableau, Smartsheets, Data visualization, Business analysis, Reporting, Forecasting, Data extraction, Communication, Teamwork, Problemsolving, Analytical thinking, Critical thinking, Initiative, Interpersonal skills, Microsoft Office","excel, access, powerpoint, powerpivot, tableau, smartsheets, data visualization, business analysis, reporting, forecasting, data extraction, communication, teamwork, problemsolving, analytical thinking, critical thinking, initiative, interpersonal skills, microsoft office","access, analytical thinking, business analysis, communication, critical thinking, data extraction, excel, forecasting, initiative, interpersonal skills, microsoft office, powerpivot, powerpoint, problemsolving, reporting, smartsheets, tableau, teamwork, visualization"
Data Engineer 3,Kraft Analytics Group,"Foxborough, MA",https://www.linkedin.com/jobs/view/data-engineer-3-at-kraft-analytics-group-3763924401,2023-12-17,Attleboro,United States,Mid senior,Onsite,"Job Details
Job Location
KAGR - Foxborough, MA
Position Type
Full Time
Education Level
Bachelor's
Job Shift
Day
Job Category
Technical/Analytics
Description
SUMMARY:
The Data Engineer 3 will drive the planning, design, and development of integration processes to build and improve the data warehouse. The Data Engineer will be responsible for building data integrations, data pipelines and other processes the data warehouse and KAGR product. This role will partner with Data Scientists, Analysts, other engineers and business stakeholders to solve complex and exciting challenges so that we can build our product to enable our clients to use data to drive their business.
Duties And Responsibilities
Building data integrations and data pipelines within the KAGR tech stack
Translate business requirements into data pipelines
Extract and load many disparate systems into a centralized data warehouse
Assist in gathering requirements for new pipelines
Implement data auditing strategies and processes to ensure data integrity
Document complex integration pipelines into easy-to-understand technical specifications
Perform data modeling to document existing and new tables in the data warehouse
Monitor and successfully troubleshoot data problems
Handle multiple projects effectively and meet deadlines
Special projects and assignments as business dictates
Responsible for the maintenance, creation and control of all personally identifiable information or any other information protected by any Confidentiality or Privacy Standards or Company policies that you have access or knowledge of, including but not limited to any state or federal regulations including HIPAA
Supervisory Responsibilities
This position has no supervisory responsibilities.
Skills And Qualifications
Bachelor's Degree in Computer Science, Information Systems, or related field
4-7 years of experience working with data using SQL or similar technology
3+ years of experience using cloud data warehouses such as Snowflake, Amazon Redshift, Google BigQuery, Azure Synapse
Experience with languages like Python, Ruby, Java, or similar language
3+ years of experience using a data integration platform, such as Talend, Snaplogic, or Informatica
Experience working in a cloud native environment
Strong understanding of data warehousing principles and methodologies
Ability to manage multiple projects in a fast-paced environment
Strong communication skills to all levels of technical expertise
Very high attention to detail
Familiarity with BI Visualization tools
PHYSICAL DEMANDS
Sitting for extended periods of time
Dexterity of hands and fingers to operate a computer keyboard, mouse, and other computing equipment
The employee frequently is required to talk or hear
The employee is occasionally required to reach with hands and arms
Specific vision abilities required by this job include close vision, distance vision, color vision, peripheral vision, depth perception, and ability to adjust focus
Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.
WORK ENVIRONMENT
The noise level in the work environment is usually moderate
Fast paced office environment
Ability to work nights and weekends as business dictates
CERTIFICATES, LICENSES, REGISTRATIONS
None required
Other Duties
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities and activities may change at any time with or without notice.
This company is an equal opportunity employer. We evaluate qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, veteran status, and other legally protected characteristics.
Show more
Show less","SQL, Data integration platforms, Data warehousing, Cloud native, Python, Ruby, Java, Snowflake, Amazon Redshift, Google BigQuery, Azure Synapse, Data modeling, BI Visualization tools, Talend, Snaplogic, Informatica","sql, data integration platforms, data warehousing, cloud native, python, ruby, java, snowflake, amazon redshift, google bigquery, azure synapse, data modeling, bi visualization tools, talend, snaplogic, informatica","amazon redshift, azure synapse, bi visualization tools, cloud native, data integration platforms, datamodeling, datawarehouse, google bigquery, informatica, java, python, ruby, snaplogic, snowflake, sql, talend"
Senior Big Data Engineer,"Roberts Recruiting, LLC","Framingham, MA",https://www.linkedin.com/jobs/view/senior-big-data-engineer-at-roberts-recruiting-llc-3576884331,2023-12-17,Attleboro,United States,Mid senior,Onsite,"We are launching exciting products such as our new Wireless Headphone and WIFI Speakers that are Connected to the Cloud. Come be part of a new team focused on driving business value from the insights gained from these Connected Products!
We are looking for a Senior Big Data Engineer to join our Big Data Analytics Platform team. In this role, you will be responsible for designing, developing, deploying and supporting the data ingestion pipeline from our Connected Products. In this role, you will be partnering with our data science community by supplying highly performant data sets for advanced analytics and will provide leadership and experience in the data engineering space.
The candidate must be results driven, customer focused, technologically savvy, and skilled at working in an agile development environment.
Job Responsibilities
Design and develop the data ingestion pipelines into the Big Data & Analytics Platform for a variety of big data use cases
Deploy and support highly optimized solutions with a focus on automation
Participate in the end to end delivery of business use-cases including data architecture to deliver results
Deliver as part of an agile scrum team on the highest business value use cases that will drive our business strategy
Partner with IT architects to define analytic architecture that best leverages the Big Data Platform to enable advanced analytics capabilities
Provide leadership to the ongoing maturity of the development process, coach/mentor the development team on best practices and methodologies for enhanced solution development.
Stay up to date on the relevant technologies, plug into user groups, understand trends and opportunities
Required Demonstrable Skills
Deep expertise is working with data – all kinds, clean, dirty, unstructured, semi-structured
Have strong expertise Apache Spark (batch and Spark streaming)
Experience in real-time and batch data processing and associated technologies
Able to demonstrate strong skills in programming/scripting languages such as Python, Scala, Java and R
Ability do design, develop and deploy end to end data pipelines that meet business requirements and use cases
Experience with all Cloudera components with focus on Impala, Hive, Hbase, Scoop and Hue
Experience with automation technologies such as Oozie
Strong experience with large cloud-compute infrastructure solutions such as Amazon Web Services, Google, Azure
Knowledge of UNIX/Linux
Experience with Text Analytics
Strong knowledge of SQL
Experience in Hadoop Platform Security and Hadoop Data Governance topics
Experience in triaging production issues to understand and resolve the issue
Experience in technical computing (optimization, statistics and machine learning)
Experience with analytics visualization software such as Tableau
Experience leading development teams, defining development processes, evaluating new technologies and practices to enhance solution delivery
Qualifications
Minimum of BS in Computer Science or similar field required
Must have at least 5-8+ years’ experience in information management and application development
Must have a minimum of 3-5 years working hands on with Big Data technologies
Show more
Show less","Big Data, Apache Spark, Realtime data processing, Batch data processing, Python, Scala, Java, R, Data pipelines, Cloudera components, Impala, Hive, Hbase, Scoop, Hue, Oozie, Amazon Web Services, Google, Azure, UNIX/Linux, Text Analytics, SQL, Hadoop Platform Security, Hadoop Data Governance, Production issues, Technical computing, Optimization, Statistics, Machine learning, Tableau, Development teams, Development processes, New technologies, Practices","big data, apache spark, realtime data processing, batch data processing, python, scala, java, r, data pipelines, cloudera components, impala, hive, hbase, scoop, hue, oozie, amazon web services, google, azure, unixlinux, text analytics, sql, hadoop platform security, hadoop data governance, production issues, technical computing, optimization, statistics, machine learning, tableau, development teams, development processes, new technologies, practices","amazon web services, apache spark, azure, batch data processing, big data, cloudera components, datapipeline, development processes, development teams, google, hadoop data governance, hadoop platform security, hbase, hive, hue, impala, java, machine learning, new technologies, oozie, optimization, practices, production issues, python, r, realtime data processing, scala, scoop, sql, statistics, tableau, technical computing, text analytics, unixlinux"
USA-Data Analyst II (IT) #: 23-06619,HireTalent - Diversity Staffing & Recruiting Firm,"Massachusetts, United States",https://www.linkedin.com/jobs/view/usa-data-analyst-ii-it-%23-23-06619-at-hiretalent-diversity-staffing-recruiting-firm-3764248366,2023-12-17,Attleboro,United States,Mid senior,Onsite,"Job Title: Data Analyst II
Job Location: Waltham , MA
Job Duration: 12 Months
We are looking for an individual with good technical skills and an interest in data modeling. Our goal is to enable data science, Client modeling, analytics, digital twins, translational immunology, and many other approaches in support of the development of new vaccines and therapeutics using mRNA technologies. Your work will help Client the data foundations for ***’s mRNA Center of Excellence and accelerate the implementation of solutions that will drive the next generation of mRNA technologies.
In This Role You Will
Collaborate with team members, stakeholders and other group experts to create logical/conceptual data models and data flows based on business needs
Maintain databases by adding new data or updating existing datasets, primarily by building Views and other table structures in Snowflake schemas
Perform analyses to assess the quality and meaning of data and to identify areas of concern
Define rules to address data quality/harmonization issues
Collaborate with other departments or teams, understand their needs, and provide them with relevant data insights or data sets based on those needs
Deliver high-value solutions by leveraging AGILE methodologies (e.g., Shape-Up) based on short feedback loops with users and collaborative ways of working (e.g., pair-programming)
Demonstrate good software engineering practices by keeping your code well-documented, version controlled in git, and following best practices (some of which you will have the opportunity to develop and define) to ensure clarity and consistency
Develop and implement scalable and efficient data processing pipelines using industry-leading data technologies.
Collaborate with cross-functional teams to understand data requirements and translate them into technical solutions.
Maintain awareness of latest developments in relevant fields, evaluate those developments, and apply them as warranted
About You
3&plus; years of experience with SQL/Snowflake, especially for table creation and data extraction
1&plus; years of experience with or experience supporting data analytics, data science, or other analytic methods (data engineering experience is a plus)
Previous experience providing data to support dashboards/visualizations (doing so with large volumes of data is a plus)
Previous experience working in cross-functional teams with other technical resources and domain/subject-matter experts
Excellent analytical and problem solving as well as communication, collaboration, and presentation skills
Good interpersonal and cooperative skills
Understanding of drug/vaccine discovery and development processes is a plus, as well as familiarity with RNA biology and RNA-centric techniques/methods
Knowledge of data governance and data quality best practices is a plus
Fluency in English (oral and written), knowledge of French is a plus
Minimum Education Bachelors Degree
Experience with SQL (Structured Query Language), Snowflake, data modeling
BioTech/ Pharma related experience
3&plus; years of experience with SQL (Structured Query Language) /Snowflake, especially for table creation and data extraction
Must have “extraction experience, ability to build something new within the system.” Nice to Have Experience :
Hands-on data analytics and data science experience
Data governance and data quality
Confluences, is a plus!
Show more
Show less","SQL, Snowflake, Data Modeling, Data Analysis, Data Science, Data Engineering, Agile Methodologies, Git, Data Pipelines, Data Governance, Data Quality, Dashboards, Visualizations, CrossFunctional Teams, Analytical Thinking, Problem Solving, Communication, Collaboration, Presentation Skills, Interpersonal Skills, Cooperative Skills, Drug/Vaccine Discovery, Development Processes, RNA Biology, RNACentric Techniques, Methods, English (Oral and Written), French, Biotechnology/Pharma Experience","sql, snowflake, data modeling, data analysis, data science, data engineering, agile methodologies, git, data pipelines, data governance, data quality, dashboards, visualizations, crossfunctional teams, analytical thinking, problem solving, communication, collaboration, presentation skills, interpersonal skills, cooperative skills, drugvaccine discovery, development processes, rna biology, rnacentric techniques, methods, english oral and written, french, biotechnologypharma experience","agile methodologies, analytical thinking, biotechnologypharma experience, collaboration, communication, cooperative skills, crossfunctional teams, dashboard, data engineering, data governance, data quality, data science, dataanalytics, datamodeling, datapipeline, development processes, drugvaccine discovery, english oral and written, french, git, interpersonal skills, methods, presentation skills, problem solving, rna biology, rnacentric techniques, snowflake, sql, visualizations"
R&D-Data Analysis Co-Op,Electric Hydrogen,"Natick, MA",https://www.linkedin.com/jobs/view/r-d-data-analysis-co-op-at-electric-hydrogen-3749353979,2023-12-17,Attleboro,United States,Mid senior,Onsite,"Electric Hydrogen
’s mission is to make molecules to decarbonize our world! Our outstanding people are our most important asset and will allow us to deliver hydrogen from renewable electrolysis for heavy industry, at prices below fossil fuels.
We're looking for a Research and Development Data Analysis Co-Op for Summer- Dec 2024 to join our research and development teams.
You will work onsite in our Natick, MA facilities and report into the R&D Engineer.
Roles And Responsibilities
Python scripts to analyze time-series and discrete data
Create data visualizations using SQL and Grafana
Incorporate new processing methods and metrics into the production codebase
Become familiar with the test infrastructure and instruments
Create presentations
Qualifications
Basic understanding of electrochemistry and thermodynamics
Enrollment in a BA/MS or PhD Degree
Compensation And Benefits
All students will have the opportunity to develop skills, gain laboratory experience and benefit from a valuable experience within a start up culture while learning about the energy industry.
$1,500 transportation stipend will be provided or $3,000 relocation stipend (depending on location)
Natick Zone
$35—$37 USD
Actual base salary offered to the hired applicant will be determined based on their work location, level, qualifications, job-related skills, as well as relevant education or training and experience.
Base salary is just one part of Electric Hydrogen’s total rewards package. We feel strongly that our team should not have to worry about having quality healthcare. In addition to the base salary offered, the hired applicant may receive:
an equity grant
time off programs
a $75/month cell phone allowance
a 4% employer 401(k) match
100% fully paid premiums for employees and their families: medical, dental, vision, life insurance, short-term & long-term disability coverage
a discretionary bonus
Electric Hydrogen’s benefits programs are subject to eligibility requirements.
COVID-19 vaccination required for all employees and contingent workers, unless a reasonable accommodation is approved. All prospective hires will be expected to provide proof of vaccination to attend an onsite interview or at their first day of employment.
About Electric Hydrogen
Electric Hydrogen is a team of the world's experts in scaling technologies for the post-carbon world, with a proven record in transforming the grid and transportation sectors. Backed by some of the world's top venture capital firms, we design and manufacture electrolytic hydrogen systems matched to renewable power sources to create green hydrogen by splitting water. We are building a cost-effective and transformative path between renewable energy and multiple large industrial sectors. Abundant and low-cost renewable energy sources will power the world, and Electric Hydrogen technology will use this energy to decarbonize industry through sustainable materials. We were founded in 2020 and are based in California and Massachusetts.
Electric Hydrogen is proud to be an equal opportunity employer. We are dedicated to building a diverse, inclusive, and authentic workplace for all to belong. We are aware that people from historically underrepresented groups are less likely to apply if they don't meet 100% of the job requirements. We are actively working on efforts to change this social norm. If you are excited about this role,
we encourage you to apply
!
Show more
Show less","Python, SQL, Grafana, Electrochemistry, Thermodynamics","python, sql, grafana, electrochemistry, thermodynamics","electrochemistry, grafana, python, sql, thermodynamics"
Sr Data Scientist - Product&Pricing Analytics,CVS Health,"Wellesley, MA",https://www.linkedin.com/jobs/view/sr-data-scientist-product-pricing-analytics-at-cvs-health-3744994250,2023-12-17,Attleboro,United States,Mid senior,Onsite,"Bring your heart to CVS Health. Every one of us at CVS Health shares a single, clear purpose: Bringing our heart to every moment of your health. This purpose guides our commitment to deliver enhanced human-centric health care for a rapidly changing world. Anchored in our brand — with heart at its center — our purpose sends a personal message that how we deliver our services is just as important as what we deliver.
Our Heart At Work Behaviors™ support this purpose. We want everyone who works at CVS Health to feel empowered by the role they play in transforming our culture and accelerating our ability to innovate and deliver solutions to make health care more personal, convenient and affordable.
Position Summary
The Pricing and Product team’s mission is to build innovative solutions that bring tangible impact to our CVS Health members, clients, and plan sponsors.
We accomplish this while offering a diverse work experience that empowers colleagues for career success, collaboration, innovation, caring, integrity, inclusiveness, and accountability.
The position will focus on our Caremark Pharmacy Benefit Management business unit where you work directly with Actuarial, Underwriting, Finance and Sales leaders to ideate and build analytical solutions and implement to show direct impact.
Position Summary:
Uses strong knowledge in algorithms and predictive models to investigate problems, detect patterns and recommend solution
Performs analyses of structured and unstructured data to solve multiple complex business problems utilizing advanced statistical techniques, mathematical analyses, and organization / industry knowledge
Collaborates with business partners to understand their problems and goals, develops predictive modeling, statistical analysis, data reports and performance metrics
Develops and participates in written presentations and consultations to various stakeholders on analytics results and solutions
Interacts with peers and managers to exchange complex information related to areas of specialization
Independently leads workstreams with minimal guidance from leaders
Required Qualifications
3+ years of progressively complex experience as an applied data scientist
3+ years of hands-ondata manipulation andmodeling experience using statistical, machine learning or deep learning methods; Experience on recommendation system is a plus
3+ years programming experience with SQL, R/Python
2+ years of experience of business stakeholder engagement or analytical project management
Preferred Qualifications
Prior experience in Insurance is highly preferred but will consider other verticals like Tech and Retail where pricing and solving for unique business problems are a big part of your day
Ability to anticipate potential challenges and proactively address them in modeling and non-modeling settings
Experience in building deep learning models, and Object-Oriented Programming skills, preferably in Python
Prior exposure to healthcare or insurance businesses
Strong proficiency in working with large data sets from multiple sources, building machine learning models, and applying statistical analyses in business settings
Strong consultative problem-solving skills, business acumen, and ability to think on your feet; experience working with mid-level stakeholders to define problem, develop and implement a solution to realize impact
Effective communication of technical concepts and business implications to non-technical stakeholders
Education
Bachelor's degree or equivalent work experience in Mathematics, Statistics, Computer Science, Business Analytics, Economics, Physics, Engineering, or related discipline.
Master's degree preferred
Pay Range
The typical pay range for this role is:
$103,500.00 - $200,000.00
This pay range represents the base hourly rate or base annual full-time salary for all positions in the job grade within which this position falls. The actual base salary offer will depend on a variety of factors including experience, education, geography and other relevant factors. This position is eligible for a CVS Health bonus, commission or short-term incentive program in addition to the base pay range listed above.
In addition to your compensation, enjoy the rewards of an organization that puts our heart into caring for our colleagues and our communities. The Company offers a full range of medical, dental, and vision benefits. Eligible employees may enroll in the Company’s 401(k) retirement savings plan, and an Employee Stock Purchase Plan is also available for eligible employees. The Company provides a fully-paid term life insurance plan to eligible employees, and short-term and long term disability benefits. CVS Health also offers numerous well-being programs, education assistance, free development courses, a CVS store discount, and discount programs with participating partners. As for time off, Company employees enjoy Paid Time Off (“PTO”) or vacation pay, as well as paid holidays throughout the calendar year. Number of paid holidays, sick time and other time off are provided consistent with relevant state law and Companypolicies.
For more detailed information on available benefits, please visitjobs.CVSHealth.com/benefits
CVS Health requires certain colleagues to be fully vaccinated against COVID-19 (including any booster shots if required), where allowable under the law, unless they are approved for a reasonable accommodation based on disability, medical condition, religious belief, or other legally recognized reasons that prevents them from being vaccinated.
You are required to have received at least one COVID-19 shot prior to your first day of employment and to provide proof of your vaccination status or apply for a reasonable accommodation within the first 10 days of your employment. Please note that in some states and roles, you may be required to provide proof of full vaccination or an approved reasonable accommodation before you can begin to actively work.
CVS Health is committed to recruiting, hiring, developing, advancing, and retaining individuals with disabilities. As such, we strive to provide equal access to the benefits and privileges of employment, including the provision of a reasonable accommodation to perform essential job functions. CVS Health can provide a request for a reasonable accommodation, including a qualified interpreter, written information in other formats, translation or other services throughColleagueRelations@CVSHealth.comIf you have a speech or hearing disability, please call 7-1-1 to utilize Telecommunications Relay Services (TRS). We will make every effort to respond to your request within 48 business hours and do everything we can to work towards a solution.
Show more
Show less","Predictive Modeling, Statistical Analysis, Data Manipulation, Deep Learning, ObjectOriented Programming, SQL, R, Python, Mathematics, Statistics, Computer Science, Business Analytics, Economics, Data Science, Machine Learning, Algorithms, Business Acumen","predictive modeling, statistical analysis, data manipulation, deep learning, objectoriented programming, sql, r, python, mathematics, statistics, computer science, business analytics, economics, data science, machine learning, algorithms, business acumen","algorithms, business acumen, business analytics, computer science, data manipulation, data science, deep learning, economics, machine learning, mathematics, objectoriented programming, predictive modeling, python, r, sql, statistical analysis, statistics"
Lead Big Data Engineer (Remote),Syrinx Consulting,"Massachusetts, United States",https://www.linkedin.com/jobs/view/lead-big-data-engineer-remote-at-syrinx-consulting-3648836284,2023-12-17,Attleboro,United States,Mid senior,Remote,"As a Lead Software Engineer, you will be directly responsible for many of the innovative features we’ll be working on, whether collaborating with our outstanding design and product team or helping our data science group work on the bleeding-edge technology to personalize technology for our ever expanding population of members.
In This Role You Will
Align and drive team members towards an inspiring vision, yet an internship to deliver value incrementally
Build highly scalable software to ensure data quality and reliability of our microservices architecture, leveraging Scala and technologies like HBase, Spark, etc.
As a hands-on lead, you will work closely with our team of software engineers and product managers to deliver rapid value
Mentor and coach technical team members
A PERFECT CANDIDATE HAS:
Experience with designing and building large scale data pipelines and data warehouses
Thrived in complex microservices ecosystem and written robust and well-performing services
The ability to bring technology to the table
Excelled in cross-functional teams, working fluidly with Product Managers, Data Scientists, Mobile Engineers, Backend Engineers, and other highly skilled specialists
Experience leading an Agile Development Team
Experience mentoring and leading people at different stages in their career
Show more
Show less","Scala, HBase, Spark, Agile Development, Microservices, Data Pipelines, Data Warehouses, Software Engineering, Product Management, Data Science, Mobile Engineering, Backend Engineering","scala, hbase, spark, agile development, microservices, data pipelines, data warehouses, software engineering, product management, data science, mobile engineering, backend engineering","agile development, backend engineering, data science, data warehouses, datapipeline, hbase, microservices, mobile engineering, product management, scala, software engineering, spark"
Data Quality Engineer,BigRio,"Massachusetts, United States",https://www.linkedin.com/jobs/view/data-quality-engineer-at-bigrio-3780005630,2023-12-17,Attleboro,United States,Mid senior,Remote,"Data Quality Engineer
Boston, MA- Remote
Contract to Hire
Must hold or be eligible for Public Trust
About BigRio:
BigRio is a remote-based, technology consulting firm with headquarters in Boston, MA. We deliver software solutions ranging from: custom development, software implementation, data analytics, and machine learning/AI integrations. We are a one-stop shop that attracts clients from a variety of industries because of our proven ability to deliver cutting-edge and cost-conscious software solutions.
Position Summary:
As a Data Quality Engineer, you will play a critical role in ensuring the reliability and quality data assets. You will work closely with data engineers, product owners, and stakeholders to create and execute testing strategies, identifying and addressing data quality issues early in the data lifecycle.
Key Responsibilities:
Create and execute testing strategies by working closely with data engineers, product owners, and stakeholders.
Validate the completion and accuracy of data by catching quality and integrity issues early in the life cycle.
Ensure that data quality checks are built and supported into work streams.
Understand and test database models using SQL scripting.
Conduct detailed source-to-destination data testing; identifying issues with data integrity, data accuracy, and transformation logic.
Qualifications:
Bachelor's degree in a relevant field (e.g., Computer Science, Information Technology, Data Science).
Proven experience as a Data QA or similar role.
Strong proficiency in SQL scripting for data testing and validation.
Solid understanding of data quality principles and best practices.
Experience with data quality tools and frameworks is a plus.
Excellent analytical and problem-solving skills.
Strong communication and collaboration skills to work effectively with cross-functional teams.
Detail-oriented and a passion for data accuracy and integrity.
Self-motivated and able to work independently.
Equal Opportunity Statement
BigRio is an equal opportunity employer. We prohibit discrimination and harassment of any kind based on race, religion, national origin, sex, sexual orientation, gender identity, age, pregnancy, status as a qualified individual with disability, protected veteran status, or other protected characteristic as outlined by federal, state, or local laws. BigRio makes hiring decisions based solely on qualifications, merit, and business needs at the time. All qualified applicants will receive equal consideration for employment.
Show more
Show less","SQL Scripting, Data Quality Principles, Data Quality Frameworks, Data Testing, Data Validation, Data Analytics, Machine Learning, Software Development","sql scripting, data quality principles, data quality frameworks, data testing, data validation, data analytics, machine learning, software development","data quality frameworks, data quality principles, data testing, data validation, dataanalytics, machine learning, software development, sql scripting"
Senior Data Engineer (Remote),Alldus,"Massachusetts, United States",https://www.linkedin.com/jobs/view/senior-data-engineer-remote-at-alldus-3669659422,2023-12-17,Attleboro,United States,Mid senior,Remote,"We are looking for a Senior Data Engineer to join a globally recognized Healthtech organization. The responsibilities of the position involve expanding and optimizing our data and data pipeline architecture, ensuring smooth data flow and collection for various teams. The primary focus is driving data initiatives and maintaining a consistent data delivery architecture across ongoing projects.
Technical Responsibilities:
Developing and maintaining efficient data pipelines and architecture.
Creating and managing large, intricate data sets that meet functional and non-functional business requirements.
Identifying, designing, and implementing internal process improvements, such as automating manual tasks, optimizing data delivery, and enhancing infrastructure scalability.
Establishing the necessary infrastructure for effective extraction, transformation, and loading of data from diverse sources.
Building data tools and providing valuable insights into healthcare data, operational efficiency, and key business performance metrics.
Assisting stakeholders at all levels with data-related technical issues and supporting their data infrastructure needs.
Developing data tools for internal team members to facilitate the construction and optimization of our product lines, making us an innovative industry leader.
Collaborating with subject matter experts to enhance the functionality of our data systems.
Analyzing internal and external data and processes to address specific business questions and identify opportunities for improvement.
Constructing processes that support data transformation, data structures, metadata, dependency, and workload management.
Working with minimal supervision, handling complex issues, and referring the most challenging problems to higher-level staff.
Mentoring department staff and collaborating with leadership to align with strategic goals.
Technical Skills:
Bachelor’s degree in Computer Science or a related field.
5 years of experience in data engineering, including proficiency in programming languages, coding, software deployment, ETL design and implementation, database methodologies, SQL queries, business information management applications, data lakes, data warehouses, or related areas.
Specifically, experience in writing SQL queries, stored procedures, functions, and views within relational databases (T-SQL, PL/SQL).
Expertise in optimizing SQL queries for performance using tools such as execution plans, query analyzers, SQL Server Profiler, or similar utilities.
Familiarity with building ETL data pipelines using SQL Server Integration Services (SSIS) or similar ETL tools.
Excellent communication and collaboration skills, both within the team and with business stakeholders.
Preferred experience in the healthcare industry.
Knowledge of Cloud environments (such as Snowflake, Azure) would be beneficial.
Familiarity with Agile development methodologies.
35713
Show more
Show less","Data Engineering, Data Architecture, Data Pipeline, Data Management, Data Analysis, Data Visualization, SQL, TSQL, PL/SQL, Stored Procedures, Functions, Views, Relational Databases, ETL, SSIS, Snowflake, Azure, Cloud Computing, Agile Development, Healthcare","data engineering, data architecture, data pipeline, data management, data analysis, data visualization, sql, tsql, plsql, stored procedures, functions, views, relational databases, etl, ssis, snowflake, azure, cloud computing, agile development, healthcare","agile development, azure, cloud computing, data architecture, data engineering, data management, data pipeline, dataanalytics, etl, functions, healthcare, plsql, relational databases, snowflake, sql, ssis, stored procedures, tsql, views, visualization"
"AVP, Cloud Data Lead",LPL Financial,"Massachusetts, United States",https://www.linkedin.com/jobs/view/avp-cloud-data-lead-at-lpl-financial-3762243393,2023-12-17,Attleboro,United States,Mid senior,Hybrid,"Are you a team player? Are you curious to learn? Are you interested in working in meaningful projects? Do you want to work with cutting-edge technology? Are you interested in being part of a team that is working to transform and do things differently? If so, LPL Financial is the place for you!
LPL Financial (Nasdaq: LPLA) was founded on the principle that the firm should work for the advisor, and not the other way around. Today, LPL is a leader* in the markets we serve, supporting more than 18,000 financial advisors, 800 institution-based investment programs and 450 independent RIA firms nationwide. We are steadfast in our commitment to the advisor-centered model and the belief that Americans deserve access to personalized guidance from a financial advisor. At LPL, independence means that advisors have the freedom they deserve to choose the business model, services, and technology resources that allow them to run their perfect practice and they have the freedom to manage their client relationships, because they know their clients best. Simply put, we take care of our advisors, so they can take care of their clients.
Job Overview:
We are currently looking to hire a Cloud Data Engineer (AVP) within the Data Technology organization, which is part of LPL’s Technology organization. This position is involved in all stages of cloud development, implementation, and management, and delivers data integration projects across a wide variety of systems in support of business functions and processes. This role is for a technology leader in the data domain, working closely with design authorities, architects, security and agile teams to help drive development of a new modern state of the art data platform for LPL.
Responsibilities:
Develop scalable and maintainable data integrations on AWS platform using native services
Lead an Agile team and drive focused delivery of modern scalable and resilient data solutions
Collaborate with Enterprise Architecture, Infrastructure, and Information Security teams
Be aware of and adhere to industry standards and methodologies for software development
Write automated test cases to ensure the correctness, responsiveness, and efficiency of the software product
Train and provide feedback to team members on standards, design patterns, architectural patterns and frameworks for services and integrations
Facilitate proper maintenance of technical documentation that foster reusing and refactoring of code
What are we looking for?
We want
strong collaborators
who can deliver a world-class client experience
. We are looking for people who thrive in a
fast-paced environment
,
are client-focused
,
team oriented
, and are able to execute in a way that encourages
creativity
and
continuous improvement
.
Requirements:
Bachelor’s degree and/or relevant experience in IT-related software development or support
4+ years of experience with key AWS Services, including API Gateway, EventBridge, Glue, Lambda, Kafka, Kinesis, EMR, S3, Dynamo, RedShift
Expert knowledge in Spark processing using Scala or Python
Experience with storage formats like parquet files, OCR, No-SQL databases
Experience with streaming data frameworks and tools (Kafka, Confluent Kafka, Kinesis)
Preferences:
Knowledge of AWS Cloud Data Ingestion Patterns and Practices
Experience with DevOps toolchain that enables CI/CD pipeline (e.g., Gitlab, Terraform, CloudFormation, Ansible, Puppet, Jenkins, TeamCity, Octopus, Puppet)
Experience with design and development of AWS services with high performance and scalability (e.g., Analytics, Integration, Compute, Container, Content Delivery, Storage)
Experience with development against non-relational database (e.g., key-value store, columnar store, graph database)
Experience with Agile Methodologies: Scrum/Kanban
Excellent oral and written communication skills
Curious and passionate about data & analytics
Pay Range:
$116,880-$175,320/year
Actual base salary varies based on factors, including but not limited to, relevant skill, prior experience, education, base salary of internal peers, demonstrated performance, and geographic location. Additionally, LPL Total Rewards package is highly competitive, designed to support your success at work, at home, and at play – such as 401K matching, health benefits, employee stock options, paid time off, volunteer time off, and more. Your recruiter will be happy to discuss all that LPL has to offer!
Why LPL?
At LPL, we believe that objective financial guidance is a fundamental need for everyone. As the nation’s leading independent broker-dealer, we offer an integrated platform of proprietary technology, brokerage, and investment advisor services. We provide you with a work environment that encourages your creativity and growth, a leadership team that is supportive and responsive, and the opportunity to create a career that has no limits, only amazing potential.
We are
one team on one mission.
We take care of our advisors, so they can take care of their clients.
Because our company is not too big and not too small, you can seize the opportunity to make a real impact. We are committed to supporting workplace equality, and we embrace the different perspectives and backgrounds of our employees. We also care for our communities, and we encourage our employees to do the same. This creates an environment in which you can do your best work.
Want to hear from our employees on what it’s like to work at LPL? Watch this!
We take social responsibility seriously. Learn more here
Want to see info on our benefits? Learn more here
Join the LPL team and help us make a difference by turning life’s aspirations into financial realities. Please log in or create an account to apply to this position. Principals only. EOE.
Information on Interviews:
LPL will only communicate with a job applicant directly from an
@lp
lfinancial.com
email address and will never conduct an interview online or in a chatroom forum. During an interview, LPL will not request any form of payment from the applicant, or information regarding an applicant’s bank or credit card. Should you have any questions regarding the application process, please contact LPL’s Human Resources Solutions Center at (800) 877-7210.
Show more
Show less","AWS, API Gateway, Spark, Scala, Python, Kafka, Kinesis, EMR, Dynamo, RedShift, EventBridge, Glue, Lambda, Hadoop, Gitlab, Terraform, CloudFormation, Ansible, Puppet, Jenkins, TeamCity, Octopus, Data integration, Streaming data frameworks, DevOps toolchain, CI/CD pipeline, Scrum, Kanban, Agile Methodologies, Nonrelational database, NoSQL databases","aws, api gateway, spark, scala, python, kafka, kinesis, emr, dynamo, redshift, eventbridge, glue, lambda, hadoop, gitlab, terraform, cloudformation, ansible, puppet, jenkins, teamcity, octopus, data integration, streaming data frameworks, devops toolchain, cicd pipeline, scrum, kanban, agile methodologies, nonrelational database, nosql databases","agile methodologies, ansible, api gateway, aws, cicd pipeline, cloudformation, data integration, devops toolchain, dynamo, emr, eventbridge, gitlab, glue, hadoop, jenkins, kafka, kanban, kinesis, lambda, nonrelational database, nosql databases, octopus, puppet, python, redshift, scala, scrum, spark, streaming data frameworks, teamcity, terraform"
Data Scientist,Parity Group PLC,"Cumbria, England, United Kingdom",https://uk.linkedin.com/jobs/view/data-scientist-at-parity-group-plc-3782032109,2023-12-17,Sunderland, United Kingdom,Mid senior,Hybrid,"Data Scientist - £500-£550pd (Outside IR35) – 12 Months - Hybrid/Warrington/Cumbria
Parity Group are delighted to partner with our client, an exceptional engineering organisation who are looking to recruit two experienced Data Scientists to provide support to our clients crucial Digital Asset Management team.
Key Responsibilities and Accountabilities
· Work with the wider DAM Project Team, Client Asset Management Capability, and ISO to develop the maturity of our clients digital asset management capability, codebase & toolsets.
· Develop code to implement enhancements and interfaces to both in-house products and custom developed applications.
· Work closely with cross-functional teams to analyse large and complex datasets.
· Extract valuable insights and drive data-driven decision making.
· Take responsibility for designing and implementing advanced statistical models and algorithms, as well as develop robust data-driven solutions to address business challenges.
· Showcase your experience in data analysis, machine learning and predictive modelling to optimise and improve customer experiences whilst identifying new opportunities.
Skills and Experience required
• Development with Microsoft Visual Studio IDE and browser versions
• Sprint backlog ratification.
• Data mapping of API interfaces as an enterprise
• Rapid prototyping and deployment.
• Design and delivery of security accredited solutions.
• REST API modelling (swagger), management and monitoring.
• C#
• Python
• Exposure to complex data environments.
• Information security constraints and best practice.
• Implementation of common data models and data migration required.
• Proven ability with JSON, YAML, REST, python
• Experience in the design and review of software development.
• Knowledge of both traditional and DevSecOps approaches to software implementation.
If this sounds like the kind of role you or someone you know would be interested in then please feel free to apply or email me directly with your CV for immediate consideration.
James Senior
j.senior@parity.net
01316037143
Show more
Show less","Microsoft Visual Studio IDE, Sprint Backlog Ratification, API Interfaces Data Mapping, Rapid Prototyping and Deployment, Security Accredited Solutions Design and Delivery, REST API Modelling Management and Monitoring, C#, Python, Complex Data Environments Exposure, Information Security Constraints and Best Practices, Common Data Models and Data Migration Implementation, JSON YAML REST Python, Software Development Design and Review Experience, Traditional and DevSecOps Approaches to Software Implementation","microsoft visual studio ide, sprint backlog ratification, api interfaces data mapping, rapid prototyping and deployment, security accredited solutions design and delivery, rest api modelling management and monitoring, c, python, complex data environments exposure, information security constraints and best practices, common data models and data migration implementation, json yaml rest python, software development design and review experience, traditional and devsecops approaches to software implementation","api interfaces data mapping, c, common data models and data migration implementation, complex data environments exposure, information security constraints and best practices, json yaml rest python, microsoft visual studio ide, python, rapid prototyping and deployment, rest api modelling management and monitoring, security accredited solutions design and delivery, software development design and review experience, sprint backlog ratification, traditional and devsecops approaches to software implementation"
Data Engineer,P3 Health Partners,"Henderson, NV",https://www.linkedin.com/jobs/view/data-engineer-at-p3-health-partners-3750679459,2023-12-17,Henderson,United States,Mid senior,Onsite,"People. Passion. Purpose.
At P3 Health Partners, our promise is to guide our communities to better health, unburden clinicians, align incentives and engage patients.
We are a physician-led organization relentless in our mission to overcome all obstacles by positively disrupting the business of health care, transforming it from sickness care into wellness guidance.
We are looking for a
Data Engineer
. If you are passionate about your work; eager to have fun; and motivated to be part of a fast-growing organization in Las Vegas, Nevada, then you should consider joining our team
.
Data Engineer
Overall Purpose:
The Data Engineer provides value-added analysis of population healthcare medical and pharmacy data. The incumbent demonstrates business and data understanding; provides advanced comprehensive reporting of healthcare data, statistical analysis, including predictive analytics, data manipulation, interpretation, presentation, and recommendations on specific courses of action. The Data Engineer is responsible to design and develop reports and analyze data to measure clinical outcomes, network performance and methodology levers. You will investigate key business problems through quantitative analyses of utilization and health care costs data. The Data Engineer is responsible for building and deploying Medicare Risk Adjustment and CMS Hierarchal Condition Code models into scoring tools as well as reports.
Education and Experience:
Bachelor’s degree in Business, Finance, Statistics or Health care-related field required.
Master’s degree strongly preferred.
A minimum of 3 years of experience in a healthcare analytic role demonstrating the ability to provide value-add analysis to compiled data.
A minimum of 2 years of experience in medical risk modeling and risk reporting.
Understanding of healthcare terminology and knowledgeable/experienced with some or all of the following: provider data, claims data, pharmacy data, lab data, and enrollment data.
Knowledge, Skills and Abilities:
3+ years of experience with Business Intelligence tools preferred.
3+ years of experience creating stored procedures and functions preferred.
2+ years of experience with Risk Adjustment methodologies; such as CMS HCCs, or similar strongly preferred.
Knowledge and experience of working on SQL Queries and database scripting (procedures, functions, jobs, physical data model creation).
Experience with Data Warehouse lifecycle data analysis, data modeling and design.
Strong aptitude for quickly troubleshooting and identifying the cause of questionable results within extracts and reports.
Demonstrated business analyst experience; communicating effectively with business clients to identify needs and evaluate alternative solutions.
Possesses outstanding oral and written communication skills.
Show more
Show less","Business Intelligence, SQL, Data Warehouse, Risk Adjustment, Healthcare Terminology, Data Analysis, Data Modeling, Data Manipulation, Data Interpretation, Data Presentation, Statistical Analysis, Predictive Analytics, Medicare Risk Adjustment, CMS Hierarchal Condition Code, Provider Data, Claims Data, Pharmacy Data, Lab Data, Enrollment Data, Stored Procedures, Functions","business intelligence, sql, data warehouse, risk adjustment, healthcare terminology, data analysis, data modeling, data manipulation, data interpretation, data presentation, statistical analysis, predictive analytics, medicare risk adjustment, cms hierarchal condition code, provider data, claims data, pharmacy data, lab data, enrollment data, stored procedures, functions","business intelligence, claims data, cms hierarchal condition code, data interpretation, data manipulation, data presentation, dataanalytics, datamodeling, datawarehouse, enrollment data, functions, healthcare terminology, lab data, medicare risk adjustment, pharmacy data, predictive analytics, provider data, risk adjustment, sql, statistical analysis, stored procedures"
Sr. Database Engineer,Curaleaf,"Las Vegas, NV",https://www.linkedin.com/jobs/view/sr-database-engineer-at-curaleaf-3779300237,2023-12-17,Henderson,United States,Mid senior,Onsite,"Curaleaf Holdings, Inc. (CSE: CURA) (OTCQX: CURLF) (""Curaleaf"") is a leading international provider of consumer products in cannabis with a mission to enhance lives by cultivating, sharing and celebrating the power of the plant. As a high-growth cannabis company known for quality, expertise and reliability, the Company and its brands, including Curaleaf, Select, and Grassroots provide industry-leading service, product selection and accessibility across the medical and adult-use markets. In the United States, Curaleaf currently operates in 19 states with 150 dispensaries and employs nearly 5,500 team members. Curaleaf International is the largest vertically integrated cannabis company in Europe with a unique supply and distribution network throughout the European market, bringing together pioneering science and research with cutting-edge cultivation, extraction and production. Home | Curaleaf | Cannabis with Confidence
Our corporate social responsibility is
Rooted in
Good.
We believe in taking corporate and social responsibility very seriously, from our educational outreach to national partnerships, state-wide initiatives and local causes. Giving back to the communities where we operate is important to us and helps to change old attitudes by showing the positive impact of cannabis in creating jobs, changing lives, and helping local communities. Social Responsibility | Curaleaf | Cannabis with Confidence
Our Mission:
To enhance lives by cultivating, sharing, and celebrating the power of the plant.
Our Vision:
To be the world's leading cannabis company by consistently delivering superior products and services and driving the global acceptance of cannabis.
Our Values:
Lead and Inspire.
Commit to Win.
ONE Curaleaf.
Driven to Deliver Excellence.
Curaleaf Culture:
At Curaleaf, we have a passionate commitment in everything we do. From innovative products to promoting social justice, from environmental sustainability to building world-class brands, we know that none of this is possible without our Team Members, which is why we’ve committed to fostering a diverse and inclusive experience for all.
About the Role:
The Sr. Database Engineer will play a critical role within the IT/IS Department. The ideal candidate is a self-motivated Engineer who is hands-on and has a hunger to learn new technologies. Our teams will be small, but experienced and this role will sit at the epicenter of building and managing a data warehouse, supporting custom applications, and developing best practices around implementing data pipelines. We are looking for an outstanding Data Engineer who is data-driven, uncompromisingly detail oriented, smart, efficient, and driven to help our business succeed.
What You’ll Do:
Interact with Analytics and Business teams to determine the organization’s data needs, and translate them into systems design and data pipelines.
Act as a technical change agent within the Data & Analytics team by advocating best practices.
Responsible for supporting the design and development of data warehouse and reporting-based solutions in a SaaS product.
Convert business requirements into technical solutions working with the data analytics team to implement and building the data model for the business requirement.
Implementing the migration of data from legacy systems to new solutions.
Examine and identify database structural necessities by evaluating performance, systems monitoring, and environment applications
Contribute to the technical team to assist in delivering a flexible and scalable solution.
Provide oversight and mentorship to other team members.
Ensure security and data integrity in a HIPAA, PII Compliant systems.
What You’ll Bring:
BS or MS in Computer Science, Information Technology or related field
5+ years of industry experience in Data Engineering or related field with a track record of manipulating, processing, and extracting value from large datasets using Agile implementation practices.
Hands on experience and advanced knowledge of Matillion ,SQL, Python, Snowflake and Azure Cloud services.
Strong experience in Data modeling, ETL/ELT Development, and Data Warehousing.
Various database development technologies, such as Snowflake, SQL Server, Azure SQL Database, including ETL tools like Matillion, and OLTP/OLAP.
Understanding of data modeling, data lakes, data-driven insight, data quality, and governance.
Experience in hands-on design, prototyping, development, configurations, testing and other implementation activities.
Experience with reporting tools (PowerBI and/or Tableau a plus).
Ability to work collaboratively within a team environment and capable of delivering under aggressive timelines.
What We Offer:
Competitive Pay
Medical, Dental, Vision Benefits
Healthcare and Dependent Care Flexible Spending Account (FSA)
Health Savings Account (HSA)
Retirement Plan - 401(K)
Life/AD&D Insurance
Short- and Long-Term Disability
Paid Parental Leave
Community Involvement Through our Rooted in Good Initiative
Employee Resource Groups
Employee Referral Payment Program
Paid Time Off (PTO)
Employee Assistance Program
Commuter Benefits
Employee Product Discounts
Curaleaf Awards and Achievements:
2023 Ragan’s Top Places to Work
2022 TIME100 Most Influential Companies
2020 Cannabis Doing Good’s Good Neighbor Award
2020 Minorities for Medical Marijuana’s Diversity & Inclusion Award
Follow us on Social Media
Instagram: @curaleaf.usa
Twitter: @Curaleaf_Inc
Nevada Hiring Range
$115,000—$140,000 USD
Not all benefits listed above are available to all employees at all locations.
Curaleaf is an equal opportunity employer. Curaleaf recruits, employs, trains, compensates, and promotes regardless of race, religion, color, national origin, gender identity, sexual orientation, physical ability, age, veteran status, and other protected status as requires by applicable law.
We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.
Individuals adversely impacted by the war on drugs are encouraged to apply.
Current Curaleaf employees should use our internal job board to submit an application for current openings: https://app2.greenhouse.io/internal_job_board
Show more
Show less","Matillion, SQL, Python, Snowflake, Azure Cloud services, Data modeling, ETL/ELT Development, Data Warehousing, PowerBI, Tableau, Agile implementation practices, HIPAA, PII Compliant systems","matillion, sql, python, snowflake, azure cloud services, data modeling, etlelt development, data warehousing, powerbi, tableau, agile implementation practices, hipaa, pii compliant systems","agile implementation practices, azure cloud services, datamodeling, datawarehouse, etlelt development, hipaa, matillion, pii compliant systems, powerbi, python, snowflake, sql, tableau"
"Senior Software Engineer, Data Science",Motional,"Las Vegas, NV",https://www.linkedin.com/jobs/view/senior-software-engineer-data-science-at-motional-3763119191,2023-12-17,Henderson,United States,Mid senior,Onsite,"Mission Summary:
The Metrics Engine team oversees the extraction of data to evaluate the behavior of our av-stack, all the way from devising the metric formulation based on traffic laws, ethics and safety, to extracting the information necessary to compute these metrics from the logs. Beyond evaluating system-level behavior, we also derive metrics to evaluate subsystems that collectively participate to AV system behavior.
As we are outputting a growing amount of metrics at various stages of the development process, our team is looking for a Senior Engineer to analyze the progression of our av-stack performance with respect to those metrics, and surface key trends. This person will get to make reports and recommendations that are of direct interest to many different teams across the organization, including leadership.
The Motional Las Vegas office is located less than 10 miles from the Las Vegas strip at 730 E Pilot Road, the Las Vegas office is home to one of the largest fleets of self-driving vehicles. The city's grid system of roads and being one of America's ""smart cities"" allows for extensive research and development testing.
What you'll be doing:
Analyze the output of our metric evaluator to provide high-level insights on the progress of our stack
Communicate with the Testing, Operations, and Autonomy teams to understand their day-to-day analysis needs
Provide recommendations on how to design experiments to get a stronger signal from the metrics
Question the results of certain metrics, dig into their formulations and contribute to improving them regularly
What we're looking for:
Strong Python coding experience
MSc in Statistics, Math, Physics, Computer Science or equivalent + 3 years of experience in data science in a tech-heavy industry
Python/Julia experience in statistical analysis
Commitment to rigorous analysis - if assumptions need to be made to infer results, they should be spelled out
Ability to communicate clearly to non-domain experts about potentially complex behaviors
An interest in contributing to the metric formulation and implementation
Bonus points:
Experience in evaluating human or autonomous driving behavior
Experience in robotics
The salary range for this role is an estimate based on a wide range of compensation factors including but not limited to specific skills, experience and expertise, role location, certifications, licenses, and business needs. The estimated compensation range listed in this job posting reflects base salary only. This role may include additional forms of compensation such as a bonus or company equity. The recruiter assigned to this role can share more information about the specific compensation and benefit details associated with this role during the hiring process.
Candidates for certain positions are eligible to participate in Motional's benefits program. Motional's benefits include but are not limited to medical, dental, vision, 401k with a company match, health saving accounts, life insurance, pet insurance, and more.
Salary Range
$159,000—$207,000 USD
Motional is a driverless technology company making autonomous vehicles a safe, reliable, and accessible reality. We're driven by something more.
Our journey is always people first.
We aren't just developing driverless cars; we're creating safer roadways, more equitable transportation options, and making our communities better places to live, work, and connect. Our team is made up of engineers, researchers, innovators, dreamers and doers, who are creating a technology with the potential to transform the way we move.
Higher purpose, greater impact.
We're creating first-of-its-kind technology that will transform transportation. To do so successfully, we must design for everyone in our cities and on our roads. We believe in building a great place to work through a progressive, global culture that is diverse, inclusive, and ensures people feel valued at every level of the organization. Diversity helps us to see the world differently; it's not only good for our business, it's the right thing to do.
Scale up, not starting up.
Our team is behind some of the industry's largest leaps forward, including the first fully-autonomous cross-country drive in the U.S, the launch of the world's first robotaxi pilot, and operation of the world's longest-standing public robotaxi fleet. We're driven to scale; we're moving towards commercialization of our technology, and we need team members who are ready to embrace change and challenges.
Formed as a joint venture between Hyundai Motor Group and Aptiv, Motional is fundamentally changing how people move through their lives. Headquartered in Boston, Motional has operations in the U.S and Asia. For more information, visit www.Motional.com and follow us on Twitter, LinkedIn, Facebook, Instagram and YouTube.
Motional AD Inc. is an EOE. We celebrate diversity and are committed to creating an inclusive environment for all employees. To comply with Federal Law, we participate in E-Verify. All newly-hired employees are queried through this electronic system established by the DHS and the SSA to verify their identity and employment eligibility.
Show more
Show less","Python, Statistics, Math, Physics, Computer Science, Data science, Julia, Autonomous driving, Robotics","python, statistics, math, physics, computer science, data science, julia, autonomous driving, robotics","autonomous driving, computer science, data science, julia, math, physics, python, robotics, statistics"
Staff Data Engineer,Recruiting from Scratch,"Las Vegas, NV",https://www.linkedin.com/jobs/view/staff-data-engineer-at-recruiting-from-scratch-3744394780,2023-12-17,Henderson,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our client is a dating app.
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, deliver analysis to further improve engagement in existing features, and empower our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Solve technical problems of the highest scope and complexity
Exert significant influence on the company’s analytical long-range goals and data architecture
Define and extend our internal standards for style, maintenance, and best practices for a high-scale data platform
Provide mentorship for all on your team to help them grow in their technical responsibilities
Propose ideas to improve the scale, performance, and capabilities of the Data Platform
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We'll Love About You
7+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field
7+ years experience using Python
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention
Location
: This role will be located in office twice a week minimum in Palo Alto, San Francisco or Chicago.
Salary Range: $180,000 - 250,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Data Engineering, Business Intelligence, Data Science, Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, PySpark, SQL, TDD, Pair Programming, Continuous Integration, Automated Testing, Deployment, Kafka, Storm, SparkStreaming, Dimensional Data Modeling, Schema Design, Data Warehouses, ETL, Data Compliance, Data Classification, Data Retention","data engineering, business intelligence, data science, python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, tdd, pair programming, continuous integration, automated testing, deployment, kafka, storm, sparkstreaming, dimensional data modeling, schema design, data warehouses, etl, data compliance, data classification, data retention","airflow, automated testing, business intelligence, continuous integration, data classification, data compliance, data engineering, data retention, data science, data warehouses, deployment, dimensional data modeling, docker, etl, helm, kafka, kubernetes, pair programming, python, schema design, snowflake, spark, sparkstreaming, sql, storm, tdd"
VTTC Database Engineer -,"Mission Technologies, a division of HII","Nellis AFB, NV",https://www.linkedin.com/jobs/view/vttc-database-engineer-at-mission-technologies-a-division-of-hii-3778309268,2023-12-17,Henderson,United States,Mid senior,Onsite,"Requisition Number: 17110
Required Travel: 0 - 10%
Employment Type: Full Time/Salaried/Exempt
Hours Per Week: 40
Security Clearance: Top Secret
Level of Experience: Senior
Summary
HII Mission Technologies Division is seeking a Database Engineer to work on a team executing integration efforts on the Modeling, Simulation, and Training environment at the Virtual Test and Training Center (VTTC) to support the U.S. Air Force Warfare Center, Joint Simulation Environment (JSE) team, at Nellis, NV. Part of a team that provides on-site M&S support for the U.S. Air Force Warfare Center (USAFWC) and Air Combatant Command (ACC); Establish and deliver high-end Advanced Tactics, Training, and Test capability for Government personnel operating under USAFWC/ACC; Provide mission execution and support to broad range of Virtual Test & Training Center (VTTC) simulator activities and Joint Simulation Environment (JSE), including training combat/tactical warfighters, C4ISR/remote sensing capabilities integration and training, and battle-space awareness; Assist Government in expansion of centrally managed capabilities beyond Nellis AFB, providing distributed VTTC training support to other Service/Joint partner facilities, including the Distributed Training Center (DTC), and the Distributed Mission Operations Center (DMOC).
HII builds the world’s most powerful, survivable naval ships and defense technology solutions that safeguard our seas, sky, land, space and cyber. Our diverse workforce of more than 43,000 includes skilled tradespeople; artificial intelligence, machine learning (AI/ML) experts; engineers; technologists; scientists; logistics experts; and business administration professionals. Recognized as one of America’s top large employers, we are a values and ethics driven organization that puts people’s safety and well-being first.
For a short video about Mission Technologies, click here: https://hii.com/capabilities/mission-technologies/ #
What You Will Do
Candidate will be part of a larger team that addresses overarching systems administration, networking, Information Technology support for a robust integration effort supporting the integration of multiple complex JSE based VTTC simulators and advanced JSE Battlespace enclaves with various security levels up to TS and beyond.
The candidate will be familiar with Information Technology (IT) functions across a broad range of hardware and software.
Supports both Microsoft and Linux based Operating Systems to ensure seamless and uninterrupted services
Ability to manage and maintain system licenses, updates, patches and status of all associated hardware/software.
Create and administer virtual machines (VM), enable LAN technologies, configure network layer 2 / 3 devices, configure DNS settings, maintain active directory, support/maintain time sync servers, develop and maintain group policies, provide shared folder resources, Support thin/zero client architecture, establish and maintain accounts (user access and logins), and support the integration of specialized JSE applications and software, both Government off-the-shelf (GOTS) and Commercial off-the-shelf (COTS).
Support configuration management by creating and maintaining standard operating procedures, Developing a concept of operations for the system, maintaining system checklists, monitoring performance and functionality of all systems/enclaves/networks, supporting audit requirements, maintaining complete hardware and software inventories, conducting asset / change management, establishing and maintaining the systems performance benchmarks, physical, and functional configuration documentation (e.g., baselines, network diagrams, physical topology, etc.).
Maintain large GPU arrays, maintain data storage, backups, virtual disks, and configure all servers and network devices.
Provide system administration support for the management of Tactical Local Area Network Encryptors (TACLANEs), firewalls, and Intrusion Detection / Prevention Systems.
What You Must Have
Must have 9 years relevant experience with Bachelors in related field; 7 years relevant experience with Masters in related field; 4 years relevant experience with PhD or Juris Doctorate in related field; or High School Diploma or equivalent and 13 years relevant experience.
Must have a current Information Assurance Technical (IAT) Level II certification - https://public.cyber.mil/wid/cwmp/dod-approved-8570-baseline-certifications/
Must have excellent communication, interpersonal and presentation skills.
Must have ability to work flexible schedule to support customer requirements.
Must hold an active or current DoD TS/SCI Clearance.
Must be a U.S. Citizen --
Physical Requirements
May require working in an office, industrial, shipboard, or laboratory environment. Capable of climbing ladders and tolerating confined spaces and extreme temperature variances.
This opportunity resides with
Live, Virtual, Constructive Solutions
, a business group within HII’s Mission Technologies division. As a trusted partner to our military customers, we design, develop and operate systems that bring together service members from across the globe to help you train like you fight, because we understand that preparation requires full coordination—not readiness in piece parts.
HII is a global engineering and defense technologies provider. With a 135-year history of trusted partnerships in advancing U.S. national security, HII delivers critical capabilities ranging from the most powerful and survivable naval ships ever built, to unmanned systems, ISR and AI/ML analytics. HII leads the industry in mission-driven solutions that support and enable a networked, all-domain force. Headquartered in Virginia, HII’s skilled workforce is 44,000 strong. For more information, please visit: hii.com.
HII is committed to cultivating an inclusive company culture to promote collaboration and enhance creativity by hiring a diverse work force. HII is an Equal Opportunity/Vets and Disabled Employer. U.S. Citizenship may be required for certain positions.
Show more
Show less","Database Engineering, Modeling, Simulation, Training, JSE Integration, Virtual Test and Training Center, System Administration, Networking, Windows, Linux, Operating Systems, VM Administration, LAN Technologies, Network Devices, DNS, Active Directory, Virtual Disks, TACLANEs, Firewalls, Intrusion Detection/Prevention Systems","database engineering, modeling, simulation, training, jse integration, virtual test and training center, system administration, networking, windows, linux, operating systems, vm administration, lan technologies, network devices, dns, active directory, virtual disks, taclanes, firewalls, intrusion detectionprevention systems","active directory, database engineering, dns, firewalls, intrusion detectionprevention systems, jse integration, lan technologies, linux, modeling, network devices, networking, operating systems, simulation, system administration, taclanes, training, virtual disks, virtual test and training center, vm administration, windows"
Senior Data Analyst,Bluberi,"Las Vegas, NV",https://www.linkedin.com/jobs/view/senior-data-analyst-at-bluberi-3761402587,2023-12-17,Henderson,United States,Mid senior,Onsite,"Description
BLUBERI VALUES:
We Are the New School
Clarity and Transparency
Find It, Fix It, Own It
Standard Operating Success
Easiest Company to do Business With
It's A Team Sport
Celebrate Wins, But Never Be Satisfied
About The Company
Headquartered in Las Vegas and backed by more than 25 years of industry experience, Bluberi is an inventive company that is dedicated to developing and delivering the highest quality casino gaming experience to players and operators. Our team is comprised of seasoned, multi-talented innovators who are focused on bringing gaming to life with entertaining games and high-performing cabinets. We are backed by the largest private equity fund in Canada with over $5 billion in assets under management and recently began expanding more aggressively into Class 2 and Class 3 markets with a potent growth plan. We are the risk-taking spirit of gambling and we add imagination and innovation to everything we create.
About The Team
The Data Team at Bluberi serves as a critical backbone to our innovative efforts in the gaming industry. We are an integral part of the Research & Development process, providing valuable insights that drive better product development. Beyond R&D, we collaborate closely with the Sales team to ensure the effective commercialization of those products.
Job Summary
At Bluberi, we're not just a leading slot gaming company; we're innovators aiming to redefine how data is leveraged in the gaming sector. This is where you come in. We're seeking a visionary Senior Data Analyst to be a transformative force within our organization.
In this pivotal role, you'll analyze and interpret complex data sets to provide actionable insights that will inform key internal stakeholders. Your expertise will have a direct impact on our Research & Development and Sales teams, aiding them in making data-driven decisions that enhance our product offerings and customer experiences. Not only will you share these invaluable insights with our internal teams, but you'll also communicate them to our customers to optimize product performance.
As the driving force behind our industry-leading performance optimization platform, you'll ensure the best possible performance of Bluberi products across all customer locations. Hands-on experience with Tableau is essential, as it's our primary tool for business intelligence and data visualization.
This role offers an unparalleled opportunity to be at the forefront of optimizing performance and generating revenue for both our customers and our business. You'll proactively manage product performance, creating a more efficient, profitable, and customer-centric operation.
If you're excited about the potential of data to transform the gaming industry and are eager to make a significant impact, this position offers an exceptional opportunity to do just that.
Essential Functions
Take the lead in driving the Bluberi Performance Optimization Program, aimed at proactive performance management of customer accounts.
Identify trends, patterns, and insights that can improve game design and user experience.
Develop and maintain dashboards, visualizations, and other reporting tools to monitor game performance.
Gather and analyze performance data from Class 2 and Class 3 properties to drive informed decisions.
Conduct property-specific analyses and develop performance optimization plans tailored to individual properties.
Responsible for expanding various data points, such as game attributes and property attributes, to enhance our analytical capabilities.
Work on customer segmentation projects to tailor our products and services more effectively to different user groups.
Collaborate with the marketing team using internal data to drive campaign initiatives and optimize marketing strategies.
Present findings to stakeholders, including executives and department heads, in a clear and actionable manner.
Work closely with the game design, development, and marketing teams to provide data-backed recommendations for game improvements and new features.
Position Requirements
Required Education And Experience
Bachelor’s or Master’s degree in Statistics, Computer Science, Business Analytics, or related field.
Minimum of 3 years of experience in data analytics in a professional setting
Proficient in Microsoft Excel for data manipulation, analysis, and visualization.
Strong knowledge of other Microsoft Office applications such as Word, PowerPoint, and Outlook is a plus.
Solid understanding and hands-on experience with SQL for database querying and data manipulation.
Experience with data visualization tools is required, with a special emphasis on proficiency in Tableau.
Preferred Education And Experience
Familiarity with the casino gaming industry is considered a strong plus.
Knowledge of programming languages such as Python and/or R for advanced data analysis.
Strong statistical analysis skills, including but not limited to regression analysis, cluster analysis, and hypothesis testing.
Excellent verbal and written communication skills for presenting findings and collaborating with both internal teams and external customers.
High level of accuracy and attention to detail, especially when handling complex data and making recommendations.
Prior experience in roles requiring client or customer interactions, especially in presenting data-driven insights.
Work environment
Quiet and within a secured office location
Physical demands
While performing the duties of this job, the employee is regularly required to talk or hear. The employee is also regularly required to stand, walk, sit, and use hands to finger, handle, or feel objects, tools or controls. The employee is occasionally required to reach with hands and arms, and to sit, climb or balance, and stoop, stretch, bend, kneel, crouch, or crawl
Specific vision abilities required by this job include close vision, distance vision, peripheral vision, depth perception, ability to see color and the ability to adjust focus. Essential responsibilities include moderate physical ability such as lift or maneuver at least ten (10) pounds, and prolonged instances of sitting during the shift
Travel Required
Ability to travel (<10%), some international
Compensation
$70,000-$85,000
Bonuses and Incentive Programs
About
About the Organization WE ARE THE NEW SCHOOL.
We are Bluberi, a group of forward-thinking individuals who embrace creative thinking and innovative ideas. We are the pioneers behind the fusion of gambling and gaming, and we enthusiastically welcome fresh perspectives.
Come with us on an exciting journey towards achieving remarkable outcomes.
EOE Statement We are an equal employment opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status or any other characteristic protected by law.
Show more
Show less","Data Analytics, Tableau, SQL, Microsoft Excel, Business Intelligence, Data Visualization, Data Mining, Data Analysis, Data Manipulation, Machine Learning, Python, R, Statistical Analysis, Regression Analysis, Cluster Analysis, Hypothesis Testing","data analytics, tableau, sql, microsoft excel, business intelligence, data visualization, data mining, data analysis, data manipulation, machine learning, python, r, statistical analysis, regression analysis, cluster analysis, hypothesis testing","business intelligence, cluster analysis, data manipulation, data mining, dataanalytics, hypothesis testing, machine learning, microsoft excel, python, r, regression analysis, sql, statistical analysis, tableau, visualization"
"Senior Data Engineer, Public Company",Recruiting from Scratch,"Las Vegas, NV",https://www.linkedin.com/jobs/view/senior-data-engineer-public-company-at-recruiting-from-scratch-3748828491,2023-12-17,Henderson,United States,Mid senior,Onsite,"This is for a client of Recruiting from Scratch.
Who is Recruiting from Scratch:
Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.
Our Client:
Our next evolution is underway as a newly public company looking to expand and continue to build meaningful experiences for our users. From social issues to original content, we’re blazing innovative paths with impact for our community, all while leveraging the latest tech stacks and striving for engineering excellence. At the heart of our work in this new chapter is a shared set of core values: openness and exploration, a bias for action, and strong support of the LGBTQ community.
With a track record of strong financial performance and plans for continued headcount growth, we’re looking to build a team of talented, passionate, and open-minded people who believe in our mission, align with our values, and are excited to work at the intersection of innovative technology and social impact. Come be a part of this exciting journey with us.
What’s so interesting about this role?
Our client is looking to bring on a Senior Data Engineer with chops in cutting edge real-time streaming technologies and ambitions to achieve high quality and reliability with TDD, automation, and continuous delivery. .
In this role, you will have the opportunity to work with our product teams, building models and APIs to drive new features, delivers analysis to further improve engagement in existing features, and empowers our business with real-time insights to drive growth in market share, engagement, and revenue. We see 1 trillion events per year and process 10TB of data daily.
What’s the job?
Design, develop and deliver data products to production, complying with internal data governance, security and scalability of our system.
Moving implementation to ownership of real-time and batch processing and data governance and policies.
Maintain and enforce the business contracts on how data should be represented and stored.
Stay on top of new technologies through R&D and prototyping to continuously improve our big data architectures and systems to streamline how we deliver value with high quality to our end users
Implementing ETL processes, moving data between systems including S3, Snowflake, Kafka, and Spark.
Work closely with our Data Scientists, SREs, and Product Managers to ensure software is high quality and meets user requirements.
What We’ll Love About You
5+ years of experience working with data at scale, including data engineering, business intelligence, data science, or related field.
7+ years experience using Python,
Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, )
Significant experience with relational databases and query authoring (SQL) in Snowflake or other distributed Databases.
Experience with agile engineering practices such as TDD, Pair Programming, Continuous Integration, automated testing, and deployment.
Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming
Experience with dimensional data modeling and schema design in Data Warehouses
Familiar with ETL (managing high-quality reliable ETL pipelines)
Be familiar with legal compliance (with data management tools) data classification, and retention.
Location: Palo Alto, San Francisco or Chicago
Salary Range: $165,000-$210,000 USD base.
Equity. Medical, Dental, Vision.
https://www.recruitingfromscratch.com/
Show more
Show less","Python, Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark, SQL, Kafka, Storm, SparkStreaming, ETL, Data Science, Machine Learning, TDD, Pair Programming, Continuous Integration, Automated Testing, Deployment, Big Data, Dimensional Data Modeling, Schema Design, Data Warehouses, Data Governance, Data Security, Data Scalability, Data Engineering, Business Intelligence","python, snowflake, airflow, kubernetes, docker, helm, spark, pyspark, sql, kafka, storm, sparkstreaming, etl, data science, machine learning, tdd, pair programming, continuous integration, automated testing, deployment, big data, dimensional data modeling, schema design, data warehouses, data governance, data security, data scalability, data engineering, business intelligence","airflow, automated testing, big data, business intelligence, continuous integration, data engineering, data governance, data scalability, data science, data security, data warehouses, deployment, dimensional data modeling, docker, etl, helm, kafka, kubernetes, machine learning, pair programming, python, schema design, snowflake, spark, sparkstreaming, sql, storm, tdd"
Data Analyst Technical Senior,Intermountain Health,"Las Vegas, NV",https://www.linkedin.com/jobs/view/data-analyst-technical-senior-at-intermountain-health-3776518494,2023-12-17,Henderson,United States,Mid senior,Onsite,"Job Description:
This position is responsible for providing analytic support for Intermountain Health Desert Region Finance and Operations functions. This position performs analysis on highly complex projects under general supervision. Acts as content expert for data related to the business area assigned. Supports business teams in reporting, research, data exploration, and care process development and tracking.
Position is 100% remote, however, currently we are unable to consider candidates for remote opportunities in the following states: California, Hawaii, Rhode Island, and Washington.
At this time, we are currently not accepting candidates that require a Visa Sponsorship
Demonstrates advanced SQL skills and understanding of SQL database design. Understands, writes, and manipulates advanced SQL queries to extract, combine, and analyze health care data from multiple sources.
Develops a deep understanding of the business, the source systems used, and the backend data from those systems. Works closely with users of source systems to solve problems and develop analytics solutions. Effectively communicates technical concepts to non-technical users.
Demonstrates strong Excel skills and ability to use Excel tools to analyze, summarize and present data in tabular and graphical form.
Assists in the identification of data quality (accuracy, completeness, etc.) issues. Assists with the process of database design for assigned projects. As appropriate, trains other individuals and teams in the routine use of data collection, analysis, and reporting and interpretation tools for measuring performance.
Creates reports and analytical applications that answer multiple related questions as well as reports with linked content, drilldown, and nested parameters. Reports are formatted in such a way that leads the user through the report in a well-organized, easy to understand fashion.
Provides project management / project leadership as appropriate to drive timelines and deliverables.
Trains others on skills and competencies required for previous levels of Data Analyst.
Collaborates with corporate and region resources to design, create, test, and implement analytic tools (databases, reports, dashboards, etc.) that support region and enterprise needs, and to improve data collection and reporting strategies.
Uses standard Intermountain Reporting tools (Tableau, Cognos) to generate reports for end users that interpret data for others to promote evidence-based decision making.
Possesses a fundamental understanding of the domain's data and associated data models and uses it to produce high quality material.
Compares data output to ensure data accuracy. Creates basic QA test plans and validates data for accuracy.
Responsible to document work / projects per enterprise-wide standards (reports, project tracking, databases, etc.).
Understands the appropriate use of indexes, partitions, ability to understand Entity Relationship Diagrams.
Uses / analyzes Intermountain data sources (EDW, IDX claims and membership, EMRs, etc.) to meet local and enterprise needs.
Provides consultation, data analysis and project management for clinical and business stakeholders across the organization. Communicates technical / analytical concepts to others.
Shares important and relevant information with the team. Proactively offers suggestions, provides resources, volunteers for assignments, and removes barriers to help the team accomplish its goals.
Contributes to Intermountain’s analyst community by participating in regular analyst community events such as ASK (Analysts Seeking Knowledge) through presenting, teaching, and mentoring.
Minimum Qualifications
Must meet one of the following:
Bachelor's degree in an analytics related field such as statistics, mathematics, information systems, computer science, finance, business management, or economics with four years of relevant work experience in a role performing data analysis.
Or
Bachelor’s degree in another area of study with six years of relevant work experience in a role performing data analysis.
Experience without a related degree will be considered if skill set and experience are robust.
Plus:
Demonstration of these skills:
Advanced SQL ability, knowledge of database design, data modeling and standardized data structures
Significant Experience in creating visually appealing, content rich and consumer centric reports and dashboards (i.e. Cognos, Tableau)
Proficiency in using word processing, spreadsheet, internet, e-mail, and scheduling applications
Highly effective verbal, written and interpersonal communication skills
Subject matter expertise in healthcare or related data (Clinical Programs, Clinical Services, Compliance, Quality, Revenue Cycle, Strategic Planning, Population Health, etc.).
Lifelong learner with a creative, collaborative approach.
Temperament to thrive in scaled agile teams (SAFe®) or similar product management practice
Preferred Qualifications
Degree in an analytics-related field such as statistics, mathematics, information systems, computer science, finance, economics, or an MBA with an emphasis in analytics.
If degree is not in an analytics related field seven years of analytics experience demonstrating proficiency performing the job essentials at this job level.
Proficiency using statistical methods and tools.
Proficiency using programming languages and tools.
Experience presenting to a variety of audiences and coaching leaders, customers and peers on data analysis best practices, trends, tools, and techniques.
Strong experience with SQL query development and tuning.
General experience in healthcare or subject matter expertise in the job area (such as Actuarial, Clinical Programs, Clinical Services, Compliance, Finance/Accounting, Human Resources, Management Engineering, Quality, Revenue Cycle, Select Health / Insurance, Strategic Planning, Costing, Chargemaster, Budgeting, etc.).
Physical Requirements:
Interact with others requiring the employee to verbally communicate information.
- and -
Operate computers and other IT equipment requiring the ability to move fingers and hands.
- and -
See and read computer monitors and documents
- and -
Remain sitting or standing for long periods of time to perform work on a computer, telephone, or other equipment.
- and -
Remain sitting or standing for long periods of time to perform work on a computer, telephone, or other equipment.
- and -
Anticipated job posting close date:
12/18/2023
Location:
Nevada Central Office
Work City:
Las Vegas
Work State:
Nevada
Scheduled Weekly Hours:
40
The hourly range for this position is listed below. Actual hourly rate dependent upon experience.
$43.06 - $67.80
We care about your well-being – mind, body, and spirit – which is why we provide our caregivers a generous benefits package that covers a wide range of programs to foster a sustainable culture of wellness that encompasses living healthy, happy, secure, connected, and engaged.
Learn more about our comprehensive benefits packages for our Idaho, Nevada, and Utah based caregivers, and for our Colorado, Montana, and Kansas based caregivers; and our commitment to diversity, equity, and inclusion.
Intermountain Health is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, age, sex, sexual orientation, gender identity, national origin, disability or protected veteran status.
Show more
Show less","SQL, Database design, Tableau, Cognos, Advanced Excel, Data analysis, Data visualization, Data modeling, Data interpretation, Data quality, Data collection, ASK, SAFe, Programming languages, Statistical methods, SQL query development, SQL tuning, Actuarial, Clinical programs, Clinical services, Compliance, Finance, Human Resources, Management Engineering, Quality, Revenue Cycle","sql, database design, tableau, cognos, advanced excel, data analysis, data visualization, data modeling, data interpretation, data quality, data collection, ask, safe, programming languages, statistical methods, sql query development, sql tuning, actuarial, clinical programs, clinical services, compliance, finance, human resources, management engineering, quality, revenue cycle","actuarial, advanced excel, ask, clinical programs, clinical services, cognos, compliance, data collection, data interpretation, data quality, dataanalytics, database design, datamodeling, finance, human resources, management engineering, programming languages, quality, revenue cycle, safe, sql, sql query development, sql tuning, statistical methods, tableau, visualization"
Sr. Data Analyst,Aston Carter,"Las Vegas, NV",https://www.linkedin.com/jobs/view/sr-data-analyst-at-aston-carter-3788821447,2023-12-17,Henderson,United States,Mid senior,Remote,"Description:*
Essential Job Functions:*
Support clients: Work and communicate closely with their clients throughout the report/dashboard development process. Provide status updates. Help clients understand their report/dashboard. Provide on-going maintenance and support.
Support HCS projects: Participate in outcome evaluation, program evaluation, and quality improvement studies. Attend meetings, answer data-related questions, and offer suggestions.
Create reports and dashboards: Create ad-hoc and routine reports. Design and develop dashboards to display key metrics and trends.
Manage data: Collect, organize, store, and share a wide variety of data.
Transform data: Clean and optimize data for analyses.
Ensure data quality: Audit data, data transformation processes, workflow, deliverables and outputs.
Perform analyses: Perform statistical analyses (descriptive and inferential analyses).
Present findings: Present data and findings in a clear and concise manner, using appropriate reporting and data visualization tools.
Create and maintain documentation: Create FDD, document report requirement, business logic and workflow. Create data dictionaries. Ensure documentation is up-to-date.
Maintain up-to-date knowledge on HCS information management systems, processes and data.
Manage compliance reporting: Maintain up-to-date knowledge of CMS, DHCS and internal compliance reporting requirement. Translate reporting requirement into reports. Work with clients to ensure accuracy of data. Submit report to external and internal agencies in a timely manner. Attend compliance trainings, meetings, and data validation webinars.
Support HCS system enhancement/implementation: Perform data-related research and testing. Stay informed of system and process changes. Identify impact on existing reports and dashboards. Modify existing reports and dashboards accordingly.
Prioritize work and keep supervisor informed: Work on multiple projects at the same time. Organize and prioritize work effectively. Inform management when requirement or due date cannot be met.
Adheres to all quality, compliance and regulatory standards to achieve HCS and SCAN outcomes.
Additional Skills & Qualifications:*
Required Education: *
Bachelor’s or Master’s degree in Analytics, Healthcare Informatics, Statistics, Computer Science, or related field.
Required Experience*
At least 5 years of experience analyzing and compiling data, preferably in a health plan setting.
Required Skills/Abilities:*
Ability to manipulate and analyze data to produce accurate results. Present results in data visualizations, dashboards, and reports.
Knowledge in CMS Medicare Advantage (Part C), CMS Prescription Drug Coverage (Part D), and CMS Special Needs Plan (SNP), and DHCS Medi-Cal Managed Care reporting requirements.
Knowledge in authorization, claims, and encounter data. Clinical code knowledge (ICD, CPT, etc) related to utilization data.
Advanced skills in Microsoft Office, SQL Transactional SQL (T-SQL), SQL Server Reporting Services (SSRS), and Tableau.
Experience in SQL Server Integration Services (SSIS), Visual Basic for Applications (VBA).
Must have analytical, communication, documentation, interpersonal, planning, presentation, problem-solving and research skills.
About Aston Carter:
Please Note: Scammers are posing as Aston Carter. We'll never contact you via Gmail, Telegram, or WhatsApp and we'll never solicit money from you.
At Aston Carter, we’re dedicated to expanding career opportunities for the skilled professionals who power our business. Our success is driven by the talented, motivated people who join our team across a range of positions – from recruiting, sales and delivery to corporate roles. As part of our team, employees have the opportunity for long-term career success, where hard work is rewarded and the potential for growth is limitless. Established in 1997, Aston Carter is a leading staffing and consulting firm, providing high-caliber talent and premium services to more than 7,000 companies across North America. Spanning four continents and more than 200 offices, we extend our clients’ capabilities by seeking solvers and delivering solutions to address today’s workforce challenges. For organizations looking for innovative solutions shaped by critical-thinking professionals, visit [AstonCarter.com.](AstonCarter.com) Aston Carter is a company within Allegis Group, a global leader in talent solutions. The company is an equal opportunity employer and will consider all applications without regards to race, sex, age, color, religion, national origin, veteran status, disability, sexual orientation, gender identity, genetic information or any characteristic protected by law. If you would like to request a reasonable accommodation, such as the modification or adjustment of the job application process or interviewing process due to a disability, please call 888-237-6835 or email [astoncarteraccommodation@astoncarter.com](mailto:%20astoncarteraccommodation@astoncarter.com) for other accommodation options. However, if you have questions about this position, please contact the Recruiter located at the bottom of the job posting. The Recruiter is the sole point of contact for questions about this position.
Show more
Show less","Data Analysis, Data Visualization, Data Management, Data Transformation, Data Quality Assurance, Statistical Analysis, Reporting, Dashboard Design, SQL, TSQL, SSRS, Tableau, SSIS, VBA, CMS Medicare Advantage (Part C), CMS Prescription Drug Coverage (Part D), CMS Special Needs Plan (SNP), DHCS MediCal Managed Care, Authorization, Claims, Encounter Data, ICD, CPT, Microsoft Office, Analytical Skills, Communication Skills, Documentation Skills, Interpersonal Skills, Planning Skills, Presentation Skills, ProblemSolving Skills, Research Skills","data analysis, data visualization, data management, data transformation, data quality assurance, statistical analysis, reporting, dashboard design, sql, tsql, ssrs, tableau, ssis, vba, cms medicare advantage part c, cms prescription drug coverage part d, cms special needs plan snp, dhcs medical managed care, authorization, claims, encounter data, icd, cpt, microsoft office, analytical skills, communication skills, documentation skills, interpersonal skills, planning skills, presentation skills, problemsolving skills, research skills","analytical skills, authorization, claims, cms medicare advantage part c, cms prescription drug coverage part d, cms special needs plan snp, communication skills, cpt, dashboard design, data management, data quality assurance, data transformation, dataanalytics, dhcs medical managed care, documentation skills, encounter data, icd, interpersonal skills, microsoft office, planning skills, presentation skills, problemsolving skills, reporting, research skills, sql, ssis, ssrs, statistical analysis, tableau, tsql, vba, visualization"
Mathematics Statistician or Statistician (Data Scientist) Direct Hire,Internal Revenue Service,"Birmingham, AL",https://www.linkedin.com/jobs/view/mathematics-statistician-or-statistician-data-scientist-direct-hire-at-internal-revenue-service-3769169967,2023-12-17,Alabama,United States,Associate,Onsite,"Click 'Apply' to view all available locations.
Summary
Positions under this announcement are being filled using a Direct Hire Authority (DHA). For information on the Recruitment Incentive please reference the Additional Information section of this announcement. Click on ""Learn more about this agency"" button below to view Eligibilities being considered and other IMPORTANT information.
WHERE CAN I FIND OUT MORE ABOUT OTHER IRS CAREERS?
Visit us on the web at www.jobs.irs.gov
Duties
WHAT IS THE LARGE BUSINESS AND INTERNATIONAL (LB&I) DIVISION? WHAT IS THE RESEARCH, APPLIED ANALYTICS, & STATISTICS (RAAS) DIVISION? WHAT IS THE TAX EXEMPT/GOVERNMENT ENTITIES (TEGE) DIVISION? WHAT IS THE SMALL BUSINESS/ SELF EMPLOYED (SB/SE) DIVISION? WHAT IS THE HUMAN CAPITAL OFFICE (HCO) DIVISION? WHAT IS THE WHISTLEBLOWER OFFICE (WO) DIVISION? A description of the business units can be found at: https://www.jobs.irs.gov/about/who/business-divisions 50 Vacancies will be filled in the following specialty areas: Large Business and International (LB&I), and Research, Applied Analytics and Statistics (RAAS). 2 Vacancies will be filled in the following specialty area: Tax Exempt and Government Entities Compliance Planning & Classification CP&C. 3 Vacancies will be filled in the following specialty area: Small Business Self Employed (SB/SE) Operations Support, Operations Support Business Development Office The following are the duties of this position at the full working level. If this vacancy includes more than one grade and you are selected at a lower grade level, you will have the opportunity to learn to perform these duties and receive training to help you grow in this position. Serves as a technical expert with responsibility for the initiation, planning, implementation, controlling, modifying and executing of all or part of an entire project, including such tasks as formulation of workload estimates for program segments, specification of the methodology to be used, preparation of appropriate specifications and procedures, and review of computer systems specifications and materials and instructions needed for assuring the adequacy of the project's design and objectives. Explores novel methods of retrieving data, and develops innovative recommendations to management and senior leadership as a means for making data driven decisions such as modification of processes and policies. Reviews, evaluates, validates, and documents the results of the findings. Assists the supervisor in determining the scope, goals and schedules for future programs in the Office. Results have a direct impact on assigned area's policy. Acts as a troubleshooter in resolving bottlenecks at any step in a project by identifying and proposing solutions; this may include coordination with other project team members or stakeholders to mitigate issues. Collaborates with business and technology partners to understand business needs, identify opportunities, and develop and implement analytical solutions. Identifies and assesses relevant data sources' validity and reliability to meet project needs. Collates, cleans, transforms, analyzes, and integrates structured and unstructured data from various sources in preparation for analysis.
Requiremennts
Telework Eligible Positions: Telework eligible positions do not guarantee telework. Employees must meet and sustain IRS telework eligibility requirements (e.g., reporting at least twice a pay period to your assigned Post of Duty (POD)) and supervisor's approval to participate in the IRS Telework Program. Employees must also be within a 200-mile radius of their official assigned post-of-duty (POD) while in a telework status. As a reminder - If you are selected for a position, you are responsible for reporting to your designated POD (location) on the negotiated start date or as directed by management. Must be a U.S. Citizen or National and provide proof of U.S. Citizenship. (Birth certificate showing birth in the U.S; Unexpired U.S. Passport; Certificate of Citizenship or Naturalization; or Report of Birth Abroad of a U.S. Citizen (Form FS-240)) Undergo a review of prior performance/conduct and an income tax verification. Refer to ""Get Your Tax Record"" at http://www.irs.gov/ (https://www.irs.gov/individuals/get-transcript) to check the status of your account, balance owed, payment history, make a payment, or review answers to tax questions. If you are not in compliance, you will be determined unsuitable for employment with IRS. Must successfully complete a background investigation, including a FBI criminal history record check (fingerprint check). Complete a Declaration for Federal Employment to determine your suitability for Federal employment, at the time requested by the agency. Go through a Personal Identity Verification (PIV) process that requires two forms of identification from the Form I-9. Federal law requires verification of the identity and employment eligibility of all new hires in the U.S. These documents must be unexpired original or certified copies.
Qualifications
Federal experience is not required. The experience may have been gained in the public sector, private sector or Volunteer Service. One year of experience refers to full-time work; part-timework is considered on a prorated basis. To ensure full credit for your work experience, please indicate dates of employment by month/year, and indicate number of hours worked per week, on your resume. You must meet the following requirements by the closing date of this announcement AND/OR time of referral: IOR BASIC REQUIREMENTS GS-1530 Statistician (Data Scientist):You must have a degree that included 15 semester hours in statistics (or in mathematics and statistics, provided at least 6 semester hours were in statistics), and 9 additional semester hours in one or more of the following: physical or biological sciences, medicine, education, or engineering; or in the social sciences including demography, history, economics, social welfare, geography, international relations, social or cultural anthropology, health sociology, political science, public administration, psychology, etc. Credit toward meeting statistical course requirements should be given for courses in which 50 percent of the course content appears to be statistical methods, e.g., courses that included studies in research methods in psychology or economics such as tests and measurements or business cycles, or courses in methods of processing mass statistical data such as tabulating methods or electronic data processing. OR COMBINATION OF EXPERIENCE AND EDUCATION: courses as shown above, plus appropriate experience or additional education. The experience should have included a full range of professional statistical work such as (a) sampling, (b) collecting, computing, and analyzing statistical data, and (c) applying statistical techniques such as measurement of central tendency, dispersion, skewness, sampling error, simple and multiple correlation, analysis of variance, and tests of significance. IOR BASIC REQUIREMENTS GS-1529 Mathematical Statistician (Data Scientist): You must have a degree that included courses in mathematics and statistics totaling at least 24 semester hours. This course work must have included a minimum of 12 semester hours of mathematics and 6 semester hours were in statistics. Courses acceptable toward meeting the mathematics course requirement must have included at least four of the following: differential calculus, integral calculus, advanced calculus, theory of equations, vector analysis, advanced algebra, linear algebra, mathematical logic, differential equations, or any other advanced course in mathematics for which one of these was a prerequisite. Courses in mathematical statistics or probability theory with a prerequisite of elementary calculus or more advanced courses will be accepted toward meeting the mathematics requirements, with the provision that the same course cannot be counted toward both the mathematics and the statistics requirement. OR COMBINATION OF EXPERIENCE AND EDUCATION: includes at least 24 semester hours of mathematics and statistics, including at least 12 hours in mathematics and 6 hours in statistics, as described above; and Experience that showed evidence of statistical work such as (a) sampling, (b) collecting, computing, and analyzing statistical data, and (c) applying known statistical techniques to data such as measurement of central tendency, dispersion, skewness, sampling error, simple and multiple correlation, analysis of variance, and tests of significance. AND GS-1529/1530-11 SPECIALIZED EXPERIENCE: To be eligible for this position at this grade level, you must meet the following requirements. In addition to the basic requirements, you must have one (1) year of specialized experience at a level of difficulty and responsibility equivalent to the GS-09 grade level in the Federal service. Examples of specialized experience for this position may include: Experience using data mining process models (such as CRISP-DM, SEMMA, etc.,) to design and execute data science project. Experience preparing and analyzing structured and unstructured datasets to explorations and evaluating data science centric models. Experience applying a range of analytic approaches, including (but not limited to) machine learning, text analytics, and natural language processing; graph theory, link analysis and optimization models; complex adaptive systems; and/or deep learning neural networks that are part of the exploration. Experience coding in various programming languages (such as R, Python, SQL, or JAVA) to conduct various phases of data science projects. Experience creating and querying different datastores and architectures (such as Sybase, Oracle, and open-source databases) to work with various types of data as part of the data science project. Experience using tools for data visualization (graphs, tables, charts, etc.,) and end-user business intelligence. OR EDUCATION: You may substitute education for specialized experience specialized experience as follows: Three (3) full academic years of progressively higher-level graduate education in Mathematics, statistics, or related fields. OR Ph. D. or equivalent doctoral degree Mathematics, statistics, or related field of study from an accredited college or university. OR COMBINATION OF EXPERIENCE AND EDUCATION: A combination of qualifying graduate education and experience equivalent to the amount required. GS-1529/1530-12 SPECIALIZED EXPERIENCE: To be eligible for this position at this grade level, you must meet the following requirements. In addition to the basic requirements, you must have one (1) year of specialized experience at a level of difficulty and responsibility equivalent to the GS-11 grade level in the Federal service. Examples of specialized experience for this position may include: Experience applying knowledge of statistical theories, principles, concepts and practices that relate to experimental design, data analysis, sampling, forecasting, quality control, and operations research to understand, model and improve program operations. Experience using data mining process models (such as CRISP-DM, SEMMA, etc.,) to design and execute data science project. Experience preparing and analyzing structured and unstructured datasets to explorations and evaluating data science centric models. Experience applying a range of analytic approaches, including (but not limited to) machine learning, text analytics, and natural language processing; graph theory, link analysis and optimization models; complex adaptive systems; and/or deep learning neural networks that are part of the exploration. Experience coding in various programming languages (such as R, Python, SQL, or JAVA) to conduct various phases of data science projects. Experience creating and querying different datastores and architectures (such as Sybase, Oracle, and open-source databases) to work with various types of data as part of the data science project. Experience using tools for data visualization (graphs, tables, charts, etc.,) and end-user business intelligence. Please see Education Section for additional qualification on the GS-13.
Education
Qualifications continued. GS-1529/1530-13 SPECIALIZED EXPERIENCE: To be eligible for this position at this grade level, you must meet the following requirements. In addition to the basic requirements, you must have one (1) year of specialized experience at a level of difficulty and responsibility equivalent to the GS-12 grade level in the Federal service. Examples of specialized experience for this position may include: Experience applying project management principles on a data science project. Experience planning and executing a variety of data science and/or analytics projects. Experience using data mining process models (such as CRISP-DM, SEMMA, etc.,) to design and execute data science project. Experience preparing and analyzing structured and unstructured datasets to explorations and evaluating data science centric models. Experience working with multiple data types and formats as a part of a data science project. Experience applying a range of analytic approaches, including (but not limited to) machine learning, text analytics, and natural language processing; graph theory, link analysis and optimization models; complex adaptive systems; and/or deep learning neural networks that are part of the exploration. Experience coding in various programming languages (such as R, Python, SQL, or JAVA) to conduct various phases of data science projects. Experience creating and querying different datastores and architectures (such as Sybase, Oracle, and open-source databases) to work with various types of data as part of the data science project. Experience using tools for data visualization (graphs, tables, charts, etc.,) and end-user business intelligence. For more information on qualifications please refer to OPM's Qualifications Standards. Education. For positions with an education requirement, or if you are qualifying for this position by substituting education or training for experience, submit a copy of your transcripts or equivalent. An official transcript will be required if you are selected. A college or university degree generally must be from an accredited (or pre-accredited) college or university recognized by the U.S. Department of Education. For a list of schools which meet these criteria, please refer to Department of Education Accreditation page. FOREIGN EDUCATION: Education completed in foreign colleges or universities may be used to meet the requirements. You must show proof the education credentials have been deemed to be at least equivalent to that gained in conventional U.S. education program. It is your responsibility to provide such evidence when applying. Click here for Foreign Education Credentialing instructions.
Additional Information
A 1-year probationary period is required. Have your salary sent to a financial institution of your choice by Direct Deposit/Electronic Funds Transfer. If you are a male applicant born after December 31, 1959, certify that you have registered with the Selective Service System or are exempt from having to do so. We may select from this announcement or any other source to fill one or more vacancies. Additional jobs may be filled. The salary range indicated in this announcement reflects the minimum locality pay up to maximum locality pay for all duty locations listed. The range will be adjusted for selected duty location. General Schedule locality pay tables may be found under Salaries & Wages. This is a bargaining unit position.
Tour of Duty:
Day Shift, Monday-Friday with start and stop times between 6:00am-6:00pm Alternative work schedule, staggered work hours or telework may be available. Relocation expenses - No. Obtain and use a Government-issued charge card for business-related travel. This is an open continuous announcement which allows applicants to apply up to the closing date of the announcement, 12/29/2023. This announcement will establish an inventory of applicants. Contingent upon funding and space requirements, one or more vacancies may be filled at the locations listed in this announcement as they come available. Applicants must apply and/or update their applications by 11:59 pm ET at the close of the following cutoff dates to be considered: 1st Cutoff date(s): 06/20/2023. 2nd cutoff: 07/10/2023.
WHY IS THE IRS A GREAT PLACE TO WORK? As an IRS employee, you will feel proud knowing you helped make America stronger while working at one of the world's biggest financial institutions. You will be part of an organization that collects over $2 trillion in revenue and processes over 200 million tax returns annually. We are proud of how we apply the tax law with integrity and fairness to all. The IRS hires talented and dedicated individuals from many backgrounds, including accounting, business, finance, law enforcement and information technology. You will be challenged & supported on a daily basis. Whether you are just starting out or looking for a new opportunity, consider an IRS career. The U.S. Department of the Treasury has a distinguished history dating back to the founding of our nation. As the steward of U.S. economic and financial systems, Treasury is a major and influential leader in today's global economy. We have over 100,000 employees across the country and around the world. Come Join the Department of the Treasury and Invest in Tomorrow.
Show more
Show less","Data Scientist, Statistician, Mathematical Statistician, R, Python, SQL, JAVA, Sybase, Oracle, Opensource databases, CRISPDM, SEMMA, Data mining process models, Machine learning, Text analytics, Natural language processing, Graph theory, Link analysis, Optimization models, Complex adaptive systems, Deep learning neural networks","data scientist, statistician, mathematical statistician, r, python, sql, java, sybase, oracle, opensource databases, crispdm, semma, data mining process models, machine learning, text analytics, natural language processing, graph theory, link analysis, optimization models, complex adaptive systems, deep learning neural networks","complex adaptive systems, crispdm, data mining process models, data scientist, deep learning neural networks, graph theory, java, link analysis, machine learning, mathematical statistician, natural language processing, opensource databases, optimization models, oracle, python, r, semma, sql, statistician, sybase, text analytics"
Supplier Relations- Data Analyst,PANGEATWO,"Birmingham, AL",https://www.linkedin.com/jobs/view/supplier-relations-data-analyst-at-pangeatwo-3784651650,2023-12-17,Alabama,United States,Associate,Onsite,"Supplier Relations - Data Analyst
$50,000-$70,000
Birmingham, AL
You will be tasked with overseeing and nurturing relationships with manufacturers and suppliers, ensuring the meticulous collection, upkeep, and provision of top-tier data. This role demands an individual with a keen eye for detail, adept communication skills, and strong organizational capabilities, fostering effective collaboration with both internal teams and external suppliers. Your pivotal responsibility as the Supplier Relationship Analyst is to safeguard the accuracy, availability, consistency, and punctuality of data, thereby contributing significantly to the overall success of the organization.
Key Responsibilities:
Supplier Relationship Management: Foster and maintain strong relationships with manufacturers and suppliers, serving as the key link between internal stakeholders and external partners. Oversee supplier contracts, assess performance, and ensure adherence to compliance standards.
Data Collection and Integrity: Work in collaboration with manufacturers and suppliers to identify data requirements, including quality standards, formats, and delivery methods. Ensure accurate and timely data collection, promptly addressing any anomalies. Implement and execute processes to enhance the quality and management of data.
Timely Data Delivery: Collaborate with manufacturers and suppliers to establish clear timelines for data delivery and enforce strict adherence. Proactively address any delays or issues in data availability through effective communication with suppliers. Implement measures to ensure timely data provision to internal teams, meeting the organization's reporting and analytics requirements.
Data Analysis and Reporting: Collaborate closely with internal teams, including IT, analytics, and finance, to define data analysis and reporting needs. Coordinate with suppliers to provide accurate and comprehensive data for reporting purposes. Analyze data trends to identify insights and opportunities for improvement.
Documentation and Records Management: Maintain accurate records of supplier contracts, agreements, and communication. Ensure proper documentation of data collection and management processes. Develop and uphold procedures and guidelines for data collection and management, ensuring standardization and consistency.
Education and Experience Requirements:
Bachelor's degree in business, supply chain management, or a related field; Master's degree is advantageous.
Proven experience in supplier management or supplier relationship management.
Thorough understanding of data management principles, data quality standards, and best practices.
Excellent communication and interpersonal skills, fostering relationships with suppliers and internal stakeholders.
Detail-oriented with an analytical mindset, proficient in identifying and rectifying data issues.
Proficient in data analysis and reporting tools.
Strong organizational and project management skills, capable of multitasking and meeting deadlines.
Preferred familiarity with supply chain operations and processes.
PangeaTwo’s
goal is to ensure that each recruitment opportunity yields rewarding dividends for both candidates and clients.
This position requires a US Citizenship or Green Card as the client is not able to provide sponsorship.
At our clients’ request, only individuals with required experience will be considered.
Please note - if you have recently submitted your resume to a PangeaTwo posting, your qualifications will be considered for other open opportunities.
Your resume will never be submitted to a client without your prior knowledge and consent to do so.
Show more
Show less","Supplier Relationship Management, Data Collection, Data Integrity, Data Delivery, Data Analysis, Data Reporting, Business, Supply Chain Management, Data Management, Data Standards, Communication, Interpersonal Skills, Data Analysis Tools, Project Management, Supply Chain Operations, US Citizenship, Green Card, Data Quality","supplier relationship management, data collection, data integrity, data delivery, data analysis, data reporting, business, supply chain management, data management, data standards, communication, interpersonal skills, data analysis tools, project management, supply chain operations, us citizenship, green card, data quality","business, communication, data analysis tools, data collection, data delivery, data integrity, data management, data quality, data reporting, data standards, dataanalytics, green card, interpersonal skills, project management, supplier relationship management, supply chain management, supply chain operations, us citizenship"
PIM Data Enrichment Analyst,Apt,"Birmingham, AL",https://www.linkedin.com/jobs/view/pim-data-enrichment-analyst-at-apt-3779162804,2023-12-17,Alabama,United States,Associate,Hybrid,"PIM Data Enrichment Analyst
Location:
Birmingham, AL (Hybrid)
Summary:
The PIM (Product Information Management) Data Analyst is responsible for managing and enhancing the quality of product data within the organization's PIM system. This role involves adding rich data to feeds, attaching it to base SKUs and SuperSKUs, reviewing and maintaining existing data for normalization, accuracy, and quality, reviewing and approving workflow data from various teams, reviewing final outputs for refinement and providing feedback to 3rd parties, working closely with the Governance Team, and reviewing and approving PDX Vendor Enriched Data. The PIM Data Analyst plays a crucial role in ensuring accurate and enriched product information to support business operations and customer experience.
Key Responsibilities:
Add rich data to feeds and associate it with base SKUs and SuperSKUs to enhance product information.
Review and maintain existing data within the PIM system, ensuring normalization, accuracy, and quality.
Process workflow data to ensure compliance with established standards and guidelines.
Review and approve data provided by external teams, including compliance, marketing, web owners, and feedback teams.
Review final outputs, such as actual websites, and refine as needed to ensure accurate and consistent product information.
Provide feedback to third-party vendors regarding data quality and enrichment efforts.
Collaborate closely with the Governance Team to align with data governance policies and standards.
Review and approve PDX Vendor Enriched Data for integration into the PIM system.
Work with cross-functional teams to ensure seamless data integration and accuracy across multiple systems.
Stay updated with industry best practices and emerging trends in PIM and data management.
Qualifications and Experience:
Bachelor's degree in Computer Science, Information Systems, or a related field.
Proven experience as a PIM Data Analyst or similar role, preferably in a retail or e-commerce environment.
Strong understanding of product data management principles and best practices.
Experience with PIM systems and tools, such as Akeneo, inRiver, or Stibo Systems.
Proficient in data entry, data cleansing, and data validation techniques.
Excellent attention to detail and accuracy in data handling.
Strong analytical and problem-solving skills to identify and resolve data-related issues.
Familiarity with workflow processing and approval mechanisms.
Experience working with cross-functional teams and stakeholders, including Compliance, Marketing, and Web Owners.
Excellent communication skills to effectively collaborate and communicate complex information.
Ability to review and refine data outputs based on real-world usage and customer feedback.
Familiarity with data governance principles and compliance requirements.
Knowledge of PDX (Product Data eXchange) standards is a plus.
Show more
Show less","Data Enrichment, Data Management, Product Data Management, PIM Systems, Akeneo, inRiver, Stibo Systems, Data Entry, Data Cleansing, Data Validation, Data Analytics, Data Quality, Data Enrichment, Data Governance, Workflow Management, CrossFunctional Collaboration, Compliance, Marketing, Web Development, Communication, Customer Feedback, PDX Product Data eXchange","data enrichment, data management, product data management, pim systems, akeneo, inriver, stibo systems, data entry, data cleansing, data validation, data analytics, data quality, data enrichment, data governance, workflow management, crossfunctional collaboration, compliance, marketing, web development, communication, customer feedback, pdx product data exchange","akeneo, communication, compliance, crossfunctional collaboration, customer feedback, data enrichment, data entry, data governance, data management, data quality, data validation, dataanalytics, datacleaning, inriver, marketing, pdx product data exchange, pim systems, product data management, stibo systems, web development, workflow management"
Data Analyst,Southern Shirt (SSCO),"Birmingham, AL",https://www.linkedin.com/jobs/view/data-analyst-at-southern-shirt-ssco-3732434805,2023-12-17,Alabama,United States,Mid senior,Onsite,"Are you ready to dive headfirst into the dynamic world of data? At SSCO, we believe that data is the compass guiding our journey to success. As a Data Analyst, you'll be at the forefront of this data-driven adventure, unlocking the secrets hidden within our financial and operational data. Your mission: To transform numbers into insights, charts into revelations, and spreadsheets into strategic plans.
As a Data Analyst, you will play a pivotal role in developing and implementing data-driven solutions that will maximize the effectiveness of the company's operations and decision-making processes. You will have the opportunity to work closely with various teams to understand their data needs and help translate them into actionable insights. If you're up for the challenge, this is the position for you.
Responsibilities
Data Analysis and Reporting:
Collect, analyze, and interpret financial and operational data to identify trends, patterns, and opportunities
Generate reports to provide insights into key performance metrics, budget variances, and operational efficiency
Collaborate with cross-functional teams to understand business requirements and develop reports and dashboards
Develop and implement data-driven solutions that will inform business decisions
Financial Performance Analysis:
Conduct in-depth financial analysis to evaluate business performance, pricing strategies, and cost optimization
Assist in the preparation of financial forecasts, budgeting, and scenario modeling to support strategic decision-making
Present findings to management, highlighting insights and suggesting actionable recommendations
Business Intelligence Solutions:
Work with cross-functional teams and 3rd party vendors to define data requirements, develop data models, and ensure the accuracy and availability of relevant data for analysis
Contribute to the development and maintenance of business intelligence tools, dashboards, and reporting systems
Continuously explore innovative BI solutions to enhance data visualization and accessibility
Collaborate with cross-functional teams to design and execute IT solutions aligned with business objectives
Integrate emerging technologies and platforms to enhance operational efficiency and productivity
Market and Competitive Analysis:
Monitor market trends, competitor performance, and industry benchmarks to provide insights on market dynamics, opportunities, and marketing KPIs
Collaborate with marketing and sales teams to identify potential growth areas and customer preferences
Financial Modeling and Forecasting:
Assist in building financial models to assess various scenarios and support long-term financial planning
Collaborate with finance and operations teams to align forecasts with business objectives and market trends
Advisory and Collaboration:
Collaborate with stakeholders across departments to understand business needs and provide data-driven insights for strategic decisions
Serve as a subject matter expert on financial and operational data analysis, offering guidance and training as needed
Process Documentation:
Document business processes and procedures thoroughly to ensure transparency and efficiency in our operations
Create comprehensive process documentation, including workflows and standard operating procedures
Requirements
Bachelor's degree in Finance, Business Analytics, Economics, Mathematics, or a related field; Master's degree preferred
5+ years of experience in a business intelligence or data analytics role
Proficiency in ERP systems such as Netsuite and familiar with creating dashboards, reports, and app implementation.
Proficient in data analysis and visualization tools such as Excel, Tableau, and SQL
Solid understanding of financial concepts, budgeting, and forecasting
Strong analytical skills with the ability to transform data into actionable insights
Excellent communication skills to convey complex findings to non-technical stakeholders
Detail-oriented approach to data accuracy and report generation
Ability to work independently, prioritize tasks, and meet deadlines in a fast-paced environment
Problem-solving mindset and a proactive approach to identifying opportunities for process improvement
Familiarity with the apparel, retail, or consumer product industry is a plus
Possess a growth mindset and positive, “can-do” attitude!
Why you are right for this position:
Data Detective: You thrive on unraveling data mysteries, seeing beyond the numbers to uncover trends and opportunities
Excel Wizardry: Your proficiency in Excel is legendary, and you wield SQL and data visualization tools like a pro
Strategic Visionary: You don't just crunch numbers; you connect the dots. Your insights are the roadmap to informed decisions
Communication Maven: Complex findings don't scare you; you have a knack for making data speak even to the non-technical crowd
Problem-Solving Guru: When others see challenges, you see opportunities for process improvement and innovation
Positive Catalyst: Your can-do attitude is contagious, and your passion for data fuels our journey toward excellence
If you are a driven and analytical professional with a passion for translating data into strategic insights, we invite you to apply for this pivotal role in our organization. Join us in leveraging data to guide our decisions and contribute to the continued success of SSCO into the future!
Benefits
Passionate Team: You'll be working with a group of like-minded trend-setters who share your love for what we do
Room to Grow: We're all about professional development and growth. Your creative ideas will be valued and nurtured
Creative Freedom: We encourage you to think outside the box and bring your innovative ideas to life
Competitive Benefits: We offer competitive compensation, benefits, and perks to keep you happy and motivated.
401K Retirement with employer matching
Medical, Dental, and Vision Insurance
Paid Time Off (PTO) and Holidays
Professional Growth Plans
Performance Bonus Opportunities
Health and Wellness Programs
Continued Education Allowances
Company Sponsored Luncheons and Events
Generous Employee Discounts on SSCO Product
Fun, Collaborative Work Environment with Great People!
Show more
Show less","Data Analysis, Reporting, Financial Analysis, Business Intelligence, Data Modeling, Data visualization, ERP systems, Excel, Tableau, SQL, Financial concepts, Budgeting, Forecasting, Analytical skills, Communication skills, Detailoriented, Problemsolving, Process improvement, Apparel, Retail, Consumer products, Growth mindset, Positive attitude","data analysis, reporting, financial analysis, business intelligence, data modeling, data visualization, erp systems, excel, tableau, sql, financial concepts, budgeting, forecasting, analytical skills, communication skills, detailoriented, problemsolving, process improvement, apparel, retail, consumer products, growth mindset, positive attitude","analytical skills, apparel, budgeting, business intelligence, communication skills, consumer products, dataanalytics, datamodeling, detailoriented, erp systems, excel, financial analysis, financial concepts, forecasting, growth mindset, positive attitude, problemsolving, process improvement, reporting, retail, sql, tableau, visualization"
Chief Data Engineer,Energy Jobline,"Birmingham, AL",https://www.linkedin.com/jobs/view/chief-data-engineer-at-energy-jobline-3773342351,2023-12-17,Alabama,United States,Mid senior,Onsite,"Xpertise has partnered with a leading tech, digital and data firm looking to rapidly increase the size of its workforce in the next few years. The firm recently opened a lovely Birmingham office, and its data engineering work will be a key pillar of its success. For now, we're looking for a Chief Data Engineer/Architect to lead the firm's Midlands Data Practice. Please see further details below.
Key Details
Salary: circa £(phone number removed)
Location: Birmingham + hybrid working model (remote)
Benefits: overall great package, and this is available upon request
Note: We would consider exceptional contractors looking at circa £(Apply online only) per day (it won't be any more than this).
Key Skills Desired
Hands-on data engineering experience
Experience working with either Azure, AWS, or GCP is essential
Python development experience is a huge bonus
Cloud database management: Snowflake, Databricks, Redshift, Data Factory or BigQuery
CI/CD pipeline build and management. Any tools, such as Jenkins, Azure DevOps, GitLab, GitHub Actions or similar, would be great, as they use many!
Recent cloud/data architecture work
Entrepreneurial mindset
Clear communication skills
Experience in a client-facing or consulting position
Role Overview
Genuinely, it's one of the most exciting roles we have seen this year! We effectively need a Lead/Principal/Chief/Head Engineer or Architect who is technically-gifted and can build and then lead a team of data professionals. Therefore, this role will require a mix of leadership, strategic mindset, drive and technical prowess. This will suit any data engineering leader looking for complete autonomy to build a data practice while having limited liability due to the protection of a strong, fledgling brand within the tech and data industries.
Interested? Please apply with your CV or email/message Billy Hall for further details.
Xpertise acts as an employment agency
Show more
Show less","Data engineering, Azure, AWS, GCP, Python, Snowflake, Databricks, Redshift, Data Factory, BigQuery, CI/CD, Jenkins, Azure DevOps, GitLab, GitHub Actions, Cloud architecture, Entrepreneurial mindset, Communication skills, Clientfacing, Consulting","data engineering, azure, aws, gcp, python, snowflake, databricks, redshift, data factory, bigquery, cicd, jenkins, azure devops, gitlab, github actions, cloud architecture, entrepreneurial mindset, communication skills, clientfacing, consulting","aws, azure, azure devops, bigquery, cicd, clientfacing, cloud architecture, communication skills, consulting, data engineering, data factory, databricks, entrepreneurial mindset, gcp, github actions, gitlab, jenkins, python, redshift, snowflake"
Senior Data Engineer,"Xpect Solutions, Inc","Huntsville, AL",https://www.linkedin.com/jobs/view/senior-data-engineer-at-xpect-solutions-inc-3674080514,2023-12-17,Alabama,United States,Mid senior,Onsite,"Company Overview
XPECT Solutions, Inc. has built a strong reputation by supporting our clients in meeting their strategic goals and mission objectives. We provide high quality resources for a wide range of IT and security solutions at best-value pricing. Our success is built on a solid foundation of well-vetted, highly technical personnel, a disciplined project management approach, and an overarching commitment to customer service. We develop, test, deploy, and support exceptional solutions that enhance system functionality, while maximizing reliability and availability, and ensure the tightest security.
Position Overview
Xpect Solutions is looking for a Senior ETL Engineer to join our team supporting our Federal customer out of Huntsville, AL.This position will provide the ability to make a significant impact to the mission while also allowing the candidate to grow their skills and career. Successful candidate will need to be able to maintain existing software as the transition from server-based to cloud-based occurs. The candidate will work with a GOTS-based Extract, Transform and Load (ETL) system that loads information into Postgres database with billions of rows and deals with a file repository that is in a Petabyte range. In addition, the candidate will be highly involved in the engineering planning and transition of the existing system software / data into a Cloud-based (currently AWS) solution.
Core Responsibilities
Apply machine learning techniques in the field of NLP to process unstructured data effectively.
Develop parsers to extract relevant information from different media types to transform it into structured and searchable entities.
Create and refine domain-specific taxonomies to ensure accurate classification and organization of data.
Collaborate with cross-functional teams, including data scientists and domain experts, to understand specific requirements and design effective NLP-based data processing solutions.
Implement data pipelines using AWS cloud services, such as AWS Glue, AWS Lambda, and AWS S3, to ensure scalability, reliability, and efficiency in processing large volumes of data.
Optimize ETL processes to ensure high performance, data quality, and data consistency throughout the data pipeline.
Collaborate with stakeholders to understand their needs and provide technical guidance and support for NLP-related tasks.
Requirements
Active Top-Secret Clearance with the ability to obtain an SCI
Bachelor’s degree in Engineering, Computer Science, or other related analytical, scientific, or technical discipline
At least 10 years’ experience with ETL.
Database ETL engineers should have experience with Oracle 11g/12c, Linux (CentOS, Red Hat), and Windows environments.
Software/scripting engineers should have experience performing software and scripting engineering for data ingest with Java or Python for ETL.
Experience with web services and/or microservices.
Experience with ETL tools such as NiFi or Informatica.
Experience using software repository tools such as GIT or SVN.
Strong understanding of ETL processes and best practices, with hands-on experience in building and maintaining data pipelines.
Familiarity with AWS cloud services, particularly AWS Glue, AWS Lambda, and AWS S3, for implementing scalable and efficient data pipelines.
Solid knowledge of data modeling, data warehousing, and data governance concepts.
Excellent problem-solving skills and the ability to work independently as well as collaboratively in a team environment.
Strong communication skills to effectively collaborate with stakeholders and present complex ideas and solutions in a clear and concise manner.
Benefits Company Culture And Benefits
Xpect Solutions, Inc.
is a one-of-a-kind employer with a talented team that is cleared at various levels and is certified in dozens of industry-recognized certifications. Our talented staff are the key to our success. They bring the knowledge, experience and technical skills to deliver the best solutions to our customers.
We support our team by providing open communication, win-win partnerships with clients and vendors, a team-oriented culture that supports employee diversity, and a focus on professional development and growth for a long-lasting and
happy
career.
We offer a benefits package that is designed to keep our most important assets – our employees – healthy, happy, energized and moving forward. Our philosophy is simple – empower our employees with the benefits, resources and the financial incentives they need to be successful.
Below is an overview of our amazing benefits and perks:
A competitive Medical, Dental, and Vision plan.
Retirement Savings Plan (with company match)
Life Insurance
AD&D Insurance
Short Term and Long Term Disability Insurance
3 weeks of annual PTO
11 days of Holiday PTO
Performance Awards
Referral Bonus Plan (of up to $2,500/year)
Education Reimbursement/Training (of up to $2,500/year)
Xpect Solutions, Inc. is an equal opportunity and affirmative action employer, who participates in E-Verify. All qualified applicants will receive consideration for employment without regard to race, color, national origin, gender, religion, age, disability, veteran’s status, or any other classification as required by applicable law. EOE/M/F/D/V
Show more
Show less","Machine Learning, NLP, Data Engineering, Data Processing, Data Pipelines, AWS Glue, AWS Lambda, AWS S3, ETL, Oracle 11g/12c, Linux, Windows, Java, Python, Web Services, Microservices, NiFi, Informatica, GIT, SVN, Data Modeling, Data Warehousing, Data Governance","machine learning, nlp, data engineering, data processing, data pipelines, aws glue, aws lambda, aws s3, etl, oracle 11g12c, linux, windows, java, python, web services, microservices, nifi, informatica, git, svn, data modeling, data warehousing, data governance","aws glue, aws lambda, aws s3, data engineering, data governance, data processing, datamodeling, datapipeline, datawarehouse, etl, git, informatica, java, linux, machine learning, microservices, nifi, nlp, oracle 11g12c, python, svn, web services, windows"
Senior Data Engineer,"Xpect Solutions, Inc","Huntsville, AL",https://www.linkedin.com/jobs/view/senior-data-engineer-at-xpect-solutions-inc-3681907677,2023-12-17,Alabama,United States,Mid senior,Onsite,"Company Overview
XPECT Solutions, Inc. has built a strong reputation by supporting our clients in meeting their strategic goals and mission objectives. We provide high quality resources for a wide range of IT and security solutions at best-value pricing. Our success is built on a solid foundation of well-vetted, highly technical personnel, a disciplined project management approach, and an overarching commitment to customer service. We develop, test, deploy, and support exceptional solutions that enhance system functionality, while maximizing reliability and availability, and ensure the tightest security.
Position Overview
Xpect Solutions is looking for a Senior ETL Engineer to join our team supporting our Federal customer out of Huntsville, AL.This position will provide the ability to make a significant impact to the mission while also allowing the candidate to grow their skills and career. Successful candidate will need to be able to maintain existing software as the transition from server-based to cloud-based occurs. The candidate will work with a GOTS-based Extract, Transform and Load (ETL) system that loads information into Postgres database with billions of rows and deals with a file repository that is in a Petabyte range. In addition, the candidate will be highly involved in the engineering planning and transition of the existing system software / data into a Cloud-based (currently AWS) solution.
Core Resbonsibilities
Design, develop, and maintain the ETL (Extract, Transform, Load) processes for master data management (MDM) system.
Build and optimize data pipelines to extract data from various sources, transform it into the required format, and load using Databricks and AWS services.
Collaborate with stakeholders to gather data requirements, understand data sources, and ensure data quality and integrity throughout the ETL process.
Implement data validation, cleansing, and enrichment techniques to improve the accuracy and completeness of data.
Monitor and troubleshoot ETL processes to identify and resolve issues in a timely manner.
Requirements
Active Top-Secret Clearance with the ability to obtain an SCI
Bachelor’s degree in Engineering, Computer Science, or other related analytical, scientific, or technical discipline.
At least 10 years experience with ETL.
Database ETL engineers should have experience with Oracle 11g/12c, Sun Solaris OS, Linux (CentOS, Red Hat), and Windows environments.
Strong proficiency in programming languages such as Scala, or Java.
Experience in designing and developing ETL workflows using tools like Apache Spark or AWS Glue.
In-depth knowledge of ETL best practices, data integration techniques, and data quality management.
Familiarity with different data storage technologies and databases, such as Amazon S3 or Amazon Redshift.
Preferred Qualifications
Understanding of concepts of Data Lakehouse architecture is a plus.
Benefits
Company Culture and Benefits:
Xpect Solutions, Inc.
is a one-of-a-kind employer with a talented team that is cleared at various levels and is certified in dozens of industry-recognized certifications. Our talented staff are the key to our success. They bring the knowledge, experience and technical skills to deliver the best solutions to our customers.
We support our team by providing open communication, win-win partnerships with clients and vendors, a team-oriented culture that supports employee diversity, and a focus on professional development and growth for a long-lasting and
happy
career.
We offer a benefits package that is designed to keep our most important assets – our employees – healthy, happy, energized and moving forward. Our philosophy is simple – empower our employees with the benefits, resources and the financial incentives they need to be successful.
Below is an overview of our amazing benefits and perks:
A competitive Medical, Dental, and Vision plan.
Retirement Savings Plan (with company match)
Life Insurance
AD&D Insurance
Short Term and Long Term Disability Insurance
3 weeks of annual PTO
11 days of Holiday PTO
Performance Awards
Referral Bonus Plan (of up to $2,500/year)
Education Reimbursement/Training (of up to $2,500/year)
Xpect Solutions, Inc. is an equal opportunity and affirmative action employer, who participates in E-Verify. All qualified applicants will receive consideration for employment without regard to race, color, national origin, gender, religion, age, disability, veteran’s status, or any other classification as required by applicable law. EOE/M/F/D/V
Show more
Show less","ETL, Databricks, AWS, Scala, Java, Apache Spark, AWS Glue, Oracle 11g/12c, Sun Solaris OS, Linux (CentOS Red Hat), Windows, Amazon S3, Amazon Redshift, Data Lakehouse","etl, databricks, aws, scala, java, apache spark, aws glue, oracle 11g12c, sun solaris os, linux centos red hat, windows, amazon s3, amazon redshift, data lakehouse","amazon redshift, amazon s3, apache spark, aws, aws glue, data lakehouse, databricks, etl, java, linux centos red hat, oracle 11g12c, scala, sun solaris os, windows"
